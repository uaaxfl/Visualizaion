2007.iwslt-1.4,W06-3711,0,0.0292572,"able speech translation system which allows an English speaker to converse with a target language speaker. Our systems have been evaluated on a regular basis as part of the DARPA TransTac program. These evaluations are run by NIST, and involve military users and target language users who have never used our system before. The evaluations consist of communicating through the translation device for a number of pre-designed scenarios (which were previously unknown to us). The tests take place both indoors and outdoors. Other systems in the TransTac program include those developed by BBN [5], IBM [6], SRI, Sehda/Fluential, and USC [7] [8]. 2. Challenges The two target languages in the TransTac program are Iraqi Arabic and Farsi. Iraqi Arabic is defined as the spoken form of Arabic used by the people of Iraq in everyday conversations. It is distinct from the formal Modern Standard Arabic (MSA) used in written communication. As Iraqi Arabic is normally not written, even with transcription conventions there is greater variability in the spelling conventions than in a standard written language. Farsi (Persian), mainly spoken in Iran and areas of Afghanistan, also uses the Arabic script, thoug"
2007.iwslt-1.4,2005.mtsummit-papers.33,1,0.758733,"ple raftin vs. raftid ""you went"". The word forms (inside of the word) may be modified to represent their colloquial pronunciation for instance khune vs. khAne 'house', midam vs. midaham 'i give'). 5.4. Language models The language model is a standard 6-gram language model with Good-Turing smoothing implemented as a suffix array (SA LM) [10]. Another option of language model is the 4-gram modified Kneser-Ney smoothing trained using the SRI language modeling toolkit (SRI LM) [11]. 5.5. Translation models 5.5.1. PESA phrase extraction In Iraqi-English we applied the PESA phrase extraction method [9]. For a given source phrase PESA tries to find the optimal sentence splits of the training sentences containing this source phrase based on inner and outer IBM1 word alignment probabilities. We applied PESA as an online phrase extraction which means that phrase pairs are dynamically extracted from the training data as needed during the translation of the test set. We compared the performance here with a standard Pharaoh phrase table but we saw considerable improvements using the PESA approach. For Iraqi-English a considerable amount of training data is available and parts of the test dialogs a"
2007.iwslt-1.4,2005.eamt-1.39,1,0.834249,"lization steps need to be agreed upon. However, it is not easy to reach a consensus since Iraqi Arabic lacks a standard writing system. Furthermore, there are issues with speaking style. Words can be used with their formal or informal/colloquial endings for example raftin vs. raftid ""you went"". The word forms (inside of the word) may be modified to represent their colloquial pronunciation for instance khune vs. khAne 'house', midam vs. midaham 'i give'). 5.4. Language models The language model is a standard 6-gram language model with Good-Turing smoothing implemented as a suffix array (SA LM) [10]. Another option of language model is the 4-gram modified Kneser-Ney smoothing trained using the SRI language modeling toolkit (SRI LM) [11]. 5.5. Translation models 5.5.1. PESA phrase extraction In Iraqi-English we applied the PESA phrase extraction method [9]. For a given source phrase PESA tries to find the optimal sentence splits of the training sentences containing this source phrase based on inner and outer IBM1 word alignment probabilities. We applied PESA as an online phrase extraction which means that phrase pairs are dynamically extracted from the training data as needed during the t"
2007.iwslt-1.4,2005.eamt-1.36,1,0.72462,"ar in the training corpus, because they occur in the phrase table only embedded in longer phrases. This leads to an unnecessary high number of untranslated words. On the other side, the PESA phrase alignment will generate translations for all n-grams including all individual words, which can be found in the training corpus. To guarantee that the phrase table can cover all source vocabulary and to leverage the PESA’s strength in arbitrary long matching, we trained two phrase tables and interpolated them. The interpolation parameters are optimized through a minimum-error-rate training framework [12]. 5.5.4. Speed constraint To limit delays, the translation has to be performed during the replay of the ASR output. This has to be the case for even very long sentences. For all practical considerations we assume to have about 200 ms on average to do the translation. Some of speeding strategies we applied is phrase table pruning and restrict the search space during the decoding process. Those techniques help to decrease the system running time significantly. 5.5.5. Decoder For this evaluation the system is running on a standard laptop with 2 GB of memory so we could use our regular decoder [2]"
2007.iwslt-1.4,2007.mtsummit-papers.72,1,0.769394,"ery long sentences. For all practical considerations we assume to have about 200 ms on average to do the translation. Some of speeding strategies we applied is phrase table pruning and restrict the search space during the decoding process. Those techniques help to decrease the system running time significantly. 5.5.5. Decoder For this evaluation the system is running on a standard laptop with 2 GB of memory so we could use our regular decoder [2]. The previous system described in [1] was running on a PDA. Due to lack of memory and computing power an earlier version of the decoder described in [18] had to be used that did not support word reordering and required heavily pruned models. 5.5.6. Translation results We report the performance of translation component in terms of BLEU score [20]. On the test sets the system achieved a score of 42.12 for English to Iraqi and 63.49 for Iraqi to English. The Farsi systems use similar technologies as the Iraqi systems. Table 8 shows the translation performance of the provided training data on various setups. Table 8: Farsi translation performance (in BLEU) Farsi→English Dev. Pharaoh + 4-gram SRI LM 24.64 PESA + 6-gram SA LM 23.06 English→Farsi Pha"
2007.iwslt-1.4,koen-2004-pharaoh,0,0.0298013,"online phrase extraction does not have to extract the phrases pairs dynamically. Instead, the online phrase extraction is only used for long or rarely seen phrases. This did not give any significant change in performance but resulted in a considerable speedup. The system uses the same corpora to extract online PESA phrases for both translation directions so we combined the Iraqi-English and English-Iraqi corpora for this. However, the pre-extracted phrases were extracted separately for each direction from the respective corpus. 5.5.3. Interpolate Pharaoh and PESA We observed that the Pharaoh [19] phrase table does not contain entries for all words in the source vocabulary. This comes from the heuristics applied to avoid unlikely translations. Therefore, some words will not be translated, even though they appear in the training corpus, because they occur in the phrase table only embedded in longer phrases. This leads to an unnecessary high number of untranslated words. On the other side, the PESA phrase alignment will generate translations for all n-grams including all individual words, which can be found in the training corpus. To guarantee that the phrase table can cover all source v"
2007.iwslt-1.4,P02-1040,0,0.073748,"the search space during the decoding process. Those techniques help to decrease the system running time significantly. 5.5.5. Decoder For this evaluation the system is running on a standard laptop with 2 GB of memory so we could use our regular decoder [2]. The previous system described in [1] was running on a PDA. Due to lack of memory and computing power an earlier version of the decoder described in [18] had to be used that did not support word reordering and required heavily pruned models. 5.5.6. Translation results We report the performance of translation component in terms of BLEU score [20]. On the test sets the system achieved a score of 42.12 for English to Iraqi and 63.49 for Iraqi to English. The Farsi systems use similar technologies as the Iraqi systems. Table 8 shows the translation performance of the provided training data on various setups. Table 8: Farsi translation performance (in BLEU) Farsi→English Dev. Pharaoh + 4-gram SRI LM 24.64 PESA + 6-gram SA LM 23.06 English→Farsi Pharaoh + SRI LM 10.07 PESA + SA LM 9.45 Pharaoh + SA LM 10.41 Pharaoh + PESA + SA LM 10.23 Unseen 23.3 19.9 14.87 14.67 15.42 16.44 6. Text-to-Speech Text-to-speech was provided by Cepstral, LLC's"
2007.iwslt-1.4,2005.iwslt-1.16,0,\N,Missing
2007.iwslt-1.4,2005.iwslt-1.6,1,\N,Missing
2010.iwslt-evaluation.1,2005.iwslt-1.19,0,0.0489962,"Missing"
2010.iwslt-evaluation.1,W07-0718,0,0.0381577,"tic evaluation scores were also calculated for case-insensitive (lower-case only) MT outputs with punctuation marks removed (no_case+no_punc). 2.4. Evaluation Specifications In this section, we summarize the subjective and automatic evaluation metrics used to assess the translation quality of the primary run submissions. 2.4.1. Subjective Evaluation Human assessments of translation quality were carried out using the Ranking metrics. For the Ranking evaluation, human graders were asked to “rank each whole sentence translation from Best to Worst relative to the other choices (ties are allowed)” [5]. The Ranking evaluation was carried out using a web-browser interface and graders had to order up to five system outputs by assigning a grade between 5 (best) and 1 (worse). This year’s evaluations were carried out by paid evaluation experts, i.e., three graders for each of the target languages. The Ranking scores were obtained as the average number of times that a system was judged better than any other system. In addition, normalized ranks (NormRank) on a per-judge basis using the method of [6] were calculated for each run submission. The Ranking metric was applied to all submitted primary"
2010.iwslt-evaluation.1,2006.amta-papers.25,0,0.0771934,"Missing"
2010.iwslt-evaluation.1,zhang-etal-2004-interpreting,0,0.0672413,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.2,0,0.0412904,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.4,0,0.0757522,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.20,0,0.0355769,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.21,0,0.0337253,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.23,0,0.0323627,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.11,0,0.0546484,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.26,0,0.0352774,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.28,0,0.039409,"Missing"
2011.iwslt-evaluation.1,P02-1040,0,0.111082,"4.2. In addition, for development purposes, ASR outputs for the IWSLT 2010 development and test sets were also made available to participants. 3.3. Evaluation Specifications The participants had to provide the result of the translation of the English audio in NIST XML format. The output had to be true-cased and had to contain punctuation. The participants could either use the audio files directly, or use the output— either first best hypotheses in CTM format or word lattices in SLF — of KIT, LIUM, and FBK from the ASR task. The quality of the translations was measured automatically with BLEU [1] by scoring against the human translations created by the TED open translation project, and by human subjective evaluation (paired comparison, Section 7). Since the reference translations from the TED website match the segmentation of the reference transcriptions of the talks, 12 automatic evaluation scores for the MT outputs could be directly computed. The evaluation specifications for the SLT task were defined as case-sensitive with punctuation marks (case+punc). Tokenization scripts were applied automatically to all run submissions prior to evaluation. Moreover, automatic evaluation scores"
2011.iwslt-evaluation.1,W07-0734,0,0.031722,"Missing"
2011.iwslt-evaluation.1,niessen-etal-2000-evaluation,0,0.0557796,"Missing"
2011.iwslt-evaluation.1,2006.amta-papers.25,0,0.148776,"Missing"
2011.iwslt-evaluation.1,2003.mtsummit-papers.51,0,0.142051,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.8,0,0.040341,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.7,0,0.030857,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.10,0,0.0773305,"Missing"
2011.iwslt-evaluation.1,2011.mtsummit-papers.59,1,0.858689,"xception of sentences with less than 5 words, which were excluded from the subjective evaluation. The IWSLT 2011 subjective evaluation focused solely on the Ranking task10 and a number of novelties were introduced with respect to the traditional system ranking evaluation carried out in previous campaigns. Firstly, this year’s evaluation was not carried out by hired expert graders but by relying on crowdsourced data. The feasibility of using crowdsourcing methodologies as an effective way to reduce the costs of MT evaluation without sacrificing quality was investigated in a previous experiment [23], where the ranking evaluation of the IWSLT 2010 Arabic-English BTEC task was replicated by hiring non-experts through Amazon’s Mechanical Turk. The analysis of the collected data showed that agreement rates for non-experts were comparable to those for experts, and that the crowd-based system ranking had a very strong correlation with expert-based ranking. Secondly, the cost reduction obtained by using crowdsourcing allowed us to focus on modifying and extending the ranking methodology in different respects, with the aim of maximizing the overall evaluation reliability. The goal of the Ranking"
2011.iwslt-evaluation.1,W07-0718,0,0.0308834,"uation is to produce a complete ordering of the systems participating in a given task. The ranking task requires human judges to decide whether one system output is better than another for a given source sentence. The judgments collected through these comparisons are used to obtain the ranking scores, which are calculated as the average number of times that a system was judged better than any other system. Traditionally, in the ranking task, the judge was presented with the output of five submissions for a given source sentence and was asked to rank them from best to worst (ties were allowed) [24]. Each evaluation block contained the implicit pairwise comparisons (i.e. each system against the other systems presented in the same block) which constituted the basis of the ranking scores. 10 Last year human evaluation was also carried out for the Fluency and Adequacy metrics. In the following sections we analyze the data that we collected by posting the ranking task on Amazon’s Mechanical Although ranking a number of translated sentences relative to each other is quite intuitive, a 5-fold ranking task is less reliable than a direct comparison between only two translated sentences due to th"
2011.iwslt-evaluation.1,D09-1030,0,0.0421625,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.6,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.3,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.9,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.13,0,\N,Missing
2011.iwslt-evaluation.1,zhang-etal-2004-interpreting,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.4,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.5,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.11,0,\N,Missing
2011.iwslt-evaluation.12,2011.iwslt-evaluation.1,1,0.870478,"an.saam|sebastian.stueker|alex.waibel}@kit.edu Abstract This paper describes our English Speech-to-Text (STT) system for the 2011 IWSLT ASR track. The system consists of 2 subsystems with different front-ends—one MVDR based, one MFCC based—which are combined using confusion network combination to provide a base for a second pass speaker adapted MVDR system. We demonstrate that this set-up produces competitive results on the IWSLT 2010 dev and test sets. 1. Introduction In this paper we describe our English Speech-to-Text (STT) system with which we participated in the 2011 IWSLT STT evaluation [1]. Our system makes use of system combination and cross-adaptation, by utilising acoustic models which are trained with different acoustic front-ends. The system has been derived from our 2010 English Quaero ASR evaluation system, by taking acoustic models out of that system and combining them with a language model that has been specifically tailored to the IWSLT lecture task. 1.1. IWSLT The goal of the International Workshop on Spoken Language Translation (IWSLT) evaluation campaign is the translation of TED Talks (http://www.ted.com/talks), short 5-25min presentations by people from various f"
2011.iwslt-evaluation.16,2011.iwslt-evaluation.15,1,\N,Missing
2011.iwslt-evaluation.16,H05-1026,1,\N,Missing
2012.iwslt-evaluation.11,2011.iwslt-evaluation.1,1,0.866959,"Missing"
2012.iwslt-evaluation.11,P07-1085,0,0.0270438,"Missing"
2012.iwslt-evaluation.11,N10-1103,0,0.0548812,"Missing"
2012.iwslt-papers.10,2011.iwslt-papers.2,1,0.667623,"ited sections contained more than 1000 words have been included in the EVAL set to make it reasonably different from the EDIT set. 4.2. Baseline ASR System The baseline hypothesis which are displayed in the web interface and are editable by the user are produced with the Janus Speech Recognition Toolkit’s Ibis Decoder [15] through a confusion network combination (CNC) [13] of ﬁve speaker independent systems developed for the 2011 Quaero Evaluation as depicted in Figure 2. It is an improved version of the 2010 evaluation system [16] and similar to the Spanish system described by Kilgour et al. [17]. The underlying systems use three different frontends, melfrequency cepstral coefﬁcients (MFCC), warped minimum variance distortionless response (MVDR), and MVDR based bottleneck features (MVDR-BNF). Additionally, two systems use graphemes instead of phonemes. The system combination has been chosen to provide state-of-the-art transcripts as basis for corrections. The language model is built from the transcripts of the quaero training data, scraped newspaper data and webdumps. The vocabulary is case-sensitive and fairly large containing roughly 300k sub-words and 480k pronunciation variants. T"
2012.iwslt-papers.10,P09-1086,0,0.0689391,"Missing"
2012.iwslt-papers.10,stuker-etal-2012-kit,1,0.817133,"raction. It is possible to redirect hypotheses of online recognition into the web interface. 4. Experimental Setup A user study was performed to evaluate the correction performance of students using the web interface. Since corrections will typically take place “ofﬂine” (not during the lecture), the initial ASR hypotheses have been generated by a system combination to achieve a high-quality basis for subsequent editing. 4.1. Corpus Characteristics For the experiment, German lectures from a variety of topics were used. The lectures form a subset of the KIT Lecture Corpus for Speech Translation [14]. The lectures “Algorithms for Planar Graphs” (ALGO), “Formal Systems” (FORM), “Cognitive Systems” (COGSYS), “Machine Translation” (MT) and “Multiprocessors” (PROC) cover different areas of computer science. The “Technical Mechanics” (MECH) lecture is from an unrelated, but still technical area, whereas the lectures about “Population Geography” (GEO), “World War 2” (WW2) and ”Copyright Law” (LAW) cover non-technical topics. 1 https://developers.google.com/web-toolkit/     Figure 2: Decoding systems for generation of baseline transcription The lectures were recorded with a c"
2013.iwslt-evaluation.1,2010.iwslt-evaluation.1,1,0.863584,"al of 217 primary runs submitted. All runs were evaluated with objective metrics on a current test set and two progress test sets, in order to compare the progresses against systems of the previous years. In addition, submissions of one of the official machine translation tracks were also evaluated with human post-editing. 1. Introduction This paper overviews the results of the evaluation campaign organized by the International Workshop of Spoken Language Translation. The IWSLT evaluation has been now running for a decade and has offered along these years a variety of speech translation tasks [1, 2, 3, 4, 5, 6, 7, 8, 9]. The 2013 IWSLT evaluation continued along the line set in 2010, by focusing on the translation of TED Talks, a collection of public speeches covering many different topics. As in the previous two years, the evaluation included tracks for all the core technologies involved in the spoken language translation task, namely: • Automatic speech recognition (ASR), i.e. the conversion of a speech signal into a transcript, • Machine translation (MT), i.e. the translation of a polished transcript into another language, • Spoken language translation (SLT), that addressed the conversion and translation"
2013.iwslt-evaluation.1,2012.eamt-1.60,1,0.897031,"on tracks, many other optional translation directions were also offered. Optional SLT directions were from English to Spanish, Portuguese (B), Italian, Chinese, Polish, Slovenian, Arabic, and Persian. Optional MT translation directions were: English from/to Arabic, Spanish, Portuguese (B), Italian, Chinese, Polish, Persian, Slovenian, Turkish, Dutch, Romanian, and Russian. For each official and optional translation direction, training and development data were supplied by the organizers through the workshop’s website. Major parallel collections made available to the participants were the WIT3 [10] corpus of TED talks, all data from the WMT 2013 workshop [11], the MULTIUN corpus [12], and the SETIMES parallel corpus [13]. A list of monolingual resources was provided too, that includes both freely available corpora and corpora available from the LDC. Test data were released at the begin of each test period, requiring participants to return one primary run and optional contrastive runs within one week. The schedule of the evaluation was organized as follows: June 8, release of training data; Sept 2-8, ASR test of period; Sept 9-15, SLT test period; Oct 7-13, MT test period; Oct 7-20, test"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.21,0,0.0238082,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.11,0,0.0818098,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.5,0,0.0447354,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.6,0,0.0540129,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.9,0,0.0348889,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.23,0,0.0935135,"Missing"
2013.iwslt-evaluation.1,2005.mtsummit-papers.11,0,0.125368,"y, the standard dev2010 and tst2010 development sets have been released as well. Tables 2 and 3 provide statistics on in-domain texts supplied for training, development and evaluation purposes for the official directions. Reference results from baseline MT systems on the development set tst2010 are provided via the WIT3 repository. This helps participants and MT scientists to assess their experimental outcomes. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [38] was applied to all languages, with the exception of Chinese and Arabic languages, which were preprocessed by, respectively: the Stanford Chinese Segmenter [39]; either AMIRA [40], in the Arabic-to-English direction, or the QCRI-normalizer,3 in the English-to-Arabic direction. The baselines were developed with the Moses toolkit [41]. Translation and lexicalized reordering models were trained on the parallel training data; 5-gram LMs with improved Kneser-Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [42]. The weights of the log-linear inte"
2013.iwslt-evaluation.1,N04-4038,0,0.0999494,"tion purposes for the official directions. Reference results from baseline MT systems on the development set tst2010 are provided via the WIT3 repository. This helps participants and MT scientists to assess their experimental outcomes. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [38] was applied to all languages, with the exception of Chinese and Arabic languages, which were preprocessed by, respectively: the Stanford Chinese Segmenter [39]; either AMIRA [40], in the Arabic-to-English direction, or the QCRI-normalizer,3 in the English-to-Arabic direction. The baselines were developed with the Moses toolkit [41]. Translation and lexicalized reordering models were trained on the parallel training data; 5-gram LMs with improved Kneser-Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [42]. The weights of the log-linear interpolation model were 3 Specifically developed for IWSLT 2013 by P. Nakov and F. Al-Obaidli at Qatar Computing Research Institute. Table 3: Bilingual resources for official languag"
2013.iwslt-evaluation.1,P07-2045,1,0.010973,"This helps participants and MT scientists to assess their experimental outcomes. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [38] was applied to all languages, with the exception of Chinese and Arabic languages, which were preprocessed by, respectively: the Stanford Chinese Segmenter [39]; either AMIRA [40], in the Arabic-to-English direction, or the QCRI-normalizer,3 in the English-to-Arabic direction. The baselines were developed with the Moses toolkit [41]. Translation and lexicalized reordering models were trained on the parallel training data; 5-gram LMs with improved Kneser-Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [42]. The weights of the log-linear interpolation model were 3 Specifically developed for IWSLT 2013 by P. Nakov and F. Al-Obaidli at Qatar Computing Research Institute. Table 3: Bilingual resources for official language pairs task MTEnF r MTDeEn MTEnDe data set train dev2010 tst2010 tst2011 tst2012 tst2013 train dev2010 tst2010 dev2012 tst2013 train dev2010 tst2010 tst20"
2013.iwslt-evaluation.1,2012.amta-papers.22,1,0.822673,"Missing"
2013.iwslt-evaluation.1,2006.amta-papers.25,0,0.207086,"Missing"
2013.iwslt-evaluation.1,J93-3001,0,0.516669,"Missing"
2013.iwslt-evaluation.1,W05-0908,0,0.0754408,"Missing"
2013.iwslt-evaluation.1,1993.eamt-1.1,0,0.4595,"Missing"
2013.iwslt-evaluation.13,stuker-etal-2012-kit,1,0.826204,"ues used to build our acoustic models is given in section 5. We describe the language model used for this evaluation in section 6 and our decoding strategy and results are presented in sections 7 and 8. 1. Introduction 2. Data Resources [1] The International Workshop on Spoken Language Translation (IWSLT) offers a comprehensive evaluation campaign on spoken language translation. One part of the campaign focuses on the translation of TED Talks (http://www.ted.com/talks), short 5-25min presentations by people from various fields related in some way to Technology, Entertainment, and Design (TED) [2]. In order to evaluate different aspects of this task IWSLT organizes several evaluation tracks on this data covering the aspects of automatic speech recognition (ASR), machine translation (MT), and the full-fledged combination of the two of them into speech translation systems. The goal of the TED ASR track is the automatic transcription of TED lectures on a given segmentation, in order to interface with the machine translation components in the speech-translation track. The quality of the resulting transcriptions are measured in word error rate (WER). In this paper we describe our English AS"
2013.iwslt-papers.13,P09-2087,0,0.0632242,"Missing"
2013.iwslt-papers.13,N10-1104,0,0.0414509,"Missing"
2013.iwslt-papers.13,N07-1048,0,0.040949,"Missing"
2013.iwslt-papers.13,J96-1002,0,0.0166919,"ronunciation Generation: For the generated subword units pronunciations need to be added to the system’s dictionary. Since in general the mapping between the writing of a word and its pronunciation, i.e. phoneme sequence, is not given or easily derivable, deducting the pronunciation of the sub-word units from the pronunciation of the original words is often not straight-forward or even impossible. Often grapheme based pronunciation dictionaries can offer a solution here. 2.2. Maximum Entropy Language Models The maximum entropy approach was introduced to language modeling more than 10 years ago[13, 14, 15]. And it is being used today the state-of-the-art language models such as ModelM[16]. ModelM[16] is an exponential class-based n-gram language model. The word n-gram and word class features are incorporated into the language model within an exponential modeling framework. The model with enhanced word • Language Model Training: Based on the new vocabulary composed of the sub-word units, and potentially mixed with whole words, a new language model needs to be trained that is then used for recognition. • Word Reconstruction: After decoding, the recognized sub-words need to be recombined in order"
2013.iwslt-papers.13,N09-1053,0,0.0309328,"the system’s dictionary. Since in general the mapping between the writing of a word and its pronunciation, i.e. phoneme sequence, is not given or easily derivable, deducting the pronunciation of the sub-word units from the pronunciation of the original words is often not straight-forward or even impossible. Often grapheme based pronunciation dictionaries can offer a solution here. 2.2. Maximum Entropy Language Models The maximum entropy approach was introduced to language modeling more than 10 years ago[13, 14, 15]. And it is being used today the state-of-the-art language models such as ModelM[16]. ModelM[16] is an exponential class-based n-gram language model. The word n-gram and word class features are incorporated into the language model within an exponential modeling framework. The model with enhanced word • Language Model Training: Based on the new vocabulary composed of the sub-word units, and potentially mixed with whole words, a new language model needs to be trained that is then used for recognition. • Word Reconstruction: After decoding, the recognized sub-words need to be recombined in order to obtain a valid word sequence. 3.1. Word Decomposition and Merging For word decomp"
2013.iwslt-papers.13,2011.iwslt-evaluation.9,1,0.796347,"ring regular decoding is too computationally intensive, again we applied the language model during n-best list re-scoring. For calculating the LM score we used the three previous stems (s −3 , s −2 , s −1 ), three previous endings (e −3 , e −2 , e −1 ) and one successor stem (s1 ) and ending (s1 ) as features. The null ending is explicitly modeled with the ∼# place-holder. For training, the CRF++ Toolkit[26] is utilized. As the training of the labels, endings in our case, within a single model was not possible due to main memory usage (more than 512GB RAM was needed), a similar approach as in [18] and [19] was applied. The idea is to train a separate model for every label. Every model evaluates then only two classes: the ending, which the models stands for versus all other endings. In testing, all models, whose corresponding endings were present in the utterance, were applied. The resulting score is given by the sum of the scores from the single models. Again we re-scored the n-best lists generated by the subword system by interpolating the language model score from the maximum entropy language model with the combined acoustic and LM scores from the sub-word system. As for the interpol"
2013.iwslt-papers.13,D09-1022,0,0.0304483,"lar decoding is too computationally intensive, again we applied the language model during n-best list re-scoring. For calculating the LM score we used the three previous stems (s −3 , s −2 , s −1 ), three previous endings (e −3 , e −2 , e −1 ) and one successor stem (s1 ) and ending (s1 ) as features. The null ending is explicitly modeled with the ∼# place-holder. For training, the CRF++ Toolkit[26] is utilized. As the training of the labels, endings in our case, within a single model was not possible due to main memory usage (more than 512GB RAM was needed), a similar approach as in [18] and [19] was applied. The idea is to train a separate model for every label. Every model evaluates then only two classes: the ending, which the models stands for versus all other endings. In testing, all models, whose corresponding endings were present in the utterance, were applied. The resulting score is given by the sum of the scores from the single models. Again we re-scored the n-best lists generated by the subword system by interpolating the language model score from the maximum entropy language model with the combined acoustic and LM scores from the sub-word system. As for the interpolation des"
2013.iwslt-papers.13,W02-2018,0,0.042073,"exp  λ i f i (x, y)  , (7) pme (λ) = Z (x) i where f i (x, y) are binary feature functions. λ i are weight factors—parameters of the model. Z (x) is the normalization factor in order to ensure that result is indeed a probability distribution. 5.2. Baseline System 4.3. Training A number of algorithms can be used for estimating the parameters of a maximum entropy model. There are both–––special methods, such as Generalized Iterative Scaling[21], Improved Iterative Scaling[22], and general purpose optimization techniques, such as gradient ascent, conjugate gradient and quasi-Newton methods. [23] in its comparison of algorithms for maximum entropy parameter estimation states that the widely used iterative scaling algorithms perform quite poorly, and for all of the test problems, a limited memory variable metric algorithm outperformed the other choices. Four our experiments we used Limited-memory BFGS a limited memory variation of the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method [24, 25], which is an implementation of the variable metric method. For this we used the CRF++ Toolkit[26]. 5. Experimental Set-Up and Results We evaluated our two approaches on Russian data that was recorded"
2013.iwslt-papers.6,2011.iwslt-papers.2,1,0.620866,"BDLex dictionary generally yielded the best performance, we found that the system combination benefited from the inclusion of the output of each additional subsystem in the combination. 3.1. Pseudographeme System In a traditional grapheme-based system, the symbols of the written word are used as the sub-units of pronunciation rather than phonemes. The feasibility of using graphemes instead of phonemes in ASR has been shown in several different works [14, 15, 16]. It was also shown that the combination of a grapheme system with phoneme systems lead to a significant reduction in word-error rate [17]. While French orthography is relatively regular vis a` vis a language like English, the mapping between sounds and graphemes is not bijective, which is to say that the correspondence between graphemes and phonemes can be weak. Often, clusters of graphemes produce the same sounds as other, shorter ones, such as “-ai” and “-´e”, which both correspond to the IPA [e]. We handle this weakness by using single or multiple graphemes as the base units of pronunciation. Our set of grapheme-phones contained 49 elements, among them the same five noise phones as in the BDLex system. Using knowledge of Fre"
2013.iwslt-papers.8,stuker-etal-2012-kit,1,0.825719,"where new data for retraining comes from the same speaker, channel and related conversation topics. Following the implications of [8] we add low confidence score data to the training, but unlike in other work we apply wordbased weighting in order to compensate for errors, as it was done by [9] for acoustic model adaptation. The assumption is that erroneous data is helpful to improve system generalization. Unlike other work, e.g. [10], we refrained from a lattice-based approach. 2. Data The experiments in this paper were conducted with the help of the KIT Lecture Corpus for Speech Translation [11]. The corpus consists of recorded scientific lectures that were held at the Karlsruhe Institute of Technology (KIT). Currently the corpus mainly contains computer science lectures, and a small amount of lectures from other departments and ceremonial talks. 2.1. Training Data The speaker-independent system that we used in our experiments was trained on about 94 hours of speech from the lecture corpus. Our experiments were constrained to two distinct speakers. As training data we had 7.4 hours for speaker A and 8.3 hours for speaker B respectively, which had not been used for training the speake"
2014.iwslt-evaluation.1,2010.iwslt-evaluation.1,1,0.792952,"pants were also asked to submit runs on the 2013 test set (progress test set), in order to measure the progress of systems with respect to the previous year. All runs were evaluated with objective metrics, and submissions for two of the official text translation tracks were also evaluated with human post-editing. 1. Introduction This paper overviews the results of the 2014 evaluation campaign organized by the International Workshop of Spoken Language Translation. The IWSLT evaluation has been running now for over a decade and has offered along these years a variety of speech translation tasks [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. The 2014 IWSLT evaluation continued along the line set in 2010, by focusing on the translation of TED Talks, a collection of public speeches covering many different topics. As in the previous two years, the evaluation included tracks for all the core technologies involved in the spoken language translation task, namely: • Automatic speech recognition (ASR), i.e. the conversion of a speech signal into a transcript, • Spoken language translation (SLT), that addressed the conversion and translation of a speech signal into a transcript in another language, • Machine translation (MT), i.e. the tr"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.15,0,0.0679155,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.14,0,0.0614042,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.7,1,0.877515,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.13,0,0.0364543,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.5,1,0.82235,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.12,0,0.0879094,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.21,0,0.047922,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.10,0,0.0535615,"Missing"
2014.iwslt-evaluation.1,1993.eamt-1.1,0,0.523227,"Missing"
2014.iwslt-evaluation.1,2005.mtsummit-papers.11,0,0.0265085,"time that Italian is involved in ASR/SLT tracks, therefore no evaluation set is available for assessing progress. A single TEDx based development set was released for each pair, together with standard TED based development sets dev2010, tst2010, tst2011 and tst2012 sets. Tables 2 and 3 provides statistics on in-domain texts supplied for training, development and evaluation purposes for the official directions. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [33] was applied to all languages, with the exception of Chinese and Arabic languages, which were sent 179k 887 1,664 818 1,124 1,026 1,305 172k 887 1,565 1,433 1,700 993 1,305 1,165 1,363 1,414 182k 887 1,529 1,433 1,704 1,402 1,183 1,056 883 tokens talks En Fr 3.63M 3.88M 1415 20,1k 20,2k 8 32,0k 33,9k 11 14,5k 15,6k 8 21,5k 23,5k 11 21,7k 23,3k 16 24,8k 27,5k 15 En De 3.46M 3.24M 1361 20,1k 19,1k 8 32,0k 30,3k 11 26,9k 26,3k 16 30,7k 29,2k 15 20,9k 19,7k 16 24,8k 23,8k 15 21,6k 20,8k 7 23,3k 22,4k 9 28,1k 27,6k 10 En It 3.68M 3.44M 1434 20,1k 17,9k 8 31,0k 28,7k 10 26,9k 24,5k 16 30,7k 28,2k 15"
2014.iwslt-evaluation.1,2012.amta-papers.22,1,0.779032,"amely the MT English-German (EnDe) track and MT English-French (EnF r) track. Following the methodology introduced last year, human evaluation was based on PostEditing, and HTER (Human-mediated Translation Edit Rate) was adopted as the official evaluation metric to rank the systems. Post-Editing, i.e. the manual correction of machine translation output, has long been investigated by the translation industry as a form of machine assistance to reduce the costs of human translation. Nowadays, Computer-aided translation (CAT) tools incorporate post-editing functionalities, and a number of studies [35, 36] demonstrate the usefulness of MT to increase professional translators’ productivity. The MT TED task offered in IWSLT can be seen as an interesting application scenario to test the utility of MT systems in a real subtitling task. 5.4. Results Table 4: BLEU and TER scores of baseline SMT systems on all tst2014 sets. († ) TEDx test set. (⋆ ) Char-level scores. pair En Fr De It Ar Es Fa He Nl Pl Pt Ro Ru Sl Tr Zh direction BLEU 32.07 18.33 27.15 11.13 31.31 11.31 15.91 22.77 9.63 31.25 18.05 11.74 8.46 7.75 ⋆ 16.49 → TER 48.62 62.11 53.19 73.01 48.29 71.20 65.62 58.38 82.81 47.25 65.25 71.99 73."
2014.iwslt-evaluation.1,2006.amta-papers.25,0,0.397152,"paign, our goal was to adopt a human evaluation framework able to maximize the benefit to the research community, both in terms of information about MT systems and data and resources to be reused. With respect to other types of human assessment, such as judgments of translation quality (i.e. adequacy/fluency and ranking tasks), the post-editing task has the double advantage of producing (i) a set of edits pointing to specific translation errors, and (ii) a set of additional reference translations. Both these byproducts are very useful for MT system development and evaluation. Furthermore, HTER[37] - which consists of measuring the minimum edit distance between the machine translation and its manually post-edited version - has been shown to correlate quite well with human judgments of MT quality. First of all, for reference purposes Table 4 shows BLEU and TER scores on the tst2014 evaluation sets of the baseline systems we developed as described in Section 5.1. The results on the official test set for each participant are shown in Appendix A.1. For most languages, we show the case-sensitive and case-insensitive BLEU and TER scores. The human evaluation setup and the collection of posted"
2014.iwslt-evaluation.1,J93-3001,0,0.560866,"Missing"
2014.iwslt-evaluation.9,2013.iwslt-evaluation.1,1,0.735211,"stems: 1. Introduction The 2014 International Workshop on Spoken Language Translation (IWSLT) offers a comprehensive evaluation campaign on spoken language translation. The evaluation is organized in different evaluation tracks covering automatic speech recognition (ASR), machine translation (MT), and the full-fledged combination of the two of them into speech translation systems (SLT). The evaluations in the tracks are conducted on TED Talks (http://www.ted.com/talks), short 5-25min presentations by people from various fields related in some way to Technology, Entertainment, and Design (TED) [1]. The goal of the TED ASR track is the automatic transcription of fully unsegmented TED lectures. The quality of the resulting transcriptions are measured in word error rate (WER). In this paper we describe our Italian, German and English ASR systems with which we participated in the TED ASR track of the 2014 IWSLT evaluation campaign. While our German and English ASR systems are based on our previous years’ evaluation systems [2] our Italian system is a completely new system that was developed from scratch. Our general system setup uses multiple complementary subsys• 200 hours of Quaero train"
2014.iwslt-evaluation.9,P10-2041,0,0.0847607,"Missing"
2015.iwslt-evaluation.1,2005.iwslt-1.19,0,\N,Missing
2015.iwslt-evaluation.1,2007.iwslt-1.1,0,\N,Missing
2015.iwslt-evaluation.1,2005.iwslt-1.1,0,\N,Missing
2015.iwslt-evaluation.1,2004.iwslt-evaluation.1,1,\N,Missing
2015.iwslt-evaluation.1,J93-3001,0,\N,Missing
2015.iwslt-evaluation.1,W05-0908,0,\N,Missing
2015.iwslt-evaluation.1,2005.mtsummit-papers.11,0,\N,Missing
2015.iwslt-evaluation.1,2015.iwslt-evaluation.16,0,\N,Missing
2015.iwslt-evaluation.1,2006.iwslt-evaluation.1,0,\N,Missing
2015.iwslt-evaluation.1,2008.iwslt-evaluation.1,0,\N,Missing
2015.iwslt-evaluation.1,2011.iwslt-evaluation.1,1,\N,Missing
2015.iwslt-evaluation.1,2009.iwslt-evaluation.1,0,\N,Missing
2015.iwslt-evaluation.1,2012.eamt-1.60,1,\N,Missing
2015.iwslt-evaluation.1,2010.iwslt-evaluation.1,1,\N,Missing
2020.iwltp-1.7,2012.eamt-1.60,0,0.030274,"(Ha et al., 2016; Ha et al., 2017; Johnson et al., 2017) where a single system is able to translate from and to multiple languages. This approach has many advantages: • It leverages the large availability of multi-way, multilingual corpora in European languages such as the corpus of European Parliament documents and speeches’ transcription (Europarl) (Koehn, 2005), the collection of legislative texts of the European Union (JRCAcquis) (Steinberger et al., 2006) or the texts extracted from the document of European Constitution (EUconst) as well as the WIT3 corpus extracted from TED talks (TED) (Cettolo et al., 2012). • It uses the multilingual information to help improve the translation of the language pairs which are considered as low-resource languages in some domains. Our research has shown that our multilingual translation system maintains parity with the translation quality of systems trained on individual language pairs on the same small amount of data. • In practice, having a small number of multilingual systems to cover all language pairs significantly reduces the development and deployment efforts compared with having one system for each pair. 5. While each of the components (ASR, punctuation, M"
2020.iwltp-1.7,2015.iwslt-papers.8,1,0.871161,"Missing"
2020.iwltp-1.7,Q17-1024,0,0.0913848,"Missing"
2020.iwltp-1.7,2005.mtsummit-papers.11,0,0.143289,"f that sentence is displayed and never changed again by the update mechanism, to under 5 seconds. Machine Translation System With the ultimate goal of featuring a translation system for all EUROSAI languages, we opt for the multilingual approach (Ha et al., 2016; Ha et al., 2017; Johnson et al., 2017) where a single system is able to translate from and to multiple languages. This approach has many advantages: • It leverages the large availability of multi-way, multilingual corpora in European languages such as the corpus of European Parliament documents and speeches’ transcription (Europarl) (Koehn, 2005), the collection of legislative texts of the European Union (JRCAcquis) (Steinberger et al., 2006) or the texts extracted from the document of European Constitution (EUconst) as well as the WIT3 corpus extracted from TED talks (TED) (Cettolo et al., 2012). • It uses the multilingual information to help improve the translation of the language pairs which are considered as low-resource languages in some domains. Our research has shown that our multilingual translation system maintains parity with the translation quality of systems trained on individual language pairs on the same small amount of"
2020.iwltp-1.7,W19-5337,1,0.607584,"al conferences and remote conferencing) is described in the respective sections below. Our multilingual systems are based on the neural sequenceto-sequence with attention framework (Bahdanau et al., 2014) and shares the internal representation across languages (Pham et al., 2017). At present, we have one manyto-many Transformer model (Vaswani et al., 2017) providing translation between all pairings of 36 languages, along with several specialized models focused on subsets of languages, in particular the project’s primary languages of English, Czech, and German, see i.a. (Popel and Bojar, 2018; Popel et al., 2019). The resulting multilingual models after training can be used immediately in deployment or can go through a language adaptation step. This language adaptation is simply continuing training the multilingual model on the data of a specific language pair for a few epochs in order to improve the individual translation performance. While we need to do this language adaptation for every single language pair in our system, it is a trivial job since we could automate the process with the same settings and it takes only a little of time and computing resources to reach decent performances. 4.2. Practi"
2020.iwltp-1.7,steinberger-etal-2006-jrc,0,0.163597,"Missing"
2020.iwslt-1.1,P18-4020,0,0.0279555,"Missing"
2020.iwslt-1.1,2005.iwslt-1.19,0,0.174539,"Missing"
2020.iwslt-1.1,2020.iwslt-1.11,0,0.0306756,"Missing"
2020.iwslt-1.1,W18-6319,0,0.0215087,"Missing"
2020.iwslt-1.1,2013.iwslt-papers.14,0,0.069836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.9,0,0.053316,"Missing"
2020.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0549836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.22,0,0.0613503,"Missing"
2020.iwslt-1.4,2020.iwslt-1.1,1,0.800451,"Missing"
2020.iwslt-1.4,P19-1126,0,0.0408966,"Missing"
2020.iwslt-1.4,P19-1285,0,0.351796,"els. After that, the WerRTCVAD toolkit (Wiseman, 2016) was used to segment the audio in two unsegmented datasets. |x| C= Offline Speech Translation (6) i=1 For more information on the ponder loss, see (Graves, 2016). By varying the parameter λ, we can produce systems with different latency regimes. However, each model produces many different latency-quality tradeoffs during training. We use sentencepiece (Kudo and Richardson, 2018) to create a shared 37000 word BPE dictionary for source and target. We then train an offline transformer (Vaswani et al., 2017) model with relative self-attention (Dai et al., 2019). Based on this, we train several ACT models with λ varying from 0.15 to 0.7. For all models, we use the Adam optimizer (Kingma and Ba, 2015). We train the offline Model We only focus on sequence-to-sequence ASR models, which are based on two different network architectures: The long short-term memory (LSTM) and the Transformer. Our LSTM-based models consist of 6 bidirectional layers of 1024 units for the encoder and 2 unidirectional layers for the decoder (Nguyen et al., 2019). Our transformerbased models presented in (Pham et al., 2019b) consist of 32 blocks for the encoder and 12 blocks for"
2020.iwslt-1.4,N19-1202,0,0.0611262,"Missing"
2020.iwslt-1.4,D18-2012,0,0.0213015,"al. (2019) to calculate the encoder-decoder attention. In order to incentivise the model to keep the delays short, we employ the ponder loss in addition to the usual cross-entropy: L(θ) = − X log p(y|x; θ) + λC(n) 4.1 (5) (x,y) X N (i) + R(i) Speech Recognition Data preparation and Segmentation tool We used two different training data sets for this evaluation. Having collected all audios from the TEDLIUM and How2 corpora provided by the organizer, we then generated 40 features of Melfilterbank coefficients for ASR training models using Janus Recognition Toolkit. We use SentencePiece toolkit (Kudo and Richardson, 2018) to train and create 4000 different byte-pair-encoding (BPE) for all models. After that, the WerRTCVAD toolkit (Wiseman, 2016) was used to segment the audio in two unsegmented datasets. |x| C= Offline Speech Translation (6) i=1 For more information on the ponder loss, see (Graves, 2016). By varying the parameter λ, we can produce systems with different latency regimes. However, each model produces many different latency-quality tradeoffs during training. We use sentencepiece (Kudo and Richardson, 2018) to create a shared 37000 word BPE dictionary for source and target. We then train an offline"
2020.iwslt-1.4,P16-1162,0,0.0436582,"notations. The training data for that are EPPS, NC and a filtered version of the ParaCrawl corpus. Then, we fine-tune the model on the TED corpus. For more details, please refer to (Pham et al., 2019a). 4.3 Machine Translation Data Preparation. This year, we use an approximating of 70 millions sentence pairs, coming from TED, EPPS, NC, CommonCrawl, ParaCrawl, Rapid and OpenSubtitles corpora, including around 26 millions back-translation sentence pairs. The data are applied tokenization and smart-casing using the Moses scripts. Furthermore, we segment words into subword units using BPE method (Sennrich et al., 2016). The smartcasing and BPE model are trained on what we call clean datasets (TED, EPPS, NC and CommonCrawl), with the number of BPE merging operation of 40000, jointly learned from English and German sides. Modeling and Training. Basically our translation system employs Transformer-based encoderdecoder model (Vaswani et al., 2017). Our model 1 The probability of whether the noise is introduced is pw noise = 0.7. The distribution of duplicating and deleting a word is pw manipulate = (0.6, 0.4). The distribution of how many words ranging from 1 to 3 (n = 1, 2, 3) is pw num = (0.6, 0.35, 0.05). Th"
2020.iwslt-1.4,D18-1100,0,0.0206536,"s into a portion of that data and mix this noised data to the original one, then do the fine-tuning. The noises are simply produced by duplicating or deleting n words in some random positions conforming to some distributions1 and inserting or deleting a punctuation from the original sentence. The main differences between the Fine-tuning configuration and the Large configuration is that we apply more strict regularizations, since the finetuning data is significantly smaller. Particularly, the dropout is now 0.3, word dropout (Gal and Ghahramani, 2016) is at 0.1 and we also implement switchout (Wang et al., 2018) with the rate of 0.95. Switchout is especially useful when we want to Segmentation Automatic speech recognition (ASR) systems typically do not generate punctuation marks or reliable casing. Using the raw output of these systems as input to MT causes a performance drop due to mismatched train and test conditions. To create segments and better match typical MT training conditions, we use a monolingual NMT system to add sentence boundaries, insert proper punctuation, and add case where appropriate before translating [15]. The idea of the monolingual machine translation system is to translate fro"
2020.lifelongnlp-1.2,N19-1312,0,0.0139166,"fine-tuning, while Kobus et al. (2016); Chu et al. (2017) combined mixed fine-tuning and adding domain tag. 3.1 Topic Adaptation In some situations, we need a model that works well in a specific domain, but the target domain data-set (also known as in-domain data-set) is too small to train a meaningful ASR model alone. In order to obtain a high-performance model in the low resource target domain, we adapt a well trained general seq2seq model to the target domain. 3.2 • Architecture centric: Baniata et al. (2018) used shared decoder and domain specific encoders to adapt NMT for new language. (Gu et al., 2019; Britz et al., 2017) trained Domain Discriminator and NMT model with some part shared in parallel. Accent Adaptation In some situations, it may be difficult to recognize the audio of non-native speakers correctly. The speakers often have a significant non-native accent which does not match the training data from native speakers or even other non-native speakers. Our 10 in-domain training data consists just of a few hours audio of non-native speakers of a specific native language. We adapt the seq2seq model on both specific accent domain and multi-accent domain with our own data-sets. 3.3 Arab"
2020.lifelongnlp-1.2,D11-1033,0,0.0491383,"decoding centric: 3 Dimensions of Adaption In our work we examine the adaptation of our system to different dimensions of variability. Sometimes these different dimensions are subsumed under the term domain (Nguyen et al., 2019). However, on other occasions domain is used to describe the topic of speech data only. We will follow the latter use of domain in this paper and will explicitly address the different dimensions of the speech data for which we examined suitable adaptation techniques: Topic adaptation, accent adaptation and vocabulary adaptation. • Data centric: (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) selected the sentences that are similar to in-domain data from out-of-domain data. • Training objective centric: Chen et al. (2017) used sentence weighting for adaptation of part-of-speech (POS) tagging, a named entity (NE) recognition task. Wang et al. (2017) used sentence weighting, domain weighting and batch weighting for NMT, and Yan et al. (2019) used word weighting. (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Neubig and Hu, 2018) used fine-tuning, and Chu and Dabre (2019) used mixed fine-tuning, while Kobus et al. (2016); Chu et al. (2017) combined mixed f"
2020.lifelongnlp-1.2,2020.lrec-1.817,1,0.811018,"Missing"
2020.lifelongnlp-1.2,W17-4712,0,0.0141389,"le Kobus et al. (2016); Chu et al. (2017) combined mixed fine-tuning and adding domain tag. 3.1 Topic Adaptation In some situations, we need a model that works well in a specific domain, but the target domain data-set (also known as in-domain data-set) is too small to train a meaningful ASR model alone. In order to obtain a high-performance model in the low resource target domain, we adapt a well trained general seq2seq model to the target domain. 3.2 • Architecture centric: Baniata et al. (2018) used shared decoder and domain specific encoders to adapt NMT for new language. (Gu et al., 2019; Britz et al., 2017) trained Domain Discriminator and NMT model with some part shared in parallel. Accent Adaptation In some situations, it may be difficult to recognize the audio of non-native speakers correctly. The speakers often have a significant non-native accent which does not match the training data from native speakers or even other non-native speakers. Our 10 in-domain training data consists just of a few hours audio of non-native speakers of a specific native language. We adapt the seq2seq model on both specific accent domain and multi-accent domain with our own data-sets. 3.3 Arabic (DA), such as Egyp"
2020.lifelongnlp-1.2,W17-3205,0,0.0161308,"ifferent dimensions are subsumed under the term domain (Nguyen et al., 2019). However, on other occasions domain is used to describe the topic of speech data only. We will follow the latter use of domain in this paper and will explicitly address the different dimensions of the speech data for which we examined suitable adaptation techniques: Topic adaptation, accent adaptation and vocabulary adaptation. • Data centric: (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) selected the sentences that are similar to in-domain data from out-of-domain data. • Training objective centric: Chen et al. (2017) used sentence weighting for adaptation of part-of-speech (POS) tagging, a named entity (NE) recognition task. Wang et al. (2017) used sentence weighting, domain weighting and batch weighting for NMT, and Yan et al. (2019) used word weighting. (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Neubig and Hu, 2018) used fine-tuning, and Chu and Dabre (2019) used mixed fine-tuning, while Kobus et al. (2016); Chu et al. (2017) combined mixed fine-tuning and adding domain tag. 3.1 Topic Adaptation In some situations, we need a model that works well in a specific domain, but the target domain"
2020.lifelongnlp-1.2,2015.iwslt-evaluation.11,0,0.044808,"ensions of the speech data for which we examined suitable adaptation techniques: Topic adaptation, accent adaptation and vocabulary adaptation. • Data centric: (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) selected the sentences that are similar to in-domain data from out-of-domain data. • Training objective centric: Chen et al. (2017) used sentence weighting for adaptation of part-of-speech (POS) tagging, a named entity (NE) recognition task. Wang et al. (2017) used sentence weighting, domain weighting and batch weighting for NMT, and Yan et al. (2019) used word weighting. (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Neubig and Hu, 2018) used fine-tuning, and Chu and Dabre (2019) used mixed fine-tuning, while Kobus et al. (2016); Chu et al. (2017) combined mixed fine-tuning and adding domain tag. 3.1 Topic Adaptation In some situations, we need a model that works well in a specific domain, but the target domain data-set (also known as in-domain data-set) is too small to train a meaningful ASR model alone. In order to obtain a high-performance model in the low resource target domain, we adapt a well trained general seq2seq model to the target domain. 3.2 • Architecture centri"
2020.lifelongnlp-1.2,P10-2041,0,0.0602578,"rchitecture centric and decoding centric: 3 Dimensions of Adaption In our work we examine the adaptation of our system to different dimensions of variability. Sometimes these different dimensions are subsumed under the term domain (Nguyen et al., 2019). However, on other occasions domain is used to describe the topic of speech data only. We will follow the latter use of domain in this paper and will explicitly address the different dimensions of the speech data for which we examined suitable adaptation techniques: Topic adaptation, accent adaptation and vocabulary adaptation. • Data centric: (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) selected the sentences that are similar to in-domain data from out-of-domain data. • Training objective centric: Chen et al. (2017) used sentence weighting for adaptation of part-of-speech (POS) tagging, a named entity (NE) recognition task. Wang et al. (2017) used sentence weighting, domain weighting and batch weighting for NMT, and Yan et al. (2019) used word weighting. (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Neubig and Hu, 2018) used fine-tuning, and Chu and Dabre (2019) used mixed fine-tuning, while Kobus et al. (2016); Chu et al. ("
2020.lifelongnlp-1.2,P17-2061,0,0.042855,"Missing"
2020.lifelongnlp-1.2,D18-1103,0,0.029235,"le adaptation techniques: Topic adaptation, accent adaptation and vocabulary adaptation. • Data centric: (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) selected the sentences that are similar to in-domain data from out-of-domain data. • Training objective centric: Chen et al. (2017) used sentence weighting for adaptation of part-of-speech (POS) tagging, a named entity (NE) recognition task. Wang et al. (2017) used sentence weighting, domain weighting and batch weighting for NMT, and Yan et al. (2019) used word weighting. (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Neubig and Hu, 2018) used fine-tuning, and Chu and Dabre (2019) used mixed fine-tuning, while Kobus et al. (2016); Chu et al. (2017) combined mixed fine-tuning and adding domain tag. 3.1 Topic Adaptation In some situations, we need a model that works well in a specific domain, but the target domain data-set (also known as in-domain data-set) is too small to train a meaningful ASR model alone. In order to obtain a high-performance model in the low resource target domain, we adapt a well trained general seq2seq model to the target domain. 3.2 • Architecture centric: Baniata et al. (2018) used shared decoder and dom"
2020.lifelongnlp-1.2,C18-1111,0,0.011564,"they prime their speech recognition system to different domains present in the training data. The paper then shows improvements in Word Error Rate (WER) when adapting the speech recognition system in an unsupervised manner using the DI vector to a domain for which no training or adaptation data is available. In contrast in our experiments we work with small amounts of adaptation data that are not sufficient for training a complete system, but can be exploited for adapting an existing system. Related Work There are a several studies about adaptation of Neural Machine Translation (NMT) systems. Chu and Wang (2018) divided NMT domain adaptation methods into four categories: Data centric, training objective centric, architecture centric and decoding centric: 3 Dimensions of Adaption In our work we examine the adaptation of our system to different dimensions of variability. Sometimes these different dimensions are subsumed under the term domain (Nguyen et al., 2019). However, on other occasions domain is used to describe the topic of speech data only. We will follow the latter use of domain in this paper and will explicitly address the different dimensions of the speech data for which we examined suitable"
2020.lifelongnlp-1.2,D19-5606,0,0.0204568,"Missing"
2020.lifelongnlp-1.2,D17-1155,0,0.242098,"need to be newly created. In this paper we are examining the use of fine-tuning for end-to-end ASR for adapting them to different domains. We thereby examine different dimensions of domain adaptation, such as adapting to topics, accents and vocabulary. In the experiments we compare fine-tuning, i.e., continuing to train an end-to-end system on adaptation data, to a technique called batch-weighting, in which we mix adaptation data with the data for training the background model in a certain ratio at mini-batch level. Batch-weighting was thereby inspired by a technique from machine translation (Wang et al., 2017) and is explained in detail in section 5. We adapt this technique for automatic speech recognition. Further, for the different dimensions of domain adaptation, we examine the fine-tuning of different parts of the end-to-end ASR systems, e.g., only the encoder or only the decoder, in order to test the hypothesis that encoder layers are mainly conWhen training speech recognition systems, one often faces the situation that sufficient amounts of training data for the language in question are available but only small amounts of data for the domain in question. This problem is even bigger for end-to"
2020.lrec-1.817,W14-2201,0,0.0628734,"Missing"
2020.lrec-1.817,N13-1066,0,0.0545409,"Missing"
2020.lrec-1.817,P06-1086,0,0.16197,"Missing"
2020.lrec-1.817,habash-etal-2012-conventional,0,0.205321,"s try to find conventions for audio transcription and collected text normalization. An important work performed was the Linguistic Data Consortium guidelines for transcribing Levantine and Iraqi (Maamouri et al., 2004). It suggests a strategy of the transcription for dialectal Arabic by using MSA-based orthographic conventions, since Arab transcribers use their MSA Knowledge for the transcription of Arabic dialects. This includes using both symbols and rules of MSA orthography e.g. writing without short vowels and diacritical marks except for nunation. Inspired by this work, CODA is invented (Habash et al., 2012), a conventional orthography for dialectal Arabic, which is intended to be for general writing purposes and abstracts from phonological variations in sub-dialects in contrast to the previous work. This Work covers Egyptian dialect EGY in details and extended by (Zribi et al., 2014) for Tunisian dialect, by (Saadane and Habash, 2015) for Algerian Arabic and by (Turki et al., 2016) for Maghrebi Arabic. For the data collection, an automatic conversion from spontaneous orthography of EGY to CODA is developed as a freely available tool called CODAFY (Eskander et al., 2013). Another challenge the da"
2020.lrec-1.817,W15-3208,0,0.045514,"Missing"
2020.lrec-1.817,zribi-etal-2014-conventional,0,0.0387481,"Missing"
2020.wanlp-1.1,abdelali-etal-2014-amara,0,0.0163818,"TED Multi UN News Commentary Wikipedia QED JW300 GlobalVoices Tatoeba Q+A-TRAIN5 Total B: Test Data TED-TEST JW-TEST Q+A-TEST5 Words (×106 ) Ar De Vocab (×103 ) Ar De 199 165 208.783 9.833 18.537 358.501 9 1 0.257 954.408 2.800 4.700 7.565 0.146 0.124 5.805 0.150 0.004 0.004 21.383 3.200 4.800 6.396 0.142 0.146 5.479 0.170 0.003 0.005 20.348 278.0 165.0 226.779 33.308 24.796 215.926 43.0 2.2 1.899 - 214.0 121.0 228.481 32.448 20.184 255.981 35.0 1.9 1.657 - 1.717 1.974 0.129 0.021 0.031 0.002 0.025 0.029 0.002 8.572 10.455 1.209 6.897 9.245 1.060 Table 2: Summary of the translation data sets (Abdelali et al., 2014). Similar to TED talks, this corpus consists of the transcription and translation of educational lectures. In this work, we use the version 1.4 of this corpus.8 Finally, the JW300 corpus was crawled from the Jehovah’s Witnesses website9 by (Agi´c and Vuli´c, 2019). The contents are of religious nature and are available in a large number of language pairs. This corpus is also made available on the OPUS repository. However, unlike the other corpora, this one comes unaligned on the sentence level. Consequently, we perform sentence alignment on the raw downloaded data. The process was held by the"
2020.wanlp-1.1,P19-1310,0,0.0200605,"Missing"
2020.wanlp-1.1,C18-1139,0,0.0130217,"ble mumbles as the system tends to insert the vowels arbitrarily. Diactritics have a crucial role in giving the Arabic text a phonetics (Abbad and Xiong, 2020). Moreover, they allow for better comprehension by reducing the level of ambiguity inherent in the Arabic transcription system. Having that said, most modern written Arabic resources omit these diacritics and rely on the ability of the native speakers to guess them from the context. In particular, all the parallel data used to train our translation systems has no diacritics. We employ a sequence-tagger trained using the Flair-framework (Akbik et al., 2018), which is a BiLSTM-CRF proposed by (Huang et al., 2015). besides (Akbik et al., 2018) introduces Contextual String Embeddings. Thereby, sentences are passed as sequences of characters into a character-level Language Model (LM) to form word-level embeddings. This is done by concatenating the output hidden state after the last character in the word from the forward LM and the output hidden state before the word’s first character from the backward LM. These two language models result from utilizing the hidden states of the forward-backward recurrent neural network (see (Akbik et al., 2018) for m"
2020.wanlp-1.1,W16-4007,0,0.0317909,"Missing"
2020.wanlp-1.1,2012.eamt-1.60,0,0.0182523,"aining data. In the following, we explain the steps we followed to overcome these limitations and to produce a reasonable translation system. Then, we show the developed systems in action through empirical evaluations. 3.1 Data Although the language pair under consideration is not commonly studied, a reasonable training set could be gathered, thanks to the different data sources publicly exposed on the Internet. In Part A of Table 2, we give a summary about the exploited data sets and some of their important attributes. The TED corpus6 is our first source of data. This corpus is collected by (Cettolo et al., 2012) from the translations generated by volunteers for the TED talks, over the course of several years. By looking at the data, we think that TED is cleaner and more suitable for speech translation. Therefore, we always start by a baseline trained on TED data only, and then we gradually introduce more data from other sources. Another extremely important source is the OPUS7 repository. OPUS is a large depot where parallel data is collected from different sources and sometimes also preprocessed for a very large number of language pairs. It turned out that not all of the data available in this reposi"
2020.wanlp-1.1,P17-2061,0,0.024325,"ed similarly to mini.que.ans.3.34h. It consists of 224 free answers on M.I.N.I questions with a duration of 42 minutes by one speaker. • mini.ques.50m: 225 M.I.N.I questions by the same speaker as from mini-ans.42m with a duration of 50 minutes. 2.2 Domain Adaptation for ASR As we will see in section 2.3, we obtain extremely poor performance on our target domain (psychiatric interview). Therefore, we are investigating many approaches to adapt our speech recognition system to the target domain. In this paper, we report about two experiments: the mixed-fine-tuning and the finetuning experiment (Chu et al., 2017). For mixed-fine-tuning, we begin with the pre-trained model on Alj.1200h data as a baseline and mixed-fine-tune it on the data resulting from mixing Alj.1200h with the domain data mini.que.ans.3.34h. For mixed-fine-tune or fine-tuning the decoder, we freeze the encoder, and train only the decoder. For fine-tuning, we don’t mix the data. Instead we use only mini.que.ans.3.34h set. For mixed-fine-tuning and fine-tuning, We use the learning rates (0.0009) and (0.00001) respectively. We report the result in section 2.3. 2.3 Experimental Results We experimented with the LSTM-based encoder-decoder"
2020.wanlp-1.1,2020.lrec-1.817,1,0.814945,"Missing"
2020.wanlp-1.1,2014.iwslt-papers.14,1,0.814027,"far, we explored two approaches to the adaptation: fine-tuning and data selection. Fine-tuning is accomplished by resuming the training for very few additional epochs using only the in-domain data. Data selection provides us with more in-domain data. This is achieved by choosing a special subset of training examples from the general domain training set. This subset consists of general domain examples, which are the most similar to the in-domain examples. The selection process is carried out using (Moore and Lewis, 2010) with more careful out-of-domain language model selection as proposed by (Mediani et al., 2014). The language models used in this procedure are 4-gram language models with Witten-Bell smoothing. We perform the selection for both languages (i.e. once for Arabic and once for German). Then, we take the intersection of the two selected subsets.12 3.3 Experimental Results The input data is preprocessed using the sentence piece13 algorithm. For Arabic the vocabulary size is set to 4000 and for German to 16000. Lines longer than 80 words are discarded. The model’s architecture is transformer encoder-decoder model (Vaswani et al., 2017b). Both the encoder and decoder are 8-layers. Each layer is"
2020.wanlp-1.1,P10-2041,0,0.0563716,"hat these sets are being actively extended, since the manual data creation is time-consuming. So far, we explored two approaches to the adaptation: fine-tuning and data selection. Fine-tuning is accomplished by resuming the training for very few additional epochs using only the in-domain data. Data selection provides us with more in-domain data. This is achieved by choosing a special subset of training examples from the general domain training set. This subset consists of general domain examples, which are the most similar to the in-domain examples. The selection process is carried out using (Moore and Lewis, 2010) with more careful out-of-domain language model selection as proposed by (Mediani et al., 2014). The language models used in this procedure are 4-gram language models with Witten-Bell smoothing. We perform the selection for both languages (i.e. once for Arabic and once for German). Then, we take the intersection of the two selected subsets.12 3.3 Experimental Results The input data is preprocessed using the sentence piece13 algorithm. For Arabic the vocabulary size is set to 4000 and for German to 16000. Lines longer than 80 words are discarded. The model’s architecture is transformer encoder-"
2020.wanlp-1.1,P02-1040,0,0.107552,"100 epochs. While the number of epochs used for fine-tuning is taken to be 10. We use the Adam scheduling with a learning rate initially set to 1 and with 2048 warming steps. The results are summarized in Table 3. System JW-TEST A: German → Arabic TED only 5.06 TED+Extra data 23.90 Fine-tuned 5.34 Fine-tune-select 23.18 B: Arabic → German TED+Extra data 17.17 Fine-tune-select 16.60 TED-TEST Q+A-TEST 12.44 14.62 12.94 14.35 7.99 8.19 12.18 19.16 9.67 9.06 6.18 15.96 Table 3: Summary of the translation experiments (results are expressed as BLEU (↑) scores) The scores in Table 3 are BLEU scores (Papineni et al., 2002). The table is subdivided into two panels, one for each translation direction. We report all tested combinations for the German → Arabic direction. For the reverse direction we report results only for the most promising configurations. The configurations shown here are as follows: 12 Other ways of combining selections from the two sides of the parallel corpus were not explored in this work. Our choice (i.e. the intersection) is motivated by our aim for higher precision. 13 https://github.com/google/sentencepiece 6 • TED only The training is performed on TED corpus only. • TED+Extra data the sy"
2021.eacl-demos.32,P19-1126,0,0.361622,"Missing"
2021.eacl-demos.32,D14-1140,0,0.362092,"Missing"
2021.eacl-demos.32,2020.iwslt-1.27,0,0.152889,"among the research partners in our project, e.g. Hindi. The scientific motivation for our efforts is to find an approach that allows to assemble laboratory system components to a practically usable product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speech recognition system, a sentence segmentation system, and an attention-based translation system providing also some pre-trained models for their tasks. (Zenkel et al., 2018) evaluated only the quali"
2021.eacl-demos.32,E17-1099,0,0.269847,"rts is to find an approach that allows to assemble laboratory system components to a practically usable product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speech recognition system, a sentence segmentation system, and an attention-based translation system providing also some pre-trained models for their tasks. (Zenkel et al., 2018) evaluated only the quality of the output translations using BLEU and WER metrics. Zheng et al. (2019) propose"
2021.eacl-demos.32,N12-1048,0,0.134304,"m the secured networks of the labs so it usually does not run into firewall issues. tions of the EU and nearby countries. Experimentally, we include also other languages based on available systems among the research partners in our project, e.g. Hindi. The scientific motivation for our efforts is to find an approach that allows to assemble laboratory system components to a practically usable product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speec"
2021.eacl-demos.32,2020.eamt-1.53,1,0.493793,"Missing"
2021.eacl-demos.32,C18-2020,1,0.876378,"Missing"
2021.eacl-demos.32,2020.iwslt-1.25,1,0.823962,"Missing"
2021.eacl-demos.32,N16-3017,1,0.800534,"Missing"
2021.eacl-demos.32,2020.acl-main.148,1,0.8252,"tem in end-to-end fashion and face engineering problems and technical issues on all layers from sound acquisition through network connections, worker configuration to subtitle presentation. • We are currently running a user study with nonGerman speakers watching German videos with our online subtitles, see Section 7.1. We aim to measure the comprehension loss caused by different subtitling options, latency or flicker. 42 languages (Johnson et al., 2017). The models are mostly Transformers (Vaswani et al., 2017) but we improve their performance in massively multilingual setting by extra depth (Zhang et al., 2020). 5.3 Interplay of ASR and MT Connecting ASR and MT systems is not straightforward because MT systems assume input in the form of complete sentences. We follow the strategy of Niehues et al. (2016), first inserting punctuation into the stream of tokens coming from ASR (Tilk and Alum¨ae, 2016), breaking it up at full stops and sending individual sentences to MT, either as unfinished sentence prefixes, or complete sentences. We are using re-translation, as ASR or punctuation updates are received. Currently, the main problem is that punctuation prediction does not have access to the sound any mor"
2021.eacl-demos.32,D19-1137,0,0.29802,"product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speech recognition system, a sentence segmentation system, and an attention-based translation system providing also some pre-trained models for their tasks. (Zenkel et al., 2018) evaluated only the quality of the output translations using BLEU and WER metrics. Zheng et al. (2019) proposed a new approach with a delay-based heuristic. The model decides to read more input (or wait for it) or wri"
2021.eacl-demos.32,2020.findings-emnlp.349,0,0.0269839,"text available, the user does not see sufficient number of words to let the brain “make up” or reconstruct the original meaning from pieces. The short-term memory of recently processed text does not seem to be sufficient for this type recovery, while seeing the words in larger context gives the user a better chance. The last step in an SLT system is the delivery of the translated content to the user. Our goal stops at the textual representation, i.e. we do not include speech synthesis and delivery of the sound, which would bring yet another set of design decisions and open problems, see e.g. Zheng et al. (2020). We experiment with two different views for our text output, both implemented as web applications. The “subtitle view” is optimized toward minimal use of screen space. Only two lines of text are available which leaves room either for e.g. a streamed video of the session or the slides, or for many languages displayed at once, if the screen is intended for a multi-lingual audience. The “paragraph view” provides more textual context to the user. 7.1 Subtitle View The subtitle view offers a simple interface with a HLS stream of the video or slides and one or more subtitles streams. Section 7.1 pr"
2021.eacl-demos.32,W19-5337,1,0.807801,"latency and hypotheses updates, as in KIT Lecture Translator (M¨uller et al., 2016). We use the hybrid ASR models based on Janus from KIT Lecture Translator, for German and English, as well as recent neural sequence-to-sequence ASR models trained on the same data (Nguyen et al., 2020). For Czech ASR, we use a Kaldi hybrid model trained on a Corpus of Czech Parliament Plenary Hearings (Kratochv´ıl et al., 2019). Czech sequence-to-sequence ASR is a work in progress. 5.2 MT Systems in ELITR We use bilingual NMT models for some high resource and well-studied language pairs e.g. for English-Czech (Popel et al., 2019; Wetesko et al., 2019). For other targets, we use multi-target models, e.g. an English-centric universal model for ELITR Flexible Architecture We always strive for the best performance for each considered language pair. With the perpetual com272 Index Name auto-iwslt2020-antrecorp(ASR) auto-iwslt2020-antrecorp(MT) auto-iwslt2020-antrecorp(MT) auto-asr-english-auditing(ASR) auto-asr-english-auditing(MT) auto-asr-english-auditing(MT) auto-iwslt2020-khanacademy(ASR) Worker en-EU-lecture KIT-s2s rb-EU fromEN-en to 41 rb-EU fromEN-en to 41 en-EU-lecture KIT-s2s rb-EU fromEN-en to 41 rb-EU fromEN-e"
2021.iwslt-1.1,2020.lrec-1.520,0,0.0759372,"Missing"
2021.iwslt-1.1,2020.iwslt-1.3,0,0.0592828,"Missing"
2021.iwslt-1.1,2021.iwslt-1.5,0,0.0628415,"Missing"
2021.iwslt-1.1,N19-1202,1,0.897776,"Missing"
2021.iwslt-1.1,2021.acl-long.224,1,0.769761,"sh-German section of the MuST-C V2 corpus7 and include training, dev, and test (Test Common), in the same structure of the MuST-C V1 corpus (Cattoni et al., 2021) used last year. Since the 2021 test set was processed using the same pipeline applied to create MuST-C V2, the use of the new training resource was strongly recommended. The main differences with respect to MuST-C v1 are: gap with respect to the traditional cascade approach (integrating ASR and MT components in a pipelined architecture). In light of last year’s IWSLT results (Ansari et al., 2020) and of the findings of recent works (Bentivogli et al., 2021) attesting that the gap between the two paradigms has substantially closed, also this year a key element of the evaluation was to set up a shared framework for their comparison. For this reason, and to reliably measure progress with respect to the past rounds, the general evaluation setting was kept unchanged. This stability mainly concerns two aspects: the allowed architectures and the test set provision. On the architecture side, participation was allowed both with cascade and end-to-end (also known as direct) systems. In the latter case, valid submissions had to be obtained by models that:"
2021.iwslt-1.1,2021.iwslt-1.22,0,0.082468,"Missing"
2021.iwslt-1.1,2021.iwslt-1.27,1,0.721453,"to use the same training and development data as in the Offline Speech Translation track. More details are available in §3.2. For the English-Japanese text-to-text track, participants could use the parallel data and monolingual data available for the English-Japanese WMT20 news task (Barrault et al., 2020). For development, participants could use the IWSLT 2017 development sets,2 the IWSLT 2021 development set3 and the simultaneous interpretation transcripts for the IWSLT 2021 development set.4 The simultaneous interpretation was recorded as a part of NAIST Simultaneous Interpretation Corpus (Doi et al., 2021). Systems were evaluated with respect to quality and latency. Quality was evaluated with the standard BLEU metric (Papineni et al., 2002a). Latency was evaluated with metrics developed for simultaneous machine translation, including average proportion (AP), average lagging (AL) and differentiable average lagging (DAL, Cherry and Foster 2019), and later extended to the task of simultaneous speech translation (Ma et al., 2020b). The evaluation was run with the S IMUL E VAL toolkit (Ma et al., 2020a). For the latency measurement of speech input systems, we contrasted computation-aware and non com"
2021.iwslt-1.1,2012.eamt-1.60,1,0.698226,"rently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun,"
2021.iwslt-1.1,2020.iwslt-1.8,1,0.838546,"Missing"
2021.iwslt-1.1,2021.iwslt-1.12,0,0.0248123,"a et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two S"
2021.iwslt-1.1,L18-1001,0,0.0652124,"Missing"
2021.iwslt-1.1,2021.acl-long.68,1,0.787992,"Missing"
2021.iwslt-1.1,L18-1275,0,0.0286278,"ng two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guidelines.19 This makes them less literal compared to standard, unconstrained translations; • OpenSubtitles 2018 (Lison et al., 2018); • Augmented LibriSpeech et al., 2018)16 (Kocabiyikoglu • Mozilla Common Voice17 ; • Unconstrained translations. These references were created from scratch20 by adhering to the usual translation guidelines. They are hence exact (more literal) translations, without paraphrasing and with proper punctuation. • LibriSpeech ASR corpus (Panayotov et al., 2015). The list of allowed development data includes the dev set from IWSLT 2010, as well as the test sets used for the 2010, 2013, 2014, 2015 and 2018 IWSLT campaigns. Using other training/development resources was allowed but, in this case, parti"
2021.iwslt-1.1,2020.iwslt-1.9,0,0.0594925,"Missing"
2021.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0742394,"Missing"
2021.iwslt-1.1,2021.iwslt-1.4,0,0.134226,"essler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and acces"
2021.iwslt-1.1,N18-2074,0,0.0499123,"Missing"
2021.iwslt-1.1,2006.amta-papers.25,0,0.370434,"Missing"
2021.iwslt-1.1,W14-3354,0,0.0745687,"Missing"
2021.iwslt-1.1,W18-6319,0,0.0144137,"cipants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guideli"
2021.iwslt-1.1,2021.iwslt-1.19,0,0.0280723,", 2021) Fondazione Bruno Kessler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; comm"
2021.iwslt-1.1,2020.findings-emnlp.230,0,0.0691772,"Missing"
2021.iwslt-1.1,2021.iwslt-1.7,0,0.0794497,"Missing"
2021.iwslt-1.1,2021.iwslt-1.16,0,0.0332503,"nyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two Swahili varieties (Congolese and Co"
2021.iwslt-1.1,2020.lrec-1.517,1,0.846095,"s were finally converted from two (stereo) to one (mono) channel and downsampled from 48 to 16 kHz, using FFmpeg.10 Upon inspection of the spectrograms of the same talks in the two versions of MuST-C, it clearly emerges that the upper limit band in the audios used in MuST-C V1 is 5 kHz, while it is at 8 kHz in the latest version, coherently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 thi"
2021.iwslt-1.1,2021.iwslt-1.6,0,0.156379,"2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource spe"
2021.iwslt-1.13,N19-1202,0,0.0654865,"condition and end-to-end condition. In the cascaded condition, we investigated different endto-end architectures for the speech recognition module. For the text segmentation module, we trained a small transformer-based model on high-quality monolingual data. For the translation module, our last year’s neural machine translation model was reused. In the end-toend condition, we improved our Speech Relative Transformer architecture to reach or even surpass the result of the cascade system. 1 2 Data Speech Corpora. For training and evaluation of our ASR models, we used Mozilla Common Voice v6.1 (Ardila et al., 2019), Europarl (Koehn, 2005), How2 (Sanabria et al., 2018), Librispeech (Panayotov et al., 2015), MuST-C v1 (Di Gangi et al., 2019), MuST-C v2 (Cattoni et al., 2021) and Tedlium v3 (Hernandez et al., 2018) dataset. The data split is presented in the following table 1. Table 1: Summary of the English data-sets used for speech recognition Introduction As in previous years, the cascade system’s pipeline is constituted by an ASR module, a text segmentation module and a machine translation module. In this year’s evaluation campaign, we investigated only sequence-to-sequence ASR models with three archit"
2021.iwslt-1.13,2005.mtsummit-papers.11,0,0.0803374,"ion. In the cascaded condition, we investigated different endto-end architectures for the speech recognition module. For the text segmentation module, we trained a small transformer-based model on high-quality monolingual data. For the translation module, our last year’s neural machine translation model was reused. In the end-toend condition, we improved our Speech Relative Transformer architecture to reach or even surpass the result of the cascade system. 1 2 Data Speech Corpora. For training and evaluation of our ASR models, we used Mozilla Common Voice v6.1 (Ardila et al., 2019), Europarl (Koehn, 2005), How2 (Sanabria et al., 2018), Librispeech (Panayotov et al., 2015), MuST-C v1 (Di Gangi et al., 2019), MuST-C v2 (Cattoni et al., 2021) and Tedlium v3 (Hernandez et al., 2018) dataset. The data split is presented in the following table 1. Table 1: Summary of the English data-sets used for speech recognition Introduction As in previous years, the cascade system’s pipeline is constituted by an ASR module, a text segmentation module and a machine translation module. In this year’s evaluation campaign, we investigated only sequence-to-sequence ASR models with three architectures. The segmentatio"
2021.iwslt-1.13,D18-2012,0,0.0174335,"ata Dataset TED Talks (TED) Europarl (EPPS) CommonCrawl Rapid ParaCrawl OpenSubtitles WikiTitle Back-translated News 3.2 Sentences 220K 2.2MK 2.1M 1.21M 25.1M 12.6M 423K 26M the end-to-end architecture is trained to directly translate English audio inputs into German text outputs (Section 3.4). 3.1 Speech Recognition Data preparation and Segmentation tool After collecting all audios from all data sets mentioned in Section 2, we calculated 40 features of Mel-filterbank coefficients for ASR training. To generate labels for the sequence-to-sequence ASR models, we used the Sentence-Piece toolkit (Kudo and Richardson, 2018) to train 4000 different bytepair-encoding (BPE). The WerRTCVAD toolkit (Wiseman, 2016) was used to segment the audio in the testing phase. Model As in previous years (Pham et al., 2019a, 2020b), we used only sequence-to-sequence ASR models, which are based on three different network architectures: The long short-term memory (LSTM), the Transformer and the Conformer. LSTM-based models (Nguyen et al., 2020) consist of 6 bidirectional layers for the encoder and 2 unidirectional layers for the decoder, both encoder and decoder layers have 1536 units. The Transformerbased models presented in (Pham"
2021.iwslt-1.13,Q19-1020,0,0.0644706,"Missing"
2021.iwslt-1.13,2020.lrec-1.517,0,0.0299261,"erman→English translation system. A large transformer architecture was trained with Relative Attention. We adapted to the in-domain by fine-tuning on TED talk data with 1 https://github.com/quanpn90/NMTGMinor stricter regularizations. The same adapted model was trained on noised data synthesized from the same TED data. The final model is the ensemble of the two. 3.4 End-to-End Model Corpora This year, the training data consists of the second version of the MUST-C corpus (Di Gangi et al., 2019), the Europarl corpus (Iranzo-S´anchez et al., 2020), the Speech Translation corpus and the CoVoST-2 (Wang et al., 2020) corpus provided by the organizer. The speech features are generated with the in-house Janus Recognition Toolkit. The ST dataset is handled with an additional filtering step using an English speech recognizer (trained with the its transcripts with the additional Tedlium-3 training data). Following the success of generating synthetic audio utterances, the transcripts in the Tedlium-3 corpus are translated into German using the cascade built in the previous year’s submission (Pham et al., 2020b). In brief, the translation process required us to preserve the audio-text alignment from the original"
2021.iwslt-1.18,W18-6402,0,0.0239373,"o develop both cascade and end-to-end systems and eventually combine them together to achieve the best possible results for this extremely low-resource setting. The report also confirms certain consistent architectural improvement added to the Transformer architecture, for all tasks: translation, transcription and speech translation. 1 2 Introduction The neural sequence-to-sequence models have revolutionalised both automatic speech recognition (ASR) and machine translation in many different aspects, from performance (Luong et al., 2015; Pham et al., 2019a) to various forms such as multimodal (Barrault et al., 2018) and multilingual (Kannan et al., 2019; Ha et al., 2016; Johnson et al., 2016). After multilingual text translation has been established, the recent focus is naturally shifted to multilingual speech translation especially with a series of public speech corpora with multiple translation being released (Iranzo-S´anchez et al., 2020; Wang et al., 2020; Salesky et al., 2021). Recent evaluation campaigns in speech translation have seen a fierce competition between traditional cascade systems and end-to-end counterparts (Jan et al., 2018, 2019; Ansari et al., 2020). The competition without a doubt w"
2021.iwslt-1.18,P19-1285,0,0.0145972,"tion functions including self-attention and feed-forward neural networks. Self-attention transforms a sequence of states using themselves as queries, keys and values, building up hierarchical representational powers since the output states are the weighted-sum of the input states that can be flexibly learned during training. Relative attention (Shaw et al., 2018) further improves the interaction between states by assigning learnable weights for each relative position. (Pham et al., 2020) incorporated this mechanism into speech models by extending the partially learnable relative positions in (Dai et al., 2019) to attend to all positions in the sequence bidirectionally. Furthermore, the Transformer models are strengthened by using dual feed-forward (FFN) layers per block instead of one (Lu et al., 2019). As such, one feed-forward network block precedes the initial self-attention in either encoder and decoder. The outputs of both FFN layers are scaled by 0.5. Besides, it is possible to help training deep Transformer better by using RELU-inspired activation functions that do not suffer from dead neurons. GELU (Hendrycks and Gimpel, 2016) and SiLU (Elfwing et al., 2018) are combined with gated linear u"
2021.iwslt-1.18,Q17-1024,0,0.0666507,"Missing"
2021.iwslt-1.18,D16-1139,0,0.0205515,"et al., 2019b), the training process is divided into rounds in which the training data is incrementally added with synthetic data coming from the refining models themselves. Starting from the original training data in round 0, we use the best settings in round n to translate the source and target sentences in the training to the counterpart language and add the synthetic translation pairs to the current training data, proceeding to round n + 1. Each synthetic pair consists of one original sentence and one synthetic sentence. The idea is the combination of backtranslation, model distillation (Kim and Rush, 2016) and data augmentation (Wang et al., 2018) without any additional data. Interestingly, thanks to the multilingual property, it is also possible to translate one sentence to a range of languages after each round, leading to different options and a massive amount of sentences to be added. However, it was empirically found out that the method did not scale after 1 round, and massively translating to all languages did not improve the training data. Therefore, after round 0, the best configuration which is an ensemble is used to generate synthetic parallel data for round 1 by just translating each"
2021.iwslt-1.18,D15-1166,0,0.0332309,"ranslation task in the IWSLT 2021 evaluation campaign. Our main approach is to develop both cascade and end-to-end systems and eventually combine them together to achieve the best possible results for this extremely low-resource setting. The report also confirms certain consistent architectural improvement added to the Transformer architecture, for all tasks: translation, transcription and speech translation. 1 2 Introduction The neural sequence-to-sequence models have revolutionalised both automatic speech recognition (ASR) and machine translation in many different aspects, from performance (Luong et al., 2015; Pham et al., 2019a) to various forms such as multimodal (Barrault et al., 2018) and multilingual (Kannan et al., 2019; Ha et al., 2016; Johnson et al., 2016). After multilingual text translation has been established, the recent focus is naturally shifted to multilingual speech translation especially with a series of public speech corpora with multiple translation being released (Iranzo-S´anchez et al., 2020; Wang et al., 2020; Salesky et al., 2021). Recent evaluation campaigns in speech translation have seen a fierce competition between traditional cascade systems and end-to-end counterparts"
2021.iwslt-1.18,W19-5202,1,0.870393,"Missing"
2021.iwslt-1.18,P16-1162,0,0.0163622,"is controlled by the language embedding vectors added directly to the word embedding at every timestep (Ha et al., 2017; Pham et al., 2019b). The language pairs are randomly sampled based on the training size of each pair (no temperature was used). Training is done using the adaptive learning rate for Adam, with maximum learning rate at 0.7 achieved after 4096 warming-up steps, and is often early-stopped after 60000 training steps, each is approximately 48000 words. Regularization is further improved via data diversification (Nguyen et al., 2019b). Carrying a similar idea of back-translation (Sennrich et al., 2016) that generates synthetic labels for untranslated monolingual data, the main idea of data diversification is to popularize the available training data with synthetic translation of both source sentences and target sentences. According to the algorithm presented in (Nguyen et al., 2019b), the training process is divided into rounds in which the training data is incrementally added with synthetic data coming from the refining models themselves. Starting from the original training data in round 0, we use the best settings in round n to translate the source and target sentences in the training to"
2021.iwslt-1.18,N18-2074,0,0.0201392,"training end-to-end SLT models. 3 General enhancement for Transformer Models In this section, we describe the overall model descriptions that were applied in all three tasks. Transformers (Vaswani et al., 2017) are constructed with blocks of transformation functions including self-attention and feed-forward neural networks. Self-attention transforms a sequence of states using themselves as queries, keys and values, building up hierarchical representational powers since the output states are the weighted-sum of the input states that can be flexibly learned during training. Relative attention (Shaw et al., 2018) further improves the interaction between states by assigning learnable weights for each relative position. (Pham et al., 2020) incorporated this mechanism into speech models by extending the partially learnable relative positions in (Dai et al., 2019) to attend to all positions in the sequence bidirectionally. Furthermore, the Transformer models are strengthened by using dual feed-forward (FFN) layers per block instead of one (Lu et al., 2019). As such, one feed-forward network block precedes the initial self-attention in either encoder and decoder. The outputs of both FFN layers are scaled b"
2021.iwslt-1.18,2020.lrec-1.517,0,0.061302,"e neural sequence-to-sequence models have revolutionalised both automatic speech recognition (ASR) and machine translation in many different aspects, from performance (Luong et al., 2015; Pham et al., 2019a) to various forms such as multimodal (Barrault et al., 2018) and multilingual (Kannan et al., 2019; Ha et al., 2016; Johnson et al., 2016). After multilingual text translation has been established, the recent focus is naturally shifted to multilingual speech translation especially with a series of public speech corpora with multiple translation being released (Iranzo-S´anchez et al., 2020; Wang et al., 2020; Salesky et al., 2021). Recent evaluation campaigns in speech translation have seen a fierce competition between traditional cascade systems and end-to-end counterparts (Jan et al., 2018, 2019; Ansari et al., 2020). The competition without a doubt would continue in multilingual speech translation especially in a low-resource condition. However, the competition between two modeling schemes suggests that each of them possesses its own strengths and advantages. Notably the cascade models can easily benefit from the separated optimized architectures of each subtask and enjoy the larger available"
2021.iwslt-1.18,D18-1100,0,0.0124275,"vided into rounds in which the training data is incrementally added with synthetic data coming from the refining models themselves. Starting from the original training data in round 0, we use the best settings in round n to translate the source and target sentences in the training to the counterpart language and add the synthetic translation pairs to the current training data, proceeding to round n + 1. Each synthetic pair consists of one original sentence and one synthetic sentence. The idea is the combination of backtranslation, model distillation (Kim and Rush, 2016) and data augmentation (Wang et al., 2018) without any additional data. Interestingly, thanks to the multilingual property, it is also possible to translate one sentence to a range of languages after each round, leading to different options and a massive amount of sentences to be added. However, it was empirically found out that the method did not scale after 1 round, and massively translating to all languages did not improve the training data. Therefore, after round 0, the best configuration which is an ensemble is used to generate synthetic parallel data for round 1 by just translating each sentence to the same language in the origi"
C16-1292,W05-0909,0,0.0342364,"Q(true) := wi yi (1) i∈ALL Note that this definition does not aim to handle document-level discourse phenomena such as coherence, cohesion, and and consistency, but estimates the average sentence-level quality for the document in order to evaluate the overall quality of the whole output hypothesis. In this paper, we weigh segments proportionally to their length, although future work may investigate more sophisticated notions of segment importance. Our definition of document quality is simplistic, but widely used in both MT and ASR communities, e.g. document- or corpus-level WER, TER, METEOR (Banerjee and Lavie, 2005), and human rankings are usually computed this way. BLEU is computed on the corpus level and thus not directly usable with our approach, but we can instead resort to computing average sentence-level BLEU variants such as BLEU+1 (Lin and Och, 2004) that essentially differ only in the smoothing details. Our lightly supervised estimation framework determines document quality in several steps. The first step is to automatically estimate the quality for each segment in the hypothesis (§4). This can be achieved by training a regressor with the desired target measure, as discussed in numerous previou"
C16-1292,P13-2097,0,0.027685,"n found challenging in both ASR (Negri et al., 2014) and MT (de Souza et al., 2015a). The WMT 2015 shared task on QE considered only the scenario of training and testing on output produced by the same system (Bojar et al., 2015). The usual way to address domain mismatch when training data for all domains is available is via adaptation/multitask learning (Beck et al., 2014; de Souza et al., 2015b). Our indicator features can be seen as a form of multitask learning. A strategy for the case where no in-domain training data is available is to obtain such training data cheaply via active learning (Beck et al., 2013). This work is probably most similar in spirit to ours in that it attempts reliable quality estimation at low labeling costs. A different line of research, crowd-sourced annotation, critically depends on quality control, as well. Approaches are usually based on comparing results between several workers (Passonneau and Carpenter, 2014), querying gold standard “testing” labels occasionally (Joglekar and Garcia-Molina, 2013), and/or automatically predicting quality (Roy et al., 2010; Gao et al., 2015). Our work can be seen as a generalization of the latter two, with the gold labels corresponding"
C16-1292,W14-3338,0,0.0219279,"information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-domain case investigated in this work, i.e. predicting quality for a previously unknown system or task, has been found challenging in both ASR (Negri et al., 2014) and MT (de Souza et al., 2015a). The WMT 2015 shared task on QE considered only the scenario of training and testing on output produced by the same system (Bojar et al., 2015). The usual way to address domain mismatch when training data for all domains is available is via adaptation/multitask learning (Beck et al., 2014; de Souza et al., 2015b). Our indicator features can be seen as a form of multitask learning. A strategy for the case where no in-domain training data is available is to obtain such training data cheaply via active learning (Beck et al., 2013). This work is probably most similar in spirit to ours in that it attempts reliable quality estimation at low labeling costs. A different line of research, crowd-sourced annotation, critically depends on quality control, as well. Approaches are usually based on comparing results between several workers (Passonneau and Carpenter, 2014), querying gold stan"
C16-1292,P07-1033,0,0.0452183,"Missing"
C16-1292,P15-1022,0,0.0463442,"Missing"
C16-1292,N15-1073,0,0.0557124,"Missing"
C16-1292,P15-1174,0,0.0243,"lly automatic baseline (§3.1). For comparison, we evaluate a mean-predictor baseline that always predicts the training mean, regardless of the input features. This baseline has been found surprisingly strong previously (Negri et al., 2014; Specia et al., 2015), which we confirm in Table 2. On segment-level, gains over the mean-predictor baseline are clearly visible only for the ASR setting. As expected, the out-of-domain tasks appear much more difficult than the in-domain setting. Note that even though the mean baseline sometimes achieves lower MAE, the XT regressor maintains the advantages 3 Graham (2015) argues that correlation is better for evaluating sentence-level QE, because MAE can be improved by transformation to match estimated global mean and variance. However, we find MAE more indicative for our purpose as it measures not only how well systems are compared against one another, but also how well overall quality is judged in absolute terms. Moreover, collecting global statistics for transformation seems problematic when flexibility for domain changes is required. 4 Tuning directly for MAE yielded similar results. 3108 MT.in-domain MT.out-of-domain ASR.in-domain ASR.out-of-domain ↓MAE m"
C16-1292,W04-3250,0,0.323366,"Missing"
C16-1292,C04-1072,0,0.326451,"verall quality of the whole output hypothesis. In this paper, we weigh segments proportionally to their length, although future work may investigate more sophisticated notions of segment importance. Our definition of document quality is simplistic, but widely used in both MT and ASR communities, e.g. document- or corpus-level WER, TER, METEOR (Banerjee and Lavie, 2005), and human rankings are usually computed this way. BLEU is computed on the corpus level and thus not directly usable with our approach, but we can instead resort to computing average sentence-level BLEU variants such as BLEU+1 (Lin and Och, 2004) that essentially differ only in the smoothing details. Our lightly supervised estimation framework determines document quality in several steps. The first step is to automatically estimate the quality for each segment in the hypothesis (§4). This can be achieved by training a regressor with the desired target measure, as discussed in numerous previous works. The 3104 second step is then to manually annotate the quality score for a certain number of segments. In our evaluation (§5), we experiment with typical amounts of tens to hundreds of annotated words. The final step is to aggregate manual"
C16-1292,C14-1171,0,0.0494527,"Missing"
C16-1292,P02-1040,0,0.103668,"chnology, enabling users and engineers to judge overall quality of the output, detect key problems, improve systems, and choose among competing systems. Although most users and engineers share these goals, the chosen evaluation approaches can differ strongly, with some people resorting to automatic, reference-based evaluation, while others rely on manual evaluation for their purposes. This is especially pronounced in the case of machine translation (MT), as pointed out by Harris et al. (2016). On one hand, much research effort has been devoted to devising reference-based methods such as BLEU (Papineni et al., 2002) that are well-correlated with human judgment. On the other hand, practitioners need to react to changing domains from customer to customer, and reflect multi-faceted quality requirements that are difficult to measure in a single, generic score, often leaving manual evaluation as the only choice. In recent years, automatic quality estimation (QE) has emerged as a method that could potentially address the lack of flexibility of reference-based evaluation to deal with changing requirements, and the high effort of manual evaluation. Automatic QE uses machine learning techniques that are trained o"
C16-1292,Q14-1025,0,0.0302706,"ia adaptation/multitask learning (Beck et al., 2014; de Souza et al., 2015b). Our indicator features can be seen as a form of multitask learning. A strategy for the case where no in-domain training data is available is to obtain such training data cheaply via active learning (Beck et al., 2013). This work is probably most similar in spirit to ours in that it attempts reliable quality estimation at low labeling costs. A different line of research, crowd-sourced annotation, critically depends on quality control, as well. Approaches are usually based on comparing results between several workers (Passonneau and Carpenter, 2014), querying gold standard “testing” labels occasionally (Joglekar and Garcia-Molina, 2013), and/or automatically predicting quality (Roy et al., 2010; Gao et al., 2015). Our work can be seen as a generalization of the latter two, with the gold labels corresponding to our fully manual baseline, the automatic estimation corresponding to our fully automatic baseline, and the workers being our systems. 7 Conclusion We proposed lightly supervised quality estimation at the document level, a framework that allows flexible quality estimation across changing domains and quality requirements, while requi"
C16-1292,2014.eamt-1.21,0,0.0283169,"ther or not the situation changes when switching to very different metrics, such as non reference-based metrics, is left as a question for future work. 3110 6 Relation to Prior Work A good overview over the state-of-the-art in automatic QE for MT is given by Bojar et al. (2015) and Bojar et al. (2016), and for ASR by Ogawa et al. (2012) and Negri et al. (2014). Document-level QE was first explored by Soricut and Echihabi (2010) by exploiting document-level features, and was later improved by using sentence-level information (Soricut and Narsale, 2012; Specia et al., 2015; Bojar et al., 2015). Scarton and Specia (2014) and Scarton et al. (2015) argue that document-level quality metrics should consider discourse information that cannot be captured when processing segments individually, and explore features for QE that capture discourse information. Quantitative assessment of discourse information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-domain case investigated in this work, i.e. predicting quality for a previously unknown system or task, has been found challenging in both ASR (Negri et al., 2014) and MT (de Souza et al., 2015a). T"
C16-1292,W15-4916,0,0.0378992,"Missing"
C16-1292,P10-1063,0,0.019873,"ar observations for MT, and we expect these findings to hold for other datasets. We also confirmed that results are similar when using BLEU+1 (Lin and Och, 2004) as the metric, instead of TER. Whether or not the situation changes when switching to very different metrics, such as non reference-based metrics, is left as a question for future work. 3110 6 Relation to Prior Work A good overview over the state-of-the-art in automatic QE for MT is given by Bojar et al. (2015) and Bojar et al. (2016), and for ASR by Ogawa et al. (2012) and Negri et al. (2014). Document-level QE was first explored by Soricut and Echihabi (2010) by exploiting document-level features, and was later improved by using sentence-level information (Soricut and Narsale, 2012; Specia et al., 2015; Bojar et al., 2015). Scarton and Specia (2014) and Scarton et al. (2015) argue that document-level quality metrics should consider discourse information that cannot be captured when processing segments individually, and explore features for QE that capture discourse information. Quantitative assessment of discourse information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-doma"
C16-1292,W12-3121,0,0.0227726,"n using BLEU+1 (Lin and Och, 2004) as the metric, instead of TER. Whether or not the situation changes when switching to very different metrics, such as non reference-based metrics, is left as a question for future work. 3110 6 Relation to Prior Work A good overview over the state-of-the-art in automatic QE for MT is given by Bojar et al. (2015) and Bojar et al. (2016), and for ASR by Ogawa et al. (2012) and Negri et al. (2014). Document-level QE was first explored by Soricut and Echihabi (2010) by exploiting document-level features, and was later improved by using sentence-level information (Soricut and Narsale, 2012; Specia et al., 2015; Bojar et al., 2015). Scarton and Specia (2014) and Scarton et al. (2015) argue that document-level quality metrics should consider discourse information that cannot be captured when processing segments individually, and explore features for QE that capture discourse information. Quantitative assessment of discourse information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-domain case investigated in this work, i.e. predicting quality for a previously unknown system or task, has been found challengin"
C16-1292,P13-4014,0,0.0470002,"Missing"
C16-1292,P15-4020,0,0.102177,"tor who replicates these exactly. Our main evaluation measure is mean absolute error (MAE)3 between the predicted and true document-level TER/WER. We use 5 datasets as indicated in Table 1, with testing data varying over all datasets, but only the ones labeled “in-domain” used for regressor training. Moreover, for each evaluated document the QE regressor was retrained with training data excluding that which corresponded to the same system or document currently tested (for in-domain tests). This makes even our in-domain scenario more challenging than some of the previous works on automatic QE (Specia et al., 2015). We use scikit-learn (Pedregosa et al., 2011) for regressor training. We assign weights to training samples proportional to their segment length, because longer segments are weighted more strongly in our aggregation strategies (§3) and are thus more important to be accurately predicted. We perform random search with 20 iterations to optimize hyper-parameters (namely, the max-depth and min-samplessplit parameters of XTs) in terms of mean squared error.4 Tuning is conducted separately for every test document, using 10-fold cross validation on the respective training data. For regressor adaptati"
C16-1292,2015.eamt-1.17,0,\N,Missing
C18-2020,2008.iwslt-papers.5,1,0.787873,"Missing"
C18-2020,W18-2712,1,0.826226,"tial sentences and the translation of full sentences (Niehues et al., 2018). One-Shot Learning In addition to overall translation quality, we identify the importance of translating rare events which do not appear many times in the training data but are critical to individual lectures They can be difficult to translate using NMT, but it is crucial for the system to translate them consistently. In order to incorporate external translations into the system, we designed a framework that allows the model to dynamically interact with external knowledge bases via both data augmentation and modeling (Pham et al., 2018). During training, we pre-train phrase-tables with the parallel corpora, and use them to annotate possible translations for the rare-words that appear less than 3 times in the training data. We consider word-splitting methods such as BPE crucial efficiently represent words that do not appear in the training data, and therefore allow proper annotation. By using the COPY-NET the model is able to learn a bias towards the annotation, which might otherwise have be assigned very small probabilities by the NMT softmax function. Finally, we use reinforcement learning to guide the search operation to e"
cho-etal-2014-corpus,fitzgerald-jelinek-2008-linguistic,0,\N,Missing
cho-etal-2014-corpus,E09-1030,0,\N,Missing
cho-etal-2014-corpus,W11-2124,1,\N,Missing
cho-etal-2014-corpus,P02-1040,0,\N,Missing
cho-etal-2014-corpus,P07-2045,0,\N,Missing
cho-etal-2014-corpus,P04-1005,0,\N,Missing
cho-etal-2014-corpus,stuker-etal-2012-kit,1,\N,Missing
cho-etal-2014-corpus,E14-4009,1,\N,Missing
cho-etal-2014-corpus,maekawa-etal-2000-spontaneous,0,\N,Missing
federico-etal-2012-iwslt,niessen-etal-2000-evaluation,0,\N,Missing
federico-etal-2012-iwslt,N04-4038,0,\N,Missing
federico-etal-2012-iwslt,P02-1040,0,\N,Missing
federico-etal-2012-iwslt,W07-0734,0,\N,Missing
federico-etal-2012-iwslt,2005.mtsummit-papers.11,0,\N,Missing
federico-etal-2012-iwslt,O07-5005,0,\N,Missing
federico-etal-2012-iwslt,2011.iwslt-evaluation.10,0,\N,Missing
federico-etal-2012-iwslt,I05-3027,0,\N,Missing
federico-etal-2012-iwslt,2011.iwslt-evaluation.1,1,\N,Missing
federico-etal-2012-iwslt,2010.iwslt-evaluation.5,1,\N,Missing
federico-etal-2012-iwslt,2010.iwslt-evaluation.1,1,\N,Missing
L16-1293,E09-1040,1,0.817996,"Missing"
L18-1533,W14-2201,0,0.0327313,"Missing"
L18-1533,L18-1531,1,0.779497,"Missing"
N16-3017,W09-0435,1,0.690583,"cribed using the Janus Recognition Toolkit (JRTk) (Woszczyna et al., 1994), which features the IBIS single-pass decoder (Soltau et al., 2001). The acoustic model was trained using several hundred hours of recordings from lectures and talks. Figure 1: User interface of the Lecture Translator showing an ongoing session For translation, we used a phrase-based decoder (Vogel, 2003). It uses advanced models for domain adaptation, bilingual and cluster language models in addition to Discriminative Word Lexica for producing the translation. We use POS-based word reordering (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). The translation model was trained on 1.8 million sentences of parallel data. It includes data from various sources and in-domain data. 4.2 System Operation The LT is in regular use for multiple years now and currently translates approx. 10 different lectures per term. We have installed this system in multiple lecture halls, among them KIT’s largest hall, called “Audimax”. In each hall, the system is tightly integrated in the PA to ensure smooth operation. The audio is captured via the PA from the microphone that the lecturer uses to address the audience. The operation of the system itself is"
N16-3017,P14-2090,0,0.0233272,"pite a difference in the overall quality of the translations, MT systems suffer from not being able to anticipate context like human interpreters. MT systems are unable to do so because of the lack of background and context knowledge. This results in a higher delay of the translation. But there has been some research towards the reduction of the latency and the translation of incomplete utterances (F¨ugen and Kolss, 2007), (Sridhar et al., 2013), (Oda et al., 83 2015). The goal is to find the optimal threshold between quality and latency (Shavarani et al., 2015), (Yarmohammadi et al., 2013), (Oda et al., 2014). With ongoing research and development, the systems have matured over the years. In order to assess whether our system helps students to better understand lectures, we have conducted a user study (M¨uller et al., 2016) (to appear). The outcome was that students actually benefit from our system. 3 Speech Translation Framework The Speech Translation Framework used for the lecture translation system is a component based architecture. It is designed to be flexible and distributed. There are 3 types of components: A central server, called the “mediator”, “workers” for performing different tasks an"
N16-3017,P15-1020,0,0.0438838,"Missing"
N16-3017,2007.tmi-papers.21,0,0.0215344,". The audio is being transcribed using the Janus Recognition Toolkit (JRTk) (Woszczyna et al., 1994), which features the IBIS single-pass decoder (Soltau et al., 2001). The acoustic model was trained using several hundred hours of recordings from lectures and talks. Figure 1: User interface of the Lecture Translator showing an ongoing session For translation, we used a phrase-based decoder (Vogel, 2003). It uses advanced models for domain adaptation, bilingual and cluster language models in addition to Discriminative Word Lexica for producing the translation. We use POS-based word reordering (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). The translation model was trained on 1.8 million sentences of parallel data. It includes data from various sources and in-domain data. 4.2 System Operation The LT is in regular use for multiple years now and currently translates approx. 10 different lectures per term. We have installed this system in multiple lecture halls, among them KIT’s largest hall, called “Audimax”. In each hall, the system is tightly integrated in the PA to ensure smooth operation. The audio is captured via the PA from the microphone that the lecturer uses to address the audience. The operati"
N16-3017,2015.iwslt-papers.14,0,0.0823231,"Missing"
N16-3017,N13-1023,0,0.030049,"e very domain specific and formalized dialogues. Later, systems supported greater variety in language, but were still built for specific domains (St¨uker et al., 2007). Despite a difference in the overall quality of the translations, MT systems suffer from not being able to anticipate context like human interpreters. MT systems are unable to do so because of the lack of background and context knowledge. This results in a higher delay of the translation. But there has been some research towards the reduction of the latency and the translation of incomplete utterances (F¨ugen and Kolss, 2007), (Sridhar et al., 2013), (Oda et al., 83 2015). The goal is to find the optimal threshold between quality and latency (Shavarani et al., 2015), (Yarmohammadi et al., 2013), (Oda et al., 2014). With ongoing research and development, the systems have matured over the years. In order to assess whether our system helps students to better understand lectures, we have conducted a user study (M¨uller et al., 2016) (to appear). The outcome was that students actually benefit from our system. 3 Speech Translation Framework The Speech Translation Framework used for the lecture translation system is a component based architectu"
N16-3017,L16-1293,1,\N,Missing
stuker-etal-2012-kit,P02-1040,0,\N,Missing
stuker-etal-2012-kit,2011.iwslt-evaluation.1,1,\N,Missing
stuker-etal-2012-kit,2011.iwslt-evaluation.16,1,\N,Missing
stuker-etal-2012-kit,2010.iwslt-evaluation.1,1,\N,Missing
W15-4657,D08-1027,0,0.313666,"Missing"
