2021.naacl-main.91,Assessing Reference-Free Peer Evaluation for Machine Translation,2021,-1,-1,2,0,3517,sweta agrawal,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Reference-free evaluation has the potential to make machine translation evaluation substantially more scalable, allowing us to pivot easily to new languages or domains. It has been recently shown that the probabilities given by a large, multilingual model can achieve state of the art results when used as a reference-free metric. We experiment with various modifications to this model, and demonstrate that by scaling it up we can match the performance of BLEU. We analyze various potential weaknesses of the approach, and find that it is surprisingly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities."
2020.wmt-1.140,Human-Paraphrased References Improve Neural Machine Translation,2020,-1,-1,2,0,3519,markus freitag,Proceedings of the Fifth Conference on Machine Translation,0,"Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by freitag2020bleu. When used in place of original references, the paraphrased versions produce metric scores that correlate better with human judgment. This effect holds for a variety of different automatic metrics, and tends to favor natural formulations over more literal (translationese) ones. In this paper we compare the results of performing end-to-end system development using standard and paraphrased references. With state-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is ignificantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements."
2020.iwslt-1.27,Re-translation versus Streaming for Simultaneous Translation,2020,14,2,4,0,18846,naveen arivazhagan,Proceedings of the 17th International Conference on Spoken Language Translation,0,"There has been great progress in improving streaming machine translation, a simultaneous paradigm where the system appends to a growing hypothesis as more source content becomes available. We study a related problem in which revisions to the hypothesis beyond strictly appending words are permitted. This is suitable for applications such as live captioning an audio feed. In this setting, we compare custom streaming approaches to re-translation, a straightforward strategy where each new source token triggers a distinct translation from scratch. We find re-translation to be as good or better than state-of-the-art streaming systems, even when operating under constraints that allow very few revisions. We attribute much of this success to a previously proposed data-augmentation technique that adds prefix-pairs to the training data, which alongside wait-k inference forms a strong baseline for streaming translation. We also highlight re-translation{'}s ability to wrap arbitrarily powerful MT systems with an experiment showing large improvements from an upgrade to its base model."
2020.emnlp-main.465,Inference Strategies for Machine Translation with Conditional Masking,2020,-1,-1,2,0,1027,julia kreutzer,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard {``}mask-predict{''} algorithm, and provide analyses of its behavior on machine translation tasks."
N19-1208,Reinforcement Learning based Curriculum Optimization for Neural Machine Translation,2019,0,9,2,0,5013,gaurav kumar,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"We consider the problem of making efficient use of heterogeneous training data in neural machine translation (NMT). Specifically, given a training dataset with a sentence-level feature such as noise, we seek an optimal curriculum, or order for presenting examples to the system during training. Our curriculum framework allows examples to appear an arbitrary number of times, and thus generalizes data weighting, filtering, and fine-tuning schemes. Rather than relying on prior knowledge to design a curriculum, we use reinforcement learning to learn one automatically, jointly with the NMT system, in the course of a single training run. We show that this approach can beat uniform baselines on Paracrawl and WMT English-to-French datasets by +3.4 and +1.3 BLEU respectively. Additionally, we match the performance of strong filtering baselines and hand-designed, state-of-the-art curricula."
P18-1008,The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation,2018,0,59,6,0,16329,mia chen,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT{'}14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets."
D18-1461,Revisiting Character-Based Neural Machine Translation with Capacity and Compression,2018,0,20,2,0,3520,colin cherry,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Translating characters instead of words or word-fragments has the potential to simplify the processing pipeline for neural machine translation (NMT), and improve results by eliminating hyper-parameters and manual feature engineering. However, it results in longer sequences in which each symbol contains less information, creating both modeling and computational challenges. In this paper, we show that the modeling problem can be solved by standard sequence-to-sequence architectures of sufficient depth, and that deep models operating at the character level outperform identical models operating over word fragments. This result implies that alternative architectures for handling character input are better viewed as methods for reducing computation time than as improved ways of modeling longer sequences. From this perspective, we evaluate several techniques for character-level NMT, verify that they do not match the performance of our deep character baseline model, and evaluate the performance versus computation time tradeoffs they offer. Within this framework, we also perform the first evaluation for NMT of conditional computation over time, in which the model learns which timesteps can be skipped, rather than having them be dictated by a fixed schedule specified before training begins."
W17-4732,{NRC} Machine Translation System for {WMT} 2017,2017,0,1,4,0.241615,13775,chikiu lo,Proceedings of the Second Conference on Machine Translation,0,None
W17-3205,Cost Weighting for Neural Machine Translation Domain Adaptation,2017,21,20,3,0,4084,boxing chen,Proceedings of the First Workshop on Neural Machine Translation,0,"In this paper, we propose a new domain adaptation technique for neural machine translation called cost weighting, which is appropriate for adaptation scenarios in which a small in-domain data set and a large general-domain data set are available. Cost weighting incorporates a domain classifier into the neural machine translation training algorithm, using features derived from the encoder representation in order to distinguish in-domain from out-of-domain data. Classifier probabilities are used to weight sentences according to their domain similarity when updating the parameters of the neural translation model. We compare cost weighting to two traditional domain adaptation techniques developed for statistical machine translation: data selection and sub-corpus weighting. Experiments on two large-data tasks show that both the traditional techniques and our novel proposal lead to significant gains, with cost weighting outperforming the traditional methods."
D17-1263,A Challenge Set Approach to Evaluating Machine Translation,2017,18,14,3,0,33186,pierre isabelle,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Neural machine translation represents an exciting leap forward in translation quality. But what longstanding weaknesses does it resolve, and which remain? We address these questions with a challenge set approach to translation evaluation and error analysis. A challenge set consists of a small set of sentences, each hand-designed to probe a system{'}s capacity to bridge a particular structural divergence between languages. To exemplify this approach, we present an English-French challenge set, and use it to analyze phrase-based and neural systems. The resulting analysis provides not only a more fine-grained picture of the strengths of neural systems, but also insight into which linguistic phenomena remain out of reach."
W16-2317,{NRC} {R}ussian-{E}nglish Machine Translation System for {WMT} 2016,2016,13,4,3,0.241615,13775,chikiu lo,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"We describe the statistical machine translation system developed at the National Research Council of Canada (NRC) for the Russian-English news translation task of the First Conference on Machine Translation (WMT 2016). Our submission is a phrase-based SMT system that tackles the morphological complexity of Russian through comprehensive use of lemmatization. The core of our lemmatization strategy is to use different views of Russian for different SMT components: word alignment and bilingual neural network language models use lemmas, while sparse features and reordering models use fully inflected forms. Some components, such as the phrase table, use both views of the source. Russian words that remain out-ofvocabulary (OOV) after lemmatization are transliterated into English using a statistical model trained on examples mined from the parallel training corpus. The NRC Russian-English MT system achieved the highest uncased BLEU and the lowest TER scores among the eight participants in WMT 2016."
2016.amta-researchers.8,Bilingual Methods for Adaptive Training Data Selection for Machine Translation,2016,-1,-1,3,0,4084,boxing chen,Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,0,"In this paper, we propose a new data selection method which uses semi-supervised convolutional neural networks based on bitokens (Bi-SSCNNs) for training machine translation systems from a large bilingual corpus. In earlier work, we devised a data selection method based on semi-supervised convolutional neural networks (SSCNNs). The new method, Bi-SSCNN, is based on bitokens, which use bilingual information. When the new methods are tested on two translation tasks (Chinese-to-English and Arabic-to-English), they significantly outperform the other three data selection methods in the experiments. We also show that the BiSSCNN method is much more effective than other methods in preventing noisy sentence pairs from being chosen for training. More interestingly, this method only needs a tiny amount of in-domain data to train the selection model, which makes fine-grained topic-dependent translation adaptation possible. In the follow-up experiments, we find that neural machine translation (NMT) is more sensitive to noisy data than statistical machine translation (SMT). Therefore, Bi-SSCNN which can effectively screen out noisy sentence pairs, can benefit NMT much more than SMT.We observed a BLEU improvement over 3 points on an English-to-French WMT task when Bi-SSCNNs were used."
W14-3363,Linear Mixture Models for Robust Machine Translation,2014,18,6,3,0,6058,marine carpuat,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"As larger and more diverse parallel texts become available, how can we leverage heterogeneous data to train robust machine translation systems that achieve good translation quality on various test domains? This challenge has been addressed so far by repurposing techniques developed for domain adaptation, such as linear mixture models which combine estimates learned on homogeneous subdomains. However, learning from large heterogeneous corpora is quite different from standard adaptation tasks with clear domain distinctions. In this paper, we show that linear mixture models can reliably improve translation quality in very heterogeneous training conditions, even if the mixtures do not use any domain knowledge and attempt to learn generic models rather than adapt them to the target domain. This surprising finding opens new perspectives for using mixture models in machine translation beyond clear cut domain adaptation tasks."
J14-2011,Book Reviews: Semi-Supervised Learning and Domain Adaptation in Natural Language Processing by Anders S{\\o}gaard,2014,5,1,1,1,3518,george foster,Computational Linguistics,0,None
2014.amta-researchers.3,Coarse {``}split and lump{''} bilingual language models for richer source information in {SMT},2014,-1,-1,4,0,12350,darlene stewart,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"Recently, there has been interest in automatically generated word classes for improving statistical machine translation (SMT) quality: e.g, (Wuebker et al, 2013). We create new models by replacing words with word classes in features applied during decoding; we call these {``}coarse models{''}. We find that coarse versions of the bilingual language models (biLMs) of (Niehues et al, 2011) yield larger BLEU gains than the original biLMs. BiLMs provide phrase-based systems with rich contextual information from the source sentence; because they have a large number of types, they suffer from data sparsity. Niehues et al (2011) mitigated this problem by replacing source or target words with parts of speech (POSs). We vary their approach in two ways: by clustering words on the source or target side over a range of granularities (word clustering), and by clustering the bilingual units that make up biLMs (bitoken clustering). We find that loglinear combinations of the resulting coarse biLMs with each other and with coarse LMs (LMs based on word classes) yield even higher scores than single coarse models. When we add an appealing {``}generic{''} coarse configuration chosen on English {\textgreater} French devtest data to four language pairs (keeping the structure fixed, but providing language-pair-specific models for each pair), BLEU gains on blind test data against strong baselines averaged over 5 runs are +0.80 for English {\textgreater} French, +0.35 for French {\textgreater} English, +1.0 for Arabic {\textgreater} English, and +0.6 for Chinese {\textgreater} English."
2014.amta-researchers.10,A comparison of mixture and vector space techniques for translation model adaptation,2014,39,4,3,0,4084,boxing chen,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"In this paper, we propose two extensions to the vector space model (VSM) adaptation technique (Chen et al., 2013b) for statistical machine translation (SMT), both of which result in significant improvements. We also systematically compare the VSM techniques to three mixture model adaptation techniques: linear mixture, log-linear mixture (Foster and Kuhn, 2007), and provenance features (Chiang et al., 2011). Experiments on NIST Chinese-to-English and Arabic-to-English tasks show that all methods achieve significant improvement over a competitive non-adaptive baseline. Except for the original VSM adaptation method, all methods yield improvements in the +1.7-2.0 BLEU range. Combining them gives further significant improvements of up to +2.6-3.3 BLEU over the baseline."
P13-1126,Vector Space Model for Adaptation in Statistical Machine Translation,2013,25,30,3,0.570211,4084,boxing chen,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (xe2x80x9cdevxe2x80x9d) set. This profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set. Then, for each phrase pair extracted from the training data, we create a vector with features defined in the same way, and calculate its similarity score with the vector representing the dev set. Thus, we obtain a decoding feature whose value represents the phrase pairxe2x80x99s closeness to the dev. This is a simple, computationally cheap form of instance weighting for phrase pairs. Experiments on large scale NIST evaluation data show improvements over strong baselines: 1.8 BLEU on Arabic to English and 1.4 BLEU on Chinese to English over a non-adapted baseline, and significant improvements in most circumstances over baselines with linear mixture model adaptation. An informal analysis suggests that VSM adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre."
N13-1114,Adaptation of Reordering Models for Statistical Machine Translation,2013,21,16,2,0.570211,4084,boxing chen,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Previous research on domain adaptation (DA) for statistical machine translation (SMT) has mainly focused on the translation model (TM) and the language model (LM). To the best of our knowledge, there is no previous work on reordering model (RM) adaptation for phrasebased SMT. In this paper, we demonstrate that mixture model adaptation of a lexicalized RM can significantly improve SMT performance, even when the system already contains a domain-adapted TM and LM. We find that, surprisingly, different training corpora can vary widely in their reordering characteristics for particular phrase pairs. Furthermore, particular training corpora may be highly suitable for training the TM or the LM, but unsuitable for training the RM, or vice versa, so mixture weights for these models should be estimated separately. An additional contribution of the paper is to propose two improvements to mixture model adaptation: smoothing the in-domain sample, and weighting instances by document frequency. Applied to mixture RMs in our experiments, these techniques (especially smoothing) yield significant performance improvements."
2013.mtsummit-papers.23,Simulating Discriminative Training for Linear Mixture Adaptation in Statistical Machine Translation,2013,-1,-1,1,1,3518,george foster,Proceedings of Machine Translation Summit XIV: Papers,0,None
2013.mtsummit-papers.24,{PEP}r: Post-Edit Propagation Using Phrase-based Statistical Machine Translation,2013,-1,-1,2,0,5047,michel simard,Proceedings of Machine Translation Summit XIV: Papers,0,None
W12-3104,"Improving {AMBER}, an {MT} Evaluation Metric",2012,10,23,3,0.640218,4084,boxing chen,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"A recent paper described a new machine translation evaluation metric, AMBER. This paper describes two changes to AMBER. The first one is incorporation of a new ordering penalty; the second one is the use of the downhill simplex algorithm to tune the weights for the components of AMBER. We tested the impact of the two changes, using data from the WMT metrics task. Each of the changes by itself improved the performance of AMBER, and the two together yielded even greater improvement, which in some cases was more than additive. The new version of AMBER clearly outperforms BLEU in terms of correlation with human judgment."
P12-1099,Mixing Multiple Translation Models in Statistical Machine Translation,2012,30,24,2,0,41426,majid razmara,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain. We propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step. In this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain. Our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation."
N12-1047,Batch Tuning Strategies for Statistical Machine Translation,2012,29,243,2,0,3520,colin cherry,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"There has been a proliferation of recent work on SMT tuning algorithms capable of handling larger feature sets than the traditional MERT approach. We analyze a number of these algorithms in terms of their sentence-level loss functions, which motivates several new approaches, including a Structured SVM. We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings. Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options."
2012.amta-papers.7,The Impact of Sentence Alignment Errors on Phrase-Based Machine Translation Performance,2012,-1,-1,3,0.276129,653,cyril goutte,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"When parallel or comparable corpora are harvested from the web, there is typically a tradeoff between the size and quality of the data. In order to improve quality, corpus collection efforts often attempt to fix or remove misaligned sentence pairs. But, at the same time, Statistical Machine Translation (SMT) systems are widely assumed to be relatively robust to sentence alignment errors. However, there is little empirical evidence to support and characterize this robustness. This contribution investigates the impact of sentence alignment errors on a typical phrase-based SMT system. We confirm that SMT systems are highly tolerant to noise, and that performance only degrades seriously at very high noise levels. Our findings suggest that when collecting larger, noisy parallel data for training phrase-based SMT, cleaning up by trying to detect and remove incorrect alignments can actually degrade performance. Although fixing errors, when applicable, is a preferable strategy to removal, its benefits only become apparent for fairly high misalignment rates. We provide several explanations to support these findings."
2011.mtsummit-papers.30,Unpacking and Transforming Feature Functions: New Ways to Smooth Phrase Tables,2011,-1,-1,3,0.766787,4084,boxing chen,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.iwslt-evaluation.19,Semantic smoothing and fabrication of phrase pairs for {SMT},2011,15,4,3,0.766787,4084,boxing chen,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In statistical machine translation systems, phrases with similar meanings often have similar but not identical distributions of translations. This paper proposes a new soft clustering method to smooth the conditional translation probabilities for a given phrase with those of semantically similar phrases. We call this semantic smoothing (SS). Moreover, we fabricate new phrase pairs that were not observed in training data, but which may be used for decoding. In learning curve experiments against a strong baseline, we obtain a consistent pattern of modest improvement from semantic smoothing, and further modest improvement from phrase pair fabrication."
W10-1702,Fast Consensus Hypothesis Regeneration for Machine Translation,2010,13,0,2,0.894527,4084,boxing chen,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper presents a fast consensus hypothesis regeneration approach for machine translation. It combines the advantages of feature-based fast consensus decoding and hypothesis regeneration. Our approach is more efficient than previous work on hypothesis regeneration, and it explores a wider search space than consensus decoding, resulting in improved performance. Experimental results show consistent improvements across language pairs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-to-English NIST task."
W10-1717,Lessons from {NRC}{'}s Portage System at {WMT} 2010,2010,9,6,3,0,5046,samuel larkin,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"NRC's Portage system participated in the English-French (E-F) and French-English (F-E) translation tasks of the ACL WMT 2010 evaluation. The most notable improvement over earlier versions of Portage is an efficient implementation of lattice MERT. While Portage has typically performed well in Chinese to English MT evaluations, most recently in the NIST09 evaluation, our participation in WMT 2010 revealed some interesting differences between Chinese-English and E-F/F-E translation, and alerted us to certain weak spots in our system. Most of this paper discusses the problems we found in our system and ways of fixing them. We learned several lessons that we think will be of general interest."
P10-1086,Bilingual Sense Similarity for Statistical Machine Translation,2010,39,15,2,0.894527,4084,boxing chen,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"This paper proposes new algorithms to compute the sense similarity between two units (words, phrases, rules, etc.) from parallel corpora. The sense similarity scores are computed by using the vector space model. We then apply the algorithms to statistical machine translation by computing the sense similarity between the source and target side of translation rule pairs. Similarity scores are used as additional features of the translation model to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system."
D10-1044,Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation,2010,24,170,1,1,3518,george foster,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure. We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines."
C10-1069,"Phrase Clustering for Smoothing {TM} Probabilities - or, How to Extract Paraphrases from Phrase Tables",2010,14,12,3,0.492659,17283,roland kuhn,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"This paper describes how to cluster together the phrases of a phrase-based statistical machine translation (SMT) system, using information in the phrase table itself. The clustering is symmetric and recursive: it is applied both to source-language and target-language phrases, and the clustering in one language helps determine the clustering in the other. The phrase clusters have many possible uses. This paper looks at one of these uses: smoothing the conditional translation model (TM) probabilities employed by the SMT system. We incorporated phrase-cluster-derived probability estimates into a baseline loglinear feature combination that included relative frequency and lexically-weighted conditional probability estimates. In Chinese-English (C-E) and French-English (F-E) learning curve experiments, we obtained a gain over the baseline in 29 of 30 tests, with a maximum gain of 0.55 BLEU points (though most gains were fairly small). The largest gains came with medium (200--400K sentence pairs) rather than with small (less than 100K sentence pairs) amounts of training data, contrary to what one would expect from the paraphrasing literature. We have only begun to explore the original smoothing approach described here."
2010.amta-papers.24,Translating Structured Documents,2010,-1,-1,1,1,3518,george foster,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"Machine Translation traditionally treats documents as sets of independent sentences. In many genres, however, documents are highly structured, and their structure contains information that can be used to improve translation quality. We present a preliminary approach to document translation that uses structural features to modify the behaviour of a language model, at sentence-level granularity. To our knowledge, this is the first attempt to incorporate structural information into statistical MT. In experiments on structured English/French documents from the Hansard corpus, we demonstrate small but statistically significant improvements."
W09-0439,Stabilizing Minimum Error Rate Training,2009,11,43,1,1,3518,george foster,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"The most commonly used method for training feature weights in statistical machine translation (SMT) systems is Och's minimum error rate training (MERT) procedure. A well-known problem with Och's procedure is that it tends to be sensitive to small changes in the system, particularly when the number of features is large. In this paper, we quantify the stability of Och's procedure by supplying different random seeds to a core component of the procedure (Powell's algorithm). We show that for systems with many features, there is extensive variation in outcomes, both on the development data and on the test data. We analyze the causes of this variation and propose modifications to the MERT procedure that improve stability while helping performance on test data."
2009.mtsummit-papers.2,Phrase Translation Model Enhanced with Association based Features,2009,-1,-1,2,0.894527,4084,boxing chen,Proceedings of Machine Translation Summit XII: Papers,0,None
C08-1115,Tighter Integration of Rule-Based and Statistical {MT} in Serial System Combination,2008,11,10,5,0,28492,nicola ueffing,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Recent papers have described machine translation (MT) based on an automatic post-editing or serial combination strategy whereby the input language is first translated into the target language by a rule-based MT (RBMT) system, then the target language output is automatically post-edited by a phrase-based statistical machine translation (SMT) system. This approach has been shown to improve MT quality over RBMT or SMT alone. In this previous work, there was a very loose coupling between the two systems: the SMT system only had access to the final 1-best translations from RBMT. Furthermore, the previous work involved European language pairs and relatively small training corpora. In this paper, we describe a more tightly integrated serial combination for the Chinese-to-English MT task. We will present experimental evaluation results on the 2008 NIST constrained data track where a significant gain in terms of both automatic and subjective metrics is achieved through the tighter coupling of the two systems."
W07-0703,Integration of an {A}rabic Transliteration Module into a Statistical Machine Translation System,2007,9,10,4,0,49040,mehdi kashani,Proceedings of the Second Workshop on Statistical Machine Translation,0,"We provide an in-depth analysis of the integration of an Arabic-to-English transliteration system into a general-purpose phrase-based statistical machine translation system. We study the integration from different aspects and evaluate the improvement that can be attributed to the integration using the BLEU metric. Our experiments show that a transliteration module can help significantly in the situation where the test data is rich with previously unseen named entities. We obtain 70% and 53% of the theoretical maximum improvement we could achieve, as measured by an oracle on development and test sets respectively for OOV words (out of vocabulary source words not appearing in the phrase table)."
W07-0717,Mixture-Model Adaptation for {SMT},2007,21,201,1,1,3518,george foster,Proceedings of the Second Workshop on Statistical Machine Translation,0,"We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components. We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to. The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system."
D07-1103,Improving Translation Quality by Discarding Most of the Phrasetable,2007,10,171,3,1,43880,howard johnson,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"It is possible to reduce the bulk of phrasetables for Statistical Machine Translation using a technique based on the significance testing of phrase pair co-occurrence in the parallel corpus. The savings can be quite substantial (up to 90%) and cause no reduction in BLEU score. In some cases, an improvement in BLEU is obtained at the same time although the effect is less pronounced if state-of-the-art phrasetable smoothing is employed."
W06-3118,{PORTAGE}: with Smoothed Phrase Tables and Segment Choice Models,2006,7,10,3,1,43880,howard johnson,Proceedings on the Workshop on Statistical Machine Translation,0,Improvements to Portage and its participation in the shared task of NAACL 2006 Workshop on Statistical Machine Translation are described. Promising ideas in phrase table smoothing and global distortion using feature-rich models are discussed as well as numerous improvements in the software base.
W06-1607,Phrasetable Smoothing for Statistical Machine Translation,2006,19,105,1,1,3518,george foster,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"We discuss different strategies for smoothing the phrasetable in Statistical MT, and give results over a range of translation settings. We show that any type of smoothing is a better idea than the relative-frequency estimates that are often used. The best smoothing techniques yield consistent gains of approximately 1% (absolute) according to the BLEU metric."
N06-1004,Segment Choice Models: Feature-Rich Models for Global Distortion in Statistical Machine Translation,2006,15,10,5,0.833333,17283,roland kuhn,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"This paper presents a new approach to distortion (phrase reordering) in phrase-based machine translation (MT). Distortion is modeled as a sequence of choices during translation. The approach yields trainable, probabilistic distortion models that are global: they assign a probability to each possible phrase reordering. These segment choice models (SCMs) can be trained on segment-aligned sentence pairs; they can be applied during decoding or rescoring. The approach yields a metric called distortion perplexity (disperp) for comparing SCMs offline on test data, analogous to perplexity for language models. A decision-tree-based SCM is tested on Chinese-to-English translation, and outperforms a baseline distortion penalty approach at the 99% confidence level."
2006.jeptalnrecital-poster.23,Syst{\\`e}me de traduction automatique statistique combinant diff{\\'e}rentes ressources,2006,-1,-1,2,0.961538,5647,fatiha sadat,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Cet article d{\'e}crit une approche combinant diff{\'e}rents mod{\`e}les statistiques pour la traduction automatique bas{\'e}e sur les segments. Pour ce faire, diff{\'e}rentes ressources sont utilis{\'e}es, dont deux corpus parall{\`e}les aux caract{\'e}ristiques diff{\'e}rentes et un dictionnaire de terminologie bilingue et ce, afin d{'}am{\'e}liorer la performance quantitative et qualitative du syst{\`e}me de traduction. Nous {\'e}valuons notre approche sur la paire de langues fran{\c{c}}ais-anglais et montrons comment la combinaison des ressources propos{\'e}es am{\'e}liore de fa{\c{c}}on significative les r{\'e}sultats."
W05-0822,{PORTAGE}: A Phrase-Based Machine Translation System,2005,7,31,4,0.961538,5647,fatiha sadat,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"This paper describes the participation of the Portage team at NRC Canada in the shared task of ACL 2005 Workshop on Building and Using Parallel Texts. We discuss Portage, a statistical phrase-based machine translation system, and present experimental results on the four language pairs of the shared task. First, we focus on the French-English task using multiple resources and techniques. Then we describe our contribution on the Finnish-English, Spanish-English and German-English language pairs using the provided data for the shared task."
H05-2001,Automatic Detection of Translation Errors: The State of the Art,2005,0,0,2,0,46591,graham russell,Proceedings of {HLT}/{EMNLP} 2005 Interactive Demonstrations,0,"The demonstration presents TransCheck, a translation quality-assurance tool developed jointly by the RALI group at the University of Montreal and the Interactive Language Technologies section of the Canadian National Research Council's Institute for Information Technology."
W04-3225,Adaptive Language and Translation Models for Interactive Machine Translation,2004,14,62,4,0,51333,laurent nepveu,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"We describe experiments carried out with adaptive language and translation models in the context of an interactive computer-assisted translation program. We developed cache-based language models which were then extended to the bilingual case for a cachebased translation model. We present the improvements we obtained in two contexts: in a theoretical setting, we achieved a drop in perplexity for the new models and, in a more practical situation simulating a user working with the system, we showed that fewer keystrokes would be needed to enter a translation."
C04-1046,Confidence Estimation for Machine Translation,2004,35,236,3,0,52360,john blatz,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We present a detailed study of confidence estimation for machine translation. Various methods for determining whether MT output is correct are investigated, for both whole sentences and words. Since the notion of correctness is not intuitively clear in this context, different ways of defining it are proposed. We present results on data from the NIST 2003 Chinese-to-English MT evaluation."
W03-0413,Confidence estimation for translation prediction,2003,13,48,2,0,51269,simona gandrabur,Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003,0,"The purpose of this work is to investigate the use of machine learning approaches for confidence estimation within a statistical machine translation application. Specifically, we attempt to learn probabilities of correctness for various model predictions, based on the native probabilites (i.e. the probabilites given by the original model) and on features of the current context. Our experiments were conducted using three original translation models and two types of neural nets (single-layer and multilayer perceptrons) for the confidence estimation task."
2003.mtsummit-papers.15,Statistical machine translation: rapid development with limited resources,2003,-1,-1,1,1,3518,george foster,Proceedings of Machine Translation Summit IX: Papers,0,"We describe an experiment in rapid development of a statistical machine translation (SMT) system from scratch, using limited resources: under this heading we include not only training data, but also computing power, linguistic knowledge, programming effort, and absolute time."
W02-1020,User-Friendly Text Prediction For Translators,2002,12,62,1,1,3518,george foster,Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002),0,"Text prediction is a form of interactive machine translation that is well suited to skilled translators. In principle it can assist in the production of a target text with minimal disruption to a translator's normal routine. However, recent evaluations of a prototype prediction system showed that it significantly decreased the productivity of most translators who used it. In this paper, we analyze the reasons for this and propose a solution which consists in seeking predictions that maximize the expected benefit to the translator, rather than just trying to anticipate some amount of upcoming text. Using a model of a typical translator constructed from data collected in the evaluations of the prediction prototype, we show that this approach has the potential to turn text prediction into a help rather than a hindrance to a translator."
foster-etal-2002-text,Text prediction with fuzzy alignment,2002,12,1,1,1,3518,george foster,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Text prediction is a form of interactive machine translation that is well suited to skilled translators. In recent work it has been shown that simple statistical translation models can be applied within a usermodeling framework to improve translator productivity by over 10{\%} in simulated results. For the sake of efficiency in making real-time predictions, these models ignore the alignment relation between source and target texts. In this paper we introduce a new model that captures fuzzy alignments in a very simple way, and show that it gives modest improvements in predictive performance without significantly increasing the time required to generate predictions."
2001.mtsummit-papers.36,Integrating bilingual lexicons in a probabilistic translation assistant,2001,10,8,2,1,7084,philippe langlais,Proceedings of Machine Translation Summit VIII,0,"In this paper, we present a way to integrate bilingual lexicons into an operational probabilistic translation assistant (TransType). These lexicons could be any resource available to the translator (e.g. terminological lexicons) or any resource statistically derived from training material. We describe a bilingual lexicon acquisition process that we developped and we evaluate from a theoretical point of view its benefits to a translation completion task."
W00-0707,Incorporating Position Information into a Maximum Entropy/Minimum Divergence Translation Model,2000,11,12,1,1,3518,george foster,Fourth Conference on Computational Natural Language Learning and the Second Learning Language in Logic Workshop,0,I describe two methods for incorporating information about the relative positions of bilingual word pairs into a Maximum Entropy/Minimum Divergence translation model. The better of the two achieves over 40% lower test corpus perplexity than an equivalent combination of a trigram language model and the classical IBM translation model 2.
W00-0507,{T}rans{T}ype: a Computer-Aided Translation Typing System,2000,13,95,2,1,7084,philippe langlais,{ANLP}-{NAACL} 2000 Workshop: Embedded Machine Translation Systems,0,"This paper describes the embedding of a statistical translation system within a text editor to produce TRANSTYPE, a system that watches over the user as he or she types a translation and repeatedly suggests completions for the text already entered. This innovative Embedded Machine Translation system is thus a specialized means of helping produce high quality translations."
P00-1006,A Maximum Entropy/Minimum Divergence Translation Model,2000,15,29,1,1,3518,george foster,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"I present empirical comparisons between a linear combination of standard statistical language and translation models and an equivalent Maximum Entropy/Minimum Divergence (MEMD) model, using several different methods for automatic feature selection. The MEMD model significantly outperforms the standard model in test corpus perplexity, even though it has far fewer parameters."
langlais-etal-2000-evaluation,"Evaluation of {TRANSTYPE}, a Computer-aided Translation Typing System: A Comparison of a Theoretical- and a User-oriented Evaluation Procedures",2000,13,14,3,1,7084,philippe langlais,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"We describe and compare two protocols xe2x80x94 one theoretical and the other in-situs xe2x80x94 for evaluating the TRANSTYPE system, a target-text mediated interactive machine translation prototype which predicts in real time the words of the ongoing translation."
A00-1019,Unit Completion for a Computer-aided Translation Typing System,2000,42,11,2,1,7084,philippe langlais,Sixth Applied Natural Language Processing Conference,0,"This work is in the context of TRANSTYPE, a system that observes its user as he or she types a translation and repeatedly suggests completions for the text already entered. The user may either accept, modify, or ignore these suggestions. We describe the design, implementation, and performance of a prototype which suggests completions of units of texts that are longer than one word."
W98-1103,Using a Probabilistic Translation Model for Cross-Language Information Retrieval,1998,7,14,3,0,4253,jianyun nie,Sixth Workshop on Very Large Corpora,0,"Abst rac t There is an increasing need for document search mechanisms capable of matching a natural language query with documents written in a different language. Recently, we conducted several experiments aimed at comparing various methods of incorporating a cross-linguistic capability to existing information retrieval (IR) systems. Our results indicate that translating queries with off-theshelf machine translation systems can result in relatively good performance. But the results also indicate that other methods can perfonn even better. More specifically, we tested a probabilistic translation model of the kind proposed by Brown & al. [2]. The parameters of that system had been estimated automatically on a different, unrelated, corpus of parallel texts. After we augmented it with a small bilingual dictionary, this probabilistic translation model outperformed machine translation systems on our cross-language IR task."
C96-1067,Word Completion- A First Step Toward Target-Text Mediated {IMT},1996,15,15,1,1,3518,george foster,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"We argue that the conventional approach to Interactive Machine Translation is not the best way to provide assistance to skilled translators, and propose an alternative whose central feature is the use of the target text as a medium of interaction. We describe an automatic word-completion system intended to serve as a vehicle for exploring the feasibility of this new approach, and give results in terms of keystrokes saved in a test corpus."
1993.tmi-1.17,Translation Analysis and Translation Automation,1993,10,51,3,0.437473,33186,pierre isabelle,Proceedings of the Fifth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"We argue that the concept of translation analysis provides a suitable foundation for a new generation of translation support tools. We show that pre-existing translations can be analyzed into a structured translation memory and describe our TransSearch bilingual concordancing system, which allows translators to harness such a memory. We claim that translation analyzers can help detect translation errors in draft translations and we present the results of an experiment on the detection of deceptive cognates conducted as part of our TransCheck project. Finally, we claim that translation analysis can facilitate the speech-to-text transcription of dictated translations and introduce our new TransTalk project."
1992.tmi-1.7,Using cognates to align sentences in bilingual corpora,1992,-1,-1,2,0,5047,michel simard,Proceedings of the Fourth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
