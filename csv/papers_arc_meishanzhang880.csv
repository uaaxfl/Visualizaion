2021.findings-emnlp.149,{APGN}: Adversarial and Parameter Generation Networks for Multi-Source Cross-Domain Dependency Parsing,2021,-1,-1,2,0,6800,ying li,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Thanks to the strong representation learning capability of deep learning, especially pre-training techniques with language model loss, dependency parsing has achieved great performance boost in the in-domain scenario with abundant labeled training data for target domains. However, the parsing community has to face the more realistic setting where the parsing performance drops drastically when labeled data only exists for several fixed out-domains. In this work, we propose a novel model for multi-source cross-domain dependency parsing. The model consists of two components, i.e., a parameter generation network for distinguishing domain-specific features, and an adversarial network for learning domain-invariant representations. Experiments on a recently released NLPCC-2019 dataset for multi-domain dependency parsing show that our model can consistently improve cross-domain parsing performance by about 2 points in averaged labeled attachment accuracy (LAS) over strong BERT-enhanced baselines. Detailed analysis is conducted to gain more insights on contributions of the two components."
2021.emnlp-main.291,A Fine-Grained Domain Adaption Model for Joint Word Segmentation and {POS} Tagging,2021,-1,-1,4,0,9288,peijie jiang,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Domain adaption for word segmentation and POS tagging is a challenging problem for Chinese lexical processing. Self-training is one promising solution for it, which struggles to construct a set of high-quality pseudo training instances for the target domain. Previous work usually assumes a universal source-to-target adaption to collect such pseudo corpus, ignoring the different gaps from the target sentences to the source domain. In this work, we start from joint word segmentation and POS tagging, presenting a fine-grained domain adaption method to model the gaps accurately. We measure the gaps by one simple and intuitive metric, and adopt it to develop a pseudo target domain corpus based on fine-grained subdomains incrementally. A novel domain-mixed representation learning model is proposed accordingly to encode the multiple subdomains effectively. The whole process is performed progressively for both corpus construction and model training. Experimental results on a benchmark dataset show that our method can gain significant improvements over a vary of baselines. Extensive analyses are performed to show the advantages of our final domain adaption model as well."
2021.emnlp-main.314,A Graph-Based Neural Model for End-to-End Frame Semantic Parsing,2021,-1,-1,3,0,9352,zhichao lin,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Frame semantic parsing is a semantic analysis task based on FrameNet which has received great attention recently. The task usually involves three subtasks sequentially: (1) target identification, (2) frame classification and (3) semantic role labeling. The three subtasks are closely related while previous studies model them individually, which ignores their intern connections and meanwhile induces error propagation problem. In this work, we propose an end-to-end neural model to tackle the task jointly. Concretely, we exploit a graph-based method, regarding frame semantic parsing as a graph construction problem. All predicates and roles are treated as graph nodes, and their relations are taken as graph edges. Experiment results on two benchmark datasets of frame semantic parsing show that our method is highly competitive, resulting in better performance than pipeline models."
2021.emnlp-main.796,{C}hinese Opinion Role Labeling with Corpus Translation: A Pivot Study,2021,-1,-1,5,0,10217,ranran zhen,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Opinion Role Labeling (ORL), aiming to identify the key roles of opinion, has received increasing interest. Unlike most of the previous works focusing on the English language, in this paper, we present the first work of Chinese ORL. We construct a Chinese dataset by manually translating and projecting annotations from a standard English MPQA dataset. Then, we investigate the effectiveness of cross-lingual transfer methods, including model transfer and corpus translation. We exploit multilingual BERT with Contextual Parameter Generator and Adapter methods to examine the potentials of unsupervised cross-lingual learning and our experiments and analyses for both bilingual and multilingual transfers establish a foundation for the future research of this task."
2021.acl-long.372,A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition,2021,-1,-1,3,0.701754,7621,fei li,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Research on overlapped and discontinuous named entity recognition (NER) has received increasing attention. The majority of previous work focuses on either overlapped or discontinuous entities. In this paper, we propose a novel span-based model that can recognize both overlapped and discontinuous entities jointly. The model includes two major steps. First, entity fragments are recognized by traversing over all possible text spans, thus, overlapped entities can be recognized. Second, we perform relation classification to judge whether a given pair of entity fragments to be overlapping or succession. In this way, we can recognize not only discontinuous entities, and meanwhile doubly check the overlapped entities. As a whole, our model can be regarded as a relation extraction paradigm essentially. Experimental results on multiple benchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly competitive for overlapped and discontinuous NER."
2021.acl-long.432,Crowdsourcing Learning as Domain Adaptation: {A} Case Study on Named Entity Recognition,2021,-1,-1,4,0,9475,xin zhang,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Crowdsourcing is regarded as one prospective solution for effective supervised learning, aiming to build large-scale annotated training data by crowd workers. Previous studies focus on reducing the influences from the noises of the crowdsourced annotations for supervised models. We take a different point in this work, regarding all crowdsourced annotations as gold-standard with respect to the individual annotators. In this way, we find that crowdsourcing could be highly similar to domain adaptation, and then the recent advances of cross-domain methods can be almost directly applied to crowdsourcing. Here we take named entity recognition (NER) as a study case, suggesting an annotator-aware representation learning model that inspired by the domain adaptation methods which attempt to capture effective domain-aware features. We investigate both unsupervised and supervised crowdsourcing learning, assuming that no or only small-scale expert annotations are available. Experimental results on a benchmark crowdsourced NER dataset show that our method is highly effective, leading to a new state-of-the-art performance. In addition, under the supervised setting, we can achieve impressive performance gains with only a very small scale of expert annotations."
2020.coling-main.263,End to End {C}hinese Lexical Fusion Recognition with Sememe Knowledge,2020,52,0,2,0,21360,yijiang liu,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this paper, we present Chinese lexical fusion recognition, a new task which could be regarded as one kind of coreference recognition. First, we introduce the task in detail, showing the relationship with coreference recognition and differences from the existing tasks. Second, we propose an end-to-end model for the task, handling mentions as well as coreference relationship jointly. The model exploits the state-of-the-art contextualized BERT representations as an encoder, and is further enhanced with the sememe knowledge from HowNet by graph attention networks. We manually annotate a benchmark dataset for the task and then conduct experiments on it. Results demonstrate that our final model is effective and competitive for the task. Detailed analysis is offered for comprehensively understanding the new task and our proposed model."
2020.coling-main.370,{H}i{T}rans: A Transformer-Based Context- and Speaker-Sensitive Model for Emotion Detection in Conversations,2020,-1,-1,4,0,7778,jingye li,Proceedings of the 28th International Conference on Computational Linguistics,0,"Emotion detection in conversations (EDC) is to detect the emotion for each utterance in conversations that have multiple speakers. Different from the traditional non-conversational emotion detection, the model for EDC should be context-sensitive (e.g., understanding the whole conversation rather than one utterance) and speaker-sensitive (e.g., understanding which utterance belongs to which speaker). In this paper, we propose a transformer-based context- and speaker-sensitive model for EDC, namely HiTrans, which consists of two hierarchical transformers. We utilize BERT as the low-level transformer to generate local utterance representations, and feed them into another high-level transformer so that utterance representations could be sensitive to the global context of the conversation. Moreover, we exploit an auxiliary task to make our model speaker-sensitive, called pairwise utterance speaker verification (PUSV), which aims to classify whether two utterances belong to the same speaker. We evaluate our model on three benchmark datasets, namely EmoryNLP, MELD and IEMOCAP. Results show that our model outperforms previous state-of-the-art models."
2020.ccl-1.75,Cross-Lingual Dependency Parsing via Self-Training,2020,-1,-1,1,1,6801,meishan zhang,Proceedings of the 19th Chinese National Conference on Computational Linguistics,0,"Recent advances of multilingual word representations weaken the input divergences across languages, making cross-lingual transfer similar to the monolingual cross-domain and semi-supervised settings. Thus self-training, which is effective for these settings, could be possibly beneficial to cross-lingual as well. This paper presents the first comprehensive study for self-training in cross-lingual dependency parsing. Three instance selection strategies are investigated, where two of which are based on the baseline dependency parsing model, and the third one adopts an auxiliary cross-lingual POS tagging model as evidence. We conduct experiments on the universal dependencies for eleven languages. Results show that self-training can boost the dependency parsing performances on the target languages. In addition, the POS tagger assistant instance selection can achieve further improvements consistently. Detailed analysis is conducted to examine the potentiality of self-training in-depth."
2020.acl-main.609,{DRTS} Parsing with Structure-Aware Encoding and Decoding,2020,31,0,4,0,13181,qiankun fu,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Discourse representation tree structure (DRTS) parsing is a novel semantic parsing task which has been concerned most recently. State-of-the-art performance can be achieved by a neural sequence-to-sequence model, treating the tree construction as an incremental sequence generation problem. Structural information such as input syntax and the intermediate skeleton of the partial output has been ignored in the model, which could be potentially useful for the DRTS parsing. In this work, we propose a structural-aware model at both the encoder and decoder phase to integrate the structural information, where graph attention network (GAT) is exploited for effectively modeling. Experimental results on a benchmark dataset show that our proposed model is effective and can obtain the best performance in the literature."
2020.acl-main.627,Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus,2020,54,0,2,0,7618,hao fei,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Many efforts of research are devoted to semantic role labeling (SRL) which is crucial for natural language understanding. Supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages such as English. While for the low-resource languages with no annotated SRL dataset, it is still challenging to obtain competitive performances. Cross-lingual SRL is one promising way to address the problem, which has achieved great advances with the help of model transferring and annotation projection. In this paper, we propose a novel alternative based on corpus translation, constructing high-quality training datasets for the target languages from the source gold-standard SRL annotations. Experimental results on Universal Proposition Bank show that the translation-based method is highly effective, and the automatic pseudo datasets can improve the target-language SRL performances significantly."
N19-1066,Enhancing Opinion Role Labeling with Semantic-Aware Word Representations from Semantic Role Labeling,2019,0,1,1,1,6801,meishan zhang,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47{\%} higher F-scores than the better one."
N19-1118,Syntax-Enhanced Neural Machine Translation with Syntax-Aware Word Representations,2019,0,7,1,1,6801,meishan zhang,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Syntax has been demonstrated highly effective in neural machine translation (NMT). Previous NMT models integrate syntax by representing 1-best tree outputs from a well-trained parsing system, e.g., the representative Tree-RNN and Tree-Linearization methods, which may suffer from error propagation. In this work, we propose a novel method to integrate source-side syntax implicitly for NMT. The basic idea is to use the intermediate hidden representations of a well-trained end-to-end dependency parser, which are referred to as syntax-aware word representations (SAWRs). Then, we simply concatenate such SAWRs with ordinary word embeddings to enhance basic NMT models. The method can be straightforwardly integrated into the widely-used sequence-to-sequence (Seq2Seq) NMT models. We start with a representative RNN-based Seq2Seq baseline system, and test the effectiveness of our proposed method on two benchmark datasets of the Chinese-English and English-Vietnamese translation tasks, respectively. Experimental results show that the proposed approach is able to bring significant BLEU score improvements on the two datasets compared with the baseline, 1.74 points for Chinese-English translation and 0.80 point for English-Vietnamese translation, respectively. In addition, the approach also outperforms the explicit Tree-RNN and Tree-Linearization methods."
D19-1092,Cross-Lingual Dependency Parsing Using Code-Mixed {T}ree{B}ank,2019,0,2,1,1,6801,meishan zhang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Treebank translation is a promising method for cross-lingual transfer of syntactic dependency knowledge. The basic idea is to map dependency arcs from a source treebank to its target translation according to word alignments. This method, however, can suffer from imperfect alignment between source and target words. To address this problem, we investigate syntactic transfer by code mixing, translating only confident words in a source treebank. Cross-lingual word embeddings are leveraged for transferring syntactic knowledge to the target from the resulting code-mixed treebank. Experiments on University Dependency Treebanks show that code-mixed treebanks are more effective than translated treebanks, giving highly competitive performances among cross-lingual parsing methods."
C18-1047,Transition-based Neural {RST} Parsing with Implicit Syntax Features,2018,0,8,2,0,7041,nan yu,Proceedings of the 27th International Conference on Computational Linguistics,0,"Syntax has been a useful source of information for statistical RST discourse parsing. Under the neural setting, a common approach integrates syntax by a recursive neural network (RNN), requiring discrete output trees produced by a supervised syntax parser. In this paper, we propose an implicit syntax feature extraction approach, using hidden-layer vectors extracted from a neural syntax parser. In addition, we propose a simple transition-based model as the baseline, further enhancing it with dynamic oracle. Experiments on the standard dataset show that our baseline model with dynamic oracle is highly competitive. When implicit syntax features are integrated, we are able to obtain further improvements, better than using explicit Tree-RNN."
D17-1182,End-to-End Neural Relation Extraction with Global Optimization,2017,14,22,1,1,6801,meishan zhang,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Neural networks have shown promising results for relation extraction. State-of-the-art models cast the task as an end-to-end problem, solved incrementally using a local classifier. Yet previous work using statistical models have demonstrated that global optimization can achieve better performances compared to local classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks."
D17-1296,Transition-Based Disfluency Detection using {LSTM}s,2017,0,6,4,0,15989,shaolei wang,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we model the problem of disfluency detection using a transition-based framework, which incrementally constructs and labels the disfluency chunk of input sentences using a new transition system without syntax information. Compared with sequence labeling methods, it can capture non-local chunk-level features; compared with joint parsing and disfluency detection methods, it is free for noise in syntax. Experiments show that our model achieves state-of-the-art f-score of 87.5{\%} on the commonly used English Switchboard test set, and a set of in-house annotated Chinese data."
P16-1040,Transition-Based Neural Word Segmentation,2016,38,56,1,1,6801,meishan zhang,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
L16-1034,{L}ib{N}3{L}:A Lightweight Package for Neural {NLP},2016,23,9,1,1,6801,meishan zhang,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present a light-weight machine learning tool for NLP research. The package supports operations on both discrete and dense vectors, facilitating implementation of linear models as well as neural models. It provides several basic layers which mainly aims for single-layer linear and non-linear transformations. By using these layers, we can conveniently implement linear models and simple neural models. Besides, this package also integrates several complex layers by composing those basic layers, such as RNN, Attention Pooling, LSTM and gated RNN. Those complex layers can be used to implement deep neural models directly."
C16-1231,Tweet Sarcasm Detection Using Deep Neural Network,2016,31,30,1,1,6801,meishan zhang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Sarcasm detection has been modeled as a binary document classification task, with rich features being defined manually over input documents. Traditional models employ discrete manual features to address the task, with much research effect being devoted to the design of effective feature templates. We investigate the use of neural network for tweet sarcasm detection, and compare the effects of the continuous automatic features with discrete manual features. In particular, we use a bi-directional gated recurrent neural network to capture syntactic and semantic information over tweets locally, and a pooling neural network to extract contextual features automatically from history tweets. Results show that neural features give improved accuracies for sarcasm detection, with different error distributions compared with discrete manual features."
P15-1045,Event-Driven Headline Generation,2015,40,11,3,0,37493,rui sun,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We propose an event-driven model for headline generation. Given an input document, the system identifies a key event chain by extracting a set of structural events that describe them. Then a novel multi-sentence compression algorithm is used to fuse the extracted events, generating a headline for the document. Our model can be viewed as a novel combination of extractive and abstractive headline generation, combining the advantages of both methods using event structures. Standard evaluation shows that our model achieves the best performance compared with previous state-of-the-art systems."
D15-1073,Neural Networks for Open Domain Targeted Sentiment,2015,35,62,1,1,6801,meishan zhang,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Open domain targeted sentiment is the joint information extraction task that finds target mentions together with the sentiment towards each mention from a text corpus. The task is typically modeled as a sequence labeling problem, and solved using state-of-the-art labelers such as CRF. We empirically study the effect of word embeddings and automatic feature combinations on the task by extending a CRF baseline using neural networks, which have demonstrated large potentials for sentiment analysis. Results show that the neural model can give better results by significantly increasing the recall. In addition, we propose a novel integration of neural and discrete features, which combines their relative advantages, leading to significantly higher results compared to both baselines."
D15-1153,Combining Discrete and Continuous Features for Deterministic Transition-based Dependency Parsing,2015,26,11,1,1,6801,meishan zhang,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We investigate a combination of a traditional linear sparse feature model and a multi-layer neural network model for deterministic transition-based dependency parsing, by integrating the sparse features into the neural model. Correlations are drawn between the hybrid model and previous work on integrating word embedding features into a discrete linear model. By analyzing the results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods."
D15-1211,"A Transition-based Model for Joint Segmentation, {POS}-tagging and Normalization",2015,31,7,3,0,37831,tao qian,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We propose a transition-based model for joint word segmentation, POS tagging and text normalization. Different from previous methods, the model can be trained on standard text corpora, overcoming the lack of annotated microblog corpora. To evaluate our model, we develop an annotated corpus based on microblogs. Experimental results show that our joint model can help improve the performance of word segmentation on microblogs, giving an error reduction in segmentation accuracy of 12.02%, compared to the traditional approach."
P14-6008,Syntactic Processing Using Global Discriminative Learning and Beam-Search Decoding,2014,15,1,2,0.0845371,884,yue zhang,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials,0,None
P14-1125,Character-Level {C}hinese Dependency Parsing,2014,30,42,1,1,6801,meishan zhang,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Recent work on Chinese analysis has led to large-scale annotations of the internal structures of words, enabling characterlevel analysis of Chinese syntactic structures. In this paper, we investigate the problem of character-level Chinese dependency parsing, building dependency trees over characters. Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving wordlevel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods."
E14-1062,Type-Supervised Domain Adaptation for Joint Segmentation and {POS}-Tagging,2014,20,27,1,1,6801,meishan zhang,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,None
C14-1051,Jointly or Separately: Which is Better for Parsing Heterogeneous Dependencies?,2014,35,4,1,1,6801,meishan zhang,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"For languages such as English, several constituent-to-dependency conversion schemes are proposed to construct corpora for dependency parsing. It is hard to determine which scheme is better because they reflect different views of dependency analysis. We usually obtain dependency parsers of different schemes by training with the specific corpus separately. It neglects the correlations between these schemes, which can potentially benefit the parsers. In this paper, we study how these correlations influence final dependency parsing performances, by proposing a joint model which can make full use of the correlations between heterogeneous dependencies, and finally we can answer the following question: parsing heterogeneous dependencies jointly or separately, which is better? We conduct experiments with two different schemes on the Penn Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that jointly parsing heterogeneous dependencies can give improved performances for both schemes over the individual models."
P13-1013,{C}hinese Parsing Exploiting Characters,2013,21,50,1,1,6801,meishan zhang,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Characters play an important role in the Chinese language, yet computational processing of Chinese has been dominated by word-based approaches, with leaves in syntax trees being words. We investigate Chinese parsing from the character-level, extending the notion of phrase-structure trees by annotating internal structures of words. We demonstrate the importance of character-level information to Chinese processing by building a joint segmentation, part-of-speech (POS) tagging and phrase-structure parsing system that integrates character-structure features. Our joint system significantly outperforms a state-of-the-art word-based baseline on the standard CTB5 test, and gives the best published results for Chinese parsing."
W12-6316,Micro blogs Oriented Word Segmentation System,2012,3,1,2,0,3664,yijia liu,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"We present a Chinese word segmentation system submitted to the first task on CLP 2012 back-offs. Our segmenter is built using a conditional random field sequence model. We set the combination of a few annotated micro blogs and People Daily corpus as the training data. We encode special words detected by rules and information extracted from unlabeled data into features. These features are used to improve our modelxe2x80x99s performance. We also derive a micro blog specified lexicon from auto-analyzed data and use lexicon related features to assist the model. When testing on the sample data of this task, these features result in 1.8% improvement over the baseline model. Finally, our model achieves F-score of 94.07% on the bakeoffxe2x80x99s test set."
W12-6330,Multiple {T}ree{B}anks Integration for {C}hinese Phrase Structure Grammar Parsing Using Bagging,2012,9,1,1,1,6801,meishan zhang,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"We describe our method of traditional Phrase Structure Grammar (PSG) parsing in CIPS-Bakeoff2012 Task3. First, bagging is proposed to enhance the baseline performance of PSG parsing. Then we suggest exploiting another TreeBank (CTB7.0) to improve the performance further. Experimental results on the development data set demonstrate that bagging can boost the baseline F1 score from 81.33% to 84.41%. After exploiting the data of CTB7.0, the F1 score reaches 85.03%. Our final results on the official test data set show that the baseline closed system using bagging gets the F1 score of 80.17%. It outperforms the best closed system by nearly 4% which uses a single model. After exploiting the CTB7.0 data, the F1 score reaches 81.16%, demonstrating further increases of about 1%."
S12-1050,{S}em{E}val-2012 Task 5: {C}hinese Semantic Dependency Parsing,2012,15,11,2,0,1017,wanxiang che,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"The paper presents the SemEval-2012 Shared Task 5: Chinese Semantic Dependency Parsing. The goal of this task is to identify the dependency structure of Chinese sentences from the semantic view. We firstly introduce the motivation of providing Chinese semantic dependency parsing task, and then describe the task in detail including data preparation, data format, task evaluation, and so on. Over ten thousand sentences were labeled for participants to train and evaluate their systems. At last, we briefly describe the submitted systems and analyze these results."
C12-1188,Stacking Heterogeneous Joint Models of {C}hinese {POS} Tagging and Dependency Parsing,2012,34,4,1,1,6801,meishan zhang,Proceedings of {COLING} 2012,0,"Previous joint models of Chinese part-of-speech (POS) tagging and dependency parsing are extended from either graphor transition-based dependency models. Our analysis shows that the two models have different error distributions. In addition, integration of graphand transition-based dependency parsers by stacked learning (stacking) has achieved significant improvements. These motivate us to study the problem of stacking graphand transition-based joint models. We conduct experiments on Chinese Penn Treebank 5.1 (CTB5.1). The results demonstrate that the guided transition-based joint model obtains better performance than the guided graph-based joint model. Further, we introduce a constituent-based joint model which derives the POS tag sequence and dependency tree from the output of PCFG parsers, and then integrate it into the guided transition-based joint model. Finally, we achieve the best performance on CTB5.1, 94.95% in tagging accuracy and 83.98% in parsing accuracy respectively. TITLE AND ABSTRACT IN CHINESE xe9x87x87xe7x94xa8xe5xa0x86xe6x96xb9xe6xb3x95xe8x9ex8dxe5x90x88xe5xbcx82xe7xa7x8dxe7x9ax84xe4xb8xadxe6x96x87xe8xafx8dxe6x80xa7xe5x92x8cxe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8b xe8xbfx87xe5x8exbbxe7x9ax84xe4xb8xadxe6x96x87xe8xafx8dxe6x80xa7xe5x92x8cxe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8bxe5x9fxbaxe6x9cxacxe4xb8x8axe9x83xbdxe6xa0xb9xe6x8dxaexe5x9fxbaxe4xbax8exe5x9bxbexe7x9ax84xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe5x88x86xe6x9ex90xe6xa8xa1xe5x9ex8bxe6x88x96xe8x80x85 xe5x9fxbaxe4xbax8exe8xbdxacxe7xa7xbbxe7x9ax84xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe5x88x86xe6x9ex90xe6xa8xa1xe5x9ex8bxe8xbfx9bxe8xa1x8cxe6x8bx93xe5xb1x95xe8x80x8cxe5xbdxa2xe6x88x90xe7x9ax84xe3x80x82xe6x88x91xe4xbbxacxe7x9ax84xe5x88x86xe6x9ex90xe7xbbx93xe6x9ex9cxe8xa1xa8xe6x98x8exe8xbfx99xe4xb8xa4xe7xa7x8dxe4xb8x8dxe5x90x8cxe7x9ax84xe6xa8xa1xe5x9ex8b xe9x94x99xe8xafxafxe5x88x86xe5xb8x83xe5xb9xb6xe4xb8x8dxe4xb8x80xe6xa0xb7,xe8x80x8cxe4xb8x94xe5x9cxa8xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe4xb8xad,xe5xb0x86xe5x9fxbaxe4xbax8exe5x9bxbexe7x9ax84xe6xa8xa1xe5x9ex8bxe5x92x8cxe5x9fxbaxe4xbax8exe8xbdxacxe7xa7xbbxe7x9ax84xe6xa8xa1xe5x9ex8bxe4xbdxbfxe7x94xa8xe5xa0x86xe6x96xb9xe6xb3x95xe8x9ex8d xe5x90x88xe4xb9x8bxe5x90x8e,xe8x83xbdxe5xa4x9fxe6x98xbexe8x91x97xe7x9ax84xe6x8fx90xe5x8dx87xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe7x9ax84xe6x80xa7xe8x83xbd,xe8xbfx99xe4xbax9bxe4xbfx83xe4xbdxbfxe6x88x91xe4xbbxacxe8xbfx9bxe4xb8x80xe6xadxa5xe7xa0x94xe7xa9xb6xe9x87x87xe7x94xa8xe5xa0x86xe6x96xb9xe6xb3x95xe5x8exbbxe8x9ex8dxe5x90x88xe5x9fxba xe4xbax8exe5x9bxbexe7x9ax84xe5x92x8cxe5x9fxbaxe4xbax8exe8xbdxacxe7xa7xbbxe7x9ax84xe8xafx8dxe6x80xa7xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8bxe3x80x82xe6x88x91xe4xbbxacxe5x9cxa8xe4xb8xadxe6x96x87xe5xaexbexe5xb7x9exe6xa0x91xe5xbax935.1xe7x89x88xe6x9cxac(CTB5.1)xe4xb8x8a xe8xbfx9bxe8xa1x8cxe8xafx95xe9xaax8c,xe5xaex9exe9xaax8cxe7xbbx93xe6x9ex9cxe8xa1xa8xe6x98x8e,xe7x9bxb8xe6xafx94xe4xbdxbfxe7x94xa8xe5x9fxbaxe4xbax8exe5x9bxbexe7x9ax84xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8bxe4xb8xbaxe8xa2xabxe6x8cx87xe5xafxbcxe6xa8xa1xe5x9ex8b,xe9x87x87xe7x94xa8xe8xbdxacxe7xa7xbbxe7x9ax84xe8x81x94xe5x90x88xe6xa8xa1 xe5x9ex8bxe4xb8xbaxe8xa2xabxe6x8cx87xe5xafxbcxe6xa8xa1xe5x9ex8bxe8x83xbdxe5x8fx96xe5xbex97xe8xbex83xe5xa5xbdxe7x9ax84xe6x80xa7xe8x83xbdxe3x80x82xe6x9bxb4xe8xbfx9bxe4xb8x80xe6xadxa5,xe6x88x91xe4xbbxacxe4xbbx8bxe7xbbx8dxe4xbax86xe5x9fxbaxe4xbax8exe7x9fxadxe8xafxadxe5x8fxa5xe6xb3x95xe7xbbx93xe6x9ex84xe7x9ax84xe8x81x94xe5x90x88xe6xa8xa1 xe5x9ex8b,xe5xaex83xe4xbbx8exe4xb8x80xe4xb8xaaxe5x8fxa5xe5xadx90xe7x9ax84xe6xa6x82xe7x8ex87xe7x9fxadxe8xafxadxe6x96x87xe6xb3x95xe5x88x86xe6x9ex90xe5x99xa8xe8xbex93xe5x87xbaxe7xbbx93xe6x9ex9cxe4xb8xadxe6x8fx90xe5x8fx96xe5x8fxa5xe5xadx90xe7x9ax84xe8xafx8dxe6x80xa7xe5xbax8fxe5x88x97xe4xbbxa5xe5x8fx8axe4xbex9dxe5xadx98xe6xa0x91xe7xbbx93 xe6x9ex9c,xe7x84xb6xe5x90x8exe6x88x91xe4xbbxacxe9x87x87xe7x94xa8xe5x9fxbaxe4xbax8exe7x9fxadxe8xafxadxe5x8fxa5xe6xb3x95xe7xbbx93xe6x9ex84xe7x9ax84xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8bxe6x9bxb4xe8xbfx9bxe4xb8x80xe6xadxa5xe6x8cx87xe5xafxbcxe5x9fxbaxe4xbax8exe8xbdxacxe7xa7xbbxe7x9ax84xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8b,xe6x9cx80xe7xbbx88 xe6x88x91xe4xbbxacxe5x9cxa8CTB5.1xe7x9ax84xe6x95xb0xe6x8dxaexe4xb8x8axe5x8fx96xe5xbex97xe4xbax86xe6x9cx80xe5xa5xbdxe7xbbx93xe6x9ex9c,xe8xafx8dxe6x80xa7xe6xa0x87xe6xb3xa8xe5x87x86xe7xa1xaexe7x8ex87xe8xbexbexe5x88xb094.95%,xe5x90x8cxe6x97xb6,xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95 xe5x87x86xe7xa1xaexe7x8ex87xe8xbexbexe5x88xb083.98%xe3x80x82"
