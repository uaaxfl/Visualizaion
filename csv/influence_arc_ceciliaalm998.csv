2021.acl-long.312,2020.coling-demos.2,1,0.887493,"eric and could be applied to any learning problem with substantial class imbalances. 1 Introduction Predictive maintenance techniques are applied to engineering systems to estimate when maintenance should be performed to reduce costs and improve operational efficiency (Carvalho et al., 2019), as well as mitigate risk and increase safety. Maintenance records are an important source of information for predictive maintenance (McArthur et al., 2018). These records are often stored in the form of technical logbooks in which each entry contains fields that identify and describe a maintenance issue (Akhbardeh et al., 2020a). Being able to classify these technical events is an important step in the development of predictive maintenance systems. In most technical logbooks, issues are manually Original Entry Pre-processed Entry fwd eng baff seeal needs resecured. r/h eng #3 intake gsk leaking. bird struck on p/w at twy. bird rmvd. location rptd as nm from rwy aprch end. forward engine baffle seal needs resecured. right engine number 3 intake gasket leaking. bird struck on pilot window at taxiway. bird removed location reported as new mexico from runway approach end. Table 1: Original and text-normalized example d"
2021.acl-long.312,2020.aacl-demo.5,1,0.8964,"eric and could be applied to any learning problem with substantial class imbalances. 1 Introduction Predictive maintenance techniques are applied to engineering systems to estimate when maintenance should be performed to reduce costs and improve operational efficiency (Carvalho et al., 2019), as well as mitigate risk and increase safety. Maintenance records are an important source of information for predictive maintenance (McArthur et al., 2018). These records are often stored in the form of technical logbooks in which each entry contains fields that identify and describe a maintenance issue (Akhbardeh et al., 2020a). Being able to classify these technical events is an important step in the development of predictive maintenance systems. In most technical logbooks, issues are manually Original Entry Pre-processed Entry fwd eng baff seeal needs resecured. r/h eng #3 intake gsk leaking. bird struck on p/w at twy. bird rmvd. location rptd as nm from rwy aprch end. forward engine baffle seal needs resecured. right engine number 3 intake gasket leaking. bird struck on pilot window at taxiway. bird removed location reported as new mexico from runway approach end. Table 1: Original and text-normalized example d"
2021.acl-long.312,D19-1371,0,0.0192359,"nce on average was FaciMain, which is the dataset which the LSTM model had the closest performance to the CNN and BERT models, and was also the only one which the LSTM model outperformed the DNN model. The pre-trained BERT model provided a reasonable classification performance compared to the other deep learning models, however as BERT is pre-trained on standard language, the performance when applying to logbook data was not optimal. Training or fine-tunning BERT to technical logbook data is likely to improve performance as observed in the legal and scientific domains (Chalkidis et al., 2020; Beltagy et al., 2019). As training or finetuning BERT requires large amounts of data, a limitation for fine-tuning a domain-specific BERT is the amount of logbook data available. 7 Conclusion and Future Work This work focused on predictive maintenance and technical event/issue classification, with a special focus on addressing class imbalance. We acquired seven logbook datasets from three technical domains containing short instances with non-standard grammar and spelling, and many abbreviations. To address RQ1, we evaluated multiple strategies to address the extreme class imbalance in these datasets and we showed"
2021.acl-long.312,N15-1075,0,0.0121103,"otive maintenance (3), automotive safety (4), and facility maintenance (5). Each instance shows how domain-specific terminology, abbreviations (Abbr.), and misspelled words (in bold font) are used by the domain expert, and also illustrates some of the event types covered. More details are provided in Section 3. notable performance by using the n-gram features with the Maximum Entropy (MaxEnt) classifier. There is also relevant research on event classification in social media. For example, Ritter et al. (2012) proposed an open-source event extraction and supervised tagger for noisy microblogs. Cherry and Guo (2015) applied word embedding-based modeling for information extraction on news-wire and tweets, comparing named entity taggers to improve their method. Hammar et al. (2018) performed experimental work on Instagram text using weakly supervised text classification to extracted clothing brand based on user descriptions in posts. The problem of class imbalance has been studied in recent years for numerous natural language processing tasks. Tayyar Madabushi et al. (2019) studied automatic propaganda event detection from a news dataset using a pre-trained BERT model. They recognized that the BERT model h"
2021.acl-long.312,W19-6207,0,0.0879888,"Missing"
2021.acl-long.312,E17-2110,0,0.0159587,"ecall, and F1 score by utilizing a macro-average over all classes, as this gives every class equal weight, and hence reveals how well the models and training data selection strategies perform. 4.2 Model Architecture and Training Different machine learning methods were considered for technical event/issue classification (e.g. engine failure, turbine failure). Each instance is an individual short logbook entry and contains approximately 2 to 20 tokens (12 words on average per instance including function words), as shown in Table 3.The methods used in this study were a Deep Neural Network (DNN) (Dernoncourt et al., 2017), a Long Short-Term Memory (LSTM) (Suzgun et al., 2019), recurrent neural network (RNN) (Pascanu et al., 2013), a Convolutional Neural Network (CNN) (Lin et al., 2018), and BERT (Devlin et al., 2019). Deep Neural Network A deep artificial neural network (DNN), as described by Dernoncourt et al. (2017), can learn abstract representation and features of the input instances that would help to achieve better performance on predicting the issue type in the logbook dataset. The DNN used was a 3 layer, fully connected feed forward neural network with an input embedding layer of dimension 300 and equa"
2021.acl-long.312,N19-1423,0,0.069764,"rchitecture and Training Different machine learning methods were considered for technical event/issue classification (e.g. engine failure, turbine failure). Each instance is an individual short logbook entry and contains approximately 2 to 20 tokens (12 words on average per instance including function words), as shown in Table 3.The methods used in this study were a Deep Neural Network (DNN) (Dernoncourt et al., 2017), a Long Short-Term Memory (LSTM) (Suzgun et al., 2019), recurrent neural network (RNN) (Pascanu et al., 2013), a Convolutional Neural Network (CNN) (Lin et al., 2018), and BERT (Devlin et al., 2019). Deep Neural Network A deep artificial neural network (DNN), as described by Dernoncourt et al. (2017), can learn abstract representation and features of the input instances that would help to achieve better performance on predicting the issue type in the logbook dataset. The DNN used was a 3 layer, fully connected feed forward neural network with an input embedding layer of dimension 300 and equal size number of words followed by 2 dense layers with 512 hidden units with ReLU activation functions followed by a dropout layer. Finally, we added a fully connected dense layer with size equal to"
2021.acl-long.312,N18-2108,0,0.0563211,"Missing"
2021.acl-long.312,W14-4320,0,0.0314244,"y supervised text classification to extracted clothing brand based on user descriptions in posts. The problem of class imbalance has been studied in recent years for numerous natural language processing tasks. Tayyar Madabushi et al. (2019) studied automatic propaganda event detection from a news dataset using a pre-trained BERT model. They recognized that the BERT model had issues in generalizing. To overcome this issue, they proposed a cost-weighting method. Al-Azani and ElAlfy (2017) analyzed polarity measurement in imbalanced tweet datasets utilizing features learned with word embeddings. Li and Nenkova (2014) studied the class imbalance problem in the task of discourse relation identification by comparing the accuracy of multiple classifiers. They showed that utilizing a unified method and further downsampling the negative instances can significantly enhance the performance of the prediction model on unbalanced binary and multi-classes. Dealing with unbalance classes is also studied well in the sentiment classification task. Li et al. (2012) introduced an active learning method that overcomes the problem of data class unbalance by choosing the significant sample of minority class for manual annota"
2021.acl-long.312,D12-1013,0,0.0385237,"-weighting method. Al-Azani and ElAlfy (2017) analyzed polarity measurement in imbalanced tweet datasets utilizing features learned with word embeddings. Li and Nenkova (2014) studied the class imbalance problem in the task of discourse relation identification by comparing the accuracy of multiple classifiers. They showed that utilizing a unified method and further downsampling the negative instances can significantly enhance the performance of the prediction model on unbalanced binary and multi-classes. Dealing with unbalance classes is also studied well in the sentiment classification task. Li et al. (2012) introduced an active learning method that overcomes the problem of data class unbalance by choosing the significant sample of minority class for manual annotation and majority class for automatic annotation to lower the amount of human annotation required. Furthermore, Damaschk et al. (2019) examined techniques to overcome the problem of dealing with high-class imbalance in classifying a collection of song lyrics. They employed neural network models including a multi-layer perceptron and a Doc2Vec model in their experiments where the finding was that undersampling the majority class can be a"
2021.acl-long.312,2020.acl-main.45,0,0.029857,"the significant sample of minority class for manual annotation and majority class for automatic annotation to lower the amount of human annotation required. Furthermore, Damaschk et al. (2019) examined techniques to overcome the problem of dealing with high-class imbalance in classifying a collection of song lyrics. They employed neural network models including a multi-layer perceptron and a Doc2Vec model in their experiments where the finding was that undersampling the majority class can be a reasonable approach to remove the data sparsity and further improve the classification performance. Li et al. (2020) also explored the problem of high data imbalance using cross-entropy criteria as well as standard performance metrics. They proposed a loss function called Dice loss that assigns equal importance to the false negatives and the false positives. In computer vision, Bowley et al. (2019) developed an automated feedback loop method to identify and classify wildlife species from Unmanned Aerial Systems imagery, for training CNNs to overcome the unbalanced class issue. On their expert imagery dataset, the error rate decreased substantially from 0.88 to 0.05. This work adapts this feedback loop strat"
2021.acl-long.312,D18-1485,0,0.157845,"rategies perform. 4.2 Model Architecture and Training Different machine learning methods were considered for technical event/issue classification (e.g. engine failure, turbine failure). Each instance is an individual short logbook entry and contains approximately 2 to 20 tokens (12 words on average per instance including function words), as shown in Table 3.The methods used in this study were a Deep Neural Network (DNN) (Dernoncourt et al., 2017), a Long Short-Term Memory (LSTM) (Suzgun et al., 2019), recurrent neural network (RNN) (Pascanu et al., 2013), a Convolutional Neural Network (CNN) (Lin et al., 2018), and BERT (Devlin et al., 2019). Deep Neural Network A deep artificial neural network (DNN), as described by Dernoncourt et al. (2017), can learn abstract representation and features of the input instances that would help to achieve better performance on predicting the issue type in the logbook dataset. The DNN used was a 3 layer, fully connected feed forward neural network with an input embedding layer of dimension 300 and equal size number of words followed by 2 dense layers with 512 hidden units with ReLU activation functions followed by a dropout layer. Finally, we added a fully connected"
2021.acl-long.312,D19-6109,0,0.0574907,"Missing"
2021.acl-long.312,maragoudakis-etal-2006-dealing,0,0.0237727,"., 2019) to apply a large class data from one technical domain to another. For example, instances that describe an engine failure in the aviation domain are distinct from engine failure instances reported in the automotive domain. In this paper we apply five different methods for selecting training data for the models to analyze their effects on classification performance: (1) under(down)and (2) over-sampling, (3) random down-sampling, (4) a feedback loop strategy, and (5) a baseline strategy which simply uses all available data. Re-sampling Under- and over-sampling are resampling techniques (Maragoudakis et al., 2006) that were used to create balanced class sizes for model training. For over-sampling, instances of the minority classes are randomly copied so that all classes would have the same number of instances as the largest class. For under-sampling, observations are randomly removed from the majority classes, so that all classes have the same number of instances as the smallest class. For both approaches, we first divided our datasets into test and Feedback Loop To address class imbalances in text classification, this work adapts the approach in Bowley et al. (2019) from the computer vision domain. Th"
2021.acl-long.312,W11-0143,0,0.0224652,"r. 7) Faci-Main contains six years of logbook reports collected for building maintenance. These technical logbooks include short, compact, and descriptive domain-specific English texts single instances usually contain between 2 and 20 tokens on average including abbreviations and domain-specific words. An example instance from Table 2, r/h fwd upper baff seal needs to be resecured, shows how the instances for a specific issue class are comprised from specific vocabulary (less ambiguity), and therefore contain a high level of granularity (level of description for an event from multiple words) (Mulkar-Mehta et al., 2011). Table 3 presents statistics for each dataset, in terms of the number of instances, average instance length, number of classes, and the minimum, average, median and maximum class size to represent how imbalanced the datasets are. An instance in the logbook can be formed as a complete description of the technical event (such as a safety or maintenance inspection) like: #2 & #4 cyl rocker cover gsk are leaking, or it might contain an incomplete description by solely referring to the damaged part/section of machinery (hyd cap chck eng light on) using few domain words. In either form of the probl"
2021.acl-long.312,U09-1014,0,0.0116095,"unstructured electronic health records provided by the U.K. National Health Service. They evaluated a deep artificial neural network model on the expertannotated textual dataset of a safety incident to identify similar events that occurred. Del´eger et al. (2010) proposed a method to deal with unstructured clinical records, using rule-based techniques to extract names of medicines and related information such as prescribed dosage. Savova et al. (2010) considered free-text electronic medical records for information extraction purposes and developed a system to obtain clinical domain knowledge. Patrick and Li (2009) proposed the cascade methods of extracting the medication records such as treatment duration or reason, obtained from patient’s historical records. Their approach for event extraction includes text normalization, tokenization, and context identification. A system using multiple features outperformed a baseline method using a bag of words model. Yetisgen-Yildiz et al. (2013) proposed the lung disease phenotypes identification method to prevent the use of a handoperated identification strategy. They employed NLP pipelines including text pre-processing and further text classification on the text"
2021.acl-long.312,D14-1162,0,0.0878609,"Missing"
2021.acl-long.312,D18-1210,0,0.0118425,"ing the number of hidden units equal to the embedding dimension, followed by a dropout layer. Finally, we added a fully connected layer with size equal to the number of classes, with a SoftMax activation function. Convolutional Neural Network Convolutional neural networks (CNNs) have demonstrated exceptional success in NLP tasks such as document classification, language modeling, or machine translation (Lin et al., 2018). As Xu et al. (2020) described, CNN models can produce consistent performance when applied to the various text types such as short sequences. We evaluated a CNN architecture (Shen et al., 2018) with a convolutional layer, followed by batch normalization, ReLU, and a dropout layer, which was followed by a maxpooling layer. The model contained 300 convolutional filters with the size of 1 by n-gram length pooling with the size of 1 by the length of the input sequence, followed by concatenation layer, then finally connected to a fully connected dense layer, and an output layer equal to the size of the dataset class using a SoftMax activation function. Bidirectional Encoder Representations We also evaluated using the pre-trained uncased Bidirectional Encoder Representations (BERT) for En"
2021.acl-long.312,W19-0128,0,0.0291754,"Missing"
2021.acl-long.312,D19-5018,0,0.0228967,"in social media. For example, Ritter et al. (2012) proposed an open-source event extraction and supervised tagger for noisy microblogs. Cherry and Guo (2015) applied word embedding-based modeling for information extraction on news-wire and tweets, comparing named entity taggers to improve their method. Hammar et al. (2018) performed experimental work on Instagram text using weakly supervised text classification to extracted clothing brand based on user descriptions in posts. The problem of class imbalance has been studied in recent years for numerous natural language processing tasks. Tayyar Madabushi et al. (2019) studied automatic propaganda event detection from a news dataset using a pre-trained BERT model. They recognized that the BERT model had issues in generalizing. To overcome this issue, they proposed a cost-weighting method. Al-Azani and ElAlfy (2017) analyzed polarity measurement in imbalanced tweet datasets utilizing features learned with word embeddings. Li and Nenkova (2014) studied the class imbalance problem in the task of discourse relation identification by comparing the accuracy of multiple classifiers. They showed that utilizing a unified method and further downsampling the negative"
2021.acl-long.312,W13-1902,0,0.0235844,"s of medicines and related information such as prescribed dosage. Savova et al. (2010) considered free-text electronic medical records for information extraction purposes and developed a system to obtain clinical domain knowledge. Patrick and Li (2009) proposed the cascade methods of extracting the medication records such as treatment duration or reason, obtained from patient’s historical records. Their approach for event extraction includes text normalization, tokenization, and context identification. A system using multiple features outperformed a baseline method using a bag of words model. Yetisgen-Yildiz et al. (2013) proposed the lung disease phenotypes identification method to prevent the use of a handoperated identification strategy. They employed NLP pipelines including text pre-processing and further text classification on the textual reports to identify the patients with a positive diagnosis for the disease. Based on the outcome, they achieve 4035 Tech. Event or Issue Label Example Instance of Technical Logbook Entry SUBSTANTIAL DAMAGE BAFFLE DAMAGE MINOR DAMAGE UNKNOWN PM SERVICE DRIVING ISSUE STOP SIGN RUNNING BUILDING PM ENG NEED REPAIR PREVENTIVE MAINT Abbr., Misspelling, Terminology (1) AFT ON T"
2021.findings-emnlp.267,N19-1423,0,0.00793748,"ern and pointed to datasets that probe gender bias (Sun et al., 2019). There is a pressing need to develop metrics, evaluation processes, and datasets able to quantitatively assess ableist biases in NLP systems. As a ﬁrst step, we critically assess how ableist biases manifest in NLP models and examine intersections of bias. 3 Methods We build on the work of Hutchinson et al. (2020) which used a ﬁll-in-the-blank analysis–originally proposed by Kurita et al. (2019)–to study ableist bias in pre-trained BERT representations. We used BERT large model (uncased), a pretrained English language model (Devlin et al., 2019). We adjusted their analysis method to examine ableist bias together with gender and racial bias. Our analysis method involves creating template sentence fragments of the form The [blank1] [blank2] [blank3] person [connecting verb] <predicted using BERT>. The slots (blank1, blank2, blank3) were ﬁlled in based on three lists with referents related to disability, gender, and race. The disability list was provided by Hutchinson et al. (2020).1 The list for race included the ﬁve categories in the U.S. census (Census, 2021)2 , and the list for gender was based on guidelines for gender inclusiveness"
2021.findings-emnlp.267,2020.acl-main.487,0,0.0667884,"esigned to reveal bias effects. Researchers have explored the utility of performance metrics for capturing differences due to bias and proposed new metrics (Dixon et al., 2018; Park et al., 2018). A recent systematic review raised this concern and pointed to datasets that probe gender bias (Sun et al., 2019). There is a pressing need to develop metrics, evaluation processes, and datasets able to quantitatively assess ableist biases in NLP systems. As a ﬁrst step, we critically assess how ableist biases manifest in NLP models and examine intersections of bias. 3 Methods We build on the work of Hutchinson et al. (2020) which used a ﬁll-in-the-blank analysis–originally proposed by Kurita et al. (2019)–to study ableist bias in pre-trained BERT representations. We used BERT large model (uncased), a pretrained English language model (Devlin et al., 2019). We adjusted their analysis method to examine ableist bias together with gender and racial bias. Our analysis method involves creating template sentence fragments of the form The [blank1] [blank2] [blank3] person [connecting verb] <predicted using BERT>. The slots (blank1, blank2, blank3) were ﬁlled in based on three lists with referents related to disability,"
2021.findings-emnlp.267,2020.gebnlp-1.2,0,0.0618458,"Missing"
2021.findings-emnlp.267,W19-3823,0,0.0883024,"trics for capturing differences due to bias and proposed new metrics (Dixon et al., 2018; Park et al., 2018). A recent systematic review raised this concern and pointed to datasets that probe gender bias (Sun et al., 2019). There is a pressing need to develop metrics, evaluation processes, and datasets able to quantitatively assess ableist biases in NLP systems. As a ﬁrst step, we critically assess how ableist biases manifest in NLP models and examine intersections of bias. 3 Methods We build on the work of Hutchinson et al. (2020) which used a ﬁll-in-the-blank analysis–originally proposed by Kurita et al. (2019)–to study ableist bias in pre-trained BERT representations. We used BERT large model (uncased), a pretrained English language model (Devlin et al., 2019). We adjusted their analysis method to examine ableist bias together with gender and racial bias. Our analysis method involves creating template sentence fragments of the form The [blank1] [blank2] [blank3] person [connecting verb] <predicted using BERT>. The slots (blank1, blank2, blank3) were ﬁlled in based on three lists with referents related to disability, gender, and race. The disability list was provided by Hutchinson et al. (2020).1 Th"
2021.findings-emnlp.267,2020.coling-main.151,0,0.0212863,"sed an averaged sentiment score. Future research can consider using other approaches to examine bias as well. Finally, future work can modify or improve different state-of-the-art debiasing approaches to remove intersectional ableist bias in NLP systems. Our ﬁndings also speak to the prior research on analyzing intersectional biases in NLP systems. Intersectionality theory posits that various categories of identities overlay on top of each other to create distinct modalities of discrimination that no single category shares. Prior work had examined this in the context of race and gender, e.g., Lepori (2020) examined bias against black women who are represented in word embeddings as less feminine than white women. To the best of our knowledge our paper was also the ﬁrst to conduct an analysis of intersectional ableist bias using different verbs. The complements likely to follow actions verbs like those in our study, e.g. innovates, leads, or supervises, may depend upon inadvertently learned stereotypes about the subject of each verb. Our analysis of these predictions helps to reveal such bias and how it may manifest in social contexts. 3120 References US Census. 2021. About Race. Richard A Armstr"
2021.findings-emnlp.267,N19-1062,0,0.0278141,"tance to social change, which can have implications for computational systems. Analyzing subspaces in text representations like word embeddings can reveal insights about NLP systems that use them (May et al., 2019; Chaloner and Maldonado, 2019). For example, Bolukbasi et al. (2016) developed a support vector machine to identify gender subspace in word embeddings and then identiﬁed gender directions by making “gender-pairs (man-woman, his-her, shehe)”. They identiﬁed eigenvectors that capture prominent variance in the data. This work has been extended to include non-binary gender distinctions (Manzini et al., 2019). Researchers have also explored contextualized word embeddings bias at the intersection of race and gender. Guo and Caliskan (2021) proposed methods for automatically identifying intersectional bias in static word embeddings. But debiasing has limitations. For example, Gonen and Goldberg (2019) pointed out that even after attempting to reduce the projection of words on a gender direction, biased/stereotypical words in the neighbors of a given word embedding remain (Gonen and Goldberg, 2019). NLP systems when used by someone from a protected group or when the input data mentions a protected gr"
2021.findings-emnlp.267,N19-1063,0,0.0203268,"reveal inﬂuential subconscious associations or implicit beliefs about people of a protected group and their stereotypical roles in societies. Some work has studied correlations between data on gender and professions and the strengths of these conceptual linkages in word embeddings (Caliskan et al., 2017; Garg et al., 2018). Findings suggest that word embeddings encode normative assumptions, or resistance to social change, which can have implications for computational systems. Analyzing subspaces in text representations like word embeddings can reveal insights about NLP systems that use them (May et al., 2019; Chaloner and Maldonado, 2019). For example, Bolukbasi et al. (2016) developed a support vector machine to identify gender subspace in word embeddings and then identiﬁed gender directions by making “gender-pairs (man-woman, his-her, shehe)”. They identiﬁed eigenvectors that capture prominent variance in the data. This work has been extended to include non-binary gender distinctions (Manzini et al., 2019). Researchers have also explored contextualized word embeddings bias at the intersection of race and gender. Guo and Caliskan (2021) proposed methods for automatically identifying intersection"
2021.findings-emnlp.267,S18-2005,0,0.0285484,"on a gender direction, biased/stereotypical words in the neighbors of a given word embedding remain (Gonen and Goldberg, 2019). NLP systems when used by someone from a protected group or when the input data mentions a protected group. Unfortunately, state-of-the-art systems pass on bias to other tasks. For example, a recent study found that BERT can perpetuate gender bias in contextualized word embeddings (Costajussà et al., 2020). Some work has explored the effect on performance measures in NLP systems after replacing (swapping) majority-minority lexicons (Zhao et al., 2018; Lu et al., 2020; Kiritchenko and Mohammad, 2018). Additionally, standard evaluation metrics usually fail to take bias into account, nor are datasets carefully designed to reveal bias effects. Researchers have explored the utility of performance metrics for capturing differences due to bias and proposed new metrics (Dixon et al., 2018; Park et al., 2018). A recent systematic review raised this concern and pointed to datasets that probe gender bias (Sun et al., 2019). There is a pressing need to develop metrics, evaluation processes, and datasets able to quantitatively assess ableist biases in NLP systems. As a ﬁrst step, we critically assess"
2021.findings-emnlp.267,D18-1302,0,0.0191274,"ecent study found that BERT can perpetuate gender bias in contextualized word embeddings (Costajussà et al., 2020). Some work has explored the effect on performance measures in NLP systems after replacing (swapping) majority-minority lexicons (Zhao et al., 2018; Lu et al., 2020; Kiritchenko and Mohammad, 2018). Additionally, standard evaluation metrics usually fail to take bias into account, nor are datasets carefully designed to reveal bias effects. Researchers have explored the utility of performance metrics for capturing differences due to bias and proposed new metrics (Dixon et al., 2018; Park et al., 2018). A recent systematic review raised this concern and pointed to datasets that probe gender bias (Sun et al., 2019). There is a pressing need to develop metrics, evaluation processes, and datasets able to quantitatively assess ableist biases in NLP systems. As a ﬁrst step, we critically assess how ableist biases manifest in NLP models and examine intersections of bias. 3 Methods We build on the work of Hutchinson et al. (2020) which used a ﬁll-in-the-blank analysis–originally proposed by Kurita et al. (2019)–to study ableist bias in pre-trained BERT representations. We used BERT large model (un"
2021.findings-emnlp.267,W17-1101,0,0.0161138,"could be triggered by tive on average than control sentences in set A. Set someone commenting neutrally about topics related 3119 Figure 1: Averaged sentiment for selected connecting verbs develops, feels, supervises and has. For control set A, verbs have near-neutral sentiment, aside from has (negative) and develops (positive). In contrast, set B (disability) and sets C and D (disability and gender or race) are negative. Per-verb differences include, e.g., supervises is most negative for set B, has most negative for set D, and feels slightly more negative for set C than set D. to disability (Schmidt and Wiegand, 2017). Automatic content ﬁltering software for websites may wrongly determine that keywords related to disability topics should be a basis for ﬁltering, thereby restricting access to information about disability topics (Fortuna and Nunes, 2018). Further, ableist biases can have an impact on the accuracy of automatic speech recognition when people discuss disabilities if language models are used. It could also impact text simpliﬁcation that is NLP-driven. These results could also be important if NLP models are used for computational social science applications. 5 Conclusion and Future Work Our ﬁndin"
2021.findings-emnlp.267,P19-1159,0,0.0116741,"Some work has explored the effect on performance measures in NLP systems after replacing (swapping) majority-minority lexicons (Zhao et al., 2018; Lu et al., 2020; Kiritchenko and Mohammad, 2018). Additionally, standard evaluation metrics usually fail to take bias into account, nor are datasets carefully designed to reveal bias effects. Researchers have explored the utility of performance metrics for capturing differences due to bias and proposed new metrics (Dixon et al., 2018; Park et al., 2018). A recent systematic review raised this concern and pointed to datasets that probe gender bias (Sun et al., 2019). There is a pressing need to develop metrics, evaluation processes, and datasets able to quantitatively assess ableist biases in NLP systems. As a ﬁrst step, we critically assess how ableist biases manifest in NLP models and examine intersections of bias. 3 Methods We build on the work of Hutchinson et al. (2020) which used a ﬁll-in-the-blank analysis–originally proposed by Kurita et al. (2019)–to study ableist bias in pre-trained BERT representations. We used BERT large model (uncased), a pretrained English language model (Devlin et al., 2019). We adjusted their analysis method to examine ab"
2021.findings-emnlp.267,D18-1521,0,0.025547,"g to reduce the projection of words on a gender direction, biased/stereotypical words in the neighbors of a given word embedding remain (Gonen and Goldberg, 2019). NLP systems when used by someone from a protected group or when the input data mentions a protected group. Unfortunately, state-of-the-art systems pass on bias to other tasks. For example, a recent study found that BERT can perpetuate gender bias in contextualized word embeddings (Costajussà et al., 2020). Some work has explored the effect on performance measures in NLP systems after replacing (swapping) majority-minority lexicons (Zhao et al., 2018; Lu et al., 2020; Kiritchenko and Mohammad, 2018). Additionally, standard evaluation metrics usually fail to take bias into account, nor are datasets carefully designed to reveal bias effects. Researchers have explored the utility of performance metrics for capturing differences due to bias and proposed new metrics (Dixon et al., 2018; Park et al., 2018). A recent systematic review raised this concern and pointed to datasets that probe gender bias (Sun et al., 2019). There is a pressing need to develop metrics, evaluation processes, and datasets able to quantitatively assess ableist biases in"
C14-1162,W13-2109,0,0.0281837,"or contradictions of those expectations can affect their self-reported confidence levels. 1719 Most relevant literature focuses on linguistic features. Language, as the primary form of human expression, is certainly critical. However, analyzing meaning may require going beyond linguistic inference, depending on the context or application. Previous studies have successfully incorporated multiple expressive modalities when examining linguistic and cognitive processes, such as facial expressions for video sentiment analysis (P´erez-Rosas et al., 2013) and pointing gestures for referring actions (Gatt and Paggio, 2013). In such studies, the additional modalities were carefully chosen based on the nature of the performed tasks. Here, we deal with experts (dermatologists) inspecting images (skin conditions) for diagnostic purposes, a task that heavily involves their use of visual perceptual expertise, in addition to conceptual domain knowledge. For this reason, we incorporate features of their eye movements in our study. There is evidence for ties between perceptual expertise and eye movements during image inspection tasks (Li et al., 2012b), and we explore if such ties may also relate to a physician’s confid"
C14-1162,W14-3412,1,0.819655,"dence values, and used it to model physicians’ confidence in diagnosis, as well as their diagnostic self-awareness. The Confidence Only problem involves the expression of confidence based on clinicians’ belief, but it is important to understand the relationship to clinicians’ actual diagnostic performance. This distinction is key because, while predicting confidence alone is a stepping stone, self-awareness is the ability to additionally align one’s confidence with unknown correctness, which involves human intuitive and analytical reasoning (another topic of interest to the medical field, see Hochberg et al. (2014)). Case studies of the most and least confident physicians revealed a complex relationship between confidence and correctness, and highlighted the need for exploring clinical self-awareness. We also defined a personalized binning scheme for physician confidence levels, taking into account a physician’s past confidence when drawing the line between high and low confidence, and compared this to a generalized binning scheme based on performance of all physicians. In tandem, these approaches to confidence binning could be used by an intelligent diagnostic support system. We incorporated previously"
C14-1162,W12-3803,1,0.736214,"Scherer et al., 1973; Pon-Barry and Shieber, 2011; Kimble and Seidel, 1991), as well as other characteristics of spoken language, such as speech disfluencies (Womack et al., 2012) and hedges (Smith and Clark, 1993). Prosodic features have been identified and successfully used in intelligent tutoring systems (Liscombe et al., 2005), where a student’s confidence (or lack thereof) can play a key role in effective system response. In medical diagnosis, prosodic and lexical features have been useful indicators of physicians’ confidence and diagnostic correctness, individually (Womack et al., 2013; McCoy et al., 2012). Other potentially useful information may be evident in speech as well. In a study by Womack et al. (2012) on a similar dataset, the authors found a relationship between speech characteristics and physician experience: attending (experienced) physicians used more filled pauses and spoke more than resident (in-training) physicians. Additionally, verbal features may expose differences in diagnostic reasoning that may be useful predictors of confidence. Rogers (1996) analyzed a dataset of spoken chest X-ray examinations by radiologists, remarking that reasoning styles influence physicians’ expec"
C14-1162,P13-1096,0,0.0673397,"Missing"
C14-1162,W12-3801,1,0.856607,"clude flawed perception, biased heuristics, and settling on a final diagnosis too early (Graber et al., 2002), all of which can be caused by overconfidence (Berner and Graber, 2008; Croskerry, 2008). Underconfidence may also be a problem if it prevents a physician from pursuing a correct diagnosis (Friedman et al., 2005). There is evidence for links between speech and confidence in terms of prosodic features, such as pitch and loudness (Scherer et al., 1973; Pon-Barry and Shieber, 2011; Kimble and Seidel, 1991), as well as other characteristics of spoken language, such as speech disfluencies (Womack et al., 2012) and hedges (Smith and Clark, 1993). Prosodic features have been identified and successfully used in intelligent tutoring systems (Liscombe et al., 2005), where a student’s confidence (or lack thereof) can play a key role in effective system response. In medical diagnosis, prosodic and lexical features have been useful indicators of physicians’ confidence and diagnostic correctness, individually (Womack et al., 2013; McCoy et al., 2012). Other potentially useful information may be evident in speech as well. In a study by Womack et al. (2012) on a similar dataset, the authors found a relationsh"
C16-1083,J90-1003,0,0.421321,"Missing"
C16-1083,W10-0204,0,0.0767897,"Missing"
C16-1083,W02-1011,0,0.0239785,"Missing"
C16-1083,P02-1053,0,0.0723437,"Missing"
C16-1083,H05-1044,0,0.0432371,"Missing"
D15-1309,clarke-etal-2012-nlp,0,0.0275138,"Missing"
D15-1309,W14-3213,1,0.519013,"advice, and provide support. This makes its data ideal for sensitive subjects not typically discussed in social media. This work makes two contributions: classifiers for identifying texts discussing domestic abuse and an analysis of discussions of domestic abuse in several subreddits. 2 Related Work Social media sites are an emerging source of data for public health studies, such as mental health, bullying, and disease tracking. These sites provide less intimidating and more accessible channels for reporting, collectively processing, and making sense of traumatic and stigmatizing experiences (Homan et al., 2014; Walther, 1996). Many researchers have focused on Twitter data, due to its prominent presence, accessibility, and the characteristics of tweets. For instance, De Choudhury et al. (2013) predicted the onset of depression from user tweets, while other studies have modeled distress (Homan et al., 2014; Lehrman et al., 2012). Most relevantly, Schrading et al. (2015) used the #WhyIStayed trend to predict whether a tweet was about staying in an abusive relationship or leaving, analyzing the lexical structures victims of abuse give for staying or leaving. Reddit has been studied less in this area, w"
D15-1309,W12-2102,1,0.842348,"Missing"
D15-1309,P14-2050,0,0.00849625,"Missing"
D15-1309,J05-1004,0,0.0240361,"d (11) appear. Also included are unique empathetic and helping discourse from comments, including let know (121), and feel free pm5 (27). This indicates that comment data could improve classification results, as support and empathy may be more prevalent in the abuse set than in the control set. 3.4 Semantic Role Attributes From the SRL tool, our dataset was tagged with various arguments of predicates. This data is particularly useful in our study, as we are interested in examining the semantic actions and stakeholders within an abusive relationship. By performing a lookup in Proposition Bank (Martha et al., 2005) with a given argument number, predicate, and sense, we retrieved unique role labels for each argument. We determined the top 100 most frequent roles and predicates in the two sets, and took only the unique roles and predicates within each set to see what frequently occurring but unique roles and predicates exist within the abuse and control group. Role Label caller, 175 thing hit, 174 agent, hitter - animate only!, 164 abuser, agent, 162 entity abused, 139 utterance, 115 patient, entity experiencing hurt/damage, 113 utterance, sound, 104 belief, 104 benefactive, 103 Predicate abuse, 433 share"
D15-1309,J08-2005,0,0.0174262,"se. The domestic abuse subreddits have far fewer active users, submissions, and comments in total. 3.1 Preprocessing All experiments used the same preprocessing steps. From the collected subreddits, only submissions with at least one comment were chosen to be included for study. We then ran the submission text through the Illinois Curator (Clarke et 3 2 # Submissions 1653 749 512 # Submissions 7286 5913 4183 837 Released on nicschrading.com/data/. Submission text is its title and selftext (an optional text body) concatenated together. 2578 4 al., 2012) to provide semantic role labeling (SRL) (Punyakanok et al., 2008). A total of 552 domestic abuse submissions were parsed, and we randomly chose an even distribution of the control subreddits (138 each), yielding a total sample size of 1104. All submissions were normalized by lowercasing, lemmatizing, and stoplisting. External links and URLs were replaced with url and references to subreddits, e.g. /r/domesticviolence, were replaced with subreddit link. 3.2 Descriptive Statistics We present basic descriptive statistics on the set of 552 abuse submissions and 552 non-abuse submissions in Table 2. Avg comments/post Avg score/post Avg tokens/post # unique submi"
D15-1309,N15-1139,1,0.777308,"c health studies, such as mental health, bullying, and disease tracking. These sites provide less intimidating and more accessible channels for reporting, collectively processing, and making sense of traumatic and stigmatizing experiences (Homan et al., 2014; Walther, 1996). Many researchers have focused on Twitter data, due to its prominent presence, accessibility, and the characteristics of tweets. For instance, De Choudhury et al. (2013) predicted the onset of depression from user tweets, while other studies have modeled distress (Homan et al., 2014; Lehrman et al., 2012). Most relevantly, Schrading et al. (2015) used the #WhyIStayed trend to predict whether a tweet was about staying in an abusive relationship or leaving, analyzing the lexical structures victims of abuse give for staying or leaving. Reddit has been studied less in this area, with work mainly focusing on mental health. In Pavalanathan and De Choudhury (2015), a large number of subreddits on the topic of mental health were identified and used to determine the differences in discourse between throwaway2 and regular accounts. They observed almost 6 times more throwaway submissions in mental health subreddits over control subreddits, and f"
D17-2003,Q16-1005,0,0.02693,"n of picture descriptions from the DementiaBank corpus (Becker et al., 1994) with the goal of assisting a medical researcher in identifying linguistic markers of Alzheimer’s disease. Readings include Szatloczki et al. (2015); Goldstein et al. (2010). • Historical Varieties of English: Students examine excerpts of literature across time periods to assist school teachers in choosing grade-appropriate readings for their classes. Readings include, e.g., Perera (1980). • Formality in Business Communications: In roles as analysts for a training agency, students use email data (Klimt and Yang, 2004; Pavlick and Tetreault, 2016, added later) to critically envision guidelines for workplace communications. Readings include Pavlick and Tetreault (2016); Lebowitz (2015). Figure 5: Syntactic tree with sentiment labels for This is the best news I’ve heard all year! • Data that has been selected and prepared for out-of-class and in-class analysis. 4 Results of Case Study Exploration Students in an introductory linguistics course worked with Linguine in assigned teams first on The Language of Dementia and a few weeks later on Formality in Business Communications. For both cases, students used Linguine and engaged • Two read"
D17-2003,P14-5010,0,0.00259279,"inguine’s functionalities and visualizations support and facilitate the situational problem solving and Learning Linguistics with Linguine Linguine is a web application designed for an educational purpose. It provides an easy-to-use interface, allowing interaction with preloaded default or custom-uploaded plain texts for performing language-based analysis. Figure 1 shows the interface for selecting the resulting analysis. For analysis functionalities, Linguine leverages widely available resources for performing natural language processing, including NLTK (Bird et al., 2009), Stanford CoreNLP (Manning et al., 2014), and SPLAT 3 , along with the web technologies, NodeJS 4 and d3 5 . Aspects that set Linguine apart are its focus on enabling class activities and ac3 5 14 http://splat-library.org https://d3js.org/ 4 https://nodejs.org/en/about/ Q 1 2 3 4 5 6 tive learning and its ability to transform machineprocessed results into intuitive visualizations. Visualizations depend on the analysis performed and include sentence-by-sentence display of syntactic trees, data summary tables, tool-tips displaying sequential annotations, and colorful markup in text. Users can inspect results in a structured representa"
D17-2003,W08-0201,0,\N,Missing
H05-1073,P97-1023,0,0.0168789,"emotional meaning, 580 such as happy or sad, matter, emotion classification probably needs to consider additional inference mechanisms. Moreover, a na¨ıve compositional approach to emotion recognition is risky due to simple linguistic facts, such as context-dependent semantics, domination of words with multiple meanings, and emotional negation. Many NLP problems address attitudinal meaning distinctions in text, e.g. detecting subjective opinion documents or expressions, e.g. (Wiebe et al, 2004), measuring strength of subjective clauses (Wilson, Wiebe and Hwa, 2004), determining word polarity (Hatzivassiloglou and McKeown, 1997) or texts’ attitudinal valence, e.g. (Turney, 2002), (Bai, Padman and Airoldi, 2004), (Beineke, Hastie and Vaithyanathan, 2003), (Mullen and Collier, 2003), (Pang and Lee, 2003). Here, it suffices to say that the targets, the domain, and the intended application differ; our goal is to classify emotional text passages in children’s stories, and eventually use this information for rendering expressive child-directed storytelling in a text-to-speech application. This can be useful, e.g. in therapeutic education of children with communication disorders (van Santen et al., 2003). 4 Empirical study"
H05-1073,W05-0625,1,0.133096,"Missing"
H05-1073,P04-1045,0,0.0053032,"n order to be effective, emotion recognition must go beyond such resources; the authors note themselves that lexical affinity is fragile. The method was tested on 20 users’ preferences for an email-client, based on user-composed text emails describing short but colorful events. While the users preferred the emotional client, this evaluation does not reveal emotion classification accuracy, nor how well the model generalizes on a large data set. Whereas work on emotion classification from the point of view of natural speech and humancomputer dialogues is fairly extensive, e.g. (Scherer, 2003), (Litman and Forbes-Riley, 2004), this appears not to be the case for text-to-speech synthesis (TTS). A short study by (Sugimoto et al., 2004) addresses sentence-level emotion recognition for Japanese TTS. Their model uses a composition assumption: the emotion of a sentence is a function of the emotional affinity of the words in the sentence. They obtain emotional judgements of 73 adjectives and a set of sentences from 15 human subjects and compute words’ emotional strength based on the ratio of times a word or a sentence was judged to fall into a particular emotion bucket, given the number of human subjects. Additionally, t"
H05-1073,W04-3253,0,0.252605,"Missing"
H05-1073,P04-1035,0,0.0487695,"Missing"
H05-1073,P02-1053,0,0.0132851,"on probably needs to consider additional inference mechanisms. Moreover, a na¨ıve compositional approach to emotion recognition is risky due to simple linguistic facts, such as context-dependent semantics, domination of words with multiple meanings, and emotional negation. Many NLP problems address attitudinal meaning distinctions in text, e.g. detecting subjective opinion documents or expressions, e.g. (Wiebe et al, 2004), measuring strength of subjective clauses (Wilson, Wiebe and Hwa, 2004), determining word polarity (Hatzivassiloglou and McKeown, 1997) or texts’ attitudinal valence, e.g. (Turney, 2002), (Bai, Padman and Airoldi, 2004), (Beineke, Hastie and Vaithyanathan, 2003), (Mullen and Collier, 2003), (Pang and Lee, 2003). Here, it suffices to say that the targets, the domain, and the intended application differ; our goal is to classify emotional text passages in children’s stories, and eventually use this information for rendering expressive child-directed storytelling in a text-to-speech application. This can be useful, e.g. in therapeutic education of children with communication disorders (van Santen et al., 2003). 4 Empirical study This part covers the experimental study with a form"
H05-1073,C02-1150,1,\N,Missing
H05-1073,J04-3002,0,\N,Missing
H05-1073,P04-1034,0,\N,Missing
N15-1139,W14-3203,0,0.0217989,"Missing"
N15-1139,W14-3213,1,0.900711,"Missing"
N15-1139,D14-1108,0,0.0405803,"Missing"
N15-1139,W12-2102,1,0.843281,"Missing"
N15-1139,P13-2109,0,0.0245185,"Missing"
N15-1139,N13-1082,0,0.0668304,"Missing"
N18-4019,J11-2010,0,0.0925229,"Missing"
N18-4019,medero-etal-2006-efficient,0,0.111431,"Missing"
N18-4019,S12-1033,0,0.0784439,"Missing"
P06-2071,C02-1144,0,0.0280169,"ry term, and for the four meta senses, without a clear trend. clusters (Sch¨utze, 1998), but may be due to the spectral clustering. Inspection showed that 6 clusters were dominated by core senses, whereas with 40 clusters a few were also dominated by RE LATED senses or PEOPLE . No cluster was dominated by an UNRELATED label, which makes sense since semantic linkage should be absent between unrelated items. 5 Comparison to previous work Space does not allow a complete review of the WSD literature. (Yarowsky, 1995) demonstrated that semi-supervised WSD could be successful. (Sch¨utze, 1998) and (Lin and Pantel, 2002a, b) show that clustering methods are helpful in this area. While ISD has received less attention, image categorization has been approached previously by adding text features. For example, (Frankel, Swain, and Athitsos, 1996)’s WebSeer system attempted to mutually distinguish photos, hand553 thors only provide an illustrative query example, and no numerical evaluation, making any comparison difficult. (Wang et al, 2004) use similar features with the goal to improve image retrieval through similarity propagation, querying specific web sites. (Fuji and Ishikawa, 2005) deal with image ambiguity"
P06-2071,J98-1004,0,0.145227,"Missing"
P06-2071,P95-1026,0,0.800445,"ept. of Computer Science University of Illinois, UC daf@uiuc.edu Also, an image often depicts a related meaning. E.g. a picture retrieved for SQUASH may depict a squash bug (i.e. an insect on a leaf of a squash plant) instead of a squash vegetable, whereas this does not really apply in WSD, where each instance concerns the ambiguous term itself. Therefore, it makes sense to consider the division between core sense, related sense, and unrelated sense in ISD, and, as an additional complication, their boundaries are often blurred. Most importantly, whereas the one-sense-per-discourse assumption (Yarowsky, 1995) also applies to discriminating images, there is no guarantee of a local collocational or co-occurrence context around the target image. Design or aesthetics may instead determine image placement. Thus, considering local text around the image may not be as helpful as local context is for standard WSD. In fact, the query term may even not occur in the text body. On the other hand, one can assume that an image spotlights the web page topic and that it highlights important document information. Also, images mostly depict concrete senses. Lastly, ISD from web data is complicated by web pages being"
P06-2071,W06-0601,1,\N,Missing
P11-2019,P10-1010,0,0.0142773,"technologies remains an unexplored area. Ethical interrogations (and guidelines) are especially important as language technologies continue to be refined and migrate to new domains. Potential problematic implications of language technologies– or how disciplinary contributions affect the linguistic world–have rarely been a point of discussion. However, there are exceptions. For example, there are convincing arguments for gains that will result from an increased engagement with topics related to endangered languages and language documentation in computational linguistics (Bird, 2009), see also Abney and Bird (2010). By implication, such efforts may contribute to linguistic and cultural sustainability. tion away from how subjective perception and production phenomena actually manifest themselves in natural language. In encouraging a focus on efforts to achieve ’high-performing’ systems (as measured along traditional lines), there is risk involved–the sacrificing of opportunities for fundamental insights that may lead to a more thorough understanding of language uses and users. Such insights may in fact decisively advance language science and artificial natural language intelligence. Acknowledgments • Int"
P11-2019,W10-1815,1,0.873318,"computing applications apply to language (Picard, 1997). One such area is automatically inferring affect in text. Work on automatic affect inference from language data has generally involved recognition or generation models that contrast a range of affective states either along affect categories (e.g. angry, happy, surprised, neutral, etc.) or dimensions (e.g. arousal and pleasantness). As one example, Alm developed an affect dataset and explored automatic prediction of affect 108 in text at the sentence level that accounted for different levels of affective granularity (Alm, 2008; Alm, 2009; Alm, 2010). There are other examples of the strong interest in affective NLP or affective interfacing (Liu et al, 2003; Holzman and Pottenger, 2003; Francisco and Gerv´as, 2006; Kalra and Karahalios, 2005; G´en´ereux and Evans, 2006; Mihalcea and Liu, 2006). Affective semantics is difficult for many automatic techniques to capture because rather than simple text-derived ‘surface’ features, it requires sophisticated, ‘deep’ natural language understanding that draws on subjective human knowledge, interpretation, and experience. At the same time, approaches that accumulate knowledge bases face issues such"
P11-2019,W06-0601,1,0.849175,"ctive complexity. For instance, images capture “both word and iconographic sense distinctions ... CRANE can refer to, e.g. a MACHINE or a BIRD; iconographic distinctions could additionally include birds standing, vs. in a marsh land, or flying, i.e. sense distinctions encoded by further descriptive modication in text.” (p. 547) (Loeff et al, 2006). In other words, images can evoke a range of subtle, subjective meaning phenomena. Challenges for annotating images according to lexical meaning (and the use of verification as one way to assess annotation quality) have been discussed in depth, cf. (Alm et al, 2006). 3.3 Case 3: Multilingual communication The world is multilingual and so are many human language technology users. Multilingual applications have strong potential to grow. Arguably, future generations of users will increasingly demand tools capable of effective multilingual tasking, communication and inference-making (besides expecting adjustments to non-native and cross-linguistic behaviors). The challenges of code-mixing include dynamically adapting sociolinguistic forms and functions, and they involve both flexible, subjective sense-making and perspective-taking. 3.4 Case 4: Individualized"
P11-2019,D08-1014,0,0.0386905,"ysis are also established in the NLP literature to cover these topics of growing inquiry. The purpose of this opinion paper is not to provide a survey of subjective natural language probIt is definitely reasonable to assume that problems involving subjective perception, meaning, and language behaviors will diversify and earn increased attention from computational approaches to language. Banea et al already noted: “We have seen a surge in interest towards the application of automatic tools and techniques for the extraction of opinions, emotions, and sentiments in text (subjectivity)” (p. 127) (Banea et al, 2008). Therefore, it is timely and useful to examine subjective natural language problems from different angles. The following account is an attempt in this direction. The first angle that the paper comments upon is what motivates investigatory efforts into such problems. Next, the paper clarifies what subjective natural language processing problems are by providing a few illustrative examples of some relevant problem-solving and application areas. This is followed by discussing yet another angle of this family of problems, namely what some of their characteristics are. Finally, potential implicati"
P11-2019,J09-3007,0,0.0162299,"cs applies to language technologies remains an unexplored area. Ethical interrogations (and guidelines) are especially important as language technologies continue to be refined and migrate to new domains. Potential problematic implications of language technologies– or how disciplinary contributions affect the linguistic world–have rarely been a point of discussion. However, there are exceptions. For example, there are convincing arguments for gains that will result from an increased engagement with topics related to endangered languages and language documentation in computational linguistics (Bird, 2009), see also Abney and Bird (2010). By implication, such efforts may contribute to linguistic and cultural sustainability. tion away from how subjective perception and production phenomena actually manifest themselves in natural language. In encouraging a focus on efforts to achieve ’high-performing’ systems (as measured along traditional lines), there is risk involved–the sacrificing of opportunities for fundamental insights that may lead to a more thorough understanding of language uses and users. Such insights may in fact decisively advance language science and artificial natural language int"
P11-2019,P06-2071,1,0.833378,"semantics is difficult for many automatic techniques to capture because rather than simple text-derived ‘surface’ features, it requires sophisticated, ‘deep’ natural language understanding that draws on subjective human knowledge, interpretation, and experience. At the same time, approaches that accumulate knowledge bases face issues such as the artificiality and limitations of trying to enumerate rather than perceive and experience human understanding. 3.2 Case 2: Image sense discrimination Image sense discrimination refers to the problem of determining which images belong together (or not) (Loeff et al, 2006; Forsyth et al, 2009). What counts as the sense of an image adds subjective complexity. For instance, images capture “both word and iconographic sense distinctions ... CRANE can refer to, e.g. a MACHINE or a BIRD; iconographic distinctions could additionally include birds standing, vs. in a marsh land, or flying, i.e. sense distinctions encoded by further descriptive modication in text.” (p. 547) (Loeff et al, 2006). In other words, images can evoke a range of subtle, subjective meaning phenomena. Challenges for annotating images according to lexical meaning (and the use of verification as on"
P11-2019,J04-3002,0,0.0259781,"understanding, problem-solving methods, and evaluation techniques in computational linguistics. The author supports a more holistic approach to such problems; a view that extends beyond opinion mining or sentiment analysis. 1 Introduction Interest in subjective meaning and individual, interpersonal or social, poetic/creative, and affective dimensions of language is not new to linguistics or computational approaches to language. Language analysts, including computational linguists, have long acknowledged the importance of such topics (B¨uhler, 1934; Lyons, 1977; Jakobson, 1996; Halliday, 1996; Wiebe et al, 2004; Wilson et al, 2005). In computational linguistics and natural language processing (NLP), current efforts on subjective natural language problems are concentrated on the vibrant field of opinion mining and sentiment analysis (Liu, 2010; T¨ackstr¨om, 2009), and ACL-HLT 2011 lists Sentiment Analysis, Opinion Mining and Text Classification as a subject area. The terms subjectivity or subjectivity analysis are also established in the NLP literature to cover these topics of growing inquiry. The purpose of this opinion paper is not to provide a survey of subjective natural language probIt is defini"
P11-2019,H05-1044,0,0.0163279,"lem-solving methods, and evaluation techniques in computational linguistics. The author supports a more holistic approach to such problems; a view that extends beyond opinion mining or sentiment analysis. 1 Introduction Interest in subjective meaning and individual, interpersonal or social, poetic/creative, and affective dimensions of language is not new to linguistics or computational approaches to language. Language analysts, including computational linguists, have long acknowledged the importance of such topics (B¨uhler, 1934; Lyons, 1977; Jakobson, 1996; Halliday, 1996; Wiebe et al, 2004; Wilson et al, 2005). In computational linguistics and natural language processing (NLP), current efforts on subjective natural language problems are concentrated on the vibrant field of opinion mining and sentiment analysis (Liu, 2010; T¨ackstr¨om, 2009), and ACL-HLT 2011 lists Sentiment Analysis, Opinion Mining and Text Classification as a subject area. The terms subjectivity or subjectivity analysis are also established in the NLP literature to cover these topics of growing inquiry. The purpose of this opinion paper is not to provide a survey of subjective natural language probIt is definitely reasonable to as"
P11-2019,J06-4012,0,\N,Missing
P16-1099,D09-1030,0,0.113471,"Missing"
P16-1099,W10-0708,0,0.0342526,"Missing"
P16-1099,P11-2008,0,0.0643799,"Missing"
P16-1099,D14-1214,0,0.0328842,"Missing"
P16-1099,W10-0204,0,0.0140948,"ps from the Coordinated Universal Time standard (UTC) to local time zone with daylight saving time taken into account. enterprise-wide micro-blog usage (De Choudhury and Counts, 2013), i.e., public social media exhibit gradual increase in PA while internal enterprise network decrease after business. This perhaps confirms our suspicion that people talk about work on public social media differently than on work-based media. (2) Word-Emotion Association Lexicon We focused on the words from EmoLex’s positive and negative categories, which represent sentiment polarities (Mohammad and Turney, 2013; Mohammad and Turney, 2010) and calculated the score for each tweet similarly as LIWC. The average daily positive and negative sentiment scores in Figure 6 display patterns analogous to Figure 5. Labor Statistics We explored associations between Twitter temporal patterns, affect, and official labor statistics (Figure 8). These monthly statistics11 include: labor force, employment, unemployment, and unemployment rate. We collected one more year of Twitter data from the same area, and applied C3 to extract the jobrelated posts from individual and business accounts (Table 12 summarizes the basic statistics), then defined t"
P18-2021,W10-3001,0,0.0207934,"Missing"
P18-2021,W11-0705,0,0.147643,"Missing"
P18-2021,P14-5010,0,0.00444702,"Missing"
P18-2021,J12-2004,0,0.0202963,"Missing"
P18-2022,N06-1014,0,0.156317,"Missing"
P18-2022,J03-1002,0,0.0194611,"Missing"
W06-0601,P06-2071,1,0.418881,"inter to regions of the Hulton collections”, (Enser, 1993), p. 35. Introduction We describe a set of annotated images, each associated with a sense of a small set of words. Building this data set exposes important sense phenomena which not only involve natural language but also vision. The context of our work is Image Sense Discrimination (ISD), where the task is to assign one of several senses to a web image retrieved by an ambiguous keyword. A companion paper introduces the task, presents an unsupervised ISD model, drawing on web page text and image features, and shows experimental results (Loeff et al., 2006). The data was subject to singleannotator labeling, with verification judgements on a part of the data set as a step toward studying agreement. Besides a test bed for ISD, the data set may be applicable to e.g. multimodal word sense disambiguation and cross-language image retrieval. The issues discussed concern concepts, and involve insights into semantics, perception, and knowledge representation, while opening up a bridge for interdisciplinary work involving vision and NLP. 2 David A. Forsyth Dept. of Computer Science University of Illinois, UC daf@uiuc.edu Users of image collections have be"
W10-1815,H05-1044,0,0.00994091,"intended” (577); this should not be confused with understanding a linguistic issue. Fig. 1 reports on a diagnostic alternative with the ratios of (dis)agreement types. This avoids the concept of ground truth, which may not hold for all language phenomena. Affect, which is highly subjective, is arguably better captured by flexible acceptability.5 Fig. 1 shows that sentences only labeled NEU TRAL were frequent, as were disagreements, which were more common for sentences marked both with NEUTRAL and one or more affect classes. This parallels findings for polarity expressions in subjective texts (Wilson et al, 2005), and shows that the border between affective and neutral is fuzzy. (Affect perception lacks clear definitions and is subjective, and neutrality suffers from the same dilemma.) A sentence with high agreement affect was defined as all four primary emotion and mood labels having the same affective label (given the merged label set). These were more common than mixed affective labels. Corpus data overview The affect dataset consists of 176 stories (more than 15,000 sentences) by Beatrix Potter, the Brothers Grimm and H. C. Andersen, manually annotated at the sentence level by pairs of annotators."
W10-1815,W03-2102,0,\N,Missing
W10-1815,J06-4012,0,\N,Missing
W12-2102,W10-1908,0,0.0143262,") or opinions on political candidates in event prediction markets (Lerman et al., 2008). In psychology, psychiatry, and criminology, studies with natural language data have found differences in behaviors for mental health patients or inmates with various mental health disorders (e.g., Andreasen and Pfohl, 1976; Harvey, 1983; Ragin and Oltmanns, 1983; Fraser et al., 1986; Endres, 2004; Gawda, 2010). Recently, computational linguists have increasingly tackled problems in health care. For example, Zhang and Patrick (2006) automatically classified meaningful content in clinical research articles. Jha and Elhadad (2010) predicted how far breast cancer patients had progressed in their disease, based on discourse available in postings 10 on web forums. As another example, Roark et al. (2007) explored the use of structural aspects of the language of individuals with mild cognitive impairment in assisting with such diagnostics. More specifically in mental health, Yu et al. (2009) classified five forms of “negative life events” in text (p. 202). Pestian et al. (2008) were able to use machine learning, taking advantage of text characteristics to classify suicide notes as written by either “simulators” or “complete"
W12-2102,C08-1060,0,0.0271363,"ethods and text-based features to the challenging task of automatically classifying mental affect states in short texts based on just a small dataset. We discuss performance both in terms of different experimental setups, which linguistic features matter, and how labels confuse with each other. 2 Relevant previous work Computational linguistics approaches have been applied to a range of challenging problems with impact outside the language technology field, e.g., to predict pricing movements on the stock market (Schumaker, 2010) or opinions on political candidates in event prediction markets (Lerman et al., 2008). In psychology, psychiatry, and criminology, studies with natural language data have found differences in behaviors for mental health patients or inmates with various mental health disorders (e.g., Andreasen and Pfohl, 1976; Harvey, 1983; Ragin and Oltmanns, 1983; Fraser et al., 1986; Endres, 2004; Gawda, 2010). Recently, computational linguists have increasingly tackled problems in health care. For example, Zhang and Patrick (2006) automatically classified meaningful content in clinical research articles. Jha and Elhadad (2010) predicted how far breast cancer patients had progressed in their"
W12-2102,W09-1323,0,0.379684,"Missing"
W12-2102,W08-0616,0,0.122654,"y tackled problems in health care. For example, Zhang and Patrick (2006) automatically classified meaningful content in clinical research articles. Jha and Elhadad (2010) predicted how far breast cancer patients had progressed in their disease, based on discourse available in postings 10 on web forums. As another example, Roark et al. (2007) explored the use of structural aspects of the language of individuals with mild cognitive impairment in assisting with such diagnostics. More specifically in mental health, Yu et al. (2009) classified five forms of “negative life events” in text (p. 202). Pestian et al. (2008) were able to use machine learning, taking advantage of text characteristics to classify suicide notes as written by either “simulators” or “completers” as accurately as mental health experts (p. 96). The authors also found that emotional content was useful for the expert clinicians, but not for the automatic inference methods. However, this might indicate that the study did not consider an appropriate feature set. In comparison, Alm (2009) explored a more comprehensive feature set for automatic affect prediction in text. Matykiewicz et al. (2009) discriminated between suicide notes and contro"
W12-2102,W07-1001,0,0.0145037,"nd differences in behaviors for mental health patients or inmates with various mental health disorders (e.g., Andreasen and Pfohl, 1976; Harvey, 1983; Ragin and Oltmanns, 1983; Fraser et al., 1986; Endres, 2004; Gawda, 2010). Recently, computational linguists have increasingly tackled problems in health care. For example, Zhang and Patrick (2006) automatically classified meaningful content in clinical research articles. Jha and Elhadad (2010) predicted how far breast cancer patients had progressed in their disease, based on discourse available in postings 10 on web forums. As another example, Roark et al. (2007) explored the use of structural aspects of the language of individuals with mild cognitive impairment in assisting with such diagnostics. More specifically in mental health, Yu et al. (2009) classified five forms of “negative life events” in text (p. 202). Pestian et al. (2008) were able to use machine learning, taking advantage of text characteristics to classify suicide notes as written by either “simulators” or “completers” as accurately as mental health experts (p. 96). The authors also found that emotional content was useful for the expert clinicians, but not for the automatic inference m"
W12-2102,W10-0502,0,0.0175786,"for what can be achieved by applying four fundamental supervised classification methods and text-based features to the challenging task of automatically classifying mental affect states in short texts based on just a small dataset. We discuss performance both in terms of different experimental setups, which linguistic features matter, and how labels confuse with each other. 2 Relevant previous work Computational linguistics approaches have been applied to a range of challenging problems with impact outside the language technology field, e.g., to predict pricing movements on the stock market (Schumaker, 2010) or opinions on political candidates in event prediction markets (Lerman et al., 2008). In psychology, psychiatry, and criminology, studies with natural language data have found differences in behaviors for mental health patients or inmates with various mental health disorders (e.g., Andreasen and Pfohl, 1976; Harvey, 1983; Ragin and Oltmanns, 1983; Fraser et al., 1986; Endres, 2004; Gawda, 2010). Recently, computational linguists have increasingly tackled problems in health care. For example, Zhang and Patrick (2006) automatically classified meaningful content in clinical research articles. J"
W12-2102,P09-2051,0,0.022522,"986; Endres, 2004; Gawda, 2010). Recently, computational linguists have increasingly tackled problems in health care. For example, Zhang and Patrick (2006) automatically classified meaningful content in clinical research articles. Jha and Elhadad (2010) predicted how far breast cancer patients had progressed in their disease, based on discourse available in postings 10 on web forums. As another example, Roark et al. (2007) explored the use of structural aspects of the language of individuals with mild cognitive impairment in assisting with such diagnostics. More specifically in mental health, Yu et al. (2009) classified five forms of “negative life events” in text (p. 202). Pestian et al. (2008) were able to use machine learning, taking advantage of text characteristics to classify suicide notes as written by either “simulators” or “completers” as accurately as mental health experts (p. 96). The authors also found that emotional content was useful for the expert clinicians, but not for the automatic inference methods. However, this might indicate that the study did not consider an appropriate feature set. In comparison, Alm (2009) explored a more comprehensive feature set for automatic affect pred"
W12-2102,U06-1027,0,0.0472321,"e language technology field, e.g., to predict pricing movements on the stock market (Schumaker, 2010) or opinions on political candidates in event prediction markets (Lerman et al., 2008). In psychology, psychiatry, and criminology, studies with natural language data have found differences in behaviors for mental health patients or inmates with various mental health disorders (e.g., Andreasen and Pfohl, 1976; Harvey, 1983; Ragin and Oltmanns, 1983; Fraser et al., 1986; Endres, 2004; Gawda, 2010). Recently, computational linguists have increasingly tackled problems in health care. For example, Zhang and Patrick (2006) automatically classified meaningful content in clinical research articles. Jha and Elhadad (2010) predicted how far breast cancer patients had progressed in their disease, based on discourse available in postings 10 on web forums. As another example, Roark et al. (2007) explored the use of structural aspects of the language of individuals with mild cognitive impairment in assisting with such diagnostics. More specifically in mental health, Yu et al. (2009) classified five forms of “negative life events” in text (p. 202). Pestian et al. (2008) were able to use machine learning, taking advantag"
W12-3612,C04-1140,0,0.0832937,"Missing"
W12-3612,W10-1110,0,0.0286993,"owiecka (2011) also report on annotating medical texts. They verified an automatic system against manual annotation of hospital discharge reports for linguistic morphologies. clinical dermatology texts. This research is also a starting point for empirically exploring the theoretical division of physicians’ decision-making systems by Croskerry (2009) into “intuitive” and “analytical” (p. 1022). We plan to investigate the relationship between thought units and Croskerry’s hypothesized differences in medical reasoning situations further. Importantly, this study responds to the need identified by Kokkinakis and Gronostaj (2010) for better methods for parsing scientific and medical data. The presented annotations schemes and the annotated data set we report upon will be useful for developing and evaluating relevant systems for processing This research was supported by NIH 1 R21 LM010039-01A1, NSF IIS-0941452, RIT GCCIS Seed Funding, and RIT Research Computing (http://rc.rit.edu). We thank Lowell A. Goldsmith, M.D., anonymous reviewers, Preethi Vaidyanathan, and transcribers. 102 6 Conclusion This study investigates two annotation schemes that capture cognitive reasoning processes of dermatologists. Our work contribut"
W12-3612,W11-0211,0,0.0303296,"set reported on in this study shows diagnostic cognitive processes through narrations spoken impromptu. Because of this, the data set captures cognitive associations, including speculative reasoning elements. Such information could be useful in a decision-support system, for instance to alert physicians to commonly confused diagnostic alternatives. Other work has been done in annotating medical texts. For example, Mowery et al. (2008) focused on finding temporal aspects of clinical texts, whereas we attempt to show the steps of the cognitive processes used by physicians during decisionmaking. Marciniak and Mykowiecka (2011) also report on annotating medical texts. They verified an automatic system against manual annotation of hospital discharge reports for linguistic morphologies. clinical dermatology texts. This research is also a starting point for empirically exploring the theoretical division of physicians’ decision-making systems by Croskerry (2009) into “intuitive” and “analytical” (p. 1022). We plan to investigate the relationship between thought units and Croskerry’s hypothesized differences in medical reasoning situations further. Importantly, this study responds to the need identified by Kokkinakis and"
W12-3612,W12-3803,1,0.753686,"Missing"
W12-3612,W08-0621,0,0.059901,"Missing"
W12-3612,W12-3801,1,\N,Missing
W12-3801,W12-3803,1,0.660339,"Missing"
W12-3801,W08-0606,0,0.0600863,"Missing"
W12-3801,J12-2001,0,\N,Missing
W12-3801,H94-1041,0,\N,Missing
W12-3803,P11-2019,1,0.802063,"ic behaviors. For example, when engaging the slower analytic system, it seems reasonable that frequent pausing could appear as an indication of, e.g., uncertainty or thoughtfulness. Several studies have explored the task of detecting uncertainty through language. Uncertainty detection necessitates inference of extra-propositional meaning and is arguably a subjective natural language problem, i.e. part of a family of problems that are increasingly receiving attention in computational linguistics. These problems involve more dynamic classification targets and different performance expectations (Alm, 2011). Pon-Barry and Shieber (2009) have shown encouraging results in finding uncertainty using acoustic-prosodic features at the word, word’s local context, and whole utterance levels. Henriksson and Velupillai (2010) used “speculative words” (e.g., could, generally, should, may, sort of, etc.) as well as “certainty amplifiers” (e.g., definitely, positively, must, etc.) to determine uncertainty in text. Velupillai (2010) also applied the same approach to medical texts and noted that acoustic-prosodic features should be considered 20 alongside salient lexical-structural features as indicators of un"
W12-3803,W10-3107,0,0.0188824,"tudies have explored the task of detecting uncertainty through language. Uncertainty detection necessitates inference of extra-propositional meaning and is arguably a subjective natural language problem, i.e. part of a family of problems that are increasingly receiving attention in computational linguistics. These problems involve more dynamic classification targets and different performance expectations (Alm, 2011). Pon-Barry and Shieber (2009) have shown encouraging results in finding uncertainty using acoustic-prosodic features at the word, word’s local context, and whole utterance levels. Henriksson and Velupillai (2010) used “speculative words” (e.g., could, generally, should, may, sort of, etc.) as well as “certainty amplifiers” (e.g., definitely, positively, must, etc.) to determine uncertainty in text. Velupillai (2010) also applied the same approach to medical texts and noted that acoustic-prosodic features should be considered 20 alongside salient lexical-structural features as indicators of uncertainty. In this work, we draw on the insight of such previous work, but we also extend the types of linguistic evidence considered for identifying possible links to diagnostic correctness. As another type of li"
W12-3803,W12-3612,1,0.75537,"Missing"
W12-3803,P99-1083,0,0.0307413,"ed the same approach to medical texts and noted that acoustic-prosodic features should be considered 20 alongside salient lexical-structural features as indicators of uncertainty. In this work, we draw on the insight of such previous work, but we also extend the types of linguistic evidence considered for identifying possible links to diagnostic correctness. As another type of linguistic evidence, disfluencies make up potentially important linguistic evidence. Zwarts and Johnson (2011) found that the occurrence of disfluencies that had been removed could be predicted to a satisfactory degree. Pakhomov (1999) observed that such disfluencies are just as common in monologues as in dialogues even though there is no need for the speakers to indicate that they wish to continue speaking. This finding is important for the work presented here because our modified use of the Master-Apprentice scenario results in a particular dialogic interaction with the listener remaining silent. Perhaps most importantly, Clark and Fox Tree (2002) postulated that filled pauses (e.g., um, uh, er, etc.) play a meaningful role in speech. For example, they may signal that the speaker is yet to finish speaking or searching for"
W12-3803,N09-2027,0,0.0255888,". For example, when engaging the slower analytic system, it seems reasonable that frequent pausing could appear as an indication of, e.g., uncertainty or thoughtfulness. Several studies have explored the task of detecting uncertainty through language. Uncertainty detection necessitates inference of extra-propositional meaning and is arguably a subjective natural language problem, i.e. part of a family of problems that are increasingly receiving attention in computational linguistics. These problems involve more dynamic classification targets and different performance expectations (Alm, 2011). Pon-Barry and Shieber (2009) have shown encouraging results in finding uncertainty using acoustic-prosodic features at the word, word’s local context, and whole utterance levels. Henriksson and Velupillai (2010) used “speculative words” (e.g., could, generally, should, may, sort of, etc.) as well as “certainty amplifiers” (e.g., definitely, positively, must, etc.) to determine uncertainty in text. Velupillai (2010) also applied the same approach to medical texts and noted that acoustic-prosodic features should be considered 20 alongside salient lexical-structural features as indicators of uncertainty. In this work, we dr"
W12-3803,W10-2102,0,0.12929,"ask of detecting uncertainty through language. Uncertainty detection necessitates inference of extra-propositional meaning and is arguably a subjective natural language problem, i.e. part of a family of problems that are increasingly receiving attention in computational linguistics. These problems involve more dynamic classification targets and different performance expectations (Alm, 2011). Pon-Barry and Shieber (2009) have shown encouraging results in finding uncertainty using acoustic-prosodic features at the word, word’s local context, and whole utterance levels. Henriksson and Velupillai (2010) used “speculative words” (e.g., could, generally, should, may, sort of, etc.) as well as “certainty amplifiers” (e.g., definitely, positively, must, etc.) to determine uncertainty in text. Velupillai (2010) also applied the same approach to medical texts and noted that acoustic-prosodic features should be considered 20 alongside salient lexical-structural features as indicators of uncertainty. In this work, we draw on the insight of such previous work, but we also extend the types of linguistic evidence considered for identifying possible links to diagnostic correctness. As another type of li"
W12-3803,W08-0606,0,0.0811129,"classification work, first explaining the methodology for the initial pilot study followed by interpretation of results. Next, the methodology of the second pilot study is described. 4.1 Generic Model Overview This work applies computational modeling designed to predict diagnostic correctness in physicians’ narratives based on linguistic features from the acoustic-prosodic and lexical-structural modalities of language, shown in Table 2. Some tests discussed in 4.2 and 4.3 were performed with these modalities separated. These features are inspired by previous work conducted by Szarvas (2008), Szarvas et al. (2008), Litman et al. (2009), Liscombe et al. (2005), and Su et al. (2010). We can formally express the created model in the following way: Let ni be an instance in a set of narratives N , let j be a classification method, and let li be a label in a set of class labels L. We want to establish a function f (ni , j) : li where li is the label assigned to the narrative based on linguistic features from a set F , where F = f1 , f2 , ...fk , as described in Table 2. The baseline for each classifier is defined as the majority class ratio. Using scripts in Praat (Boersma, 2001), Python, and NLTK (Bird et a"
W12-3803,P08-1033,0,0.0215613,"on discusses the classification work, first explaining the methodology for the initial pilot study followed by interpretation of results. Next, the methodology of the second pilot study is described. 4.1 Generic Model Overview This work applies computational modeling designed to predict diagnostic correctness in physicians’ narratives based on linguistic features from the acoustic-prosodic and lexical-structural modalities of language, shown in Table 2. Some tests discussed in 4.2 and 4.3 were performed with these modalities separated. These features are inspired by previous work conducted by Szarvas (2008), Szarvas et al. (2008), Litman et al. (2009), Liscombe et al. (2005), and Su et al. (2010). We can formally express the created model in the following way: Let ni be an instance in a set of narratives N , let j be a classification method, and let li be a label in a set of class labels L. We want to establish a function f (ni , j) : li where li is the label assigned to the narrative based on linguistic features from a set F , where F = f1 , f2 , ...fk , as described in Table 2. The baseline for each classifier is defined as the majority class ratio. Using scripts in Praat (Boersma, 2001), Pyth"
W12-3803,W10-3103,0,0.0204986,"lored the task of detecting uncertainty through language. Uncertainty detection necessitates inference of extra-propositional meaning and is arguably a subjective natural language problem, i.e. part of a family of problems that are increasingly receiving attention in computational linguistics. These problems involve more dynamic classification targets and different performance expectations (Alm, 2011). Pon-Barry and Shieber (2009) have shown encouraging results in finding uncertainty using acoustic-prosodic features at the word, word’s local context, and whole utterance levels. Henriksson and Velupillai (2010) used “speculative words” (e.g., could, generally, should, may, sort of, etc.) as well as “certainty amplifiers” (e.g., definitely, positively, must, etc.) to determine uncertainty in text. Velupillai (2010) also applied the same approach to medical texts and noted that acoustic-prosodic features should be considered 20 alongside salient lexical-structural features as indicators of uncertainty. In this work, we draw on the insight of such previous work, but we also extend the types of linguistic evidence considered for identifying possible links to diagnostic correctness. As another type of li"
W12-3803,P11-1071,0,0.0140361,"ell as “certainty amplifiers” (e.g., definitely, positively, must, etc.) to determine uncertainty in text. Velupillai (2010) also applied the same approach to medical texts and noted that acoustic-prosodic features should be considered 20 alongside salient lexical-structural features as indicators of uncertainty. In this work, we draw on the insight of such previous work, but we also extend the types of linguistic evidence considered for identifying possible links to diagnostic correctness. As another type of linguistic evidence, disfluencies make up potentially important linguistic evidence. Zwarts and Johnson (2011) found that the occurrence of disfluencies that had been removed could be predicted to a satisfactory degree. Pakhomov (1999) observed that such disfluencies are just as common in monologues as in dialogues even though there is no need for the speakers to indicate that they wish to continue speaking. This finding is important for the work presented here because our modified use of the Master-Apprentice scenario results in a particular dialogic interaction with the listener remaining silent. Perhaps most importantly, Clark and Fox Tree (2002) postulated that filled pauses (e.g., um, uh, er, etc"
W14-0902,P11-1077,0,0.022717,"The latter study expanded on previous work by exploring three attributes: gender, age, and native/non-native speakers. There have been previous avenues of research into categorizing speakers based on different individual sociolinguistic factors. However, not many studies have attempted this categorization with fictional characters. Literary texts are complex, reflecting authors’ decision-making and creative processes. From the perspective of digital humanities, such a focus complements computational sociolinguistic modeling of contemporary user-generated text types (such as emails, or blogs (Rosenthal and McKeown, 2011)). As Lindquist (2009) points out, social data for interlocutors is less often attached to openly available linguistic corpora, and interest is strong in developing corpus methods to help explore social language behavior (see Lindquist (2009) and Baker (2010)). Previous investigation into social dimensions of language has established strong links between language and social attributes of speech communities (for an overview, see Mesthrie et al. (2009)). However, such inquiry has generally had a firm foundation in field-based research and has usually focused on one or just a few linguistic varia"
W14-0902,P05-1054,0,0.218522,"ors 11 Proceedings of the 3rd Workshop on Computational Linguistics for Literature (CLfL) @ EACL 2014, pages 11–16, c Gothenburg, Sweden, April 27, 2014. 2014 Association for Computational Linguistics The characters’ lines do not include the metadata needed for considering spoken features, since usually these are added at the discretion of the performer. This may make our problem more challenging, since some of these indicators may be reliable for identifying gender, such as backchannel responses and affirmations from females, and assertively “holding the floor” with filled pauses from males (Boulis and Ostendorf, 2005). Moreover, there are prosodic features that clearly differ between males and females due to physical characteristics (e.g. F0 , predominant for pitch perception). We do not take advantage of acoustic/prosodic cues in this work. Our text is also artificial discourse, as opposed to natural speech; therefore these characters’ lines may rather express how writers choose to convey sociolinguistic attributes of their characters. In terms of features, we have explored observations from previous studies. For instance, common lexical items have been shown successful, with males tending to use more obs"
W14-0902,P09-1080,0,0.628065,"tory experimentation with classification algorithms. for natural language processing, including Python (http://www.python.org/), the Natural Language Toolkit (http://www.nltk.org/) for part of the preprocessing, and the scikit-learn machine learning library for the computational modeling. Translated texts that reside in the public domain were collected from the Gutenberg Archive (http://www.gutenburg.org/wiki/Main Page/). 2 Previous Work A pilot study by Hota et al. (2006) on automatic gender identification in Shakespeare’s texts, as well as a few primarily gender-oriented studies surveyed in Garera and Yarowsky (2009), have set the stage for further inquiry. The latter study expanded on previous work by exploring three attributes: gender, age, and native/non-native speakers. There have been previous avenues of research into categorizing speakers based on different individual sociolinguistic factors. However, not many studies have attempted this categorization with fictional characters. Literary texts are complex, reflecting authors’ decision-making and creative processes. From the perspective of digital humanities, such a focus complements computational sociolinguistic modeling of contemporary user-generat"
W14-0902,W12-2502,0,0.0725491,"Missing"
W14-3213,P11-2019,1,0.769538,"s in the affective sciences lack a single agreed-upon definition for emotion. Accordingly, different theoretical constructs have been proposed to describe affect and affect-related behaviors (Picard, 1997). In addition, research on affect in language has shown that such phenomena tend to be subjective, lack real ground truth (often resulting in moderate kappa scores), and have particularly fuzzy semantics in the gray zone where neutrality and emotion meet (Alm, 2008). These kinds of problem characteristics bring with them their own set of demanding challenges from a computational perspective (Alm, 2011). Yet, the nature of such problems make them incredibly important to study, despite the challenges involved. 3 Methods Our methods involve four main phases: (1) We filtered a corpus, obtained from Sadilek et al. (2012), of approximately 2.5 million tweets from 6,237 unique users in the New York City area that were sent during a 1-month period between May and June, 2010, into a set of 2,000 tweets that are relatively likely to be centered around suicide risk factors. (2) We annotated each of these 2,000 tweets with their level of distress, and also analyzed the annotations in detail. (3) We the"
W14-3213,W09-1323,0,0.0312853,"Missing"
W14-3213,S12-1033,0,0.0334335,". (3) We then trained support vector machines and topic models with the annotated data, except for a held-out subset of 200 tweets. (4) Finally, we assessed the effectiveness of these methods on the held-out data. Sentiment analysis has been widely studied in a number of computational settings, including on various social networking sites. A rather substantial body of work already exists on the use of Twitter to study emotion (Bollen et al., 2011b; Dodds et al., 2011; Wang et al., 2012; Pfitzner et al., 2012; Kim et al., 2012; Bollen et al., 2011a; Pfitzner et al., 2012; Bollen et al., 2011c; Mohammad, 2012; Golder and Macy, 2011; De Choud109 Source tweets Filtered tweets Categories distribution Number of tweets Unique geo-active users “Follows” relationships “Friends” relationships Number of tweets Unique users Unique unigrams Unique bigrams Unique trigrams LIWC sad Depressive feeling Suicide ideation Depression symptoms Self harm Family violence/discord Bullying Gun ownership Drug abuse Impulsivity Prior suicide attempts Suicide around individual Psychological disorders 2,535,706 6,237 102,739 31,874 2,000 1,467 1,714,167 9,246,715 1,306,1142 1,370 283 123 72 67 47 10 10 6 6 2 2 2 depressive f"
W14-3213,pak-paroubek-2010-twitter,0,0.0137178,". However, when it comes to preventive contexts, such data are less insightful. For preventive health, access to real-time healthrelated data that dynamically evolve can allow us to address macro-level analysis. Social media provide an additional opportunity to model the phenomena of interest at scale. We use methods that take advantage of lexical analysis to retrieve microblog posts (tweets) from Twitter and compare the performance of human 108 hury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Counts, 2013; Hannak et al., 2012; Thelwall et al., 2011; Pak and Paroubek, 2010). For instance, Golder and and Macy study aggregate global trends in “mood,” and show, among other things, that people wake up in a relatively good mood that decays as the day progresses (Golder and Macy, 2011). Bollen et al. (2011c) show that tweets from users who took a standard diagnostic instrument for mood are often tied to current events, such as elections and holidays. conjunction with substantial practical field experience. In a task such as medical image inspection, the subtle cues that point an observer to evidence that allow them to identify a clinical condition, while accessible to"
W14-3213,W08-0616,0,0.00963505,"particular aspect of suicidality, namely distress. While not equivalent to suicide ideation, according to Nock et al. (2010) distress is an important risk factor in suicide, and one that is observable from microblog text, though admittedly observing suicide risk behavior is a subjective and noisy venture. Lehrman et al. (2012) conducted an early study on the computational modeling of distress based on short forum texts, yet left many areas wide open for continued study. For example, analysis at scale is one such open issue. More specifically, Pestian and colleagues (Matykiewicz et al., 2009; Pestian et al., 2008) used computational methods to understand suicide notes. However, when it comes to preventive contexts, such data are less insightful. For preventive health, access to real-time healthrelated data that dynamically evolve can allow us to address macro-level analysis. Social media provide an additional opportunity to model the phenomena of interest at scale. We use methods that take advantage of lexical analysis to retrieve microblog posts (tweets) from Twitter and compare the performance of human 108 hury et al., 2012a; De Choudhury et al., 2012b; De Choudhury et al., 2013; De Choudhury and Cou"
W14-3213,W12-2102,1,0.730155,"ning of medical domain knowledge requires advanced education in In this paper, we take steps toward the automatic detection of suicide risk among individuals via social media. Suicide ideation is a complex behavior and its connection to suicide itself remains poorly understood. We focus on a particular aspect of suicidality, namely distress. While not equivalent to suicide ideation, according to Nock et al. (2010) distress is an important risk factor in suicide, and one that is observable from microblog text, though admittedly observing suicide risk behavior is a subjective and noisy venture. Lehrman et al. (2012) conducted an early study on the computational modeling of distress based on short forum texts, yet left many areas wide open for continued study. For example, analysis at scale is one such open issue. More specifically, Pestian and colleagues (Matykiewicz et al., 2009; Pestian et al., 2008) used computational methods to understand suicide notes. However, when it comes to preventive contexts, such data are less insightful. For preventive health, access to real-time healthrelated data that dynamically evolve can allow us to address macro-level analysis. Social media provide an additional opport"
W14-3213,W12-3801,1,0.822888,"e., depression, substance abuse, suicidal ideation, self-harm, or impulsivity), family history of suicide, interpersonal conflicts (i.e., family violence or bullying), and means for suicidal behavior (e.g., firearms), are commonly cited risk factors for suicidal behavior (Nock et al., 2008; Crosby et al., 2011; Gaynes et al., 2004; Harriss and Hawton, 2005; Shaffer et al., 2004; Brown et al., 2000). Regarding the use of annotation for predictive modeling, evidence suggests that when it comes to judgments that involve clinical phenomena, experts and novices behave differently (Li et al., 2012; Womack et al., 2012). Such distinctions intuitively make sense, as the learning of medical domain knowledge requires advanced education in In this paper, we take steps toward the automatic detection of suicide risk among individuals via social media. Suicide ideation is a complex behavior and its connection to suicide itself remains poorly understood. We focus on a particular aspect of suicidality, namely distress. While not equivalent to suicide ideation, according to Nock et al. (2010) distress is an important risk factor in suicide, and one that is observable from microblog text, though admittedly observing su"
W14-3412,P11-2019,1,0.418964,"Missing"
W14-3412,W12-3612,1,0.794625,"Missing"
W14-4919,P11-2019,1,0.755543,"hysicians were correct in their final diagnosis. (Confidence mentions were removed in narratives presented to annotators, to avoid any potential bias.) This work describes computational modeling for automatic annotation of decision style using this annotated dataset, on the basis of linguistic, speaker, and image case features. 1.1 Contributions To date, this appears to be the first study attempting to computationally predict physician decision style. Similar to the case of affect, automatic annotation of decision style can be characterized as a subjective natural language processing problem (Alm, 2011). This adds special challenges to the modeling process. Accordingly, this work details a thorough process for moving from manual to automatic annotation. This study contributes to cognitive psychology, annotation methodology, and clinical computational linguistic analysis. Methodologically, the study details a careful process for selecting and labeling manually annotated data for modeling in the realm of subjective natural language phenomena, thus addressing the need for their characterization (Alm, 2011). Theoretically, acceptable annotator reliability on decision style, along with successful"
W14-4919,W14-3412,1,0.870904,"Missing"
W14-4919,W12-3801,1,0.825346,"eatures matches surface strings rather than senses; future work might operate on the sense rather than token level. 5 Related Work Lauri et al. (2001) asked nurses in five countries to rate statements representative of intuitive or analytical decision-making on a 5-point scale. They found that reasoning varies with context and that styles in the middle of the cognitive continuum predominate. In this work, annotation ratings were prevalent in the middle of the spectrum. Thus, both studies endorse that most decision-making occurs in the central part of the continuum (Hamm, 1988; Hammond, 1981). Womack et al. (2012) proposed that silent pauses in physician narration may indicate cognitive processing. Here, silent pauses were also important, perhaps because analytical decision-making may recruit more cognitive resources than intuitive decision-making. 6 Conclusion This work suggests that decision style is revealed in language use, in line with claims that linguistic data reflect speakers’ cognitive processes (Pennebaker & King, 1999; Tausczik & Pennebaker, 2010). Theoretically, the study adds validity to the dual process and cognitive continuum theories. Methodologically, it articulates a method of transi"
W15-0111,P13-2138,0,0.151231,"he problem of integrating the two data streams as analogous to the alignment of a parallel corpus, or bitext, in machine translation, in which the words of a sentence in one language are aligned to their corresponding translations in another language. For our problem, eye movements on images are considered to be the visual language comprising visual units of analysis, while the transcribed narratives contain the linguistic units of analysis. Previous work has investigated the association of words with pictures, words with videos, and words with objects and image regions (Forsyth et al., 2009; Kuznetsova et al., 2013; Kong et al., 2014). But the combination of perceptual information (via eye movements) and 76 Proceedings of the 11th International Conference on Computational Semantics, pages 76–81, c London, UK, April 15-17 2015. 2015 Association for Computational Linguistics more naturally obtained conceptual information (via narratives) will greatly improve the understanding of the semantics of an image, allowing image regions that are relevant for an image inspection task to be annotated with meaningful linguistic descriptors. In this ongoing work, we use dermatological images as a test case to learn th"
W15-0111,N06-1014,0,0.584069,"ned enhancements to the methods are also discussed. 1 Introduction The ability to identify and describe the important regions of an image is useful for a range of visionbased reasoning tasks. When expert observers communicate the outcome of vision-based reasoning, spoken language is the most natural and convenient instrument for conveying their understanding. This paper reports on a novel approach for semantically annotating important regions of an image with natural language descriptors. The proposed method builds on prior work in machine translation for bitext alignment (Vogel et al., 1996; Liang et al., 2006) but differs in the insight that such methods can be applied to align multimodal visual-linguistic data. Using these methods, we report on initial steps to integrate eye movements and transcribed spoken narratives elicited from expert observers inspecting medical images. Our work relies on the fact that observers’ eye movements over an image reveal what they consider to be the important image regions, their relation to one another, and their relation to the image inspection objectives. Observers’ co-captured narrations about the image naturally express relevant meaning and, especially in exper"
W15-0111,N07-1051,0,0.0106801,"e units is maintained and reflected in the parallel data windows. therefore to extract these visual and linguistic units from the eye-tracking data and audio recordings. The audio recordings of the observers were transcribed verbatim, as shown in Figure 1. Most dermatological concepts present in an image tend to involve either noun phrases or adjectives. Accordingly, the linguistic units for alignment are nouns and adjectives, which we identify using the following process. Utterance boundaries are added and silent and filled pauses are removed before parsing the data with the Berkeley parser (Petrov and Klein, 2007). From the parsed output, tokens in noun phrases and adjective phrases are extracted. The linear order of the data is maintained. Following the extraction process, we filter out regular stopwords along with a set of tokens reflecting the data collection scenario (e.g., differential, certainty, diagnosis).1 Names of diagnoses (psoriasis, basal cell carcinoma, etc.) are also removed since such words do not correspond to any particular image region but instead reflect the disease that an image depicts as a whole. The resulting filtered and linearly ordered sequences of linguistic units serve as o"
W15-0111,C96-2141,0,0.253696,"image regions. Planned enhancements to the methods are also discussed. 1 Introduction The ability to identify and describe the important regions of an image is useful for a range of visionbased reasoning tasks. When expert observers communicate the outcome of vision-based reasoning, spoken language is the most natural and convenient instrument for conveying their understanding. This paper reports on a novel approach for semantically annotating important regions of an image with natural language descriptors. The proposed method builds on prior work in machine translation for bitext alignment (Vogel et al., 1996; Liang et al., 2006) but differs in the insight that such methods can be applied to align multimodal visual-linguistic data. Using these methods, we report on initial steps to integrate eye movements and transcribed spoken narratives elicited from expert observers inspecting medical images. Our work relies on the fact that observers’ eye movements over an image reveal what they consider to be the important image regions, their relation to one another, and their relation to the image inspection objectives. Observers’ co-captured narrations about the image naturally express relevant meaning and"
W15-0111,W12-3801,1,0.845238,"nowledge for vision-based problem solving in their domain (Li et al., 2013). 3 Data Collection Twenty-nine dermatologists were eye tracked and audio recorded while they inspected and described 29 images of dermatological conditions. A Sensomotoric Instruments (SMI) eye tracking apparatus and a TASCAM audio recording equipment were used. The participants were asked to describe the image to the experimenter, using a modified version of the Master-Apprentice method (Beyer and Holtzblatt, 1997), a data elicitation methodology from human-computer interaction for eliciting rich spoken descriptions (Womack et al., 2012). Certain data were missing or excluded for quality reasons, leaving a multimodal corpus consisting of 26 observers and 29 images. 4 Aligning eye movements and transcribed narratives The goal of this research is to develop a method for aligning an observer’s eye movements over an image (the visual units) with his spoken description of that image (the linguistic units). Our first step is 77 SIL of um SIL uh SIL serpiginous SIL uh erythematous SIL uh SIL this plaque uh extending from the SIL interdigital space between the right SIL great toe and first toe SIL uh just distal though looks like the"
W15-2802,N06-1014,0,0.150318,"Missing"
W15-2802,W15-0111,1,0.87463,"Missing"
W16-0302,D11-1024,0,0.121009,"Missing"
W16-0302,D09-1026,0,0.00713047,"ls with those of structured data, with vs. without cognitive assessment scores. Experiments support texts’ viability as a useful source for dementia classification, as an important complement to structured data, or alone when structured data are missing. LDA was also studied as interpretable dimensionality reduction. With a larger sample size, the LDA model may converge to a more stable set of topics, but other appropriate public datasets (with both linguistic and non-linguistic data) are presently not available. An alternative is to apply supervised versions of LDA (Blei and McAuliffe, 2007; Ramage et al., 2009). Furthermore, with access to a pool of clinical specialists, it would be useful to integrate experts in evaluating the latent topics. Chang et al. (2009) proposed various such human evaluation techniques, such as the word intrusion task, in which human evaluators are presented with a list of n high probability terms of a randomly chosen topic, and one additional low probability term from that topic, and asked to identify the former. A drawback is that it would require access to a large enough pool of dementia specialists. Other avenues of future work would include the incorporation of lexical"
W16-0302,D13-1133,0,0.0316053,"onditions, such as dementia, as well as other illnesses of interest, are not well understood and their onsets gradually evolve over long periods of time. Further13 more, diagnosing such conditions is often primarily a function of experts’ analysis, transcribed into notes. Thus, discovering lexical associations with the progression of these conditions could be tremendously beneficial, and could also help to validate and enhance the use of resources such as the Alzheimer’s Disease Ontology (Malhotra et al., 2013). Topic models have produced interesting results across domains (Chan et al., 2013; Resnik et al., 2013; McCallum et al., 2007; Paul and Dredze, 2011). Latent Semantic Indexing (LSI) has been used in medicine to discover statistical relationships between lexical items in a corpus. LSI has been used to supplement the development of a clinical vocabulary associated with post-traumatic stress disorder (Luther et al., 2011), and for forecasting ambulatory falls in elderly patients (McCart et al., 2013). However, LSI often requires around 300–500 concepts or dimensions to produce stable results (Bradford, 2008). This limitation can be overcome by using LDA, whose identified groups of related terms a"
W16-0309,P11-1040,0,0.0559226,"Missing"
W16-0309,D14-1214,0,0.0234927,"ied data with other 86 records having these identifiers. In contrast to traditional anonymization and deidentification methods, generation of synthetic data can handle various aspects of hiding individuals, by aggregating and severing data from individual users, yet maintaining the statistical properties of the data used to train generation models. For this paper we explore several forms of data generation, using social media (Twitter) data about life-changing events as a case study. For example, Twitter data has been used for studying important life-changing events (De Choudhry et al., 2013; Li et al., 2014). Other studies present methods for anonymizing Twitter datasets. Terrovitis et al. (2008) model social media as an undirected, unlabeled graph which does retain privacy of social media users. Daubert et al. (2014) discuss the different methods for anonymization of Twitter data. However, there is a lack of work that addresses synthetic data creation using machine generation models. This paper compares traditional statistical language models and Long Short Term Memory (LSTM) models to learn models from a training set of Twitter data to generate synthetic tweets. LSTMs are recurrent neural netwo"
W16-0309,P02-1040,0,0.095656,"ion, and part-of-speech tagging. The SRI Language Modeling Toolkit (SRILM) was used to build 4-gram word- and character-based language models (Stolcke, 2002). Using these models, we then generate synthetic tweets using the OpenGRM Ngram library (Roark et al., 2012). 4.3 Table 7: Mean BLEU scores and their standard deviation over ten generated test sets of 200 tweets per model, by topic, model, and linguistic unit. Experimental Design For each event category, we divided the dataset of 2000 tweets into 1800 training and 200 testing instances. We used the machine translation quality metric BLEU (Papineni et al., 2002) to measure the similarity between machine generated tweets and the held out tests sets. For each model, we generated ten sets of 200 tweets. We calculated BLEU scores (without the brevity penalty) using the full 200-tweet test set as the reference for each candidate tweet and report the average of the BLEU scores of all ten sets of tweets generated by a given model. To gain further insight into the effectiveness of the machine generated data, we asked human annotators to evaluate the generated tweets. We selected 800 tweets by randomly sampling: 400 human generated tweets (100 from each categ"
W16-0309,P14-1108,0,0.0616374,"Missing"
W16-0309,P12-3011,0,0.0251968,"demonstrate the particular utility of LSTMs for generating realistic tweets, the output of our character- and word-based LSTM methods was compared to that of standard n-gram backoff language models. Such models are widely used to model the probability of word sequences for many NLP applications, including machine translation, automatic speech recognition, and part-of-speech tagging. The SRI Language Modeling Toolkit (SRILM) was used to build 4-gram word- and character-based language models (Stolcke, 2002). Using these models, we then generate synthetic tweets using the OpenGRM Ngram library (Roark et al., 2012). 4.3 Table 7: Mean BLEU scores and their standard deviation over ten generated test sets of 200 tweets per model, by topic, model, and linguistic unit. Experimental Design For each event category, we divided the dataset of 2000 tweets into 1800 training and 200 testing instances. We used the machine translation quality metric BLEU (Papineni et al., 2002) to measure the similarity between machine generated tweets and the held out tests sets. For each model, we generated ten sets of 200 tweets. We calculated BLEU scores (without the brevity penalty) using the full 200-tweet test set as the refe"
W16-0309,D15-1014,0,0.0586445,"Missing"
W16-0309,D15-1199,0,0.0212837,"Missing"
W17-1801,P14-5010,0,0.00274431,"and GSR on their non-dominant hands, while Camtasia6 recorded subjects’ faces and upper bodies and their screens (see Figure 1). 3.1 Linguistic Data Processing In order to cover more texts and minimize boredom and fatigue, we asked subjects to label only the first mention of each stakeholder in each text. Coreference resolution identifies multiple references to the same individual in a given text; for example, my father, he, and dad might refer to the same individual that together form a disambiguated coreference chain. Automatic coreference resolvers such as CoreNLP7 are reasonably accurate (Manning et al., 2014). We used CoreNLP to collect the remaining mentions of these stakeholders. Then, we manually inspected and corrected coreference linkages; one issue addressed was falsely non-linked chains. Stakeholder labels assigned by subjects were associated with their coreference chain by use of an algorithm that took into account the similarity between these two labeled sets of text. This algorithm minimized the Levenshtein distance between words contained in the subject-labeled text and the coreference text, placing a higher weight on matching noun/pronoun headwords. Then, each chain was assigned an agg"
W17-1801,J08-2005,0,0.0143359,"nd pulse data. From there, we calculated the average GSR and pulse per text per participant. GSR, when measured in KOhms, decreases during periods of stress or arousal as skin conductivity increases. In order to compare results across subjects, all GSR data was normalized using feature standardization. Figure 3: Diagram of data processing pipeline (P = reader-participant, I = investigator, C = computerbased processing). label based on the label most frequently assigned to it. Next, each text was run through the Illinois Semantic Role Labeler, part of the Illinois Curator package of NLP tools (Punyakanok et al., 2008). We considered the assigned text labels of (verb) predicates and of the following arguments linked to them: A0, typically the subject, and A1, typically the (direct) object. As an example, given the sentence He protected his brother, for the predicate protect, the A0 he may be labeled protector and A1 brother labeled protected. The same algorithm that was used to find the coreference chain associated with a given reader’s annotation text was used to automatically associate the semantic role nodes with their coreference chain. Figure 3 shows the entire mapping framework. Once complete, each co"
W17-1801,D15-1309,1,0.841339,"of IPV. Nonetheless, it is extremely difficult to establish reference annotations useful for predictive modeling for discourse topics as emotionally charged as IPV. We meet these challenges with a combination of annotator labeling, analyzing annotations, applying semantic processing techniques (coreference resolution, semantic role labeling, sentiment analysis), developing classifiers, and studying physiological sensor measurements collected in real-time from annotators as they read and annotate texts. Our contributions include: categorization most faithfully captures our studied narratives. Schrading et al. (2015a) developed classifiers to determine whether a Reddit post described abuse. In the study, the subreddit to which the post belonged was used to map to binary gold-standard labels: if a post came from a subreddit such as /r/survivorsofabuse, it was categorized as a post about abuse. We also draw upon such social media text data as a basis for our study, as Reddit allows us to consider narrative texts. However, we consider human perception and text annotation in conjunction with biophysical data sensed from reader-annotators. Our study makes use of coreference resolution and semantic role labeli"
W17-1801,N15-1139,1,0.7492,"of IPV. Nonetheless, it is extremely difficult to establish reference annotations useful for predictive modeling for discourse topics as emotionally charged as IPV. We meet these challenges with a combination of annotator labeling, analyzing annotations, applying semantic processing techniques (coreference resolution, semantic role labeling, sentiment analysis), developing classifiers, and studying physiological sensor measurements collected in real-time from annotators as they read and annotate texts. Our contributions include: categorization most faithfully captures our studied narratives. Schrading et al. (2015a) developed classifiers to determine whether a Reddit post described abuse. In the study, the subreddit to which the post belonged was used to map to binary gold-standard labels: if a post came from a subreddit such as /r/survivorsofabuse, it was categorized as a post about abuse. We also draw upon such social media text data as a basis for our study, as Reddit allows us to consider narrative texts. However, we consider human perception and text annotation in conjunction with biophysical data sensed from reader-annotators. Our study makes use of coreference resolution and semantic role labeli"
