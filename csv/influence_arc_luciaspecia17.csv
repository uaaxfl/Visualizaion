2006.eamt-1.28,P05-1048,0,0.075225,"Missing"
2006.eamt-1.28,J94-4003,0,0.0751692,"a sentence or text. For MT purposes, however, the context may also include the translation in the target language, i.e., words in the text which have already been translated. Although intuitively plausible, this strategy has not been explored specifically for WSD. On the other hand, some related approaches have exploited similar strategies for other purposes. For example, some approaches for MT, especially the statistics-based approaches, make use of the words which have already been translated as context, implicitly accomplishing basic WSD during the translation process (Egedi et al., 1994; Dagan and Itai, 1994). Some approaches for monolingual WSD use techniques which are similar to ours to gather cooccurrence evidence from bilingual corpus in order either to carry out WSD (Mihalcea and Moldovan, 1999), or to create monolingual sense tagged corpora (Fernández et al., 2004). Other monolingual related approaches somehow explore the already disambiguated or unambiguous words by taking into account the senses of the other words in the sentence in order to disambiguate a given word (Lesk, 1986; Hirst, 1987; Cowie et al., 1992). In this paper we investigate the use of the translation context, that is, the"
2006.eamt-1.28,1994.amta-1.7,0,0.0356531,"surrounding words in a sentence or text. For MT purposes, however, the context may also include the translation in the target language, i.e., words in the text which have already been translated. Although intuitively plausible, this strategy has not been explored specifically for WSD. On the other hand, some related approaches have exploited similar strategies for other purposes. For example, some approaches for MT, especially the statistics-based approaches, make use of the words which have already been translated as context, implicitly accomplishing basic WSD during the translation process (Egedi et al., 1994; Dagan and Itai, 1994). Some approaches for monolingual WSD use techniques which are similar to ours to gather cooccurrence evidence from bilingual corpus in order either to carry out WSD (Mihalcea and Moldovan, 1999), or to create monolingual sense tagged corpora (Fernández et al., 2004). Other monolingual related approaches somehow explore the already disambiguated or unambiguous words by taking into account the senses of the other words in the sentence in order to disambiguate a given word (Lesk, 1986; Hirst, 1987; Cowie et al., 1992). In this paper we investigate the use of the translatio"
2006.eamt-1.28,fernandez-etal-2004-automatic,0,0.0191785,"On the other hand, some related approaches have exploited similar strategies for other purposes. For example, some approaches for MT, especially the statistics-based approaches, make use of the words which have already been translated as context, implicitly accomplishing basic WSD during the translation process (Egedi et al., 1994; Dagan and Itai, 1994). Some approaches for monolingual WSD use techniques which are similar to ours to gather cooccurrence evidence from bilingual corpus in order either to carry out WSD (Mihalcea and Moldovan, 1999), or to create monolingual sense tagged corpora (Fernández et al., 2004). Other monolingual related approaches somehow explore the already disambiguated or unambiguous words by taking into account the senses of the other words in the sentence in order to disambiguate a given word (Lesk, 1986; Hirst, 1987; Cowie et al., 1992). In this paper we investigate the use of the translation context, that is, the surrounding words which have already been translated, as knowledge source for multilingual WSD. We present experiments on the disambiguation of 10 ambiguous verbs in English-Portuguese translation. The target language contextual information is applied by analysing t"
2006.eamt-1.28,P99-1020,0,0.0311478,"uitively plausible, this strategy has not been explored specifically for WSD. On the other hand, some related approaches have exploited similar strategies for other purposes. For example, some approaches for MT, especially the statistics-based approaches, make use of the words which have already been translated as context, implicitly accomplishing basic WSD during the translation process (Egedi et al., 1994; Dagan and Itai, 1994). Some approaches for monolingual WSD use techniques which are similar to ours to gather cooccurrence evidence from bilingual corpus in order either to carry out WSD (Mihalcea and Moldovan, 1999), or to create monolingual sense tagged corpora (Fernández et al., 2004). Other monolingual related approaches somehow explore the already disambiguated or unambiguous words by taking into account the senses of the other words in the sentence in order to disambiguate a given word (Lesk, 1986; Hirst, 1987; Cowie et al., 1992). In this paper we investigate the use of the translation context, that is, the surrounding words which have already been translated, as knowledge source for multilingual WSD. We present experiments on the disambiguation of 10 ambiguous verbs in English-Portuguese translati"
2006.eamt-1.28,W06-2505,1,0.900824,"WordNet (Miller, 1990), to identify the monolingual senses, which are then mapped into the target language translations. However, mapping senses between languages is a very complex issue. One of the reasons for this complexity is the difference in the sense inventories of the languages, as already discussed in (Hutchins and Somers, 1992) and recently evidenced by studies with certain pairs of languages. For example, Bentivogli et al. (2004) investigate the sense inventory discrepancies for English-Italian, Miháltz (2005), for English-Hungarian, Chatterjee et al. (2005), for English-Hindi, and Specia et al. (2006), for English-Portuguese. They show that there is not a one-to-one relation between the number of senses in the source language and their translations into another language. More specifically, they show that many source language senses are translated into a unique target language word, while some senses need to be split into different translations, conveying sense distinctions that only exist in the target language. In addition to the differences in the sense inventory, the disambiguation process can vary according to the application. For instance, in monolingual WSD the main information is th"
2006.eamt-1.28,H05-1097,0,0.0734031,"Missing"
2006.eamt-1.28,C04-1053,0,\N,Missing
2009.eamt-1.5,P07-1111,0,0.0306719,"from MT evaluation, in CE reference translations are not available to compute the quality estimates. Therefore, CE approaches cannot be directly compared to the several recently proposed metrics for sentence-level MT evaluation that also use machine learning algorithms and sometimes similar features to those used in CE. For example, (Kulesza and Shieber, 2004) use Support Vector Machines (SVM) with n-gram precision and other reference-based features to predict if a sentence is produced by a human translator (presumably good) or by a MT system (presumably bad) (human-likeness classification). (Albrecht and Hwa, 2007a) rely on regression-based algorithms and features, like string and syntax matching of the translation over the corresponding references, to measure the quality of sentences as a continuous score. In (Albrecht and Hwa, 2007b), pseudo-references (produced by other MT systems) are used instead of human references, but this scenario with multiple MT systems is different from that of CE. The most comprehensive study on CE at the sentence level to date is that of (Blatz et al., 2004). Multi-layer perceptrons and Naive Bayes are trained on 91 features extracted for translations tagged according to"
2009.eamt-1.5,W06-3114,0,0.0491798,"Missing"
2009.eamt-1.5,P07-1038,0,0.105762,"from MT evaluation, in CE reference translations are not available to compute the quality estimates. Therefore, CE approaches cannot be directly compared to the several recently proposed metrics for sentence-level MT evaluation that also use machine learning algorithms and sometimes similar features to those used in CE. For example, (Kulesza and Shieber, 2004) use Support Vector Machines (SVM) with n-gram precision and other reference-based features to predict if a sentence is produced by a human translator (presumably good) or by a MT system (presumably bad) (human-likeness classification). (Albrecht and Hwa, 2007a) rely on regression-based algorithms and features, like string and syntax matching of the translation over the corresponding references, to measure the quality of sentences as a continuous score. In (Albrecht and Hwa, 2007b), pseudo-references (produced by other MT systems) are used instead of human references, but this scenario with multiple MT systems is different from that of CE. The most comprehensive study on CE at the sentence level to date is that of (Blatz et al., 2004). Multi-layer perceptrons and Naive Bayes are trained on 91 features extracted for translations tagged according to"
2009.eamt-1.5,W04-3250,0,0.211486,"Missing"
2009.eamt-1.5,2004.tmi-1.8,0,0.0221387,"l score, it sould also be viewed as a proxy to some automatic or manual metric, like NIST (Doddington, 2002) or 1-5 adequacy. Other estimates include the time that would be necessary to post-edit such translation, or simply a “good” / “bad” indicator. Differently from MT evaluation, in CE reference translations are not available to compute the quality estimates. Therefore, CE approaches cannot be directly compared to the several recently proposed metrics for sentence-level MT evaluation that also use machine learning algorithms and sometimes similar features to those used in CE. For example, (Kulesza and Shieber, 2004) use Support Vector Machines (SVM) with n-gram precision and other reference-based features to predict if a sentence is produced by a human translator (presumably good) or by a MT system (presumably bad) (human-likeness classification). (Albrecht and Hwa, 2007a) rely on regression-based algorithms and features, like string and syntax matching of the translation over the corresponding references, to measure the quality of sentences as a continuous score. In (Albrecht and Hwa, 2007b), pseudo-references (produced by other MT systems) are used instead of human references, but this scenario with mu"
2009.eamt-1.5,C04-1046,0,0.931944,"ced by a human translator (presumably good) or by a MT system (presumably bad) (human-likeness classification). (Albrecht and Hwa, 2007a) rely on regression-based algorithms and features, like string and syntax matching of the translation over the corresponding references, to measure the quality of sentences as a continuous score. In (Albrecht and Hwa, 2007b), pseudo-references (produced by other MT systems) are used instead of human references, but this scenario with multiple MT systems is different from that of CE. The most comprehensive study on CE at the sentence level to date is that of (Blatz et al., 2004). Multi-layer perceptrons and Naive Bayes are trained on 91 features extracted for translations tagged according to NIST and word error rate. Scores are thresholded to label the 5th or (Quirk, 2004) uses linear regression with features similar to those used in (Blatz et al., 2004) to estimate sentence translation quality considering also a small set of translations manually labeled as correct / incorrect. Models trained on this small dataset (350 sentences) outperform those trained on a larger set of automatically labeled data. Given the small amount of manually annotated data and the fact tha"
2009.eamt-1.5,P02-1040,0,0.0980737,"system and language-pair, it is not clear how results can be generalized. The contribution of different features is not investigated. (Gamon et al., 2005) train an SVM classifier using a number of linguistic features (grammar productions, semantic relationships, etc.) extracted from machine and human translations to distinguish between human and machine translations (human-likeness classification). The predictions of SVM, when combined to a 4-gram language model score, only slightly increase the correlation with human judgements and such correlation is still lower than that achieved by BLEU (Papineni et al., 2002). Moreover, as shown in (Albrecht and Hwa, 2007a), high human-likeness does not necessarily imply good MT quality. Besides estimating the quality of machine translations directly, we use a larger set of features, which are meant to cover many more aspects of the translations. These features are all resource-independent, allowing to generalize this method across translations produced by several MT systems and for different language-pairs. Although our goal is very similar to that of (Blatz et al., 2004; Quirk, 2004), it is not possible to compare our results to these previous works, since we es"
2009.eamt-1.5,W08-0309,0,0.0199833,"Missing"
2009.eamt-1.5,2007.tmi-papers.19,0,0.0496208,"Missing"
2009.eamt-1.5,quirk-2004-training,0,0.55578,"yntax matching of the translation over the corresponding references, to measure the quality of sentences as a continuous score. In (Albrecht and Hwa, 2007b), pseudo-references (produced by other MT systems) are used instead of human references, but this scenario with multiple MT systems is different from that of CE. The most comprehensive study on CE at the sentence level to date is that of (Blatz et al., 2004). Multi-layer perceptrons and Naive Bayes are trained on 91 features extracted for translations tagged according to NIST and word error rate. Scores are thresholded to label the 5th or (Quirk, 2004) uses linear regression with features similar to those used in (Blatz et al., 2004) to estimate sentence translation quality considering also a small set of translations manually labeled as correct / incorrect. Models trained on this small dataset (350 sentences) outperform those trained on a larger set of automatically labeled data. Given the small amount of manually annotated data and the fact that translations come from a single MT system and language-pair, it is not clear how results can be generalized. The contribution of different features is not investigated. (Gamon et al., 2005) train"
2009.eamt-1.5,2005.eamt-1.35,0,0.00583991,"ime. In both cases, none of the features is found to be significantly more relevant than the others. This seems to point out that many of the features are redundant, but this aspect is not investigated. and are therefore independent on MT systems. In the remaining of this paper we first discuss the previous work on CE for MT (Section 2), to then describe our experimental setting (Section 3) and method (Section 4) and present and discuss the results obtained (Sections 5 and 6). 2 Related work Early work on CE for MT aimed at estimating the quality at the word level (Gandrabur and Foster, 2003; Ueffing and Ney, 2005; Kadri and Nie, 2006). Sentence-level CE appears to be a more natural set-up for practical applications of MT. One should consider as real-world scenario for CE an MT system in use, which would provide to the user, together with each sentence translation, an estimate of its quality. If this estimate is in the form a numerical score, it sould also be viewed as a proxy to some automatic or manual metric, like NIST (Doddington, 2002) or 1-5 adequacy. Other estimates include the time that would be necessary to post-edit such translation, or simply a “good” / “bad” indicator. Differently from MT e"
2009.eamt-1.5,2005.eamt-1.15,0,0.544236,"l the 5th or (Quirk, 2004) uses linear regression with features similar to those used in (Blatz et al., 2004) to estimate sentence translation quality considering also a small set of translations manually labeled as correct / incorrect. Models trained on this small dataset (350 sentences) outperform those trained on a larger set of automatically labeled data. Given the small amount of manually annotated data and the fact that translations come from a single MT system and language-pair, it is not clear how results can be generalized. The contribution of different features is not investigated. (Gamon et al., 2005) train an SVM classifier using a number of linguistic features (grammar productions, semantic relationships, etc.) extracted from machine and human translations to distinguish between human and machine translations (human-likeness classification). The predictions of SVM, when combined to a 4-gram language model score, only slightly increase the correlation with human judgements and such correlation is still lower than that achieved by BLEU (Papineni et al., 2002). Moreover, as shown in (Albrecht and Hwa, 2007a), high human-likeness does not necessarily imply good MT quality. Besides estimating"
2009.eamt-1.5,W03-0413,0,0.0983171,"l features except one at a time. In both cases, none of the features is found to be significantly more relevant than the others. This seems to point out that many of the features are redundant, but this aspect is not investigated. and are therefore independent on MT systems. In the remaining of this paper we first discuss the previous work on CE for MT (Section 2), to then describe our experimental setting (Section 3) and method (Section 4) and present and discuss the results obtained (Sections 5 and 6). 2 Related work Early work on CE for MT aimed at estimating the quality at the word level (Gandrabur and Foster, 2003; Ueffing and Ney, 2005; Kadri and Nie, 2006). Sentence-level CE appears to be a more natural set-up for practical applications of MT. One should consider as real-world scenario for CE an MT system in use, which would provide to the user, together with each sentence translation, an estimate of its quality. If this estimate is in the form a numerical score, it sould also be viewed as a proxy to some automatic or manual metric, like NIST (Doddington, 2002) or 1-5 adequacy. Other estimates include the time that would be necessary to post-edit such translation, or simply a “good” / “bad” indicator"
2009.eamt-1.5,I08-1042,0,\N,Missing
2009.mtsummit-papers.16,P07-1111,0,0.0412585,"Missing"
2009.mtsummit-papers.16,C04-1046,0,0.76137,"Missing"
2009.mtsummit-papers.16,W08-0309,0,0.160397,"the other hand, machine translation (MT) systems, and particularly statistical machine translation (SMT), only recently have started to attract language-service providers’ and translators’ attention. As any other CAT tool, MT is seen as an instrument to save translators’ time. As with translation memories, the usual workflow is to apply an MT system and then manually post-edit the translation to correct mistakes. It is nowadays easy to set-up an SMT system from existing tools and parallel data. Moreover, improvements in the average quality of such systems have been observed in the last years (Callison-Burch et al., 2008). However, there is no guarantee that a given translated segment will be good enough for post-edition. Human translators need to read the segment many times to find out that it is better to delete it and start from scratch. The time spent to read a translation and attempt to post-edit it before dropping it out may be even longer than the time to translate the source sentence from scratch. The lack of information about the quality of an SMT system’s output is certainly one of the reasons hampering the use of these systems. The research area addressing this problem is referred to as Confidence E"
2009.mtsummit-papers.16,2005.eamt-1.15,0,0.105902,"Missing"
2009.mtsummit-papers.16,W06-3118,0,0.0558004,"Missing"
2009.mtsummit-papers.16,2009.eamt-smart.4,0,0.0553162,"Missing"
2009.mtsummit-papers.16,2004.tmi-1.8,0,0.0330287,"Missing"
2009.mtsummit-papers.16,W07-0734,0,0.0292442,"Missing"
2009.mtsummit-papers.16,P02-1040,0,0.112776,"Missing"
2009.mtsummit-papers.16,quirk-2004-training,0,0.259893,"Missing"
2009.mtsummit-papers.16,H05-1095,0,0.0180993,"Missing"
2009.mtsummit-papers.16,2006.amta-papers.25,0,0.187713,"Missing"
2009.mtsummit-papers.16,2009.eamt-1.5,1,0.882921,"that is, which can be extracted from any MT system, given only the input (source) and translation (target) sentences, and possibly monolingual or parallel corpora. We call these “black-box” features. The decision to use only black-box features aims to allow performing the task of CE across different MT systems, which may use different frameworks, to which we may not have access. It was also motivated by our observation, in previous work, that for the language pair addressed in this paper, more elaborated (and computationally more costly) features do not yield significant gains in performance (Specia et al., 2009). We extract all linguistic resource- and MT system- independent features that have been proposed in previous work, and also some new features. In what follows, we describe our 77 features, grouped for space reasons. A ‘*’ is used to indicate new features with respect to previous work on CE. • source & target sentence lengths and their ratios • source & target sentence 3-gram language model probability & perplexity • source & target sentence type/token ratio • source sentence 1 to 3-gram frequency statistics in a given frequency quartile of a monolingual corpus • alignment score for source and"
2009.mtsummit-papers.16,D09-1107,0,\N,Missing
2009.mtsummit-papers.16,P07-1038,0,\N,Missing
2010.amta-papers.3,P07-1038,0,0.243096,"s are used as examples of good translations, and machine translations as examples of bad translations. A number of language model and linguistic features are extracted based on the translations and/or references, including branching properties of the parser, function word density, etc. Similarly, Kulesza and Shieber (2004) and Gamon et al. (2005) use a number of reference-based features to predict human-likeness. While this approach has the advantage of not requiring human annotation, the predictions obtained have very low correlation with human judgments, which is an indication, as shown in (Albrecht and Hwa, 2007a), that high human-likeness does not necessarily imply good MT quality and vice-versa. Albrecht and Hwa (2007a) use a regression algorithm with string-based and syntax-based features extracted from MT output, reference translations and target language corpus to improve sentencelevel MT evaluation. Albrecht and Hwa (2007b; 2008) rely instead on pseudo-references, which are translations produced by other MT systems. The training is performed based on 1-5 human judgments for translation fluency and adequacy. This approach is the most closely related to ours, but it does not exploit source depend"
2010.amta-papers.3,W08-0330,0,0.034705,"Missing"
2010.amta-papers.3,C04-1046,0,0.211581,"h can be directly interpreted according to a given task (translation post-editing, for example). CE metrics cannot rely on reference translations, since unseen texts will usually be given to the system for translation. These metrics use features extracted given only the source and translation text, and optionally monolingual and bilingual corpora or information about the MT system used to produce the translations. Such features are given to a machine learning algorithm in order to learn a model to predict quality estimates for a certain language pair from data annotated with automatic scores (Blatz et al., 2004) or directly from data annotated with human scores (Quirk, 2004; Specia et al., 2009). CE metrics have been shown to correlate significantly better with human evaluation than standard metrics like BLEU and NIST (Specia et al., 2010b). Since by definition CE metrics are aimed at estimating the quality of a particular MT system for the translation of a given input segment, they are heavily dependent on features regarding the input segment, that is, features reflecting the difficulty of translating the source segment. Therefore, they are not very suitable for comparing multiple MT systems transla"
2010.amta-papers.3,W08-0309,0,0.224936,"Section 5. 4 Reference-based Evaluation Metrics For reference-based metrics, we rely on the repository of metrics available as part of the A SIYA Toolkit (Gim´enez and M`arquez, 2010a)1 . This includes a rich set of n-gram-based metrics and metrics operating at different linguistic levels (lexical, syntactic and semantic). Linguistic metrics have been shown to produce more reliable system rankings than standard n-gram based metrics, especially when the systems under evaluation are of different natures (Gim´enez and M`arquez, 2007). They have also performed well in recent evaluation campaigns (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). Moreover, they have been shown to present a high degree of complementarity with lexical metrics. Some of the linguistic metrics suffer a substantial decrease as sentencelevel quality predictors, mainly due to parsing errors. Therefore, better results are usually achieved by combining n-gram-based and linguistic metrics (Gim´enez and M`arquez, 2010b). A drawback of linguistic metrics is that they rely on automatic linguistic processors and are, therefore, language dependent and in general much slower to compute than n-gram based metri"
2010.amta-papers.3,W10-1703,0,0.0387091,"nce-based metrics, we rely on the repository of metrics available as part of the A SIYA Toolkit (Gim´enez and M`arquez, 2010a)1 . This includes a rich set of n-gram-based metrics and metrics operating at different linguistic levels (lexical, syntactic and semantic). Linguistic metrics have been shown to produce more reliable system rankings than standard n-gram based metrics, especially when the systems under evaluation are of different natures (Gim´enez and M`arquez, 2007). They have also performed well in recent evaluation campaigns (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). Moreover, they have been shown to present a high degree of complementarity with lexical metrics. Some of the linguistic metrics suffer a substantial decrease as sentencelevel quality predictors, mainly due to parsing errors. Therefore, better results are usually achieved by combining n-gram-based and linguistic metrics (Gim´enez and M`arquez, 2010b). A drawback of linguistic metrics is that they rely on automatic linguistic processors and are, therefore, language dependent and in general much slower to compute than n-gram based metrics. For our experiments we have selected a representative s"
2010.amta-papers.3,P01-1020,0,0.0302396,"ut the CE (Section 3) and evaluation (Section 4) metrics used, and report the experiments combining both (Section 5). 2 Related Work While the combination of MT evaluation and CE metrics has not been attempted before, a number of previous efforts address related issues: using machine learning algorithms and human annotated data for MT evaluation, combining different MT evaluation metrics, using source-dependent features for MT evaluation and attempting to improve MT evaluation at the sentence-level. The first attempt to tackle sentence-level MT evaluation as a learning problem was proposed by Corston-Oliver et al. (2001). A classifier is trained to distinguish between human translations (presumably good) and MT system translations (presumably bad) at the sentence level (human-likeness classification). Reference translations are used as examples of good translations, and machine translations as examples of bad translations. A number of language model and linguistic features are extracted based on the translations and/or references, including branching properties of the parser, function word density, etc. Similarly, Kulesza and Shieber (2004) and Gamon et al. (2005) use a number of reference-based features to p"
2010.amta-papers.3,W10-1751,0,0.0152669,"). We denote by ‘†e ’ the metrics available for translations into English only. In the following, we provide a brief description of the metrics grouped according to the linguistic level at which they operate. 4.1 Lexical Similarity BLEUs (Papineni et al., 2002) Smoothed cumulative 4-gram BLEU score as described by Lin and Och (2004b). NIST (Doddington, 2002) Default cumulative 5gram NIST score. GTMe (Melamed et al., 2003) Three variants of GTM taking different values of the e parame1 http://www.lsi.upc.edu/˜nlp/Asiya ter (e ∈ {1, 2, 3}) weighting the importance of the matching length. METEOR (Denkowski and Lavie, 2010) Four variants of METEOR 1.2: • • • • METEORex → only exact matching. METEORst → stem matching. METEORsy †e → synonym matching. METEORpa → paraphrase matching. ROUGE (Lin and Och, 2004a). Four variants of ROUGE: • ROUGEL → longest common subsequence (LCS). • ROUGES? → skip bigrams with no max-gap-length. • ROUGESU ? → skip bigrams with no max-gap-length, including unigrams. • ROUGEW → weighted longest common subsequence (WLCS) with weighting factor w = 1.2. WER (Word Error Rate) (Nießen et al., 2000) We use −WER to make this into a precision metric. PER (Position-independent Word Error Rate) ("
2010.amta-papers.3,2005.eamt-1.15,0,0.374775,"s a learning problem was proposed by Corston-Oliver et al. (2001). A classifier is trained to distinguish between human translations (presumably good) and MT system translations (presumably bad) at the sentence level (human-likeness classification). Reference translations are used as examples of good translations, and machine translations as examples of bad translations. A number of language model and linguistic features are extracted based on the translations and/or references, including branching properties of the parser, function word density, etc. Similarly, Kulesza and Shieber (2004) and Gamon et al. (2005) use a number of reference-based features to predict human-likeness. While this approach has the advantage of not requiring human annotation, the predictions obtained have very low correlation with human judgments, which is an indication, as shown in (Albrecht and Hwa, 2007a), that high human-likeness does not necessarily imply good MT quality and vice-versa. Albrecht and Hwa (2007a) use a regression algorithm with string-based and syntax-based features extracted from MT output, reference translations and target language corpus to improve sentencelevel MT evaluation. Albrecht and Hwa (2007b; 2"
2010.amta-papers.3,W07-0738,1,0.936531,"Missing"
2010.amta-papers.3,W06-3118,0,0.0262113,"tasets and present the results of our experiments. 5.1 LSP English→Spanish Translations Four datasets were produced in a controlled environment as part of a project with a Language Service Provider (LSP). Each dataset consist of 4,000 Spanish translations for English sentences taken from the Europarl development and test sets provided by WMT08 (Callison-Burch et al., 2008). The translations were produced by training four Statistical MT (SMT) systems on 1.2 million English-Spanish sentence pairs from the Europarl training corpus as also provided by WMT08: Matrax (Simard et al., 2005), Portage (Johnson et al., 2006), Sinuhe (K¨aa¨ ri¨ainen, 2009) and MMR (Maximum Margin Regression) (Saunders, 2008). In the following we anonymize these systems by arbitrarily naming them S1-S4. The translations produced by each system were manually annotated by professional translators with 1-4 scores, which is a range commonly used by them to indicate the quality of translations with respect to the need for post-editing2 : • • • • 1 = requires complete retranslation 2 = post editing quicker than retranslation 3 = little post editing needed 4 = fit for purpose The resulting datasets consist of four sets of 4, 000 distinct"
2010.amta-papers.3,D09-1107,0,0.0295427,"Missing"
2010.amta-papers.3,2004.tmi-1.8,0,0.0241328,"sentence-level MT evaluation as a learning problem was proposed by Corston-Oliver et al. (2001). A classifier is trained to distinguish between human translations (presumably good) and MT system translations (presumably bad) at the sentence level (human-likeness classification). Reference translations are used as examples of good translations, and machine translations as examples of bad translations. A number of language model and linguistic features are extracted based on the translations and/or references, including branching properties of the parser, function word density, etc. Similarly, Kulesza and Shieber (2004) and Gamon et al. (2005) use a number of reference-based features to predict human-likeness. While this approach has the advantage of not requiring human annotation, the predictions obtained have very low correlation with human judgments, which is an indication, as shown in (Albrecht and Hwa, 2007a), that high human-likeness does not necessarily imply good MT quality and vice-versa. Albrecht and Hwa (2007a) use a regression algorithm with string-based and syntax-based features extracted from MT output, reference translations and target language corpus to improve sentencelevel MT evaluation. Al"
2010.amta-papers.3,W07-0734,0,0.0425154,"development and system comparison. The most commonly used metrics like ´ Gim´enez Jesus TALP Research Center, LSI Department, Universitat Polit`ecnica de Catalunya, Barcelona, Spain jgimenez@lsi.upc.edu BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) are based on reference translations to compute some form of overlap between n-grams in the MT system output and one or more human translations. More complex reference-based metrics replace or complement n-gram matching with alternative lexical features, such as lemma- or synonymbased alignment between the machine and human translations (Lavie and Agarwal, 2007), or sometimes use syntactic and semantic features, such as the matching of syntactic constituents, dependency relations or semantic roles (Liu and Gildea, 2005; Gim´enez and M`arquez, 2010b). Although evaluation campaigns have shown that such metrics correlate reasonably well with human judgments at the corpus level, their correlation at the segment level (e.g. sentences) is usually much lower. While recent metrics like METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006a) try to overcome this limitation, segment-level scoring is still a limitation, particularly for the de facto met"
2010.amta-papers.3,P04-1077,0,0.0258038,"metrics. For our experiments we have selected a representative set of 52 metrics. All these metrics are available for translations into English (datasets described in Section 5.2), however, only 28 of them are available for translations into Spanish (datasets described in Section 5.1). We denote by ‘†e ’ the metrics available for translations into English only. In the following, we provide a brief description of the metrics grouped according to the linguistic level at which they operate. 4.1 Lexical Similarity BLEUs (Papineni et al., 2002) Smoothed cumulative 4-gram BLEU score as described by Lin and Och (2004b). NIST (Doddington, 2002) Default cumulative 5gram NIST score. GTMe (Melamed et al., 2003) Three variants of GTM taking different values of the e parame1 http://www.lsi.upc.edu/˜nlp/Asiya ter (e ∈ {1, 2, 3}) weighting the importance of the matching length. METEOR (Denkowski and Lavie, 2010) Four variants of METEOR 1.2: • • • • METEORex → only exact matching. METEORst → stem matching. METEORsy †e → synonym matching. METEORpa → paraphrase matching. ROUGE (Lin and Och, 2004a). Four variants of ROUGE: • ROUGEL → longest common subsequence (LCS). • ROUGES? → skip bigrams with no max-gap-length. •"
2010.amta-papers.3,C04-1072,0,0.0387355,"metrics. For our experiments we have selected a representative set of 52 metrics. All these metrics are available for translations into English (datasets described in Section 5.2), however, only 28 of them are available for translations into Spanish (datasets described in Section 5.1). We denote by ‘†e ’ the metrics available for translations into English only. In the following, we provide a brief description of the metrics grouped according to the linguistic level at which they operate. 4.1 Lexical Similarity BLEUs (Papineni et al., 2002) Smoothed cumulative 4-gram BLEU score as described by Lin and Och (2004b). NIST (Doddington, 2002) Default cumulative 5gram NIST score. GTMe (Melamed et al., 2003) Three variants of GTM taking different values of the e parame1 http://www.lsi.upc.edu/˜nlp/Asiya ter (e ∈ {1, 2, 3}) weighting the importance of the matching length. METEOR (Denkowski and Lavie, 2010) Four variants of METEOR 1.2: • • • • METEORex → only exact matching. METEORst → stem matching. METEORsy †e → synonym matching. METEORpa → paraphrase matching. ROUGE (Lin and Och, 2004a). Four variants of ROUGE: • ROUGEL → longest common subsequence (LCS). • ROUGES? → skip bigrams with no max-gap-length. •"
2010.amta-papers.3,W05-0904,0,0.223889,", Barcelona, Spain jgimenez@lsi.upc.edu BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) are based on reference translations to compute some form of overlap between n-grams in the MT system output and one or more human translations. More complex reference-based metrics replace or complement n-gram matching with alternative lexical features, such as lemma- or synonymbased alignment between the machine and human translations (Lavie and Agarwal, 2007), or sometimes use syntactic and semantic features, such as the matching of syntactic constituents, dependency relations or semantic roles (Liu and Gildea, 2005; Gim´enez and M`arquez, 2010b). Although evaluation campaigns have shown that such metrics correlate reasonably well with human judgments at the corpus level, their correlation at the segment level (e.g. sentences) is usually much lower. While recent metrics like METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006a) try to overcome this limitation, segment-level scoring is still a limitation, particularly for the de facto metrics BLEU and NIST. Moreover, the scores given by existing metrics usually cannot be interpreted in absolute terms. For example, it is difficult to reason about"
2010.amta-papers.3,N07-1006,0,0.0122233,"pseudo-references, which are translations produced by other MT systems. The training is performed based on 1-5 human judgments for translation fluency and adequacy. This approach is the most closely related to ours, but it does not exploit source dependent and other CE features. Pad´o et al. (2009) use a regression algorithm with features motivated by textual entailment between the translation and the reference sentences, along with lexical similarity and other linguistic features to predict pairwise preference judgments among MT hypotheses. Source-dependent or other CE features are not used. Liu and Gildea (2007) exploit features that constrain the reference-based n-grams matchings according to the input segments. For example, they constrain the matching of words in the reference and MT output to those cases which are aligned to the same words in the source sentence. The features are combined using a learning framework trained to maximize the Pearson correlation of the combination of features with human judgments. Source features which are independent from the reference translations are not used. To the best of our knowledge, the approach presented in this paper is the first to use a learning framewor"
2010.amta-papers.3,N03-2021,0,0.0226887,"ese metrics are available for translations into English (datasets described in Section 5.2), however, only 28 of them are available for translations into Spanish (datasets described in Section 5.1). We denote by ‘†e ’ the metrics available for translations into English only. In the following, we provide a brief description of the metrics grouped according to the linguistic level at which they operate. 4.1 Lexical Similarity BLEUs (Papineni et al., 2002) Smoothed cumulative 4-gram BLEU score as described by Lin and Och (2004b). NIST (Doddington, 2002) Default cumulative 5gram NIST score. GTMe (Melamed et al., 2003) Three variants of GTM taking different values of the e parame1 http://www.lsi.upc.edu/˜nlp/Asiya ter (e ∈ {1, 2, 3}) weighting the importance of the matching length. METEOR (Denkowski and Lavie, 2010) Four variants of METEOR 1.2: • • • • METEORex → only exact matching. METEORst → stem matching. METEORsy †e → synonym matching. METEORpa → paraphrase matching. ROUGE (Lin and Och, 2004a). Four variants of ROUGE: • ROUGEL → longest common subsequence (LCS). • ROUGES? → skip bigrams with no max-gap-length. • ROUGESU ? → skip bigrams with no max-gap-length, including unigrams. • ROUGEW → weighted lo"
2010.amta-papers.3,niessen-etal-2000-evaluation,0,0.0820569,"/˜nlp/Asiya ter (e ∈ {1, 2, 3}) weighting the importance of the matching length. METEOR (Denkowski and Lavie, 2010) Four variants of METEOR 1.2: • • • • METEORex → only exact matching. METEORst → stem matching. METEORsy †e → synonym matching. METEORpa → paraphrase matching. ROUGE (Lin and Och, 2004a). Four variants of ROUGE: • ROUGEL → longest common subsequence (LCS). • ROUGES? → skip bigrams with no max-gap-length. • ROUGESU ? → skip bigrams with no max-gap-length, including unigrams. • ROUGEW → weighted longest common subsequence (WLCS) with weighting factor w = 1.2. WER (Word Error Rate) (Nießen et al., 2000) We use −WER to make this into a precision metric. PER (Position-independent Word Error Rate) (Tillmann et al., 1997) We use −PER. TER (Translation Edit Rate) (Snover et al., 2006b) Four variants of −TER: • TER→ default (i.e., no paraphrases). • TERbase → base (i.e., no stemming, no synonymy, no paraphrases). • TERp †e → with phrase substitutions. • TERpA †e → tuned towards adequacy. Ol (Lexical overlap) (Gim´enez and M`arquez, 2010b). This metric is a particular instance of a more general Overlap metric. System and reference translations are considered as unordered sets of linguistic elements"
2010.amta-papers.3,J03-1002,0,0.00338274,"nce type/token ratio • average source word length • average number of occurrences of all target words within the target sentence • source & target sentence 3-gram language model probabilities and perplexities obtained using large monolingual corpora • target sentence 3-gram language model probability trained on a corpus of POS-tags of words • percentage of 1 to 3-grams in the source sentence belonging to each frequency quartile of a large monolingual corpus • alignment score (IBM Model 4) for source and target sentences and percentage of different types of word alignments, as given by GIZA++ (Och and Ney, 2003) using a large parallel corpus (∼1.2 million sentences) • average number of translations per source word in the sentence (as given by probabilistic dictionaries like IBM Model 1), unweighted or weighted by the (inverse) frequency of the words • percentages of numbers, content- / non-content words in the source & target sentences • number of mismatching opening/closing brackets and quotation marks in the target sentence • percentages and number of mismatches of each of the following superficial constructions between the source and target sentences: brackets, punctuation symbols, numbers. The da"
2010.amta-papers.3,W09-0404,0,0.0714141,"Missing"
2010.amta-papers.3,P02-1040,0,0.0827884,"input segment, and therefore are less appropriate for comparing multiple systems translating the same input segments. We show that these two classes of metrics are complementary and can be combined to provide MT evaluation metrics that achieve higher correlation with human judgments at the segment level. 1 Introduction Machine Translation (MT) evaluation metrics are essential for system development and system comparison. The most commonly used metrics like ´ Gim´enez Jesus TALP Research Center, LSI Department, Universitat Polit`ecnica de Catalunya, Barcelona, Spain jgimenez@lsi.upc.edu BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) are based on reference translations to compute some form of overlap between n-grams in the MT system output and one or more human translations. More complex reference-based metrics replace or complement n-gram matching with alternative lexical features, such as lemma- or synonymbased alignment between the machine and human translations (Lavie and Agarwal, 2007), or sometimes use syntactic and semantic features, such as the matching of syntactic constituents, dependency relations or semantic roles (Liu and Gildea, 2005; Gim´enez and M`arquez, 2010b). Although evalua"
2010.amta-papers.3,quirk-2004-training,0,0.275397,"st-editing, for example). CE metrics cannot rely on reference translations, since unseen texts will usually be given to the system for translation. These metrics use features extracted given only the source and translation text, and optionally monolingual and bilingual corpora or information about the MT system used to produce the translations. Such features are given to a machine learning algorithm in order to learn a model to predict quality estimates for a certain language pair from data annotated with automatic scores (Blatz et al., 2004) or directly from data annotated with human scores (Quirk, 2004; Specia et al., 2009). CE metrics have been shown to correlate significantly better with human evaluation than standard metrics like BLEU and NIST (Specia et al., 2010b). Since by definition CE metrics are aimed at estimating the quality of a particular MT system for the translation of a given input segment, they are heavily dependent on features regarding the input segment, that is, features reflecting the difficulty of translating the source segment. Therefore, they are not very suitable for comparing multiple MT systems translating the same input segments. While the two types of metrics ha"
2010.amta-papers.3,2006.amta-papers.25,0,0.706737,"ative lexical features, such as lemma- or synonymbased alignment between the machine and human translations (Lavie and Agarwal, 2007), or sometimes use syntactic and semantic features, such as the matching of syntactic constituents, dependency relations or semantic roles (Liu and Gildea, 2005; Gim´enez and M`arquez, 2010b). Although evaluation campaigns have shown that such metrics correlate reasonably well with human judgments at the corpus level, their correlation at the segment level (e.g. sentences) is usually much lower. While recent metrics like METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006a) try to overcome this limitation, segment-level scoring is still a limitation, particularly for the de facto metrics BLEU and NIST. Moreover, the scores given by existing metrics usually cannot be interpreted in absolute terms. For example, it is difficult to reason about a BLEU score of 0.35 in terms of translation quality, as such a score heavily depends on the corpus used for the evaluation (size, distribution of n-grams, etc.), the number of reference translations available and the type of MT system (rule-based, statistical, etc.), among other factors. Confidence Estimation (CE) metrics,"
2010.amta-papers.3,2009.eamt-1.5,1,0.95375,"or example). CE metrics cannot rely on reference translations, since unseen texts will usually be given to the system for translation. These metrics use features extracted given only the source and translation text, and optionally monolingual and bilingual corpora or information about the MT system used to produce the translations. Such features are given to a machine learning algorithm in order to learn a model to predict quality estimates for a certain language pair from data annotated with automatic scores (Blatz et al., 2004) or directly from data annotated with human scores (Quirk, 2004; Specia et al., 2009). CE metrics have been shown to correlate significantly better with human evaluation than standard metrics like BLEU and NIST (Specia et al., 2010b). Since by definition CE metrics are aimed at estimating the quality of a particular MT system for the translation of a given input segment, they are heavily dependent on features regarding the input segment, that is, features reflecting the difficulty of translating the source segment. Therefore, they are not very suitable for comparing multiple MT systems translating the same input segments. While the two types of metrics have different objective"
2010.amta-papers.3,specia-etal-2010-dataset,1,0.841825,"s use features extracted given only the source and translation text, and optionally monolingual and bilingual corpora or information about the MT system used to produce the translations. Such features are given to a machine learning algorithm in order to learn a model to predict quality estimates for a certain language pair from data annotated with automatic scores (Blatz et al., 2004) or directly from data annotated with human scores (Quirk, 2004; Specia et al., 2009). CE metrics have been shown to correlate significantly better with human evaluation than standard metrics like BLEU and NIST (Specia et al., 2010b). Since by definition CE metrics are aimed at estimating the quality of a particular MT system for the translation of a given input segment, they are heavily dependent on features regarding the input segment, that is, features reflecting the difficulty of translating the source segment. Therefore, they are not very suitable for comparing multiple MT systems translating the same input segments. While the two types of metrics have different objectives, we believe that the advantages of both can be exploited by combining them. We aim to enrich reference-based MT evaluation metrics by using the"
2010.eamt-1.31,N06-1003,0,0.193211,"aspects of the process that generated them, such as the appropriateness of the replacement in the context of the specific source sentence, allowing for example reach to be preferred to strike or attack in replacing hit in “We hit the city at lunch time”. Dynamic and static biphrases compete during the search for an optimal translation. At training time, standard techniques such as MERT (Minimum Error Rate Training) (Och, 2003), which attempt to maximize automatic metrics like BLEU (Papineni et al., 2002) based on a bilingual corpus, are directly applicable. However, as has been discussed in (Callison-Burch et al., 2006; Mirkin et al., 2009), such automatic measures are poor indicators of improvements in translation quality in presence of semantic modifications of the kind we are considering here. Therefore, we perform the training and evaluation on the basis of human annotations. We use a form of active learning to focus the annotation effort on a small set of candidates which are useful for the training. Sentences containing OOV words represent a fairly small fraction of the sentences to be translated2 . Thus, to avoid human annotation of a large sample with relatively few cases of OOV words, for the purpo"
2010.eamt-1.31,W08-0309,0,0.0194641,"Missing"
2010.eamt-1.31,2005.mtsummit-papers.30,0,0.0118202,"sters”, where the rank between clusters is clear, but the elements inside each cluster are considered indistinguishable. The annotators are also asked to concentrate their judgment on the portions of the sentences which are affected by the different replacements. To cover potential cases of cognates, annotators can choose the actual OOV as the best “replacement”. Active sampling In order to keep the sample of candidate translations to be annotated for a given OOV source sentence small, but still informative for training, we adopt an active learning scheme (Settles, 2010; Haffari et al., 2009; Eck et al., 2005). We do not extract a priori a sample of translation candidates for each sentence in the OOV training set and ask the annotators to work on these samples — which would mean that they might have to compare candidates that have little chance of being selected by the end-model after training. Instead, This is an iterative process, with a slice of the OOV training set selected for each iteration. When sampling candidate translations (out of a given slice of the OOV training set) to be annotated in the next iteration, we use the translations produced by the model Λ ⊕ M obtained so far, after traini"
2010.eamt-1.31,N09-1047,0,0.0131136,"in a few distinct “clusters”, where the rank between clusters is clear, but the elements inside each cluster are considered indistinguishable. The annotators are also asked to concentrate their judgment on the portions of the sentences which are affected by the different replacements. To cover potential cases of cognates, annotators can choose the actual OOV as the best “replacement”. Active sampling In order to keep the sample of candidate translations to be annotated for a given OOV source sentence small, but still informative for training, we adopt an active learning scheme (Settles, 2010; Haffari et al., 2009; Eck et al., 2005). We do not extract a priori a sample of translation candidates for each sentence in the OOV training set and ask the annotators to work on these samples — which would mean that they might have to compare candidates that have little chance of being selected by the end-model after training. Instead, This is an iterative process, with a slice of the OOV training set selected for each iteration. When sampling candidate translations (out of a given slice of the OOV training set) to be annotated in the next iteration, we use the translations produced by the model Λ ⊕ M obtained s"
2010.eamt-1.31,J10-4005,0,0.0120582,"us the annotation task on the specific problem of sentences containing OOV words, and (ii) even for these sentences, we should only hand the annotators a small, well-chosen, sample of translation candidates to assess, not an exhaustive list. Finally, we need to be careful not to bias training towards the human annotated sample in such a way that the integrated decoder becomes better on the OOV sentences, but is degraded on the “normal” sentences. We address these requirements as follows. 2 Integrated decoding The integrated decoder consists of a standard phrase-based SMT decoder (Lopez, 2008; Koehn, 2010) enhanced with the ability to add dynamic biphrases at runtime and attempting to maximize a variant of the standard “log-linear” objective function. The standard SMT decoder tries to find argmax(a,t) Λ · G(s, t, a), where Λ is a vector of weights, and G(s, t, a) a vector of features depending on the source sentence s, the target sentence t and the phrase-level alignment a. The integrated decoder tries to find argmax(a,t) Λ · G(s, t, a) + M · H(s, t, a) where M is an additional vector of weights and H(s, t, a) an additional vector of “dynamic” features associated with the dynamic biphrases and"
2010.eamt-1.31,D09-1040,0,0.228261,"al data are scarce or the text to be translated is not from the same domain as the data used to train the system. One approach consists in replacing the OOV word by a paraphrase, i.e. a word that is equivalent and known to the phrase-table. For instance, in the sentence “The police hit the protester”, if the source word “hit” is OOV, it could be replaced by its paraphrase “struck”. In previous work such paraphrases are learnt by “pivoting” through parallel texts involving multiple languages (CallisonBurch et al., 2006) or on the basis of monolingual data and distributional similarity metrics (Marton et al., 2009). Mirkin et al. (2009) go beyond the use of paraphrase to incorporate the notion of an entailed phrase, that is, a word which is implied by the OOV word, but is not necessarily equivalent to it — for example, this could result in “hit” being replaced by the entailed phrase “attacked”. Both paraphrases and entailed phrases are obtained using monolingual resources such as WordNet (Fellbaum, 1998). This approach results in higher coverage and human acceptability of the translations produced relative to approaches based only on paraphrases. In (Mirkin et al., 2009) a replacement for the OOV word i"
2010.eamt-1.31,P04-1036,0,0.029228,"he vectors of all the content words in it. 3 Technically, this ratio is only defined for π (M ) (X) 6= 0, i.e. for cases where the pair of translations differ in their M projections; in the rare instances where this might not be true, we can simply ignore the pair in the learning process. Domain similarity Score representing how well rep can replace oov in general in texts of a given domain. It is computed as the cosine similarity between the LSA vectors of the two words and is intended to give preference to replacements which correspond to more frequent senses of the OOV word in that domain (McCarthy et al., 2004). Information loss Measures the distance in WordNet’s hierarchy, denoted d, between oov and rep: 1 S(unk, sub) = 1 − ( d+1 ), where the distance between synonyms is 0, and the further the hypernym is up the hierarchy, the smaller the score. This can be considered a simple approximation to the notion of information loss, that is, the further the rep is from the oov in a hierarchy, the fewer semantic traits exist between the two, and therefore the more information is lost if we use rep. Identity Binary feature to mark the cases where the OOV is kept in the sentence, what we call an “identity” re"
2010.eamt-1.31,P09-1089,1,0.237426,"the text to be translated is not from the same domain as the data used to train the system. One approach consists in replacing the OOV word by a paraphrase, i.e. a word that is equivalent and known to the phrase-table. For instance, in the sentence “The police hit the protester”, if the source word “hit” is OOV, it could be replaced by its paraphrase “struck”. In previous work such paraphrases are learnt by “pivoting” through parallel texts involving multiple languages (CallisonBurch et al., 2006) or on the basis of monolingual data and distributional similarity metrics (Marton et al., 2009). Mirkin et al. (2009) go beyond the use of paraphrase to incorporate the notion of an entailed phrase, that is, a word which is implied by the OOV word, but is not necessarily equivalent to it — for example, this could result in “hit” being replaced by the entailed phrase “attacked”. Both paraphrases and entailed phrases are obtained using monolingual resources such as WordNet (Fellbaum, 1998). This approach results in higher coverage and human acceptability of the translations produced relative to approaches based only on paraphrases. In (Mirkin et al., 2009) a replacement for the OOV word is chosen based on a sc"
2010.eamt-1.31,P03-1021,0,0.00474717,"rapp´e) and (hit, a attaqu´e) from the static ones (struck, a frapp´e) and (attacked, a attaqu´e). Such dynamic biphrases are assigned several features that characterize different aspects of the process that generated them, such as the appropriateness of the replacement in the context of the specific source sentence, allowing for example reach to be preferred to strike or attack in replacing hit in “We hit the city at lunch time”. Dynamic and static biphrases compete during the search for an optimal translation. At training time, standard techniques such as MERT (Minimum Error Rate Training) (Och, 2003), which attempt to maximize automatic metrics like BLEU (Papineni et al., 2002) based on a bilingual corpus, are directly applicable. However, as has been discussed in (Callison-Burch et al., 2006; Mirkin et al., 2009), such automatic measures are poor indicators of improvements in translation quality in presence of semantic modifications of the kind we are considering here. Therefore, we perform the training and evaluation on the basis of human annotations. We use a form of active learning to focus the annotation effort on a small set of candidates which are useful for the training. Sentences"
2010.eamt-1.31,P02-1040,0,0.0818727,"´e) and (attacked, a attaqu´e). Such dynamic biphrases are assigned several features that characterize different aspects of the process that generated them, such as the appropriateness of the replacement in the context of the specific source sentence, allowing for example reach to be preferred to strike or attack in replacing hit in “We hit the city at lunch time”. Dynamic and static biphrases compete during the search for an optimal translation. At training time, standard techniques such as MERT (Minimum Error Rate Training) (Och, 2003), which attempt to maximize automatic metrics like BLEU (Papineni et al., 2002) based on a bilingual corpus, are directly applicable. However, as has been discussed in (Callison-Burch et al., 2006; Mirkin et al., 2009), such automatic measures are poor indicators of improvements in translation quality in presence of semantic modifications of the kind we are considering here. Therefore, we perform the training and evaluation on the basis of human annotations. We use a form of active learning to focus the annotation effort on a small set of candidates which are useful for the training. Sentences containing OOV words represent a fairly small fraction of the sentences to be"
2010.eamt-1.31,H05-1095,1,0.886826,"Missing"
2010.jec-1.5,C04-1046,0,0.766984,"em. This problem has been addressed with metrics of “Confidence Estimation” (CE) for MT. CE metrics use features extracted from the machine translations, and usually also from the source text and monolingual and bilingual corpora, and optionally information about the MT system used to produce the translations. Such features are given to a machine learning algorithm in order to learn a model to predict quality estimates for a certain language pair, MT system and text domain/genre from data annotated with scores reflecting translation quality derived either from automatic MT evaluation metrics (Blatz et al., 2004) such as NIST (Doddington, 2002) and WER (Tillmann et al., 1997) or using human annotation (Quirk, 2004; Specia et al., 2009a). CE metrics may provide a score to end-users of MT systems for each word or phrase (Gandrabur and Foster, 2003; Ueffing and Ney, 2005; Kadri and Nie, 2006), sentence (see Section 2) or document (Soricut and Echihabi, 2010) translated by the MT system. This paper focuses on sentence-level confidence estimation. Early work has focused on binary indicators of translation quality to filter out bad translations from 1 http://www.nlptechnologies.ca Ventsislav Zhechev (ed.):"
2010.jec-1.5,2009.mtsummit-commercial.4,1,0.855819,"Missing"
2010.jec-1.5,W03-0413,0,0.0274954,"and optionally information about the MT system used to produce the translations. Such features are given to a machine learning algorithm in order to learn a model to predict quality estimates for a certain language pair, MT system and text domain/genre from data annotated with scores reflecting translation quality derived either from automatic MT evaluation metrics (Blatz et al., 2004) such as NIST (Doddington, 2002) and WER (Tillmann et al., 1997) or using human annotation (Quirk, 2004; Specia et al., 2009a). CE metrics may provide a score to end-users of MT systems for each word or phrase (Gandrabur and Foster, 2003; Ueffing and Ney, 2005; Kadri and Nie, 2006), sentence (see Section 2) or document (Soricut and Echihabi, 2010) translated by the MT system. This paper focuses on sentence-level confidence estimation. Early work has focused on binary indicators of translation quality to filter out bad translations from 1 http://www.nlptechnologies.ca Ventsislav Zhechev (ed.): Proceedings of the Second Joint EM+/CNGL Workshop “Bringing MT to the User: Research on Integrating MT in the Translation Industry” (JEC ’10), pp. 33–41. Denver, CO, 4 November 2010. 33 JEC 2010 “Bringing MT to the User” being post-edite"
2010.jec-1.5,P10-1064,0,0.229073,"Missing"
2010.jec-1.5,W06-3118,0,0.0200566,"Missing"
2010.jec-1.5,quirk-2004-training,0,0.708476,"s extracted from the machine translations, and usually also from the source text and monolingual and bilingual corpora, and optionally information about the MT system used to produce the translations. Such features are given to a machine learning algorithm in order to learn a model to predict quality estimates for a certain language pair, MT system and text domain/genre from data annotated with scores reflecting translation quality derived either from automatic MT evaluation metrics (Blatz et al., 2004) such as NIST (Doddington, 2002) and WER (Tillmann et al., 1997) or using human annotation (Quirk, 2004; Specia et al., 2009a). CE metrics may provide a score to end-users of MT systems for each word or phrase (Gandrabur and Foster, 2003; Ueffing and Ney, 2005; Kadri and Nie, 2006), sentence (see Section 2) or document (Soricut and Echihabi, 2010) translated by the MT system. This paper focuses on sentence-level confidence estimation. Early work has focused on binary indicators of translation quality to filter out bad translations from 1 http://www.nlptechnologies.ca Ventsislav Zhechev (ed.): Proceedings of the Second Joint EM+/CNGL Workshop “Bringing MT to the User: Research on Integrating MT"
2010.jec-1.5,2006.amta-papers.25,0,0.543652,"correlate very well with human judgments at the segment level. However, producing human annotation is a time-consuming and subjective task, and unless annotators are well trained for the task of assigning absolute quality scores to translations, the scores obtained may be inconsistent and therefore not adequate to train machine learning algorithms. In this paper we exploit a simpler, cheaper and more objective type of score, produced by a semi-automatic MT evaluation metric that is known to correlate well with human judgments at the segment level: Human-targeted Translation Edit Rate (HTER) (Snover et al., 2006). This metric consists in measuring the edit distance between the translation produced by the MT system and its minimally post-edited version produced by a human translator. Our learning framework is therefore trained on HTER scores to directly estimate translation postediting effort in terms of this metric. We show that this metric is more reliable in filtering out bad translations than other simple criteria commonly used in the translation industry, such as sentence length. In the remainder of the paper we first refer to related work on CE (Section 2), then describe our experimental setup (S"
2010.jec-1.5,P10-1063,0,0.048766,"machine learning algorithm in order to learn a model to predict quality estimates for a certain language pair, MT system and text domain/genre from data annotated with scores reflecting translation quality derived either from automatic MT evaluation metrics (Blatz et al., 2004) such as NIST (Doddington, 2002) and WER (Tillmann et al., 1997) or using human annotation (Quirk, 2004; Specia et al., 2009a). CE metrics may provide a score to end-users of MT systems for each word or phrase (Gandrabur and Foster, 2003; Ueffing and Ney, 2005; Kadri and Nie, 2006), sentence (see Section 2) or document (Soricut and Echihabi, 2010) translated by the MT system. This paper focuses on sentence-level confidence estimation. Early work has focused on binary indicators of translation quality to filter out bad translations from 1 http://www.nlptechnologies.ca Ventsislav Zhechev (ed.): Proceedings of the Second Joint EM+/CNGL Workshop “Bringing MT to the User: Research on Integrating MT in the Translation Industry” (JEC ’10), pp. 33–41. Denver, CO, 4 November 2010. 33 JEC 2010 “Bringing MT to the User” being post-edited by professional translators (Blatz et al., 2004; Quirk, 2004). More recent work focuses on estimating a contin"
2010.jec-1.5,2009.eamt-1.5,1,0.942059,"rom the machine translations, and usually also from the source text and monolingual and bilingual corpora, and optionally information about the MT system used to produce the translations. Such features are given to a machine learning algorithm in order to learn a model to predict quality estimates for a certain language pair, MT system and text domain/genre from data annotated with scores reflecting translation quality derived either from automatic MT evaluation metrics (Blatz et al., 2004) such as NIST (Doddington, 2002) and WER (Tillmann et al., 1997) or using human annotation (Quirk, 2004; Specia et al., 2009a). CE metrics may provide a score to end-users of MT systems for each word or phrase (Gandrabur and Foster, 2003; Ueffing and Ney, 2005; Kadri and Nie, 2006), sentence (see Section 2) or document (Soricut and Echihabi, 2010) translated by the MT system. This paper focuses on sentence-level confidence estimation. Early work has focused on binary indicators of translation quality to filter out bad translations from 1 http://www.nlptechnologies.ca Ventsislav Zhechev (ed.): Proceedings of the Second Joint EM+/CNGL Workshop “Bringing MT to the User: Research on Integrating MT in the Translation In"
2010.jec-1.5,2009.mtsummit-papers.16,1,0.936827,"rom the machine translations, and usually also from the source text and monolingual and bilingual corpora, and optionally information about the MT system used to produce the translations. Such features are given to a machine learning algorithm in order to learn a model to predict quality estimates for a certain language pair, MT system and text domain/genre from data annotated with scores reflecting translation quality derived either from automatic MT evaluation metrics (Blatz et al., 2004) such as NIST (Doddington, 2002) and WER (Tillmann et al., 1997) or using human annotation (Quirk, 2004; Specia et al., 2009a). CE metrics may provide a score to end-users of MT systems for each word or phrase (Gandrabur and Foster, 2003; Ueffing and Ney, 2005; Kadri and Nie, 2006), sentence (see Section 2) or document (Soricut and Echihabi, 2010) translated by the MT system. This paper focuses on sentence-level confidence estimation. Early work has focused on binary indicators of translation quality to filter out bad translations from 1 http://www.nlptechnologies.ca Ventsislav Zhechev (ed.): Proceedings of the Second Joint EM+/CNGL Workshop “Bringing MT to the User: Research on Integrating MT in the Translation In"
2010.jec-1.5,specia-etal-2010-dataset,1,0.842876,"uman annotations, the use of the estimated scores in a practical application was not tested. In (Specia et al., 2009b), the technique of Inductive Confidence Machines was used to allow the automatic identification of a threshold to map a continuous predicted score (based on human annotation) into good / bad categories for filtering out bad-quality translations. This threshold is defined according to the expected confidence level of the system. The application of the estimated confidence scores to filter out bad sentences or select the best translation from multiple MT systems is presented in (Specia et al., 2010b). While promising results were found for both applications, the approach is dependent on human annotation to train the system. He et al. (2010) use CE to recommend a translation from either an MT system or a Translation Memory (TM) system for post-editing. Standard translation Edit Rate (TER) is used to measure the distance between a reference translation (produced independently from the MT/TM systems) and each of these systems’ output. This information is used to annotate source sentences with a binary score to indicate its lowest TER (MT or TM) and train a classifier to recommend the trans"
2010.jec-1.5,2005.eamt-1.35,0,0.0159909,"about the MT system used to produce the translations. Such features are given to a machine learning algorithm in order to learn a model to predict quality estimates for a certain language pair, MT system and text domain/genre from data annotated with scores reflecting translation quality derived either from automatic MT evaluation metrics (Blatz et al., 2004) such as NIST (Doddington, 2002) and WER (Tillmann et al., 1997) or using human annotation (Quirk, 2004; Specia et al., 2009a). CE metrics may provide a score to end-users of MT systems for each word or phrase (Gandrabur and Foster, 2003; Ueffing and Ney, 2005; Kadri and Nie, 2006), sentence (see Section 2) or document (Soricut and Echihabi, 2010) translated by the MT system. This paper focuses on sentence-level confidence estimation. Early work has focused on binary indicators of translation quality to filter out bad translations from 1 http://www.nlptechnologies.ca Ventsislav Zhechev (ed.): Proceedings of the Second Joint EM+/CNGL Workshop “Bringing MT to the User: Research on Integrating MT in the Translation Industry” (JEC ’10), pp. 33–41. Denver, CO, 4 November 2010. 33 JEC 2010 “Bringing MT to the User” being post-edited by professional trans"
2011.eamt-1.12,P02-1040,0,0.0966166,"e segments from scratch, without the aid of an MT system. Identifying such segments and filtering them out from the post-editing task is a problem addressed in the field of “Confidence Estimation” (CE), also called “Quality Estimation”, for MT. CE metrics are usually prediction models induced from data using standard machine learning algorithms fed with examples of source and translation features, as well as some form of annotation on the quality of the translations. Early work on sentence-level CE use annotations derived from automatic MT evaluation metrics (Blatz et al., 2004) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and WER (Tillmann et al., 1997) at training time. The resulting models have not been shown to be effective, since the automatic metrics used do not correlate well with human judgments at the segment level and are difficult to interpret as absolute indicators of quality. More recent work focuses on having humans assigning absolute quality scores to translations, which has shown more promising results (Quirk, 2004; Specia et al., 2009a). Obtaining explicit human annotations for translation quality, i.e., absolute scores reflecting postediting effort, can however be a ti"
2011.eamt-1.12,quirk-2004-training,0,0.535915,"f the translations. Early work on sentence-level CE use annotations derived from automatic MT evaluation metrics (Blatz et al., 2004) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and WER (Tillmann et al., 1997) at training time. The resulting models have not been shown to be effective, since the automatic metrics used do not correlate well with human judgments at the segment level and are difficult to interpret as absolute indicators of quality. More recent work focuses on having humans assigning absolute quality scores to translations, which has shown more promising results (Quirk, 2004; Specia et al., 2009a). Obtaining explicit human annotations for translation quality, i.e., absolute scores reflecting postediting effort, can however be a time-consuming and subjective task, requiring well trained annotators. In this paper we contrast prediction models learnt based on this type of annotation against two simpler and more objective variations of response variables: post-editing time and edit distance between automatic and post-edited translaMikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine"
2011.eamt-1.12,2006.amta-papers.25,0,0.708538,"notation types used the experiments, as well as some future work (Section 6). 2 Specia et al. (2009a) use a number of “blackbox” (MT system-independent) and “glass-box” (MT system-dependent) features to train a regression algorithm to estimate both NIST and human scores. While satisfactory accuracies were achieved with human annotations, the use of the estimated scores in a practical application was not tested. He et al. (2010) use CE to recommend, for each source segment, a translation from either an MT system or a Translation Memory (TM) system for post-editing. Translation Edit Rate (TER) (Snover et al., 2006) is used to measure the distance between a reference translation (produced independently from the MT/TM systems) and each of these systems’ output. At training time, this information is used to annotate sentences with a binary score indicating the system with the lowest TER (MT or TM). Based on a number of standard CE features, a classifier is trained to recommend the MT or TM for each new source segment. Therefore, TER is not directly used as an indicator of post-editing effort and is computed using references translations. Specia and Farzindar (2010) computed TER in a different way: between"
2011.eamt-1.12,C04-1046,0,0.491657,"more effort than translating those segments from scratch, without the aid of an MT system. Identifying such segments and filtering them out from the post-editing task is a problem addressed in the field of “Confidence Estimation” (CE), also called “Quality Estimation”, for MT. CE metrics are usually prediction models induced from data using standard machine learning algorithms fed with examples of source and translation features, as well as some form of annotation on the quality of the translations. Early work on sentence-level CE use annotations derived from automatic MT evaluation metrics (Blatz et al., 2004) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and WER (Tillmann et al., 1997) at training time. The resulting models have not been shown to be effective, since the automatic metrics used do not correlate well with human judgments at the segment level and are difficult to interpret as absolute indicators of quality. More recent work focuses on having humans assigning absolute quality scores to translations, which has shown more promising results (Quirk, 2004; Specia et al., 2009a). Obtaining explicit human annotations for translation quality, i.e., absolute scores reflecting po"
2011.eamt-1.12,P10-1063,0,0.167917,"e helpful in a range of evaluation tasks. This may be due to the fact that the automatic metrics used do not correlate well with human judgments. It may be also the case that the translations produced by the SMT systems at that time were too homogeneous in terms of quality: most translations would probably have been considered of bad quality by humans. Quirk (2004) uses classifiers and a pre-defined threshold for “bad” and “good” translations considering a small set of 350 translations manually labeled for quality. Models trained on this dataset outperform those trained on a larger set of autoSoricut and Echihabi (2010) focus on documentlevel CE. The goal is to rank the documents according to their estimated quality and, given a threshold defined by the end-user, select the top n documents for publishing. These are seen as documents whose automatic translation can be “trusted” as good enough for publishing, while the remaining documents are seen as not feasible for machine translation. Document-level CE constitutes a different problem, requiring different features and types of annotations (in their case, BLEU is used). Nevertheless, the view of CE as a ranking task to decide which texts are suitable for MT i"
2011.eamt-1.12,W10-1703,0,0.0687533,"Missing"
2011.eamt-1.12,2010.jec-1.5,1,0.72496,"m for post-editing. Translation Edit Rate (TER) (Snover et al., 2006) is used to measure the distance between a reference translation (produced independently from the MT/TM systems) and each of these systems’ output. At training time, this information is used to annotate sentences with a binary score indicating the system with the lowest TER (MT or TM). Based on a number of standard CE features, a classifier is trained to recommend the MT or TM for each new source segment. Therefore, TER is not directly used as an indicator of post-editing effort and is computed using references translations. Specia and Farzindar (2010) computed TER in a different way: between machine translations and their post-edited versions (HTER). These scores were then used to train a regression algorithm with standard CE features. While promising results were found in terms of correlation with human scores, they were not compared to models using any other form of human annotation. Evaluations with real applications to show the usefulness of the predicted scores were not performed. Related Work Blatz et al. (2004) present a number of experiments with CE at the sentence level based on annotations using automatic MT evaluation metrics. R"
2011.eamt-1.12,2009.eamt-1.5,1,0.676943,"tions. Early work on sentence-level CE use annotations derived from automatic MT evaluation metrics (Blatz et al., 2004) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and WER (Tillmann et al., 1997) at training time. The resulting models have not been shown to be effective, since the automatic metrics used do not correlate well with human judgments at the segment level and are difficult to interpret as absolute indicators of quality. More recent work focuses on having humans assigning absolute quality scores to translations, which has shown more promising results (Quirk, 2004; Specia et al., 2009a). Obtaining explicit human annotations for translation quality, i.e., absolute scores reflecting postediting effort, can however be a time-consuming and subjective task, requiring well trained annotators. In this paper we contrast prediction models learnt based on this type of annotation against two simpler and more objective variations of response variables: post-editing time and edit distance between automatic and post-edited translaMikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 738"
2011.eamt-1.12,2009.mtsummit-papers.16,1,0.944351,"tions. Early work on sentence-level CE use annotations derived from automatic MT evaluation metrics (Blatz et al., 2004) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and WER (Tillmann et al., 1997) at training time. The resulting models have not been shown to be effective, since the automatic metrics used do not correlate well with human judgments at the segment level and are difficult to interpret as absolute indicators of quality. More recent work focuses on having humans assigning absolute quality scores to translations, which has shown more promising results (Quirk, 2004; Specia et al., 2009a). Obtaining explicit human annotations for translation quality, i.e., absolute scores reflecting postediting effort, can however be a time-consuming and subjective task, requiring well trained annotators. In this paper we contrast prediction models learnt based on this type of annotation against two simpler and more objective variations of response variables: post-editing time and edit distance between automatic and post-edited translaMikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 738"
2011.eamt-1.12,P10-1064,0,0.286662,"Missing"
2011.eamt-1.12,specia-etal-2010-dataset,1,0.848655,"ic allow for matching of synonyms and paraphrases (Snover et al., 2010). However, we use standard HTER, which looks for exact matches only, since the post-edited translations here are expected to be as close as possible to the MT output, with only real errors corrected. Edits in HTER include standard insertion, deletion and substitution of single words, as well as the shifting of word sequences. We set HTER options to tokenize the text, ignore case and use equal cost for all edits: The only datasets available from previous work on CE with human annotation focus on a single type of annotation (Specia et al., 2010). In this paper we present new datasets with the three annotation variants. These datasets were collected using news source sentences from development and test sets provided in different years of the WMT (CallisonBurch et al., 2010) evaluation campaign for two language-pairs, with variable sizes. The Moses toolkit (Koehn et al., 2007) was used to build SMT systems to produce translations for these source texts, based on the corpora and guidelines for the baseline system in WMT1 : • fr-en news-test2009: 2,525 French news sentences and their Moses translations into English (corpus-level BLEU = 0"
2011.eamt-1.12,P07-2045,0,0.00593072,"n of single words, as well as the shifting of word sequences. We set HTER options to tokenize the text, ignore case and use equal cost for all edits: The only datasets available from previous work on CE with human annotation focus on a single type of annotation (Specia et al., 2010). In this paper we present new datasets with the three annotation variants. These datasets were collected using news source sentences from development and test sets provided in different years of the WMT (CallisonBurch et al., 2010) evaluation campaign for two language-pairs, with variable sizes. The Moses toolkit (Koehn et al., 2007) was used to build SMT systems to produce translations for these source texts, based on the corpora and guidelines for the baseline system in WMT1 : • fr-en news-test2009: 2,525 French news sentences and their Moses translations into English (corpus-level BLEU = 0.2447). • en-es news-test2010: first 1,000 English news sentences and their Moses translations into Spanish (corpus-level BLEU = 0.2830). HTER = #edits #postedited words Even though the translators used in this paper were trained in the same way, both translation and quality annotations are subjective tasks and as a consequence certai"
2011.eamt-1.12,J03-1002,0,0.00182728,"translator drawn from her experience with previous translation tasks. Once again, this emphasizes the need for CE models built for specific translators. The annotation process resulted in three types of sentence-level annotation for each dataset: • percentage of 1 to 3-grams in the source sentence belonging to each frequency quartile of the source side of the parallel corpus used to train the SMT system 1. Post-editing distance: a continuous score in [0, 1], henceforth: HTER. • average number of translations per source sentence word, as given by probabilistic dictionaries produced by GIZA++ (Och and Ney, 2003) trained on the same parallel corpus used to build the translation model of the SMT system 2. Post-editing effort score: a discrete integer score in {1,2,3,4}, henceforth: effort. 3. Post-editing time: average number of seconds to post-edit each word in the sentence, that is, number of seconds to post-edit the sentence normalized by the number of words in that sentence, henceforth: time. • percentages of numbers, content- / noncontent words in the source & translation sentences The two datasets with these three types of annotations can be downloaded from http: //pers-www.wlv.ac.uk/˜in1316/ res"
2011.mtsummit-papers.58,P11-1022,0,0.105777,"ions (HTER). The estimated scores showed to correlate very well with human post-editing effort. Subsequently, Specia (2011) focuses on a more objective type of annotation: post-editing time. This has shown to be the most useful to allow ranking translations according to the post-editing effort they require. A recent direction in QE is the addition of linguistic information as features. Focusing on word-error detection through the estimation of WER, Xiong et al. (2010) use POS tags of neighbor words and a link grammar parser to indicate words that are not connected to the rest of the sentence. Bach et al. (2011) check whether the dependency relations in the source sentence are preserved in the translation. Both approaches have shown the potential of linguistic features, but only Bach et al. (2011) use features contrasting the source and translation texts. However, these papers either focus on word-level quality estimation or on the estimation of automatic evaluation metrics. Moreover, they do not distinguish the types of errors in terms of ﬂuency and adequacy: a substitution error referring to a simple morphological variation (with no effect on adequacy) is considered in the same way as a content wor"
2011.mtsummit-papers.58,N10-1099,0,0.0282637,"ease refer to (Blatz et al., 2003) for a complete list of source complexity and ﬂuency features. SF - Source complexity features: • • • • source sentence length source sentence type/token ratio average source word length source sentence 3-gram language model probability obtained based on the source side of the parallel corpus used to build the translation model of the SMT system TF - Target ﬂuency features: • target sentence 3-gram language model probability obtained based on a large indomain corpus of the target language • translation sentence length • coherence of the target sentence as in (Burstein et al., 2010) AF - Adequacy features: • ratio of number of tokens in source and target and vice-versa • absolute difference between number of tokens and source and target normalized by source length • ratio of percentages of numbers, content/ non-content words in the source & target • ratio of percentage of nouns/verbs/etc in the source and target • absolute difference between number of superﬁcial constructions in the source and target: brackets, numbers, punctuation symbols • proportion of dependency relations with constituents aligned between source and target • absolute difference between the depth of t"
2011.mtsummit-papers.58,W08-1301,0,0.0446022,"Missing"
2011.mtsummit-papers.58,W10-1751,0,0.0437188,"Missing"
2011.mtsummit-papers.58,P10-1074,0,0.151173,"tem For feature extraction, all the datasets were preprocessed as follows: Arabic (source): word transliteration, segmentation and morphological analysis using MADA (Habash and Rambow, 2005); POS tagging and chunking using AMIRA (Diab, 2009), constituent and dependency parsing using the Stanford parser (Green and Manning, 2010), and NER using a model learned from projections of English named entities (Section 3.2.1). English (target): chunking using OpenNLP2 , constituent and dependency parsing using the Stanford parser (de Marneffe and Manning, 2008), NER using a combination of the Stanford (Finkel and Manning, 2010) and OpenNLP NER systems. 3.2 Features The feature set used in this paper includes features from all categories shown in Figure 2. In total, 122 MT system-independent features were extracted for both S1 ans S2 datasets. In what follows we describe the adequacy features proposed in this paper, as well 2 of translations scored using the scheme MT S1 S2 S1 S2 S1 S2 http://incubator.apache.org/opennlp/ as provide some examples of the non-adequacy related features - please refer to (Blatz et al., 2003) for a complete list of source complexity and ﬂuency features. SF - Source complexity features: •"
2011.mtsummit-papers.58,C10-1045,0,0.0525677,"Missing"
2011.mtsummit-papers.58,P05-1071,0,0.104287,"Missing"
2011.mtsummit-papers.58,P10-1064,0,0.140521,"Missing"
2011.mtsummit-papers.58,J03-1002,0,0.00412171,"and type of entities in the source and target sentences. Since no freely available wide-coverage Named Entity Recognizer (NER) for Arabic exists, we implemented a simple model based on the projection of English NE obtained using a large Arabic-English in-domain parallel corpus. The English side of the parallel corpus is ﬁrst annotated for NEs (Person, Location and Organization). We use both the Stanford (Finkel and Manning, 2010) and the OpenNLP NER systems3 . The English annotations are projected to the Arabic side using word-alignment information. We align the parallel corpus using GIZA++ (Och and Ney, 2003) in the both directions (ar-en and en-ar) to produce the symmetrized alignment using tools provided by the Moses toolkit4 . We then collect entities and their types to compute the context-independent probability distribution p(ar|tag). More speciﬁcally, the word alignment and the source annotation is used to extract synchronous productions in a similar way to the rule extraction in tree-based SMT. The collection of annotated phrases is stored in a rule table with some relevant scores as exempliﬁed by Figure 3. We use the resulting rule table to estimate the probability of assigning a given ent"
2011.mtsummit-papers.58,P02-1040,0,0.088385,"Experiments with Arabic-English translations show that the proposed prediction models can yield more reliable adequacy estimators for new translations. In Section 2 we present related work in the ﬁeld of quality estimation for MT. In Section 3 we describe the proposed approach, the datasets, features, resources and algorithms used. In Section 4 we present our experiments and results. 2 Related Work Most work on sentence-level quality estimation (QE) – also called conﬁdence estimation – proposed so far has focused on (i) estimating general quality scores - such as automatic metrics like BLEU (Papineni et al., 2002) - for tasks like n-best list reordering or MT system selection (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009; Specia et al., 2010) and (ii) estimating post-editing effort (He et al., 2010; Specia and Farzindar, 2010; Specia, 2011). The ﬁrst signiﬁcant effort towards sentence level quality estimation is presented in (Blatz et al., 2003). A large number of source, target and MT system features are used to train machine learning algorithms to estimate automatic metrics such as NIST (Doddington, 2002), which are then thresholded into binary scores to distinguish “good” from “bad” translat"
2011.mtsummit-papers.58,quirk-2004-training,0,0.233792,"ble adequacy estimators for new translations. In Section 2 we present related work in the ﬁeld of quality estimation for MT. In Section 3 we describe the proposed approach, the datasets, features, resources and algorithms used. In Section 4 we present our experiments and results. 2 Related Work Most work on sentence-level quality estimation (QE) – also called conﬁdence estimation – proposed so far has focused on (i) estimating general quality scores - such as automatic metrics like BLEU (Papineni et al., 2002) - for tasks like n-best list reordering or MT system selection (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009; Specia et al., 2010) and (ii) estimating post-editing effort (He et al., 2010; Specia and Farzindar, 2010; Specia, 2011). The ﬁrst signiﬁcant effort towards sentence level quality estimation is presented in (Blatz et al., 2003). A large number of source, target and MT system features are used to train machine learning algorithms to estimate automatic metrics such as NIST (Doddington, 2002), which are then thresholded into binary scores to distinguish “good” from “bad” translations. The results were not very encouraging, possibly due to the fact that the automatic metrics"
2011.mtsummit-papers.58,2006.amta-papers.25,0,0.095262,"anslations. Specia et al. (2009) use similar features to train 514 a regression algorithm on larger datasets annotated by humans for post-editing effort. Satisfactory results were achieved when using the estimated scores for practical applications such as the selection of the best translation among alternatives from different MT systems (Specia et al., 2010). He et al. (2010) propose using QE to recommend a translation from either an MT or a Translation Memory (TM) system for each source segment for post-editing. The QE model is trained on automatic annotation for Translation Edit Rate (TER) (Snover et al., 2006) and the goal is to predict the translation that would yield the minimum edit distance to a reference translation. Specia and Farzindar (2010) use TER to estimate the distance between machine translations and their post-edited versions (HTER). The estimated scores showed to correlate very well with human post-editing effort. Subsequently, Specia (2011) focuses on a more objective type of annotation: post-editing time. This has shown to be the most useful to allow ranking translations according to the post-editing effort they require. A recent direction in QE is the addition of linguistic infor"
2011.mtsummit-papers.58,2010.jec-1.5,1,0.937272,"ion for MT. In Section 3 we describe the proposed approach, the datasets, features, resources and algorithms used. In Section 4 we present our experiments and results. 2 Related Work Most work on sentence-level quality estimation (QE) – also called conﬁdence estimation – proposed so far has focused on (i) estimating general quality scores - such as automatic metrics like BLEU (Papineni et al., 2002) - for tasks like n-best list reordering or MT system selection (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009; Specia et al., 2010) and (ii) estimating post-editing effort (He et al., 2010; Specia and Farzindar, 2010; Specia, 2011). The ﬁrst signiﬁcant effort towards sentence level quality estimation is presented in (Blatz et al., 2003). A large number of source, target and MT system features are used to train machine learning algorithms to estimate automatic metrics such as NIST (Doddington, 2002), which are then thresholded into binary scores to distinguish “good” from “bad” translations. The results were not very encouraging, possibly due to the fact that the automatic metrics used do not correlate well with human judgments at the sentence-level. In fact, Quirk (2004) showed that using a small set of t"
2011.mtsummit-papers.58,2009.eamt-1.5,1,0.841078,"estimators for new translations. In Section 2 we present related work in the ﬁeld of quality estimation for MT. In Section 3 we describe the proposed approach, the datasets, features, resources and algorithms used. In Section 4 we present our experiments and results. 2 Related Work Most work on sentence-level quality estimation (QE) – also called conﬁdence estimation – proposed so far has focused on (i) estimating general quality scores - such as automatic metrics like BLEU (Papineni et al., 2002) - for tasks like n-best list reordering or MT system selection (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009; Specia et al., 2010) and (ii) estimating post-editing effort (He et al., 2010; Specia and Farzindar, 2010; Specia, 2011). The ﬁrst signiﬁcant effort towards sentence level quality estimation is presented in (Blatz et al., 2003). A large number of source, target and MT system features are used to train machine learning algorithms to estimate automatic metrics such as NIST (Doddington, 2002), which are then thresholded into binary scores to distinguish “good” from “bad” translations. The results were not very encouraging, possibly due to the fact that the automatic metrics used do not correlat"
2011.mtsummit-papers.58,2011.eamt-1.12,1,0.939109,"describe the proposed approach, the datasets, features, resources and algorithms used. In Section 4 we present our experiments and results. 2 Related Work Most work on sentence-level quality estimation (QE) – also called conﬁdence estimation – proposed so far has focused on (i) estimating general quality scores - such as automatic metrics like BLEU (Papineni et al., 2002) - for tasks like n-best list reordering or MT system selection (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009; Specia et al., 2010) and (ii) estimating post-editing effort (He et al., 2010; Specia and Farzindar, 2010; Specia, 2011). The ﬁrst signiﬁcant effort towards sentence level quality estimation is presented in (Blatz et al., 2003). A large number of source, target and MT system features are used to train machine learning algorithms to estimate automatic metrics such as NIST (Doddington, 2002), which are then thresholded into binary scores to distinguish “good” from “bad” translations. The results were not very encouraging, possibly due to the fact that the automatic metrics used do not correlate well with human judgments at the sentence-level. In fact, Quirk (2004) showed that using a small set of translations man"
2011.mtsummit-papers.58,P10-1062,0,0.0340104,"ce to a reference translation. Specia and Farzindar (2010) use TER to estimate the distance between machine translations and their post-edited versions (HTER). The estimated scores showed to correlate very well with human post-editing effort. Subsequently, Specia (2011) focuses on a more objective type of annotation: post-editing time. This has shown to be the most useful to allow ranking translations according to the post-editing effort they require. A recent direction in QE is the addition of linguistic information as features. Focusing on word-error detection through the estimation of WER, Xiong et al. (2010) use POS tags of neighbor words and a link grammar parser to indicate words that are not connected to the rest of the sentence. Bach et al. (2011) check whether the dependency relations in the source sentence are preserved in the translation. Both approaches have shown the potential of linguistic features, but only Bach et al. (2011) use features contrasting the source and translation texts. However, these papers either focus on word-level quality estimation or on the estimation of automatic evaluation metrics. Moreover, they do not distinguish the types of errors in terms of ﬂuency and adequa"
2012.amta-wptp.2,aziz-etal-2012-pet,1,0.787719,"et (common) was edited by all the eight post-editors, that is, all of them post-edited the same 20 machine translations. The machine translations in the second dataset (main) were randomly distributed amongst the post-editors so that each of them only edited one translation for a given source sentence, and all of them edited a similar number of translations from each MT system (on average 23 per system). In sum, each post-editor edited 203 sentences (20 in common and 183 in main). For each translation, post-editing effort indicators were logged using PET,1 a freely available postediting tool (Aziz et al., 2012). Among these indicators, of particular interest to our study are: 1 http://pers-www.wlv.ac.uk/˜in1676/pet/ • TIME the post-editing time of a sentence; • SPW seconds per word, that is, the TIME the translator spent to post-edit the sentence divided by the length (in tokens) of the postedited translation; • KEYS the number of keystrokes pressed during the post-editing of the sentence (PKEYS is the subset of printable keys, that is, those that imply a visible change in the text); and • HTER the standard edit distance between the original machine translation and its post-edited version (Snover et"
2012.amta-wptp.2,2011.mtsummit-papers.17,0,0.134559,"ion and its post-edited version have a crucial limitation: they cannot fully capture the effort resulting from post-editing such a translation. Certain operations can be more difficult than others, based not only on the type of edit (deletion, insertion, substitution), but also on the words being edited. Edits due to incorrect morphological variants or function words are generally treated the same way as more complex edits such as fixing an untranslated content word. While variants of such metric assigning weights for specific edits or classes of words can be implemented (Snover et al., 2010; Blain et al., 2011), defining classes of complex words to postedit requires a lexicalised, linguistically- motivated and thus language-dependent approach. In addition, the complexity of a correction cannot always be characterized based only on a local edit, as it may depend on the neighbourhood of that edit. Recently, Koponen (2012) conducted an error analysis on post-edited translations with HTER and 1-5 scores assigned by humans for post-editing effort. A number of cases were found where postedited translations with low HTER (few edits) were assigned low quality scores (high post-editing effort), and vice-vers"
2012.amta-wptp.2,2009.mtsummit-posters.5,0,0.479427,"Missing"
2012.amta-wptp.2,W12-3123,1,0.7105,"ncorrect morphological variants or function words are generally treated the same way as more complex edits such as fixing an untranslated content word. While variants of such metric assigning weights for specific edits or classes of words can be implemented (Snover et al., 2010; Blain et al., 2011), defining classes of complex words to postedit requires a lexicalised, linguistically- motivated and thus language-dependent approach. In addition, the complexity of a correction cannot always be characterized based only on a local edit, as it may depend on the neighbourhood of that edit. Recently, Koponen (2012) conducted an error analysis on post-edited translations with HTER and 1-5 scores assigned by humans for post-editing effort. A number of cases were found where postedited translations with low HTER (few edits) were assigned low quality scores (high post-editing effort), and vice-versa. This seems to indicate that certain edits require more cognitive effort than others, which is not captured by HTER. Post-editing effort consists of different aspects: temporal, technical and cognitive (Krings, 2001). However, these aspects are highly interconnected. The temporal effort (time spent on post-editi"
2012.amta-wptp.2,padro-etal-2010-freeling,0,0.0395098,"Missing"
2012.amta-wptp.2,2006.amta-papers.25,0,0.565698,"as a way of evaluating the quality of machine translations, particularly for the purpose of comparing various MT systems. Effective ways of measuring post-editing effort – and thus MT quality – in both scenarios is a very relevant but open problem. Standard MT evaluation metrics have proved to correlate significantly better with human assessments of quality when computed having a postedited version of the automatic translation as reference as opposed to translations created independently from automatic translations. One of these metrics is HTER – the “Human-targeted Translation Edit Rate” – (Snover et al., 2006), which was used as the official metric in the DARPA GALE program (Olive et al., 2011). HTER is an edit distance metric that computes the minimum number of edits between the system output and its (often minimally) post-edited version. It is a simple metric which has nevertheless shown to be very effective. However, this and other metrics that estimate the similarity or distance between a system translation and its post-edited version have a crucial limitation: they cannot fully capture the effort resulting from post-editing such a translation. Certain operations can be more difficult than othe"
2012.amta-wptp.2,R11-1014,1,0.590295,"ice in the translation industry. Tatsumi (2009) examines the correlation between post-editing time and certain automatic metrics measuring textual differences. They find that the relationship between these two measures is not always linear, and offer some variables such as source sentence length and structure as well as specific types of dependency errors as possible explanations. (Temnikova and Orasan, 2009; Temnikova, 2010) contrast the time translators spent fixing translations for texts produced according to a controlled language, versus translations produced using noncontrolled language. Sousa et al. (2011) compare the time spent on post-editing translations from different MT systems and on translating from scratch. The study has shown that sentences requiring less time to post-edit are more often tagged by humans as demanding low effort. It has also shown that post-editing time has good correlation with HTER for ranking both systems and segments. Specia (2011) uses post-editing time as a way of evaluating quality estimation systems. A comparison is made between the post-editing of sentences predicted to be good and average quality sentences, showing that sentences in the first batch can be post"
2012.amta-wptp.2,2011.eamt-1.12,1,0.737598,"le explanations. (Temnikova and Orasan, 2009; Temnikova, 2010) contrast the time translators spent fixing translations for texts produced according to a controlled language, versus translations produced using noncontrolled language. Sousa et al. (2011) compare the time spent on post-editing translations from different MT systems and on translating from scratch. The study has shown that sentences requiring less time to post-edit are more often tagged by humans as demanding low effort. It has also shown that post-editing time has good correlation with HTER for ranking both systems and segments. Specia (2011) uses post-editing time as a way of evaluating quality estimation systems. A comparison is made between the post-editing of sentences predicted to be good and average quality sentences, showing that sentences in the first batch can be postedited much faster. Focusing on sub-segments, Doherty and O’Brien (2009) use an eye-tracker tool to log the fixation and gaze counts and time of translators while reading the output of an MT system. Overall translation quality was quantified on the basis of the number and the duration of fixations. Results show that fixation counts correlate well with human j"
2012.amta-wptp.2,2009.mtsummit-posters.20,0,0.392199,"h edited word as a separate action, PEAs incorporate several interrelated edit operations. For example, changing a noun propagates changes to its attributes (number, gender) which are then treated as one action. This approach has the disadvantages that it is hardly generalizable across languages, and it requires annotated corpus to train a model to classify PEAs for new texts. A practical alternative, measuring time as a way of assessing post-editing effort, has only recently started to be used by researchers, although we believe this may be a more common practice in the translation industry. Tatsumi (2009) examines the correlation between post-editing time and certain automatic metrics measuring textual differences. They find that the relationship between these two measures is not always linear, and offer some variables such as source sentence length and structure as well as specific types of dependency errors as possible explanations. (Temnikova and Orasan, 2009; Temnikova, 2010) contrast the time translators spent fixing translations for texts produced according to a controlled language, versus translations produced using noncontrolled language. Sousa et al. (2011) compare the time spent on p"
2012.amta-wptp.2,temnikova-2010-cognitive,0,0.334492,"A practical alternative, measuring time as a way of assessing post-editing effort, has only recently started to be used by researchers, although we believe this may be a more common practice in the translation industry. Tatsumi (2009) examines the correlation between post-editing time and certain automatic metrics measuring textual differences. They find that the relationship between these two measures is not always linear, and offer some variables such as source sentence length and structure as well as specific types of dependency errors as possible explanations. (Temnikova and Orasan, 2009; Temnikova, 2010) contrast the time translators spent fixing translations for texts produced according to a controlled language, versus translations produced using noncontrolled language. Sousa et al. (2011) compare the time spent on post-editing translations from different MT systems and on translating from scratch. The study has shown that sentences requiring less time to post-edit are more often tagged by humans as demanding low effort. It has also shown that post-editing time has good correlation with HTER for ranking both systems and segments. Specia (2011) uses post-editing time as a way of evaluating qu"
2012.amta-wptp.2,vilar-etal-2006-error,0,0.0478576,"Missing"
2012.amta-wptp.2,2012.eamt-1.31,0,\N,Missing
2012.amta-wptp.2,2012.tc-1.5,1,\N,Missing
2012.eamt-1.33,aziz-etal-2012-pet,1,0.850289,"Missing"
2012.eamt-1.33,P05-1074,0,0.0416939,"vies and other audiovisual materials. The English-Brazilian Portuguese portion of the corpus amounts to 28 million subtitle pairs. We selected the top 14 million pairs to build a translation model, which we judged to be enough for a PB-SMT system. The data is already automatically pre-processed: tokenized, truecased and word-aligned. To generate the tuning and test sets we took the most recent episodes of three TV series from the same source of fan-made subtitles, which were not included in the Opus release: Dexter (D), How I 4 Experiments with popular methods to generate paraphrases such as (Bannard and Callison-Burch, 2005) resulted in very poor paraphrases for this domain, most likely due to the highly non-literal nature of translations. 5 http://www.opensubtitles.org 107 Models and baselines In all cases, the tuning of the systems was performed individually for each TV series. 4.3 Evaluation In order to objectively evaluate our approach for both translation and compression, we have human translators post-editing the machine translations and collect various information from this process. Meta-information from post-editing has been successfully used in previous work to avoid the subjective nature of explicit sco"
2012.eamt-1.33,P11-2031,0,0.016716,"lators and adapted after a pilot experiment with 150 subtitles post-edited per translators. In a nutshell, translators should minimally correct translations to make them fluent and adequate (style and consistency should be disregarded) and compress them when necessary. The following instructions summarise the guidelines: 5 In this section we discuss the performance of the systems in terms of automatic metrics computed using the human post-edited translations for the 3 test sets (i.e. D, H and T). Note that translation quality and compression are jointly evaluated. We use the multeval toolkit (Clark et al., 2011) to score the systems and test them for statistical significance.7 We report BLEU, TER and the hypothesis length over the reference length in percentage (LENGTH).8 To make the reference set we put together all post-edited translations that were length compliant. In addition, references longer than the ideal length were kept only if no compliant paraphrase was produced by any of the annotators (we observed only 5 of those cases). For all test sets (Tables 1 to 3), systems trained using subtitles data outperform B1 (Google) by a large margin, which shows that parallel subtitles provide phrase pa"
2012.eamt-1.33,D11-1108,0,0.0808727,"Missing"
2012.eamt-1.33,W06-2907,0,0.0607647,"Missing"
2012.eamt-1.33,P07-2045,0,0.00493119,"ntrol of translation quality and compression rate in a single step. Additional paraphrases generated by any means could also be added to the phrase table, for example, following the method in (Ganitkevitch et al., 2011). Compression may incur some loss of information. To prevent unnecessary and excessive compression, we treat compression as a less deterministic process by dynamically modeling the need for compression as a function of the time/space constraints of each specific source segment. Our approach models time/space constraints by (i) adding model components to the Moses PB-SMT system (Koehn et al., 2007) to control the need of compression, and (ii) guiding the tuning process to prefer shorter translations. Each of these strategies is described in what follows. 3.3 Dynamic length penalty Time and space constraints can be represented as a function of the time available for the source subtitle, as described in Section 3.1. In practice, these constraints will affect the length of the target subtitle, and therefore hereafter we refer to them as a length constraint. To incorporate this constraint into the Moses decoder, we define a characterbased length penalty to adjust translations so that they m"
2012.eamt-1.33,W07-0734,0,0.0294797,"ss a dataset for which gold translations are known is used to incrementally tune the model parameters towards improving a measure of quality, traditionally BLEU (Papineni et al., 2002). In order to guide the model to select translation candidates that are likely to be good while complying with the length constraint, at tuning time, when compression is necessary the model must reward phrases that are shorter. This can be done by i) biasing the evaluation metric towards shorter translations (Ganitkevitch et al., 2011); ii) using evaluation metrics that go beyond string matching, such as METEOR (Lavie and Agarwal, 2007), which also matches synonyms and paraphrases; iii) adding multiple reference translations that vary in length; or (iv) filtering the tuning set so that it contains only pairs of segments that comply with the length constraint. These strategies do not necessarily exclude each other, and can rather complement each other. An evaluation metric that rewards compression in general does not suit our application to subtitle translation, where segments should only be compressed when necessary. As for tuning with metrics like METEOR, the lack of quality indomain Portuguese paraphrases for the subtitle"
2012.eamt-1.33,P02-1040,0,0.0855942,"ength. If instead the source subtitle is longer, the model targets the ideal length, aiming at producing a translation that observes the time and space constraints even though the original text is too lengthy. 3.4 Tuning process Adding a new component to the model requires learning its contribution and its interaction with the other components. These model parameters are adjusted in a process often referred to as tuning. In this process a dataset for which gold translations are known is used to incrementally tune the model parameters towards improving a measure of quality, traditionally BLEU (Papineni et al., 2002). In order to guide the model to select translation candidates that are likely to be good while complying with the length constraint, at tuning time, when compression is necessary the model must reward phrases that are shorter. This can be done by i) biasing the evaluation metric towards shorter translations (Ganitkevitch et al., 2011); ii) using evaluation metrics that go beyond string matching, such as METEOR (Lavie and Agarwal, 2007), which also matches synonyms and paraphrases; iii) adding multiple reference translations that vary in length; or (iv) filtering the tuning set so that it cont"
2012.eamt-1.33,piperidis-etal-2004-multimodal,0,0.0719133,"Missing"
2012.eamt-1.33,W00-0506,0,0.113837,"Missing"
2012.eamt-1.33,2006.amta-papers.25,0,0.0604581,"anslation observes time/space constraints. It uses colors to facilitate the visualization of the compression needs and indicates the number of characters that need to be compressed or remain to be used in the translation. Each test set was given to human translators along with the post-editing tool and guidelines for translation correction and compression. Eight Brazilian Portuguese native speakers and fluent speakers of English with significant experience in English-Portuguese translation post-edited the MT outputs. We base our evaluation on the computation of automatic metrics such as HTER (Snover et al., 2006) between the machine translation and its post-edited version (Section 5). 4.3.1 Post-editing guidelines and task design Guidelines and examples of translations were given to the translators and adapted after a pilot experiment with 150 subtitles post-edited per translators. In a nutshell, translators should minimally correct translations to make them fluent and adequate (style and consistency should be disregarded) and compress them when necessary. The following instructions summarise the guidelines: 5 In this section we discuss the performance of the systems in terms of automatic metrics comp"
2012.eamt-1.33,R11-1014,1,0.864466,"hrases for this domain, most likely due to the highly non-literal nature of translations. 5 http://www.opensubtitles.org 107 Models and baselines In all cases, the tuning of the systems was performed individually for each TV series. 4.3 Evaluation In order to objectively evaluate our approach for both translation and compression, we have human translators post-editing the machine translations and collect various information from this process. Meta-information from post-editing has been successfully used in previous work to avoid the subjective nature of explicit scoring schemes (Specia, 2011; Sousa et al., 2011). We use a post-editing tool6 that gathers postediting effort indicators on a per-subtitle basis, including keystrokes, time spent by translators to post-edit the subtitle and the actual post-edited 6 http://pers-www.wlv.ac.uk/˜in1676/pet/ subtitle (Aziz et al., 2012). The tool allows the specification of the length constraints and renders the tasks differently according to how well the translation observes time/space constraints. It uses colors to facilitate the visualization of the compression needs and indicates the number of characters that need to be compressed or remain to be used in the"
2012.eamt-1.33,2011.eamt-1.12,1,0.837329,"ery poor paraphrases for this domain, most likely due to the highly non-literal nature of translations. 5 http://www.opensubtitles.org 107 Models and baselines In all cases, the tuning of the systems was performed individually for each TV series. 4.3 Evaluation In order to objectively evaluate our approach for both translation and compression, we have human translators post-editing the machine translations and collect various information from this process. Meta-information from post-editing has been successfully used in previous work to avoid the subjective nature of explicit scoring schemes (Specia, 2011; Sousa et al., 2011). We use a post-editing tool6 that gathers postediting effort indicators on a per-subtitle basis, including keystrokes, time spent by translators to post-edit the subtitle and the actual post-edited 6 http://pers-www.wlv.ac.uk/˜in1676/pet/ subtitle (Aziz et al., 2012). The tool allows the specification of the length constraints and renders the tasks differently according to how well the translation observes time/space constraints. It uses colors to facilitate the visualization of the compression needs and indicates the number of characters that need to be compressed or rem"
2012.eamt-1.33,W04-1015,0,0.0128661,"Missing"
2012.eamt-1.39,P11-1022,0,0.0233876,"lish translations in groups of news commentary documents in different domains has shown promising results for both sentence and document ranking. 2 Related work A considerable amount of work has been dedicated in recent years to estimating the quality of machine translated texts, i.e., the problem of predicting the quality of translated text without access to reference translations. Most related work focus on predicting different types of sentence-level quality scores, including automatic and semi-automatic MT evaluation metrics such as TER (He et al., 2010), HTER (Specia and Farzindar, 2010; Bach et al., 2011), post-editing effort scores and postediting time (Specia, 2011). At document level, similar to this paper, Soricut and Echihabi (2010) focus on the ranking translated documents according to their estimated quality so that the top n documents can be selected for publishing. A range of indicators from the MT system, source and translation texts have been used in previous work. However, none of these include the notion of informativeness of the texts. The sentence ranking problem has been widely studied in particular for document summarization, where different approaches have been proposed to qu"
2012.eamt-1.39,W11-2103,0,0.030057,"andard and system rankings for the documents are compared. Since there are only five documents within each set of documents, Spearman’s rank correlation coefficient would not be reliable. We instead evaluate the pairwise rankings of documents using Cohen’s Kappa coefficient (κ) (Cohen, (E) 1960), defined as: κ = P (A)−P 1−P (E) , where P (A) is the proportion of times the goldstandard and system ranking agree on the ranking of a pair of documents and P (E) is the proportion of times they could agree by chance. This probability is empirically computed by observing the frequency of ties, as in (Callison-Burch et al., 2011). 6 Experiments and results In what follows we show the results of the quality estimation and relevance ranking methods on their own and then we present the results obtained with the combination of these two methods. 6.1 Quality estimation The performance of the quality estimation method is shown in Table 2. The average regression error is measured using Root Mean Squared Error, q P N 1 bi )2 , where N is the RMSE = i=1 (yi − y N number of test sentences, yb is the predicted score and y is the actual score for that test sentence. The performance is generally lower than what has been reported i"
2012.eamt-1.39,P10-1064,0,0.0534758,"Missing"
2012.eamt-1.39,P04-3020,0,0.0325651,"ly studied in particular for document summarization, where different approaches have been proposed to quantify the amount of information contained in each sentence. In (Goldstein et al., 1999), a technique called Maximal Marginal Relevance (MMR) was introduced to measure the relevance of each sentence in a document according to a user provided query. Other approaches represent a document as a set of trees and take the position of a sentence in a tree is indicative of its importance (Carlson et al., 2001). Graph theory has been extensively used to rank sentences (Yeh et al., 2008) or keywords (Mihalcea, 2004), with their importance determined using graph connectivity measures such as in-degree or PageRank. A sentence extraction method based on Singular Value Decomposition over term-by-sentence matrices was introduced in (Gong and Xin, 2002). The combination of relevance and translation quality scores has been recently proposed in the context of cross-language document summarization. In (Wan et al., 2010), sentences in a document were ranked using the product of quality estimation and relevance scores, both computed using the source text only. The best five sentences were added to a summary, and th"
2012.eamt-1.39,P10-1063,0,0.057947,"d document ranking. 2 Related work A considerable amount of work has been dedicated in recent years to estimating the quality of machine translated texts, i.e., the problem of predicting the quality of translated text without access to reference translations. Most related work focus on predicting different types of sentence-level quality scores, including automatic and semi-automatic MT evaluation metrics such as TER (He et al., 2010), HTER (Specia and Farzindar, 2010; Bach et al., 2011), post-editing effort scores and postediting time (Specia, 2011). At document level, similar to this paper, Soricut and Echihabi (2010) focus on the ranking translated documents according to their estimated quality so that the top n documents can be selected for publishing. A range of indicators from the MT system, source and translation texts have been used in previous work. However, none of these include the notion of informativeness of the texts. The sentence ranking problem has been widely studied in particular for document summarization, where different approaches have been proposed to quantify the amount of information contained in each sentence. In (Goldstein et al., 1999), a technique called Maximal Marginal Relevance"
2012.eamt-1.39,2010.jec-1.5,1,0.790038,"n evaluation with French-English translations in groups of news commentary documents in different domains has shown promising results for both sentence and document ranking. 2 Related work A considerable amount of work has been dedicated in recent years to estimating the quality of machine translated texts, i.e., the problem of predicting the quality of translated text without access to reference translations. Most related work focus on predicting different types of sentence-level quality scores, including automatic and semi-automatic MT evaluation metrics such as TER (He et al., 2010), HTER (Specia and Farzindar, 2010; Bach et al., 2011), post-editing effort scores and postediting time (Specia, 2011). At document level, similar to this paper, Soricut and Echihabi (2010) focus on the ranking translated documents according to their estimated quality so that the top n documents can be selected for publishing. A range of indicators from the MT system, source and translation texts have been used in previous work. However, none of these include the notion of informativeness of the texts. The sentence ranking problem has been widely studied in particular for document summarization, where different approaches have"
2012.eamt-1.39,2011.eamt-1.12,1,0.912684,"domains has shown promising results for both sentence and document ranking. 2 Related work A considerable amount of work has been dedicated in recent years to estimating the quality of machine translated texts, i.e., the problem of predicting the quality of translated text without access to reference translations. Most related work focus on predicting different types of sentence-level quality scores, including automatic and semi-automatic MT evaluation metrics such as TER (He et al., 2010), HTER (Specia and Farzindar, 2010; Bach et al., 2011), post-editing effort scores and postediting time (Specia, 2011). At document level, similar to this paper, Soricut and Echihabi (2010) focus on the ranking translated documents according to their estimated quality so that the top n documents can be selected for publishing. A range of indicators from the MT system, source and translation texts have been used in previous work. However, none of these include the notion of informativeness of the texts. The sentence ranking problem has been widely studied in particular for document summarization, where different approaches have been proposed to quantify the amount of information contained in each sentence. In"
2012.eamt-1.39,P10-1094,0,0.0202742,"f trees and take the position of a sentence in a tree is indicative of its importance (Carlson et al., 2001). Graph theory has been extensively used to rank sentences (Yeh et al., 2008) or keywords (Mihalcea, 2004), with their importance determined using graph connectivity measures such as in-degree or PageRank. A sentence extraction method based on Singular Value Decomposition over term-by-sentence matrices was introduced in (Gong and Xin, 2002). The combination of relevance and translation quality scores has been recently proposed in the context of cross-language document summarization. In (Wan et al., 2010), sentences in a document were ranked using the product of quality estimation and relevance scores, both computed using the source text only. The best five sentences were added to a summary, and then translated to the target language. (Boudin et al., 2010) used both source and target language features for quality estimation and targeted multi-document summarization, selecting sentences from different translated documents to generate a summary. This paper extends previous work in the attempt to rank translated sentences within documents, but 154 with a different objective: instead of selecting"
2012.tc-1.5,aziz-etal-2012-pet,1,0.207823,"Missing"
2012.tc-1.5,2012.eamt-1.33,1,0.674991,"measuring translation quality and diagnosing MT systems. 1 http://www.trados.com/en/ http://www.wordfast.net/ 3 http://translate.google.com/ 4 http://www.systran.co.uk/ 2 1 Figure 1: Annotation window These limitations mostly constrain developers of translation technologies and researchers in machine (or computer-aided) translation. For a detailed study on translation tools that allow post-editing (e.g. Caitra,5 Lingotek,6 D´ej`a Vu X2,7 and OmegaT8 ) and requirements from the human translator’s perspective, we refer the reader to Vieira and Specia (2011). We present PET (Post-Editing Tool) (Aziz et al., 2012a), a simple, freely available opensource standalone tool that allows the PE of any MT system and records various segment-level information. While PET is not yet a full-fledge tool for post-editing, offering limited built-in functionalities (dictionaries, etc.), it offers the flexibility that other tools lack (i) to enable easy design of post-editing tasks with specific requirements (such as constraints on the revisions produced in terms of length, word use, etc.), and (ii) to collect a number of (customisable) effort indicators and statistics on post-editing tasks. 2 Basic functioning PET was"
2012.tc-1.5,2012.amta-wptp.2,1,0.862933,"Missing"
2012.tc-1.5,R11-1014,1,0.843693,"his case, only the post-edited segment and time indicators are logged and the unit was edited only once. Figure 4: Extract of output file 5 Conclusions We have presented a simple tool for post-editing and assessing translations that is MT systemindependent and allows customisation at various levels, including the types of assessments that can be collected and restrictions on the post-editing process. The tool has already been used in different experiments, including (i) comparing different translation systems (Sankaran et al., 2012), (ii) contrasting post-editing and translation from scratch (Sousa et al., 2011), (iii) collecting information to build and compare quality estimation models (Specia, 2011), and (iv) measuring translation quality through post-editing for subtitles, where the tool dynamically restricts the length of each post-edited translation based on the length of the source segment and general time and space constraints for units Aziz et al. (2012b). A more detailed analysis on the use of information collected by the tool for measuring post-editing effort is presented in (Koponen et al., 2012). PET is freely available for download at http://pers-www.wlv.ac.uk/~in1676/pet. Documentation"
2012.tc-1.5,2011.eamt-1.12,1,0.431545,"out the post-editing process that could be used for measuring translation quality and diagnosing MT systems. 1 http://www.trados.com/en/ http://www.wordfast.net/ 3 http://translate.google.com/ 4 http://www.systran.co.uk/ 2 1 Figure 1: Annotation window These limitations mostly constrain developers of translation technologies and researchers in machine (or computer-aided) translation. For a detailed study on translation tools that allow post-editing (e.g. Caitra,5 Lingotek,6 D´ej`a Vu X2,7 and OmegaT8 ) and requirements from the human translator’s perspective, we refer the reader to Vieira and Specia (2011). We present PET (Post-Editing Tool) (Aziz et al., 2012a), a simple, freely available opensource standalone tool that allows the PE of any MT system and records various segment-level information. While PET is not yet a full-fledge tool for post-editing, offering limited built-in functionalities (dictionaries, etc.), it offers the flexibility that other tools lack (i) to enable easy design of post-editing tasks with specific requirements (such as constraints on the revisions produced in terms of length, word use, etc.), and (ii) to collect a number of (customisable) effort indicators and statis"
2012.tc-1.5,P02-1040,0,\N,Missing
2012.tc-1.5,P07-2045,0,\N,Missing
2012.tc-1.5,W10-1703,0,\N,Missing
2013.mtsummit-papers.21,P11-1022,0,0.0622798,". 2 Related work Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting sub-segments that need revision (Bach et al., 2011). For an overview of various algorithms and features we refer the reader to the WMT12 shared task on QE (Callison-Burch et al., 2012). Most previous work on QE use machine learning algorithms such as SVMs, which are robust to redundant/noisy features to a certain extent. In what follows we summarise recent work using explicit feature selection methods in the WMT12 QE shared task. Gonz´alez-Rubio et al. (2012) performed feature selection on a set of 475 sentence- and subsentence level features. Principal Component Analysis and a greedy selection algorithm to iteratively create subsets of increa"
2013.mtsummit-papers.21,C04-1046,0,0.455932,"ed to as Quality Estimation (QE). Different from standard MT evaluation metrics, QE metrics do not have access to reference (human) translations; they are aimed at MT systems in use. Applications of QE include: • Decide which segments need revision by a translator (quality assurance); • Decide whether a reader gets a reliable gist of the text; • Estimate how much effort it will be needed to post-edit a segment; • Select among alternative translations produced by different MT systems. Work in QE started with the goal of estimating automatic metrics such as BLEU (Papineni et al., 2002) and WER (Blatz et al., 2004). However, these metrics are difficult to interpret, particularly at the sentence-level, and results proved unsuccessful. A new surge of interest in the field started recently, motivated by the widespread use of MT systems in the translation industry, as a consequence of better translation quality, more userfriendly tools, and higher demand for translation. In order to make MT maximally useful in this scenario, a quantification of the quality of translated segments similar to “fuzzy match scores” from translation memory systems is needed. QE work addresses this problem by using more complex me"
2013.mtsummit-papers.21,W12-3110,1,0.931089,"set of 475 sentence- and subsentence level features. Principal Component Analysis and a greedy selection algorithm to iteratively create subsets of increasing size with the best-scoring individual features were exploited. Both selection methods yielded better performance than all features, with greedy selection achieving the best MAE scores with 254 features. Langlois et al. (2012) reported positive results with a greedy backward selection algorithm that removes 21 poor features from an initial set of 66 features based on error minimisation on a development set. In an oracle-like experiment, Felice and Specia (2012) use a sequential forward selection method, which starts from an empty set and adds one feature at a time as long as it decreases the model’s error, evaluating the performance of the feature subsets on the test set directly. 37 features out of 147 are selected, and these significantly improved the overall performance. Avramidis (2012) tested a few feature selection methods using both greedy stepwise and best first search to select among their 266 features with 10fold cross-validation on the training set. These resulted in sets of 30-80 features, all outperforming the complete feature set. Corr"
2013.mtsummit-papers.21,W12-3111,0,0.0394924,"Missing"
2013.mtsummit-papers.21,P10-1064,0,0.0653301,"Missing"
2013.mtsummit-papers.21,W12-3113,0,0.128781,"Missing"
2013.mtsummit-papers.21,W12-3114,0,0.0577687,"ime as long as it decreases the model’s error, evaluating the performance of the feature subsets on the test set directly. 37 features out of 147 are selected, and these significantly improved the overall performance. Avramidis (2012) tested a few feature selection methods using both greedy stepwise and best first search to select among their 266 features with 10fold cross-validation on the training set. These resulted in sets of 30-80 features, all outperforming the complete feature set. Correlation-based selection with best first search strategy was reported to perform the best. Conversely, Moreau and Vogel (2012) reported no improvements in performance in experiments with several selection methods. 168 Finally, (Soricut et al., 2012), the winning system in the WMT12 QE shared task, used a computationally-intensive method on a development set. For each of the official evaluation metrics (e.g. MAE), from an initial set of 24 features, all 224 possible combinations were tested, followed by an exhaustive search to find the best combinations. The 15 features belonging to most of the top combinations were selected. Other rounds were added to deal with POS features, but the final feature sets included 14-15"
2013.mtsummit-papers.21,P02-1040,0,0.0857352,"of translations. This is referred to as Quality Estimation (QE). Different from standard MT evaluation metrics, QE metrics do not have access to reference (human) translations; they are aimed at MT systems in use. Applications of QE include: • Decide which segments need revision by a translator (quality assurance); • Decide whether a reader gets a reliable gist of the text; • Estimate how much effort it will be needed to post-edit a segment; • Select among alternative translations produced by different MT systems. Work in QE started with the goal of estimating automatic metrics such as BLEU (Papineni et al., 2002) and WER (Blatz et al., 2004). However, these metrics are difficult to interpret, particularly at the sentence-level, and results proved unsuccessful. A new surge of interest in the field started recently, motivated by the widespread use of MT systems in the translation industry, as a consequence of better translation quality, more userfriendly tools, and higher demand for translation. In order to make MT maximally useful in this scenario, a quantification of the quality of translated segments similar to “fuzzy match scores” from translation memory systems is needed. QE work addresses this pro"
2013.mtsummit-papers.21,P10-1063,0,0.050169,"viously selected using other methods. They also allowed us to identify a small number of wellperforming features across datasets (Section 4). We discuss the feasibility of extracting these features based on their dependence on external resources or specific languages. 2 Related work Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting sub-segments that need revision (Bach et al., 2011). For an overview of various algorithms and features we refer the reader to the WMT12 shared task on QE (Callison-Burch et al., 2012). Most previous work on QE use machine learning algorithms such as SVMs, which are robust to redundant/noisy features to a certain extent. In what follows we summarise recent work using explicit feature select"
2013.mtsummit-papers.21,2009.eamt-1.5,1,0.889115,"re selection to improve overall regression results, often outperforming published results on features that had been previously selected using other methods. They also allowed us to identify a small number of wellperforming features across datasets (Section 4). We discuss the feasibility of extracting these features based on their dependence on external resources or specific languages. 2 Related work Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting sub-segments that need revision (Bach et al., 2011). For an overview of various algorithms and features we refer the reader to the WMT12 shared task on QE (Callison-Burch et al., 2012). Most previous work on QE use machine learning algorithms such as SVMs, which are"
2013.mtsummit-papers.21,W12-3112,0,0.0385844,"on-Burch et al., 2012) and our figures. This difference can also be explained by the learning algorithms: while we used GPs, participants have used SVRs, M5P and other algorithms. Some of these feature sets already result from feature selection techniques. SDL (Soricut et al., 2012): 15 features selected after an exhaustive search algorithm based on all possible combinations of features. This is the optimal set used by the winning submission. It includes many of the baseline features, the pseudoreference feature, phrase table probabilities, and a few part-of-speech tag alignment features. UU (Hardmeier et al., 2012): 82 features, a subset of those used in the shared-task as the parse tree features (based on tree-kernels) were not provided by the participants. These are similar to the common BL and BB features presented above and include various source and target LM features, average number of translations per source word, number of tokens matching certain patterns (hyphens, ellipsis, etc.), percentage of n-grams seen in corpus, percentage of non-aligned words, etc. UEdin (Buck, 2012): 56 black-box features including source translatability, named entities, LM back-off features, discriminative word-lexicon"
2013.mtsummit-papers.21,2011.eamt-1.12,1,0.865351,"ve overall regression results, often outperforming published results on features that had been previously selected using other methods. They also allowed us to identify a small number of wellperforming features across datasets (Section 4). We discuss the feasibility of extracting these features based on their dependence on external resources or specific languages. 2 Related work Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting sub-segments that need revision (Bach et al., 2011). For an overview of various algorithms and features we refer the reader to the WMT12 shared task on QE (Callison-Burch et al., 2012). Most previous work on QE use machine learning algorithms such as SVMs, which are robust to redu"
2013.mtsummit-posters.12,O10-3002,0,0.0338159,"Missing"
2013.mtsummit-posters.13,P11-1022,0,0.0216977,"ddington, 2002)) as the quality label. Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting subsegments that need revision (Bach et al., 2011). For an overview on various feature sets and machine learning algorithms, we refer the reader to the recent shared-task on the topic (Callison-Burch et al., 2012). Most QE work focuses on estimating a score that indicates overall quality having professional translators as intended user, e.g. post-editing effort. Little work has been done for other applications of MT, such as gisting. One notable exception is the work by Specia et al. (2011), where adequacy scores for Arabic-English translations are predicted. The feature set used include standard features that try to capture general aspects o"
2013.mtsummit-posters.13,C04-1046,0,0.238333,"robust than those used in previous work, which depend on linguistic processors that are often unreliable on automatic translations. Experiments with a number of datasets show promising results: the use of topic models outperforms the state-of-the-art approaches by a large margin in all datasets annotated for adequacy. 1 Introduction Quality estimation (QE) for machine translation (MT) is an area concerned with predicting a quality indicator for an automatically translated text without referring to human translations (the so-called reference translations typical of most MT evaluation metrics) (Blatz et al., 2004; Specia et al., 2009). The widespread use of MT in the translation industry has strongly motivated work in this area. As a consequence, the majority of existing work focuses on predicting some form of post-editing effort to help professional translators (Section 2). However, one equally appealing application is that of estimating the adequacy of translations for gisting purposes. An indicator of such a type is particularly relevant in contexts where the reader does not know the source language. QE is generally addressed as a machine learning task. Intuitively, it is expected that features use"
2013.mtsummit-posters.13,W10-1703,0,0.0187796,"est) to 5 scale (lowest). The three scores per segment pair are weighted and averaged in order to obtain one continuous score (∈ [1; 5]). From this dataset, 1, 832 segments are used to train the QE models while 422 segments are used as a test set. To build the topic models, we use the parallel corpora used by the M OSES system which generated the Spanish translations. This 2 3 http://www.tausdata.org/ http://www.bing.com/translator/ 298 corpus contains the concatenation of Europarl v5 and the News Commentary corpus from WMT10 translation task in English and Spanish (∼ 1.7M translation pairs) (Callison-Burch et al., 2010). 4.2 Additional Features In addition to the TM features described in Section 3, for all datasets, for comparison we consider a baseline set of 17 features that performed well across languages in previous work and were used as the official baseline in the WMT12 QE task (Callison-Burch et al., 2012): – number of tokens in the source & target sentences; – average source token length; – average number of occurrences of the target word within the target sentence; – number of punctuation marks in source and target sentences; – language model (LM) probability of source and target sentences using 3-g"
2013.mtsummit-posters.13,W12-3102,1,0.866279,"Missing"
2013.mtsummit-posters.13,P10-1064,0,0.0295252,"Missing"
2013.mtsummit-posters.13,P07-2045,0,0.00602471,"n of all Arabic-English newswire parallel data provided by LDC and the Arabic-English UN data,1 totalling ∼ 6.4M translation pairs after removing sentences longer than 80 words. This was virtually the same parallel corpus used to build the SMT systems. User-Generated Data The user-generated content is composed of 694 sentences taken from an English IT-related online forum, translated into French by three automatic translators considered as black-box systems: M OSES, B ING 1 http://www.uncorpora.org/ and S YSTRAN. The M OSES system is a standard phrase-based SMT system trained using the Moses (Koehn et al., 2007) and IRSTLM (Federico et al., 2008) toolkits and optimised on a development set against BLEU (Papineni et al., 2002) using MERT (Och, 2003). A trigram Language Model (LM) was built using Witten-Bell smoothing. This system was trained using in-domain translation memories (up to ∼ 1.6M translation units) plus ∼ 1M translation units from the Computer Software domain (obtained from the TAUS Data Association2 ) for the translation model, as well as additional monolingual forum data (up to 20K French sentences) for the LM. The B ING system is a freely available generic SMT system, Bing Translator,3"
2013.mtsummit-posters.13,C10-1070,0,0.0138361,"n 2 2 i=1 (ui ) × i=1 (vi ) Instead of measuring the distance between two n-dimensional sets of points, it is also possible to compute the sum of the absolute differences of their coordinates. These metrics, usually categorised as the Minkowski family of distance metrics, are inspired by a grid, city-like, organisation of the space. Usually referred as rectilinear or Manhattan distance, the city-block distance is directly inspired by the Euclidean distance (Krause, 1975) and has shown interesting results when applied to language processing tasks, such as context-based terminology translation (Laroche and Langlais, 2010). The city-block distance between two n-dimensional vectors u and v is given by (3): n X cityblock(u, v) = |ui − vi | (3) i=1 These three metrics allow us to compute the distance between source and target distributions assuming that they are represented in an Euclidean space. To avoid this constraint, we use two other measures in this study, based on probabilistic uncertainty as introduced by Shannon’s work (Shannon, 1948). With the measure of relative entropy, an asymmetric way of comparing two distributions suggested in (Kullback and Leibler, 1951) is given by (4): n X ui KL(u, v) = ui ln (4"
2013.mtsummit-posters.13,W12-3122,0,0.354907,"professional translators (Section 2). However, one equally appealing application is that of estimating the adequacy of translations for gisting purposes. An indicator of such a type is particularly relevant in contexts where the reader does not know the source language. QE is generally addressed as a machine learning task. Intuitively, it is expected that features used to capture general aspects of quality are different from features that define adequacy. Previous work on adequacy estimation has focused on linguistic features contrasting the source and translation texts (Specia et al., 2011; Mehdad et al., 2012), e.g. the proportion of overlapping typed dependency relations in the source and target sentences with arguments that align to each other (based on wordalignment information). While these can provide interesting indicators, they are often very sparse and noisy. Sparsity happens because many of these features do not apply to most sentences, such as features comparing named entities in the source and target sentences. A significant amount of noise can come from the fact that linguistic processors, such as syntactic parsers and named entity recognisers, need to be applied to potentially low-qual"
2013.mtsummit-posters.13,D09-1092,0,0.10337,"ose used in previous work in that they focus on important (content) words in the source and target texts, as opposed to more abstract linguistic relationships between these words. We believe these are more robust as they do not depend on further analysis, and that they can be made less sparse through the exploitation of models with different dimensionalities and the use of distance metrics between topic distributions as opposed to topic distributions themselves. A challenge we face is how to model topics in a bilingual setting. We exploit two variants of TMs for that: Polylingual Topic Model (Mimno et al., 2009) and Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 295–302. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. a joint Latent Dirichlet Allocation approach (Blei et al., 2003), and a few variants of features based on these models, including the word distribution themselves and distance metrics between source and target distributions (Section 3). We experiment with three families of datasets: two annotated for ad"
2013.mtsummit-posters.13,P03-1021,0,0.00239403,"ntences longer than 80 words. This was virtually the same parallel corpus used to build the SMT systems. User-Generated Data The user-generated content is composed of 694 sentences taken from an English IT-related online forum, translated into French by three automatic translators considered as black-box systems: M OSES, B ING 1 http://www.uncorpora.org/ and S YSTRAN. The M OSES system is a standard phrase-based SMT system trained using the Moses (Koehn et al., 2007) and IRSTLM (Federico et al., 2008) toolkits and optimised on a development set against BLEU (Papineni et al., 2002) using MERT (Och, 2003). A trigram Language Model (LM) was built using Witten-Bell smoothing. This system was trained using in-domain translation memories (up to ∼ 1.6M translation units) plus ∼ 1M translation units from the Computer Software domain (obtained from the TAUS Data Association2 ) for the translation model, as well as additional monolingual forum data (up to 20K French sentences) for the LM. The B ING system is a freely available generic SMT system, Bing Translator,3 accessed through the second version of their API. Finally, the last system is the Systran Enterprise Server version 6, customised with the"
2013.mtsummit-posters.13,P02-1040,0,0.107966,"translation pairs after removing sentences longer than 80 words. This was virtually the same parallel corpus used to build the SMT systems. User-Generated Data The user-generated content is composed of 694 sentences taken from an English IT-related online forum, translated into French by three automatic translators considered as black-box systems: M OSES, B ING 1 http://www.uncorpora.org/ and S YSTRAN. The M OSES system is a standard phrase-based SMT system trained using the Moses (Koehn et al., 2007) and IRSTLM (Federico et al., 2008) toolkits and optimised on a development set against BLEU (Papineni et al., 2002) using MERT (Och, 2003). A trigram Language Model (LM) was built using Witten-Bell smoothing. This system was trained using in-domain translation memories (up to ∼ 1.6M translation units) plus ∼ 1M translation units from the Computer Software domain (obtained from the TAUS Data Association2 ) for the translation model, as well as additional monolingual forum data (up to 20K French sentences) for the LM. The B ING system is a freely available generic SMT system, Bing Translator,3 accessed through the second version of their API. Finally, the last system is the Systran Enterprise Server version"
2013.mtsummit-posters.13,2011.mtsummit-papers.27,0,0.0889134,"mised with the use of a domain specific 10K+ dictionary entries. Each translation is evaluated by a professional translator using two possible labels: 0 if the translation does not preserve the meaning of the source sentence and 1 if the meaning is preserved. The final dataset contains, for each of the three translations generated by the three MT systems, 694 source segments, 694 translated segments, and one adequacy score. From this dataset, 500 segments are used to train the QE models and 194 segments are held out for evaluation purposes. More information about this dataset can be found in (Roturier and Bensadoun, 2011). To build the topic models for the adequacy features, we use the in-domain Translation Memories used to train the M OSES system. WMT12 QE Data The WMT12 QE dataset (Callison-Burch et al., 2012) is composed of 2, 254 English sentences translated into Spanish by a M OSES phrase-based system and evaluated by three professional translators in terms of postediting effort on a 1 (highest) to 5 scale (lowest). The three scores per segment pair are weighted and averaged in order to obtain one continuous score (∈ [1; 5]). From this dataset, 1, 832 segments are used to train the QE models while 422 seg"
2013.mtsummit-posters.13,W12-3117,1,0.696965,"11), extracted from both source and target sentences, e.g. common dependency relations in source and target sentences. Both previous approaches for adequacy estimation severely suffer from data sparsity while attempting to model contrastive linguistic information between source and target sentences. As a consequence, the reported results are poor, sometimes even below simple baselines such as the majority class on the training data. None of the previous work uses lexicalised features or topic models built based on those features for adequacy estimation. As we will discuss in the next section, Rubino et al. (2012) used topic models as part of a larger feature set to estimate post-editing effort. However, the contribution of this information source was not tested. 3 Topic Models for Quality Estimation The first study on using topic modelling for QE was conducted in (Rubino et al., 2012) as part of the WMT12 QE shared task to estimate the postediting effort of news texts translated from English to Spanish. A joint LDA approach was used. We expand on that work by exploring two types of bilingual topic models and defining a number of variants of features based on source and target (translation) distributio"
2013.mtsummit-posters.13,P10-1063,0,0.176662,"g more reliable and less subjective quality labels (Specia and Farzindar, 2010; Specia, 2011). Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic measure (e.g. NIST (Doddington, 2002)) as the quality label. Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting subsegments that need revision (Bach et al., 2011). For an overview on various feature sets and machine learning algorithms, we refer the reader to the recent shared-task on the topic (Callison-Burch et al., 2012). Most QE work focuses on estimating a score that indicates overall quality having professional translators as intended user, e.g. post-editing effort. Little work has been done fo"
2013.mtsummit-posters.13,2010.jec-1.5,1,0.750247,"elves and distance metrics between source and target distributions (Section 3). We experiment with three families of datasets: two annotated for adequacy, containing newswire and user-generated content (from a product forum), and one news dataset annotated for postediting effort (Section 4). We show that TM features are more effective for both adequacyannotated types of datasets (Section 5). 2 Related Work Most research work on QE for machine translation is focused on feature engineering and feature selection, with some recent work on devising more reliable and less subjective quality labels (Specia and Farzindar, 2010; Specia, 2011). Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic measure (e.g. NIST (Doddington, 2002)) as the quality label. Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system"
2013.mtsummit-posters.13,2009.eamt-1.5,1,0.650408,"ed in previous work, which depend on linguistic processors that are often unreliable on automatic translations. Experiments with a number of datasets show promising results: the use of topic models outperforms the state-of-the-art approaches by a large margin in all datasets annotated for adequacy. 1 Introduction Quality estimation (QE) for machine translation (MT) is an area concerned with predicting a quality indicator for an automatically translated text without referring to human translations (the so-called reference translations typical of most MT evaluation metrics) (Blatz et al., 2004; Specia et al., 2009). The widespread use of MT in the translation industry has strongly motivated work in this area. As a consequence, the majority of existing work focuses on predicting some form of post-editing effort to help professional translators (Section 2). However, one equally appealing application is that of estimating the adequacy of translations for gisting purposes. An indicator of such a type is particularly relevant in contexts where the reader does not know the source language. QE is generally addressed as a machine learning task. Intuitively, it is expected that features used to capture general a"
2013.mtsummit-posters.13,2011.mtsummit-papers.58,1,0.919036,"diting effort to help professional translators (Section 2). However, one equally appealing application is that of estimating the adequacy of translations for gisting purposes. An indicator of such a type is particularly relevant in contexts where the reader does not know the source language. QE is generally addressed as a machine learning task. Intuitively, it is expected that features used to capture general aspects of quality are different from features that define adequacy. Previous work on adequacy estimation has focused on linguistic features contrasting the source and translation texts (Specia et al., 2011; Mehdad et al., 2012), e.g. the proportion of overlapping typed dependency relations in the source and target sentences with arguments that align to each other (based on wordalignment information). While these can provide interesting indicators, they are often very sparse and noisy. Sparsity happens because many of these features do not apply to most sentences, such as features comparing named entities in the source and target sentences. A significant amount of noise can come from the fact that linguistic processors, such as syntactic parsers and named entity recognisers, need to be applied t"
2013.mtsummit-posters.13,2011.eamt-1.12,1,0.598309,"etween source and target distributions (Section 3). We experiment with three families of datasets: two annotated for adequacy, containing newswire and user-generated content (from a product forum), and one news dataset annotated for postediting effort (Section 4). We show that TM features are more effective for both adequacyannotated types of datasets (Section 5). 2 Related Work Most research work on QE for machine translation is focused on feature engineering and feature selection, with some recent work on devising more reliable and less subjective quality labels (Specia and Farzindar, 2010; Specia, 2011). Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic measure (e.g. NIST (Doddington, 2002)) as the quality label. Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translatio"
2013.tc-1.10,W12-3102,1,\N,Missing
2013.tc-1.10,P02-1040,0,\N,Missing
2013.tc-1.10,W14-3302,1,\N,Missing
2013.tc-1.10,W13-2201,1,\N,Missing
2014.amta-researchers.22,P11-1022,0,0.0180661,"mproving post-editing efficiency by filtering out low quality segments which would require more effort to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), ranking or selecting the best translation from multiple MT systems (Specia et al., 2010; Hildebrand and Vogel, 2013; Avramidis, 2013; Avramidis and Popovi´c, 2013), or between translations from either an MT system or a translation memory (He et al., 2010), and highlighting sub-segments that need revision (Bach et al., 2011). Generally speaking, QE models are built using supervised machine learning algorithms from examples of translations at a given granularity level (e.g. sentences). For training, these examples are annotated with quality labels and described by a number of features that can approximate quality (or errors). “Quality” is therefore defined according to the problem at hand and the labelled data, for example, post-editing time for a sentence or word-level errors. For an overview of various algorithms and features we refer the reader to the WMT12-14 shared tasks on QE (Callison-Burch et al., 2012; Bo"
2014.amta-researchers.22,C04-1046,0,0.0546442,"eld Regent Court, 211 Portobello Street, S4 1DP, UK Abstract We present a first attempt at predicting the quality of translations produced by human, professional translators. We examine datasets annotated for quality at sentence- and word-level for four language pairs and provide experiments with prediction models for these datasets. We compare the performance of such models against that of models built from machine translations, highlighting a number of challenges in estimating quality and detecting errors in human translations. 1 Introduction Metrics for translation quality estimation (QE) (Blatz et al., 2004; Specia et al., 2009) aim at providing an estimate on the quality of a translated text. Such metrics have no access to reference translations, as they are intended for translation systems in use. QE has shown promising results in several applications in the context of Machine Translation (MT), such as improving post-editing efficiency by filtering out low quality segments which would require more effort to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), ra"
2014.amta-researchers.22,W13-2201,1,0.808092,"1). Generally speaking, QE models are built using supervised machine learning algorithms from examples of translations at a given granularity level (e.g. sentences). For training, these examples are annotated with quality labels and described by a number of features that can approximate quality (or errors). “Quality” is therefore defined according to the problem at hand and the labelled data, for example, post-editing time for a sentence or word-level errors. For an overview of various algorithms and features we refer the reader to the WMT12-14 shared tasks on QE (Callison-Burch et al., 2012; Bojar et al., 2013, 2014). So far, QE has only been applied to machine translated texts. However, the above mentioned applications are also valid in the context of human translation. In particular, in scenarios where translations produced by humans may be of variable or questionable levels of reliability (e.g. crowdsourcing), it becomes important to estimate translation quality to, for example, select among multiple options of human translations (or even a mix of human and machine translations). In addition, even with professionally created translations, quality assurance is a common process and an estimation m"
2014.amta-researchers.22,W14-3302,1,0.869978,"Missing"
2014.amta-researchers.22,W12-3102,1,0.88128,"ed revision (Bach et al., 2011). Generally speaking, QE models are built using supervised machine learning algorithms from examples of translations at a given granularity level (e.g. sentences). For training, these examples are annotated with quality labels and described by a number of features that can approximate quality (or errors). “Quality” is therefore defined according to the problem at hand and the labelled data, for example, post-editing time for a sentence or word-level errors. For an overview of various algorithms and features we refer the reader to the WMT12-14 shared tasks on QE (Callison-Burch et al., 2012; Bojar et al., 2013, 2014). So far, QE has only been applied to machine translated texts. However, the above mentioned applications are also valid in the context of human translation. In particular, in scenarios where translations produced by humans may be of variable or questionable levels of reliability (e.g. crowdsourcing), it becomes important to estimate translation quality to, for example, select among multiple options of human translations (or even a mix of human and machine translations). In addition, even with professionally created translations, quality assurance is a common process"
2014.amta-researchers.22,W12-3110,1,0.876554,"Missing"
2014.amta-researchers.22,2005.eamt-1.15,0,0.0474846,"Missing"
2014.amta-researchers.22,P10-1064,0,0.0393224,"Missing"
2014.amta-researchers.22,W13-2246,0,0.0184688,". Such metrics have no access to reference translations, as they are intended for translation systems in use. QE has shown promising results in several applications in the context of Machine Translation (MT), such as improving post-editing efficiency by filtering out low quality segments which would require more effort to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), ranking or selecting the best translation from multiple MT systems (Specia et al., 2010; Hildebrand and Vogel, 2013; Avramidis, 2013; Avramidis and Popovi´c, 2013), or between translations from either an MT system or a translation memory (He et al., 2010), and highlighting sub-segments that need revision (Bach et al., 2011). Generally speaking, QE models are built using supervised machine learning algorithms from examples of translations at a given granularity level (e.g. sentences). For training, these examples are annotated with quality labels and described by a number of features that can approximate quality (or errors). “Quality” is therefore defined according to the problem at hand and the labelled da"
2014.amta-researchers.22,P10-1063,0,0.0226086,"mation (QE) (Blatz et al., 2004; Specia et al., 2009) aim at providing an estimate on the quality of a translated text. Such metrics have no access to reference translations, as they are intended for translation systems in use. QE has shown promising results in several applications in the context of Machine Translation (MT), such as improving post-editing efficiency by filtering out low quality segments which would require more effort to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), ranking or selecting the best translation from multiple MT systems (Specia et al., 2010; Hildebrand and Vogel, 2013; Avramidis, 2013; Avramidis and Popovi´c, 2013), or between translations from either an MT system or a translation memory (He et al., 2010), and highlighting sub-segments that need revision (Bach et al., 2011). Generally speaking, QE models are built using supervised machine learning algorithms from examples of translations at a given granularity level (e.g. sentences). For training, these examples are annotated with quality labels and described by a number of features that can"
2014.amta-researchers.22,2011.eamt-1.12,1,0.838275,"ty and detecting errors in human translations. 1 Introduction Metrics for translation quality estimation (QE) (Blatz et al., 2004; Specia et al., 2009) aim at providing an estimate on the quality of a translated text. Such metrics have no access to reference translations, as they are intended for translation systems in use. QE has shown promising results in several applications in the context of Machine Translation (MT), such as improving post-editing efficiency by filtering out low quality segments which would require more effort to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), ranking or selecting the best translation from multiple MT systems (Specia et al., 2010; Hildebrand and Vogel, 2013; Avramidis, 2013; Avramidis and Popovi´c, 2013), or between translations from either an MT system or a translation memory (He et al., 2010), and highlighting sub-segments that need revision (Bach et al., 2011). Generally speaking, QE models are built using supervised machine learning algorithms from examples of translations at a given granularity level (e.g. sentences"
2014.amta-researchers.22,P13-4014,1,0.722568,"Missing"
2014.amta-researchers.22,2009.eamt-1.5,1,0.745395,"1 Portobello Street, S4 1DP, UK Abstract We present a first attempt at predicting the quality of translations produced by human, professional translators. We examine datasets annotated for quality at sentence- and word-level for four language pairs and provide experiments with prediction models for these datasets. We compare the performance of such models against that of models built from machine translations, highlighting a number of challenges in estimating quality and detecting errors in human translations. 1 Introduction Metrics for translation quality estimation (QE) (Blatz et al., 2004; Specia et al., 2009) aim at providing an estimate on the quality of a translated text. Such metrics have no access to reference translations, as they are intended for translation systems in use. QE has shown promising results in several applications in the context of Machine Translation (MT), such as improving post-editing efficiency by filtering out low quality segments which would require more effort to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), ranking or selecting the"
2014.eamt-1.21,W08-0330,0,0.0168787,"(Graesser et al., 2004). Different from lexical cohesion features, LSA features are able to find correlations among different words, which are not repetitions and may not be synonyms, but are instead related (as given by co-occurrence patterns). 3.3 Pseudo-references Pseudo-references are translations produced by other MT systems than the system we want to predict the quality for. They are used as references to evaluate the output of the MT system of interest. They have also been used for other purposes, e.g., to fulfil the lack of human references available in reference-based MT evaluation (Albrecht and Hwa, 2008) and automatic summary evaluation (Louis and Nenkova, 2013). The application we are interested in, originally proposed in (Soricut and Echihabi, 2010), is to generate features for 104 QE. In this scenario, reference-based evaluation metrics (such as BLEU) are computed between the MT system output and the pseudo-references and used to train quality prediction models. Soricut and Echihabi (2010) discussed the importance of the pseudo-references being generated by MT system(s) which are as different as possible from the MT system of interest, and preferably of much better quality. This should ens"
2014.eamt-1.21,W11-2104,0,0.0441326,"rful in the framework. For QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) conduct a feature analysis, at sentence-level, on a number of datasets and show that the BLEU-based pseudoreference feature contributes the most to prediction performance. In terms of other types of linguistic features for QE, Xiong et al. (2010) and Bach et al. (2011) propose features for word-level QE and show that these improve over the state-of-the-art results. At sentence-level, Avramidis et al. (2011), Hardmeier (2011) and Almaghout and Specia (2013) consider syntactic features, achieving better results compared to competitive feature sets. Pighin and M`arquez (2011) obtain improvements over strong baselines from exploiting semantic role labelling to score MT outputs at sentence-level. Felice and Specia (2012) introduce several linguistic features for QE at sentence-level. These did not show improvement over shallower features, but feature selection analysis showed that linguistic features were among the best performing ones. 3 Features for document-level QE QE is traditionally done at sen"
2014.eamt-1.21,P11-1022,0,0.0238051,"l features for documentlevel prediction. The authors claim that a pseudoreferences-based feature (based in BLEU) is one of the most powerful in the framework. For QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) conduct a feature analysis, at sentence-level, on a number of datasets and show that the BLEU-based pseudoreference feature contributes the most to prediction performance. In terms of other types of linguistic features for QE, Xiong et al. (2010) and Bach et al. (2011) propose features for word-level QE and show that these improve over the state-of-the-art results. At sentence-level, Avramidis et al. (2011), Hardmeier (2011) and Almaghout and Specia (2013) consider syntactic features, achieving better results compared to competitive feature sets. Pighin and M`arquez (2011) obtain improvements over strong baselines from exploiting semantic role labelling to score MT outputs at sentence-level. Felice and Specia (2012) introduce several linguistic features for QE at sentence-level. These did not show improvement over shallower features, but feature selection a"
2014.eamt-1.21,P02-1040,0,0.0906257,"nts with features extracted from pseudo-references led to the best results, but the discursive features also proved promising. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Bojar et al., 2013). This task is usually addressed with machine learning models trained on datasets composed of source texts, their machine translations, and a quality label assigned by humans or by an automatic metric (e.g.: BLEU (Papineni et al., 2002)). A common use of quality predictions is the estimation of postc 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 101 editing effort in order to decide whether to translate a text from scratch or post-edit its machine translation. Another use is the ranking of translations in order to select the best text from multiple MT systems. Feature engineering is an important component in QE. Although several feature sets have already been explored, most approaches focus on sentence-level quality prediction, with sentencelevel f"
2014.eamt-1.21,W05-0909,0,0.0456588,"ted work Work related to this research includes documentlevel MT evaluation metrics, QE features, and QE prediction, as well as work focusing on other linguistic features, and work using pseudoreferences. Wong and Kit (2012) use lexical cohesion metrics for MT evaluation at document-level. Lexical cohesion relates to word choices, captured in their work by reiteration and collocation. Words and stems were used for reiteration, and synonyms, near-synonyms and superordinates, for collocations. These metrics are integrated with traditional metrics like BLEU, TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005). The highest correlation against human assessments was found for the combination of METEOR and the discursive features. Rubino et al. (2013) explore topic model features for QE at sentence-level. Latent Dirichlet Allocation is used to model the topics in two ways: a bilingual view, where the bilingual corpus is concatenated at sentence-level to build a single model with two languages; and a polylingual view, where one topic model is built for each language. While the topics models are generated with information 102 from the entire corpus, the features are extracted at sentence-level. These ar"
2014.eamt-1.21,W11-1001,0,0.168984,"Missing"
2014.eamt-1.21,C04-1046,0,0.569591,"tend to explore very short contexts within sentence boundaries. In addition, most work has targeted sentence-level quality prediction. In this paper, we focus on documentlevel QE using novel discursive features, as well as exploiting pseudo-reference translations. Experiments with features extracted from pseudo-references led to the best results, but the discursive features also proved promising. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Bojar et al., 2013). This task is usually addressed with machine learning models trained on datasets composed of source texts, their machine translations, and a quality label assigned by humans or by an automatic metric (e.g.: BLEU (Papineni et al., 2002)). A common use of quality predictions is the estimation of postc 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 101 editing effort in order to decide whether to translate a text from scratch or post-edit its machine translation. Another use is"
2014.eamt-1.21,W12-3156,0,0.088453,"y Rubino et al. (2013) considered discourse-related information by studying topic model features for sentencelevel prediction. Soricut and Echihabi (2010) explored document-level quality prediction, but they did not use explicit discourse information, e.g. information to capture text cohesion or coherence. In this paper we focus on document-level features and document-level prediction. We believe that judgements on translation quality depend on units longer than just a given sentence, taking into account discourse phenomena for lexical choice, consistency, style and connectives, among others (Carpuat and Simard, 2012). This is particularly important in MT evaluation contexts, since most MT systems, and in particular statistical MT (SMT) systems, process sentences one by one, in isolation. Our hypothesis is that features that capture discourse phenomena can improve document-level prediction. We consider two families of features that have been successfully applied in reference-based MT evaluation (Wong and Kit, 2012) and readability assessment (Graesser et al., 2004). In terms of applications, document-level QE is very important in scenarios where the entire text needs to be used/published without posteditio"
2014.eamt-1.21,W12-3110,1,0.831406,"seudoreference feature contributes the most to prediction performance. In terms of other types of linguistic features for QE, Xiong et al. (2010) and Bach et al. (2011) propose features for word-level QE and show that these improve over the state-of-the-art results. At sentence-level, Avramidis et al. (2011), Hardmeier (2011) and Almaghout and Specia (2013) consider syntactic features, achieving better results compared to competitive feature sets. Pighin and M`arquez (2011) obtain improvements over strong baselines from exploiting semantic role labelling to score MT outputs at sentence-level. Felice and Specia (2012) introduce several linguistic features for QE at sentence-level. These did not show improvement over shallower features, but feature selection analysis showed that linguistic features were among the best performing ones. 3 Features for document-level QE QE is traditionally done at sentence-level. This happens mainly because the majority of MT systems translate texts at this level. Evaluating sentences instead of documents can be useful for many scenarios, e.g., post-editing effort prediction. 1 http://www.statmt.org/wmt12/ However, some linguistic phenomena can only be captured by considering"
2014.eamt-1.21,2011.eamt-1.32,0,0.141422,"r QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) conduct a feature analysis, at sentence-level, on a number of datasets and show that the BLEU-based pseudoreference feature contributes the most to prediction performance. In terms of other types of linguistic features for QE, Xiong et al. (2010) and Bach et al. (2011) propose features for word-level QE and show that these improve over the state-of-the-art results. At sentence-level, Avramidis et al. (2011), Hardmeier (2011) and Almaghout and Specia (2013) consider syntactic features, achieving better results compared to competitive feature sets. Pighin and M`arquez (2011) obtain improvements over strong baselines from exploiting semantic role labelling to score MT outputs at sentence-level. Felice and Specia (2012) introduce several linguistic features for QE at sentence-level. These did not show improvement over shallower features, but feature selection analysis showed that linguistic features were among the best performing ones. 3 Features for document-level QE QE is traditionally done at sentence-level. This"
2014.eamt-1.21,P07-2045,0,0.00722867,"Missing"
2014.eamt-1.21,J13-2002,0,0.0200886,"features, LSA features are able to find correlations among different words, which are not repetitions and may not be synonyms, but are instead related (as given by co-occurrence patterns). 3.3 Pseudo-references Pseudo-references are translations produced by other MT systems than the system we want to predict the quality for. They are used as references to evaluate the output of the MT system of interest. They have also been used for other purposes, e.g., to fulfil the lack of human references available in reference-based MT evaluation (Albrecht and Hwa, 2008) and automatic summary evaluation (Louis and Nenkova, 2013). The application we are interested in, originally proposed in (Soricut and Echihabi, 2010), is to generate features for 104 QE. In this scenario, reference-based evaluation metrics (such as BLEU) are computed between the MT system output and the pseudo-references and used to train quality prediction models. Soricut and Echihabi (2010) discussed the importance of the pseudo-references being generated by MT system(s) which are as different as possible from the MT system of interest, and preferably of much better quality. This should ensure that string similarity features (like BLEU) indicate mo"
2014.eamt-1.21,2013.mtsummit-posters.13,1,0.915273,"Missing"
2014.eamt-1.21,2013.mtsummit-papers.21,1,0.925431,"applications, document-level QE is very important in scenarios where the entire text needs to be used/published without postedition. Soricut and Echihabi (2010) and Soricut and Narsale (2012) explored a feature based on pseudo-references for document-level QE. Pseudo-references are translations produced by one or more external MT systems, which are different from the one producing the translations we want to predict the quality for. These are used as references against which the output of the MT system of interest can be compared using standard metrics such as BLEU. Soricut et al. (2012) and Shah et al. (2013) explored pseudo-references for sentence-level QE. In both cases, features based on pseudo-references led to significant improvements in prediction accuracy. Here we also use pseudoreferences for document-level QE, with a number of string similarity metrics to produce documentlevel scores as features, which are arguably more reliable than sentence-level scores, particularly for metrics like BLEU. In the remainder of this paper, Section 2 presents related work. Section 3 introduces the documentlevel QE features we propose. Section 4 describes the experimental setup of this work. Section 5 prese"
2014.eamt-1.21,2006.amta-papers.25,0,0.796928,"on 5 presents the results. 2 Related work Work related to this research includes documentlevel MT evaluation metrics, QE features, and QE prediction, as well as work focusing on other linguistic features, and work using pseudoreferences. Wong and Kit (2012) use lexical cohesion metrics for MT evaluation at document-level. Lexical cohesion relates to word choices, captured in their work by reiteration and collocation. Words and stems were used for reiteration, and synonyms, near-synonyms and superordinates, for collocations. These metrics are integrated with traditional metrics like BLEU, TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005). The highest correlation against human assessments was found for the combination of METEOR and the discursive features. Rubino et al. (2013) explore topic model features for QE at sentence-level. Latent Dirichlet Allocation is used to model the topics in two ways: a bilingual view, where the bilingual corpus is concatenated at sentence-level to build a single model with two languages; and a polylingual view, where one topic model is built for each language. While the topics models are generated with information 102 from the entire corpus, the features are"
2014.eamt-1.21,P10-1063,0,0.624,"Missing"
2014.eamt-1.21,W12-3121,0,0.409559,"MT evaluation contexts, since most MT systems, and in particular statistical MT (SMT) systems, process sentences one by one, in isolation. Our hypothesis is that features that capture discourse phenomena can improve document-level prediction. We consider two families of features that have been successfully applied in reference-based MT evaluation (Wong and Kit, 2012) and readability assessment (Graesser et al., 2004). In terms of applications, document-level QE is very important in scenarios where the entire text needs to be used/published without postedition. Soricut and Echihabi (2010) and Soricut and Narsale (2012) explored a feature based on pseudo-references for document-level QE. Pseudo-references are translations produced by one or more external MT systems, which are different from the one producing the translations we want to predict the quality for. These are used as references against which the output of the MT system of interest can be compared using standard metrics such as BLEU. Soricut et al. (2012) and Shah et al. (2013) explored pseudo-references for sentence-level QE. In both cases, features based on pseudo-references led to significant improvements in prediction accuracy. Here we also use"
2014.eamt-1.21,W12-3118,0,0.0581428,"Missing"
2014.eamt-1.21,2009.eamt-1.5,1,0.876274,"y short contexts within sentence boundaries. In addition, most work has targeted sentence-level quality prediction. In this paper, we focus on documentlevel QE using novel discursive features, as well as exploiting pseudo-reference translations. Experiments with features extracted from pseudo-references led to the best results, but the discursive features also proved promising. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Bojar et al., 2013). This task is usually addressed with machine learning models trained on datasets composed of source texts, their machine translations, and a quality label assigned by humans or by an automatic metric (e.g.: BLEU (Papineni et al., 2002)). A common use of quality predictions is the estimation of postc 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 101 editing effort in order to decide whether to translate a text from scratch or post-edit its machine translation. Another use is the ranking of transl"
2014.eamt-1.21,P13-4014,1,0.901413,"Missing"
2014.eamt-1.21,D12-1097,0,0.469149,"ents on translation quality depend on units longer than just a given sentence, taking into account discourse phenomena for lexical choice, consistency, style and connectives, among others (Carpuat and Simard, 2012). This is particularly important in MT evaluation contexts, since most MT systems, and in particular statistical MT (SMT) systems, process sentences one by one, in isolation. Our hypothesis is that features that capture discourse phenomena can improve document-level prediction. We consider two families of features that have been successfully applied in reference-based MT evaluation (Wong and Kit, 2012) and readability assessment (Graesser et al., 2004). In terms of applications, document-level QE is very important in scenarios where the entire text needs to be used/published without postedition. Soricut and Echihabi (2010) and Soricut and Narsale (2012) explored a feature based on pseudo-references for document-level QE. Pseudo-references are translations produced by one or more external MT systems, which are different from the one producing the translations we want to predict the quality for. These are used as references against which the output of the MT system of interest can be compared"
2014.eamt-1.21,P10-1062,0,0.0194684,"egation of sentence-level features for documentlevel prediction. The authors claim that a pseudoreferences-based feature (based in BLEU) is one of the most powerful in the framework. For QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) conduct a feature analysis, at sentence-level, on a number of datasets and show that the BLEU-based pseudoreference feature contributes the most to prediction performance. In terms of other types of linguistic features for QE, Xiong et al. (2010) and Bach et al. (2011) propose features for word-level QE and show that these improve over the state-of-the-art results. At sentence-level, Avramidis et al. (2011), Hardmeier (2011) and Almaghout and Specia (2013) consider syntactic features, achieving better results compared to competitive feature sets. Pighin and M`arquez (2011) obtain improvements over strong baselines from exploiting semantic role labelling to score MT outputs at sentence-level. Felice and Specia (2012) introduce several linguistic features for QE at sentence-level. These did not show improvement over shallower features,"
2014.eamt-1.22,W13-2242,0,0.0455214,"Missing"
2014.eamt-1.22,C04-1046,0,0.0987899,"Missing"
2014.eamt-1.22,W13-2244,0,0.0606634,"Missing"
2014.eamt-1.22,2005.eamt-1.15,0,0.0793506,"Missing"
2014.eamt-1.22,P10-1064,0,0.0871476,"Missing"
2014.eamt-1.22,W13-2246,0,0.0607735,"Missing"
2014.eamt-1.22,2009.eamt-1.5,1,0.897018,"Missing"
2014.eamt-1.22,P13-4014,1,0.860926,"ndard metrics for regression (MAE: mean absolute error; RMSE: root mean squared error) and classification (precision, recall and F1). For each Table and dataset, bold-faced figures represent results that are significantly better (paired ttest with p ≤ 0.05) with respect to the baseline. 4 Figure 1: Distribution of true scores by lang. pair. ID LSP1 LSP2 LSP3 LSP4 LSP5 SVC, we consider the “one-against-all” approach for multi-class classification with all classes are weighted equally. # Test 233 738 297 388 677 Table 2: Number of out-of-domain test sentences. Features We use the QuEst toolkit [Specia et al., 2013, Shah et al., 2013] to extract two feature sets for each dataset: • BL: 17 features used as baseline in the WMT shared tasks on QE. • AF: 80 common MT system-independent features (superset of BL). The resources used to extract these features (language models, etc.) are also available as part of the WMT14 shared task on QE. Learning algorithms We use the Support Vector Machines implementation within QuEst to perform either regression (SVR) or classification (SVC) with Radial Basis Function as kernel and parameters optimised using grid search. For 111 Classification experiments Our main motivat"
2014.eamt-1.7,ambati-etal-2010-active,0,\N,Missing
2014.eamt-1.7,N07-1008,0,\N,Missing
2014.eamt-1.7,D11-1033,0,\N,Missing
2014.eamt-1.7,N10-1134,0,\N,Missing
2014.eamt-1.7,D08-1089,0,\N,Missing
2014.eamt-1.7,W13-2206,0,\N,Missing
2014.eamt-1.7,N09-1047,0,\N,Missing
2014.eamt-1.7,P10-2041,0,\N,Missing
2014.eamt-1.7,W11-2131,0,\N,Missing
2014.eamt-1.7,P07-2045,0,\N,Missing
2014.eamt-1.7,D07-1104,0,\N,Missing
2014.eamt-1.7,D07-1036,0,\N,Missing
2014.eamt-1.7,P11-1052,0,\N,Missing
2014.eamt-1.7,P10-1088,0,\N,Missing
2014.eamt-1.7,P05-1032,0,\N,Missing
2014.eamt-1.7,W04-3250,0,\N,Missing
2014.eamt-1.7,N13-1086,0,\N,Missing
2014.eamt-1.7,2012.eamt-1.65,0,\N,Missing
2014.eamt-1.7,2005.eamt-1.19,0,\N,Missing
2014.eamt-1.7,2005.iwslt-1.7,0,\N,Missing
2014.iwslt-evaluation.11,2012.eamt-1.60,0,0.0264834,"Missing"
2014.iwslt-evaluation.11,W14-3300,0,0.191444,"Missing"
2014.iwslt-evaluation.11,P10-2041,0,0.0751895,"Missing"
2014.iwslt-evaluation.11,2012.iwslt-papers.15,0,0.0392782,"Missing"
2014.iwslt-evaluation.11,P07-2045,0,0.00308277,"source language to form pseudo ASR outputs, which contained no case and punctuation information. Numbers, symbols and acronyms were also converted to their verbal forms with lookup tables. We then used this synthesised corpus of pseudo ASR as the source, and the original corpus as the target of our monolingual MT. The monolingual translation system was trained on 37.6M words (Table 2). It performed monotonic translation with phrases of as long as 7 words. 5. Decoding The evaluation systems for ASR and MT are multi-pass systems with resource optimisation and environment management capabilities [11, 18]. The ASR is a two-stream multipass system. It is illustrated in Figure 1. The two streams ASR1 and ASR2 differ by the acoustic model training data (detailed in Table 1) and also the tandem configurations (detailed in §3). Both streams follow the same routine along the multi-pass decoding system. In pass 1, a unified decoding result was generated using a non-VTLN DNN and GMM-HMM tandem system with cepstral mean and variance (CMVN) normalisation trained on ASR2 data. These 88 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 Tab"
2014.iwslt-evaluation.11,N13-1073,0,0.0612584,"Missing"
2014.iwslt-evaluation.11,P13-2121,0,0.0286297,"Missing"
2014.iwslt-evaluation.11,P07-1019,0,0.0553151,"Missing"
2014.iwslt-evaluation.11,P13-4014,1,0.794414,"MT system. System integration experiments were tried in the En→Fr SLT task and the results were submitted as contrastive systems. Figure 2 depicts the integrated system and its comparison with the pipeline system. In the integrated system, ASR system hypotheses are expanded in the form of lattices, confusion networks or N -best lists. A quality estimation (QE) module evaluated and rescored the ASR outputs before they were fed to the MT system. In our implementation, 10-best outputs from the ASR system on the IWSLT 2011 evaluation data were used for QE training. The QE module derived 117 QuEst [21, 22] features from each sentence to describe its linguistic, statistical properties as well as the statistics from the ASR and MT models. Out of the 117 features, top 58 features were selected using the Gaussian Process (GP) with RBF kernel as described in [23]. Further, GP was used to learn the relationship between the selected features and the translation performance of the sentence (in this case, sentence-based METEOR score) [24]. During testing, the estimated translation performance was used to rescore the 10-best ASR output. Details of the integrated system were described in [25]. Table 7: Co"
2014.iwslt-evaluation.11,2013.mtsummit-papers.21,1,0.725335,"are expanded in the form of lattices, confusion networks or N -best lists. A quality estimation (QE) module evaluated and rescored the ASR outputs before they were fed to the MT system. In our implementation, 10-best outputs from the ASR system on the IWSLT 2011 evaluation data were used for QE training. The QE module derived 117 QuEst [21, 22] features from each sentence to describe its linguistic, statistical properties as well as the statistics from the ASR and MT models. Out of the 117 features, top 58 features were selected using the Gaussian Process (GP) with RBF kernel as described in [23]. Further, GP was used to learn the relationship between the selected features and the translation performance of the sentence (in this case, sentence-based METEOR score) [24]. During testing, the estimated translation performance was used to rescore the 10-best ASR output. Details of the integrated system were described in [25]. Table 7: Contrastive SLT system performance (En→Fr) Setting Contrastive 1 (baseline) Contrastive 2 Tst12 31.33 Tst14 23.18 (+ 10-best list rescoring) 31.51 23.27 31.87 23.44 7. Summary In this paper, the USFD SLT system for IWSLT 2014 was described. Automatic speech r"
2014.iwslt-evaluation.11,W14-3348,0,0.0121121,"MT system. In our implementation, 10-best outputs from the ASR system on the IWSLT 2011 evaluation data were used for QE training. The QE module derived 117 QuEst [21, 22] features from each sentence to describe its linguistic, statistical properties as well as the statistics from the ASR and MT models. Out of the 117 features, top 58 features were selected using the Gaussian Process (GP) with RBF kernel as described in [23]. Further, GP was used to learn the relationship between the selected features and the translation performance of the sentence (in this case, sentence-based METEOR score) [24]. During testing, the estimated translation performance was used to rescore the 10-best ASR output. Details of the integrated system were described in [25]. Table 7: Contrastive SLT system performance (En→Fr) Setting Contrastive 1 (baseline) Contrastive 2 Tst12 31.33 Tst14 23.18 (+ 10-best list rescoring) 31.51 23.27 31.87 23.44 7. Summary In this paper, the USFD SLT system for IWSLT 2014 was described. Automatic speech recognition (ASR) is achieved by two multi-pass deep neural network systems with slightly different tandem configurations and different training data. Machine translation (MT)"
2014.iwslt-evaluation.11,N12-1047,0,0.0726943,"Missing"
2014.iwslt-evaluation.11,2011.iwslt-papers.7,0,\N,Missing
2014.iwslt-evaluation.11,2013.iwslt-evaluation.3,0,\N,Missing
2015.eamt-1.16,W08-0312,0,0.0220594,"d NIST scores compare against standard BLEU and NIST scores at the system, document and sentence levels. 2 2.1 N-gram based evaluation BLEU Due to the sparsity of n-grams with large n and the geometric average of n-gram precisions, BLEU is not suitable for sentence-level evaluation. Several smoothing approaches have been proposed to alleviate this issue, such as the standard plus-one smoothing (Lin and Och, 2004) and combinations of smoothing techniques (Chen and Cherry, 2014). A great deal of methods have been proposed to improve the performance of BLEU. These include metrics such as m-bleu (Agarwal and Lavie, 2008) and Amber (Chen and Kuhn, 2011). However, these metrics still treat n-grams in different references equally, regardless of whether the n-gram appears only once or is found in all references. 2.2 The BLEU metric applies a straightforward method of counting the n-grams that overlap in the system translation and given human translations under the assumption that human translations precisely reproduce the meaning of the source text. The closer to the reference, the higher the translation quality of the system translation will be. The core formula is given in Eq. 1 (Papineni et al., 2002), so that"
2015.eamt-1.16,W14-3302,1,0.821527,"NIST metrics. Our metrics are tested in two into-English machine translation datasets. They lead to a significant increase in Pearson’s correlation with human fluency judgements at system-level evaluation. The new NIST metric also outperforms the standard NIST for documentlevel evaluation. 1 Introduction Quality evaluation plays a critical role in Machine Translation (MT). Since its conception, the BLEU metric (Papineni et al., 2002) has had a significant impact on MT development. Although human evaluation has been used in recent evaluation campaigns such as WMT (Workshop on Statistical MT) (Bojar et al., 2014) and other forms of reference-less metrics have been proposed (Gamon et al., 2005; Specia et al., 2010), the merit of language and resource-independent n-gram based metrics such as BLEU is undeniable. Despite its c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. criticisms, BLEU is thus still considered the de facto or at least a baseline metric for MT quality evaluation. Due to the cost of human translation, often only one reference translation is available at evaluation time. However, generally there are numerous val"
2015.eamt-1.16,W14-3346,0,0.0214792,"anslations and references (Section 3), and the experiments performed and their results (Section 4). These illustrate how the modified BLEU and NIST scores compare against standard BLEU and NIST scores at the system, document and sentence levels. 2 2.1 N-gram based evaluation BLEU Due to the sparsity of n-grams with large n and the geometric average of n-gram precisions, BLEU is not suitable for sentence-level evaluation. Several smoothing approaches have been proposed to alleviate this issue, such as the standard plus-one smoothing (Lin and Och, 2004) and combinations of smoothing techniques (Chen and Cherry, 2014). A great deal of methods have been proposed to improve the performance of BLEU. These include metrics such as m-bleu (Agarwal and Lavie, 2008) and Amber (Chen and Kuhn, 2011). However, these metrics still treat n-grams in different references equally, regardless of whether the n-gram appears only once or is found in all references. 2.2 The BLEU metric applies a straightforward method of counting the n-grams that overlap in the system translation and given human translations under the assumption that human translations precisely reproduce the meaning of the source text. The closer to the refer"
2015.eamt-1.16,W11-2105,0,0.0190785,"rd BLEU and NIST scores at the system, document and sentence levels. 2 2.1 N-gram based evaluation BLEU Due to the sparsity of n-grams with large n and the geometric average of n-gram precisions, BLEU is not suitable for sentence-level evaluation. Several smoothing approaches have been proposed to alleviate this issue, such as the standard plus-one smoothing (Lin and Och, 2004) and combinations of smoothing techniques (Chen and Cherry, 2014). A great deal of methods have been proposed to improve the performance of BLEU. These include metrics such as m-bleu (Agarwal and Lavie, 2008) and Amber (Chen and Kuhn, 2011). However, these metrics still treat n-grams in different references equally, regardless of whether the n-gram appears only once or is found in all references. 2.2 The BLEU metric applies a straightforward method of counting the n-grams that overlap in the system translation and given human translations under the assumption that human translations precisely reproduce the meaning of the source text. The closer to the reference, the higher the translation quality of the system translation will be. The core formula is given in Eq. 1 (Papineni et al., 2002), so that we can subsequently compare it"
2015.eamt-1.16,N12-1017,0,0.0183525,"lations for a given sentence or document. Different references provide valid variations in linguistic aspects such as style, word choice and word order. Therefore, having multiple reference translations is key to improve the reliability of n-gram based evaluation metrics: the more references, the more chances for n-grams correctly translated to be captured. HyTER, an n-gram matching metric based on an exponential number of reference translations for a given target sentence, demonstrates the potential for better machine translation evaluation results from having as many references as possible (Dreyer and Marcu, 2012). Nevertheless, in the more realistic case where only a few references are available, if these are simply taken as bags of n-grams, increasing the number of references will not lead to the best possible results, as pointed out by Doddington (2002). In this paper we explore how to use multiple references by means other than simply viewing them as bags of n-gram like BLEU, NIST (Doddington, 2002) and other n-gram co-occurrence based metrics do. Our assumption is that each reference reflects the complete meaning of the source segment. The semantic entirety of the translation will be adversely aff"
2015.eamt-1.16,2005.eamt-1.15,0,0.0300963,"sets. They lead to a significant increase in Pearson’s correlation with human fluency judgements at system-level evaluation. The new NIST metric also outperforms the standard NIST for documentlevel evaluation. 1 Introduction Quality evaluation plays a critical role in Machine Translation (MT). Since its conception, the BLEU metric (Papineni et al., 2002) has had a significant impact on MT development. Although human evaluation has been used in recent evaluation campaigns such as WMT (Workshop on Statistical MT) (Bojar et al., 2014) and other forms of reference-less metrics have been proposed (Gamon et al., 2005; Specia et al., 2010), the merit of language and resource-independent n-gram based metrics such as BLEU is undeniable. Despite its c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. criticisms, BLEU is thus still considered the de facto or at least a baseline metric for MT quality evaluation. Due to the cost of human translation, often only one reference translation is available at evaluation time. However, generally there are numerous valid translations for a given sentence or document. Different references provide va"
2015.eamt-1.16,C02-1117,0,0.0163991,", the distribution of n-grams is taken into account to improve Eq. 3. Less overlap among references may indicate that the translation is difficult, or that several different valid translations exist. In this scenario, recurring n-grams tend to be function words rather than content words. For instance, only function words repeat in the three references below, which may indicate that the source can be translated in many ways: a. At this time, the police have blocked the bombing scene. Using Zipf’s law An alternative approach of neutralising function words in references is to use the Zipf’s law. Ha et al. (2002) verify Zipf’s law on n-grams by ranking all n-grams (n ≥ 1). So the n-grams recurring in references in Eq. 3 can be represented by the product between frequency f and the ranking order r of n-grams divided by ref no, as in Eq. 7. R0 = log(1 + r × f /ref no) The new BLEU score, denoted as S BM , i.e., Score of BM, is rewritten in Eq. 8, S BM = BP × exp( b. They have now sealed off the spot. To address the problem, a unit called n-gram divergence is defined as in Eq. 4 to describe the degree of concentration of n-grams among references. The more divergent the distribution of ngrams in the refer"
2015.eamt-1.16,2003.mtsummit-papers.32,0,0.0922099,"fore, reducing the importance of these common n-grams is not beneficial to quality evaluation. 2.3 Improvements on n-gram based metrics Current improvements on the n-gram cooccurrence evaluation metrics can be divided into three categories. The first category extends the scope of similarity detection by using a more 114 flexible matching strategy, for example using WordNet to capture synonyms as in METEOR (Banerjee and Lavie, 2005). The second category uses different functions to calculate the degree of similarity, for example edit distance, error rate, semantic distance (Nießen et al., 2000; Leusch et al., 2003; Snover et al., 2006; Snover et al., 2009). And the last category weights or combines the outcome of similarity functions as features (Liu et al., 2010; Gim´enez and M`arquez, 2010). These methods focus on different forms of comparison between candidates and references. However, to our knowledge there are no other attempts to mine recurring information from multiple references if these are provided. Assuming all the possible translations form a “semantic” space, each reference only covers a subspace. The recurring ngrams among them should constitute the core part of this semantic space, which"
2015.eamt-1.16,W10-1754,0,0.0179351,"s on the n-gram cooccurrence evaluation metrics can be divided into three categories. The first category extends the scope of similarity detection by using a more 114 flexible matching strategy, for example using WordNet to capture synonyms as in METEOR (Banerjee and Lavie, 2005). The second category uses different functions to calculate the degree of similarity, for example edit distance, error rate, semantic distance (Nießen et al., 2000; Leusch et al., 2003; Snover et al., 2006; Snover et al., 2009). And the last category weights or combines the outcome of similarity functions as features (Liu et al., 2010; Gim´enez and M`arquez, 2010). These methods focus on different forms of comparison between candidates and references. However, to our knowledge there are no other attempts to mine recurring information from multiple references if these are provided. Assuming all the possible translations form a “semantic” space, each reference only covers a subspace. The recurring ngrams among them should constitute the core part of this semantic space, which is more likely to represent the meaning of the source text. It is this kind of information that we want to explore and apply with our n-gram weighting"
2015.eamt-1.16,niessen-etal-2000-evaluation,0,0.0940947,"y core meaning. Therefore, reducing the importance of these common n-grams is not beneficial to quality evaluation. 2.3 Improvements on n-gram based metrics Current improvements on the n-gram cooccurrence evaluation metrics can be divided into three categories. The first category extends the scope of similarity detection by using a more 114 flexible matching strategy, for example using WordNet to capture synonyms as in METEOR (Banerjee and Lavie, 2005). The second category uses different functions to calculate the degree of similarity, for example edit distance, error rate, semantic distance (Nießen et al., 2000; Leusch et al., 2003; Snover et al., 2006; Snover et al., 2009). And the last category weights or combines the outcome of similarity functions as features (Liu et al., 2010; Gim´enez and M`arquez, 2010). These methods focus on different forms of comparison between candidates and references. However, to our knowledge there are no other attempts to mine recurring information from multiple references if these are provided. Assuming all the possible translations form a “semantic” space, each reference only covers a subspace. The recurring ngrams among them should constitute the core part of this"
2015.eamt-1.16,P02-1040,0,0.0960889,"formation on the n-gram distribution and on divergences in multiple references, we propose a method of ngram weighting and implement it to generate new versions of the popular BLEU and NIST metrics. Our metrics are tested in two into-English machine translation datasets. They lead to a significant increase in Pearson’s correlation with human fluency judgements at system-level evaluation. The new NIST metric also outperforms the standard NIST for documentlevel evaluation. 1 Introduction Quality evaluation plays a critical role in Machine Translation (MT). Since its conception, the BLEU metric (Papineni et al., 2002) has had a significant impact on MT development. Although human evaluation has been used in recent evaluation campaigns such as WMT (Workshop on Statistical MT) (Bojar et al., 2014) and other forms of reference-less metrics have been proposed (Gamon et al., 2005; Specia et al., 2010), the merit of language and resource-independent n-gram based metrics such as BLEU is undeniable. Despite its c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. criticisms, BLEU is thus still considered the de facto or at least a baseline me"
2015.eamt-1.16,2006.amta-papers.25,0,0.0556441,"portance of these common n-grams is not beneficial to quality evaluation. 2.3 Improvements on n-gram based metrics Current improvements on the n-gram cooccurrence evaluation metrics can be divided into three categories. The first category extends the scope of similarity detection by using a more 114 flexible matching strategy, for example using WordNet to capture synonyms as in METEOR (Banerjee and Lavie, 2005). The second category uses different functions to calculate the degree of similarity, for example edit distance, error rate, semantic distance (Nießen et al., 2000; Leusch et al., 2003; Snover et al., 2006; Snover et al., 2009). And the last category weights or combines the outcome of similarity functions as features (Liu et al., 2010; Gim´enez and M`arquez, 2010). These methods focus on different forms of comparison between candidates and references. However, to our knowledge there are no other attempts to mine recurring information from multiple references if these are provided. Assuming all the possible translations form a “semantic” space, each reference only covers a subspace. The recurring ngrams among them should constitute the core part of this semantic space, which is more likely to re"
2015.eamt-1.16,W09-0441,0,0.0203168,"mon n-grams is not beneficial to quality evaluation. 2.3 Improvements on n-gram based metrics Current improvements on the n-gram cooccurrence evaluation metrics can be divided into three categories. The first category extends the scope of similarity detection by using a more 114 flexible matching strategy, for example using WordNet to capture synonyms as in METEOR (Banerjee and Lavie, 2005). The second category uses different functions to calculate the degree of similarity, for example edit distance, error rate, semantic distance (Nießen et al., 2000; Leusch et al., 2003; Snover et al., 2006; Snover et al., 2009). And the last category weights or combines the outcome of similarity functions as features (Liu et al., 2010; Gim´enez and M`arquez, 2010). These methods focus on different forms of comparison between candidates and references. However, to our knowledge there are no other attempts to mine recurring information from multiple references if these are provided. Assuming all the possible translations form a “semantic” space, each reference only covers a subspace. The recurring ngrams among them should constitute the core part of this semantic space, which is more likely to represent the meaning of"
2015.eamt-1.16,takezawa-etal-2002-toward,0,0.0858887,"tio for recurring n-grams. For instance, for bigrams appearing twice in 10 references, the outcome of Eq. 5 is 0.3424, while for bigrams appearing once, the outcome of Eq. 5 is 0.3222. However since there are only four references, the weighting ratio is larger, 0.3979/0.3522. In other words, the larger the number of references, the lower the impact of the reweighting method on the results. Increasing the number of references could help discriminate function words and content words as well. To check the recurrence of n-grams in larger numbers of references, we investigate the devset13 of BTEC (Takezawa et al., 2002), which contains 1512 source sentences, each with 16 English references. We show the average 1-4grams distribution over 2 to 16 translations in Figure 3. As expected, the proportion of n-grams covered by multiple references decreases as the number of references increases, showing that more translation variety is Conclusions and future work Recurring n-grams in references can help capture important words and sequences of words that are chosen by various translators. By combining recurrence distributions, divergence information and the length of n-grams, a modified weighting strategy for BLEU an"
2015.eamt-1.16,C04-1072,0,0.0499198,"assigning the weights of n-grams that are common in system translations and references (Section 3), and the experiments performed and their results (Section 4). These illustrate how the modified BLEU and NIST scores compare against standard BLEU and NIST scores at the system, document and sentence levels. 2 2.1 N-gram based evaluation BLEU Due to the sparsity of n-grams with large n and the geometric average of n-gram precisions, BLEU is not suitable for sentence-level evaluation. Several smoothing approaches have been proposed to alleviate this issue, such as the standard plus-one smoothing (Lin and Och, 2004) and combinations of smoothing techniques (Chen and Cherry, 2014). A great deal of methods have been proposed to improve the performance of BLEU. These include metrics such as m-bleu (Agarwal and Lavie, 2008) and Amber (Chen and Kuhn, 2011). However, these metrics still treat n-grams in different references equally, regardless of whether the n-gram appears only once or is found in all references. 2.2 The BLEU metric applies a straightforward method of counting the n-grams that overlap in the system translation and given human translations under the assumption that human translations precisely"
2015.eamt-1.16,W05-0909,0,\N,Missing
2015.eamt-1.17,2012.eamt-1.33,1,0.857982,"Missing"
2015.eamt-1.17,C04-1046,0,0.11191,"h levels. We highlight crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between"
2015.eamt-1.17,W12-3156,0,0.102075,"sing automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an"
2015.eamt-1.17,P14-1065,0,0.0530227,"Missing"
2015.eamt-1.17,P10-1064,1,0.914336,"Missing"
2015.eamt-1.17,P14-2047,0,0.0164777,"the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use automatic evaluation m"
2015.eamt-1.17,W13-3303,0,0.0260842,"ssues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use a"
2015.eamt-1.17,P02-1040,0,0.0918502,"and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a document that is fit for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall"
2015.eamt-1.17,P09-2004,0,0.0370577,"omly selected from the full corpus. For PE1 and PE2, only source (English) paragraphs with 3-8 sentences were selected (filter SNUMBER) to ensure that there is enough information beyond sentence-level to be evaluated and make the task feasible for the annotators. These paragraphs were further filtered to select those with cohesive devices. Cohesive devices are linguistic units that play a role in establishing cohesion between clauses, sentences or paragraphs (Halliday and Hasan, 1976). Pronouns and discourse connectives are examples of such devices. A list of pronouns and the connectives from Pitler and Nenkova (2009) was considered for that. Finally, paragraphs were ranked according to the number of cohesive devices they contain and the top 200 paragraphs were selected (filter C-DEV). Table 3 shows the statistics of the initial corpus and the resulting selection after each filter. FULL CORPUS S-NUMBER C-DEV Number of Paragraphs 1, 215 394 200 Number of Cohesive devices 6, 488 3, 329 2, 338 Table 3: WMT13 English source corpus. For the PE1 experiment, the paragraphs in CDEV were randomised. Then, sets containing seven paragraphs each were created. For each set, the sentences of its paragraphs were also ran"
2015.eamt-1.17,potet-etal-2012-collection,0,0.122026,"Missing"
2015.eamt-1.17,2014.eamt-1.21,1,0.877315,"onsider more information than sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a document that is fit for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT"
2015.eamt-1.17,2006.amta-papers.25,0,0.41877,"s on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the final version – HTER (Snover et al., 2006). c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. There are, however, scenarios where quality prediction beyond sentence level is needed, most notably in cases when automatic translations without post-editing are required. This is the case, for example, of quality prediction for an entire product review translation in order to decide whether or not it can be published as is, so that customers speaking other languages can understand it. The quality of a document is often seen as some form of aggregation of the quality"
2015.eamt-1.17,P10-1063,0,0.608392,"han sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a document that is fit for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes)"
2015.eamt-1.17,2009.eamt-1.5,1,0.876811,"ht crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the"
2015.eamt-1.17,D14-1025,0,0.053694,"Missing"
2015.eamt-1.17,C14-2028,0,\N,Missing
2015.eamt-1.17,W13-2201,1,\N,Missing
2015.eamt-1.8,W14-3342,0,0.0188062,"ST toolkit (Specia et al., 2013). It trains QE models using sklearn1 versions of Support Vector Machine (SVM) classifier (for ternary classification task, Section 4.4) and SVM regression (for HTER prediction, Section 4.5). The wordlevel version of Q U E ST2 was used for word-level feature extraction. Word-level classifiers were trained with CRFSuite3 . The CRF error models were trained with CRF++4 . POS tagging was performed with TreeTagger (Schmid, 1994). Sentence-level QuEst uses 17 baseline features5 for all tasks. Word-level QuEst reimplements the set of 30 baseline features described in (Luong et al., 2014). The QE models were built and tested based on the data provided for the WMT14 English–Spanish QE shared task (Section 4.3). The statistics on error distributions were computed using the English–Spanish part of training data for WMT13 shared task on QE6 . The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus (Koehn, 2005). We trained the alignment model with FastAlign (Dyer et al., 2013) and extracted the lexical probabilities tables for words using scripts for phrase table building in Moses (Koehn et al., 2007). For all the"
2015.eamt-1.8,C10-2075,0,0.0231349,"): first the start symbol &lt; s &gt; is generated, then the next words are taken based on the word probability given the already generated words. Other approaches to discriminative LMs use the n-best list of the MT system as training data (Li and Khudanpur, 2008). The translation variant which is closest to the oracle (e.g. has the highest BLEU score) is used as a positive example, while the variant with high system score and low BLEU score is used as a negative example. Such dataset allows the classifier to reduce the differences between the model score and the actual quality score of a sentence. Li et al. (2010) simulate the generation of an n-best list using translation tables from SMT systems. By taking entries from the translation table with the same source side they create a set of alternative translations for a given target phrase. For each sentence, these are combined, generating a confusion set for this sentence. 2.2 Quality estimation for MT QE can be modelled as a classification task where the goal is to distinguish good from bad translations, or to provide a quality score to each translation. Therefore, examples of bad sentences or 52 words produced by the MT system are needed. To the best"
2015.eamt-1.8,P07-1010,0,0.0363864,"results. 2 2.1 Previous work Discriminative language modelling One example of task that requires low quality examples is discriminative language modelling (DLM), i.e., the classification of sentences as ”good” or ”bad”. It was first introduced in a monolingual context within automatic speech recognition (Collins et al., 2005), and later applied to MT. While in speech recognition negative examples can be created from system outputs that differ from the reference (Bhanuprasad and Svenson, 2008), in MT there are multiple correct outputs, so negative examples need to be defined more carefully. In Okanohara (2007) bad sentences used as negative training instances are drawn from the distribution P (wi |wi−N +1 , ..., wi−1 ): first the start symbol &lt; s &gt; is generated, then the next words are taken based on the word probability given the already generated words. Other approaches to discriminative LMs use the n-best list of the MT system as training data (Li and Khudanpur, 2008). The translation variant which is closest to the oracle (e.g. has the highest BLEU score) is used as a positive example, while the variant with high system score and low BLEU score is used as a negative example. Such dataset allows"
2015.eamt-1.8,W05-0909,0,0.0354436,"rate negative training data for QE model learning. Previous work has followed the hypothesis that machine translations can be assumed to have low quality (Gamon et al., 2005). However, this is not the case nowadays: many translations can be considered flawless. Particularly for word-level QE, it is unrealistic to presume that every single word in the MT output is incorrect. Another possibility is to use automatic quality evaluation metrics based on reference translations to provide a quality score for MT data. Metrics such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) can be used to compare the automatic and reference translations. However, these scores can be very unreliable, especially for word-level QE, as every word that differs in form or position would be annotated as bad. Previous efforts have been made for negative data generation, including random generation of sentences from word distributions and the use of translations in low-ranked positions in n-best lists produced by statistical MT (SMT) systems. These methods are however unsuitable for QE at the word level, as they provide no information about the quality of individual words in a sentence."
2015.eamt-1.8,P02-1040,0,0.0966892,"is therefore desirable to devise automatic procedures to generate negative training data for QE model learning. Previous work has followed the hypothesis that machine translations can be assumed to have low quality (Gamon et al., 2005). However, this is not the case nowadays: many translations can be considered flawless. Particularly for word-level QE, it is unrealistic to presume that every single word in the MT output is incorrect. Another possibility is to use automatic quality evaluation metrics based on reference translations to provide a quality score for MT data. Metrics such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) can be used to compare the automatic and reference translations. However, these scores can be very unreliable, especially for word-level QE, as every word that differs in form or position would be annotated as bad. Previous efforts have been made for negative data generation, including random generation of sentences from word distributions and the use of translations in low-ranked positions in n-best lists produced by statistical MT (SMT) systems. These methods are however unsuitable for QE at the word level, as they provide no"
2015.eamt-1.8,I08-2113,0,0.030742,"Missing"
2015.eamt-1.8,P06-1032,0,0.124478,"assumed that these are non-native speakers, who possibly translate the text from their native language. The difference is that in this task the source text is a hidden variable, whereas in MT it is observed. The strategy of adding errors to correct sentences has also been used for this task. Human errors are more intuitive to simulate as language learners explicitly attempt to use natural language grammars. Therefore, rule-based systems can be used to model some grammar errors, particularly those affecting closed class words, e.g. determiner errors (Izumi et al., 2003) or countability errors (Brockett et al., 2006). More recent statistical methods use the distributions of errors in corpora and small seed sets of errors. They often also concentrate on a single error type, usually with closed class words such as articles and prepositions (Rozovskaya and Roth, 2010). Felice and Yuan (2014) go beyond closed class words to evaluate how errors of different types are influenced by various linguistic parameters: text domain, learner’s first language, POS tags and semantic classes of erroneous words. The approach led to the generation of high-quality artificial data for human error correction. However, it could"
2015.eamt-1.8,D10-1094,0,0.0256279,"sentences has also been used for this task. Human errors are more intuitive to simulate as language learners explicitly attempt to use natural language grammars. Therefore, rule-based systems can be used to model some grammar errors, particularly those affecting closed class words, e.g. determiner errors (Izumi et al., 2003) or countability errors (Brockett et al., 2006). More recent statistical methods use the distributions of errors in corpora and small seed sets of errors. They often also concentrate on a single error type, usually with closed class words such as articles and prepositions (Rozovskaya and Roth, 2010). Felice and Yuan (2014) go beyond closed class words to evaluate how errors of different types are influenced by various linguistic parameters: text domain, learner’s first language, POS tags and semantic classes of erroneous words. The approach led to the generation of high-quality artificial data for human error correction. However, it could not be used for MT error identification, as MT errors are different from human errors and usually cannot be assigned to a single type. 3 Generation of artificial data The easiest choice for artificial data generation is to create a sentence by taking al"
2015.eamt-1.8,P05-1063,0,0.0177213,"tely improve upon baseline models by extending the training data with suitable artificially created examples. In Section 2 we further review existing strategies for artificial data generation. We explain our generation strategies in Section 3. In Section 4 we describe our experiment and their results. 2 2.1 Previous work Discriminative language modelling One example of task that requires low quality examples is discriminative language modelling (DLM), i.e., the classification of sentences as ”good” or ”bad”. It was first introduced in a monolingual context within automatic speech recognition (Collins et al., 2005), and later applied to MT. While in speech recognition negative examples can be created from system outputs that differ from the reference (Bhanuprasad and Svenson, 2008), in MT there are multiple correct outputs, so negative examples need to be defined more carefully. In Okanohara (2007) bad sentences used as negative training instances are drawn from the distribution P (wi |wi−N +1 , ..., wi−1 ): first the start symbol &lt; s &gt; is generated, then the next words are taken based on the word probability given the already generated words. Other approaches to discriminative LMs use the n-best list o"
2015.eamt-1.8,N13-1073,0,0.0173659,"Sentence-level QuEst uses 17 baseline features5 for all tasks. Word-level QuEst reimplements the set of 30 baseline features described in (Luong et al., 2014). The QE models were built and tested based on the data provided for the WMT14 English–Spanish QE shared task (Section 4.3). The statistics on error distributions were computed using the English–Spanish part of training data for WMT13 shared task on QE6 . The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus (Koehn, 2005). We trained the alignment model with FastAlign (Dyer et al., 2013) and extracted the lexical probabilities tables for words using scripts for phrase table building in Moses (Koehn et al., 2007). For all the methods, errors were injected into the News Commentary corpus7 . 1 http://scikit-learn.org/ http://github.com/ghpaetzold/quest 3 http://www.chokkan.org/software/crfsuite/ 4 https://code.google.com/p/crfpp/ 5 http://www.quest.dcs.shef.ac.uk/ quest files/features blackbox baseline 17 6 http://www.quest.dcs.shef.ac.uk/ wmt13 qe.html 7 http://statmt.org/wmt14/ training-parallel-nc-v9.tgz 2 4.2 Generated data Combining three methods of errors generation and tw"
2015.eamt-1.8,E14-3013,0,0.11273,"for this task. Human errors are more intuitive to simulate as language learners explicitly attempt to use natural language grammars. Therefore, rule-based systems can be used to model some grammar errors, particularly those affecting closed class words, e.g. determiner errors (Izumi et al., 2003) or countability errors (Brockett et al., 2006). More recent statistical methods use the distributions of errors in corpora and small seed sets of errors. They often also concentrate on a single error type, usually with closed class words such as articles and prepositions (Rozovskaya and Roth, 2010). Felice and Yuan (2014) go beyond closed class words to evaluate how errors of different types are influenced by various linguistic parameters: text domain, learner’s first language, POS tags and semantic classes of erroneous words. The approach led to the generation of high-quality artificial data for human error correction. However, it could not be used for MT error identification, as MT errors are different from human errors and usually cannot be assigned to a single type. 3 Generation of artificial data The easiest choice for artificial data generation is to create a sentence by taking all or some of its words f"
2015.eamt-1.8,2005.eamt-1.15,0,0.0334587,"ated by c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 51 Lucia Specia University of Sheffield Sheffield, United Kingdom l.specia@sheffield.ac.uk human translators, who post-edit automatic translations, mark errors in translations, or rate translations for quality. This process is slow and expensive. It is therefore desirable to devise automatic procedures to generate negative training data for QE model learning. Previous work has followed the hypothesis that machine translations can be assumed to have low quality (Gamon et al., 2005). However, this is not the case nowadays: many translations can be considered flawless. Particularly for word-level QE, it is unrealistic to presume that every single word in the MT output is incorrect. Another possibility is to use automatic quality evaluation metrics based on reference translations to provide a quality score for MT data. Metrics such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) can be used to compare the automatic and reference translations. However, these scores can be very unreliable, especially for word-level QE, as ever"
2015.eamt-1.8,P07-2045,0,0.00371887,"described in (Luong et al., 2014). The QE models were built and tested based on the data provided for the WMT14 English–Spanish QE shared task (Section 4.3). The statistics on error distributions were computed using the English–Spanish part of training data for WMT13 shared task on QE6 . The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus (Koehn, 2005). We trained the alignment model with FastAlign (Dyer et al., 2013) and extracted the lexical probabilities tables for words using scripts for phrase table building in Moses (Koehn et al., 2007). For all the methods, errors were injected into the News Commentary corpus7 . 1 http://scikit-learn.org/ http://github.com/ghpaetzold/quest 3 http://www.chokkan.org/software/crfsuite/ 4 https://code.google.com/p/crfpp/ 5 http://www.quest.dcs.shef.ac.uk/ quest files/features blackbox baseline 17 6 http://www.quest.dcs.shef.ac.uk/ wmt13 qe.html 7 http://statmt.org/wmt14/ training-parallel-nc-v9.tgz 2 4.2 Generated data Combining three methods of errors generation and two methods of errors insertion into sentences resulted in a total of six artificial datasets. Here we perform some analysis on t"
2015.eamt-1.8,2005.mtsummit-papers.11,0,0.0348799,"4 . POS tagging was performed with TreeTagger (Schmid, 1994). Sentence-level QuEst uses 17 baseline features5 for all tasks. Word-level QuEst reimplements the set of 30 baseline features described in (Luong et al., 2014). The QE models were built and tested based on the data provided for the WMT14 English–Spanish QE shared task (Section 4.3). The statistics on error distributions were computed using the English–Spanish part of training data for WMT13 shared task on QE6 . The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus (Koehn, 2005). We trained the alignment model with FastAlign (Dyer et al., 2013) and extracted the lexical probabilities tables for words using scripts for phrase table building in Moses (Koehn et al., 2007). For all the methods, errors were injected into the News Commentary corpus7 . 1 http://scikit-learn.org/ http://github.com/ghpaetzold/quest 3 http://www.chokkan.org/software/crfsuite/ 4 https://code.google.com/p/crfpp/ 5 http://www.quest.dcs.shef.ac.uk/ quest files/features blackbox baseline 17 6 http://www.quest.dcs.shef.ac.uk/ wmt13 qe.html 7 http://statmt.org/wmt14/ training-parallel-nc-v9.tgz 2 4.2"
2015.eamt-1.8,2006.amta-papers.25,0,0.0284299,"vise automatic procedures to generate negative training data for QE model learning. Previous work has followed the hypothesis that machine translations can be assumed to have low quality (Gamon et al., 2005). However, this is not the case nowadays: many translations can be considered flawless. Particularly for word-level QE, it is unrealistic to presume that every single word in the MT output is incorrect. Another possibility is to use automatic quality evaluation metrics based on reference translations to provide a quality score for MT data. Metrics such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) can be used to compare the automatic and reference translations. However, these scores can be very unreliable, especially for word-level QE, as every word that differs in form or position would be annotated as bad. Previous efforts have been made for negative data generation, including random generation of sentences from word distributions and the use of translations in low-ranked positions in n-best lists produced by statistical MT (SMT) systems. These methods are however unsuitable for QE at the word level, as they provide no information about the quali"
2015.eamt-1.8,W09-0441,0,0.0202375,"original sentence, and thus the more natural it looks (lower perplexity). Note that sentences where errors were inserted from a general distribution (unigramWI) have lower perplexity than those generated using using paraphrases. This can be because the unigramWI model tends to choose high-frequency words with lower perplexity, while the constructed paraphrases contain more noise and rare words. 4.3 Experimental setup We evaluated the performance of the artificially generated data in three tasks: the ternary classification of sentences as “good”, “almost good” or “bad”, the prediction of HTER (Snover et al., 2009) score for a sentence, and the classification of words in a sentence as “good” or “bad” (tasks 1.1, 1.2 and 2 of WMT14 QE shared task8 , respectively). 8 http://statmt.org/wmt14/ quality-estimation-task.html 55 The goal of the experiments was to check whether it is possible to improve upon the baseline results by adding artificially generated examples to the training sets. The baseline models for all tasks were trained on the data provided for the corresponding shared tasks for the English–Spanish language pair. All models were tested on the official test sets provided for the corresponding sh"
2015.eamt-1.8,P13-4014,1,0.892559,"Missing"
2015.tc-1.3,W15-5202,0,0.0466142,"cant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a). Resources like MyMemory2 contain large number of bi-segments that can be used in translation memories, but not all the bi-segments are true translations. For this reason, 2 https://mymemory.translated.net/ 19 Barbu (2015) proposed a method based on machine learning for cleaning existing translation memories. 2.3 Incorporation of Language Technology in Translation Memories Translation memories are among the most successfully used tools by professional translators. However, most of these tools rely on little language processing when they match and retrieve segments. Research carried out in the EXPERT project shows that even incorporation of simple language processing such as paraphrasing can help translators (Gupta and Or˘asan, 2014). Rather than expanding the segments stored in a translation memory with all the"
2015.tc-1.3,2014.tc-1.6,1,0.723752,"metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a). Resources like MyMemory2 contain large number of bi-segments that can be used in translation memories, but not all the bi-segments are true translations. For this reason, 2 https://mymemory.translated.net/ 19 Barbu (2015) proposed a method based on machine learning for cleaning existing translation memorie"
2015.tc-1.3,D14-1062,1,0.857312,"Missing"
2015.tc-1.3,C14-1182,1,0.796863,"Missing"
2015.tc-1.3,N15-1043,1,0.871728,"Missing"
2015.tc-1.3,2015.mtsummit-papers.22,1,0.879605,"Missing"
2015.tc-1.3,2014.eamt-1.2,0,0.0541452,"Missing"
2015.tc-1.3,W15-4905,1,0.824476,"Missing"
2015.tc-1.3,2014.amta-researchers.19,1,0.706099,"professional translators reveals that the post-editing effort is also reduced. Logacheva and Specia (2015) investigate ways to collect and generate negative human feedback in various forms, including post-editing, and learn how to improve machine translation systems from this feedback, for example, by building word-level quality estimation models to mimic user feedback and introducing the predictions in SMT decoders. 2.5 Hybrid Approaches to Translation All the existing methods in MT have strengths and weaknesses and one of the most common ways to improve their performance is to combine them. Li et al. (2014) proposed a method for incorporating translation memories and linguistic knowledge in SMT, showing that for English-Chinese and English-French the proposed methods lead to better translations. Translation into morphological rich languages poses challenges to current methods in statistical machine translation. For this problem, Daiber and Sima’an (2015) propose a method which consists of two steps: first the source string is enriched with target morphological features and then fed into a translation model which takes care of reordering and lexical choice that 20 matches the provided morphologic"
2015.tc-1.3,W15-4907,1,0.836766,"by Scarton and Specia (2014) in the EXPERT project focuses on document level quality estimation. Automatic post-editing provides an additional way to simplifying the work of professional translators. Pal (2015) shows how it is possible to apply Hierarchical Phrase Based Statistical Machine Translation to the task of monolingual Statistical Automatic Post-editing. Evaluation using standard MT metrics shows that automatically post-edited texts are better than the raw translations. In addition, an experiment with four professional translators reveals that the post-editing effort is also reduced. Logacheva and Specia (2015) investigate ways to collect and generate negative human feedback in various forms, including post-editing, and learn how to improve machine translation systems from this feedback, for example, by building word-level quality estimation models to mimic user feedback and introducing the predictions in SMT decoders. 2.5 Hybrid Approaches to Translation All the existing methods in MT have strengths and weaknesses and one of the most common ways to improve their performance is to combine them. Li et al. (2014) proposed a method for incorporating translation memories and linguistic knowledge in SMT,"
2015.tc-1.3,P02-1040,0,0.0939051,"uggested was to generate segments on fly from fragments of previously translated segments. An implementation based on pattern matching showed that even such a simple approach can be potentially useful. Another way to address the needs of translators is to design flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible"
2015.tc-1.3,W15-4107,0,0.0154388,"n flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a)."
2015.tc-1.3,2015.mtsummit-wptp.4,0,0.0124165,"n flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a)."
2015.tc-1.3,2015.mtsummit-papers.11,0,0.0130963,"n flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a)."
2015.tc-1.3,W15-5201,0,0.0277695,"largely due to the fact that in many cases the real needs of translators were not considered when designing these tools. To this end, a survey with professional translators was carried out in order to find out their views and requirements regarding various technologies, and their current work practices. Thanks to the help of the commercial partners in the project, the survey received 736 complete responses, from a total of over 1300 responses, which is more than in other similar surveys. A first analysis of the data is presented in (Zaretskaya et al., 2015) with more analyses underway. Parra Escartín (2015) carried out another study with professional translators in an attempt to find out “missing functionalities” of translation memories that could potentially improve their productivity. An interesting feature suggested was to generate segments on fly from fragments of previously translated segments. An implementation based on pattern matching showed that even such a simple approach can be potentially useful. Another way to address the needs of translators is to design flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently test"
2015.tc-1.3,2014.eamt-1.21,1,0.710839,"em in translation memories and statistical machine translation. 2.4 The Human Translator in the Loop Post-editing is one of the most promising ways of integrating the output of machine translation methods in the workflows used by translation companies. Quality estimation methods are used to decide whether a sentence should be translated from scratch or it is good enough to be given to a post-editor. Most of the existing methods focus on estimating the quality of sentences, but in some cases it is necessary to estimate the quality of the translation of a whole document. The work carried out by Scarton and Specia (2014) in the EXPERT project focuses on document level quality estimation. Automatic post-editing provides an additional way to simplifying the work of professional translators. Pal (2015) shows how it is possible to apply Hierarchical Phrase Based Statistical Machine Translation to the task of monolingual Statistical Automatic Post-editing. Evaluation using standard MT metrics shows that automatically post-edited texts are better than the raw translations. In addition, an experiment with four professional translators reveals that the post-editing effort is also reduced. Logacheva and Specia (2015)"
2015.tc-1.3,W14-3323,0,0.0605642,"Missing"
2015.tc-1.3,2015.eamt-1.6,1,\N,Missing
2020.aacl-main.39,Q17-1024,0,0.0656433,"Missing"
2020.aacl-main.39,W19-5406,0,0.137105,"Missing"
2020.aacl-main.39,W17-4763,0,0.0505567,"of the translation in the target language or its adequacy with regard to the source sentence (Specia et al., 2018b). Current SOTA models are learnt with the use of neural networks (NN) (Specia et al., 2018a; Fonseca et al., 2019). The assumption is that representations learned can, to some extent, account for source complexity, target fluency and source-target adequacy. These are fine-tuned from pre-trained word representations extracted using multilingual or cross-lingual sentence encoders such as BERT (Devlin et al., 2018), XLM-R (Conneau et al., 2019) or LASER (Artetxe and Schwenk, 2019). Kim et al. (2017) propose the first breakthrough in neural-based QE with the Predictor-Estimator modular architecture. The Predictor model is an encoder-decoder Recurrent Neural Network (RNN) model trained on a huge amount of parallel data for a word prediction task. Its output is fed to the Estimator, a unidirectional RNN trained on QE data, to produce the quality estimates. Kepler et al. (2019) use a similar architecture where the Predictor model is replaced by pretrained contextualised word representations such as BERT (Devlin et al., 2018) or XLM-R (Conneau et al., 2019). Despite achieving strong performan"
2020.aacl-main.39,I17-2050,0,0.057001,"et al. (2020) propose exploiting information provided by the NMT system itself. By exploring uncertainty quantification methods, they show that the confidence with which the NMT system produces its translation correlates well with its quality. Although not performing as well as SOTA supervised models, their approach has the main advantage to be unsupervised and not rely on labelled data. Multilinguality Multilinguality allows training a single model to perform a task from and to multiple languages. This principle has been successfully applied to NMT (Dong et al., 2015; Firat et al., 2016b,a; Nguyen and Chiang, 2017). Aharoni et al. (2019) stretches this approach by translating up to 102 languages from and to English using a Transformer model (Vaswani et al., 2017). They show that multilingual many-to-many models are effective in low resource settings. Multilinguality also allows for zero-shot translation (Johnson et al., 2017). With a simple encoder-decoder architecture and without explicit bridging between source and target languages, they show that their model is able to build a form of inter-lingual representation between all involved language pairs. Shah and Specia (2016) is the only work in QE that"
2020.aacl-main.39,N16-1069,1,0.781305,"15; Firat et al., 2016b,a; Nguyen and Chiang, 2017). Aharoni et al. (2019) stretches this approach by translating up to 102 languages from and to English using a Transformer model (Vaswani et al., 2017). They show that multilingual many-to-many models are effective in low resource settings. Multilinguality also allows for zero-shot translation (Johnson et al., 2017). With a simple encoder-decoder architecture and without explicit bridging between source and target languages, they show that their model is able to build a form of inter-lingual representation between all involved language pairs. Shah and Specia (2016) is the only work in QE that attempted to explore models for more than one language. They use multitask learning with annotators or languages as multiple tasks. In a traditional black-box feature-based approach with Gaussian Processes as learning algorithm, their results suggest that adequately modelling the additional data is as important as the additional data itself. The multilingual models led to marginal improvements over bilingual ones. In addition, the experiments were only conducted with English translation into two closely related languages (French and Spanish). 3 Multilingual QE In t"
2020.aacl-main.39,W18-6451,1,0.904979,"Missing"
2020.aacl-main.39,2009.eamt-1.5,1,0.722617,"Missing"
2020.aacl-main.39,2020.acl-main.558,1,0.829792,"Missing"
2020.acl-main.113,W19-5356,1,0.845684,"nces and measure the (cosine) similarity between them. Similarly, in (Fomicheva et al., 2015; Servan et al., 2016; T¨attar and Fishel, 2017) two words are considered to match if their cosine distance in the embedding space is above a certain threshold. The embeddings are thus used to provide a binary decision. MEANT 2.0 (Lo, 2017) and YISI (Lo, 2019) also relies on matching of words in the embedding space, but this is only used to score the similarity between pairs of words that have already been aligned based on their semantic roles, rather than to find the alignments between words. Finally, Chow et al. (2019) and Echizen’ya et al. (2019) perform the alignment in the embedding space using Earth Mover’s Distance with some special treatment for word order. All of these metrics are however still limited to variance in the words used (even in the continuous space), 1219 rather than more general stylistic or structural variations which can only be captured with multiple references. Another way of incorporating linguistic variation is pseudo-reference approach by Albrecht and Hwa (2007). They leverage various off-the-shelf MT systems to generate additional imperfect references and use them instead or alo"
2020.acl-main.113,W14-3348,0,0.335471,"nction for computing hyp-mt∗ , we are evaluating recall on the MT output, whereas BLEU is designed as a precision-oriented metric. But the choice of similarity function is orthogonal to the goal of this paper, and we leave further refinements in this direction to future work. 1222 TER (Translation Edit Rate) (Snover et al., 2006). TER computes the edit distance defined as the minimum number of word substitutions, deletions, insertions and shifts that are needed to convert MT into the reference. ChrF (Popovi´c, 2015). ChrF calculates the Fscore of character n-grams of maximum length 6. Meteor (Denkowski and Lavie, 2014). Meteor aligns MT output to the reference translation using synonyms and paraphrases besides exact word matching. The similarity is based on the proportion of aligned words in the candidate and in the reference and a fragmentation penalty. BERTScore. (Zhang et al., 2019). We also looked at this very recent metric (published after the submission of this paper), which uses powerful pre-trained embeddings. BERTScore computes a cosine similarity score for each token in the MT output with each token in the reference sentence using contextual embeddings from BERT (Devlin et al., 2019), which can ge"
2020.acl-main.113,D19-1632,1,0.893232,"Missing"
2020.acl-main.113,2005.mtsummit-papers.11,0,0.0154581,"ian-English dataset. This is a new dataset we collected which contains 1K sentences randomly selected from Wikipedia articles in Estonian and translated into English. Two human reference translations were generated independently by two professional translators. All the NMT models were trained using the Fairseq toolkit based on the standard Transformer architecture (Vaswani et al., 2017a) and the training settings described in Ott et al. (2018). We used publicly available parallel datasets for training the models: the Rapid corpus of EU press releases (Rozis and Skadin¸sˇ , 2017) and Europarl (Koehn, 2005), which amount to around 4M parallel sentences in total. A set of 400 segments were translated by the model variants described in §3 to assess the impact of uncertainty types. The following settings were used for model variants. For MC dropout we use dropout rate of 0.3, same as for training the basic Transformer model. Additional hypotheses were produced by performing N stochastic forward passes through the network with dropout, as described in §3. For this analysis we use N = 30, which was shown to perform well for uncertainty quantification (Dong et al., 2018). We also test how the number o"
2020.acl-main.113,W19-5302,0,0.171592,"potheses by exploring uncertainty in NMT models. We show that a light-weight Bayesian approximation method – Monte Carlo Dropout, which allows for uncertainty quantification by using dropout at inference time (Gal and Ghahramani, 2016) – works the best for the purpose of automatic MT evaluation; (2) We devise methods to effectively explore multiple MT hypotheses to better evaluate MT output quality with existing evaluation metrics. On two different datasets, we achieve a large improvement in correlation with human judgments automatic evaluation can be by and large considered a solved problem (Ma et al., 2019a). 2 The goal of this paper is not to evaluate the search space of the MT system, but to improve the evaluation of the given MT output by using additional hypotheses. Evaluating the NMT search space beyond the generated output could be an interesting direction to explore in future work. over using both single reference and multiple references. To the best of our knowledge, this is the first work to leverage NMT model uncertainty for automatic MT evaluation. 2 Related Work Meteor (Banerjee and Lavie, 2005) was the first MT evaluation metric to relax the exact match constraint between MT system"
2020.acl-main.113,W17-4771,0,0.0358305,"Missing"
2020.acl-main.113,W18-6301,0,0.025601,"95 segments with an average DA score of 80.22) to minimise this issue, but the results reported here for English-Czech should be interpreted with caution. Wikipedia Estonian-English dataset. This is a new dataset we collected which contains 1K sentences randomly selected from Wikipedia articles in Estonian and translated into English. Two human reference translations were generated independently by two professional translators. All the NMT models were trained using the Fairseq toolkit based on the standard Transformer architecture (Vaswani et al., 2017a) and the training settings described in Ott et al. (2018). We used publicly available parallel datasets for training the models: the Rapid corpus of EU press releases (Rozis and Skadin¸sˇ , 2017) and Europarl (Koehn, 2005), which amount to around 4M parallel sentences in total. A set of 400 segments were translated by the model variants described in §3 to assess the impact of uncertainty types. The following settings were used for model variants. For MC dropout we use dropout rate of 0.3, same as for training the basic Transformer model. Additional hypotheses were produced by performing N stochastic forward passes through the network with dropout, a"
2020.acl-main.113,P02-1040,0,0.110798,"se comparisons for H 0 hypotheses. As before, ∗ corresponds to different ways of combining similarity scores: average, minimum and maximum. Second, as before, we give a higher weight to the MT output whose quality we wish to evaluate (o). To that end we compare the MT output against additional generated hypotheses. This comparison Figure 2: Methods to explore similarities between MT output, system hypotheses and references. 4.3 Similarity Functions To measure similarity amongst hypotheses and against the reference(s), we experiment with the following standard MT evaluation metrics:3 sentBLEU (Papineni et al., 2002). BLEU measures the similarity between MT and the reference translation based on the number of matching ngrams. We use a smoothed version of BLEU as described by Lin and Och (2004) with N = 4. 3 We use these metrics out of the box. Better results could possibly be achieved by adapting them to our settings, e.g. by changing the weight of precision and recall depending on the direction of the comparison between MT output, hypotheses and the reference. For instance, when using BLEU as similarity function for computing hyp-mt∗ , we are evaluating recall on the MT output, whereas BLEU is designed a"
2020.acl-main.113,D19-1073,0,0.113681,"Missing"
2020.acl-main.113,W16-2342,0,0.0121873,"rated output could be an interesting direction to explore in future work. over using both single reference and multiple references. To the best of our knowledge, this is the first work to leverage NMT model uncertainty for automatic MT evaluation. 2 Related Work Meteor (Banerjee and Lavie, 2005) was the first MT evaluation metric to relax the exact match constraint between MT system output and reference translation by allowing matching of lemmas, synonyms or paraphrases. However, this requires linguistic resources which do not exist for most languages. Character-based metrics (Popovi´c, 2015; Wang et al., 2016) also relax the exact word match constraint by allowing the matching of characters. However, ultimately they still assume a surfacelevel similarity between reference and MT. A more recent direction compares MT and reference sentences in the embedding space. Chen and Guo (2015) extract word embedding representations for the two sentences and measure the (cosine) similarity between them. Similarly, in (Fomicheva et al., 2015; Servan et al., 2016; T¨attar and Fishel, 2017) two words are considered to match if their cosine distance in the embedding space is above a certain threshold. The embedding"
2020.acl-main.114,W18-6402,1,0.718226,"whole sentence, while document-level QE predicts the translation quality of an entire document, even though in practice in literature the documents have been limited to a small set of 3-5 sentences (Specia et al., 2018b). Existing work has only explored textual context. We posit that to judge (or estimate) the quality of a translated text, additional context is paramount. Sentences or short documents taken out of context may lack information on the correct translation of certain (esp. ambiguous) constructions. Inspired by recent work on multimodal machine learning (Baltrusaitis et al., 2019; Barrault et al., 2018), we propose to explore the visual modality in addition to the text modality for this task. Multimodality through vision offers interesting opportunities for real-life data since texts are in∗ Two authors contributed equally. creasingly accompanied with visual elements such as images or videos, especially in social media but also in domains such as e-commerce. Multimodality has not yet been applied to QE. Table 1 shows an example from our e-commerce dataset in which multimodality could help to improve QE. Here, the English noun shorts is translated by the adjective court (for the adjective sho"
2020.acl-main.114,C04-1046,0,0.179445,"ence-level and document-level predictions, we show that state-of-the-art neural and feature-based QE frameworks obtain better results when using the additional modality. 1 MT (FR) Danskin Women’s Bermuda Shorts Bermuda Danskin f´eminines court Table 1: Example of incorrectly machine-translated text: the word shorts is used to indicate short trousers, but gets translated in French as court, the adjective short. Here multimodality could help to detect the error (extracted from the Amazon Reviews Dataset of McAuley et al., 2015). Introduction Quality Estimation (QE) for Machine Translation (MT) (Blatz et al., 2004; Specia et al., 2009) aims to predict the quality of a machine-translated text without using reference translations. It estimates a label (a category, such as ‘good’ or ‘bad’, or a numerical score) for a translation, given text in a source language and its machine translation in a target language (Specia et al., 2018b). QE can operate at different linguistic levels, including sentence and document levels. Sentence-level QE estimates the translation quality of a whole sentence, while document-level QE predicts the translation quality of an entire document, even though in practice in literature"
2020.acl-main.114,N19-1423,0,0.0199887,"veral ways of incorporating visual information in neural-based and featurebased QE architectures; and (iii) we achieve the state-of-the-art performance for such architectures in document and sentence-level QE. 2 2.1 Experimental Settings QE Frameworks and Models through a dense layer with sigmoid activation to generate the quality estimates. Additionally, we propose and experiment with BERT-BiRNN, a variant of the BiRNN model. Rather than training the token embeddings with the task at hand, we use large-scale pre-trained token-level representations from the multilingual cased base BERT model (Devlin et al., 2019). During training, the BERT model is fine-tuned by unfreezing the weights of the last four hidden layers along with the token embedding layer. This performs comparably to the state-of-the-art predictorestimator neural model in Kepler et al. (2019). 2.2 We explore feature-based and neural-based models from two open-source frameworks: QuEst++: QuEst++ (Specia et al., 2015) is a feature-based QE framework composed of two modules: a feature extractor module, to extract the relevant QE features from both the source sentences and their translations, and a machine learning module. We only use this fr"
2020.acl-main.114,W19-5401,0,0.0257184,"Missing"
2020.acl-main.114,P15-1174,0,0.067934,"or each word, it can be expected to have a bigger impact on the result. 4 Results We use the standard training, development and test datasets from the WMT’18 Task 4 track. For feature-based systems, we follow the built-in crossvalidation in QuEst++, and train a single model with the hyperparameters found by cross-validation. For neural-based models, we use early-stopping with a patience of 10 to avoid over-fitting, and all reported figures are averaged over 5 runs corresponding to different seeds. We follow the evaluation method of the WMT QE tasks: Pearson’s r correlation as the main metric (Graham, 2015), Mean-Absolute Error (MAE) and Root-Mean-Squared Error (RMSE) as secondary metrics. For statistical significance on Pearson’s r, we compute Williams test (Williams, 1959) as suggested by Graham and Baldwin (2014). For all neural-based models, we experiment with the all three integration strategies (‘embed’, ‘annot’ and ‘last’) and all three fusion strategies (‘conc’, ‘mult’ and ‘mult2’) presented in Section 3.2. This leads to 6 multimodal models for each BiRNN and BERT-BiRNN. In Tables 2 and 4, as well as in Figures 2 and 3, we report the top three performing models. We refer the reader to th"
2020.acl-main.114,D14-1020,0,0.021333,"follow the built-in crossvalidation in QuEst++, and train a single model with the hyperparameters found by cross-validation. For neural-based models, we use early-stopping with a patience of 10 to avoid over-fitting, and all reported figures are averaged over 5 runs corresponding to different seeds. We follow the evaluation method of the WMT QE tasks: Pearson’s r correlation as the main metric (Graham, 2015), Mean-Absolute Error (MAE) and Root-Mean-Squared Error (RMSE) as secondary metrics. For statistical significance on Pearson’s r, we compute Williams test (Williams, 1959) as suggested by Graham and Baldwin (2014). For all neural-based models, we experiment with the all three integration strategies (‘embed’, ‘annot’ and ‘last’) and all three fusion strategies (‘conc’, ‘mult’ and ‘mult2’) presented in Section 3.2. This leads to 6 multimodal models for each BiRNN and BERT-BiRNN. In Tables 2 and 4, as well as in Figures 2 and 3, we report the top three performing models. We refer the reader to the Appendix for the full set of results. 4.1 Sentence-level MQE The first part of Table 2 presents the results for sentence-level multimodal QE with BiRNN. The best model is BiRNN+Vis-embed-mult2, achieving a Pears"
2020.acl-main.114,C18-1266,1,0.811029,"d models from two open-source frameworks: QuEst++: QuEst++ (Specia et al., 2015) is a feature-based QE framework composed of two modules: a feature extractor module, to extract the relevant QE features from both the source sentences and their translations, and a machine learning module. We only use this framework for our experiments on document-level QE, since it does not perform well enough for sentence-level prediction. We use the same model (Support Vector Regression), hyperparameters and feature settings as the baseline model for the document-level QE task at WMT’18. deepQuest: deepQuest (Ive et al., 2018) is a neural-based framework that provides state-of-theart models for multi-level QE. We use the BiRNN model, a light-weight architecture which can be trained at either sentence or document level. The BiRNN model uses an encoder-decoder architecture: it takes on its input both the source sentence and its translation which are encoded separately by two independent bi-directional Recurrent Neural Networks (RNNs). The two resulting sentence representations are then concatenated as a weighted sum of their word vectors, generated by an attention mechanism. For sentence-level predictions, the weight"
2020.acl-main.114,P19-3020,0,0.0355499,"Missing"
2020.acl-main.114,2009.eamt-1.5,1,0.718831,"ent-level predictions, we show that state-of-the-art neural and feature-based QE frameworks obtain better results when using the additional modality. 1 MT (FR) Danskin Women’s Bermuda Shorts Bermuda Danskin f´eminines court Table 1: Example of incorrectly machine-translated text: the word shorts is used to indicate short trousers, but gets translated in French as court, the adjective short. Here multimodality could help to detect the error (extracted from the Amazon Reviews Dataset of McAuley et al., 2015). Introduction Quality Estimation (QE) for Machine Translation (MT) (Blatz et al., 2004; Specia et al., 2009) aims to predict the quality of a machine-translated text without using reference translations. It estimates a label (a category, such as ‘good’ or ‘bad’, or a numerical score) for a translation, given text in a source language and its machine translation in a target language (Specia et al., 2018b). QE can operate at different linguistic levels, including sentence and document levels. Sentence-level QE estimates the translation quality of a whole sentence, while document-level QE predicts the translation quality of an entire document, even though in practice in literature the documents have be"
2020.acl-main.114,P15-4020,1,0.831562,"experiment with BERT-BiRNN, a variant of the BiRNN model. Rather than training the token embeddings with the task at hand, we use large-scale pre-trained token-level representations from the multilingual cased base BERT model (Devlin et al., 2019). During training, the BERT model is fine-tuned by unfreezing the weights of the last four hidden layers along with the token embedding layer. This performs comparably to the state-of-the-art predictorestimator neural model in Kepler et al. (2019). 2.2 We explore feature-based and neural-based models from two open-source frameworks: QuEst++: QuEst++ (Specia et al., 2015) is a feature-based QE framework composed of two modules: a feature extractor module, to extract the relevant QE features from both the source sentences and their translations, and a machine learning module. We only use this framework for our experiments on document-level QE, since it does not perform well enough for sentence-level prediction. We use the same model (Support Vector Regression), hyperparameters and feature settings as the baseline model for the document-level QE task at WMT’18. deepQuest: deepQuest (Ive et al., 2018) is a neural-based framework that provides state-of-theart mode"
2020.acl-main.424,I17-1030,1,0.922576,"Missing"
2020.acl-main.424,D19-3009,1,0.86211,"ms added, deleted and kept by the simplification system. It does so by comparing the output of the simplification model to multiple references and the original sentence, using both precision and recall. BLEU has shown positive correlation with human judgements of grammaticality and meaning preserˇ vation (Stajner et al., 2014; Wubben et al., 2012; Xu et al., 2016), while SARI has high correlation with judgements of simplicity gain (Xu et al., 2016). In our experiments, we used the implementations of these metrics available in the EASSE package for automatic sentence simplification evaluation (Alva-Manchego et al., 2019).5 We computed all the scores at sentence-level as in the experiment by Xu et al. (2016), where they compared sentencelevel correlations of FKGL, BLEU and SARI with human ratings. We used a smoothed sentence-level version of BLEU so that comparison is possible, 5 https://github.com/feralvam/easse System Outputs. We used publicly-available simplifications produced by automatic SS systems: PBSMT-R (Wubben et al., 2012), which is a phrase-based MT model; Hybrid (Narayan and Gardent, 2014), which uses phrase-based MT coupled with semantic analysis; SBSMT-SARI (Xu et al., 2016), which relies on syn"
2020.acl-main.424,2020.cl-1.4,1,0.846425,"experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed. 1 Introduction Sentence Simplification (SS) consists in modifying the content and structure of a sentence to make it easier to understand, while retaining its main idea and most of its original meaning (Alva-Manchego et al., 2020). Simplified texts can benefit non-native speakers (Paetzold, 2016), people suffering from aphasia (Carroll et al., 1998), dyslexia (Rello et al., 2013) or autism (Evans et al., 2014). They also help language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Silveira and ∗ Equal Contribution Branco, 2012), and machine translation (Hasler et al., 2017). In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e."
2020.acl-main.424,W19-5301,0,0.0593597,"Missing"
2020.acl-main.424,D18-1289,0,0.0383172,"Missing"
2020.acl-main.424,C96-2183,0,0.822657,"T, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed. 1 Introduction Sentence Simplification (SS) consists in modifying the content and structure of a sentence to make it easier to understand, while retaining its main idea and most of its original meaning (Alva-Manchego et al., 2020). Simplified texts can benefit non-native speakers (Paetzold, 2016), people suffering from aphasia (Carroll et al., 1998), dyslexia (Rello et al., 2013) or autism (Evans et al., 2014). They also help language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Silveira and ∗ Equal Contribution Branco, 2012), and machine translation (Hasler et al., 2017). In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the sentence more complicated (Petersen, 2007; Alu´ısio et al., 2008; Bott and Saggion, 2011). However, models for automatic SS are evaluated on datasets whose simplifications are not representative of this va"
2020.acl-main.424,P11-2117,0,0.0921157,"ese studies, it can be argued that the scope of rewriting transformations involved in the simplification process goes beyond only replacing words with simpler synonyms. In fact, human perception of complexity is most affected by syntactic features related to sentence structure (Brunato et al., 2018). Therefore, since human editors make several changes to both the lexical content and syntactic structure of sentences when simplifying them, we should expect that models for automatic sentence simplification can also make such changes. Evaluation Data for SS Most datasets for SS (Zhu et al., 2010; Coster and Kauchak, 2011; Hwang et al., 2015) consist of automatic sentence alignments between related articles in English Wikipedia (EW) and Simple English Wikipedia (SEW). In SEW, contributors are asked to write texts using simpler language, such as by shortening sentences or by using words from Basic English (Ogden, 1930). However, Yasseri et al. (2012) found that the syntactic complexity of sentences in SEW is almost the same as in EW. In addition, Xu et al. (2015) determined that automaticallyaligned simple sentences are sometimes just as complex as their original counterparts, with only a few words replaced or"
2020.acl-main.424,W14-1215,0,0.239238,"ivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed. 1 Introduction Sentence Simplification (SS) consists in modifying the content and structure of a sentence to make it easier to understand, while retaining its main idea and most of its original meaning (Alva-Manchego et al., 2020). Simplified texts can benefit non-native speakers (Paetzold, 2016), people suffering from aphasia (Carroll et al., 1998), dyslexia (Rello et al., 2013) or autism (Evans et al., 2014). They also help language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Silveira and ∗ Equal Contribution Branco, 2012), and machine translation (Hasler et al., 2017). In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the sentence more complicated (Petersen, 2007; Alu´ısio et al., 2008; Bott and Saggion, 2011). However, models for automat"
2020.acl-main.424,W13-2305,0,0.114853,"to submit their level of agreement (0: Strongly disagree, 100: Strongly agree) with the following statements: 1. The Simplified sentence adequately expresses the meaning of the Original, perhaps omitting the least important information. 2. The Simplified sentence is fluent, there are no grammatical errors. 3. The Simplified sentence is easier to understand than the Original sentence. Using continuous scales when crowdsourcing human evaluations is common practice in Machine Translation (Bojar et al., 2018; Barrault et al., 2019), since it results in higher levels of interannotator consistency (Graham et al., 2013). The six sentence pairs for the Rating QT consisted of: • Three submissions to the Annotation QT, manually selected so that one contains splitting, one has a medium level of compression, and one contains grammatical and spelling mistakes. These allowed to check that the particular characteristics of each sentence pair affect the corresponding evaluation criteria. • One sentence pair from WikiLarge where the Original and the Simplification had no relation to each other. This served to check the attention level of the worker. All submitted ratings were manually reviewed to validate the quality"
2020.acl-main.424,N15-1022,0,0.0552503,"ued that the scope of rewriting transformations involved in the simplification process goes beyond only replacing words with simpler synonyms. In fact, human perception of complexity is most affected by syntactic features related to sentence structure (Brunato et al., 2018). Therefore, since human editors make several changes to both the lexical content and syntactic structure of sentences when simplifying them, we should expect that models for automatic sentence simplification can also make such changes. Evaluation Data for SS Most datasets for SS (Zhu et al., 2010; Coster and Kauchak, 2011; Hwang et al., 2015) consist of automatic sentence alignments between related articles in English Wikipedia (EW) and Simple English Wikipedia (SEW). In SEW, contributors are asked to write texts using simpler language, such as by shortening sentences or by using words from Basic English (Ogden, 1930). However, Yasseri et al. (2012) found that the syntactic complexity of sentences in SEW is almost the same as in EW. In addition, Xu et al. (2015) determined that automaticallyaligned simple sentences are sometimes just as complex as their original counterparts, with only a few words replaced or dropped and the rest"
2020.acl-main.424,W02-0109,0,0.0656209,"table) have been shown to be best indicators of word complexity (Paetzold and Specia, 2016). The ratio is then the value of this score on the simplification divided by that of the original sentence. In order to quantify the rewriting transformations, we computed several low-level features for all simplification instances using the tseval package (Martin et al., 2018): • Number of sentence splits: Corresponds to the difference between the number of sentences in the simplification and the number of sentences in the original sentence. In tseval, the number of sentences is calculated using NLTK (Loper and Bird, 2002). • Compression level: Number of characters in the simplification divided by the number of characters in the original sentence. • Dependency tree depth ratio: We compute the ratio of the depth of the dependency parse tree of the simplification relative to that of the original sentence. When a simplification is composed by more than one sentence, we choose the maximum depth of all dependency trees. Parsing is performed using spaCy.4 This feature serves as a proxy to measure improvements in structural simplicity. • Replace-only Levenshtein distance: Computed as the normalised character-level Lev"
2020.acl-main.424,W18-7005,1,0.877343,"Missing"
2020.acl-main.424,P14-1041,0,0.0247229,"mentations of these metrics available in the EASSE package for automatic sentence simplification evaluation (Alva-Manchego et al., 2019).5 We computed all the scores at sentence-level as in the experiment by Xu et al. (2016), where they compared sentencelevel correlations of FKGL, BLEU and SARI with human ratings. We used a smoothed sentence-level version of BLEU so that comparison is possible, 5 https://github.com/feralvam/easse System Outputs. We used publicly-available simplifications produced by automatic SS systems: PBSMT-R (Wubben et al., 2012), which is a phrase-based MT model; Hybrid (Narayan and Gardent, 2014), which uses phrase-based MT coupled with semantic analysis; SBSMT-SARI (Xu et al., 2016), which relies on syntax-based MT; NTSSARI (Nisioi et al., 2017), a neural sequence-tosequence model with a standard encoder-decoder architecture; and ACCESS (Martin et al., 2020), an encoder-decoder architecture conditioned on explicit attributes of sentence simplification. Collection of Human Ratings. We randomly chose 100 original sentences from ASSET and, for each of them, we sampled one system simplification. The automatic simplifications were selected so that the distribution of simplification transf"
2020.acl-main.424,P17-2014,0,0.10479,"scores at sentence-level as in the experiment by Xu et al. (2016), where they compared sentencelevel correlations of FKGL, BLEU and SARI with human ratings. We used a smoothed sentence-level version of BLEU so that comparison is possible, 5 https://github.com/feralvam/easse System Outputs. We used publicly-available simplifications produced by automatic SS systems: PBSMT-R (Wubben et al., 2012), which is a phrase-based MT model; Hybrid (Narayan and Gardent, 2014), which uses phrase-based MT coupled with semantic analysis; SBSMT-SARI (Xu et al., 2016), which relies on syntax-based MT; NTSSARI (Nisioi et al., 2017), a neural sequence-tosequence model with a standard encoder-decoder architecture; and ACCESS (Martin et al., 2020), an encoder-decoder architecture conditioned on explicit attributes of sentence simplification. Collection of Human Ratings. We randomly chose 100 original sentences from ASSET and, for each of them, we sampled one system simplification. The automatic simplifications were selected so that the distribution of simplification transformations (e.g. sentence splitting, compression, paraphrases) would match that from human simplifications in ASSET. That was done so that we could obtain"
2020.acl-main.424,C16-1069,1,0.880195,"racteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed. 1 Introduction Sentence Simplification (SS) consists in modifying the content and structure of a sentence to make it easier to understand, while retaining its main idea and most of its original meaning (Alva-Manchego et al., 2020). Simplified texts can benefit non-native speakers (Paetzold, 2016), people suffering from aphasia (Carroll et al., 1998), dyslexia (Rello et al., 2013) or autism (Evans et al., 2014). They also help language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Silveira and ∗ Equal Contribution Branco, 2012), and machine translation (Hasler et al., 2017). In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the se"
2020.acl-main.424,P02-1040,0,0.116616,"of the Association for Computational Linguistics, pages 4668–4679 c July 5 - 10, 2020. 2020 Association for Computational Linguistics original sentence). Simplifications in ASSET were collected via crowdsourcing (§ 3), and encompass a variety of rewriting transformations (§ 4), which make them simpler than those in TurkCorpus and HSplit (§ 5), thus providing an additional suitable benchmark for comparing and evaluating automatic SS models. In addition, we study the applicability of standard metrics for evaluating SS using simplifications in ASSET as references (§ 6). We analyse whether BLEU (Papineni et al., 2002) or SARI (Xu et al., 2016) scores correlate with human judgements of fluency, adequacy and simplicity, and find that neither of the metrics shows a strong correlation with simplicity ratings. This motivates the need for developing better metrics for assessing SS when multiple rewriting transformations are performed. We make the following contributions: • A high quality large dataset for tuning and evaluation of SS models containing simplifications produced by applying multiple rewriting transformations.1 • An analysis of the characteristics of the dataset that turn it into a new suitable bench"
2020.acl-main.424,Q16-1005,0,0.027315,"mplification), we crowdsourced 15 human ratings on fluency (i.e. grammaticality), adequacy (i.e. meaning preservation) and simplicity, using the same worker selection criteria and HIT design of the Qualification Test as in § 5.1. 6.2 Inter-Annotator Agreement We followed the process suggested in (Graham et al., 2013). First, we normalised the scores of each rater by their individual mean and standard deviation, which helps eliminate individual judge preferences. Then, the normalised continuous scores were converted to five interval categories using equally spaced bins. After that, we followed Pavlick and Tetreault (2016) and computed quadratic weighted Cohen’s κ (Cohen, 1968) simulating two raters: for each sentence, we chose one worker’s rating as the category for annotator A, and selected the rounded average scores for the remaining workers as the category for annotator B. We then computed κ for this pair over the whole dataset. We repeated the process 1,000 times to compute the mean and variance of κ. The resulting values are: 0.687 ± 0.028 for Fluency, 0.686 ± 0.030 for Meaning and 0.628 ± 0.032 for Simplicity. All values point to a moderate level 4675 Metric References BLEU ASSET TurkCorpus ASSET TurkCor"
2020.acl-main.424,W14-1210,0,0.0497303,"litting the sentences. This prevents evaluating a model’s ability to perform a more diverse set of rewriting transformations when simplifying sentences. HSplit (Sulem et al., 2018a), on the other hand, provides simplifications involving only splitting for sentences in the test set of TurkCorpus. We build on TurkCorpus and HSplit by collecting a dataset that provides several manuallyproduced simplifications involving multiple types of rewriting transformations. 2.3 Crowdsourcing Manual Simplifications A few projects have been carried out to collect manual simplifications through crowdsourcing. Pellow and Eskenazi (2014a) built a corpus of everyday documents (e.g. driving test preparation materials), and analysed the feasibly of crowdsourcing their sentence-level simplifications. Of all the quality control measures taken, the most successful was providing a training session to workers, since it allowed to block spammers and those without the skills to perform the task. Additionally, they proposed to use workers’ self-reported confidence scores to flag submissions that could be discarded or reviewed. Later on, Pellow and Eskenazi (2014b) presented a preliminary study on producing simplifications through a col"
2020.acl-main.424,L18-1685,1,0.858597,"uld be discarded or reviewed. Later on, Pellow and Eskenazi (2014b) presented a preliminary study on producing simplifications through a collaborative process. Groups of four workers were assigned one sentence to simplify, and they had to discuss and agree on the process to perform it. Unfortunately, the data collected in these studies is no longer publicly available. Simplifications in TurkCorpus were also collected through crowdsourcing. Regarding the methodology followed, Xu et al. (2016) only report removing bad workers after manual check of their first several submissions. More recently, Scarton et al. (2018) used volunteers to collect simplifications for SimPA, a dataset with sentences from the Public Administration domain. One particular characteristic of the methodology followed is that lexical and syntactic simplifications were performed independently. 3 Creating ASSET We extended TurkCorpus (Xu et al., 2016) by using the same original sentences, but crowdsourced manual simplifications that encompass a richer set of rewriting transformations. Since TurkCorpus was adopted as the standard dataset for evaluating SS models, several system outputs on this data are already publicly available (Zhang"
2020.acl-main.424,D18-1081,0,0.370088,"Missing"
2020.acl-main.424,N18-1063,0,0.0727483,"Missing"
2020.acl-main.424,L18-1615,0,0.026282,"SEW is almost the same as in EW. In addition, Xu et al. (2015) determined that automaticallyaligned simple sentences are sometimes just as complex as their original counterparts, with only a few words replaced or dropped and the rest of the sentences left unchanged. More diverse simplifications are available in the Newsela corpus (Xu et al., 2015), a dataset of 1,130 news articles that were each manually simplified 4669 to up to 5 levels of simplicity. The parallel articles can be automatically aligned at the sentence level to train and test simplification models (Alvaˇ Manchego et al., 2017; Stajner et al., 2018). However, the Newsela corpus can only be accessed after signing a restrictive license that prevents publicly sharing train/test splits of the dataset, which impedes reproducibility. Evaluating models on automatically-aligned sentences is problematic. Even more so if only one (potentially noisy) reference simplification for each original sentence is available. With this concern in mind, Xu et al. (2016) collected the TurkCorpus, a dataset with 2,359 original sentences from EW, each with 8 manual reference simplifications. The dataset is divided into two subsets: 2,000 sentences for validation"
2020.acl-main.424,C10-1152,0,0.326813,"nces). From all these studies, it can be argued that the scope of rewriting transformations involved in the simplification process goes beyond only replacing words with simpler synonyms. In fact, human perception of complexity is most affected by syntactic features related to sentence structure (Brunato et al., 2018). Therefore, since human editors make several changes to both the lexical content and syntactic structure of sentences when simplifying them, we should expect that models for automatic sentence simplification can also make such changes. Evaluation Data for SS Most datasets for SS (Zhu et al., 2010; Coster and Kauchak, 2011; Hwang et al., 2015) consist of automatic sentence alignments between related articles in English Wikipedia (EW) and Simple English Wikipedia (SEW). In SEW, contributors are asked to write texts using simpler language, such as by shortening sentences or by using words from Basic English (Ogden, 1930). However, Yasseri et al. (2012) found that the syntactic complexity of sentences in SEW is almost the same as in EW. In addition, Xu et al. (2015) determined that automaticallyaligned simple sentences are sometimes just as complex as their original counterparts, with onl"
2020.acl-main.424,W14-1201,0,0.016775,"utputs: BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016). BLEU is a precision-oriented metric that relies on the number of n-grams in the output that match n-grams in the references, independently of position. SARI measures improvement in the simplicity of a sentence based on the n-grams added, deleted and kept by the simplification system. It does so by comparing the output of the simplification model to multiple references and the original sentence, using both precision and recall. BLEU has shown positive correlation with human judgements of grammaticality and meaning preserˇ vation (Stajner et al., 2014; Wubben et al., 2012; Xu et al., 2016), while SARI has high correlation with judgements of simplicity gain (Xu et al., 2016). In our experiments, we used the implementations of these metrics available in the EASSE package for automatic sentence simplification evaluation (Alva-Manchego et al., 2019).5 We computed all the scores at sentence-level as in the experiment by Xu et al. (2016), where they compared sentencelevel correlations of FKGL, BLEU and SARI with human ratings. We used a smoothed sentence-level version of BLEU so that comparison is possible, 5 https://github.com/feralvam/easse Sy"
2020.acl-main.424,P12-1107,0,0.313438,"Missing"
2020.acl-main.424,Q15-1021,0,0.263892,"syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the sentence more complicated (Petersen, 2007; Alu´ısio et al., 2008; Bott and Saggion, 2011). However, models for automatic SS are evaluated on datasets whose simplifications are not representative of this variety of transformations. For instance, TurkCorpus (Xu et al., 2016), a standard dataset for assessment in SS, contains simplifications produced mostly by lexical paraphrasing, while reference simplifications in HSplit (Sulem et al., 2018a) focus on splitting sentences. The Newsela corpus (Xu et al., 2015) contains simplifications produced by professionals applying multiple rewriting transformations, but sentence alignments are automatically computed and thus imperfect, and its data can only be accessed after signing a restrictive publicsharing licence and cannot be redistributed, hampering reproducibility. These limitations in evaluation data prevent studying models’ capabilities to perform a broad range of simplification transformations. Even though most SS models are trained on simplification instances displaying several text transformations (e.g. WikiLarge (Zhang and Lapata, 2017)), we curr"
2020.acl-main.424,Q16-1029,0,0.180298,"ranco, 2012), and machine translation (Hasler et al., 2017). In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the sentence more complicated (Petersen, 2007; Alu´ısio et al., 2008; Bott and Saggion, 2011). However, models for automatic SS are evaluated on datasets whose simplifications are not representative of this variety of transformations. For instance, TurkCorpus (Xu et al., 2016), a standard dataset for assessment in SS, contains simplifications produced mostly by lexical paraphrasing, while reference simplifications in HSplit (Sulem et al., 2018a) focus on splitting sentences. The Newsela corpus (Xu et al., 2015) contains simplifications produced by professionals applying multiple rewriting transformations, but sentence alignments are automatically computed and thus imperfect, and its data can only be accessed after signing a restrictive publicsharing licence and cannot be redistributed, hampering reproducibility. These limitations in evaluation data prevent studying"
2020.acl-main.424,D17-1062,0,0.658579,"Newsela corpus (Xu et al., 2015) contains simplifications produced by professionals applying multiple rewriting transformations, but sentence alignments are automatically computed and thus imperfect, and its data can only be accessed after signing a restrictive publicsharing licence and cannot be redistributed, hampering reproducibility. These limitations in evaluation data prevent studying models’ capabilities to perform a broad range of simplification transformations. Even though most SS models are trained on simplification instances displaying several text transformations (e.g. WikiLarge (Zhang and Lapata, 2017)), we currently do not measure their performance in more abstractive scenarios, i.e. cases with substantial modifications to the original sentences. In this paper we introduce ASSET (Abstractive Sentence Simplification Evaluation and Tuning), a new dataset for tuning and evaluation of automatic SS models. ASSET consists of 23,590 human simplifications associated with the 2,359 original sentences from TurkCorpus (10 simplifications per 4668 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4668–4679 c July 5 - 10, 2020. 2020 Association for Computati"
2020.acl-main.424,D18-1355,0,0.229799,"o collect simplifications for SimPA, a dataset with sentences from the Public Administration domain. One particular characteristic of the methodology followed is that lexical and syntactic simplifications were performed independently. 3 Creating ASSET We extended TurkCorpus (Xu et al., 2016) by using the same original sentences, but crowdsourced manual simplifications that encompass a richer set of rewriting transformations. Since TurkCorpus was adopted as the standard dataset for evaluating SS models, several system outputs on this data are already publicly available (Zhang and Lapata, 2017; Zhao et al., 2018; Martin et al., 2020). Therefore, we can now assess the capabilities of these and other systems in scenarios with varying simplification expectations: lexical paraphrasing with TurkCorpus, sentence splitting with HSplit, and multiple transformations with ASSET. 3.1 Data Collection Protocol Manual simplifications were collected using Amazon Mechanical Turk (AMT). AMT allows us to publish HITs (Human Intelligence Tasks), which workers can choose to work on, submit an answer, and collect a reward if the work is approved. This was also the platform used for TurkCorpus. Worker Requirements. Partic"
2020.acl-main.424,P06-4018,0,\N,Missing
2020.acl-main.424,S16-1085,1,\N,Missing
2020.acl-main.424,W18-6401,0,\N,Missing
2020.acl-main.558,C04-1046,0,0.439201,"Missing"
2020.acl-main.558,N19-1423,0,0.0263753,"ollowing the ratio of approximately 8 to 1 to 1. Table 1 presents statistics of the QE datasets. size (K) Dataset WMT18∗ WMT19 langs dom. syst. train dev test en-de IT IT SMT NMT 21.8 11.5 2.7 1.4 2.7 1.4 en-cs IT SMT 33.0 4.1 4.1 en-lv SCI SCI SMT NMT 9.8 11.1 1.2 1.3 1.2 1.3 de-en SCI SMT 21.6 2.7 2.7 en-de IT NMT 13.4 1.0 1.0 en-ru Tech NMT 15.0 1.0 1.0 Table 1: Statistics of QE datasets. WMT18∗ contains random splits of the publicly available training data since the official test sets are not publicly available. 2.2 Models BERT We experiment with a strong neural QE approach based on BERT (Devlin et al., 2019). In particular, we focus on the bert-base-cased version of the multilingual BERT.2 We join the source and translated sentences together using the special SEP token and predict the QE score from the vector representation of the final CLS token via a Multilayer Perceptron (MLP) layer. Our models perform competitively to the state-of-theart QE models (Kepler et al., 2019a; Kim et al., 2019). However, we do not treat this as a multitask learning problem where word-level labels are also needed because this is severely limited by the availability of data. We also do not do further optimizations (e."
2020.acl-main.558,P19-1554,0,0.021653,"Missing"
2020.acl-main.558,W19-5401,0,0.0313561,"Missing"
2020.acl-main.558,P15-1174,0,0.046617,"Missing"
2020.acl-main.558,N18-2017,0,0.0218904,"irrespective of the fluency. To evaluate if our models learn both aspects of translation quality, we run partial input experiments, where we train systems with only the source or target sentences and analyze the discrepancies w.r.t to the full-input experiments. Lack of lexical diversity Most QE datasets come from a single domain (e.g., IT, life sciences), and certain lexical items can be associated with high-quality translations. Lexical artifacts are also observed in monolingual datasets across different tasks (Goyal et al., 2017; Jia and Liang, 2017; Kaushik and Lipton, 2018). For example, Gururangan et al. (2018) find that annotators are responsible for introducing lexical artifacts into some natural language inference datasets because they adopt heuristics to generate plausible hypothesis during annotation quickly. Here, we use Normalized Pointwise Mutual Information (NPMI) (Bouma, 2009) to find possible lexical artifacts associated with different levels of HTER. 2.1 Experimental Setup We experiment with recent QE datasets from WMT18 and WMT19. For every dataset, a Statistical Machine Translation (SMT) system or Neural Machine Translation (NMT) system was used to translate the source sentences. The t"
2020.acl-main.558,D17-1215,0,0.0221593,"a mixture of instances that model both high and low adequacy irrespective of the fluency. To evaluate if our models learn both aspects of translation quality, we run partial input experiments, where we train systems with only the source or target sentences and analyze the discrepancies w.r.t to the full-input experiments. Lack of lexical diversity Most QE datasets come from a single domain (e.g., IT, life sciences), and certain lexical items can be associated with high-quality translations. Lexical artifacts are also observed in monolingual datasets across different tasks (Goyal et al., 2017; Jia and Liang, 2017; Kaushik and Lipton, 2018). For example, Gururangan et al. (2018) find that annotators are responsible for introducing lexical artifacts into some natural language inference datasets because they adopt heuristics to generate plausible hypothesis during annotation quickly. Here, we use Normalized Pointwise Mutual Information (NPMI) (Bouma, 2009) to find possible lexical artifacts associated with different levels of HTER. 2.1 Experimental Setup We experiment with recent QE datasets from WMT18 and WMT19. For every dataset, a Statistical Machine Translation (SMT) system or Neural Machine Translat"
2020.acl-main.558,D18-1546,0,0.0167359,"s that model both high and low adequacy irrespective of the fluency. To evaluate if our models learn both aspects of translation quality, we run partial input experiments, where we train systems with only the source or target sentences and analyze the discrepancies w.r.t to the full-input experiments. Lack of lexical diversity Most QE datasets come from a single domain (e.g., IT, life sciences), and certain lexical items can be associated with high-quality translations. Lexical artifacts are also observed in monolingual datasets across different tasks (Goyal et al., 2017; Jia and Liang, 2017; Kaushik and Lipton, 2018). For example, Gururangan et al. (2018) find that annotators are responsible for introducing lexical artifacts into some natural language inference datasets because they adopt heuristics to generate plausible hypothesis during annotation quickly. Here, we use Normalized Pointwise Mutual Information (NPMI) (Bouma, 2009) to find possible lexical artifacts associated with different levels of HTER. 2.1 Experimental Setup We experiment with recent QE datasets from WMT18 and WMT19. For every dataset, a Statistical Machine Translation (SMT) system or Neural Machine Translation (NMT) system was used t"
2020.acl-main.558,W19-5406,0,0.249649,"Missing"
2020.acl-main.558,P19-3020,0,0.102619,"Missing"
2020.acl-main.558,W19-5407,0,0.0650995,"WMT18∗ contains random splits of the publicly available training data since the official test sets are not publicly available. 2.2 Models BERT We experiment with a strong neural QE approach based on BERT (Devlin et al., 2019). In particular, we focus on the bert-base-cased version of the multilingual BERT.2 We join the source and translated sentences together using the special SEP token and predict the QE score from the vector representation of the final CLS token via a Multilayer Perceptron (MLP) layer. Our models perform competitively to the state-of-theart QE models (Kepler et al., 2019a; Kim et al., 2019). However, we do not treat this as a multitask learning problem where word-level labels are also needed because this is severely limited by the availability of data. We also do not do further optimizations (e.g. model ensembling) given that our focus is on what can be learned with the current data, and not maximizing performance. Our simpler models allow us to carefully analyze and determine the effects of source and translated sentences on the performance of the models. We expect the trends to be the same as other neural QE models. 6263 1 2 http://www.umiacs.umd.edu/ snover/terp/ https://gith"
2020.acl-main.558,S18-2023,0,0.0563627,"Missing"
2020.acl-main.558,2006.amta-papers.25,0,0.247297,"Missing"
2020.acl-main.558,P15-4020,1,0.928444,"Missing"
2020.acl-main.558,P13-4014,1,0.885085,"Missing"
2020.acl-main.558,2009.eamt-1.5,1,0.817538,"Missing"
2020.cl-1.4,D18-1399,0,0.0202962,"functions, Lrec and Ldenoi , are used for reconstructing sentences and denoising, respectively. The full architecture can be seen in Figure 4. The proposed model (UNTS) was trained using an English Wikipedia dump that was partitioned into Complex and Simple sets using a threshold based on Flesch Reading Ease scores. They also used 10,000 sentence pairs from EW-SEW (Hwang et al. 2015) and WebSplit (Narayan et al. 2017) data sets to train a model (UNTS+10K) with minimal supervision. Their models were compared against unsupervised systems from the MT literature (Artetxe, Labaka, and Agirre 2018; Artetxe et al. 2018), as well as SS models like NTS (Nisioi et al. 2017) and SBSMT (Xu et al. 2016), and using TurkCorpus as test data. When evaluated using automatic metrics, SBSMT scored the highest on SARI, but both UNTS and UNTS+10K were not far from the supervised models. This same behavior was observed with human evaluations. Even though the unsupervised model was trained using instances of sentence splitting from WebSplit, the authors do not report testing it on data for that specific text transformation. Figure 4 Model architecture for UNTS. Extracted from Surya et al. (2019). 169 Computational Linguistic"
2020.cl-1.4,bott-etal-2012-text,0,0.0652514,"Missing"
2020.cl-1.4,D15-1075,0,0.035628,"ored modeling, different from standard MT-based sequence-to-sequence approaches. 4.3.1 Split-and-Rephrase. Narayan et al. (2017) introduce a new task called split-andrephrase, focused on splitting a sentence into several others, and making the necessary changes to ensure grammaticality. No deletions should be performed so as to preserve meaning. The authors use the W EB S PLIT data set (described in Section 2.3) to train and test five models for the split-and-rephrase task: (1) Hybrid (Narayan and Gardent 2014); (2) Seq2Seq, which is an encoder-decoder with local-p attention (Luong, Pham, and Manning 2015); (3) MultiSeq2Seq, which is a multi-source sequence-to-sequence model (Zoph and Knight 2016) that takes as input the original sentence and its MR triples; and (4) one that models the problem in two steps: first learn to split, and then learn to rephrase. In this last model, the splitting step uses the original sentence and its MR to split the latter into several MR sets. However, two variations are explored for the rephrasing step: (1) Split-MultiSeq2Seq learns to rephrase from the split MRs and the original sentence in a multi-source fashion, while (2) Split-Seq2Seq only uses the split MRs a"
2020.cl-1.4,W15-1604,0,0.060762,"Missing"
2020.cl-1.4,W11-1601,0,0.403512,"pedia for research in SS comes from publicly available automatically collected alignments between sentences of equivalent articles in EW and SEW. Several techniques have been explored to produce such alignments with reasonable quality. A first approach consists of aligning texts according to their term frequency–inverse document frequency (tf-idf) cosine similarity. For the PWKP corpus, Zhu, Bernhard, and Gurevych (2010) measured this directly at sentence-level between all sentences of each article pair, and sentences whose similarity was above a certain threshold were aligned. For the C&K-1 (Coster and Kauchak 2011b) and C&K-2 (Kauchak 2013) corpora, the authors first aligned paragraphs with tf-idf cosine similarity, and then found the best overall sentence alignment with the dynamic programming algorithm proposed by Barzilay and Elhadad (2003). This algorithm takes context into consideration: The similarity between two sentences is affected by their proximity to pairs of sentences with 1 https://simple.wikipedia.org. 2 https://wikipedia.org. 139 Computational Linguistics Volume 46, Number 1 Table 1 Summary of parallel corpora extracted from EW and SEW. An original sentence can be aligned to one (1-to-1"
2020.cl-1.4,P11-2117,0,0.587682,"pedia for research in SS comes from publicly available automatically collected alignments between sentences of equivalent articles in EW and SEW. Several techniques have been explored to produce such alignments with reasonable quality. A first approach consists of aligning texts according to their term frequency–inverse document frequency (tf-idf) cosine similarity. For the PWKP corpus, Zhu, Bernhard, and Gurevych (2010) measured this directly at sentence-level between all sentences of each article pair, and sentences whose similarity was above a certain threshold were aligned. For the C&K-1 (Coster and Kauchak 2011b) and C&K-2 (Kauchak 2013) corpora, the authors first aligned paragraphs with tf-idf cosine similarity, and then found the best overall sentence alignment with the dynamic programming algorithm proposed by Barzilay and Elhadad (2003). This algorithm takes context into consideration: The similarity between two sentences is affected by their proximity to pairs of sentences with 1 https://simple.wikipedia.org. 2 https://wikipedia.org. 139 Computational Linguistics Volume 46, Number 1 Table 1 Summary of parallel corpora extracted from EW and SEW. An original sentence can be aligned to one (1-to-1"
2020.cl-1.4,P07-2009,0,0.0986503,"Missing"
2020.cl-1.4,W14-3348,0,0.0678559,"Missing"
2020.cl-1.4,P03-2041,0,0.0387327,"ata-Driven Sentence Simplification hand-sides (source and target) that are related. For example, we show a SCFG for a sentence in English and its translation to Japanese (Chiang 2006): S → hNP1 VP2 |NP1 VP2 i VP → hV1 NP2 |NP2 V1 i NP → hi|watashi wai NP → hthe box|hako woi V → hopen|akemasui The numbers in the non-terminals serve as links between nodes in the source and target. These links are 1-to-1 and every non-terminal is always linked to another. SCFGs have the limitation of only being able to relabel and reorder sibling nodes. In contrast, Synchronous Tree Substitution Grammars (STSGs, Eisner 2003) are able to perform more long-distance swapping. In a STSG, productions are pairs of elementary trees, which are tree fragments whose leaves can be non-terminal or terminal symbols: S S VP NP1 V misses VP NP2 NP2 NP NP John Jean NP NP Mary Marie NP V manque P NP1 a` SCFGs impose an isomorphism constraint between the aligned trees. This requirement is relaxed by the STSGs. However, to account for all the different movement patterns that could exist in a language would require powerful and, perhaps, slow grammars (Smith and Eisner 2006). Quasi-synchronous Grammars (QG, Smith and Eisner 2006) re"
2020.cl-1.4,W12-2038,0,0.0209691,"Missing"
2020.cl-1.4,W14-1215,0,0.178549,"Missing"
2020.cl-1.4,W13-2901,0,0.0547674,"Missing"
2020.cl-1.4,W08-1105,0,0.10375,"Missing"
2020.cl-1.4,N13-1092,0,0.163213,"Missing"
2020.cl-1.4,P17-1017,0,0.0573133,"Missing"
2020.cl-1.4,R13-2011,0,0.0675807,"Missing"
2020.cl-1.4,P15-2011,0,0.0605659,"Missing"
2020.cl-1.4,2015.mtsummit-papers.2,0,0.0930123,"Missing"
2020.cl-1.4,P16-1154,0,0.0631277,"Missing"
2020.cl-1.4,C18-1039,0,0.0248237,"Missing"
2020.cl-1.4,P17-1104,0,0.0478523,"Missing"
2020.cl-1.4,E17-1090,0,0.0217875,"Data-Driven Sentence Simplification classification approaches to determine if a sentence is simple or not may not be the appropriate way to model the task. Consequently, Vajjala and Meurers (2015) proposed using pair wise ranking to assess the readability of simplified sentences. They used the same features of the document-level model of Vajjala and Meurers (2014a), but now they attempt to learn to predict which of two given sentences is simpler than the other. Ambati, Reddy, and Steedman (2016) tested the usefulness of syntactic features extracted from an incremental parser for the task, and Howcroft and Demberg (2017) explored using more psycholinguistic features, such as idea density, surprisal, integration cost, and embedding depth. Although not detailed in this section, some research has used METEOR (Denkowski and Lavie 2014) from the MT literature, and ROUGE (Lin 2004), borrowed from summarization research. 3.3 Discussion In this section we have described how the outputs of SS models are evaluated using both human judgments and automatic metrics. We have attempted to not only explain these methods, but also to point out their advantages and disadvantages. In the case of human evaluation, one important"
2020.cl-1.4,N15-1022,0,0.673103,"wo sentences is affected by their proximity to pairs of sentences with 1 https://simple.wikipedia.org. 2 https://wikipedia.org. 139 Computational Linguistics Volume 46, Number 1 Table 1 Summary of parallel corpora extracted from EW and SEW. An original sentence can be aligned to one (1-to-1) or more (1-to-N) unique simplified sentences. A (*) indicates that some aligned simplified sentences may not be unique. Corpora PWKP (Zhu, Bernhard, and Gurevych 2010) C&K-1 (Coster and Kauchak 2011b) RevisionWL (Woodsend and Lapata 2011a) AlignedWL (Woodsend and Lapata 2011a) C&K-2 (Kauchak 2013) EW-SEW (Hwang et al. 2015) sscorpus (Kajiwara and Komachi 2016) WikiLarge (Zhang and Lapata 2017) Instances Alignment Types 108K 137K 15K 142K 167K 392K 493K 286K 1-to-1, 1-to-N 1-to-1, 1-to-N 1-to-1*, 1-to-N*, N-to-1* 1-to-1, 1-to-N 1-to-1, 1-to-N 1-to-1 1-to-1 1-to-1*, 1-to-N*, N-to-1* high similarity. Finally, Woodsend and Lapata (2011a) also adopt the two-step process of Coster and Kauchak (2011b), using tf-idf when compiling the AlignedWL corpus. Another approach is to take advantage of the revision histories in Wikipedia articles. When editors change the content of an article, they need to comment on what the cha"
2020.cl-1.4,A00-1043,0,0.386294,"cation, some deletion of content can also be performed. However, we could additionally replace words by more explanatory phrases, make co-references explicit, add connectors to improve fluency, and so forth. As a consequence, a simplified text could end up being longer than its original version while still improving the readability of the text. Therefore, although summarization and simplification are related, they have different objectives. Another related task is sentence compression, which consists of reducing the length of a sentence without losing its main idea and keeping it grammatical (Jing 2000). Most approaches focus on deleting unnecessary words. As such, this could be considered as a subtask of the simplification process, which also encompasses more complex transformations. Abstractive sentence compression (Cohn and Lapata 2013), on the other hand, does include transformations like substitution, reordering, and insertion. However, the goal is still to reduce content without necessarily improving readability. Split-and-rephrase (Narayan et al. 2017) focuses on splitting a sentence into several shorter ones, and making the necessary rephrasings to preserve meaning and grammaticality"
2020.cl-1.4,Q17-1024,0,0.0452132,"Missing"
2020.cl-1.4,C16-1109,0,0.0602558,"sentence similarity have also been explored. For their EW-SEW corpus, Hwang et al. (2015) implemented an alignment method using word-level semantic similarity based on Wiktionary.3 They first created a graph using synonym information and word-definition co-occurrence in Wiktionary. Then, similarity is measured based on the number of shared neighbors between words. This word-level similarity metric is then combined with a similarity score between dependency structures. This final similarity rate is used by a greedy algorithm that forces 1-to-1 matches between original and simplified sentences. Kajiwara and Komachi (2016) propose several similarity measures based on word embeddings alignments. Given two sentences, their best metric (1) finds, for each word in one sentence, the word that is most similar to it in the other sentence, and (2) averages the similarities for all words in the sentence. For symmetry, this measure is calculated twice (simplified → original, original → simplified) and their average is the final similarity measure between the two sentences. This metric was used to align original and simplified sentences from articles in a 2016 Wikipedia dump and produce the sscorpus. It contains 1-to-1 al"
2020.cl-1.4,P13-1151,0,0.0525306,"ublicly available automatically collected alignments between sentences of equivalent articles in EW and SEW. Several techniques have been explored to produce such alignments with reasonable quality. A first approach consists of aligning texts according to their term frequency–inverse document frequency (tf-idf) cosine similarity. For the PWKP corpus, Zhu, Bernhard, and Gurevych (2010) measured this directly at sentence-level between all sentences of each article pair, and sentences whose similarity was above a certain threshold were aligned. For the C&K-1 (Coster and Kauchak 2011b) and C&K-2 (Kauchak 2013) corpora, the authors first aligned paragraphs with tf-idf cosine similarity, and then found the best overall sentence alignment with the dynamic programming algorithm proposed by Barzilay and Elhadad (2003). This algorithm takes context into consideration: The similarity between two sentences is affected by their proximity to pairs of sentences with 1 https://simple.wikipedia.org. 2 https://wikipedia.org. 139 Computational Linguistics Volume 46, Number 1 Table 1 Summary of parallel corpora extracted from EW and SEW. An original sentence can be aligned to one (1-to-1) or more (1-to-N) unique s"
2020.cl-1.4,W13-2902,0,0.0606494,"Missing"
2020.cl-1.4,P17-4012,0,0.0225399,"d×d , and Wh ∈ Rd×d ; |V |is the output vocabulary size and d is the hidden unit size. hTt is the hidden state of the decoder LSTM that summarizes y1:t (what has been generated so far): hTt = LSTM(yt , hTt−1 ) (19) The dynamic context vector ct is a weighted sum of the hidden states of the source sentence, whose weights αti are determined by an attention mechanism: ct = |X| X i=1 α ti hSi exp(hTt · hSi ) α ti = P S T i exp(ht · hi ) (20) NTS: Nisioi et al. (2017) introduced the first Neural Text Simplification approach using the encoder-decoder with attention architecture provided by OpenNMT (Klein et al. 2017). They experimented with using the default system, and also with combining pre-trained word2vec word embeddings (Mikolov et al. 2013) with locally trained ones. They also generated two candidate hypotheses for each beam size, and used BLEU and SARI to determine which hypothesis to choose from the n-best list of candidates. EW-SEW was used for training, and TurkCorpus for validation and testing. When compared against PBSMT-R and SBSMT (PPDB+SARI), NTS with its default features achieved the highest grammaticality and meaning preservation scores in human evaluation. SBSMT (PPDB+SARI) was still th"
2020.cl-1.4,klerke-sogaard-2012-dsim,0,0.0378873,"Missing"
2020.cl-1.4,P07-2045,0,0.012619,"g a best-first search algorithm, like A*, but exploring the entire search space of possible translations is expensive. Therefore, 154 Alva-Manchego, Scarton, and Specia Data-Driven Sentence Simplification Table 2 Performance of PBSMT-based sentence simplification models as reported by their authors. Model Moses (Brazilian Portuguese) Moses (English) Moses-Del PBSMT-R Train Corpus Test Corpus BLEU ↑ FKGL ↓ PorSimples C&K C&K PWKP PorSimples C&K C&K PWKP 60.75 59.87 60.46 43.00 13.38 decoders use beam-search to only retain, at every step, the most promising states to continue the search. Moses (Koehn et al. 2007) is a popular PBSMT system, freely available.9 It provides tools for easy training, tuning, and testing of translation models based on this SMT approach. Specia (2010) was the first to use this toolkit, with no adaptations, for the simplification task. Experiments were carried out on a parallel corpus of original and manually simplified newspaper articles in Brazilian Portuguese (Caseli et al. 2009). The trained model mostly executes lexical simplifications and simple rewritings. However, as expected, it is overcautious and cannot perform long distance operations like subjectverb-object reorde"
2020.cl-1.4,W04-1013,0,0.0659056,"s. They used the same features of the document-level model of Vajjala and Meurers (2014a), but now they attempt to learn to predict which of two given sentences is simpler than the other. Ambati, Reddy, and Steedman (2016) tested the usefulness of syntactic features extracted from an incremental parser for the task, and Howcroft and Demberg (2017) explored using more psycholinguistic features, such as idea density, surprisal, integration cost, and embedding depth. Although not detailed in this section, some research has used METEOR (Denkowski and Lavie 2014) from the MT literature, and ROUGE (Lin 2004), borrowed from summarization research. 3.3 Discussion In this section we have described how the outputs of SS models are evaluated using both human judgments and automatic metrics. We have attempted to not only explain these methods, but also to point out their advantages and disadvantages. In the case of human evaluation, one important but often overlooked aspect is that it should be carried out by individuals from the same target audience of the data on which the SS model was trained. This is especially relevant when collecting simplicity judgments because of its subjective nature: What a n"
2020.cl-1.4,D15-1166,0,0.0539924,"Missing"
2020.cl-1.4,E17-1083,0,0.0602083,"Missing"
2020.cl-1.4,C14-1188,0,0.0547247,"Missing"
2020.cl-1.4,2013.mtsummit-posters.8,0,0.118773,"Missing"
2020.cl-1.4,W14-5603,0,0.0692028,"Missing"
2020.cl-1.4,E17-1038,0,0.0297781,"nput sentences indicating (1) the grade level of the simplification instance, and/or (2) one of four possible text transformations: identical, elaboration, splitting, or joining. At test time, the text transformation token is either predicted (using a simple features-based naive Bayes classifier) or an oracle label is used. They experimented using the standard neural architecture available in OpenNMT and data from the Newsela corpus. Results showed improvements in BLEU, SARI, and Flesch scores when using this extra information. N SE L STM: Vu et al. (2018) used Neural Semantic Encoders (NSEs, Munkhdalai and Yu 2017) instead of LSTMs for the encoder. At any encoding time step, a NSE has access to all the tokens in the input sequence, and is thus able to capture more context information while encoding the current token, instead of only relying on the previous hidden state. Their approach is tested on PWKP, TurkCorpus, and Newsela. Two models 165 Computational Linguistics Volume 46, Number 1 are presented, one tuned using BLEU (N SE L STM -B) and one using SARI (N SE L STM -S). When compared against other models, N SE L STM -B achieved the best BLEU scores in the Newsela and TurkCorpus data sets, while N SE"
2020.cl-1.4,W10-0406,0,0.0959421,"Missing"
2020.cl-1.4,P14-1041,0,0.222183,"approach does not explicitly model sentence splitting. Table 4 summarizes the performance of the models trained with the SS approaches described. Unfortunately, results are not directly comparable. Overall, grammarinduction-based approaches, because of their pipeline architecture, offer more flexibility on how the rules are learned and how they are applied, as compared with end-toend approaches. Even though Woodsend and Lapata (2011a) were the only ones who attempted model splitting, the other approaches could be modified in a similar way, since the formalisms allow it. 4.3 Semantics-Assisted Narayan and Gardent (2014) argue that the simplification transformation of splitting is semantics-driven. In many cases, splitting occurs when an entity takes part in two (or more) distinct events described in a single sentence. For example, in Sentence (1), bricks is involved in two events: “being resistant to cold” and “enabling the construction of permanent buildings.” (1) Original: Being more resistant to cold, bricks enabled the construction of permanent buildings. 161 Computational Linguistics (2) Volume 46, Number 1 Simplified: Bricks were more resistant to cold. Bricks enabled the construction of permanent buil"
2020.cl-1.4,W16-6620,0,0.0911308,"e tree. The SS model is trained and tested using the PWKP corpus. Sentences for which Boxer failed to extract a semantic representation were excluded during training, and directly passed to the PBSMT system in testing. For evaluation, the model is compared against QG+ILP, PBSMT-R, and TSM. Hybrid performs splits closer in proportion to those of the references. It also achieves the highest BLEU score and smaller edit distance to references. With human evaluation, Hybrid obtains the highest score in simplicity, and is a close second to PBSMT-R for grammaticality and meaning preservation. UNSUP: Narayan and Gardent (2016) propose a method that does not require aligned original-simplified sentences to train a TS model. Their approach first uses a context-aware lexical simplifier (Biran, Brody, and Elhadad 2011) that learns simplification rules from articles of EW and SEW. Given an original sentence, these rules are applied and the best combination of simplifications is found using dynamic programming. Then, they use Boxer to extract the semantic representation of the sentence and identify the events/predicates. After that, they estimate the maximum likelihood of the sequences of semantic role sets that would re"
2020.cl-1.4,C16-2036,0,0.0261361,"old 2016), children (De Belder and Moens 2010), and people suffering from aphasia (Devlin and Tait 1998; Carroll et al. 1998), dyslexia (Rello et al. 2013b), or autism (Evans, Orasan, and Dornescu 2014). Furthermore, simplifying sentences automatically could improve performance on other Natural Language Processing tasks, which has become evident in parsing (Chandrasekar, Doran, and Srinivas 1996), summarization (Siddharthan, Nenkova, and McKeown 2004; Vanderwende et al. 2007; Silveira and Branco 2012), information extraction (Klebanov, Knight, and Marcu 2004; Evans 2011), relation extraction (Niklaus et al. 2016), semantic role labeling Vickrey and Koller 2008, and MT (Mirkin, Venkatapathy, and Dymetman 2013; Mishra et al. ˇ 2014; Stajner and Popovi´c 2016; Hasler et al. 2017). We refer the interested reader to Siddharthan (2014) for a more in-depth review of studies on the benefits of simplification for different target audiences and Natural Language Processing applications. 1.2 Text Transformations for Simplification A few corpus studies have been carried out to determine how humans simplify sentences. These studies shed some light on the simplification transformations that an automatic SS model sho"
2020.cl-1.4,P17-2014,0,0.543791,"here g(· ) is a neural network with one hidden layer and parametrized as follows: g(hTt , ct ) = Wo tanh(Uh hTt + Wh ct ) (18) where Wo ∈ R|V|×d , Uh ∈ Rd×d , and Wh ∈ Rd×d ; |V |is the output vocabulary size and d is the hidden unit size. hTt is the hidden state of the decoder LSTM that summarizes y1:t (what has been generated so far): hTt = LSTM(yt , hTt−1 ) (19) The dynamic context vector ct is a weighted sum of the hidden states of the source sentence, whose weights αti are determined by an attention mechanism: ct = |X| X i=1 α ti hSi exp(hTt · hSi ) α ti = P S T i exp(ht · hi ) (20) NTS: Nisioi et al. (2017) introduced the first Neural Text Simplification approach using the encoder-decoder with attention architecture provided by OpenNMT (Klein et al. 2017). They experimented with using the default system, and also with combining pre-trained word2vec word embeddings (Mikolov et al. 2013) with locally trained ones. They also generated two candidate hypotheses for each beam size, and used BLEU and SARI to determine which hypothesis to choose from the n-best list of candidates. EW-SEW was used for training, and TurkCorpus for validation and testing. When compared against PBSMT-R and SBSMT (PPDB+SARI)"
2020.cl-1.4,J04-4002,0,0.0300174,"Missing"
2020.cl-1.4,W13-4813,1,0.919517,"s chosen, the node it depends on is also chosen), coherence (if one partition of a split sentence is chosen, the other partition is also chosen), and always one (and only one) simplification per sentence. The authors trained two models: one extracting rules from the AlignedLP corpus (AlignILP) and the other using the RevisionWL corpus (RevILP). For evaluation, they used the test split from the PWKP instances. RevILP was their best model, achieving the closest scores to the references using both Flesh-Kincaid and human judgments on simplicity, grammaticality, and meaning preservation. T3+Rank: Paetzold and Specia (2013) extract candidate tree rewriting rules using T3 (Cohn and Lapata 2009), an abstractive sentence compression model that uses STSGs for deletion, reordering, and substitution. Using word-aligned parallel sentences, the model maps the word alignment into a constituent-level alignment between the source and target trees by adapting the alignment template method of Och and Ney (2004). These constituent alignments are then generalized (i.e., aligned nodes are replaced with links) to extract rules. This generalization is performed by a recursive algorithm that attempts to find the minimal most gener"
2020.cl-1.4,E17-2006,1,0.86031,"lume 46, Number 1 4.4.5 Simplification as Sequence Labeling. Alva-Manchego et al. (2017) model SS as a Sequence Labeling problem, identifying simplification transformations at word or phrase level. They use the token-level annotation algorithms of MASSAlign (Paetzold, Alva-Manchego, and Specia 2017) to automatically generate annotated data from which an LSTM learns to predict simplification transformations; more specifically, deletions and replacements. During decoding, words labeled to be deleted are just not included in the output. To produce replacements, they use the lexical simplifier of Paetzold and Specia (2017a). The proposed approach is compared against MT-based models: Moses (Koehn et al. 2007), Nematus (Sennrich et al. 2017), and NTS+word2vec (with default settings) using data from the Newsela corpus. Alva-Manchego et al. (2017) achieve the highest SARI score in the test set, and best simplicity score with human judgments. This approach is inspired in the abstractive sentence compression model of Bingel and Søgaard (2016), who propose a tree labeling approach to remove or paraphrase syntactic units in the dependency tree of a given sentence, using a Conditional Random Fields predictor. Most sequ"
2020.cl-1.4,I17-3001,1,0.924433,"Missing"
2020.cl-1.4,C16-1069,1,0.939527,"k of different approaches on common data sets so as to compare them and highlight their strengths and limitations. We expect that this survey will serve as a starting point for researchers interested in the task and help spark new ideas for future developments. 1. Introduction Text Simplification (TS) is the task of modifying the content and structure of a text in order to make it easier to read and understand, while retaining its main idea and approximating its original meaning. A simplified version of a text could benefit users with several reading difficulties, such as non-native speakers (Paetzold 2016), people with aphasia (Carroll et al. 1998), dyslexia (Rello et al. 2013b), or autism (Evans, Orasan, and Dornescu 2014). Simplifying a text automatically could also help improve performance Submission received: 8 June 2018; revise d version received: 9 August 2019; accepted for publication: 15 September 2019. https://doi.org/10.1162/COLI a 00370 © 2020 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 46, Number 1 on other language processing tasks, s"
2020.cl-1.4,P02-1040,0,0.109185,"cheaper results. Some of these metrics are based on comparing the automatic simplifications to manually produced references; others compute the readability of the text based on psycholinguistic metrics; whereas others are trained on specially annotated data so as to learn to predict the quality or usefulness of the simplification being evaluated. 3.2.1 String Similarity Metrics. These metrics are mostly borrowed from the MT literature, since SS can be seen as translating a text from complex to simple. The most commonly used are BLEU and TER. BLEU (BiLingual Evaluation Understudy), proposed by Papineni et al. (2002), is a precision-oriented metric, which means that it depends on the number of n-grams in the candidate translation that match with n-grams of the reference, independent of position. BLEU values range from 0 to 1 (or to 100); the higher the better. BLEU calculates a modified n-gram precision: (i) count the maximum number of times that an n-gram occurs in any of the references, (ii) clip the total count of each candidate n-gram by its maximum reference count (i.e, Countclip = min(Count, MaxRefCount)), and (iii) add these clipped counts up, and divide by the total (unclipped) number of candidate"
2020.cl-1.4,P16-2024,0,0.11142,"xtract the paraphrases, they used bilingual corpora with the following intuition: “two strings that translate to the same foreign string can be assumed to have the same meaning.” The authors utilized the synchronous contextfree grammar formalism to collect paraphrases. Using MT technology, they extracted grammar rules from foreign-to-English corpora. Then, the paraphrase is created from rule pairs where the left-hand side and foreign string match. Each paraphrase in PPDB has a similarity score, which was calculated using monolingual distributional similarity. 2.3.8 Simple Paraphrase Database. Pavlick and Callison-Burch (2016) created the Simple PPDB, a subset of the PPDB tailored for SS. They used machine learning models to select paraphrases that generate a simplification and preserve its meaning. First, they selected 1,000 words from PPDB which also appear in the Newsela corpus. They then selected up to 10 paraphrases for each word. After that, they crowd-sourced the manual evaluation of these paraphrases in two stages: (1) rate their meaning preservation in a scale of 1 to 5, and (2) label the ones with rates higher than 2 as simpler or not. Next, these data were used to train a multi-class logistic regression"
2020.cl-1.4,W18-6319,0,0.0154542,"ntly use metrics that rely on multiple references, like SARI. We do not use the Newsela corpus in our benchmark because researchers are prohibited from publicly releasing models’ outputs on these data. 5.1.2 Overall Performance Comparison. We first compare the models’ outputs using automatic metrics so as to obtain an overall measure of simplification quality. We calculate BLEU, SARI, SAMSA, and FKGL. We compute the scores for these metrics using EASSE (Alva-Manchego et al. 2019),10 a Python package for single access to (re)implementations of these metrics. Specifically, it uses S ACRE B LEU (Post 2018)11 to calculate BLEU, a re-implementation of SARI’s corpus-level version in Python (it was originally available in Java), a slightly modified version of the original SAMSA implementation12 for improved execution speed, and a re-implementation of FKGL based on publicly available scripts13 that fixes some edge case inconsistencies. 5.1.3 Transformation-Based Performance Comparison. We are also interested in an in-depth study of the simplification capabilities of each model. In particular, we want to determine which simplification transformations each model performs more effectively. In order to"
2020.cl-1.4,W13-2226,0,0.0312662,"TurkCorpus TurkCorpus PWKP Own TurkCorpus TurkCorpus TurkCorpus 38.00 99.05 74.48 72.36 FKGL ↓ SARI ↑ 7.9 12.88 10.75 10.90 26.05 34.18 37.91 The proposed simplification model also relies on paraphrasing rules available in the PPDB, which are expressed as a Synchronous Context-Free Grammar (SCFG). The authors also added nine new features to each rule in the PPDB (each rule already contains 33). These new features are simplification-specific, for example: length in characters, length in words, number of syllables, among others. These modifications were implemented in the SBSMT toolkit Joshua (Post et al. 2013) and performed experiments using TurkCorpus (described in Section 2.3) on three versions of the SBSMT system, changing the tuning metric (BLEU, FKBLEU, and SARI). Evaluations using human judgments show that all three models achieved better grammatically, meaning preservation, and simplicity gain than PBSMT-R (Wubben, van den Bosch, and Krahmer 2012). Table 3 summarizes the performance of the syntax-based models trained using the SS approaches described. These values are not directly comparable, because each approach used a different corpus for testing. In the case of the models based on Joshua"
2020.cl-1.4,J18-3002,0,0.0226795,"ns. Then, the accuracy of their responses is used to qualify the helpfulness of the simplified texts in the particular comprehension task. This type of human evaluation could be more goal-oriented, but they are costly to create and execute. Automatic metrics are useful for quickly assessing models and comparing different architectures. They could even be considered more objective than humans since personal biases do not play a role. However, the metrics used in SS research are flawed. BLEU has been found to only be reliable for assessment in MT but not other Natural Language Generation tasks (Reiter 2018), and it is not adequate for most rewriting transformations in SS (Sulem, Abend, and Rappoport 2018a). SARI is only useful as a proxy for simplicity gain assessment, limited to lexical simplifications and shortdistance reordering despite more text transformations being possible. Commonly-used Flesch metrics were developed to assess complete documents and not sentences, which is the focus of most simplification research nowadays. Therefore, when evaluating models using these automatic scores, it is essential to keep all their particular limitations 153 Computational Linguistics Volume 46, Numbe"
2020.cl-1.4,L18-1685,1,0.926262,"Missing"
2020.cl-1.4,L18-1553,1,0.929258,"Missing"
2020.cl-1.4,P18-2113,1,0.842686,"this standard neural architecture for SS, but used the implementation provided by Nematus (Sennrich et al. 2017). They experimented with different types of original-simplified sentence alignments extracted from the Newsela corpus. When experimenting with all possible sentence alignments, the model tended to be too aggresive, mostly performing deletions. When using only 1-to-1 alignments, the model became more conservative, and the simplifications performed were restricted to deletions and one-word replacements. targeTS: Inspired by the work of Johnson et al. (2017) on multilingual neural MT, Scarton and Specia (2018) enriched the encoder’s input with information about the target audience and the (predicted) simplification transformations to be performed. Concretely, an artificial token was added to the beginning of the input sentences indicating (1) the grade level of the simplification instance, and/or (2) one of four possible text transformations: identical, elaboration, splitting, or joining. At test time, the text transformation token is either predicted (using a simple features-based naive Bayes classifier) or an oracle label is used. They experimented using the standard neural architecture available"
2020.cl-1.4,P17-1099,0,0.0991683,"Missing"
2020.cl-1.4,E17-3017,0,0.0481566,"Missing"
2020.cl-1.4,W03-2314,0,0.373702,"n Sentence Simplification simplifications to comply with the readability requirements of the grade level of the current version. Xu, Callison-Burch, and Napoles (2015) also presented an analysis of the most frequent syntax patterns in original and simplified texts for PWKP and Newsela. These patterns correspond to parent node (head node) → children node(s) structures. Overall, the Wikipedia corpus has a higher tendency to retain complex patterns in its simple counterpart than Newsela. Finally, the authors present a study on discourse connectives that are important for readability according to Siddharthan (2003). They report that simple cue words are more likely to appear in Newsela’s simplifications, and that complex connectives have a higher probability to be retained in Wikipedia’s. This could enable research on how discourse features influence simplification. 2.2.1 Simplification Instances. Newsela is a corpus that can be obtained for free for research purposes,4 but it cannot be redistributed. As such, it is not possible to produce and release sentence alignments for the research community in SS. This is certainly a disadvantage, because it is difficult to compare SS models developed using this"
2020.cl-1.4,W11-2802,0,0.0345876,"ng text transformations from parallel corpora of aligned original-simplified sentences in English. Compared with approaches based on hand-crafted rules, data-driven approaches can perform multiple simplification transformations simultaneously, as well as learn very specific and complex rewriting patterns. As a result, they make it possible to model interdependencies among different transformations more naturally. Therefore, we do not include approaches to sentence simplification based on sets of hand-crafted rules, such as rules for splitting and reordering sentences (Candido Jr. et al. 2009; Siddharthan 2011; Bott, Saggion, and Mille 2012), nor approaches that only learn lexical simplifications, that is, which target one-word replacements (see Paetzold and Specia [2017b] for a survey). We classify data-driven approaches for SS as relying on statistical MT techniques (Section 4.1), induction of synchronous grammars (Section 4.2), semantics-assisted (Section 4.3), and neural sequence-to-sequence models (Section 4.4). 4.1 Monolingual Statistical Machine Translation Several approaches treat SS as a monolingual MT task, with original and simplified as source and target languages, respectively. Whereas"
2020.cl-1.4,C04-1129,0,0.199974,"Missing"
2020.cl-1.4,W06-3104,0,0.125517,"Missing"
2020.cl-1.4,2006.amta-papers.25,0,0.133887,"elation with human assessments of grammaticality and meaning preservation, but not simplicity. Also, Sulem, Abend, and Rappoport (2018a) show that this correlation is low or non-existent when sentence splitting has been performed. As such, BLEU 148 Alva-Manchego, Scarton, and Specia Data-Driven Sentence Simplification should not be used as the only metric for evaluation and comparison of SS models. In addition, because of its definition, this metric is more useful with simplification corpora that provides multiple references for each original sentence. TER (Translation Edit Rate), designed by Snover et al. (2006), measures the minimum number of edits necessary to change a candidate translation so that it matches perfectly to one of the references, normalized by the average length of the references. Only the reference that is closest (according to TER) is considered for the final score. The edits to be considered are insertions, deletions, substitutions of single words, and shifts (positional changes) of word sequences. TER is an edit-distance metric Equation (3), with values ranging from 0 to 100; lower values are better. TER = # of edits average # of reference words (3) In order to calculate the numb"
2020.cl-1.4,D18-1081,0,0.141163,"Missing"
2020.cl-1.4,N18-1063,0,0.0638026,"Missing"
2020.cl-1.4,P12-2008,0,0.0350483,"it distance using minimumedit-distance and dynamic programming. For simplification research, TER’s intermediate calculations (i.e., the edits counts) have been used to show the simplification operations that an SS model is able to perform (Zhang and Lapata 2017). However, this is not a general practice and no studies have been conducted to verify that the edits correlate with simplification transformations. Scarton, Paetzold, and Specia (2018b) use TER to study the differences between different simplification versions in articles of the Newsela corpus. iBLEU is a variant of BLEU introduced by Sun and Zhou (2012) as a way to measure the quality of a candidate paraphrase. The metric balances the semantic similarity between the candidate and the reference, with the dissimilarity between the candidate and the source. Given a candidate paraphrase c, human references rs , and input text s, iBLEU is computed as in Equation (4), with values ranging from 0 to 1 (or to 100); higher values are better. iBLEU(s, rs , c) = α × BLEU(c, rs ) − (1 − α ) × BLEU(c, s) (4) After empirical evaluations, the authors recommend using a value of α between 0.7 and 0.9. For example, Mallinson, Sennrich, and Lapata (2017) experi"
2020.cl-1.4,P19-1198,0,0.139372,"of the encoder, and corresponding generated outputs. The model was trained only using WikiLarge, and tested on TurkCorpus and Newsela. The authors evaluated using both mechanisms, DCSS and DMASS, independently, as well as in conjunction. Then compared to other models, DMASS+DCSS achieved the highest SARI score in both test sets. They also estimated the correctness of rule utilization based on ground-truth from SPPDB, and showed that their models also improved compared to previous work. 168 Alva-Manchego, Scarton, and Specia Data-Driven Sentence Simplification 4.4.4 Unsupervised Architectures. Surya et al. 2019 proposed an unsupervised approach for developing a simplification system. Their motivation was to design an architecture that could be exploited to train SS models for languages or domains that do not have large resources of parallel original-simplified instances. Their proposal is based on a modified auto encoder that uses a shared encoder E and two dedicated decoders: one for generating complex sentences (Gd ) and one for simple sentences (Gs ). In addition, their model relies on Discriminator and Classifier modules. The Discriminator determines if a given context vector sequence (from eith"
2020.cl-1.4,P17-2016,0,0.0913733,"s,4 but it cannot be redistributed. As such, it is not possible to produce and release sentence alignments for the research community in SS. This is certainly a disadvantage, because it is difficult to compare SS models developed using this corpus without a common split of the data and the same document, paragraph, and sentence alignments. Xu, Callison-Burch, and Napoles (2015) align sentences between consecutive versions of articles in the corpus using Jaccard similarity (Jaccard 1912) based on overlapping word lemmas. Alignments with the highest similarity become simplification instances. ˇ Stajner et al. (2017) explore three similarity metrics and two alignment methods to produce paragraph and sentence alignments in Newsela. The first similarity metric uses a character 3-gram model (Mcnamee and Mayfield 2004) with cosine similarity. The second metric averages the word embeddings (trained in EW) of the text snippet and then uses cosine similarity. The third metric computes the cosine similarity between all word embeddings in the text snippet (instead of the average). Regarding the alignment methods, the first one uses any of the previous metrics to compute the similarity between all possible sentence"
2020.cl-1.4,L18-1615,0,0.218429,"Missing"
2020.cl-1.4,W14-1201,0,0.184351,"Missing"
2020.cl-1.4,W16-3411,0,0.0516602,"Missing"
2020.cl-1.4,W18-0535,0,0.0790745,"Missing"
2020.cl-1.4,E14-1031,0,0.0251225,"aggion (2014)’s work ˇ with features from Stajner, Popovi´c, and B´echera (2016) to analyze how different feature groups correlate with human judgments on grammaticality, meaning preservation, and simplicity using data from QATS. Using Quality Estimation research for reference-less evaluation in simplification is still an area not sufficiently explored, mainly because it requires human annotations on example instances that can be used as training data, which can be expensive to collect. Another group of approaches is interested in ranking sentences according to their predicted reading levels. Vajjala and Meurers, (2014a,b) showed that, in the PWKP (Zhu, Bernhard, and Gurevych 2010) data set and and earlier version of the OneStopEnglish (Vajjala and Luˇci´c 2018) corpus, even if all simplified sentences were simpler than their aligned original counterpart, some sentences in the “simple” section had a higher reading level than some in the “original” section. As such, attempting to use binary 8 In Quality Estimation, the goal is to evaluate an output translation without comparing it to a reference. For a comprehensive review of this area of research, please refer to Specia, Scarton, and Paetzold (2018). 152 Al"
2020.cl-1.4,P08-1040,0,0.26931,"mance Submission received: 8 June 2018; revise d version received: 9 August 2019; accepted for publication: 15 September 2019. https://doi.org/10.1162/COLI a 00370 © 2020 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 46, Number 1 on other language processing tasks, such as parsing (Chandrasekar, Doran, and Srinivas 1996), summarization (Vanderwende et al. 2007; Silveira and Branco 2012), information extraction (Evans 2011), semantic role labeling (Vickrey and Koller 2008), and Machine Translation (MT) (Hasler et al. 2017). Most research on TS has focused on studying simplification of individual sentences. Reducing the scope of the problem has allowed the easier collection and curation of corpora, as well as adapting methods from other text generation tasks, mainly MT. It can be argued that “true” TS (i.e., document-level) cannot be achieved by simplifying sentences one at a time, and we make a call in Section 6 for the field to move in that direction. However, because the goal of this article is to review what has been done in TS so far, our survey is limited"
2020.cl-1.4,N18-2013,0,0.077644,"n artificial token was added to the beginning of the input sentences indicating (1) the grade level of the simplification instance, and/or (2) one of four possible text transformations: identical, elaboration, splitting, or joining. At test time, the text transformation token is either predicted (using a simple features-based naive Bayes classifier) or an oracle label is used. They experimented using the standard neural architecture available in OpenNMT and data from the Newsela corpus. Results showed improvements in BLEU, SARI, and Flesch scores when using this extra information. N SE L STM: Vu et al. (2018) used Neural Semantic Encoders (NSEs, Munkhdalai and Yu 2017) instead of LSTMs for the encoder. At any encoding time step, a NSE has access to all the tokens in the input sequence, and is thus able to capture more context information while encoding the current token, instead of only relying on the previous hidden state. Their approach is tested on PWKP, TurkCorpus, and Newsela. Two models 165 Computational Linguistics Volume 46, Number 1 are presented, one tuned using BLEU (N SE L STM -B) and one using SARI (N SE L STM -S). When compared against other models, N SE L STM -B achieved the best BL"
2020.cl-1.4,P18-1042,0,0.0200272,"u, and Bansal (2018) proposed learning this mixing ratio dynamically using a multi-armed bandits based controller. Basically, at each round, the controller selects a task based on some noise value estimates, observes “rewards” for the selected task (in their case, the reward was the negative validation loss of the main task), and switches accordingly. The proposed model was trained and tested using PWKP, WikiLarge (with TurkCorpus as test set), and Newsela for SS; the SNLI (Bowman et al. 2015) and the MultiNLI (Williams, Nangia, and Bowman 2018) corpora for entailment generation; and ParaNMT (Wieting and Gimpel 2018) for paraphrase generation. Using automatic metrics, PointerCopy+MTL achieved the highest SARI score only in the Newsela corpus. With human judgments, their model scored as the best in simplicity. 167 Computational Linguistics Volume 46, Number 1 Figure 3 Model architecture for PointerCopy+MTL. Extracted from Guo, Pasunuru, and Bansal (2018). 4.4.3 Adding External Knowledge. The previously described models attempted to learn how to simplify only using information from the training data sets. Zhao et al. (2018) argued that the relatively small size of these data sets prevents models from genera"
2020.cl-1.4,N18-1101,0,0.0747723,"Missing"
2020.cl-1.4,D11-1038,0,0.111902,"and Elhadad (2003). This algorithm takes context into consideration: The similarity between two sentences is affected by their proximity to pairs of sentences with 1 https://simple.wikipedia.org. 2 https://wikipedia.org. 139 Computational Linguistics Volume 46, Number 1 Table 1 Summary of parallel corpora extracted from EW and SEW. An original sentence can be aligned to one (1-to-1) or more (1-to-N) unique simplified sentences. A (*) indicates that some aligned simplified sentences may not be unique. Corpora PWKP (Zhu, Bernhard, and Gurevych 2010) C&K-1 (Coster and Kauchak 2011b) RevisionWL (Woodsend and Lapata 2011a) AlignedWL (Woodsend and Lapata 2011a) C&K-2 (Kauchak 2013) EW-SEW (Hwang et al. 2015) sscorpus (Kajiwara and Komachi 2016) WikiLarge (Zhang and Lapata 2017) Instances Alignment Types 108K 137K 15K 142K 167K 392K 493K 286K 1-to-1, 1-to-N 1-to-1, 1-to-N 1-to-1*, 1-to-N*, N-to-1* 1-to-1, 1-to-N 1-to-1, 1-to-N 1-to-1 1-to-1 1-to-1*, 1-to-N*, N-to-1* high similarity. Finally, Woodsend and Lapata (2011a) also adopt the two-step process of Coster and Kauchak (2011b), using tf-idf when compiling the AlignedWL corpus. Another approach is to take advantage of the revision histories in Wikipedia artic"
2020.cl-1.4,P12-1107,0,0.374645,"Missing"
2020.cl-1.4,Q16-1029,0,0.0550075,"odels. 2.3 Other Resources for English In this section, we describe some additional resources that are used for SS in English with very specific reasons: tuning and testing of models in general purpose (TurkCorpus) and domain-specific (SimPA) data, evaluation of sentence splitting (HSplit), readability assessment (OneStopEnglish), training and testing of split-and-rephrase (W EB S PLIT and WikiSplit), and learning paraphrases (PPDB and SPPDB). 2.3.1 TurkCorpus. Just like with other text rewriting tasks, there is no single correct simplification possible for a given original sentence. As such, Xu et al. (2016) asked workers on Amazon Mechanical Turk to simplify 2,350 sentences extracted from the PWKP corpus to collect eight references for each one. This corpus was then randomly split into two sets: one with 2,000 instances intended to be used for system tuning, and one with 350 instances for measuring the performance of SS models using metrics that rely on multiple references (see SARI in Sec. 3.2.3). However, the instances chosen from PWKP are those that focus on paraphrasing (1-to-1 alignments with almost similar lengths), thus limiting the range of simplification operations that SS models can be"
2020.cl-1.4,P01-1067,0,0.339894,"Missing"
2020.cl-1.4,N10-1056,0,0.0936837,"e to the subject-verb-object order for their sentences, and avoiding compound sentences (Simple Wikipedia 2017a). 2.1.1 Simplification Instances. Much of the popularity of using Wikipedia for research in SS comes from publicly available automatically collected alignments between sentences of equivalent articles in EW and SEW. Several techniques have been explored to produce such alignments with reasonable quality. A first approach consists of aligning texts according to their term frequency–inverse document frequency (tf-idf) cosine similarity. For the PWKP corpus, Zhu, Bernhard, and Gurevych (2010) measured this directly at sentence-level between all sentences of each article pair, and sentences whose similarity was above a certain threshold were aligned. For the C&K-1 (Coster and Kauchak 2011b) and C&K-2 (Kauchak 2013) corpora, the authors first aligned paragraphs with tf-idf cosine similarity, and then found the best overall sentence alignment with the dynamic programming algorithm proposed by Barzilay and Elhadad (2003). This algorithm takes context into consideration: The similarity between two sentences is affected by their proximity to pairs of sentences with 1 https://simple.wiki"
2020.cl-1.4,D17-1062,0,0.269917,"Missing"
2020.cl-1.4,D18-1355,0,0.173206,"iams, Nangia, and Bowman 2018) corpora for entailment generation; and ParaNMT (Wieting and Gimpel 2018) for paraphrase generation. Using automatic metrics, PointerCopy+MTL achieved the highest SARI score only in the Newsela corpus. With human judgments, their model scored as the best in simplicity. 167 Computational Linguistics Volume 46, Number 1 Figure 3 Model architecture for PointerCopy+MTL. Extracted from Guo, Pasunuru, and Bansal (2018). 4.4.3 Adding External Knowledge. The previously described models attempted to learn how to simplify only using information from the training data sets. Zhao et al. (2018) argued that the relatively small size of these data sets prevents models from generalizing well, considering the vast amount of possible simplification transformations that exist. Therefore, they proposed to include human-curated paraphrasing rules from Simple Paraphrase Database (SPPDB; Pavlick and Callison-Burch 2016) into a neural encoderdecoder architecture. This intuition is similar to Xu et al. (2016), who incorporated those rewriting rules into a SBSMT-based model. In addition, the authors moved from the RNN-based architecture to one based on the Transformer (Vaswani et al. 2017). The"
2020.cl-1.4,C10-1152,0,0.727312,"Missing"
2020.cl-1.4,N16-1004,0,0.0479442,"Missing"
2020.cl-1.4,Q15-1021,0,\N,Missing
2020.cl-1.4,C96-2183,0,\N,Missing
2020.cl-1.4,W03-1004,0,\N,Missing
2020.cl-1.4,N16-1120,0,\N,Missing
2020.cl-1.4,P16-2055,0,\N,Missing
2020.cl-1.4,P17-1080,0,\N,Missing
2020.cl-1.4,W18-0503,0,\N,Missing
2020.cl-1.4,W18-7005,0,\N,Missing
2020.cl-1.4,W09-2105,1,\N,Missing
2020.cl-1.4,W14-1214,1,\N,Missing
2020.coling-main.210,E06-1032,0,0.142652,"eni et al., 2002) and M ETEOR (Denkowski and Lavie, 2014) are increasingly used for other tasks, along with more task-oriented metrics such as ROUGE (Lin, 2004) for summarisation and C IDER (Vedantam et al., 2015) for visual captioning. Reiter and Belz (2009) remark that it is not sufficient to conclude on the usefulness of a natural language generation system’s output by solely relying on metrics that quantify the similarity of the output to human-authored texts. Previous criticisms concerning automatic metrics corroborate this perspective to some degree. To cite a few, in the context of MT, Callison-Burch et al. (2006) state that human judgments may not correlate with B LEU and an increase in B LEU does not always indicate an improvement in quality. Further, Mathur et al. (2020) challenge the stability of the common practices measuring correlation between metrics and human judgments, the standard approach in the MT community, and show that they may be severely impacted by outlier systems and the sample size. In the context of image captioning, Wang and Chan (2019) claim that the consensus-based evaluation protocol of C IDER actually penalises output diversity. Similar problems have also been discussed in th"
2020.coling-main.210,P11-1020,0,0.0539204,"ts are in line with the concurrent work of Mathur et al. (2020) which suggests that important conclusions, such as comparative judgments about systems, should not be drawn based only on small changes in automatic metrics. 3.3 Single representative sentence Following the observations from the previous experiments, we search over the training set for a single representative sentence (SS) which maximises test set B LEU6 when used as a system output for every test set instance.7 We explore tasks and datasets which include the ones previously introduced (§ 3.1). For visual captioning, we add MSVD (Chen and Dolan, 2011), a widely known dataset of 1,970 videos, with up to 41 English captions per video. For image captioning, we use English Flickr30k (Young et al., 2014), and the STAIR (Yoshikawa et al., 2017) dataset which provides Japanese captions for COCO images. We also explore the multi-turn dialogue dataset DailyDialog (Li et al., 2017) which contains conversations that cover 10 different daily life topics, and its multi-reference test set (Gupta et al., 2019). Table 4 summarises the statistics about the datasets explored for this experiment. Table 5 draws a comparison between the scores obtained for the"
2020.coling-main.210,W14-3348,0,0.0578935,"uthored texts, (ii) can be insensitive to correct translations of rare words, (iii) can yield surprisingly high scores when given a single sentence as system output for the entire test set. 1 Introduction Human assessment is the best practice at hand for evaluating language generation tasks such as machine translation (MT), dialogue systems, visual captioning and abstractive summarisation. In practice, however, we rely on automatic metrics which compare system outputs to human-authored references. Initially proposed for MT evaluation, metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Denkowski and Lavie, 2014) are increasingly used for other tasks, along with more task-oriented metrics such as ROUGE (Lin, 2004) for summarisation and C IDER (Vedantam et al., 2015) for visual captioning. Reiter and Belz (2009) remark that it is not sufficient to conclude on the usefulness of a natural language generation system’s output by solely relying on metrics that quantify the similarity of the output to human-authored texts. Previous criticisms concerning automatic metrics corroborate this perspective to some degree. To cite a few, in the context of MT, Callison-Burch et al. (2006) state that human judgments m"
2020.coling-main.210,W16-2345,0,0.0205924,"Discussion In this work, we explore cases where commonly used language generation evaluation metrics exhibit counter-intuitive behaviour. Although the main goal in language generation tasks is to generate ‘humanquality’ texts, our analysis in § 3.1 shows that metrics have a preference towards machine-generated texts rather than human references. Our perturbation experiments in § 3.2.2 highlight potential insensitivity of metrics to lexical changes in infrequent n-grams. This is a major concern for tasks such as multimodal machine translation (Specia et al., 2016) or pronoun resolution in MT (Guillou et al., 2016), where the metrics are expected to capture lexical changes which are due to rarely occurring linguistic ambiguities. We believe that targeted probes (Isabelle et al., 2017) are much more reliable than sentence or corpus level metrics for such tasks. Finally, we reveal that metrics tend to produce unexpectedly high scores when each test set hypothesis is set to a particular training set sentence, which can be thought of as finding the sweet spot of the corpus (§ 3.3). Therefore, we note that a high correlation between metrics and human judgments is not sufficient to characterise the reliabilit"
2020.coling-main.210,W19-5944,0,0.0838856,"very test set instance.7 We explore tasks and datasets which include the ones previously introduced (§ 3.1). For visual captioning, we add MSVD (Chen and Dolan, 2011), a widely known dataset of 1,970 videos, with up to 41 English captions per video. For image captioning, we use English Flickr30k (Young et al., 2014), and the STAIR (Yoshikawa et al., 2017) dataset which provides Japanese captions for COCO images. We also explore the multi-turn dialogue dataset DailyDialog (Li et al., 2017) which contains conversations that cover 10 different daily life topics, and its multi-reference test set (Gupta et al., 2019). Table 4 summarises the statistics about the datasets explored for this experiment. Table 5 draws a comparison between the scores obtained for the retrieved single sentences, baselines and state-of-the-art systems when available. We observe that: (i) M SVD exhibits the highest scores with 30.6 B LEU and 23.4 M ETEOR, the latter being very close to a strong captioning baseline (Venugopalan et al., 2015), (ii) the SS scores for the DAILY D IALOG are surprisingly on par with a recent baseline and a state-of-the-art system, and (iii) C IDER is more robust against the single sentence adversary as"
2020.coling-main.210,D17-1263,0,0.0176702,"e generation tasks is to generate ‘humanquality’ texts, our analysis in § 3.1 shows that metrics have a preference towards machine-generated texts rather than human references. Our perturbation experiments in § 3.2.2 highlight potential insensitivity of metrics to lexical changes in infrequent n-grams. This is a major concern for tasks such as multimodal machine translation (Specia et al., 2016) or pronoun resolution in MT (Guillou et al., 2016), where the metrics are expected to capture lexical changes which are due to rarely occurring linguistic ambiguities. We believe that targeted probes (Isabelle et al., 2017) are much more reliable than sentence or corpus level metrics for such tasks. Finally, we reveal that metrics tend to produce unexpectedly high scores when each test set hypothesis is set to a particular training set sentence, which can be thought of as finding the sweet spot of the corpus (§ 3.3). Therefore, we note that a high correlation between metrics and human judgments is not sufficient to characterise the reliability of a metric. The latter probably requires a thorough exploration and mitigation of adversarial cases such as the proposed single sentence baseline. Acknowledgments We than"
2020.coling-main.210,I17-1099,0,0.0197106,"e training set for a single representative sentence (SS) which maximises test set B LEU6 when used as a system output for every test set instance.7 We explore tasks and datasets which include the ones previously introduced (§ 3.1). For visual captioning, we add MSVD (Chen and Dolan, 2011), a widely known dataset of 1,970 videos, with up to 41 English captions per video. For image captioning, we use English Flickr30k (Young et al., 2014), and the STAIR (Yoshikawa et al., 2017) dataset which provides Japanese captions for COCO images. We also explore the multi-turn dialogue dataset DailyDialog (Li et al., 2017) which contains conversations that cover 10 different daily life topics, and its multi-reference test set (Gupta et al., 2019). Table 4 summarises the statistics about the datasets explored for this experiment. Table 5 draws a comparison between the scores obtained for the retrieved single sentences, baselines and state-of-the-art systems when available. We observe that: (i) M SVD exhibits the highest scores with 30.6 B LEU and 23.4 M ETEOR, the latter being very close to a strong captioning baseline (Venugopalan et al., 2015), (ii) the SS scores for the DAILY D IALOG are surprisingly on par w"
2020.coling-main.210,W04-1013,0,0.103135,"when given a single sentence as system output for the entire test set. 1 Introduction Human assessment is the best practice at hand for evaluating language generation tasks such as machine translation (MT), dialogue systems, visual captioning and abstractive summarisation. In practice, however, we rely on automatic metrics which compare system outputs to human-authored references. Initially proposed for MT evaluation, metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Denkowski and Lavie, 2014) are increasingly used for other tasks, along with more task-oriented metrics such as ROUGE (Lin, 2004) for summarisation and C IDER (Vedantam et al., 2015) for visual captioning. Reiter and Belz (2009) remark that it is not sufficient to conclude on the usefulness of a natural language generation system’s output by solely relying on metrics that quantify the similarity of the output to human-authored texts. Previous criticisms concerning automatic metrics corroborate this perspective to some degree. To cite a few, in the context of MT, Callison-Burch et al. (2006) state that human judgments may not correlate with B LEU and an increase in B LEU does not always indicate an improvement in quality"
2020.coling-main.210,2020.acl-main.448,0,0.0316017,"isation and C IDER (Vedantam et al., 2015) for visual captioning. Reiter and Belz (2009) remark that it is not sufficient to conclude on the usefulness of a natural language generation system’s output by solely relying on metrics that quantify the similarity of the output to human-authored texts. Previous criticisms concerning automatic metrics corroborate this perspective to some degree. To cite a few, in the context of MT, Callison-Burch et al. (2006) state that human judgments may not correlate with B LEU and an increase in B LEU does not always indicate an improvement in quality. Further, Mathur et al. (2020) challenge the stability of the common practices measuring correlation between metrics and human judgments, the standard approach in the MT community, and show that they may be severely impacted by outlier systems and the sample size. In the context of image captioning, Wang and Chan (2019) claim that the consensus-based evaluation protocol of C IDER actually penalises output diversity. Similar problems have also been discussed in the area of automatic summarisation with respect to ROUGE (Schluter, 2017). Nevertheless, automatic metrics like these are a necessity and remain popular, especially"
2020.coling-main.210,P02-1040,0,0.114542,"lly prefer system outputs to human-authored texts, (ii) can be insensitive to correct translations of rare words, (iii) can yield surprisingly high scores when given a single sentence as system output for the entire test set. 1 Introduction Human assessment is the best practice at hand for evaluating language generation tasks such as machine translation (MT), dialogue systems, visual captioning and abstractive summarisation. In practice, however, we rely on automatic metrics which compare system outputs to human-authored references. Initially proposed for MT evaluation, metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Denkowski and Lavie, 2014) are increasingly used for other tasks, along with more task-oriented metrics such as ROUGE (Lin, 2004) for summarisation and C IDER (Vedantam et al., 2015) for visual captioning. Reiter and Belz (2009) remark that it is not sufficient to conclude on the usefulness of a natural language generation system’s output by solely relying on metrics that quantify the similarity of the output to human-authored texts. Previous criticisms concerning automatic metrics corroborate this perspective to some degree. To cite a few, in the context of MT, Callison-Burch et"
2020.coling-main.210,J09-4008,0,0.0310954,"uman assessment is the best practice at hand for evaluating language generation tasks such as machine translation (MT), dialogue systems, visual captioning and abstractive summarisation. In practice, however, we rely on automatic metrics which compare system outputs to human-authored references. Initially proposed for MT evaluation, metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Denkowski and Lavie, 2014) are increasingly used for other tasks, along with more task-oriented metrics such as ROUGE (Lin, 2004) for summarisation and C IDER (Vedantam et al., 2015) for visual captioning. Reiter and Belz (2009) remark that it is not sufficient to conclude on the usefulness of a natural language generation system’s output by solely relying on metrics that quantify the similarity of the output to human-authored texts. Previous criticisms concerning automatic metrics corroborate this perspective to some degree. To cite a few, in the context of MT, Callison-Burch et al. (2006) state that human judgments may not correlate with B LEU and an increase in B LEU does not always indicate an improvement in quality. Further, Mathur et al. (2020) challenge the stability of the common practices measuring correlati"
2020.coling-main.210,E17-2007,0,0.0222589,"and an increase in B LEU does not always indicate an improvement in quality. Further, Mathur et al. (2020) challenge the stability of the common practices measuring correlation between metrics and human judgments, the standard approach in the MT community, and show that they may be severely impacted by outlier systems and the sample size. In the context of image captioning, Wang and Chan (2019) claim that the consensus-based evaluation protocol of C IDER actually penalises output diversity. Similar problems have also been discussed in the area of automatic summarisation with respect to ROUGE (Schluter, 2017). Nevertheless, automatic metrics like these are a necessity and remain popular, especially given the increasing number of open evaluation challenges. In this paper, we further probe B LEU , M ETEOR , C IDER -D and ROUGEL metrics (§ 2) that are commonly used to quantify progress in language generation tasks. We also probe a recently proposed contextualised embeddings-based metric called B ERT S CORE (Zhang et al., 2020). We first conduct leave-oneout average scoring with multiple references and show that, counter-intuitively, metrics tend to reward system outputs more than human-authored refer"
2020.coling-main.210,W16-2346,1,0.891855,"Missing"
2020.coling-main.210,P12-2068,0,0.0808053,"Missing"
2020.coling-main.210,P17-2066,0,0.0164142,"ll changes in automatic metrics. 3.3 Single representative sentence Following the observations from the previous experiments, we search over the training set for a single representative sentence (SS) which maximises test set B LEU6 when used as a system output for every test set instance.7 We explore tasks and datasets which include the ones previously introduced (§ 3.1). For visual captioning, we add MSVD (Chen and Dolan, 2011), a widely known dataset of 1,970 videos, with up to 41 English captions per video. For image captioning, we use English Flickr30k (Young et al., 2014), and the STAIR (Yoshikawa et al., 2017) dataset which provides Japanese captions for COCO images. We also explore the multi-turn dialogue dataset DailyDialog (Li et al., 2017) which contains conversations that cover 10 different daily life topics, and its multi-reference test set (Gupta et al., 2019). Table 4 summarises the statistics about the datasets explored for this experiment. Table 5 draws a comparison between the scores obtained for the retrieved single sentences, baselines and state-of-the-art systems when available. We observe that: (i) M SVD exhibits the highest scores with 30.6 B LEU and 23.4 M ETEOR, the latter being v"
2020.coling-main.210,Q14-1006,0,0.0260507,"hould not be drawn based only on small changes in automatic metrics. 3.3 Single representative sentence Following the observations from the previous experiments, we search over the training set for a single representative sentence (SS) which maximises test set B LEU6 when used as a system output for every test set instance.7 We explore tasks and datasets which include the ones previously introduced (§ 3.1). For visual captioning, we add MSVD (Chen and Dolan, 2011), a widely known dataset of 1,970 videos, with up to 41 English captions per video. For image captioning, we use English Flickr30k (Young et al., 2014), and the STAIR (Yoshikawa et al., 2017) dataset which provides Japanese captions for COCO images. We also explore the multi-turn dialogue dataset DailyDialog (Li et al., 2017) which contains conversations that cover 10 different daily life topics, and its multi-reference test set (Gupta et al., 2019). Table 4 summarises the statistics about the datasets explored for this experiment. Table 5 draws a comparison between the scores obtained for the retrieved single sentences, baselines and state-of-the-art systems when available. We observe that: (i) M SVD exhibits the highest scores with 30.6 B"
2020.eamt-1.16,C04-1046,0,0.505771,"slation errors in a sentence while minimising the need for direct human annotation. For that purpose, we use transfer-learning to leverage large scale noisy annotations and small sets of high-quality human annotated translation errors to train QE models. Experiments on four language pairs and translations obtained by statistical and neural models show consistent gains over strong baselines. 1 Introduction Quality Estimation (QE) for Machine Translation (MT) is the task of predicting the overall quality of an automatically generated translation e.g., on either word, sentence or document level (Blatz et al., 2004; Ueffing and Ney, 2007). In opposition to automatic metrics and manual evaluation which rely on gold standard reference translations, QE models can produce quality estimates on unseen data, c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. and at runtime. QE has already proven its usefulness in many applications such as improving productivity in post-editing of MT, and recent neuralbased approaches to QE have been shown to provide promising performance in predicting quality of neural MT output (Fonseca et al., 2019)."
2020.eamt-1.16,P13-1004,1,0.801381,"scores (Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016; Bojar et al., 2017; Specia et al., 2018; Fonseca et al., 2019). Transfer-learning for QE Transfer-learning (TL) is a machine learning approach where models trained on a source task are adapted to a related target task (Pan et al., 2010; Yosinski et al., 2014). Transfer-learning methods have been widely used in NLP, e.g., machine translation (Zoph et al., 2016) and text classification (Howard and Ruder, 2018). Previous work on TL for QE focused on adapting models for labels produced by different annotators (Cohn and Specia, 2013; Shah and Specia, 2016) which is different to this work. More recent work on TL techniques for QE explore pre-trained word representations. This was first done by P OSTECH (Kim et al., 2017), best performing neural-based architecture in the QE shared task at WMT’17 (Bojar et al., 2017). P OSTECH re-purposes a recurrent neural network encoder pre-trained on large parallel corpora, to predict HTER scores using multi-task learning at different levels of granularity (e.g., word, phrase, or sentence). Then, Kepler et al. (2019) used a predictorestimator architecture similar to P OSTECH alongside v"
2020.eamt-1.16,P15-1174,0,0.0165917,"standard labels. We also experimented with two approaches for finetuning: (1) unfreezing all the layers at the same time; and (2) a gradual unfreezing approach proposed by (Howard and Ruder, 2018). We use Adam (Kingma and Ba, 2014) with default parameters, and a batch size of 100. For the Hybrid model, we optimise the L2 regularisation penalty. Table 2 reports on the optimal values determined by hyper-parameters optimisation. 5 Results Tables 3 and 4 show respectively the average absolute Pearson’s r correlation co-efficient and the Root Mean Square Error (the official metrics for this task (Graham, 2015)) between actual and predicted MQM error proportions in six combinations of MT models (PBMT, NMT) and language pairs (EN-DE, EN-LV, DE-EN and EN-CS). First, we observe that the baseline model (LRQEfeat) performs fairly well on predicting the proportion of errors, especially for the EN-DE and EN-CS PBMT. However, it is not robust across language pairs and types of translation systems. 4 We have also tested a Support Vector Regression with a radial basis function kernel, but it yielded lower performance. 5 We did not observe noticeable differences in performance using smaller or larger size in e"
2020.eamt-1.16,P18-1031,0,0.171417,"ween annotators is generally low (Specia, 2011). More recently, sentence-level QE models are most typically trained on HTER scores (Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016; Bojar et al., 2017; Specia et al., 2018; Fonseca et al., 2019). Transfer-learning for QE Transfer-learning (TL) is a machine learning approach where models trained on a source task are adapted to a related target task (Pan et al., 2010; Yosinski et al., 2014). Transfer-learning methods have been widely used in NLP, e.g., machine translation (Zoph et al., 2016) and text classification (Howard and Ruder, 2018). Previous work on TL for QE focused on adapting models for labels produced by different annotators (Cohn and Specia, 2013; Shah and Specia, 2016) which is different to this work. More recent work on TL techniques for QE explore pre-trained word representations. This was first done by P OSTECH (Kim et al., 2017), best performing neural-based architecture in the QE shared task at WMT’17 (Bojar et al., 2017). P OSTECH re-purposes a recurrent neural network encoder pre-trained on large parallel corpora, to predict HTER scores using multi-task learning at different levels of granularity (e.g., wor"
2020.eamt-1.16,C18-1266,1,0.91762,"Missing"
2020.eamt-1.16,W19-5406,0,0.039002,"Missing"
2020.eamt-1.16,W17-4763,0,0.0444526,"(TL) is a machine learning approach where models trained on a source task are adapted to a related target task (Pan et al., 2010; Yosinski et al., 2014). Transfer-learning methods have been widely used in NLP, e.g., machine translation (Zoph et al., 2016) and text classification (Howard and Ruder, 2018). Previous work on TL for QE focused on adapting models for labels produced by different annotators (Cohn and Specia, 2013; Shah and Specia, 2016) which is different to this work. More recent work on TL techniques for QE explore pre-trained word representations. This was first done by P OSTECH (Kim et al., 2017), best performing neural-based architecture in the QE shared task at WMT’17 (Bojar et al., 2017). P OSTECH re-purposes a recurrent neural network encoder pre-trained on large parallel corpora, to predict HTER scores using multi-task learning at different levels of granularity (e.g., word, phrase, or sentence). Then, Kepler et al. (2019) used a predictorestimator architecture similar to P OSTECH alongside very large scale pre-trained representations from BERT (Devlin et al., 2018) and XLM (Lample and Conneau, 2019), and ensembling techniques, to win the QE tasks at WMT’19 (Fonseca et al., 2019)"
2020.eamt-1.16,quirk-2004-training,0,0.143311,"e introduce a new task of predicting the proportion of actual translation errors using transfer-learning for QE1 , by leveraging large scale noisy HTER annotations and smaller but of higher quality expert MQM annotations; (2) we show that our simple yet effective approach using transfer-learning yields better performance at predicting the proportion of actual errors in MT, compared to models trained directly on expert-annotated MQM or HTER-only data; (3) we report experiments on four language pairs and both statistical and neural MT systems. 2 Related Work Quality labels for sentence-level QE Quirk (2004) introduced the use of manually created 1 https://github.com/sheffieldnlp/tlqe quality labels for evaluating MT systems. With a rather small dataset (approximately 350 sentences), they reported better results than those obtained with a much larger set of instances annotated automatically. Similarly, Specia et al. (2009) proposed the use of a (1-4) Likert scale representing a translator’s perception on quality with regard to the degree of difficulty to fix a translation. However, sentence-level quality annotations appear to be subjective while agreement between annotators is generally low (Spec"
2020.eamt-1.16,N16-1069,1,0.821075,"013; Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016; Bojar et al., 2017; Specia et al., 2018; Fonseca et al., 2019). Transfer-learning for QE Transfer-learning (TL) is a machine learning approach where models trained on a source task are adapted to a related target task (Pan et al., 2010; Yosinski et al., 2014). Transfer-learning methods have been widely used in NLP, e.g., machine translation (Zoph et al., 2016) and text classification (Howard and Ruder, 2018). Previous work on TL for QE focused on adapting models for labels produced by different annotators (Cohn and Specia, 2013; Shah and Specia, 2016) which is different to this work. More recent work on TL techniques for QE explore pre-trained word representations. This was first done by P OSTECH (Kim et al., 2017), best performing neural-based architecture in the QE shared task at WMT’17 (Bojar et al., 2017). P OSTECH re-purposes a recurrent neural network encoder pre-trained on large parallel corpora, to predict HTER scores using multi-task learning at different levels of granularity (e.g., word, phrase, or sentence). Then, Kepler et al. (2019) used a predictorestimator architecture similar to P OSTECH alongside very large scale pre-trai"
2020.eamt-1.16,2006.amta-papers.25,0,0.321078,"MT models themselves, such as drastic degradation of their performance on out-of-domain data. As an alternative, QE models are often trained under weak supervision, using training instances labelled from noisy or limited sources (e.g. data labelled with automatic metrics for MT). Here, we focus on sentence-level QE, where given a pair of sentences (the source and its translation), the aim is to train supervised Machine Learning (ML) models that can predict a quality label as a numerical value. The most widely used label for sentence-level QE is the Human-mediated Translation Edit Rate (HTER) (Snover et al., 2006), which represents the post-editing effort. HTER consists of the minimum number of edits a human language expert is required to make in order to fix the translation errors in a sentence, taking values between 0 and 1. The main limitation of HTER is that it does not represent an actual translation error rate, but its noisy approximation. The noise stems mostly from errors in the heuristics used to automatically align the machine translation and its post-edited version, but also from the fact that some edits represent preferential choices of humans, rather than errors. To overcome such limitatio"
2020.eamt-1.16,2009.eamt-1.5,1,0.770558,"rformance at predicting the proportion of actual errors in MT, compared to models trained directly on expert-annotated MQM or HTER-only data; (3) we report experiments on four language pairs and both statistical and neural MT systems. 2 Related Work Quality labels for sentence-level QE Quirk (2004) introduced the use of manually created 1 https://github.com/sheffieldnlp/tlqe quality labels for evaluating MT systems. With a rather small dataset (approximately 350 sentences), they reported better results than those obtained with a much larger set of instances annotated automatically. Similarly, Specia et al. (2009) proposed the use of a (1-4) Likert scale representing a translator’s perception on quality with regard to the degree of difficulty to fix a translation. However, sentence-level quality annotations appear to be subjective while agreement between annotators is generally low (Specia, 2011). More recently, sentence-level QE models are most typically trained on HTER scores (Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016; Bojar et al., 2017; Specia et al., 2018; Fonseca et al., 2019). Transfer-learning for QE Transfer-learning (TL) is a machine learning approach wher"
2020.eamt-1.16,P15-4020,1,0.844269,"mall learning rate following (Howard and Ruder, 2018). Hybrid Finally, we hypothesise that linguistic information (e.g., number of tokens in the source/target sentence, language model probability of source/target sentence, etc.) might be complementary to the source-target representations obtained by our BiRNN-MQMT L +FT model. For that purpose, we first extract a representation of the source and translated sentence by removing the BiRNN-MQMT L +FT output layer and then we concatenate it with the widely used 17 blackbox sentence-level QE features extracted with the open-source QuEst++ toolkit (Specia et al., 2015). The joint neural and linguistic information of the source and target sentences is fed into a linear regression2 model using a L2 regularisation penalty. 4 4.1 Experimental Setup Data For our experiments, we use the freely available QT21 dataset3 (Specia et al., 2017) used in the QE shared task (Bojar et al., 2017; Specia et al., 2018). This dataset contains both post-edited (HTER) and error-annotated (MQM) data in four language pairs: English into German, Latvian and Czech, and German into English; and phrase-based statistical (PBMT) and neural (NMT) translation models. The annotation for er"
2020.eamt-1.16,W18-6451,1,0.907321,"Missing"
2020.eamt-1.16,2011.eamt-1.12,1,0.763893,"004) introduced the use of manually created 1 https://github.com/sheffieldnlp/tlqe quality labels for evaluating MT systems. With a rather small dataset (approximately 350 sentences), they reported better results than those obtained with a much larger set of instances annotated automatically. Similarly, Specia et al. (2009) proposed the use of a (1-4) Likert scale representing a translator’s perception on quality with regard to the degree of difficulty to fix a translation. However, sentence-level quality annotations appear to be subjective while agreement between annotators is generally low (Specia, 2011). More recently, sentence-level QE models are most typically trained on HTER scores (Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016; Bojar et al., 2017; Specia et al., 2018; Fonseca et al., 2019). Transfer-learning for QE Transfer-learning (TL) is a machine learning approach where models trained on a source task are adapted to a related target task (Pan et al., 2010; Yosinski et al., 2014). Transfer-learning methods have been widely used in NLP, e.g., machine translation (Zoph et al., 2016) and text classification (Howard and Ruder, 2018). Previous work on TL fo"
2020.eamt-1.16,J07-1003,0,0.0660234,"sentence while minimising the need for direct human annotation. For that purpose, we use transfer-learning to leverage large scale noisy annotations and small sets of high-quality human annotated translation errors to train QE models. Experiments on four language pairs and translations obtained by statistical and neural models show consistent gains over strong baselines. 1 Introduction Quality Estimation (QE) for Machine Translation (MT) is the task of predicting the overall quality of an automatically generated translation e.g., on either word, sentence or document level (Blatz et al., 2004; Ueffing and Ney, 2007). In opposition to automatic metrics and manual evaluation which rely on gold standard reference translations, QE models can produce quality estimates on unseen data, c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. and at runtime. QE has already proven its usefulness in many applications such as improving productivity in post-editing of MT, and recent neuralbased approaches to QE have been shown to provide promising performance in predicting quality of neural MT output (Fonseca et al., 2019). QE models are trained un"
2020.eamt-1.16,D16-1163,0,0.0285634,"appear to be subjective while agreement between annotators is generally low (Specia, 2011). More recently, sentence-level QE models are most typically trained on HTER scores (Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016; Bojar et al., 2017; Specia et al., 2018; Fonseca et al., 2019). Transfer-learning for QE Transfer-learning (TL) is a machine learning approach where models trained on a source task are adapted to a related target task (Pan et al., 2010; Yosinski et al., 2014). Transfer-learning methods have been widely used in NLP, e.g., machine translation (Zoph et al., 2016) and text classification (Howard and Ruder, 2018). Previous work on TL for QE focused on adapting models for labels produced by different annotators (Cohn and Specia, 2013; Shah and Specia, 2016) which is different to this work. More recent work on TL techniques for QE explore pre-trained word representations. This was first done by P OSTECH (Kim et al., 2017), best performing neural-based architecture in the QE shared task at WMT’17 (Bojar et al., 2017). P OSTECH re-purposes a recurrent neural network encoder pre-trained on large parallel corpora, to predict HTER scores using multi-task learn"
2020.emnlp-main.184,D18-1337,0,0.199294,"trade-off between the quality of the translation and the latency incurred in producing it. Previous work has considered rulebased strategies that rely on waiting until some constraint is satisfied, which includes approaches based on syntactic constraints (Bub et al., 1997; Ryu et al., 2006), segment/chunk/alignment information (Bangalore et al., 2012) heuristic-based conditions during decoding (Cho and Esipova, 2016) or deterministic policies with pre-determined latency constraints (Ma et al., 2019). An alternative line of research focuses on learning the decision policy: Gu et al. (2017) and Alinejad et al. (2018) frame SiMT as learning to generate READ/WRITE actions and employ reinforcement learning (RL) to formulate the problem as a policy agent interacting with its environment (i.e. a pre-trained MT model). Recent work has also explored supervised learning of the policy, by using oracle action sequences predicted by a pre-trained MT using confidence-based heuristics (Zheng et al., 2019) or external word aligners (Arthur et al., 2020) (details in §2). Thus far, all prior research has focused on unimodal interpretation1 . In this paper, we explore SiMT for multimodal machine translation (MMT) (Specia"
2020.emnlp-main.184,P19-1126,0,0.0613006,"Missing"
2020.emnlp-main.184,2020.iwslt-1.27,0,0.04374,"al. (2018) pointed out a potential mismatch between the training and decoding regimens of such approaches and proposed fine-tuning the models using chunked data or prefix pairs. Ma et al. (2019) proposed an end-to-end, fixed-latency framework called ‘waitk’ which allows prefix-to-prefix training using a deterministic policy: the agent starts by reading a specified number of source tokens (k), followed by alternating WRITE and READ actions. Arivazhagan et al. (2019) extended the wait-k framework using an advanced attention mechanism and optimising a differential latency metric (DAL). Recently, Arivazhagan et al. (2020) explored a radically different approach which enriches full-sentence training with prefix pairs (Niehues et al., 2018) and allows re-translation of previously committed target tokens to increase the translation quality. Another line of research focuses on learning adaptive policies in a supervised way by using oracle READ/WRITE actions generated with heuristic or alignment-based approaches. Zheng et al. (2019) extracted action sequences from a pre-trained NMT model with a confidence-based heuristic and used them to train a separate policy network while Arthur et al. (2020) explored jointly tr"
2020.emnlp-main.184,2021.eacl-main.233,0,0.930166,"Missing"
2020.emnlp-main.184,N12-1048,0,0.0230046,"nslation reliably, and how long the listener has to wait for the translation. In contrast to consecutive machine translation where source sentences are available in their entirety before translation, the challenge in SiMT is thus the design of a strategy to find a good trade-off between the quality of the translation and the latency incurred in producing it. Previous work has considered rulebased strategies that rely on waiting until some constraint is satisfied, which includes approaches based on syntactic constraints (Bub et al., 1997; Ryu et al., 2006), segment/chunk/alignment information (Bangalore et al., 2012) heuristic-based conditions during decoding (Cho and Esipova, 2016) or deterministic policies with pre-determined latency constraints (Ma et al., 2019). An alternative line of research focuses on learning the decision policy: Gu et al. (2017) and Alinejad et al. (2018) frame SiMT as learning to generate READ/WRITE actions and employ reinforcement learning (RL) to formulate the problem as a policy agent interacting with its environment (i.e. a pre-trained MT model). Recent work has also explored supervised learning of the policy, by using oracle action sequences predicted by a pre-trained MT us"
2020.emnlp-main.184,W18-6402,1,0.869037,"− C ∗ ) + 1] + βbDt − D∗ c+ where Ct denotes the CW metric introduced here to avoid long consecutive waits and Dt refers to AP (see § 3.4.1 for metrics). D∗ and C ∗ are hyperparameters that determine the expected/target values for AP and CW, respectively. The optimal quality-latency trade-off is achieved by balancing the two reward terms. 4 Experimental Setup 4.1 Dataset We use the Multi30k dataset (Elliott et al., 2016)5 which has been the primary corpus for MMT research across the three shared tasks of the “Conference on Machine Translation (WMT)” (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Multi30k extends the Flickr30k image captioning dataset (Young et al., 2014) to provide caption translations in German, French and Czech. In this work, we focus on the English→German and English→French (Elliott et al., 2017) language directions (Table 1). We use flickr2016 (2016), flickr2017 (2017) and coco2017 (COCO) for model evaluation. The latter test set is explicitly designed (Elliott et al., 2017) to contain at least 5 Preprocessing. We use Moses scripts (Koehn et al., 2007) to lowercase, punctuation-normalise and tokenise the sentences with hyphen splitting. We then create word vocab"
2020.emnlp-main.184,W17-4746,1,0.865441,"Missing"
2020.emnlp-main.184,W16-2358,1,0.892582,"Missing"
2020.emnlp-main.184,N19-1422,1,0.913351,"formance (Table 2). We observe that the decoder-attention using object detection features (DEC-OD) performs better than other variants. We also see that the improvements on flickr2017 (⇑ 0.5) and coco2017 (⇑ 1.03) test sets are higher than flickr2016 (⇑ 0.1) on average. A possible explanation is that flickr2017 and coco2017 are more distant from the training set distribution (higher OOV count, see Table 1) and thus there is more room for improvement with the visual cues. In summary, unlike previous conclusions in MMT where improvements were not found to be substantial (Gr¨onroos et al., 2018; Caglayan et al., 2019), we observe that the benefit of the visual modality is more pronounced here. We believe that this is due to (i) the encoder being now unidirectional different from state-of-the-art NMT, (ii) the modality representations being passed through layer normalisation (Ba et al., 2016), and (iii) the representational power of OD features. 5.2 Unimodal SiMT baselines We now compare unimodal SiMT approaches to get an initial understanding of how they perform on Multi30k. Figure 2 contrasts AL and BLEU for three trained wait-k systems, wait-if-diff (WID) decoding with k ∈ {1, 2} and δ=1, reinforcement l"
2020.emnlp-main.184,W16-2359,0,0.467061,"using auxiliary sources of information (Sulubacak et al., 2020). The most typical framework explored in previous work makes use of the images when translating their descriptions between languages, with the hypothesis that visual grounding could provide contextual cues to resolve linguistic phenomena such as word-sense disambiguation or gender marking. Existing work often rely on the use of visual features extracted from state-of-the-art CNN models pre-trained on large-scale visual tasks. The methods can be grouped into two branches depending on the feature type used: (i) multimodal attention (Calixto et al., 2016; Caglayan et al., 2016; Libovick´y and Helcl, 2017; Delbrouck and Dupont, 2017) which implements a soft attention (Bahdanau 2 During our initial experiments we also explored the RLbased SiMT policy (Gu et al., 2017) but could not find good hyper-parameter settings, especially settings which were stable across two language pairs. Therefore, we did not proceed with RL for multimodal SiMT. et al., 2014) over spatial feature maps, and (ii) multimodal interaction between a pooled visual feature vector and linguistic representations (Calixto and Liu, 2017; Caglayan et al., 2017a; Elliott and K´ad´a"
2020.emnlp-main.184,D17-1105,0,0.103036,"feature type used: (i) multimodal attention (Calixto et al., 2016; Caglayan et al., 2016; Libovick´y and Helcl, 2017; Delbrouck and Dupont, 2017) which implements a soft attention (Bahdanau 2 During our initial experiments we also explored the RLbased SiMT policy (Gu et al., 2017) but could not find good hyper-parameter settings, especially settings which were stable across two language pairs. Therefore, we did not proceed with RL for multimodal SiMT. et al., 2014) over spatial feature maps, and (ii) multimodal interaction between a pooled visual feature vector and linguistic representations (Calixto and Liu, 2017; Caglayan et al., 2017a; Elliott and K´ad´ar, 2017; Gr¨onroos et al., 2018). 2.2 Simultaneous Neural MT Simultaneous NMT was first explored by Cho and Esipova (2016) in a greedy decoding framework where heuristic waiting criteria are used to decide whether the model should read more source words or emit a target word. Gu et al. (2017) instead utilised a pre-trained NMT model in conjunction with a reinforcement learning agent whose goal is to learn a READ/WRITE policy by maximising quality and minimising latency. Alinejad et al. (2018) further extended the latter approach by adding a PREDICT a"
2020.emnlp-main.184,D14-1179,0,0.0521052,"Missing"
2020.emnlp-main.184,N18-2079,0,0.0385922,"edy decoding framework where heuristic waiting criteria are used to decide whether the model should read more source words or emit a target word. Gu et al. (2017) instead utilised a pre-trained NMT model in conjunction with a reinforcement learning agent whose goal is to learn a READ/WRITE policy by maximising quality and minimising latency. Alinejad et al. (2018) further extended the latter approach by adding a PREDICT action whose purpose is to anticipate the next source word. A common property of the above approaches is their reliance on consecutive NMT models pretrained on full-sentences. Dalvi et al. (2018) pointed out a potential mismatch between the training and decoding regimens of such approaches and proposed fine-tuning the models using chunked data or prefix pairs. Ma et al. (2019) proposed an end-to-end, fixed-latency framework called ‘waitk’ which allows prefix-to-prefix training using a deterministic policy: the agent starts by reading a specified number of source tokens (k), followed by alternating WRITE and READ actions. Arivazhagan et al. (2019) extended the wait-k framework using an advanced attention mechanism and optimising a differential latency metric (DAL). Recently, Arivazhaga"
2020.emnlp-main.184,W17-4718,1,0.88393,"d as: rtD = α [sgn(Ct − C ∗ ) + 1] + βbDt − D∗ c+ where Ct denotes the CW metric introduced here to avoid long consecutive waits and Dt refers to AP (see § 3.4.1 for metrics). D∗ and C ∗ are hyperparameters that determine the expected/target values for AP and CW, respectively. The optimal quality-latency trade-off is achieved by balancing the two reward terms. 4 Experimental Setup 4.1 Dataset We use the Multi30k dataset (Elliott et al., 2016)5 which has been the primary corpus for MMT research across the three shared tasks of the “Conference on Machine Translation (WMT)” (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Multi30k extends the Flickr30k image captioning dataset (Young et al., 2014) to provide caption translations in German, French and Czech. In this work, we focus on the English→German and English→French (Elliott et al., 2017) language directions (Table 1). We use flickr2016 (2016), flickr2017 (2017) and coco2017 (COCO) for model evaluation. The latter test set is explicitly designed (Elliott et al., 2017) to contain at least 5 Preprocessing. We use Moses scripts (Koehn et al., 2007) to lowercase, punctuation-normalise and tokenise the sentences with hyphen splitting. W"
2020.emnlp-main.184,W16-3210,1,0.891154,"Missing"
2020.emnlp-main.184,I17-1014,0,0.0922802,"Missing"
2020.emnlp-main.184,W18-6439,0,0.0844784,"Missing"
2020.emnlp-main.184,E17-1099,0,0.29126,"ategy to find a good trade-off between the quality of the translation and the latency incurred in producing it. Previous work has considered rulebased strategies that rely on waiting until some constraint is satisfied, which includes approaches based on syntactic constraints (Bub et al., 1997; Ryu et al., 2006), segment/chunk/alignment information (Bangalore et al., 2012) heuristic-based conditions during decoding (Cho and Esipova, 2016) or deterministic policies with pre-determined latency constraints (Ma et al., 2019). An alternative line of research focuses on learning the decision policy: Gu et al. (2017) and Alinejad et al. (2018) frame SiMT as learning to generate READ/WRITE actions and employ reinforcement learning (RL) to formulate the problem as a policy agent interacting with its environment (i.e. a pre-trained MT model). Recent work has also explored supervised learning of the policy, by using oracle action sequences predicted by a pre-trained MT using confidence-based heuristics (Zheng et al., 2019) or external word aligners (Arthur et al., 2020) (details in §2). Thus far, all prior research has focused on unimodal interpretation1 . In this paper, we explore SiMT for multimodal machine"
2020.emnlp-main.184,2020.wmt-1.70,0,0.328621,"ons and employ reinforcement learning (RL) to formulate the problem as a policy agent interacting with its environment (i.e. a pre-trained MT model). Recent work has also explored supervised learning of the policy, by using oracle action sequences predicted by a pre-trained MT using confidence-based heuristics (Zheng et al., 2019) or external word aligners (Arthur et al., 2020) (details in §2). Thus far, all prior research has focused on unimodal interpretation1 . In this paper, we explore SiMT for multimodal machine translation (MMT) (Specia et al., 2016), where in addition to 1 We note that Imankulova et al. (2020) also attempted to explore multimodality in SiMT. However, their paper overestimates the impact of visual cues, and in personal correspondence with the authors about the mismatch in the findings, they discovered critical bugs in their implementation. 2350 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2350–2361, c November 16–20, 2020. 2020 Association for Computational Linguistics the source sentence, we have access to visual information in the form of an image. We believe that having access to a complementary context should help the models antic"
2020.emnlp-main.184,P02-1040,0,0.106801,"nd mini-batch size to 0.0004 and 6, respectively. For each sentence pair in a batch, ten trajectories are sampled. For inference, greedy sampling is used to pick action sequences. We set the hyperparameters C ∗ =2, D∗ =0.3, α=0.025 and β= − 1. To encourage exploration, the negative entropy policy term is weighed empirically with 0.1 and 0.3 for En→Fr and En→De directions, respectively. Training. We use nmtpytorch (Caglayan et al., 2017b) with PyTorch (Paszke et al., 2019) v1.4 for our experiments7 . We train each model for a maximum of 50 epochs and early stop the training if validation BLEU (Papineni et al., 2002) does not improve for 10 epochs. We also halve the learning rate if no improvement is obtained for two epochs. On a single NVIDIA RTX2080-Ti GPU, it takes around 35 minutes for the unimodal and multimodal encoder variants to complete train6 7 https://github.com/multi30k/dataset 2354 https://github.com/nyu-dl/dl4mt-simul-trans https://github.com/ImperialNLP/pysimt English→German 2016 2017 COCO English→French 2016 2017 COCO NMT 34.6 26.4 22.1 57.8 50.3 41.4 – ENC-OD ⇓ 0.6 ⇑ 0.3 ⇑ 0.8 ⇑ 0.3 ⇑ 0.1 ⇑ 1.1 ⇑ 0.33 DEC-OC ⇑ 0.4 ⇑ 0.8 ⇑ 0.7 ⇓ 0.3 ⇑ 0.2 ⇑ 0.7 ⇑ 0.52 DEC-OD ⇑ 0.7 ⇑ 1.0 ⇑ 1.7 ⇑ 0.1 ⇑ 0.6 ⇑"
2020.emnlp-main.184,P07-2045,0,0.00695164,"ee shared tasks of the “Conference on Machine Translation (WMT)” (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Multi30k extends the Flickr30k image captioning dataset (Young et al., 2014) to provide caption translations in German, French and Czech. In this work, we focus on the English→German and English→French (Elliott et al., 2017) language directions (Table 1). We use flickr2016 (2016), flickr2017 (2017) and coco2017 (COCO) for model evaluation. The latter test set is explicitly designed (Elliott et al., 2017) to contain at least 5 Preprocessing. We use Moses scripts (Koehn et al., 2007) to lowercase, punctuation-normalise and tokenise the sentences with hyphen splitting. We then create word vocabularies on the training subset of the dataset. We did not use subword segmentation to avoid its potential side effects on SiMT and to be able to analyse the grounding capability of the models better. The resulting English, French and German vocabularies contain 9.8K, 11K and 18K tokens, respectively. 4.2 Reproducibility Hyperparameters. The dimensions of embeddings and GRU hidden states are set to 200 and 320, respectively. The decoder’s input and output embeddings are shared (Press"
2020.emnlp-main.184,W18-6319,0,0.0164835,") on the validation set with patience set to ten epochs. The number of learnable parameters is around 6M. 4.3 5.1 2 3 4 ... 13.1 1 WAIT1 WAIT2 2 3 4 ... 11.4 RL Consecutive WAIT3 Figure 2: AL vs BLEU comparison across unimodal SiMT approaches: wait-k systems are “trained”. Evaluation To mitigate variance in results due to different initialisations, we repeat each experiment three times, with random seeds. Following previous work, we decode translations with greedy search, using the checkpoint that achieved the lowest perplexity. We report average BLEU scores across three runs using sacreBLEU (Post, 2018), which is also used for computing sentence-level scores for the oracle experiments. 5 1 Results Consecutive baselines We first present the impact of the visual integration approaches on consecutive NMT performance (Table 2). We observe that the decoder-attention using object detection features (DEC-OD) performs better than other variants. We also see that the improvements on flickr2017 (⇑ 0.5) and coco2017 (⇑ 1.03) test sets are higher than flickr2016 (⇑ 0.1) on average. A possible explanation is that flickr2017 and coco2017 are more distant from the training set distribution (higher OOV coun"
2020.emnlp-main.184,P17-2031,0,0.121743,"Missing"
2020.emnlp-main.184,E17-2025,0,0.0166106,"2007) to lowercase, punctuation-normalise and tokenise the sentences with hyphen splitting. We then create word vocabularies on the training subset of the dataset. We did not use subword segmentation to avoid its potential side effects on SiMT and to be able to analyse the grounding capability of the models better. The resulting English, French and German vocabularies contain 9.8K, 11K and 18K tokens, respectively. 4.2 Reproducibility Hyperparameters. The dimensions of embeddings and GRU hidden states are set to 200 and 320, respectively. The decoder’s input and output embeddings are shared (Press and Wolf, 2017). We use ADAM (Kingma and Ba, 2014) as the optimiser and set the learning rate and mini-batch size to 0.0004 and 64, respectively. A weight decay of 1e−5 is applied for regularisation. We clip the gradients if the norm of the full parameter vector exceeds 1 (Pascanu et al., 2013). For the RL baseline, we closely follow (Gu et al., 2017)6 . The agent is implemented by a 320-dimensional GRU followed by a softmax layer and the baseline network – used for variance reduction of policy gradient – is similar to the agent except with a scalar output layer. We use ADAM as the optimiser and set the lear"
2020.emnlp-main.184,P06-2088,0,0.396269,"e between how much context is needed to generate the translation reliably, and how long the listener has to wait for the translation. In contrast to consecutive machine translation where source sentences are available in their entirety before translation, the challenge in SiMT is thus the design of a strategy to find a good trade-off between the quality of the translation and the latency incurred in producing it. Previous work has considered rulebased strategies that rely on waiting until some constraint is satisfied, which includes approaches based on syntactic constraints (Bub et al., 1997; Ryu et al., 2006), segment/chunk/alignment information (Bangalore et al., 2012) heuristic-based conditions during decoding (Cho and Esipova, 2016) or deterministic policies with pre-determined latency constraints (Ma et al., 2019). An alternative line of research focuses on learning the decision policy: Gu et al. (2017) and Alinejad et al. (2018) frame SiMT as learning to generate READ/WRITE actions and employ reinforcement learning (RL) to formulate the problem as a policy agent interacting with its environment (i.e. a pre-trained MT model). Recent work has also explored supervised learning of the policy, by"
2020.emnlp-main.184,E17-3017,0,0.0321164,"Missing"
2020.emnlp-main.184,W16-2346,1,0.910315,"Missing"
2020.emnlp-main.184,Q14-1006,0,0.0477757,"id long consecutive waits and Dt refers to AP (see § 3.4.1 for metrics). D∗ and C ∗ are hyperparameters that determine the expected/target values for AP and CW, respectively. The optimal quality-latency trade-off is achieved by balancing the two reward terms. 4 Experimental Setup 4.1 Dataset We use the Multi30k dataset (Elliott et al., 2016)5 which has been the primary corpus for MMT research across the three shared tasks of the “Conference on Machine Translation (WMT)” (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Multi30k extends the Flickr30k image captioning dataset (Young et al., 2014) to provide caption translations in German, French and Czech. In this work, we focus on the English→German and English→French (Elliott et al., 2017) language directions (Table 1). We use flickr2016 (2016), flickr2017 (2017) and coco2017 (COCO) for model evaluation. The latter test set is explicitly designed (Elliott et al., 2017) to contain at least 5 Preprocessing. We use Moses scripts (Koehn et al., 2007) to lowercase, punctuation-normalise and tokenise the sentences with hyphen splitting. We then create word vocabularies on the training subset of the dataset. We did not use subword segmenta"
2020.emnlp-main.184,D19-1137,0,0.513174,"g decoding (Cho and Esipova, 2016) or deterministic policies with pre-determined latency constraints (Ma et al., 2019). An alternative line of research focuses on learning the decision policy: Gu et al. (2017) and Alinejad et al. (2018) frame SiMT as learning to generate READ/WRITE actions and employ reinforcement learning (RL) to formulate the problem as a policy agent interacting with its environment (i.e. a pre-trained MT model). Recent work has also explored supervised learning of the policy, by using oracle action sequences predicted by a pre-trained MT using confidence-based heuristics (Zheng et al., 2019) or external word aligners (Arthur et al., 2020) (details in §2). Thus far, all prior research has focused on unimodal interpretation1 . In this paper, we explore SiMT for multimodal machine translation (MMT) (Specia et al., 2016), where in addition to 1 We note that Imankulova et al. (2020) also attempted to explore multimodality in SiMT. However, their paper overestimates the impact of visual cues, and in personal correspondence with the authors about the mismatch in the findings, they discovered critical bugs in their implementation. 2350 Proceedings of the 2020 Conference on Empirical Meth"
2020.emnlp-main.24,W17-5221,0,0.060233,"Missing"
2020.emnlp-main.24,D18-1216,0,0.0206203,"ng. If the weaknesses of the training datasets or the models are anticipated, strategies can be tailored to mitigate such weaknesses. For example, augmenting the training data with genderswapped input texts helps reduce gender bias in the models (Park et al., 2018; Zhao et al., 2018). Adversarial training can prevent the models from exploiting irrelevant and/or protected features (Jaiswal et al., 2019; Zhang et al., 2018). With a limited number of training examples, using human rationales or prior knowledge together with training labels can help the models perform better (Zaidan et al., 2007; Bao et al., 2018; Liu and Avci, 2019). Nonetheless, there are side-effects of suboptimal datasets that cannot be predicted and are only found after training thanks to post-hoc error analysis. To rectify such problems, there have been attempts to enable humans to fix the trained models (i.e., to perform model debugging) (Stumpf et al., 2009; Teso and Kersting, 2019). Since the models are usually too complex to understand, manually modifying the model parameters is not possible. Existing techniques, therefore, allow humans to provide feedback on individual predictions instead. Then, additional training examples"
2020.emnlp-main.24,2020.acl-main.673,0,0.0280999,"ugging. For instance, BERT-base (Devlin et al., 2019) has 768 features (before the final dense layer) which require lots of human effort to perform investigation. In this case, it would be more efficient to use FIND to disable attention heads rather than individual features (Voita et al., 2019). Third, it is possible that one feature detects several patterns (Jacovi et al., 2018) and it will be difficult to disable the feature if some of the detected patterns are useful while the others are harmful. Hence, FIND would be more effective when used together with disentangled text representations (Cheng et al., 2020). Acknowledgments We would like to thank Nontawat Charoenphakdee and anonymous reviewers for helpful comments. Also, the first author wishes to thank the support from Anandamahidol Foundation, Thailand. See Appendix F for the full results from all experiments. 340 References Maximilian Alber, Sebastian Lapuschkin, Philipp Seegerer, Miriam H¨agele, Kristof T Sch¨utt, Gr´egoire Montavon, Wojciech Samek, Klaus-Robert M¨uller, Sven D¨ahne, and Pieter-Jan Kindermans. 2018. innvestigate neural networks! arXiv preprint arXiv:1808.04260. Leila Arras, Franziska Horn, Gr´egoire Montavon, Klaus-Robert M¨"
2020.emnlp-main.24,W17-3013,0,0.0578588,"Missing"
2020.emnlp-main.24,W14-3333,0,0.0701783,"Missing"
2020.emnlp-main.24,N18-2017,0,0.0502388,"Missing"
2020.emnlp-main.24,2020.acl-demos.22,0,0.0349537,"s in LSTM cells (instead of tanh) renders the features non-negative. So, they can be summarized using one word cloud which is more practical for debugging. In general, the principle of FIND is understanding the features and then disabling the irrelevant ones. The process makes visualizations and interpretability more actionable. Over the past few years, we have seen rapid growth of 6 scientific research in both topics (visualizations and interpretability) aiming to understand many emerging advanced models including the popular transformer-based models (Jo and Myaeng, 2020; Voita et al., 2019; Hoover et al., 2020). We believe that our work will inspire other researchers to foster advances in both topics towards the more tangible goal of model debugging. Limitations Nevertheless, FIND has some limitations. First, the word clouds may reveal sensitive contents in the training data to human debuggers. Second, the more hidden features the model has, the more human effort FIND needs for debugging. For instance, BERT-base (Devlin et al., 2019) has 768 features (before the final dense layer) which require lots of human effort to perform investigation. In this case, it would be more efficient to use FIND to dis"
2020.emnlp-main.24,W18-5408,0,0.199831,"deep NLP models – There has been substantial work in gaining better understanding of complex, deep neural NLP models. By visualizing dense hidden vectors, Li et al. (2016) found that some dimensions of the final representation learned by recurrent neural networks capture the effect of intensification and negation in the input text. Karpathy et al. (2015) revealed the existence of interpretable cells in a character-level LSTM model for language modelling. For example, they found a cell acting as a line length counter and cells checking if the current letter is inside a parenthesis or a quote. Jacovi et al. (2018) presented interesting findings about CNNs for text classification including the fact that one convolutional filter may detect more than one n-gram pattern and may also suppress negative n-grams. Many recent papers studied several types of knowledge in BERT (Devlin et al., 2019), a deep transformer-based model for language understanding, and found that syntactic information is mostly captured in the middle BERT layers while the final BERT layers are the most task-specific (Rogers et al., 2020). Inspired by many findings, we make the assumption that each dimension of the final representation (i"
2020.emnlp-main.24,D14-1181,0,0.00479571,"uring testing. The main differences between our work and existing work are: (i) first, FIND leverages human feedback on the model components, not the individual predictions, to perform debugging; (ii) second, FIND targets deep text classifiers which are more convoluted than traditional classifiers used in existing work (such as Naive Bayes classifiers and Support Vector Machines). We conducted three human experiments (one feasibility study and two debugging experiments) to demonstrate the usefulness of FIND. For all the experiments, we used as classifiers convolutional neural networks (CNNs) (Kim, 2014), which are a popular, well-performing architecture for many text classification tasks including the tasks we experimented with (Gamb¨ack and Sikdar, 2017; Johnson and Zhang, 2015; Zhang et al., 2019). The overall results show that FIND with human-in-the-loop can improve the text classifiers and mitigate the said problems in the datasets. After the experiments, we discuss the generalization of the proposed framework to other tasks and models. Overall, the main contributions of this paper are: • We propose using word clouds as visual explanations of the features learned. • We propose a techniqu"
2020.emnlp-main.24,D18-1302,0,0.0656345,"2019; Gururangan et al., 2018). These can lead to suboptimal models with undesirable properties. For example, the models may have biases against some sub-populations or may not work effectively in the wild as they overfit the imperfect training data. To improve the models, previous work has looked into different techniques beyond standard model fitting. If the weaknesses of the training datasets or the models are anticipated, strategies can be tailored to mitigate such weaknesses. For example, augmenting the training data with genderswapped input texts helps reduce gender bias in the models (Park et al., 2018; Zhao et al., 2018). Adversarial training can prevent the models from exploiting irrelevant and/or protected features (Jaiswal et al., 2019; Zhang et al., 2018). With a limited number of training examples, using human rationales or prior knowledge together with training labels can help the models perform better (Zaidan et al., 2007; Bao et al., 2018; Liu and Avci, 2019). Nonetheless, there are side-effects of suboptimal datasets that cannot be predicted and are only found after training thanks to post-hoc error analysis. To rectify such problems, there have been attempts to enable humans to f"
2020.emnlp-main.24,D14-1162,0,0.0841323,"lowing sections. For each classification task, we ran and improved three models, using different random seeds, independently of one another, and the reported results are the average of the three runs. Regarding the models, we used 1D CNNs with the same structures for all the tasks and datasets. The convolution layer had three filter sizes [2, 3, 4] with 10 filters for each size (i.e., d = 10 × 3 = 30). All the activation functions were ReLU except the softmax at the output layer. The input documents were padded or trimmed to have 150 words (L = 150). We used pre-trained 300-dim GloVe vectors (Pennington et al., 2014) as non-trainable weights in the embedding layers. All the models were implemented using Keras and trained with Adam optimizer. We used iNNvestigate (Alber et al., 2018) to run LRP on CNN features. In particular, we used the LRP- propagation rule to stabilize the relevance scores ( = 10−7 ). Finally, we used Amazon Mechanical Turk (MTurk) to collect crowdsourced responses for selecting features to disable. Each question was answered by ten workers and the answers were aggregated using majority votes or average scores depending on the question type (as explained next). 5 Exp 1: Feasibility St"
2020.emnlp-main.24,D16-1011,0,0.0550111,"Missing"
2020.emnlp-main.24,D19-1523,1,0.844074,"Missing"
2020.emnlp-main.24,N16-1082,0,0.0167728,"Section 2 explains related work about analyzing, explaining, and human-debugging text classifiers. Section 3 proposes FIND, our debugging framework. Section 4 explains the experimental setup followed by the three human experiments in Section 5 to 7. Finally, Section 8 discusses generalization of the framework and concludes the paper. Code and datasets of this paper are available at https://github.com/plkumjorn/FIND. 2 Related Work Analyzing deep NLP models – There has been substantial work in gaining better understanding of complex, deep neural NLP models. By visualizing dense hidden vectors, Li et al. (2016) found that some dimensions of the final representation learned by recurrent neural networks capture the effect of intensification and negation in the input text. Karpathy et al. (2015) revealed the existence of interpretable cells in a character-level LSTM model for language modelling. For example, they found a cell acting as a line length counter and cells checking if the current letter is inside a parenthesis or a quote. Jacovi et al. (2018) presented interesting findings about CNNs for text classification including the fact that one convolutional filter may detect more than one n-gram patt"
2020.emnlp-main.24,P19-1631,0,0.126411,"ses of the training datasets or the models are anticipated, strategies can be tailored to mitigate such weaknesses. For example, augmenting the training data with genderswapped input texts helps reduce gender bias in the models (Park et al., 2018; Zhao et al., 2018). Adversarial training can prevent the models from exploiting irrelevant and/or protected features (Jaiswal et al., 2019; Zhang et al., 2018). With a limited number of training examples, using human rationales or prior knowledge together with training labels can help the models perform better (Zaidan et al., 2007; Bao et al., 2018; Liu and Avci, 2019). Nonetheless, there are side-effects of suboptimal datasets that cannot be predicted and are only found after training thanks to post-hoc error analysis. To rectify such problems, there have been attempts to enable humans to fix the trained models (i.e., to perform model debugging) (Stumpf et al., 2009; Teso and Kersting, 2019). Since the models are usually too complex to understand, manually modifying the model parameters is not possible. Existing techniques, therefore, allow humans to provide feedback on individual predictions instead. Then, additional training examples are created based on"
2020.emnlp-main.24,N16-3020,0,0.483438,"ication. Therefore, understanding the roles of these dimensions (we refer to them as features) is a prerequisite for effective human-in-the-loop model debugging, and we exploit an explanation method to gain such an understanding. Explaining predictions from text classifiers – Several methods have been devised to generate explanations supporting classifications in many forms, such as natural language texts (Liu et al., 2019), rules (Ribeiro et al., 2018), extracted rationales (Lei et al., 2016), and attribution scores (Lertvittayakumjorn and Toni, 2019). Some explanation methods, such as LIME (Ribeiro et al., 2016) and SHAP (Lundberg and Lee, 2017), are model-agnostic and do not require access to model parameters. Other methods access the model architectures and parameters to generate the explanations, such as DeepLIFT (Shrikumar et al., 2017) and LRP (layer-wise relevance propagation) (Bach et al., 2015; Arras et al., 2016). In this work, we use LRP to explain not the predictions but the learned features so as to expose the model behavior to humans and enable informed model debugging. 333 Debugging text classifiers using human feedback – Early work in this area comes from the human-computer interaction"
2020.emnlp-main.24,2020.tacl-1.54,0,0.0172221,"as a line length counter and cells checking if the current letter is inside a parenthesis or a quote. Jacovi et al. (2018) presented interesting findings about CNNs for text classification including the fact that one convolutional filter may detect more than one n-gram pattern and may also suppress negative n-grams. Many recent papers studied several types of knowledge in BERT (Devlin et al., 2019), a deep transformer-based model for language understanding, and found that syntactic information is mostly captured in the middle BERT layers while the final BERT layers are the most task-specific (Rogers et al., 2020). Inspired by many findings, we make the assumption that each dimension of the final representation (i.e., the vector before the output layer) captures patterns or qualities in the input which are useful for classification. Therefore, understanding the roles of these dimensions (we refer to them as features) is a prerequisite for effective human-in-the-loop model debugging, and we exploit an explanation method to gain such an understanding. Explaining predictions from text classifiers – Several methods have been devised to generate explanations supporting classifications in many forms, such as"
2020.emnlp-main.24,D11-1136,0,0.0225539,"mpf et al. (2009) studied the types of feedback humans usually give in response to machine-generated predictions and explanations. Also, some of the feedback collected (i.e., important words of each category) was used to improve the classifier via a user co-training approach. Kulesza et al. (2015) presented an explanatory debugging approach in which the system explains to users how it made each prediction, and the users then rectify the model by adding/removing words from the explanation and adjusting important weights. Even without explanations shown, an active learning framework proposed by Settles (2011) asks humans to iteratively label some chosen features (i.e., words) and adjusts the model parameters that correspond to the features. However, these early works target simpler machine learning classifiers (e.g., Naive Bayes classifiers with bag-of-words) and it is not clear how to apply the proposed approaches to deep text classifiers. Recently, there have been new attempts to use explanations and human feedback to debug classifiers in general. Some of them were tested on traditional text classifiers. For instance, Ribeiro et al. (2016) showed a set of LIME explanations for individual SVM pre"
2020.emnlp-main.24,N19-1108,1,0.83395,"ing; (ii) second, FIND targets deep text classifiers which are more convoluted than traditional classifiers used in existing work (such as Naive Bayes classifiers and Support Vector Machines). We conducted three human experiments (one feasibility study and two debugging experiments) to demonstrate the usefulness of FIND. For all the experiments, we used as classifiers convolutional neural networks (CNNs) (Kim, 2014), which are a popular, well-performing architecture for many text classification tasks including the tasks we experimented with (Gamb¨ack and Sikdar, 2017; Johnson and Zhang, 2015; Zhang et al., 2019). The overall results show that FIND with human-in-the-loop can improve the text classifiers and mitigate the said problems in the datasets. After the experiments, we discuss the generalization of the proposed framework to other tasks and models. Overall, the main contributions of this paper are: • We propose using word clouds as visual explanations of the features learned. • We propose a technique to disable the learned features which are irrelevant or harmful to the classification task so as to improve the classifier. This technique and the word clouds form the human-debugging framework – FI"
2020.emnlp-main.24,P19-1580,0,0.0262781,"activation functions in LSTM cells (instead of tanh) renders the features non-negative. So, they can be summarized using one word cloud which is more practical for debugging. In general, the principle of FIND is understanding the features and then disabling the irrelevant ones. The process makes visualizations and interpretability more actionable. Over the past few years, we have seen rapid growth of 6 scientific research in both topics (visualizations and interpretability) aiming to understand many emerging advanced models including the popular transformer-based models (Jo and Myaeng, 2020; Voita et al., 2019; Hoover et al., 2020). We believe that our work will inspire other researchers to foster advances in both topics towards the more tangible goal of model debugging. Limitations Nevertheless, FIND has some limitations. First, the word clouds may reveal sensitive contents in the training data to human debuggers. Second, the more hidden features the model has, the more human effort FIND needs for debugging. For instance, BERT-base (Devlin et al., 2019) has 768 features (before the final dense layer) which require lots of human effort to perform investigation. In this case, it would be more effici"
2020.emnlp-main.24,N18-2003,0,0.0227745,"t al., 2018). These can lead to suboptimal models with undesirable properties. For example, the models may have biases against some sub-populations or may not work effectively in the wild as they overfit the imperfect training data. To improve the models, previous work has looked into different techniques beyond standard model fitting. If the weaknesses of the training datasets or the models are anticipated, strategies can be tailored to mitigate such weaknesses. For example, augmenting the training data with genderswapped input texts helps reduce gender bias in the models (Park et al., 2018; Zhao et al., 2018). Adversarial training can prevent the models from exploiting irrelevant and/or protected features (Jaiswal et al., 2019; Zhang et al., 2018). With a limited number of training examples, using human rationales or prior knowledge together with training labels can help the models perform better (Zaidan et al., 2007; Bao et al., 2018; Liu and Avci, 2019). Nonetheless, there are side-effects of suboptimal datasets that cannot be predicted and are only found after training thanks to post-hoc error analysis. To rectify such problems, there have been attempts to enable humans to fix the trained model"
2020.emnlp-main.24,N16-2013,0,0.0351624,"tions, we firmly mentioned in the instructions that gender-related terms should not be used as an indicator for one or the other class. Exp 2: Training Data with Biases Given a biased training dataset, a text classifier may absorb the biases and produce biased predictions against some sub-populations. We hypothesize that if the biases are captured by some of the learned features, we can apply FIND to disable such features and reduce the model biases. 6.1 Datasets and Metrics We focus on reducing gender bias of CNN models trained on two datasets – Biosbias (De-Arteaga et al., 2019) and Waseem (Waseem and Hovy, 2016). For Biosbias, the task is predicting the occupation of a given bio paragraph, i.e., whether the person is ‘a surgeon’ (class 0) or ‘a nurse’ (class 1). Due to the gender imbalance in each occupation, a classifier usually exploits gender information when making predictions. As a result, bios of female surgeons and male nurses are often misclassified. For Waseem, the task is abusive language detection – assessing if a given text is abusive (class 1) or not abusive (class 0). Previous work found that this dataset contains a strong negative bias against females (Park et al., 2018). In other word"
2020.emnlp-main.24,N19-1060,0,0.0196984,"ed under different types of imperfect datasets (including datasets with biases and datasets with dissimilar traintest distributions). 1 Introduction Deep learning has become the dominant approach to address most Natural Language Processing (NLP) tasks, including text classification. With sufficient and high-quality training data, deep learning models can perform incredibly well (Zhang et al., 2015; Wang et al., 2019). However, in real-world cases, such ideal datasets are scarce. Often times, the available datasets are small, full of regular but irrelevant words, and contain unintended biases (Wiegand et al., 2019; Gururangan et al., 2018). These can lead to suboptimal models with undesirable properties. For example, the models may have biases against some sub-populations or may not work effectively in the wild as they overfit the imperfect training data. To improve the models, previous work has looked into different techniques beyond standard model fitting. If the weaknesses of the training datasets or the models are anticipated, strategies can be tailored to mitigate such weaknesses. For example, augmenting the training data with genderswapped input texts helps reduce gender bias in the models (Park"
2020.lrec-1.455,E17-1050,0,0.0181637,"on coefficient is used as the primary evaluation metric for the scoring task (with Mean Absolute Error – MAE – as the secondary metric). lang r MAE EN–NL EN–FR EN–PT 0.38 0.58 0.38 0.14 0.14 0.08 Table 5: Pearson’s r correlation coefficient and MAE scores for the sentence-level QE task measured for the internal test set (536 sentences). The datasets described in this paper open new avenues for research in QE, particularly in this challenging setting where a large proportion of the translations require no edits. 5.2. Automatic Post-Editing Automatic Post-Editing (APE) (Simard and Foster, 2013; Chatterjee et al., 2017) seeks to reduce the burden of human post-editors and automatically corrects errors in MT outputs. APE is usually performed by monolingual translation models that “translate” from the raw MT to PE. Inputs to those systems are source sentences and raw MT that is expected to be corrected in the output. Given the high quality of current NMT outputs, the task has become particularly challenging. This increases the chance of APE systems to overfit or overcorrect new inputs at test time. copycat APE Systems For our APE task we apply the recently introduced copycat networks (Ive et al., 2019), a Tran"
2020.lrec-1.455,W19-5402,0,0.0911165,"and Lavie, 2014) the resulting figures indicate rather low translation quality. Existing PE datasets, e.g., the Autodesk dataset (Zhechev, 2012), are in their majority generated by editing the output of statistical MT systems and are no longer useful for NMT systems as the errors and nature of required corrections are different. The PE NMT datasets used in the annual WMT shared APE and QE tasks are created using the previous generation of NMT systems (mostly RNNbased, which exhibit inferior quality to the current systems) and/or cover only the IT or life sciences domains (Specia et al., 2017; Chatterjee et al., 2019). We propose a PE dataset built using translations from SOTA neural architectures and for a new domain. Finally, the language pairs we propose are also new (current NMT PE datasets propose English-German, English-Latvian, English-Italian and English-Russian translations). In the remainder of this paper we first describe our data sources (Section 2) and the MT systems built (Section 3) to translate this data. We present the PE process and its results 3692 and qualitative analysis in Section 4. Section 5 demonstrates two use cases of the dataset. 2. Post-Editing Data The APE-QUEST project aims t"
2020.lrec-1.455,W14-3348,0,0.0342593,"sists of around 31K tuples including an English source sentence, the respective machine translation by an NMT system (into Dutch, French and Portuguese), a post-edited version of such translation by professional translators, and an independently created reference translation (31,403 cases all together). Interestingly, our English-Dutch, English-French and EnglishPortuguese machine translations require very few postedits – as expected. However, when compared to independently created human references using standard metrics like BLEU (Papineni et al., 2002), TER (Snover et al., 2009) and METEOR (Denkowski and Lavie, 2014) the resulting figures indicate rather low translation quality. Existing PE datasets, e.g., the Autodesk dataset (Zhechev, 2012), are in their majority generated by editing the output of statistical MT systems and are no longer useful for NMT systems as the errors and nature of required corrections are different. The PE NMT datasets used in the annual WMT shared APE and QE tasks are created using the previous generation of NMT systems (mostly RNNbased, which exhibit inferior quality to the current systems) and/or cover only the IT or life sciences domains (Specia et al., 2017; Chatterjee et al"
2020.lrec-1.455,P16-2013,1,0.852218,"(NMT) has reached remarkable progress. This has also pushed the boundaries of manual and automatic evaluation procedures relying on human reference translations. For example, recent studies have shown that reference translations are often judged by humans as having lower translation quality than top NMT systems (Hassan et al., 2018), with follow-up studies showing that this is partly due to the limited quality of the human translations (Toral et al., 2018). Even though problems with independently collected (single) human reference translations for evaluation have been highlighted in the past (Fomicheva and Specia, 2016), this practice is more questionable with high quality NMT systems. In these conditions, metrics based on human postediting of the machine translations become particularly important. This feedback can be used to assess MT quality directly, as well as to build and benchmark metrics for the automatic evaluation of Machine Translation (MT) output, and to build Quality Estimation (QE) and Automatic PE (APE) models. We describe a machine translation dataset in the legal domain resulting from activities performed in the framework of the APE-QUEST project (http://ape-quest.eu). This project aims to i"
2020.lrec-1.455,D19-1318,1,0.812982,"Chatterjee et al., 2017) seeks to reduce the burden of human post-editors and automatically corrects errors in MT outputs. APE is usually performed by monolingual translation models that “translate” from the raw MT to PE. Inputs to those systems are source sentences and raw MT that is expected to be corrected in the output. Given the high quality of current NMT outputs, the task has become particularly challenging. This increases the chance of APE systems to overfit or overcorrect new inputs at test time. copycat APE Systems For our APE task we apply the recently introduced copycat networks (Ive et al., 2019), a Transformer-based pointer network framework. In the dual-source setting, the network can generate new words or copy words from either the source language or the original machine translation. The network has been shown to be rather conservative and make very few few corrections to good quality raw MT – as a result of learning predominantly to copy. We follow the procedure in (Ive et al., 2019) and train a dual-source with double attention copycat model. Again following Ive et al. (2019), we mimic MT data using 500K from general in-domain corpora available at OPUS13 and pre-train our models"
2020.lrec-1.455,P19-3020,0,0.0498622,"Missing"
2020.lrec-1.455,W17-4763,0,0.02357,"a score for unseen MT units. Very often this score is HTER like in WMT QE shared tasks (Fonseca et al., 2019). Predictions for various types of units are possible: documents, paragraphs, sentences, words and phrases, with sentence-level predictions being the most common. OpenKiwi QE Systems Our models utilise OpenKiwi (Kepler et al., 2019), an open-source framework for QE that implements a range of QE systems from the WMT 201518 shared tasks. We extend it to leverage recently proposed pre-trained models via transfer learning techniques. We follow OpenKiwi’s Predictor-Estimator implementation (Kim et al., 2017). Predictor-Estimator is a modular architecture that revolves around an encoder-decoder architecture (so-called Predictor), stacked with a bidirectional RNN (so called Estimator) that is trained to produce quality estimates. It predicts quality using the weights assigned by the Predictor to the words we seek to evaluate, which are concatenated with the representations of their left and right one-word contexts, and then used to feed the Estimator. The training data for the Predictor is the same as the respective NMT systems, while the Estimator is trained on labelled PE data. To build the model"
2020.lrec-1.455,P07-2045,0,0.0225909,"very 1, 000 updates. The usage of the RNN architecture promotes the diversity of SOTA outputs in our dataset. 4. Post-Editing Process Post-editing was done by professional translators, each being assigned a different portion of the data. They were asked to only correct actual errors and refrain from making stylistic improvements. The resulting data triplets comprise 10-11k sentences for each language pair. The statistics summarising the post-editing datasets are reported in Table 2. For the computation of statistics and automatic scores, the data was tokenised using the Moses toolkit scripts (Koehn et al., 2007). Table 3 measures the edit distance (HTER), HBLEU and HMETEOR between MT and PE, and TER, BLEU and METEOR between MT and the independent reference (REF). HTER is defined as the minimum number of edits (substitution, insertion, deletion and shift) required to change an MT hypothesis so that it exactly matches a human post-edition of this hypothesis. HBLEU measures n-gram precision between MT hypotheses and post-edits, whereas HMETEOR – unigram precision and recall. The human-targeted HTER/HBLEU/HMETEOR (as compared to TER/BLEU/METEOR) variants are measured using actual post-edited MT rather th"
2020.lrec-1.455,2005.mtsummit-papers.11,0,0.148694,"mark metrics for the automatic evaluation of Machine Translation (MT) output, and to build Quality Estimation (QE) and Automatic PE (APE) models. We describe a machine translation dataset in the legal domain resulting from activities performed in the framework of the APE-QUEST project (http://ape-quest.eu). This project aims to integrate MT, QE and APE. The dataset focuses on the areas of online dispute resolution (ODR), procurement and justice. It continues the well-established tradition in MT of using the legal data resulting from the EU procedures that was started with the Europarl corpus (Koehn, 2005). The data consists of around 31K tuples including an English source sentence, the respective machine translation by an NMT system (into Dutch, French and Portuguese), a post-edited version of such translation by professional translators, and an independently created reference translation (31,403 cases all together). Interestingly, our English-Dutch, English-French and EnglishPortuguese machine translations require very few postedits – as expected. However, when compared to independently created human references using standard metrics like BLEU (Papineni et al., 2002), TER (Snover et al., 2009"
2020.lrec-1.455,P02-1040,0,0.108573,"s started with the Europarl corpus (Koehn, 2005). The data consists of around 31K tuples including an English source sentence, the respective machine translation by an NMT system (into Dutch, French and Portuguese), a post-edited version of such translation by professional translators, and an independently created reference translation (31,403 cases all together). Interestingly, our English-Dutch, English-French and EnglishPortuguese machine translations require very few postedits – as expected. However, when compared to independently created human references using standard metrics like BLEU (Papineni et al., 2002), TER (Snover et al., 2009) and METEOR (Denkowski and Lavie, 2014) the resulting figures indicate rather low translation quality. Existing PE datasets, e.g., the Autodesk dataset (Zhechev, 2012), are in their majority generated by editing the output of statistical MT systems and are no longer useful for NMT systems as the errors and nature of required corrections are different. The PE NMT datasets used in the annual WMT shared APE and QE tasks are created using the previous generation of NMT systems (mostly RNNbased, which exhibit inferior quality to the current systems) and/or cover only the"
2020.lrec-1.455,P16-1162,0,0.032457,"EFDIGITAL/eTranslation 3 https://github.com/ESPD/ESPD-Service/tre e/master/espd-web/src/main/resources/i18n 4 https://ec.europa.eu/consumers/odr 5 We took the English-Dutch parts of the Paracrawl Bicleaner v4.0 (https://paracrawl.eu/releases.html) and EUbookshop (http://opus.nlpl.eu/EUbookshop.ph p) corpora. 6 https://e-justice.europa.eu 7 https://github.com/paracrawl/Malign 8 http://mokk.bme.hu/en/resources/hunalign 9 https://github.com/bitextor/bicleaner 10 https://github.com/OpenNMT/OpenNMT-tf 11 https://github.com/tensorflow/tensor2ten sor 12 https://github.com/marian-nmt/marian approach (Sennrich et al., 2016) with 32K merge operations for EN–NL (32K subtokens joint vocabulary), with 90K merge operations for EN–FR (about 70K subtokens joint vocabulary after filtering the least frequent out) and with about 90K merge operations for EN–PT (about 90K subtokens joint vocabulary). We train the EN–NL model for 25 iterations, using the transformer small training parameters, a learning rate of 2.0 and 8K warmup steps. We average the last 8 checkpoints to obtain the final model. For EN–FR, we use the transformer big parameters with a learning rate of 0.05 with 8K warmup steps. For EN–PT, we train an amun RNN"
2020.lrec-1.455,2013.mtsummit-papers.24,0,0.0684687,"Missing"
2020.lrec-1.455,W09-0441,0,0.0179281,"orpus (Koehn, 2005). The data consists of around 31K tuples including an English source sentence, the respective machine translation by an NMT system (into Dutch, French and Portuguese), a post-edited version of such translation by professional translators, and an independently created reference translation (31,403 cases all together). Interestingly, our English-Dutch, English-French and EnglishPortuguese machine translations require very few postedits – as expected. However, when compared to independently created human references using standard metrics like BLEU (Papineni et al., 2002), TER (Snover et al., 2009) and METEOR (Denkowski and Lavie, 2014) the resulting figures indicate rather low translation quality. Existing PE datasets, e.g., the Autodesk dataset (Zhechev, 2012), are in their majority generated by editing the output of statistical MT systems and are no longer useful for NMT systems as the errors and nature of required corrections are different. The PE NMT datasets used in the annual WMT shared APE and QE tasks are created using the previous generation of NMT systems (mostly RNNbased, which exhibit inferior quality to the current systems) and/or cover only the IT or life sciences domains"
2020.lrec-1.455,W18-6312,0,0.0503354,", Post-editing Dataset, Legal Domain, Automatic Post-Editing, Quality Estimation 1. Introduction Current state-of-the-art (SOTA) in Neural Machine Translation (NMT) has reached remarkable progress. This has also pushed the boundaries of manual and automatic evaluation procedures relying on human reference translations. For example, recent studies have shown that reference translations are often judged by humans as having lower translation quality than top NMT systems (Hassan et al., 2018), with follow-up studies showing that this is partly due to the limited quality of the human translations (Toral et al., 2018). Even though problems with independently collected (single) human reference translations for evaluation have been highlighted in the past (Fomicheva and Specia, 2016), this practice is more questionable with high quality NMT systems. In these conditions, metrics based on human postediting of the machine translations become particularly important. This feedback can be used to assess MT quality directly, as well as to build and benchmark metrics for the automatic evaluation of Machine Translation (MT) output, and to build Quality Estimation (QE) and Automatic PE (APE) models. We describe a mach"
2020.lrec-1.455,2012.amta-wptp.10,0,0.229919,"Portuguese), a post-edited version of such translation by professional translators, and an independently created reference translation (31,403 cases all together). Interestingly, our English-Dutch, English-French and EnglishPortuguese machine translations require very few postedits – as expected. However, when compared to independently created human references using standard metrics like BLEU (Papineni et al., 2002), TER (Snover et al., 2009) and METEOR (Denkowski and Lavie, 2014) the resulting figures indicate rather low translation quality. Existing PE datasets, e.g., the Autodesk dataset (Zhechev, 2012), are in their majority generated by editing the output of statistical MT systems and are no longer useful for NMT systems as the errors and nature of required corrections are different. The PE NMT datasets used in the annual WMT shared APE and QE tasks are created using the previous generation of NMT systems (mostly RNNbased, which exhibit inferior quality to the current systems) and/or cover only the IT or life sciences domains (Specia et al., 2017; Chatterjee et al., 2019). We propose a PE dataset built using translations from SOTA neural architectures and for a new domain. Finally, the lan"
2020.ngt-1.19,P07-2045,0,0.0162757,"on of these corpora contains 22.42 million parallel sentence pairs. The STAPLE dataset, which contains 4000 source sentences with 526,466 translations, is used as in-domain data for finetuning. Since in the STAPLE dataset a source sentence have an average number of 131 reference translations, we constructed parallel data by duplicating the source sentence to match the number of translations, as shown in Figure 1. source STAPLE target1 source target1 target2 source target2 target3 source target3 target4 source target4 target5 source target5 Parallel data All sentences are tokenized with Moses (Koehn et al., 2007), and then processed via Byte-PairEncoding (BPE) (Sennrich et al., 2016). A shared vocabulary of 40,000 subwords is constructed for both English and Portuguese. The training data was then cleaned by removing sentence pairs with more than 250 subwords or with length ratio over 1.5, using the clean-corpus-n.perl10 script in Moses. 3.2 4 http://opus.nlpl.eu/ParaCrawl-v5.php http://opus.nlpl.eu/EUbookshop-v2.php 6 http://opus.nlpl.eu/Europarl-v8.php 7 http://opus.nlpl.eu/Wikipedia-v1.0. php 8 http://opus.nlpl.eu/QED-v2.0a.php 9 http://opus.nlpl.eu/Tatoeba-v20190709. php Model and hyperparameters W"
2020.ngt-1.19,2020.ngt-1.28,0,0.0315855,"clara? |0.0878 minha explanac¸a˜ o est´a clara? |0.0572 est´a clara minha explicac¸a˜ o? |0.0443 minha explanac¸a˜ o e´ clara? |0.0392 ... Table 1: An example of weighted translations in the STAPLE dataset for English-Portuguese. Introduction Machine Translation (MT) systems are typically used to produce a single output for a given source sentence, whereas in human translation the same source sentence can often be translated in various different ways while still preserving its meaning. In the 2020 Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE) (Mayhew et al., 2020), participating MT systems are evaluated using multiple reference translations to measure their ability to generate diverse, yet high quality translations. For that, a new dataset with multiple human translations for each source sentence is provided. These human translations were produced by language learners as part of a translation exercise on the Duolingo platform2 where they were asked to translate sentences from the language they were learning (e.g. English) to their native language. Each translation 1 2 https://github.com/Nickeilf/STAPLE20 https://www.duolingo.com In this paper, we exper"
2020.ngt-1.19,N19-4009,0,0.147906,"-5 lexical substitutions from frequencysorted and probability-sorted dictionaries are listed in Table 2. We filtered the substitution dictionary with a stopword list3 and a threshsold (which can be either frequency count or substitution probability), to avoid generating ungrammatical translations. Vijayakumar et al. (2016) proposed the Diverse Beam Search algorithm to improve the diversity of beam hypotheses. The algorithm proceeds by dividing the beam budget into groups and enforcing diversity between groups of beams. In our experiments we use the implementation of this algorithm in fairseq (Ott et al., 2019) with default parameters. 2.4 Ensembling Mixture of Experts Shen et al. (2019) introduced the Mixture of Experts (MoE) framework to capture the inherent uncertainty of the MT task where the same input sen162 Frequency substitution count neste-nesse 5091 ir´a-vai 4920 vou-irei 4645 local-lugar 2989 bem-bastante 2694 Probability substitution baixar-&gt;descarregar descarregar-&gt;baixar situa-se-&gt;fica achasse-&gt;encontrasse localizasse-&gt;achasse prob 1.0 1.0 1.0 1.0 1.0 Table 2: Top-5 lexical substitutions in frequency-sorted and probability-sorted dictionaries. 3 http://snowball.tartarus.org/ algorithms"
2020.ngt-1.19,P16-1162,0,0.0528093,"he STAPLE dataset, which contains 4000 source sentences with 526,466 translations, is used as in-domain data for finetuning. Since in the STAPLE dataset a source sentence have an average number of 131 reference translations, we constructed parallel data by duplicating the source sentence to match the number of translations, as shown in Figure 1. source STAPLE target1 source target1 target2 source target2 target3 source target3 target4 source target4 target5 source target5 Parallel data All sentences are tokenized with Moses (Koehn et al., 2007), and then processed via Byte-PairEncoding (BPE) (Sennrich et al., 2016). A shared vocabulary of 40,000 subwords is constructed for both English and Portuguese. The training data was then cleaned by removing sentence pairs with more than 250 subwords or with length ratio over 1.5, using the clean-corpus-n.perl10 script in Moses. 3.2 4 http://opus.nlpl.eu/ParaCrawl-v5.php http://opus.nlpl.eu/EUbookshop-v2.php 6 http://opus.nlpl.eu/Europarl-v8.php 7 http://opus.nlpl.eu/Wikipedia-v1.0. php 8 http://opus.nlpl.eu/QED-v2.0a.php 9 http://opus.nlpl.eu/Tatoeba-v20190709. php Model and hyperparameters We used the Transformer model (Vaswani et al., 2017) as our baseline mode"
2020.tacl-1.35,D16-1025,0,0.021127,"the advent of neural models, Machine Translation (MT) systems have made substantial progress, reportedly achieving near-human quality for high-resource language pairs (Hassan et al., 2018; Barrault et al., 2019). However, translation quality is not consistent across language pairs, domains, and datasets. This is problematic for low-resource scenarios, where there is not enough training data and translation quality significantly lags behind. Additionally, neural MT (NMT) systems can be deceptive to the end user as they can generate fluent translations that differ in meaning from the original (Bentivogli et al., 2016; Castilho et al., 2017). 539 Transactions of the Association for Computational Linguistics, vol. 8, pp. 539–555, 2020. https://doi.org/10.1162/tacl a 00330 Action Editor: Stefan Riezler. Submission batch: 1/2020; Revision batch: 4/2020; Published 9/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. probability mass to predictions that are far from the training data (Gal and Ghahramani, 2016). To overcome such deficiencies, we propose ways to exploit output distributions beyond the top-1 prediction by exploring uncertainty quantification methods for"
2020.tacl-1.35,2020.eamt-1.16,1,0.787809,"/facebookresearch/mlqe. 545 5 Experiments and Results Below we analyze how our unsupervised QE indicators correlate with human judgments. 5.1 Settings Benchmark Supervised QE Systems We compare the performance of the proposed unsupervised QE indicators against the best performing supervised approaches with available open-source implementation, namely, the Predictor-Estimator (PredEst) architecture (Kim et al., 2017b) provided by OpenKiwi toolkit (Kepler et al., 2019b), and an improved version of the BiRNN model provided by DeepQuest toolkit (Ive et al., 2018), which we refer to as BERT-BiRNN (Blain et al., 2020). PredEst. We trained PredEst models (see §2) using the same parameters as in the default configurations provided by Kepler et al. (2019b). Predictor models were trained for 6 epochs on the same training and development data as the NMT systems, while the Estimator models were trained for 10 epochs on the training and development sets of our dataset (see §4). Unlike Kepler et al. (2019b), the Estimator was not trained using multitask learning, as our dataset currently does not contain any word-level annotation. We use the model corresponding to the best epoch as identified by the metric of refe"
2020.tacl-1.35,C04-1046,0,0.353281,"Missing"
2020.tacl-1.35,W17-4755,0,0.0539918,"Missing"
2020.tacl-1.35,W16-2302,0,0.0478511,"Missing"
2020.tacl-1.35,D19-1308,0,0.0163557,"experiment we focus on Et-En. For each system variant we translated 400 sentences from the test partition of our dataset and collected the DA accordingly. As baseline, we use a standard Transformer model with beam search decoding. All system variants are trained using Fairseq implementation (Ott et al., 2019) for 30 epochs, with the best checkpoint chosen according to the validation loss. First, we consider three system variants with differences in architecture or training: RNN-based NMT (Bahdanau et al., 2015; Luong et al., 2015), Mixture of Experts (MoE, He et al., 2018; Shen et al., 2019; Cho et al., 2019), and model ensemble (Garmash and Monz, 2016). Shen et al. (2019) use the MoE framework to capture the inherent uncertainty of the MT task where the same input sentence can have multiple Method r DA TP-Beam 0.482 58.88 TP-Sampling TP-Diverse beam 0.533 0.424 42.02 55.12 TP-RNN TP-Ensemble TP-MoE 0.502 0.538 0.449 43.63 61.19 51.20 D-TP 0.526 58.88 Table 4: Pearson correlation (r ) between sequence-level output probabilities (TP) and average DA for translations generated by different NMT systems. Figure 3: Pearson correlation between translation quality and model probabilities (orange), and Met"
2020.tacl-1.35,W14-3348,0,0.0203514,"ity outputs on NMT training with back translations. Second, we measure lexical variation between the MT outputs generated for the same source segment when running inference with dropout. We posit that differences between likely MT hypotheses may also capture uncertainty and potential ambiguity and complexity of the original sentence. We compute an average similarity score (sim) between the set H of translation hypotheses: |H ||H| 1 XX D-Lex-Sim = sim(hi , hj ) C i=1 j =1 where hi , hj ∈ H, i 6= j and C = 2−1 |H|(|H |− 1) is the number of pairwise comparisons for |H| hypotheses. We use Meteor (Denkowski and Lavie, 2014) to compute similarity scores. 3.3 Attention Attention weights represent the strength of connection between source and target tokens, which may be indicative of translation quality (Rikters and Fishel, 2017). One way to measure it is to compute the entropy of the attention distribution: I Att-Ent = − J 1 XX αji log αji I i=1 j =1 where α represents attention weights, I is the number of target tokens and J is the number of source tokens. This mechanism can be applied to any NMT model with encoder-decoder attention. We focus on attention in Transformer models, as it is currently the most widely"
2020.tacl-1.35,N19-1423,0,0.0129677,"ration (Guo et al., 2017). On the other hand, due to the small amount of training data the model can overfit, resulting in inferior results both in terms of translation quality and correlation. It is noteworthy, however, that supervised QE system suffers a larger drop in performance than unsupervised indicators, as its 6 We note that PredEst models are systematically and significantly outperformed by BERT-BiRNN. This is not surprising, as large-scale pretrained representations have been shown to boost model performance for QE (Kepler et al., 2019a) and other natural language processing tasks (Devlin et al., 2019). 7 Models for these languages were trained using Transformer-Big architecture from Vaswani et al. (2017). Low-resource Mid-resource High-resource Method Si-En Ne-En Et-En Ro-En En-De En-Zh I TP Softmax-Ent (-) Sent-Std (-) 0.399 0.457 0.418 0.482 0.528 0.472 0.486 0.421 0.471 0.647 0.613 0.595 0.208 0.147 0.264 0.257 0.251 0.301 II D-TP D-Var (-) D-Combo (-) D-Lex-Sim 0.460 0.307 0.286 0.513 0.558 0.299 0.418 0.600 0.642 0.356 0.475 0.612 0.693 0.332 0.383 0.669 0.259 0.164 0.189 0.172 0.321 0.232 0.225 0.313 III AW : Ent-Min (-) AW : Ent-Avg (-) AW : best head/layer (-) 0.097 0.10 0.255 0.26"
2020.tacl-1.35,P18-1069,0,0.0928735,"ed. However, this approach is limited by the fact that there are many possible correct translations for a given sentence and only one human translation is available in practice. Although the goal of this paper is to devise an unsupervised solution for the QE task, the analysis presented here provides new insights into calibration in NMT. Different from existing work, we study the relation between model probabilities and human judgments of translation correctness. Uncertainty quantification methods have been successfully applied to various practical tasks, for example, neural semantic parsing (Dong et al., 2018), hate speech classification (Miok et al., 2019), or back-translation for NMT (Wang et al., 2019). Wang et al. (2019), whose work is the closest to our work, explore a small set of uncertaintybased metrics to minimize the weight of erroneous synthetic sentence pairs for back translation in NMT. However, improved NMT training with weighted synthetic data does not necessarily imply better prediction of MT quality. In fact, metrics that Wang et al. (2019) report to perform the best for back-translation do not perform well for QE (see §3.2). 3.1 Exploiting the Softmax Distribution We start by defi"
2020.tacl-1.35,C16-1133,0,0.0483943,"Missing"
2020.tacl-1.35,C16-1294,0,0.0197172,"me of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the reliability of this metric for assessing the performance of QE systems has been shown to be questionable (Graham et al., 2016). The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2015b), where raters evaluate the MT on a continuous 1–100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015a). DA methodology is currently used for manual evaluation of MT quality at the WMT translation tasks, as well as for assessing the performance of 541 reference-based automatic MT evaluation metrics at the WMT Metrics Task (Bojar et al., 2016, 201"
2020.tacl-1.35,N15-1124,0,0.135368,"translation quality can be extracted from multihead attention. To evaluate our approach in challenging settings, we collect a new dataset for QE with 6 language pairs representing NMT training in high, medium, and low-resource scenarios. To reduce the chance of overfitting to particular domains, our dataset is constructed from Wikipedia documents. We annotate 10K segments per language pair. By contrast to the vast majority of work on QE that uses semi-automatic metrics based on post-editing distance as gold standard, we perform quality labeling based on the Direct Assessment (DA) methodology (Graham et al., 2015b), which has been widely used for popular MT evaluation campaigns in the recent years. At the same time, the collected data differs from the existing datasets annotated with DA judgments for the well known WMT Metrics task1 in two important ways: We provide enough data to train supervised QE models and access to the NMT systems used to generate the translations, thus allowing for further exploration of the glass-box unsupervised approach to QE for NMT introduced in this paper. Our main contributions can be summarized as follows: (i) A new, large-scale dataset for sentencelevel2 QE annotated w"
2020.tacl-1.35,W13-2305,0,0.432265,"tion of high- and low-quality translations for high-resource language pairs, we selected the sentences with minimal lexical overlap with respect to the NMT training data. NMT systems For medium- and high-resource language pairs we trained the MT models based on the standard Transformer architecture (Vaswani et al., 2017) and followed the implementation details described in Ott et al. (2018b). We used publicly available MT datasets such as Paracrawl (Espl`a et al., 2019) and Europarl (Koehn, 2005). 4 DA Judgments We followed the FLORES setup (Guzm´an et al., 2019), which presents a form of DA (Graham et al., 2013). The annotators are asked to rate each sentence from 0–100 according to the perceived translation quality. Specifically, the 0–10 range represents an incorrect translation; 11–29, a translation with few correct keywords, but the overall meaning is different from the source; 30–50, a translation with major mistakes; 51–69, a translation which is understandable and conveys the overall meaning of the source but contains typos or grammatical errors; 70–90, a translation that closely preserves the semantics of the source sentence; and 91–100, a perfect translation. Each segment was evaluated indep"
2020.tacl-1.35,W19-6721,0,0.0372157,"Missing"
2020.tacl-1.35,D19-1632,1,0.899651,"Missing"
2020.tacl-1.35,P19-3020,0,0.0842974,"Missing"
2020.tacl-1.35,W17-4763,0,0.168703,"London 5 Facebook AI 1 {m.fomicheva,f.blain,n.aletras,l.specia}@sheffield.ac.uk 2 {ssun32}@jhu.edu 3 {lisa.yankovskaya,fishel}@ut.ee 5 {fguzman,vishrav}@fb.com Abstract Thus, it is crucial to have a feedback mechanism to inform users about the trustworthiness of a given MT output. Quality estimation (QE) aims to predict the quality of the output provided by an MT system at test time when no gold-standard human translation is available. State-of-the-art (SOTA) QE models require large amounts of parallel data for pretraining and in-domain translations annotated with quality labels for training (Kim et al., 2017a; Fonseca et al., 2019). However, such large collections of data are only available for a small set of languages in limited domains. Current work on QE typically treats the MT system as a black box. In this paper we propose an alternative glass-box approach to QE that allows us to address the task as an unsupervised problem. We posit that encoder-decoder NMT models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) offer a rich source of information for directly estimating translation quality: (a) the output probability distribution from the NMT system (i.e., the probabilit"
2020.tacl-1.35,K18-1056,0,0.0255158,"del ensembling). For this smallscale experiment we focus on Et-En. For each system variant we translated 400 sentences from the test partition of our dataset and collected the DA accordingly. As baseline, we use a standard Transformer model with beam search decoding. All system variants are trained using Fairseq implementation (Ott et al., 2019) for 30 epochs, with the best checkpoint chosen according to the validation loss. First, we consider three system variants with differences in architecture or training: RNN-based NMT (Bahdanau et al., 2015; Luong et al., 2015), Mixture of Experts (MoE, He et al., 2018; Shen et al., 2019; Cho et al., 2019), and model ensemble (Garmash and Monz, 2016). Shen et al. (2019) use the MoE framework to capture the inherent uncertainty of the MT task where the same input sentence can have multiple Method r DA TP-Beam 0.482 58.88 TP-Sampling TP-Diverse beam 0.533 0.424 42.02 55.12 TP-RNN TP-Ensemble TP-MoE 0.502 0.538 0.449 43.63 61.19 51.20 D-TP 0.526 58.88 Table 4: Pearson correlation (r ) between sequence-level output probabilities (TP) and average DA for translations generated by different NMT systems. Figure 3: Pearson correlation between translation quality and"
2020.tacl-1.35,2005.mtsummit-papers.11,0,0.0830211,"nese we selected 20K sentences from the top 100 documents in English Wikipedia. To ensure sufficient representation of high- and low-quality translations for high-resource language pairs, we selected the sentences with minimal lexical overlap with respect to the NMT training data. NMT systems For medium- and high-resource language pairs we trained the MT models based on the standard Transformer architecture (Vaswani et al., 2017) and followed the implementation details described in Ott et al. (2018b). We used publicly available MT datasets such as Paracrawl (Espl`a et al., 2019) and Europarl (Koehn, 2005). 4 DA Judgments We followed the FLORES setup (Guzm´an et al., 2019), which presents a form of DA (Graham et al., 2013). The annotators are asked to rate each sentence from 0–100 according to the perceived translation quality. Specifically, the 0–10 range represents an incorrect translation; 11–29, a translation with few correct keywords, but the overall meaning is different from the source; 30–50, a translation with major mistakes; 51–69, a translation which is understandable and conveys the overall meaning of the source but contains typos or grammatical errors; 70–90, a translation that clos"
2020.tacl-1.35,C18-1266,1,0.840227,"he DA judgments are available at https://github. com/facebookresearch/mlqe. 545 5 Experiments and Results Below we analyze how our unsupervised QE indicators correlate with human judgments. 5.1 Settings Benchmark Supervised QE Systems We compare the performance of the proposed unsupervised QE indicators against the best performing supervised approaches with available open-source implementation, namely, the Predictor-Estimator (PredEst) architecture (Kim et al., 2017b) provided by OpenKiwi toolkit (Kepler et al., 2019b), and an improved version of the BiRNN model provided by DeepQuest toolkit (Ive et al., 2018), which we refer to as BERT-BiRNN (Blain et al., 2020). PredEst. We trained PredEst models (see §2) using the same parameters as in the default configurations provided by Kepler et al. (2019b). Predictor models were trained for 6 epochs on the same training and development data as the NMT systems, while the Estimator models were trained for 10 epochs on the training and development sets of our dataset (see §4). Unlike Kepler et al. (2019b), the Estimator was not trained using multitask learning, as our dataset currently does not contain any word-level annotation. We use the model corresponding"
2020.tacl-1.35,1983.tc-1.13,0,0.671274,"Missing"
2020.tacl-1.35,D15-1166,0,0.133746,"Missing"
2020.tacl-1.35,J82-2005,0,0.698459,"Missing"
2020.tacl-1.35,N19-4009,0,0.237923,"nd the empirical frequencies of the predicted labels, or by assessing generalization of uncertainty under domain shift (see §6). Only a few studies have analyzed calibration in NMT and they came to contradictory conclusions. Kumar and Sarawagi (2019) measure calibration error by comparing model probabilities and the percentage of times NMT output matches reference translation, and conclude that NMT probabilities are poorly calibrated. However, the calibration error metrics they use are designed for binary classification tasks and cannot be easily transferred to NMT (Kuleshov and Liang, 2015). Ott et al. (2019) analyze uncertainty in NMT by comparing predictive probability distributions with the empirical distribution observed in human translation data. They conclude that NMT models ministic NMT (§3.1) or (ii) using uncertainty quantification (§3.2), and (iii) attention weights (§3.3). are well calibrated. However, this approach is limited by the fact that there are many possible correct translations for a given sentence and only one human translation is available in practice. Although the goal of this paper is to devise an unsupervised solution for the QE task, the analysis presented here provides"
2020.tacl-1.35,W18-6450,0,0.0135015,"he current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2015b), where raters evaluate the MT on a continuous 1–100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015a). DA methodology is currently used for manual evaluation of MT quality at the WMT translation tasks, as well as for assessing the performance of 541 reference-based automatic MT evaluation metrics at the WMT Metrics Task (Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Existing datasets with sentence-level DA judgments from the WMT Metrics Task could in principle be used for benchmarking QE systems. However, they contain only a few hundred segments per language pair and thus hardly allow for training supervised systems, as illustrated by the weak correlation results for QE on DA judgments based on the Metrics Task data recently reported by Fonseca et al. (2019). Furthermore, for each language pair the data contains translations from a number of MT systems often using different architectures, and these MT systems are not readily available, making it"
2020.tacl-1.35,W18-6301,0,0.1208,"e sampled documents and then translated them into English using the MT models described below. For German and Chinese we selected 20K sentences from the top 100 documents in English Wikipedia. To ensure sufficient representation of high- and low-quality translations for high-resource language pairs, we selected the sentences with minimal lexical overlap with respect to the NMT training data. NMT systems For medium- and high-resource language pairs we trained the MT models based on the standard Transformer architecture (Vaswani et al., 2017) and followed the implementation details described in Ott et al. (2018b). We used publicly available MT datasets such as Paracrawl (Espl`a et al., 2019) and Europarl (Koehn, 2005). 4 DA Judgments We followed the FLORES setup (Guzm´an et al., 2019), which presents a form of DA (Graham et al., 2013). The annotators are asked to rate each sentence from 0–100 according to the perceived translation quality. Specifically, the 0–10 range represents an incorrect translation; 11–29, a translation with few correct keywords, but the overall meaning is different from the source; 30–50, a translation with major mistakes; 51–69, a translation which is understandable and conve"
2020.tacl-1.35,W19-5302,0,0.0298412,"Missing"
2020.tacl-1.35,W12-3116,0,0.0450527,"Missing"
2020.tacl-1.35,2021.eacl-main.115,1,0.859327,"Missing"
2020.tacl-1.35,W12-3114,0,0.0324498,"system. Existing work on glass-box QE is limited to features extracted from statistical MT, such as language model probabilities or number of hypotheses in the n-best list (Blatz et al., 2004; Specia et al., 2013). The few approaches for unsupervised QE are also inspired by the work on statistical MT 2 While the paper covers QE at sentence level, the extension of our unsupervised metrics to word-level QE would be straightforward and we leave it for future work. 1 h t t p : / /www.statmt.org/wmt19/metricstask.html. 540 and perform significantly worse than supervised approaches (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018). For example, Etchegoyhen et al. (2018) use lexical translation probabilities from word alignment models and language model probabilities. Their unsupervised approach averages these features to produce the final score. However, it is largely outperformed by the neural-based supervised QE systems (Specia et al., 2018). The only works that explore internal information from neural models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based"
2020.tacl-1.35,D15-1182,0,0.0718096,"Missing"
2020.tacl-1.35,W19-8671,0,0.0592008,"Missing"
2020.tacl-1.35,2006.amta-papers.25,0,0.114,"nterpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the reliability of this metric for assessing the performance of QE systems has been shown to be questionable (Graham et al., 2016). The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2015b), where raters evaluate the MT on a continuous 1–100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015a). DA methodology is currently used for manual evaluation of MT quality at the WM"
2020.tacl-1.35,W19-4808,0,0.0153618,"l information from neural models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based indicators perform competitively only when combined with other QE features in a supervised framework. Furthermore, this approach is not directly applicable to the SOTA Transformer model that uses multihead attention mechanism. Recent work on attention interpretability showed that attention weights in Transformer networks might not be readily interpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the re"
2020.tacl-1.35,P13-4014,1,0.932381,"Missing"
2020.tacl-1.35,P19-1580,0,0.162993,"models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based indicators perform competitively only when combined with other QE features in a supervised framework. Furthermore, this approach is not directly applicable to the SOTA Transformer model that uses multihead attention mechanism. Recent work on attention interpretability showed that attention weights in Transformer networks might not be readily interpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the reliability of this met"
2020.tacl-1.35,2009.eamt-1.5,1,0.860291,"t unsupervised QE indicators obtained from well-calibrated NMT model probabilities rival strong supervised SOTA models in terms of correlation with human judgments. 2 Related Work QE QE is typically addressed as a supervised machine learning task where the goal is to predict MT quality in the absence of reference translation. Traditional feature-based approaches relied on manually designed features, extracted from the MT system (glass-box features) or obtained from the source and translated sentences, as well as external resources, such as monolingual or parallel corpora (black-box features) (Specia et al., 2009). Currently, the best performing approaches to QE use NNs to learn useful representations for source and target sentences (Kim et al., 2017b; Wang et al., 2018; Kepler et al., 2019a). A notable example is the Predictor-Estimator (PredEst) model (Kim et al., 2017b), which consists of an encoder-decoder RNN (predictor) trained on parallel data for a word prediction task and a unidirectional RNN (estimator) that produces quality estimates leveraging the context representations generated by the predictor. Despite achieving strong performances, neural-based approaches are resource-heavy and require"
2020.tacl-1.35,W18-6465,0,0.0275911,"s. 2 Related Work QE QE is typically addressed as a supervised machine learning task where the goal is to predict MT quality in the absence of reference translation. Traditional feature-based approaches relied on manually designed features, extracted from the MT system (glass-box features) or obtained from the source and translated sentences, as well as external resources, such as monolingual or parallel corpora (black-box features) (Specia et al., 2009). Currently, the best performing approaches to QE use NNs to learn useful representations for source and target sentences (Kim et al., 2017b; Wang et al., 2018; Kepler et al., 2019a). A notable example is the Predictor-Estimator (PredEst) model (Kim et al., 2017b), which consists of an encoder-decoder RNN (predictor) trained on parallel data for a word prediction task and a unidirectional RNN (estimator) that produces quality estimates leveraging the context representations generated by the predictor. Despite achieving strong performances, neural-based approaches are resource-heavy and require a significant amount of in-domain labeled data for training. They do not use any internal information from the MT system. Existing work on glass-box QE is lim"
2020.tacl-1.35,D19-1073,0,0.314906,"ns for a given sentence and only one human translation is available in practice. Although the goal of this paper is to devise an unsupervised solution for the QE task, the analysis presented here provides new insights into calibration in NMT. Different from existing work, we study the relation between model probabilities and human judgments of translation correctness. Uncertainty quantification methods have been successfully applied to various practical tasks, for example, neural semantic parsing (Dong et al., 2018), hate speech classification (Miok et al., 2019), or back-translation for NMT (Wang et al., 2019). Wang et al. (2019), whose work is the closest to our work, explore a small set of uncertaintybased metrics to minimize the weight of erroneous synthetic sentence pairs for back translation in NMT. However, improved NMT training with weighted synthetic data does not necessarily imply better prediction of MT quality. In fact, metrics that Wang et al. (2019) report to perform the best for back-translation do not perform well for QE (see §3.2). 3.1 Exploiting the Softmax Distribution We start by defining a simple QE measure based on sequence-level translation probability normalized by length: TP"
2020.tacl-1.35,W18-6466,1,0.857973,"d approaches (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018). For example, Etchegoyhen et al. (2018) use lexical translation probabilities from word alignment models and language model probabilities. Their unsupervised approach averages these features to produce the final score. However, it is largely outperformed by the neural-based supervised QE systems (Specia et al., 2018). The only works that explore internal information from neural models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based indicators perform competitively only when combined with other QE features in a supervised framework. Furthermore, this approach is not directly applicable to the SOTA Transformer model that uses multihead attention mechanism. Recent work on attention interpretability showed that attention weights in Transformer networks might not be readily interpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challe"
2020.tacl-1.35,W18-6451,1,\N,Missing
2020.tacl-1.35,W19-5301,1,\N,Missing
2020.tacl-1.35,W19-5401,1,\N,Missing
2020.wmt-1.116,C04-1046,0,0.103907,"Missing"
2020.wmt-1.116,2020.acl-main.747,1,0.846095,"Missing"
2020.wmt-1.116,N19-1423,0,0.0390624,"data for all languages and use it for training our regression models. Note that we do not add any language identification markers and the system does not require them for making predictions. This can be useful for multilingual translation systems where the user does not need to identify the input languages, and especially for zero-shot settings where a given language pair may not have been seen at training time. 2.2 Black-box We explore a baseline neural QE model and a multitask learning QE model, both of which are built on top of pre-trained contextualised representations (CR) such as BERT (Devlin et al., 2019) and XLMR (Conneau et al., 2020). Baseline QE model (BASE) Given a source sentence sX in language X and a target sentence sY in language Y , we model the QE function f by stacking a 2-layer multilayer perceptron (MLP) on the vector representation of the [CLS] token from a contextualised representations model (CR): f (sX , sY ) =W2 · ReLU ( W1 · Ecls (sX , sY ) + b1 (1) ) + b2 where W2 ∈ R1×4h , b2 ∈ R, W1 ∈ R4h×h and b1 ∈ R4h . Ecls is a function that extracts the vector representation of the [CLS] token after encoding the concatenation of sX and sY with CR and 1011 ReLU is the Rectified Linea"
2020.wmt-1.116,C18-1266,1,0.719629,"Unit activation function. Note that h is the output dimension of Ecls . We explore two training strategies: The bilingual (BL) strategy trains a QE model for every language pair while the multilingual (ML) strategy trains a single multilingual QE model for all language pairs, where the training data is simply pooled together without any language identifier. We note that this multilingual model here corresponds to a pooled, single-task learning approach. MTL-LS submodel trained on the same language pair as the test set. BiRNN We compared the above approaches to the BiRNN model from deepQuest (Ive et al., 2018). The BiRNN model uses an encoder-decoder architecture: it encodes both source and translation sentences independently using two bi-directional Recurrent Neural Networks (RNNs). The two resulting sentence representations are concatenated afterwards as the weighted sum of their word vectors, generated by an attention mechanism. For predictions at sentence-level, the weighted representation of the two input sentences is passed through a dense layer with sigmoid activation to generate the quality estimates. This is a light-weight variant of the black-box approaches above that does not rely on hea"
2020.wmt-1.116,P19-3020,0,0.0589584,"Missing"
2020.wmt-1.116,2006.amta-papers.25,0,0.110843,"QE) (Blatz et al., 2004; Specia et al., 2009) is an important part of Machine Translation (MT) pipeline. It allows us to evaluate how good a translation is without comparison to reference sentences. As part of the WMT20 Shared Task on Quality Estimation, two sentence-level tasks were proposed. In Task 1, participants are asked to predict human judgements of MT quality generated following a methodology similar to Direct Assessment (DA) (Graham et al., 2017). The goal of Task 2 is to estimate the post-editing effort required in order to correct the MT outputs and measured using the HTER metric (Snover et al., 2006). 1 http://www.statmt.org/wmt20/ quality-estimation-task.html ∗ Equal contribution. Approach Below we first describe our glass-box submissions based on the quality indicators that can be obtained as a by-product of decoding with an NMT system. Second, we present our neural-based QE submissions, which explore transfer learning with pretrained representations. In both cases, we describe how QE is addressed as a multilingual task. 2.1 Glass-box Glass-box approaches to QE are based on information from the NMT system used to translate the sentences, rather than looking at source and target sentence"
2020.wmt-1.116,2009.eamt-1.5,1,0.913127,"Missing"
2020.wmt-1.116,N15-1124,0,0.0451071,"Missing"
2020.wmt-1.4,abdelali-etal-2014-amara,1,0.827823,"ne translated all comments using an in-house transformer-based model into Japanese and German. The goal of that was to be able to examine potential differences in source and (one example of) translation segments.3 We then pre-processed and automatically annotated all 17K segments with the following soft labels for catastrophic errors: Development Data The task specified the following data to help participants evaluate their system’s performance on unseen and multiple domains. • English-German: participants can use the development data from the News translation task, development data from QED (Abdelali et al., 2014) corpus, development data from WMT19 Medical translation task, and development data from the WMT16 IT translation task. 1. Introduction of toxicity: we checked both source and machine translation for toxic words (using in-house lists) and labelled as positive (i.e. potentially containing errors) cases where the source does not contain such words, but the translation does (at least one). • English-Japanese: participants can use the development data from the News translation task, and development data from the MTNT dataset, which contains noisy social media texts and their clean translations. 3."
2020.wmt-1.4,D17-1158,0,0.0130333,"Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The"
2020.wmt-1.4,N19-1311,0,0.0437239,"Missing"
2020.wmt-1.4,P18-1128,0,0.014933,"Naver Labs and LIMSI made specific efforts towards the task of Robustness. Both of them used lightweight domain adaptors proposed by Bapna and Firat (2019). Both teams UEDIN: Team UEDIN also mainly trained their system towards News translation task, but added Gumbel noise to the output layer of the systems submitted to the Robustness task. They followed standard NMT training pipeline and boasted their systems with additional data filtered from the paracrawl corpus. The data was carefully selected using dual cross-entropy (Junczys-Dowmunt, 2018) and length-normalized cross-entropy. 81 script6 (Dror et al., 2018) with p <0.05. The result of significance test in likert score is used for the human judgement ranking. Interestingly, the correlation in the system rankings between human judgments and BLEU is not strong. In other words, the best performing systems in BLEU do not rank high according human judgement, sometimes even rank the lowest. For example, in Ja→En (set2), the online-B system ranks first in BLEU but last in likert score. OPPO outperforms all systems in both directions on set2, and is overall the best system among the constrained, zero-shot submissions. To get insight on the proportion of"
2020.wmt-1.4,2020.lrec-1.520,0,0.0937737,"raped, filtered for noisy comments and translated by professional translators. This year, data was collected for two translation directions: English→Japanese and Japanese→English. For English, comments were collected from the /r/all feed, which encompasses all communities, and filtered for English. Since Japanese is a minority language on Reddit, comments were scraped from a selection of japanese-speaking communities, detailed in Michel and Neubig (2018). Common Voices Test Set (set3): This data was obtained from from the CoVoST corpus (Wang et al., 2020). CoVoST is derived from Common Voice (Ardila et al., 2020), a crowdsourced speech recognition corpus with an open CC0 license. Transcripts were sent to professional translators and the quality of translations was controlled by automatic and manual checks (Guzm´an et al., 2019). For this task, we used the German→English test set with source German sentences deduplicated. 1 Bad: translation errors are so severe that they cause the target text to be incomprehensible. This may be mainly due to major grammatical, typographical, or lexical errors, or omission of critical or important salient information. 2 Poor: the target text contains translation errors,"
2020.wmt-1.4,N19-1154,1,0.82504,"luation and the results discussed in Section 5. We hope that this task leads to more efforts from the community in building robust MT models. Introduction In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial examples (Koehn and Knowles, 2017; Belinkov and Bisk, 2018; Durrani et al., 2019; Anastasopoulos et al., 2019; Michel et al., 2019). From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets. Following the first shared task on Machine Translation (MT) Robustness, we now propose a second edition, which aims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in the following two scenarios: 2 Related Work Do"
2020.wmt-1.4,D19-1165,0,0.282315,"n a critical way. Critical errors are those that lead to misleading translations which may carry religious, health, safety, legal or financial implications, or introduce toxicity. The set of critical errors used for the guidelines (which also included examples of these errors) includes – but is not limited to – the cases below: Naver Labs (NLE): They participated in Chat and Biomedical tasks along with the Robustness task. They trained a general big-transformer model using FairSeq toolkit (Ott et al., 2019) and adapted it towards different tasks using lightweight adapter layers for each task (Bapna and Firat, 2019). They compared results against the more traditional finetuning method (Luong and Manning, 2015) to show that the former provides a viable alternative, while significantly reducing the amount of parameters per task. They also explored using embedding from pre-trained language models in their NMT system of which they tried two MLM variants: i) using NMT encoder’s setting, using Roberta (Liu et al., 2019). The latter was found more robust to novel domains and noise. The authors found that initializing with first 8 layers instead of the entire model to 80 OPPO: Team OPPO also trained their system"
2020.wmt-1.4,D19-1632,1,0.894086,"Missing"
2020.wmt-1.4,W17-4712,0,0.0183942,"ne-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively u"
2020.wmt-1.4,W17-3205,0,0.0203483,"to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task pr"
2020.wmt-1.4,P17-2061,0,0.0561223,"Missing"
2020.wmt-1.4,C18-1111,0,0.0137039,"6). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et al., 2017; Anastasopo"
2020.wmt-1.4,W18-6478,0,0.0263186,"different models to obtain further improvements. Only two teams, namely Naver Labs and LIMSI made specific efforts towards the task of Robustness. Both of them used lightweight domain adaptors proposed by Bapna and Firat (2019). Both teams UEDIN: Team UEDIN also mainly trained their system towards News translation task, but added Gumbel noise to the output layer of the systems submitted to the Robustness task. They followed standard NMT training pipeline and boasted their systems with additional data filtered from the paracrawl corpus. The data was carefully selected using dual cross-entropy (Junczys-Dowmunt, 2018) and length-normalized cross-entropy. 81 script6 (Dror et al., 2018) with p <0.05. The result of significance test in likert score is used for the human judgement ranking. Interestingly, the correlation in the system rankings between human judgments and BLEU is not strong. In other words, the best performing systems in BLEU do not rank high according human judgement, sometimes even rank the lowest. For example, in Ja→En (set2), the online-B system ranks first in BLEU but last in likert score. OPPO outperforms all systems in both directions on set2, and is overall the best system among the cons"
2020.wmt-1.4,P17-4012,0,0.0234194,"the decoder. This allows the test sets from known domains to use adapter layers and for novel domains to use the generic system. They created a noisy domain by adding synthetic noise to source data. The idea is that residual adapter layer learned from such data learns how to deal with noisy domain and is also able to preserve performance on the cleaner domains. However this did not work as well. The residual adapter fine-tuned using the ParaCrawl corpus gave better performance. PROMPT: Team PROMPT also participated mainly in the News translation task. Their systems were trained using OpenNMT (Klein et al., 2017) toolkit. They applied several stages of data preprocessing including length-based filtering, removing duplications, and using in-house classifier based on Hunalign aligner to identify and discard non-parallel sentences. They used two types of synthetic data to improve their systems: i) randomly selecting subset of Wikipedia equal to the size of news data and generating parallel corpus through back-translation, ii) creating synthetic data with unknown words using the procedure described in (Pinnis et al., 2017). Systems were trained with tags to differentiate between original data and syntheti"
2020.wmt-1.4,P02-1040,0,0.114721,"l can bias the selection to cases that are challenging for this particular model. In future work following this methodology, we recommend that multiple MT models be used. 4 https://cloud.google. com/natural-language/docs/ analyzing-sentiment 5 https://github.com/carpedm20/emoji/) 2 www.kaggle.com/c/ jigsaw-toxic-comment-classification- challenge 78 3.5 5. Presence of idioms: we checked if the source contains idiomatic expressions, using an inhouse list of idioms built from various sources, and labelled those cases as positive. Evaluation protocol Automatic evaluation: We first computed BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except En→Ja, we used the original reference and SacreBLEU with the default options. In the case of En→Ja, we used the reference tokenized with KyTea and the option --tokenize none. We note that the automatic labelling using our various pre-processing techniques may have introduced errors, but we believe that basing the selection on such heuristics will still lead to higher chances of selecting very challenging source segments than arbitrarily sampling the data. Human evaluation: The system outputs were evaluated by professi"
2020.wmt-1.4,W17-3204,1,0.837046,"e evaluated both automatically and via a human evaluation and the results discussed in Section 5. We hope that this task leads to more efforts from the community in building robust MT models. Introduction In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial examples (Koehn and Knowles, 2017; Belinkov and Bisk, 2018; Durrani et al., 2019; Anastasopoulos et al., 2019; Michel et al., 2019). From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets. Following the first shared task on Machine Translation (MT) Robustness, we now propose a second edition, which aims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in"
2020.wmt-1.4,W18-6459,0,0.0133196,"rvey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et al., 2017; Anastasopoulos et al., 2019; Lee et al., 2018). Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet. The previous iteration of the shared task focused on robustness of MT systems to such noise (Li et al., 2019). We refer to that report for more details. 3 participants to explore novel training and modeling approaches so that models have more robust performance at test time on multiple domains, including unseen and diversified domains. We offer two language pairs: English-German (En→De) and English-Japanese (En→Ja), with different test sets focusing on on"
2020.wmt-1.4,W19-5303,1,0.901081,"e 5th Conference on Machine Translation (WMT), pages 76–91 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predomin"
2020.wmt-1.4,W18-6319,0,0.0156248,"or this particular model. In future work following this methodology, we recommend that multiple MT models be used. 4 https://cloud.google. com/natural-language/docs/ analyzing-sentiment 5 https://github.com/carpedm20/emoji/) 2 www.kaggle.com/c/ jigsaw-toxic-comment-classification- challenge 78 3.5 5. Presence of idioms: we checked if the source contains idiomatic expressions, using an inhouse list of idioms built from various sources, and labelled those cases as positive. Evaluation protocol Automatic evaluation: We first computed BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except En→Ja, we used the original reference and SacreBLEU with the default options. In the case of En→Ja, we used the reference tokenized with KyTea and the option --tokenize none. We note that the automatic labelling using our various pre-processing techniques may have introduced errors, but we believe that basing the selection on such heuristics will still lead to higher chances of selecting very challenging source segments than arbitrarily sampling the data. Human evaluation: The system outputs were evaluated by professional translators. The translators were presen"
2020.wmt-1.4,E17-2045,0,0.0383395,"o domain shift assume the existence of significant amounts of parallel data in both the source and target domain. In this scenario, a common approach is to first train an MT system on a (generic) source domain and then to fine-tune it on a (specific) target domain (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Servan et al., 2016; Chu 76 Proceedings of the 5th Conference on Machine Translation (WMT), pages 76–91 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of mono"
2020.wmt-1.4,2021.ccl-1.108,0,0.105338,"Missing"
2020.wmt-1.4,P16-1162,0,0.0120732,"ns at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously"
2020.wmt-1.4,2015.iwslt-evaluation.11,0,0.64331,"ims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in the following two scenarios: 2 Related Work Domain mismatch is a key challenge in machine translation (Koehn and Knowles, 2017). Most approaches for improving robustness of MT systems to domain shift assume the existence of significant amounts of parallel data in both the source and target domain. In this scenario, a common approach is to first train an MT system on a (generic) source domain and then to fine-tune it on a (specific) target domain (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Servan et al., 2016; Chu 76 Proceedings of the 5th Conference on Machine Translation (WMT), pages 76–91 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approac"
2020.wmt-1.4,N19-1314,1,0.844424,"hope that this task leads to more efforts from the community in building robust MT models. Introduction In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial examples (Koehn and Knowles, 2017; Belinkov and Bisk, 2018; Durrani et al., 2019; Anastasopoulos et al., 2019; Michel et al., 2019). From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets. Following the first shared task on Machine Translation (MT) Robustness, we now propose a second edition, which aims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in the following two scenarios: 2 Related Work Domain mismatch is a key challenge in machine transla"
2020.wmt-1.4,D18-1050,1,0.940705,"tation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et al., 2017; Anastasopoulos et al., 2019; Lee et al., 2018). Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet. The previous iteration of the shared task focused on robustness of MT systems to such noise (Li et al., 2019). We refer to that report for more details. 3 participants to explore novel training and modeling approaches so that models have more robust performance at test time on multiple domains, including unseen and diversified domains. We offer two language pairs: English-German (En→De) and English-Japanese (En→Ja), with different test sets focusing on one or both these language p"
2020.wmt-1.4,2020.lrec-1.517,1,0.814412,"comments from the social media website reddit.com were scraped, filtered for noisy comments and translated by professional translators. This year, data was collected for two translation directions: English→Japanese and Japanese→English. For English, comments were collected from the /r/all feed, which encompasses all communities, and filtered for English. Since Japanese is a minority language on Reddit, comments were scraped from a selection of japanese-speaking communities, detailed in Michel and Neubig (2018). Common Voices Test Set (set3): This data was obtained from from the CoVoST corpus (Wang et al., 2020). CoVoST is derived from Common Voice (Ardila et al., 2020), a crowdsourced speech recognition corpus with an open CC0 license. Transcripts were sent to professional translators and the quality of translations was controlled by automatic and manual checks (Guzm´an et al., 2019). For this task, we used the German→English test set with source German sentences deduplicated. 1 Bad: translation errors are so severe that they cause the target text to be incomprehensible. This may be mainly due to major grammatical, typographical, or lexical errors, or omission of critical or important salient inform"
2020.wmt-1.4,N19-4007,1,0.808507,"as well as an analysis of catastrophic errors (Section 5.2). 5.1 General Quality Overall, the correlation between human judgments and BLEU is not strong. For En→De (set1), the Pearson’s correlation coefficient is 0.97, while for the other four tasks the coefficients are lower, with 0.78, 0.65, 0.52, 0.79 for En→De (set1), Ja→En (set2), En→Ja (set2), and De→En (set3) respectively. Automatic Evaluation The automatic evaluation (BLEU) results of the Shared Task are summarized in Table 2, where we also include the three online translation systems. We performed significance test using compare-mt (Neubig et al., 2019) where systems are considered as significantly different at p <0.05. The result of significance test is used for the automatic evaluation ranking. Overall, the unconstrained online-B system provides strong results and outperforms most systems in the five language pairs, except the De→En (set3) and En→Ja (set1). Among the participating teams, the best zeroshot systems were OPPO, which outperforms other zero-shot systems in En→De (set1), Ja→En (set2), and En→Ja (set2) tasks, and NLE, which outperforms other systems in the other two tasks. Only Naver Labs participated in the few-shot stage (NLE-f"
2020.wmt-1.4,P17-2089,0,0.0494679,"Missing"
2020.wmt-1.4,D17-1155,0,0.0161114,"Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at traini"
2020.wmt-1.4,D17-1147,0,0.0351203,"Missing"
2020.wmt-1.4,D16-1160,0,0.025774,"ples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et"
2020.wmt-1.79,N13-1073,0,0.0416194,"rip • Source side: Each word in the source side is labelled as OK (correctly translated) or BAD (caused a translation error). • Target side: Each word in the target side is labelled as OK (a correct translation) or BAD (should be replaced or deleted). Additionally, we consider gap ‘tokens’ at the beginning of the sentence, at the end and between each two words. They are labelled OK if no word should be inserted in that position (according to the post-edited version), and BAD otherwise. In order to obtain the labels, we first align source and MT using the IBM Model 2 alignments from FastAlign (Dyer et al., 2013), and compute edit distances between the generated and post-edited translations with TERCOM, using default settings and disabled shifts. 2.3 Task 3: Predicting document-level MQM This task consists in finding document-level translation errors and estimating a quality score according to the amount of minor, major, and critical errors present in the translation. The predictions are compared to a ground-truth obtained from annotations produced by crowd-sourced human translators from Unbabel community. Each document contains zero or more errors, annotated according to the MQM taxonomy6 , and 5 htt"
2020.wmt-1.79,D19-1632,1,0.894336,"Missing"
2020.wmt-1.79,2020.wmt-1.117,0,0.0617894,"Missing"
2020.wmt-1.79,2020.evalnlgeval-1.4,0,0.061105,"Missing"
2020.wmt-1.79,W19-5406,1,0.692374,"Missing"
2020.wmt-1.79,P19-3020,1,0.743217,"Missing"
2020.wmt-1.79,W17-4763,0,0.358683,"and 2019 to the training set, keeping the same development set from 2019, and released a new test set. The documents are short product title and descriptions in English, extracted from the Amazon Product Reviews dataset (McAuley et al., 2015; He and McAuley, 2016) (Sports and Outdoors category). The documents were machine translated into French using a state of the art online neural MT system. The dataset statistics are presented in Table 2. 3 Baseline systems Sentence-level baseline systems: For Tasks 1 and 2, both word and sentence-level, we used the LSTM-based Predictor-Estimator approach (Kim et al., 2017), implemented in OpenKiwi (Kepler et al., 2019b). The Predictor model was trained on the same parallel data as the NMT systems for each language pair (made available at the task website),7 while the the Estimator was trained on the 7, 000 QE labelled data for each task. Word-level baseline systems: For Task 2, we also used the Predictor-Estimator as above, but it was trained to predict jointly word-level tags and sentence-level scores. Document-level baseline system: For Task 3, similarly as last year, we used a baseline which treats sentences independently and casts the problem as word-level"
2020.wmt-1.79,2020.wmt-1.118,0,0.169813,"Missing"
2020.wmt-1.79,2020.wmt-1.119,1,0.89237,"Missing"
2020.wmt-1.79,2020.wmt-1.120,0,0.0548717,"Missing"
2020.wmt-1.79,W19-5333,0,0.0229976,"with a bilingual parallel corpus, and the entire model is then fine-tuned on the training quality labelled dataset of the shared task. At test time, the translation outputs, which are estimated with teacher forcing and special masking, are put together with the source sentences and put through a unified neural network model to predict the quality of the translations. Mak (T1): Mak represents the source and its translation sentence pairs as a set of 70 blackbox sentence-level features extracted with Quest++(Specia et al., 2015), using the resources used to train the English-Russian NMT system (Ng et al., 2019). Those features are then fitted into a support vector regressor with default settings. NICT Kyoto (T2): The English–German and English-Chinese sentence-level QE systems for Task 2 are ensembles of pre-trained crosslingual language models (XLM) (Conneau and Lample, 2019), fine-tuned in a multi-task fashion with two linear output layers for sentence and word-level quality estimation. A total of 8 XLM models with various masking hyper-parameters were domain-adapted using a subset of the additional resources provided by the QE shared task organisers, as well as a subset of the WikiMatrix corpus ["
2020.wmt-1.79,N19-4009,0,0.0363276,"refer to as direct assessment (DA). For that, a new dataset, was created containing seven languages pairs using sentences mostly from Wikipedia2 . These language pairs are divided into 3 categories: the high-resource English→German (En-De), English→Chinese (En-Zh) and Russian→English (Ru-En) pairs; the medium-resource Romanian→English (RoEn) and Estonian→English (Et-En) pairs; and the low-resource Sinhala→English (Si-En) and Nepali→English (Ne-En) pairs. Translations were produced with state-of-theart transformer-based NMT models trained using publicly available data and the fairseq toolkit (Ott et al., 2019); and were manually annotated for perceived quality. The quality label for this task ranges from 0 to 100, following the FLORES guidelines (Guzm´an et al., 2019). According to the guidelines given to annotators, the 0-10 range represents an incorrect translation; 11-29, a translation with few correct keywords, but the overall meaning is different from the source; 30-50, a translation with major mistakes; 51-69, a translation which is understandable and conveys the overall meaning of the source but contains typos or grammatical errors; 70-90, a translation that closely preserves the semantics o"
2020.wmt-1.79,2020.wmt-1.122,0,0.191132,"Missing"
2020.wmt-1.79,D19-1410,0,0.0226745,". For sentence-level, the different models are used as feature extractors, which are used as inputs of a dense layer to produce the predictions. For word-level, they use majority voting to ensemble the different models. Papago (T1, T3): Papago’s submission for Task 1 749 En-De is an ensemble of three models based on pre-trained contextualised representations: multilingual BERT (mBERT), XLM-MaskedLanguage-Modelling (XLM-MLM), and XLM-Causal-Language-Modelling (XLMCLM). Three scores were produced from these models: an extension of BERTScore using the multilingual BERT model, SentenceBERT score (Reimers and Gurevych, 2019), and target (German) language model score using a pre-trained GPT-2 model. Additionally, the scores were computed for synthetic data created using WMT News translation data by randomly performing different methods, including swapping word order, omiting words or repeating phrases. The three models are pre-trained from these data in a multi-task regression setting. Lastly, these pre-trained models are fine-tuned using the QE corpus. For Task 3, the submitted system uses an ensemble of four models leveraging either multilingual BERT or XLM. The training scheme is very task-oriented: erroneous s"
2020.wmt-1.79,2020.wmt-1.121,0,0.0571448,"Missing"
2020.wmt-1.79,2016.amta-researchers.2,0,0.082682,"for errors: minor (if it is not misleading nor changes meaning), major (if it changes meaning), and critical (if it changes meaning and carries any kind of implication, possibly offensive). Figure 1 shows an example of fine-grained error annotations for a sentence. Note that there is an annotation composed by two discontinuous spans: a whitespace and the token Grip — in this case, the annotation indicates wrong word order, and Grip should have been at the whitespace position. Document-level scores were then generated from the word-level errors and their severity using the method described in Sanchez-Torron and Koehn (2016, footnote 6). Namely, denoting by n the number of words in the document, and by nmin , nmaj , and ncri the number of annotated minor, major, and critical errors, the final quality scores were computed as: see MQM = 1 − 745 nminor + 5nmajor + 10ncrit (1) n Note that MQM values can be negative if the total severity exceeds the number of words. As this year’s dataset, we reused the training data from previous years, adding the test sets from 2018 and 2019 to the training set, keeping the same development set from 2019, and released a new test set. The documents are short product title and descri"
2020.wmt-1.79,P15-4020,1,0.771605,"irectional LSTM. The parameters of the Transformer bottleneck layer are first optimised with a bilingual parallel corpus, and the entire model is then fine-tuned on the training quality labelled dataset of the shared task. At test time, the translation outputs, which are estimated with teacher forcing and special masking, are put together with the source sentences and put through a unified neural network model to predict the quality of the translations. Mak (T1): Mak represents the source and its translation sentence pairs as a set of 70 blackbox sentence-level features extracted with Quest++(Specia et al., 2015), using the resources used to train the English-Russian NMT system (Ng et al., 2019). Those features are then fitted into a support vector regressor with default settings. NICT Kyoto (T2): The English–German and English-Chinese sentence-level QE systems for Task 2 are ensembles of pre-trained crosslingual language models (XLM) (Conneau and Lample, 2019), fine-tuned in a multi-task fashion with two linear output layers for sentence and word-level quality estimation. A total of 8 XLM models with various masking hyper-parameters were domain-adapted using a subset of the additional resources provi"
2020.wmt-1.79,2020.acl-main.558,1,0.893099,"Missing"
2020.wmt-1.79,2020.wmt-1.123,0,0.0620909,"Missing"
2020.wmt-1.79,2020.wmt-1.124,0,0.0510029,"Missing"
2020.wmt-1.79,2020.wmt-1.125,0,0.0843421,"Missing"
2021.acl-long.503,W19-5301,0,0.0612231,"Missing"
2021.acl-long.503,2020.emnlp-main.184,1,0.775492,"imodal Machine Translation Multimodal Machine Translation (MMT) attempts to improve MT quality by incorporating information from modalities other than language (Sulubacak et al., 2020). In our case, we train B ERTG EN for E N↔D E and E N↔F R MMT tasks and use the M ULTI 30 K dataset, the main dataset for image-informed translation, which provides caption translations for F LICKR 30 K images in German and French. To evaluate B ERT G EN on MMT tasks, we use the original 2016 test set which contains 1,000 examples. For a comprehensive comparison with previous work, we train a SoTA recurrent MMT (Caglayan et al., 2020) solely on the M ULTI 30 K dataset, which applies a secondary (visual) attention in the decoder over the RoI features i.e. the same features that are also used by B ERT G EN (§ 2.1). There are two GRU (Cho et al., 2014) layers in both the encoder and the decoder and the embedding & hidden dimensions in the model are set to 200 and 320, respectively. Each model has ∼5.6M parameters excluding the word embeddings. Besides the state-of-the-art constrained recurrent MMT model described above, we further compare B ERT G EN – which is trained on various other MT and IC corpora – to an unconstrained T"
2021.acl-long.503,N19-1422,1,0.846466,"erent tasks for image captioning. Adversarial evaluation. Following Elliott (2018), we probe B ERT G EN’s ability for integrating multiple modalities effectively. Specifically, we decode translations by shuffling {image, source caption} mappings so that the images do not correspond to the sentences to be translated. The E N→D E results showed that the incongruence leads to 1.1 and 0.9 point drops in BLEU and METEOR, respectively. For E N→F R, the drops are much more prominent with 3.1 and 2.3 points again for BLEU and METEOR. This indicates that the features are not ignored at all, unlike in (Caglayan et al., 2019), where they showed that sequence-to-sequence MMT models can learn to ignore the images when the linguistic signal is sufficient to perform the task. Zero-shot performance. The results in Table 4 show the surprising ability of B ERT G EN to perform MMT on directions unseen during training. 6445 TASK FAIRSEQ BL MT B ERT G EN BL MT I WSLT E N→D E I WSLT D E→E N I WSLT E N→F R I WSLT F R→E N S ETIMES E N→T R S ETIMES T R→E N 27.4 33.6 41.0 39.1 14.1 17.3 27.8 35.6 40.2 40.0 13.5 19.0 47.1 33.8 59.8 36.4 18.9 25.8 B ERT G EN TARTU‡ M SRA‡ 48.4 34.7 60.5 36.8 19.1 26.9 Table 5: Comparison of text-o"
2021.acl-long.503,2012.eamt-1.60,0,0.00925684,"e embedding & hidden dimensions in the model are set to 200 and 320, respectively. Each model has ∼5.6M parameters excluding the word embeddings. Besides the state-of-the-art constrained recurrent MMT model described above, we further compare B ERT G EN – which is trained on various other MT and IC corpora – to an unconstrained Transformer-based MMT trained on ∼9M additional E N→D E sentences (Libovick´y, 2019)4 in addition to M ULTI 30 K. 2.2.3 Text-only Machine Translation We incorporate six text-only MT tasks into our training protocol. We use E N↔D E and E N↔F R MT datasets from IWSLT’14 (Cettolo et al., 2012) which consists of TED Talks’ subtitles and their translations. We take the prepare-iwslt14 recipe from FAIRSEQ (Ott et al., 2019) to prepare the dev and test sets. This yields an E N↔D E test set of 6,750 sentences which consists of dev2010, dev2012.TEDX, tst2010, tst2011 and tst2012. Similarly, the E N↔F R test set consists of dev2010, tst2010, tst2011 and tst2012, which amounts to 4,493 sentences. For E N↔T R directions, we use the SETIMES2 (Tiedemann, 2012) news dataset for training. For development and test sets, we take the official WMT test sets (Bojar et al., 2018), namely, newstest201"
2021.acl-long.503,D14-1179,0,0.0491152,"Missing"
2021.acl-long.503,D19-5611,0,0.112154,"0), video (Sun et al., 2019), and speech (Chuang et al., 2020). Most of these approaches follow a task-specific fine-tuning step after the model is pre-trained. However, there has been little work on exploiting pre-trained MLMs for natural language generation (NLG) tasks. Previous work argues that the MLM objective is ill-suited for generation tasks such as machine translation (Yang et al., 2019; Rothe et al., 2020). Recent work in this direction has predominantly investigated the use of pre-trained models to either initialise Transformer-based encoderdecoder models (Imamura and Sumita, 2019; Clinchant et al., 2019; Yang et al., 2020; Rothe et al., 2020) or to distill knowledge for sequence generation tasks (Chen et al., 2020). In this work, we present B ERT G EN, which extends BERT in a generative setting (§ 2.1). This results in a single generator – without a separation between the encoder and the decoder – capable of consuming multiple input modalities and generating in multiple languages. The latter features are achieved by transferring knowledge from state-of-the-art pretrained models, namely VL-B ERT (Su et al., 2020) and multilingual BERT (M-B ERT) (Devlin et al., 2019). We train B ERT G EN on va"
2021.acl-long.503,W14-3348,0,0.0207618,"nes, but not necessarily be SoTA compared to novel & sophisticated NMT models, which also make use of a lot more training data. 3 Results and Findings We train B ERT G EN on lowercased sentences for 45 epochs, after which the overall performance on the tasks reached a plateau. We define one B ERT G EN epoch as a single pass over all of the training data for the M ULTI 30 K E N→D E MMT task and denote this task as the reference task. We use greedy search for all systems that we trained and merge back the word pieces before evaluation. We compute tokenised5 BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and CIDEr (Vedantam et al., 2015) using cococaption6 . In what follows, we provide detailed quantitative and qualitative findings. 3.1 Image Captioning Table 2 provides an overview of B ERT G EN’s image captioning performance on different test sets and languages. First of all, on English F LICKR 30 K, B ERT G EN is clearly able to outperform strong captioning models (§ 2.2.1) S ENTINEL (Lu et al., 2017) and N BT (Lu et al., 2018), even though they use beam search for decoding. On COCO (Chen et al., 2015), an image captioning corpus much larger and diverse than F LICKR 30 K, we evaluate B ERTG"
2021.acl-long.503,N19-1423,0,0.573965,"tperforms many strong baselines across the tasks explored. We also show B ERT G EN’s ability for zero-shot language generation, where it exhibits competitive performance to supervised counterparts. Finally, we conduct ablation studies which demonstrate that B ERT G EN substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models. 1 Introduction Recent work in unsupervised and self-supervised pre-training has revolutionised the field of natural language understanding (NLU), resulting in high performance ceilings across multiple tasks (Devlin et al., 2019; Yang et al., 2019; Dong et al., 2019). The recent success of language model pre-training with masked language modelling (MLM) such as BERT (Devlin et al., 2019) further paved the way for more complex approaches that combine language pre-training with images (Tan and Bansal, 2019; Su et al., 2020; Lu et al., 2020), video (Sun et al., 2019), and speech (Chuang et al., 2020). Most of these approaches follow a task-specific fine-tuning step after the model is pre-trained. However, there has been little work on exploiting pre-trained MLMs for natural language generation (NLG) tasks. Previous work"
2021.acl-long.503,P15-1166,0,0.020367,"ctive for task-specific generative pre-training and UniLM (Dong et al., 2019), which introduces uni-directional, bi-directional and sequence-to-sequence LM objectives by carefully adjusting the self-attention masks during training. Zhou et al. (2020) extend UniLM to vision & language pre-training using Conceptual Captions (Sharma et al., 2018) as the pre-training dataset. However, these models require a further fine-tuning step for generative tasks, unlike B ERTG EN that is trained only once. 4.3 Multi-task learning for generation Several approaches exist for multi-task learning & generation (Dong et al., 2015; Luong et al., 2016) in NLP, especially in multilingual NMT, where tasks denote different language pairs (Zoph and Knight, 2016; Firat et al., 2016). The multi-task (and zero-shot) generation ability of B ERT G EN is mostly inspired by Ha et al. (2016) and Johnson et al. (2017). Both of these introduced target language specifiers to select the output language when decoding translations from their model. Our multilingual & multimodal take on multitask generation is most similar to Kaiser et al. (2017), where a single Transformer model is trained on different tasks including image captioning, o"
2021.acl-long.503,2020.acl-main.705,0,0.0225232,"uning step after the model is pre-trained. However, there has been little work on exploiting pre-trained MLMs for natural language generation (NLG) tasks. Previous work argues that the MLM objective is ill-suited for generation tasks such as machine translation (Yang et al., 2019; Rothe et al., 2020). Recent work in this direction has predominantly investigated the use of pre-trained models to either initialise Transformer-based encoderdecoder models (Imamura and Sumita, 2019; Clinchant et al., 2019; Yang et al., 2020; Rothe et al., 2020) or to distill knowledge for sequence generation tasks (Chen et al., 2020). In this work, we present B ERT G EN, which extends BERT in a generative setting (§ 2.1). This results in a single generator – without a separation between the encoder and the decoder – capable of consuming multiple input modalities and generating in multiple languages. The latter features are achieved by transferring knowledge from state-of-the-art pretrained models, namely VL-B ERT (Su et al., 2020) and multilingual BERT (M-B ERT) (Devlin et al., 2019). We train B ERT G EN on various tasks, including image captioning, machine translation and multimodal machine translation, and datasets in f"
2021.acl-long.503,D18-1329,0,0.0176166,",T R ,F R sentences into English. Gray background indicates zero-shot outputs. The last example is from C OCO while the others are from F LICKR 30 K. COCO test set, as we can see in the third example. A set of additional examples in the Appendix shows that B ERT G EN does not simply retrieve caption translations learned from the E N→F R task. Overall, both quantitative and qualitative results provide evidence of the utility of multimodal and multilingual initialisation as well as the efficacy of knowledge transfer across different tasks for image captioning. Adversarial evaluation. Following Elliott (2018), we probe B ERT G EN’s ability for integrating multiple modalities effectively. Specifically, we decode translations by shuffling {image, source caption} mappings so that the images do not correspond to the sentences to be translated. The E N→D E results showed that the incongruence leads to 1.1 and 0.9 point drops in BLEU and METEOR, respectively. For E N→F R, the drops are much more prominent with 3.1 and 2.3 points again for BLEU and METEOR. This indicates that the features are not ignored at all, unlike in (Caglayan et al., 2019), where they showed that sequence-to-sequence MMT models can"
2021.acl-long.503,W16-3210,1,0.86434,"Missing"
2021.acl-long.503,N16-1101,0,0.0233954,"uence LM objectives by carefully adjusting the self-attention masks during training. Zhou et al. (2020) extend UniLM to vision & language pre-training using Conceptual Captions (Sharma et al., 2018) as the pre-training dataset. However, these models require a further fine-tuning step for generative tasks, unlike B ERTG EN that is trained only once. 4.3 Multi-task learning for generation Several approaches exist for multi-task learning & generation (Dong et al., 2015; Luong et al., 2016) in NLP, especially in multilingual NMT, where tasks denote different language pairs (Zoph and Knight, 2016; Firat et al., 2016). The multi-task (and zero-shot) generation ability of B ERT G EN is mostly inspired by Ha et al. (2016) and Johnson et al. (2017). Both of these introduced target language specifiers to select the output language when decoding translations from their model. Our multilingual & multimodal take on multitask generation is most similar to Kaiser et al. (2017), where a single Transformer model is trained on different tasks including image captioning, object classification, machine translation, speech recognition and parsing. However, their architecture depends on particular structures such as encod"
2021.acl-long.503,D19-5603,0,0.0624692,"al., 2020; Lu et al., 2020), video (Sun et al., 2019), and speech (Chuang et al., 2020). Most of these approaches follow a task-specific fine-tuning step after the model is pre-trained. However, there has been little work on exploiting pre-trained MLMs for natural language generation (NLG) tasks. Previous work argues that the MLM objective is ill-suited for generation tasks such as machine translation (Yang et al., 2019; Rothe et al., 2020). Recent work in this direction has predominantly investigated the use of pre-trained models to either initialise Transformer-based encoderdecoder models (Imamura and Sumita, 2019; Clinchant et al., 2019; Yang et al., 2020; Rothe et al., 2020) or to distill knowledge for sequence generation tasks (Chen et al., 2020). In this work, we present B ERT G EN, which extends BERT in a generative setting (§ 2.1). This results in a single generator – without a separation between the encoder and the decoder – capable of consuming multiple input modalities and generating in multiple languages. The latter features are achieved by transferring knowledge from state-of-the-art pretrained models, namely VL-B ERT (Su et al., 2020) and multilingual BERT (M-B ERT) (Devlin et al., 2019). W"
2021.acl-long.503,N19-4009,0,0.029246,"embeddings. Besides the state-of-the-art constrained recurrent MMT model described above, we further compare B ERT G EN – which is trained on various other MT and IC corpora – to an unconstrained Transformer-based MMT trained on ∼9M additional E N→D E sentences (Libovick´y, 2019)4 in addition to M ULTI 30 K. 2.2.3 Text-only Machine Translation We incorporate six text-only MT tasks into our training protocol. We use E N↔D E and E N↔F R MT datasets from IWSLT’14 (Cettolo et al., 2012) which consists of TED Talks’ subtitles and their translations. We take the prepare-iwslt14 recipe from FAIRSEQ (Ott et al., 2019) to prepare the dev and test sets. This yields an E N↔D E test set of 6,750 sentences which consists of dev2010, dev2012.TEDX, tst2010, tst2011 and tst2012. Similarly, the E N↔F R test set consists of dev2010, tst2010, tst2011 and tst2012, which amounts to 4,493 sentences. For E N↔T R directions, we use the SETIMES2 (Tiedemann, 2012) news dataset for training. For development and test sets, we take the official WMT test sets (Bojar et al., 2018), namely, newstest2016 and newstest2017 as the development 4 We obtained test set outputs from the author and preprocessed with M-B ERT tokeniser to en"
2021.acl-long.503,P02-1040,0,0.1101,"lpark as these strong NMT baselines, but not necessarily be SoTA compared to novel & sophisticated NMT models, which also make use of a lot more training data. 3 Results and Findings We train B ERT G EN on lowercased sentences for 45 epochs, after which the overall performance on the tasks reached a plateau. We define one B ERT G EN epoch as a single pass over all of the training data for the M ULTI 30 K E N→D E MMT task and denote this task as the reference task. We use greedy search for all systems that we trained and merge back the word pieces before evaluation. We compute tokenised5 BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and CIDEr (Vedantam et al., 2015) using cococaption6 . In what follows, we provide detailed quantitative and qualitative findings. 3.1 Image Captioning Table 2 provides an overview of B ERT G EN’s image captioning performance on different test sets and languages. First of all, on English F LICKR 30 K, B ERT G EN is clearly able to outperform strong captioning models (§ 2.2.1) S ENTINEL (Lu et al., 2017) and N BT (Lu et al., 2018), even though they use beam search for decoding. On COCO (Chen et al., 2015), an image captioning corpus much larger and diverse t"
2021.acl-long.503,N18-1202,0,0.0332059,"ns 0 5 10 15 Passes over EN-DE MMT data 20 Figure 6: Validation scores on M ULTI 30 K E N→D E MMT for the multi-tasking ablation: The default multi-task B ERT G EN outperforms the single-task one. of catastrophic forgetting (French, 1999). Based on these observations, we expect similar model behavior to hold for other tasks. 4 4.1 Related Work Multimodal multilingual pre-training Research in NLP and related fields has been increasingly focusing on transfer learning approaches where a model is first pre-trained on a data-rich task, and then transferred to downstream tasks (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). This framework presumably allows the model to capture useful inductive biases that generalise to a variety of NLP tasks, often after performing a task-specific fine-tuning (Raffel et al., 2020). Of these, the most relevant studies to our work are BERT (Devlin et al., 2019) and its multilingual version M-B ERT, which pre-train a Transformer (Vaswani et al., 2017) on large monolingual corpora using the masked language modelling (MLM) objective. 6447 Recent research has also attempted to combine linguistic inputs with other modalities such as vision and speech, to achieve"
2021.acl-long.503,W10-0721,0,0.0685612,"ecified natural language. We train B ERTG EN for English, German and Turkish captioning tasks. Specifically, we use the F LICKR 30 K dataset (Young et al., 2014) that provides 29K training images, each with five English captions collected through crowd-sourcing. The validation and test sets contain approximately 1K images each. We use the M ULTI 30 K dataset (Elliott et al., 2016), which annotates F LICKR 30 K images with five German captions. Finally, we use the TASVIR E T dataset (Unal et al., 2016) which provides two Turkish captions for each of the 8,092 images in the F LICKR 8 K dataset (Rashtchian et al., 2010). Since F LICKR 8 K is a subset of F LICKR 30 K, we create a new split of TASVIR E T to avoid data leakage between training and test splits. The resulting training, validation and test splits contain 6914, 543, and 543 images, respectively. To evaluate B ERT G EN’s performance on IC, we compare it against previous work with strong performance on COCO (Chen et al., 2015) and F LICKR 30 K. More precisely, A DAPTIVE ATTEN TION (S ENTINEL ) (Lu et al., 2017), which uses a sentinel token to distinguish between visual and non-visual representations, and N EURAL BABY TALK (N BT ), which follows a slo"
2021.acl-long.503,2020.tacl-1.18,0,0.0360184,"delling (MLM) such as BERT (Devlin et al., 2019) further paved the way for more complex approaches that combine language pre-training with images (Tan and Bansal, 2019; Su et al., 2020; Lu et al., 2020), video (Sun et al., 2019), and speech (Chuang et al., 2020). Most of these approaches follow a task-specific fine-tuning step after the model is pre-trained. However, there has been little work on exploiting pre-trained MLMs for natural language generation (NLG) tasks. Previous work argues that the MLM objective is ill-suited for generation tasks such as machine translation (Yang et al., 2019; Rothe et al., 2020). Recent work in this direction has predominantly investigated the use of pre-trained models to either initialise Transformer-based encoderdecoder models (Imamura and Sumita, 2019; Clinchant et al., 2019; Yang et al., 2020; Rothe et al., 2020) or to distill knowledge for sequence generation tasks (Chen et al., 2020). In this work, we present B ERT G EN, which extends BERT in a generative setting (§ 2.1). This results in a single generator – without a separation between the encoder and the decoder – capable of consuming multiple input modalities and generating in multiple languages. The latter"
2021.acl-long.503,P18-1238,0,0.0571727,"Missing"
2021.acl-long.503,D19-1514,0,0.102636,"ly benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models. 1 Introduction Recent work in unsupervised and self-supervised pre-training has revolutionised the field of natural language understanding (NLU), resulting in high performance ceilings across multiple tasks (Devlin et al., 2019; Yang et al., 2019; Dong et al., 2019). The recent success of language model pre-training with masked language modelling (MLM) such as BERT (Devlin et al., 2019) further paved the way for more complex approaches that combine language pre-training with images (Tan and Bansal, 2019; Su et al., 2020; Lu et al., 2020), video (Sun et al., 2019), and speech (Chuang et al., 2020). Most of these approaches follow a task-specific fine-tuning step after the model is pre-trained. However, there has been little work on exploiting pre-trained MLMs for natural language generation (NLG) tasks. Previous work argues that the MLM objective is ill-suited for generation tasks such as machine translation (Yang et al., 2019; Rothe et al., 2020). Recent work in this direction has predominantly investigated the use of pre-trained models to either initialise Transformer-based encoderdecoder m"
2021.acl-long.503,tiedemann-2012-parallel,0,0.00931242,"anslation We incorporate six text-only MT tasks into our training protocol. We use E N↔D E and E N↔F R MT datasets from IWSLT’14 (Cettolo et al., 2012) which consists of TED Talks’ subtitles and their translations. We take the prepare-iwslt14 recipe from FAIRSEQ (Ott et al., 2019) to prepare the dev and test sets. This yields an E N↔D E test set of 6,750 sentences which consists of dev2010, dev2012.TEDX, tst2010, tst2011 and tst2012. Similarly, the E N↔F R test set consists of dev2010, tst2010, tst2011 and tst2012, which amounts to 4,493 sentences. For E N↔T R directions, we use the SETIMES2 (Tiedemann, 2012) news dataset for training. For development and test sets, we take the official WMT test sets (Bojar et al., 2018), namely, newstest2016 and newstest2017 as the development 4 We obtained test set outputs from the author and preprocessed with M-B ERT tokeniser to ensure comparability. 6443 set (6,007 sentences), and newstest2018 (6,000 sentences) as the test set. Both IWSLT and SETIMES2 corpora are medium-scale resources often used in MT research community, and have much harder test sets than the MMT and IC tasks, due to a significant domain shift. Finally, for each translation direction, we tr"
2021.acl-long.503,Q14-1006,0,0.012894,"TI 30 K MMT 29,000 M ULTI 30 K MMT DE→EN FR→EN EN→FR EN→DE 464K 464K 560K 582K F LICKR 30 K IC IM→EN IM→DE 145,000 2.39M 2.48M I WSLT MT DE→EN EN→DE 158,388 3.85M 4.43M I WSLT MT FR→EN EN→FR 163,328 4.01M 4.78M S ETIMES MT TR→EN EN→TR 185,318 6.01M 8.40M 29,000 2.2.2 Table 1: Training statistics of B ERT G EN: the last column is the number of samples after sequence unrolling. 2.2.1 Image Captioning Image captioning (IC) involves describing images in a specified natural language. We train B ERTG EN for English, German and Turkish captioning tasks. Specifically, we use the F LICKR 30 K dataset (Young et al., 2014) that provides 29K training images, each with five English captions collected through crowd-sourcing. The validation and test sets contain approximately 1K images each. We use the M ULTI 30 K dataset (Elliott et al., 2016), which annotates F LICKR 30 K images with five German captions. Finally, we use the TASVIR E T dataset (Unal et al., 2016) which provides two Turkish captions for each of the 8,092 images in the F LICKR 8 K dataset (Rashtchian et al., 2010). Since F LICKR 8 K is a subset of F LICKR 30 K, we create a new split of TASVIR E T to avoid data leakage between training and test spli"
2021.acl-long.503,N16-1004,0,0.0252626,"nal and sequence-to-sequence LM objectives by carefully adjusting the self-attention masks during training. Zhou et al. (2020) extend UniLM to vision & language pre-training using Conceptual Captions (Sharma et al., 2018) as the pre-training dataset. However, these models require a further fine-tuning step for generative tasks, unlike B ERTG EN that is trained only once. 4.3 Multi-task learning for generation Several approaches exist for multi-task learning & generation (Dong et al., 2015; Luong et al., 2016) in NLP, especially in multilingual NMT, where tasks denote different language pairs (Zoph and Knight, 2016; Firat et al., 2016). The multi-task (and zero-shot) generation ability of B ERT G EN is mostly inspired by Ha et al. (2016) and Johnson et al. (2017). Both of these introduced target language specifiers to select the output language when decoding translations from their model. Our multilingual & multimodal take on multitask generation is most similar to Kaiser et al. (2017), where a single Transformer model is trained on different tasks including image captioning, object classification, machine translation, speech recognition and parsing. However, their architecture depends on particular str"
2021.acl-short.25,C04-1046,0,0.0908166,"users, who will employ predictions in practice. To address this challenge, we propose an online Bayesian meta-learning framework for the continuous training of QE models that is able to adapt them to the needs of different users, while being robust to distributional shifts in training and test data. Experiments on data with varying number of users and language characteristics validate the effectiveness of the proposed approach. 1 Introduction Quality Estimation (QE) models aim to evaluate the output of Machine Translation (MT) systems at run-time, when no reference translations are available (Blatz et al., 2004; Specia et al., 2009). QE models can be applied for instance to improve translation productivity by selecting high-quality translations amongst several candidates. A number of approaches have been proposed for this task (Specia et al., 2009, 2015; Kim et al., 2017; Kepler et al., 2019; Ranasinghe et al., 2020), and a shared task yearly benchmarks proposed approaches (Fonseca et al., 2019; Specia et al., 2020). Different users of MT output have varying quality needs and standards, depending for instance on the downstream task at hand, or the level of their knowledge of the languages involved,"
2021.acl-short.25,N19-1423,0,0.0185063,"per-parameter learning rates using meta-learning; MTL-IID is trained on the concatenated and shuffled data from all users for multiple epochs in multi-task fashion. It assumes i.i.d access to the data from all users, and thus serves as an upper-bound for the performance. (a) QT21 en-lv (nmt) PE ID Train Dev Test PE1 PE2 PE3 PE4 PE5 9952 3445 8770 4579 7651 2488 862 2193 1145 1913 559 193 537 276 435 Total 34397 8601 2000 QE Model The quality estimation model used by all continual learning methods is based on multilingual DistilBERT (Sanh et al., 2019), a smaller version of multi-lingual BERT (Devlin et al., 2019) trained with knowledge distillation (Buciluˇa et al., 2006; Hinton et al., 2015). It accepts as input the source and machine translation outputs concatenated as a single text, separated by a ‘[SEP]’ token and prepended with a ‘[CLS]’ token. The representation of the ‘[CLS]’ token is then passed to a linear layer to predict HTER (Snover et al., 2006) values as regression targets. (b) QT21 en-cs (smt) Table 1: Number of instances per post-editor (PE) for the QT21 dataset. from both statistical (smt) and neural (nmt) machine translation systems in multiple language directions.1 This is the large"
2021.acl-short.25,P19-3020,0,0.214409,"Missing"
2021.acl-short.25,W19-4326,1,0.82741,"and Uk denotes k steps of a gradient descent learning rule such as SGD. In order to account for uncertainty and improve robustness, Bayesian approaches to meta-learning have also been proposed (Kim et al., 2018; Finn et al., 2018; Ravi and Beatson, 2019; Wang et al., 2020; Nguyen et al., 2020). 2.3 Meta-Learning for Continual Learning Meta-learning for continual learning methods generally make use of the meta-learning objective one task at a time to ensure that learning on the current task does not lead to catastrophic forgetting on previous tasks. For instance, both Riemer et al. (2019) and Obamuyide and Vlachos (2019) propose to combine REPTILE (Nichol and Schulman, 2018), a first order meta-learning algorithm, together with experience replay to improve performance during continual learning. Javed and White (2019) proposed an online-aware meta-learning (OML) objective for learning representations that are less prone to catastrophic forgetting during continual learning. Holla et al. (2020) proposed to combine the OML objective together with experience replay for improved continual learning performance. Recently, Gupta et al. (2020) proposed Look-Ahead MAML (LA-MAML), which meta-learns per-parameter learning"
2021.acl-short.25,W17-4763,0,0.131103,"shifts in training and test data. Experiments on data with varying number of users and language characteristics validate the effectiveness of the proposed approach. 1 Introduction Quality Estimation (QE) models aim to evaluate the output of Machine Translation (MT) systems at run-time, when no reference translations are available (Blatz et al., 2004; Specia et al., 2009). QE models can be applied for instance to improve translation productivity by selecting high-quality translations amongst several candidates. A number of approaches have been proposed for this task (Specia et al., 2009, 2015; Kim et al., 2017; Kepler et al., 2019; Ranasinghe et al., 2020), and a shared task yearly benchmarks proposed approaches (Fonseca et al., 2019; Specia et al., 2020). Different users of MT output have varying quality needs and standards, depending for instance on the downstream task at hand, or the level of their knowledge of the languages involved, and training for the task. Thus, the perception of the quality of MT output can be subjective, and therefore the quality estimates obtained from a model trained on data from one set of users may not serve the needs of a different set users. However, most existing Q"
2021.acl-short.25,2020.wmt-1.122,0,0.506245,"iments on data with varying number of users and language characteristics validate the effectiveness of the proposed approach. 1 Introduction Quality Estimation (QE) models aim to evaluate the output of Machine Translation (MT) systems at run-time, when no reference translations are available (Blatz et al., 2004; Specia et al., 2009). QE models can be applied for instance to improve translation productivity by selecting high-quality translations amongst several candidates. A number of approaches have been proposed for this task (Specia et al., 2009, 2015; Kim et al., 2017; Kepler et al., 2019; Ranasinghe et al., 2020), and a shared task yearly benchmarks proposed approaches (Fonseca et al., 2019; Specia et al., 2020). Different users of MT output have varying quality needs and standards, depending for instance on the downstream task at hand, or the level of their knowledge of the languages involved, and training for the task. Thus, the perception of the quality of MT output can be subjective, and therefore the quality estimates obtained from a model trained on data from one set of users may not serve the needs of a different set users. However, most existing QE models are trained and evaluated in a static"
2021.acl-short.25,2006.amta-papers.25,0,0.199229,"on the learning rate (line 11), and on the parameters of the QE model (line 12). Because this approach can also be considered the online counterpart to the Bayesian Model Agnostic Meta-Learning approach of Kim et al. (2018), we refer to it as Continual Quality Estimation with Online Bayesian Meta-Learning (CQE-OBML). 4 Experiments and Results The QT21 Dataset We evaluate our approach with the publicly available QT21 (Specia et al., 2017), a large-scale dataset containing translations 192 Algorithm 1 Continual Quality Estimation with Online Bayesian Meta-Learning (CQE-OBML) computing the HTER (Snover et al., 2006) values between each source sentence and its post-edited translation. We thereafter split the data into train, dev and test splits for each post-editor. A breakdown of the number of train, dev and test instances per post-editor is available in Table 1. Require: QE model fW0 , learning rates α0 , β Require: Buffer B, update probability p 1: Initialize W0 , α0 2: for t = 1,2,3,... do 3: for each (Xt , Yt ) in Dttrain do 4: if random() &lt; p then 5: Update B ← B ∪ (Xt , Yt ) 6: end if 7: for k = 1,..K do 8: Wk = SV GD(Wk−1 , α0 , Xt , Yt ) 9: end for 10: (Xv , Yv ) ← (Xt , Yt ) ∪ sample(B) 11: α0 ←"
2021.acl-short.25,P15-1022,0,0.0396908,"Missing"
2021.acl-short.25,P15-4020,1,0.910774,"Missing"
2021.acl-short.25,2009.eamt-1.5,1,0.732045,"oy predictions in practice. To address this challenge, we propose an online Bayesian meta-learning framework for the continuous training of QE models that is able to adapt them to the needs of different users, while being robust to distributional shifts in training and test data. Experiments on data with varying number of users and language characteristics validate the effectiveness of the proposed approach. 1 Introduction Quality Estimation (QE) models aim to evaluate the output of Machine Translation (MT) systems at run-time, when no reference translations are available (Blatz et al., 2004; Specia et al., 2009). QE models can be applied for instance to improve translation productivity by selecting high-quality translations amongst several candidates. A number of approaches have been proposed for this task (Specia et al., 2009, 2015; Kim et al., 2017; Kepler et al., 2019; Ranasinghe et al., 2020), and a shared task yearly benchmarks proposed approaches (Fonseca et al., 2019; Specia et al., 2020). Different users of MT output have varying quality needs and standards, depending for instance on the downstream task at hand, or the level of their knowledge of the languages involved, and training for the t"
2021.acl-short.25,P14-1067,0,0.0411877,"Missing"
2021.cinlp-1.4,N18-3011,0,0.0281844,"Missing"
2021.cinlp-1.4,N19-1357,0,0.0258037,"method provides meaningful reasoning. Interpretability in NLP. Interpreting aspects of the input as conducive to the model prediction aims to: a) increase model trustworthiness, and b) allow for greater understanding of the data (Molnar, 2019). One way to do this is via local explanations, which explain the result of a single prediction, such as the view of high, learnt, class-agnostic attention weights (Bahdanau et al., 2015) applied to Recurrent Neural Network (RNN) hidden states, as proxies of word importance. However, the use of attention weights as explanations has been controversial, as Jain and Wallace (2019) have pointed out that for the same prediction, there can be counterfactual attentional explanations. Inversely, Wiegreffe and Pinter (2019) offer the justification that there may indeed exist multiple plausible explanations, out of which the attention mechanism captures one. An additional limitation of the attention mechanism is the inability to provide class-specific insights, due to the use of the softmax activation function. Alternatively, the predictions of blackbox classifiers can be interpreted through modelagnostic frameworks, such as LIME (Ribeiro et al., 2016) and SHAP (Lundberg and"
2021.cinlp-1.4,N18-1149,0,0.05964,"Missing"
2021.cinlp-1.4,W18-5415,0,0.0767098,"ncy” is an important factor. Since paper subject ground truth is not necessarily available, we can treat the abstract of a paper as a proxy for its subject, similar to Veitch et al. (2019). Previous approaches that aim to reconcile NLP with causal inference are limited to either binary treatment variables (Veitch et al., 2019; Roberts et al., 2020; Saha et al., 2019) or nominal confounders (Pryzant et al., 2017, 2018a,b). This paper takes a first step towards using learnt natural language embeddings both as the treatment and the confounder. We achieve this by generating deconfounded lexicons (Pryzant et al., 2018a) through Despite peer-reviewing being an essential component of academia since the 1600s, it has repeatedly received criticisms for lack of transparency and consistency. We posit that recent work in machine learning and explainable AI provide tools that enable insights into the decisions from a given peer-review process. We start by simulating the peer-review process using an ML classifier and extracting global explanations in the form of linguistic features that affect the acceptance of a scientific paper for publication on an open peerreview dataset. Second, since such global explanations"
2021.cinlp-1.4,2020.acl-main.474,0,0.0609228,"Missing"
2021.cinlp-1.6,N18-1146,0,0.0125268,"irly challenging. Drawing inspiration from the textual causality literature (Keith et al., 2020), there are a number of Suggestions Since this task is significantly understudied in the context of online hate speech, there are a lot of possible directions to follow. For example, it might prove meaningful to extract high-level representations like sentiment features or toxicity scores. The latter is supported by von Essen and Jansson (2020), who claim that ""hate begets hate"", meaning that hateful content triggers replies of the same style. Furthermore, one could use the framework implemented by Pryzant et al. (2018), which automatically induces representative lexicons for social science tasks, while controlling for potential confounding factors. Having previously established some factors as such, it is possible to employ this framework and observe the performance of the 79 lexicon. Lastly, it could prove fruitful to explore pre-trained language representation models that are focused on capturing profanity (e.g. HateBERT (Caselli et al., 2020)) or causality (e.g. CausalBERT (Khetan et al., 2020; Veitch et al., 2020)), or even attempt to fine-tune a hate- and causal-specific version of the original BERT mo"
2021.cinlp-1.6,2020.acl-main.474,0,0.0311507,"Missing"
2021.cinlp-1.6,P18-1125,0,0.0273001,"usczik and Pennebaker (2010)) in the second. There is great need for experimentation and improvement to attempt more holistic approaches to the problem. Suggestions While controlling for confounders, and in case they are categorical or numerical variables, it is possible to follow well established techniques such as matching (De Graaf et al., 2011). In simple terms, matching is a method to create pairs of samples from the same category and of different outcomes, to approximate randomised conditions and obliterate confounding bias so that any effect that remains must be realistic. For example, Zhang et al. (2018) perform their task while controlling for topical confounding, by creating pairs of ""good"" and ""bad"" conversations from the same Wikipedia page. This way, they ensure that any differences caused by the nature of the Wikipage topic are removed. However, it is not always possible to know the confounding factors. In that case, it is more useful to consider ways of eradicating confounding in a holistic way, such as the clustering approach of (Cheng et al., 2019). Additionally, it is highly likely for features of this language-dominated topic to be textual. In that case, treating the covariates can"
2021.cinlp-1.6,W17-1101,0,0.0316417,"has been an emergence of several social phenomena. Some of them can have a positive impact on mental health and eventually prove beneficial for societies; for example, it has been shown that the use of social networks from individuals of advanced age can lead to social benefits as well as have promising cognitive effects (Quinn, 2018). There is, however, a plethora of societal issues on the digital world which are proliferating in these platforms; such are several types of online misbehaviours. Despite the clear interest of the research community to prevent, detect and filter harmful content (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018; MacAvaney et al., 2019; Mishra et al., 2019; Tontodimamma et al., 2021; Vidgen et al., 2019), the problem is very complex and still far from addressed. For instance, its multifactorial character poses already a large obstacle with manifold parameters to consider: diversity of platforms, abundance 74 Proceedings of CI+NLP: First Workshop on Causal Inference and NLP, pages 74–82 November 10, 2021. ©2021 Association for Computational Linguistics which analyse phenomena of digital misbehaviours from a causal perspective, quantifying the influence on fundamental sociologi"
2021.eacl-main.112,W17-4746,1,0.840307,"Missing"
2021.eacl-main.112,W16-2358,1,0.790083,"Missing"
2021.eacl-main.112,N19-1422,1,0.84499,"clude Gr¨onroos et al. (2018) as their improvements (45.5 BLEU) were not due to multi-modality but rather to other modifications such as heavy parallel data augmentation, domain fine-tuning, and ensembling. 3.2 Explicit masking Here, we will evaluate the extent to which the visual information is taken into account (i) when TLM/VTLM predicts masked tokens, and (ii) when the fine-tuned NMT and MMT models are forced to translate source sentences with missing visual entities. For the latter, we use Flickr30k entities (Plummer et al., 2015) to mask head nouns in 2016 test set sentences, similar to Caglayan et al. (2019). Last-word masking. In this experiment, we measure the target word prediction accuracy, when last tokens7 of input caption pairs are systematically masked during evaluation. Table 2 suggests 7 We pre-process the sentences to ensure that they do not end with punctuation marks, which would make the task easier for masked punctuation. VALID EN TLM VTLM +shuf DE T EST B OTH EN DE B OTH 89.0 87.3 55.2 88.5 86.3 53.6 ⇑ 0.9 ⇑ 1.4 ⇑ 5.0 ⇑ 1.1 ⇑ 2.2 ⇑ 5.8 ⇓ 1.0 ⇓ 0.2 ⇓ 7.7 ⇓ 1.3 ⇓ 0.3 ⇓ 7.4 Table 2: Masked last-word prediction accuracies: VTLM gains are with respect to TLM, whereas the incongruent (+s"
2021.eacl-main.112,W16-2359,0,0.020165,"l representations. Unlike most of the prior work that use classification or retrieval based downstream evaluation, we focus on the generative task of multimodal machine translation (MMT), where images accompany captions during translation (Sulubacak et al., 2020). Once pre-trained, we transfer the VTLM encoder to a Transformer-based (Vaswani et al., 2017) MMT and fine-tune it for the MMT task. To our knowledge, this is also the first attempt of pre-training & fine-tuning for MMT, where the current state of the art mostly relies on training multimodal sequence-to-sequence systems from scratch (Calixto et al., 2016; Caglayan et al., 2016; Libovick´y and Helcl, 2017; Elliott and K´ad´ar, 2017; Caglayan et al., 2017; Yin et al., 2020). Our findings highlight the effectiveness of crosslingual visual pre-training: when fine-tuned on the English→German direction of the Multi30k dataset (Elliott et al., 2016), our MMT model surpasses our constrained MMT baseline by about 10 BLEU and 8 METEOR points. The rest of the paper is organised as follows: §2 describes our pre-training and fine-tuning protocol, §3 presents our quantitative and qualitative analyses, and §4 concludes the paper with pointers for future wor"
2021.eacl-main.112,2020.acl-main.747,0,0.083837,"Missing"
2021.eacl-main.112,W18-6439,0,0.046352,"Missing"
2021.eacl-main.112,N19-1423,0,0.0297915,"pre-training methods. In this paper, we combine these two approaches to learn visually-grounded crosslingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain stateof-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations. 1 Introduction Pre-trained language models (Peters et al., 2018; Devlin et al., 2019) have been proven valuable tools for contextual representation extraction. Many studies have shown their effectiveness in discovering linguistic structures (Tenney et al., 2019), which is useful for a wide variety of NLP tasks (Talmor et al., 2019; Kondratyuk and Straka, 2019; Petroni et al., 2019). These positive results led to further exploration of (i) cross-lingual pretraining (Lample and Conneau, 2019; Conneau et al., 2020; Wang et al., 2020) through the use of multiple mono-lingual and parallel resources, and (ii) visual pre-training where large-scale image captioning corpora are used to"
2021.eacl-main.112,D18-1329,0,0.038105,"Missing"
2021.eacl-main.112,W16-3210,1,0.849846,"Missing"
2021.eacl-main.112,I17-1014,0,0.0412046,"Missing"
2021.eacl-main.112,W18-6441,0,0.0368568,"Missing"
2021.eacl-main.112,D19-1279,0,0.0274918,"ng with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain stateof-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations. 1 Introduction Pre-trained language models (Peters et al., 2018; Devlin et al., 2019) have been proven valuable tools for contextual representation extraction. Many studies have shown their effectiveness in discovering linguistic structures (Tenney et al., 2019), which is useful for a wide variety of NLP tasks (Talmor et al., 2019; Kondratyuk and Straka, 2019; Petroni et al., 2019). These positive results led to further exploration of (i) cross-lingual pretraining (Lample and Conneau, 2019; Conneau et al., 2020; Wang et al., 2020) through the use of multiple mono-lingual and parallel resources, and (ii) visual pre-training where large-scale image captioning corpora are used to induce grounded vision & language representations (Lu et al., 2019; Tan and Bansal, 2019; Li et al., 2020a; Su et al., 2020; Li et al., 2020b). The latter is usually achieved by extending the masked language modelling (MLM) objective (Devlin et al., 2019) with auxiliary visi"
2021.eacl-main.112,P17-2031,0,0.0460798,"Missing"
2021.eacl-main.112,W19-5333,0,0.0151261,"does not exist in large-scale. To ad1 The “faster rcnn inception resnet v2 atrous oid v4” model from TensorFlow. 2 Although this choice is mostly practical, we hypothesise that using the same signal for both language and visual masking can be beneficial for grounding. dress this, we extend3 the Conceptual Captions (CC) (Sharma et al., 2018) dataset with German translations. CC is a large-scale collection of ∼3.3M images retrieved from the Internet, with noisy alt-text captions in English. The translation of English captions into German was automatically performed using an existing NMT model (Ng et al., 2019) provided4 in the Fairseq (Ott et al., 2019) toolkit. Since some of the images are no longer accessible, the final corpus’ size is reduced to ∼3.1M triplets. We used byte pair encoding (BPE) (Sennrich et al., 2016) to learn a joint 50k BPE model on the CC dataset. The pre-training was conducted for 1.5M steps, using a single RTX2080-Ti GPU, and best checkpoints were selected with respect to validation set accuracy. Settings. We use a small version of the TLM (Lample and Conneau, 2019)5 and set the model dimension, feed-forward layer dimension, number of layers and number of attention heads to"
2021.eacl-main.112,N19-4009,0,0.0180257,"“faster rcnn inception resnet v2 atrous oid v4” model from TensorFlow. 2 Although this choice is mostly practical, we hypothesise that using the same signal for both language and visual masking can be beneficial for grounding. dress this, we extend3 the Conceptual Captions (CC) (Sharma et al., 2018) dataset with German translations. CC is a large-scale collection of ∼3.3M images retrieved from the Internet, with noisy alt-text captions in English. The translation of English captions into German was automatically performed using an existing NMT model (Ng et al., 2019) provided4 in the Fairseq (Ott et al., 2019) toolkit. Since some of the images are no longer accessible, the final corpus’ size is reduced to ∼3.1M triplets. We used byte pair encoding (BPE) (Sennrich et al., 2016) to learn a joint 50k BPE model on the CC dataset. The pre-training was conducted for 1.5M steps, using a single RTX2080-Ti GPU, and best checkpoints were selected with respect to validation set accuracy. Settings. We use a small version of the TLM (Lample and Conneau, 2019)5 and set the model dimension, feed-forward layer dimension, number of layers and number of attention heads to d = 512, f = 2048, l = 6 and h = 8, respecti"
2021.eacl-main.112,N18-1202,0,0.0260693,"ss-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded crosslingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain stateof-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations. 1 Introduction Pre-trained language models (Peters et al., 2018; Devlin et al., 2019) have been proven valuable tools for contextual representation extraction. Many studies have shown their effectiveness in discovering linguistic structures (Tenney et al., 2019), which is useful for a wide variety of NLP tasks (Talmor et al., 2019; Kondratyuk and Straka, 2019; Petroni et al., 2019). These positive results led to further exploration of (i) cross-lingual pretraining (Lample and Conneau, 2019; Conneau et al., 2020; Wang et al., 2020) through the use of multiple mono-lingual and parallel resources, and (ii) visual pre-training where large-scale image captioni"
2021.eacl-main.112,D19-1250,0,0.0406297,"Missing"
2021.eacl-main.112,P16-1162,0,0.0194482,"language and visual masking can be beneficial for grounding. dress this, we extend3 the Conceptual Captions (CC) (Sharma et al., 2018) dataset with German translations. CC is a large-scale collection of ∼3.3M images retrieved from the Internet, with noisy alt-text captions in English. The translation of English captions into German was automatically performed using an existing NMT model (Ng et al., 2019) provided4 in the Fairseq (Ott et al., 2019) toolkit. Since some of the images are no longer accessible, the final corpus’ size is reduced to ∼3.1M triplets. We used byte pair encoding (BPE) (Sennrich et al., 2016) to learn a joint 50k BPE model on the CC dataset. The pre-training was conducted for 1.5M steps, using a single RTX2080-Ti GPU, and best checkpoints were selected with respect to validation set accuracy. Settings. We use a small version of the TLM (Lample and Conneau, 2019)5 and set the model dimension, feed-forward layer dimension, number of layers and number of attention heads to d = 512, f = 2048, l = 6 and h = 8, respectively. We randomly initialise model parameters, instead of using pre-trained LM checkpoints such as BERT or XLM. We use Adam (Kingma and Ba, 2014) with the mini-batch size"
2021.eacl-main.112,P18-1238,0,0.0464265,"Missing"
2021.eacl-main.112,N19-1421,0,0.0170387,"nd perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain stateof-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations. 1 Introduction Pre-trained language models (Peters et al., 2018; Devlin et al., 2019) have been proven valuable tools for contextual representation extraction. Many studies have shown their effectiveness in discovering linguistic structures (Tenney et al., 2019), which is useful for a wide variety of NLP tasks (Talmor et al., 2019; Kondratyuk and Straka, 2019; Petroni et al., 2019). These positive results led to further exploration of (i) cross-lingual pretraining (Lample and Conneau, 2019; Conneau et al., 2020; Wang et al., 2020) through the use of multiple mono-lingual and parallel resources, and (ii) visual pre-training where large-scale image captioning corpora are used to induce grounded vision & language representations (Lu et al., 2019; Tan and Bansal, 2019; Li et al., 2020a; Su et al., 2020; Li et al., 2020b). The latter is usually achieved by extending the masked language modelling (MLM) objective (Devlin et a"
2021.eacl-main.112,D19-1514,0,0.0177997,"tion. Many studies have shown their effectiveness in discovering linguistic structures (Tenney et al., 2019), which is useful for a wide variety of NLP tasks (Talmor et al., 2019; Kondratyuk and Straka, 2019; Petroni et al., 2019). These positive results led to further exploration of (i) cross-lingual pretraining (Lample and Conneau, 2019; Conneau et al., 2020; Wang et al., 2020) through the use of multiple mono-lingual and parallel resources, and (ii) visual pre-training where large-scale image captioning corpora are used to induce grounded vision & language representations (Lu et al., 2019; Tan and Bansal, 2019; Li et al., 2020a; Su et al., 2020; Li et al., 2020b). The latter is usually achieved by extending the masked language modelling (MLM) objective (Devlin et al., 2019) with auxiliary vision & language tasks such as masked region classification and image sentence matching. In this paper, we present the first attempt to bring together cross-lingual and visual pre-training. Our visual translation language modelling (VTLM) objective combines the translation language modelling (TLM) (Lample and Conneau, 2019) with masked region classification (MRC) (Chen et al., 2020; Su et al., 2020) to learn grou"
2021.eacl-main.112,P19-1452,0,0.0221486,"odelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain stateof-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations. 1 Introduction Pre-trained language models (Peters et al., 2018; Devlin et al., 2019) have been proven valuable tools for contextual representation extraction. Many studies have shown their effectiveness in discovering linguistic structures (Tenney et al., 2019), which is useful for a wide variety of NLP tasks (Talmor et al., 2019; Kondratyuk and Straka, 2019; Petroni et al., 2019). These positive results led to further exploration of (i) cross-lingual pretraining (Lample and Conneau, 2019; Conneau et al., 2020; Wang et al., 2020) through the use of multiple mono-lingual and parallel resources, and (ii) visual pre-training where large-scale image captioning corpora are used to induce grounded vision & language representations (Lu et al., 2019; Tan and Bansal, 2019; Li et al., 2020a; Su et al., 2020; Li et al., 2020b). The latter is usually achieved b"
2021.eacl-main.112,2020.acl-main.273,0,0.0971636,"on the generative task of multimodal machine translation (MMT), where images accompany captions during translation (Sulubacak et al., 2020). Once pre-trained, we transfer the VTLM encoder to a Transformer-based (Vaswani et al., 2017) MMT and fine-tune it for the MMT task. To our knowledge, this is also the first attempt of pre-training & fine-tuning for MMT, where the current state of the art mostly relies on training multimodal sequence-to-sequence systems from scratch (Calixto et al., 2016; Caglayan et al., 2016; Libovick´y and Helcl, 2017; Elliott and K´ad´ar, 2017; Caglayan et al., 2017; Yin et al., 2020). Our findings highlight the effectiveness of crosslingual visual pre-training: when fine-tuned on the English→German direction of the Multi30k dataset (Elliott et al., 2016), our MMT model surpasses our constrained MMT baseline by about 10 BLEU and 8 METEOR points. The rest of the paper is organised as follows: §2 describes our pre-training and fine-tuning protocol, §3 presents our quantitative and qualitative analyses, and §4 concludes the paper with pointers for future work. 2 Method We propose Visual Translation Language Modelling (VTLM) objective to learn multimodal crosslingual represent"
2021.eacl-main.112,Q14-1006,0,0.0392572,"aset) models without transferring weights from the pre-trained TLM/VTLM models. We refer to these models as from-scratch. For the fine-tuning experiments, we train three runs with different seeds. For evaluation, we use the models with the lowest validation set perplexity to decode translations with beam size equal to 8. 3 https://hucvl.github.io/VTLM The transformer.wmt19.en-de model. 5 https://github.com/facebookresearch/XLM 4 Dataset. We use the standard MMT corpus Multi30k (Elliott et al., 2016) for both fine-tuning and from-scratch runs. It contains 30k image descriptions from Flickr30k (Young et al., 2014) and their human translations in German for training, along with three test sets of 1K samples each: the original and the most in-domain 2016 test set, as well as 2017 and COCO test sets created using images and descriptions collected from sources other than Flickr. Settings. For fine-tuning, we use the same hyperparameters as the pre-training phase, apart from decreasing the learning rate to 1e−5. For MT models that are trained from scratch, we increase the dropout rate to 0.4 and linearly warm up the learning rate from 1e−7 to 1e−4 during the first 4,000 iterations. Inverse square-root annea"
2021.eacl-main.164,W17-3204,0,0.0460976,"Missing"
2021.eacl-main.164,P18-1165,0,0.0143886,"within the SAC framework (Section 3.4). We demonstrate its efficacy in translating ambiguous words, particularly the rare senses of such words. Our datasets and settings are described in Section 4, and our experiments in Section 5. 2 Related Work Reinforcement Learning for MT RL has been successfully applied to MT to bridge the gap between training and testing by optimising the sequence-level objective directly (Yu et al., 2017; Ranzato et al., 2015; Bahdanau et al., 2016). However, thus far mainly the REINFORCE (Williams, 1992) algorithm and its variants have been used (Ranzato et al., 2015; Kreutzer et al., 2018). These are simpler algorithms that handle the large natural language action space, but they employ a sequencelevel reward which tends to be sparse. To reduce model variance, Actor-Critic (AC) models consider the reward at each decoding step and use the Critic model to guide future actions (Konda and Tsitsiklis, 2000). This approach has also been explored for MT (Bahdanau et al., 2016; He et al., 2017). However, more advanced AC models with Q-Learning are rarely applied to language generation problems. This is due to the difficulty of approximating the Q-function for the large action space. Th"
2021.eacl-main.164,W19-1808,1,0.889783,"arget side of the 1 https://github.com/multi30k/dataset corpus. The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for testing. We use flickr2016 (2016), flickr2017 (2017) and coco2017 (COCO) test sets for model evaluation. 2016 is the most in-domain test set since it was taken from the same superset of descriptions as the training set, whereas 2017 and COCO are from different image description corpora and are thus considered out-of-domain. For more fine-grained assessment of our models with unsupervised reward, we use the MLT test set (Lala and Specia, 2018; Lala et al., 2019), an annotated subset of the Multi30K corpus where each instance is a 3-tuple consisting of an ambiguous source word, its textual context (a source sentence), and its correct translation. The test set contains 1,298 sentences for English-French and 1,708 for English-German. It was designed to benchmark models in their ability to select the right lexical choice for words with multiple translations, especially when some of these translations are rarer. Additionally, to allow for comparison with previous work, we evaluate on the IWSLT 2014 German-to-English dataset (Cettolo et al., 2012) from TED"
2021.eacl-main.164,L18-1602,1,0.834803,"., 2016)) only on the target side of the 1 https://github.com/multi30k/dataset corpus. The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for testing. We use flickr2016 (2016), flickr2017 (2017) and coco2017 (COCO) test sets for model evaluation. 2016 is the most in-domain test set since it was taken from the same superset of descriptions as the training set, whereas 2017 and COCO are from different image description corpora and are thus considered out-of-domain. For more fine-grained assessment of our models with unsupervised reward, we use the MLT test set (Lala and Specia, 2018; Lala et al., 2019), an annotated subset of the Multi30K corpus where each instance is a 3-tuple consisting of an ambiguous source word, its textual context (a source sentence), and its correct translation. The test set contains 1,298 sentences for English-French and 1,708 for English-German. It was designed to benchmark models in their ability to select the right lexical choice for words with multiple translations, especially when some of these translations are rarer. Additionally, to allow for comparison with previous work, we evaluate on the IWSLT 2014 German-to-English dataset (Cettolo et"
2021.eacl-main.164,N19-4009,0,0.0416353,"Missing"
2021.eacl-main.164,P02-1040,0,0.116325,"hout taking the sequential nature of language into account. At inference time, however, the generator will take the previous sampled output as the input at next time step, rather than the ground truth word. MLE training thus causes: (a) the problem of “exposure bias” as a result of recursive conditioning on its own errors at test time, since the model has never been exclusively “exposed” to its own predictions during training; (b) a mismatch between the training objective and the test objective, where the latter relies on evaluation using discrete and non-differentiable measures such as BLEU (Papineni et al., 2002). The current solution for both problems is mainly based on Reinforcement Learning (RL), where a seq2seq model (Sutskever et al., 2014; Bahdanau et al., 2014) is used as the policy which generates actions (tokens) and at each step receives rewards based on a discrete metric taking into account importance of immediate and future rewards. However, RL methods for seq2seq MT models also have their challenges: high-dimensional discrete action space, efficient sampling and exploration, choice of baseline reward, among others (Choshen et al., 2020). The typical metrics used as rewards (e.g., BLEU) ar"
2021.eacl-main.164,E17-2025,0,0.0604201,"Missing"
2021.eacl-main.164,P16-1162,0,0.0536261,"a) will be close to 0. Thus, frequent translations will be suppressed and search for less frequent translations will be encouraged in order to receive a reward larger than 0. Such a reward is less sparse than the traditional ones and is also dynamic which prevents memorising and overfitting. 4 Experimental Setup 4.1 Data We perform experiments on the Multi30K dataset (Elliott et al., 2016)1 of image description translations and focus on the English-German (ENDE) and English-French (EN-FR) (Elliott et al., 2017) language directions. Following best practises, we use sub-word segmentation (BPE (Sennrich et al., 2016)) only on the target side of the 1 https://github.com/multi30k/dataset corpus. The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for testing. We use flickr2016 (2016), flickr2017 (2017) and coco2017 (COCO) test sets for model evaluation. 2016 is the most in-domain test set since it was taken from the same superset of descriptions as the training set, whereas 2017 and COCO are from different image description corpora and are thus considered out-of-domain. For more fine-grained assessment of our models with unsupervised reward, we use the MLT test set (Lala and"
2021.eacl-main.164,P16-1159,0,0.0499926,"Missing"
2021.eacl-main.164,2006.amta-papers.25,0,0.274156,"Missing"
2021.eacl-main.164,E17-3017,0,0.0415781,"Missing"
2021.eacl-main.281,D18-1337,0,0.139807,"t et al., 2018). Although deterministic policies have been recently explored for simultaneous MMT (Caglayan et al., 2020; Imankulova et al., 2020), there are no studies regarding how multimodal information can be exploited to build flexible and adaptive policies for simultaneous machine translation (SiMT). Applications of reinforcement learning (RL) for unimodal SiMT have highlighted the challenges for the agent to maintain good translation quality while learning an optimal translation path (i.e. a sequence of READ/WRITE decisions at every time step) (Grissom II et al., 2016; Gu et al., 2017; Alinejad et al., 2018). Incomplete source information will have detrimental effect especially in the cases where significant restructuring is needed while translating from one language to another. In addition, the lack of information generally leads to high variance during the training in the RL setup. We posit that multimodality in adaptive SiMT could help the agent by providing extra signals, which would in turn improve training stability and thus the quality of the estimator and translation decoder. In this paper, we present the first exploration on multimodal RL approaches for the task of SiMT. As visual signal"
2021.eacl-main.281,2020.emnlp-main.184,1,0.50697,"while keeping the latency low. 1 Introduction Research into automating real-time interpretation has explored deterministic and adaptive approaches to build policies that address the issue of translation delay (Ryu et al., 2006; Cho and Esipova, 2016; Gu et al., 2017). In another recent development, the availability of multimodal data (such as visual information) has driven the community towards multimodal approaches for machine translation (MMT) (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Although deterministic policies have been recently explored for simultaneous MMT (Caglayan et al., 2020; Imankulova et al., 2020), there are no studies regarding how multimodal information can be exploited to build flexible and adaptive policies for simultaneous machine translation (SiMT). Applications of reinforcement learning (RL) for unimodal SiMT have highlighted the challenges for the agent to maintain good translation quality while learning an optimal translation path (i.e. a sequence of READ/WRITE decisions at every time step) (Grissom II et al., 2016; Gu et al., 2017; Alinejad et al., 2018). Incomplete source information will have detrimental effect especially in the cases where signifi"
2021.eacl-main.281,W16-2359,0,0.021612,"hS } unidirectionally. The attention layer receives H as key-values whereas the hidden states of the first decoder GRU provide the queries. The context vector cT t produced by the attention layer is given as input to the second GRU. Finally, the output token (yt ) probabilities are obtained by applying a softmax layer on top of the concatenation of the previous word embedding, context vector and the second GRU’s hidden state. For consecutive NMT, all source tokens are observed before the decoder begins the process of generation. Multimodal MT. We extend unimodal MT with multimodal attention (Calixto et al., 2016; Caglayan et al., 2016) in the decoder, in order to incorporate visual information into the baseline NMT. Let us denote the visual counterpart of textual hidden states H by V . Multimodal attention simply applies another attention layer on top of V , which yields a visual context vector cV t at each decoding timestep t. The final multimodal context vector that would be given as input to the second GRU is simply the sum of both context vectors. Unimodal wait-k NMT. We explore deterministic wait-k (Ma et al., 2019) approach as a unimodal baseline1 for simultaneous NMT. The wait-k model starts b"
2021.eacl-main.281,2020.iwslt-1.27,0,0.0120224,"whose goal is to learn a READ/WRITE policy by maximising quality and minimising latency. Alinejad et al. (2018) further extend the latter approach by adding a PREDICT action with an aim to capture the anticipation of the next source word. Ma et al. (2019) propose an end-to-end, fixed-latency framework called ‘wait-k’ which allows prefix-to-prefix training using a deterministic policy: the agent starts by reading a specified number of source tokens (k), followed by alternating WRITE and READ actions. Other approaches to SiMT include re-translation of previous outputs depending on new outputs (Arivazhagan et al., 2020; Niehues et al., 2018) or learning adaptive policies guided by a heuristic or alignment-based approaches (Zheng et al., 2019; Arthur et al., 2020). A general theme in these approaches is their reliance on consecutive NMT models pre-trained on full-sentences. However, Dalvi et al. (2018) discuss potential mismatches between the training and decoding regimens of these approaches and propose to perform fine-tuning of the models using chunked data or prefix pairs. 2.2 Multimodal Machine Translation MMT aims at improving the quality of automatic translation using additional sources of information"
2021.eacl-main.281,D17-1105,0,0.0171555,"f automatic translation using additional sources of information (Sulubacak et al., 2020). Different methods for fusing textual and visual information have been proposed. These include initialising the textual encoder or decoder with the visual information (Elliott and K´ad´ar, 2017; Caglayan et al., 2017), combining the visual information through spatial feature maps using soft attention (Caglayan et al., 2016; Libovick´y and Helcl, 2017; Huang et al., 2016; Calixto et al., 2017), and projecting a summary of the visual representations to a common context space via a trained projection matrix (Calixto and Liu, 2017; Caglayan et al., 2017; Elliott and K´ad´ar, 2017; Gr¨onroos et al., 2018). Further, recent work has also focused on exploring Multimodal Pivots (Hitschler et al., 2016) and latent variable models (Calixto et al., 2019) in the context of multimodal machine translation. In this paper, we explore all these strategies, and also the use of visual concepts, similar to the approach by Ive et al. (2019). 2.3 Multimodal Reinforcement Learning Previous work has explored RL with language inputs (Andreas et al., 2017; Bahdanau et al., 2018; Goyal et al., 2019) by making use of language to improve the po"
2021.eacl-main.281,2021.eacl-main.233,0,0.0417834,"Missing"
2021.eacl-main.281,P17-1175,0,0.0311011,"Missing"
2021.eacl-main.281,W18-6402,1,0.822673,"y and latency of simultaneous translation models, and demonstrate that visual cues lead to higher quality while keeping the latency low. 1 Introduction Research into automating real-time interpretation has explored deterministic and adaptive approaches to build policies that address the issue of translation delay (Ryu et al., 2006; Cho and Esipova, 2016; Gu et al., 2017). In another recent development, the availability of multimodal data (such as visual information) has driven the community towards multimodal approaches for machine translation (MMT) (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Although deterministic policies have been recently explored for simultaneous MMT (Caglayan et al., 2020; Imankulova et al., 2020), there are no studies regarding how multimodal information can be exploited to build flexible and adaptive policies for simultaneous machine translation (SiMT). Applications of reinforcement learning (RL) for unimodal SiMT have highlighted the challenges for the agent to maintain good translation quality while learning an optimal translation path (i.e. a sequence of READ/WRITE decisions at every time step) (Grissom II et al., 2016; Gu et al., 2017; Alinejad et al."
2021.eacl-main.281,W17-4746,1,0.875774,"Missing"
2021.eacl-main.281,W16-2358,1,0.845664,"Missing"
2021.eacl-main.281,D14-1179,0,0.0167785,"Missing"
2021.eacl-main.281,P11-2031,0,0.0178277,"n. Pre-processing. We use Moses scripts (Koehn et al., 2007) to lowercase, normalise and tokenise the sentences. We then create word vocabularies on the training subset of the dataset. We did not use subword segmentation to avoid its potential side effects on fixed policy SiMT and to be able to better analyse the grounding capability of the models. The resulting English, French and German vocabularies contain 9.8K, 11K and 18K tokens, respectively. 4.2 Evaluation We use BLEU (Papineni et al., 2002) for quality, and perform significance testing via bootstrap resampling using the Multeval tool (Clark et al., 2011). For latency, we measure Average proportion (AVP) (Cho and Esipova, 2016). AVP is the average number of source tokens required to commit a translation. This metric is sensitive to the difference in lengths between source and target. 4 https://github.com/multi30k/dataset Hence, as our main latency metric we measure Average Lagging (AVL) (Ma et al., 2019) which estimates the number of tokens the “writer” is lagging behind the “reader”, as a function of the number of input tokens read. 4.3 Training Hyperparameters. We set the embeddings dimensionality and GRU hidden states to 200 and 320, respec"
2021.eacl-main.281,N18-2079,0,0.0154582,"framework called ‘wait-k’ which allows prefix-to-prefix training using a deterministic policy: the agent starts by reading a specified number of source tokens (k), followed by alternating WRITE and READ actions. Other approaches to SiMT include re-translation of previous outputs depending on new outputs (Arivazhagan et al., 2020; Niehues et al., 2018) or learning adaptive policies guided by a heuristic or alignment-based approaches (Zheng et al., 2019; Arthur et al., 2020). A general theme in these approaches is their reliance on consecutive NMT models pre-trained on full-sentences. However, Dalvi et al. (2018) discuss potential mismatches between the training and decoding regimens of these approaches and propose to perform fine-tuning of the models using chunked data or prefix pairs. 2.2 Multimodal Machine Translation MMT aims at improving the quality of automatic translation using additional sources of information (Sulubacak et al., 2020). Different methods for fusing textual and visual information have been proposed. These include initialising the textual encoder or decoder with the visual information (Elliott and K´ad´ar, 2017; Caglayan et al., 2017), combining the visual information through spa"
2021.eacl-main.281,W17-4718,1,0.911433,"gies affect the quality and latency of simultaneous translation models, and demonstrate that visual cues lead to higher quality while keeping the latency low. 1 Introduction Research into automating real-time interpretation has explored deterministic and adaptive approaches to build policies that address the issue of translation delay (Ryu et al., 2006; Cho and Esipova, 2016; Gu et al., 2017). In another recent development, the availability of multimodal data (such as visual information) has driven the community towards multimodal approaches for machine translation (MMT) (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Although deterministic policies have been recently explored for simultaneous MMT (Caglayan et al., 2020; Imankulova et al., 2020), there are no studies regarding how multimodal information can be exploited to build flexible and adaptive policies for simultaneous machine translation (SiMT). Applications of reinforcement learning (RL) for unimodal SiMT have highlighted the challenges for the agent to maintain good translation quality while learning an optimal translation path (i.e. a sequence of READ/WRITE decisions at every time step) (Grissom II et al., 2016; Gu et al"
2021.eacl-main.281,W16-3210,1,0.90178,"Missing"
2021.eacl-main.281,I17-1014,0,0.032278,"Missing"
2021.eacl-main.281,K16-1010,0,0.0650304,"Missing"
2021.eacl-main.281,W18-6439,0,0.0496399,"Missing"
2021.eacl-main.281,E17-1099,0,0.107521,"sing reinforcement learning, with strategies to integrate visual and textual information in both the agent and the environment. We provide an exploration on how different types of visual information and integration strategies affect the quality and latency of simultaneous translation models, and demonstrate that visual cues lead to higher quality while keeping the latency low. 1 Introduction Research into automating real-time interpretation has explored deterministic and adaptive approaches to build policies that address the issue of translation delay (Ryu et al., 2006; Cho and Esipova, 2016; Gu et al., 2017). In another recent development, the availability of multimodal data (such as visual information) has driven the community towards multimodal approaches for machine translation (MMT) (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Although deterministic policies have been recently explored for simultaneous MMT (Caglayan et al., 2020; Imankulova et al., 2020), there are no studies regarding how multimodal information can be exploited to build flexible and adaptive policies for simultaneous machine translation (SiMT). Applications of reinforcement learning (RL) for unimodal S"
2021.eacl-main.281,P16-1227,0,0.0185184,". These include initialising the textual encoder or decoder with the visual information (Elliott and K´ad´ar, 2017; Caglayan et al., 2017), combining the visual information through spatial feature maps using soft attention (Caglayan et al., 2016; Libovick´y and Helcl, 2017; Huang et al., 2016; Calixto et al., 2017), and projecting a summary of the visual representations to a common context space via a trained projection matrix (Calixto and Liu, 2017; Caglayan et al., 2017; Elliott and K´ad´ar, 2017; Gr¨onroos et al., 2018). Further, recent work has also focused on exploring Multimodal Pivots (Hitschler et al., 2016) and latent variable models (Calixto et al., 2019) in the context of multimodal machine translation. In this paper, we explore all these strategies, and also the use of visual concepts, similar to the approach by Ive et al. (2019). 2.3 Multimodal Reinforcement Learning Previous work has explored RL with language inputs (Andreas et al., 2017; Bahdanau et al., 2018; Goyal et al., 2019) by making use of language to improve the policy or reward function: for example, the task of navigating in the world grid environment using language instructions (Andreas et al., 2016). Alternatively, RL with lang"
2021.eacl-main.281,W16-2360,0,0.0231835,"s and propose to perform fine-tuning of the models using chunked data or prefix pairs. 2.2 Multimodal Machine Translation MMT aims at improving the quality of automatic translation using additional sources of information (Sulubacak et al., 2020). Different methods for fusing textual and visual information have been proposed. These include initialising the textual encoder or decoder with the visual information (Elliott and K´ad´ar, 2017; Caglayan et al., 2017), combining the visual information through spatial feature maps using soft attention (Caglayan et al., 2016; Libovick´y and Helcl, 2017; Huang et al., 2016; Calixto et al., 2017), and projecting a summary of the visual representations to a common context space via a trained projection matrix (Calixto and Liu, 2017; Caglayan et al., 2017; Elliott and K´ad´ar, 2017; Gr¨onroos et al., 2018). Further, recent work has also focused on exploring Multimodal Pivots (Hitschler et al., 2016) and latent variable models (Calixto et al., 2019) in the context of multimodal machine translation. In this paper, we explore all these strategies, and also the use of visual concepts, similar to the approach by Ive et al. (2019). 2.3 Multimodal Reinforcement Learning"
2021.eacl-main.281,2020.wmt-1.70,0,0.0220318,"cy low. 1 Introduction Research into automating real-time interpretation has explored deterministic and adaptive approaches to build policies that address the issue of translation delay (Ryu et al., 2006; Cho and Esipova, 2016; Gu et al., 2017). In another recent development, the availability of multimodal data (such as visual information) has driven the community towards multimodal approaches for machine translation (MMT) (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Although deterministic policies have been recently explored for simultaneous MMT (Caglayan et al., 2020; Imankulova et al., 2020), there are no studies regarding how multimodal information can be exploited to build flexible and adaptive policies for simultaneous machine translation (SiMT). Applications of reinforcement learning (RL) for unimodal SiMT have highlighted the challenges for the agent to maintain good translation quality while learning an optimal translation path (i.e. a sequence of READ/WRITE decisions at every time step) (Grissom II et al., 2016; Gu et al., 2017; Alinejad et al., 2018). Incomplete source information will have detrimental effect especially in the cases where significant restructuring is need"
2021.eacl-main.281,P19-1653,1,0.85301,"al., 2016; Libovick´y and Helcl, 2017; Huang et al., 2016; Calixto et al., 2017), and projecting a summary of the visual representations to a common context space via a trained projection matrix (Calixto and Liu, 2017; Caglayan et al., 2017; Elliott and K´ad´ar, 2017; Gr¨onroos et al., 2018). Further, recent work has also focused on exploring Multimodal Pivots (Hitschler et al., 2016) and latent variable models (Calixto et al., 2019) in the context of multimodal machine translation. In this paper, we explore all these strategies, and also the use of visual concepts, similar to the approach by Ive et al. (2019). 2.3 Multimodal Reinforcement Learning Previous work has explored RL with language inputs (Andreas et al., 2017; Bahdanau et al., 2018; Goyal et al., 2019) by making use of language to improve the policy or reward function: for example, the task of navigating in the world grid environment using language instructions (Andreas et al., 2016). Alternatively, RL with language output can be shaped as sequential decision making for language generation, while conditioning on other modalities. This includes image captioning (Ren et al., 2017), video captioning (Wang et al., 2018), question answering ("
2021.eacl-main.281,P07-2045,0,0.0120261,"e perform experiments on the Multi30k dataset (Elliott et al., 2016)4 which extends the Flickr30k image captioning dataset (Young et al., 2014) with caption translations in German and French (Elliott et al., 2017). Multi30k is a standard MMT dataset that contains parallel sentences in two languages that describe the images. The training set for each language direction comprises 29,000 image-source-target triplets whereas the development and the test sets have around 1,000 samples. We use the corresponding test sets from 2016, 2017 and 2018 for evaluation. Pre-processing. We use Moses scripts (Koehn et al., 2007) to lowercase, normalise and tokenise the sentences. We then create word vocabularies on the training subset of the dataset. We did not use subword segmentation to avoid its potential side effects on fixed policy SiMT and to be able to better analyse the grounding capability of the models. The resulting English, French and German vocabularies contain 9.8K, 11K and 18K tokens, respectively. 4.2 Evaluation We use BLEU (Papineni et al., 2002) for quality, and perform significance testing via bootstrap resampling using the Multeval tool (Clark et al., 2011). For latency, we measure Average proport"
2021.eacl-main.281,P06-2088,0,0.0201775,"ach to simultaneous machine translation using reinforcement learning, with strategies to integrate visual and textual information in both the agent and the environment. We provide an exploration on how different types of visual information and integration strategies affect the quality and latency of simultaneous translation models, and demonstrate that visual cues lead to higher quality while keeping the latency low. 1 Introduction Research into automating real-time interpretation has explored deterministic and adaptive approaches to build policies that address the issue of translation delay (Ryu et al., 2006; Cho and Esipova, 2016; Gu et al., 2017). In another recent development, the availability of multimodal data (such as visual information) has driven the community towards multimodal approaches for machine translation (MMT) (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Although deterministic policies have been recently explored for simultaneous MMT (Caglayan et al., 2020; Imankulova et al., 2020), there are no studies regarding how multimodal information can be exploited to build flexible and adaptive policies for simultaneous machine translation (SiMT). Applications of r"
2021.eacl-main.281,P17-2031,0,0.0402759,"Missing"
2021.eacl-main.281,W16-2346,1,0.871947,"Missing"
2021.eacl-main.281,P02-1040,0,0.110215,"and the test sets have around 1,000 samples. We use the corresponding test sets from 2016, 2017 and 2018 for evaluation. Pre-processing. We use Moses scripts (Koehn et al., 2007) to lowercase, normalise and tokenise the sentences. We then create word vocabularies on the training subset of the dataset. We did not use subword segmentation to avoid its potential side effects on fixed policy SiMT and to be able to better analyse the grounding capability of the models. The resulting English, French and German vocabularies contain 9.8K, 11K and 18K tokens, respectively. 4.2 Evaluation We use BLEU (Papineni et al., 2002) for quality, and perform significance testing via bootstrap resampling using the Multeval tool (Clark et al., 2011). For latency, we measure Average proportion (AVP) (Cho and Esipova, 2016). AVP is the average number of source tokens required to commit a translation. This metric is sensitive to the difference in lengths between source and target. 4 https://github.com/multi30k/dataset Hence, as our main latency metric we measure Average Lagging (AVL) (Ma et al., 2019) which estimates the number of tokens the “writer” is lagging behind the “reader”, as a function of the number of input tokens r"
2021.eacl-main.281,Q14-1006,0,0.0335654,"https://hub.docker.com/r/airsplay/bottom-up-attention (a) (b) Figure 1: Our multimodal RL SiMT models: the agent interacts with the environment to receive new translation and at each time step produces the READ/WRITE action. For each action it receives a reward. The image information can be integrated into the agent by means of an attention mechanism (a, RL-att), or into the environment decoder (b, RL-env) producing the next translation. 4 Experimental Setup 4.1 Dataset We perform experiments on the Multi30k dataset (Elliott et al., 2016)4 which extends the Flickr30k image captioning dataset (Young et al., 2014) with caption translations in German and French (Elliott et al., 2017). Multi30k is a standard MMT dataset that contains parallel sentences in two languages that describe the images. The training set for each language direction comprises 29,000 image-source-target triplets whereas the development and the test sets have around 1,000 samples. We use the corresponding test sets from 2016, 2017 and 2018 for evaluation. Pre-processing. We use Moses scripts (Koehn et al., 2007) to lowercase, normalise and tokenise the sentences. We then create word vocabularies on the training subset of the dataset."
2021.eacl-main.281,D19-1137,0,0.0126568,"latter approach by adding a PREDICT action with an aim to capture the anticipation of the next source word. Ma et al. (2019) propose an end-to-end, fixed-latency framework called ‘wait-k’ which allows prefix-to-prefix training using a deterministic policy: the agent starts by reading a specified number of source tokens (k), followed by alternating WRITE and READ actions. Other approaches to SiMT include re-translation of previous outputs depending on new outputs (Arivazhagan et al., 2020; Niehues et al., 2018) or learning adaptive policies guided by a heuristic or alignment-based approaches (Zheng et al., 2019; Arthur et al., 2020). A general theme in these approaches is their reliance on consecutive NMT models pre-trained on full-sentences. However, Dalvi et al. (2018) discuss potential mismatches between the training and decoding regimens of these approaches and propose to perform fine-tuning of the models using chunked data or prefix pairs. 2.2 Multimodal Machine Translation MMT aims at improving the quality of automatic translation using additional sources of information (Sulubacak et al., 2020). Different methods for fusing textual and visual information have been proposed. These include initi"
2021.eacl-main.281,D14-1162,0,0.086664,"k (CNN) (He et al., 2016) pre-trained on ImageNet (Deng et al., 2009) for object classification. The size of the final feature tensor being 8x8x2048, the visual attention is applied on a grid of 64 equally-sized regions. Visual Concepts (VC) are explicit object representations where local regions are detected as objects and subsequently encoded with 100dimensional word representations. For a given image, the detector provides 36 object and 36 attribute region proposals which are abstract concepts associated with the image. We represent each of the detected region with its corresponding GloVe (Pennington et al., 2014) word vectors. An image is thus represented by a feature tensor of size 72x100 and the visual attention is now applied on these visual concepts, rather than the uniform grid of the first approach above. We hypothesise that this type of information can result in better referential grounding by using conceptually meaningful units rather than global features. The detector used here is a Faster R-CNN/ResNet-101 object detector (with 1600 object labels) (Anderson et al., 2018)3 pre-trained on the Visual Genome dataset (Krishna et al., 2017). To further reduce the variance of the gradient estimator,"
2021.eacl-main.50,Q19-1038,0,0.039126,"Missing"
2021.eacl-main.50,C04-1046,0,0.434269,"e to the substantial improvements achieved from Neural Machine Translation (NMT). However, even with improved performance, translation quality is not consistent across language pairs, domains, and sentences. This can be detrimental to end-user’s trust and can cause unintended consequences arising from poor translations. Thus, having metrics to assess the quality of translated content is crucial to ensure that only high-quality translations are provided to end-users or downstream tasks. Quality Estimation (QE) metrics aim to predict translation quality without access to reference translations (Blatz et al., 2004; Specia et al., 2009, 2013). State-of-the-art QE techniques have leveraged MT systems and language-specific human annotations as supervision, including direct assessment and post-editing (Kepler et al., 2019a; Fonseca et al., 2019; Sun et al., 2020). However, these annotations are costly and time-consuming, particularly for word-level QE, where each token needs a label. Some unsupervised approaches take inspiration from statistical MT (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018) or apply uncertainty quantification (Fomicheva et al., 2020) for QE. However, their performan"
2021.eacl-main.50,N19-1423,0,0.0263014,"2019). As displayed in Figure 2, we perform text-infilling by applying three operations: (1) randomly substituting a proportion of tokens with a &lt;mask&gt; token, (2) deleting consecutive tokens, and (3) inserting additional consecutive &lt;mask&gt; tokens. We determine the lengths of consecutive deletions and insertions by drawing them from a Poisson distribution with mean λ = 1 shifted by 1 to avoid zero-length insertions or deletions. We then use a pre-trained masked language model (MLM) supplied with the source sentence as input to infill the masked reference sentence. We select multilingual BERT (Devlin et al., 2019) as it is pre-trained on Wikipedia which is in-domain to our test set. We present the target-rewriting approach in detail in Algorithm 2. In Section 4, we will investigate the performance 4 Experiments and Results We focus on data released by the WMT20 shared task on QE for predicting post-editing effort, which includes English-to-German (En-De) and English-to-Chinese (En-Zh) word-level data and their sentence-level HTER (Specia et al., 2020).1 As the human-annotated data is sampled from Wikipedia, we choose to synthesize data from WikiMatrix (Schwenk et al., 2019a), which consists of mined Wi"
2021.eacl-main.50,2020.emnlp-main.480,1,0.793674,"Missing"
2021.eacl-main.50,2020.aacl-main.62,1,0.887366,"Missing"
2021.eacl-main.50,W18-6461,0,0.0184278,"Estimation (QE) metrics aim to predict translation quality without access to reference translations (Blatz et al., 2004; Specia et al., 2009, 2013). State-of-the-art QE techniques have leveraged MT systems and language-specific human annotations as supervision, including direct assessment and post-editing (Kepler et al., 2019a; Fonseca et al., 2019; Sun et al., 2020). However, these annotations are costly and time-consuming, particularly for word-level QE, where each token needs a label. Some unsupervised approaches take inspiration from statistical MT (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018) or apply uncertainty quantification (Fomicheva et al., 2020) for QE. However, their performance is inferior to that of supervised models. In related areas such as automatic post-editing, parallel data has been used to create synthetic post-editing data (Negri et al., 2018), however this technique only compares machine-translated sentences to references. Our approach augments MT errors with additional errors via masked language model rewriting. We leverage noisy, mined comparable sentences obtained by weakly-supervised techniques (ElKishky et al., 2020b). These noisy bitexts have been mined fr"
2021.eacl-main.50,W19-5401,0,0.0941013,"al to end-user’s trust and can cause unintended consequences arising from poor translations. Thus, having metrics to assess the quality of translated content is crucial to ensure that only high-quality translations are provided to end-users or downstream tasks. Quality Estimation (QE) metrics aim to predict translation quality without access to reference translations (Blatz et al., 2004; Specia et al., 2009, 2013). State-of-the-art QE techniques have leveraged MT systems and language-specific human annotations as supervision, including direct assessment and post-editing (Kepler et al., 2019a; Fonseca et al., 2019; Sun et al., 2020). However, these annotations are costly and time-consuming, particularly for word-level QE, where each token needs a label. Some unsupervised approaches take inspiration from statistical MT (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018) or apply uncertainty quantification (Fomicheva et al., 2020) for QE. However, their performance is inferior to that of supervised models. In related areas such as automatic post-editing, parallel data has been used to create synthetic post-editing data (Negri et al., 2018), however this technique only compares machine-tran"
2021.eacl-main.50,W19-5406,0,0.347644,"Missing"
2021.eacl-main.50,P19-3020,0,0.0449671,"Missing"
2021.eacl-main.50,W17-4763,0,0.418579,"Missing"
2021.eacl-main.50,2020.acl-main.703,0,0.0644187,"Missing"
2021.eacl-main.50,W12-3114,0,0.0286302,"wnstream tasks. Quality Estimation (QE) metrics aim to predict translation quality without access to reference translations (Blatz et al., 2004; Specia et al., 2009, 2013). State-of-the-art QE techniques have leveraged MT systems and language-specific human annotations as supervision, including direct assessment and post-editing (Kepler et al., 2019a; Fonseca et al., 2019; Sun et al., 2020). However, these annotations are costly and time-consuming, particularly for word-level QE, where each token needs a label. Some unsupervised approaches take inspiration from statistical MT (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018) or apply uncertainty quantification (Fomicheva et al., 2020) for QE. However, their performance is inferior to that of supervised models. In related areas such as automatic post-editing, parallel data has been used to create synthetic post-editing data (Negri et al., 2018), however this technique only compares machine-translated sentences to references. Our approach augments MT errors with additional errors via masked language model rewriting. We leverage noisy, mined comparable sentences obtained by weakly-supervised techniques (ElKishky et al., 2020b). These noisy"
2021.eacl-main.50,L18-1004,0,0.0242229,"ssessment and post-editing (Kepler et al., 2019a; Fonseca et al., 2019; Sun et al., 2020). However, these annotations are costly and time-consuming, particularly for word-level QE, where each token needs a label. Some unsupervised approaches take inspiration from statistical MT (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018) or apply uncertainty quantification (Fomicheva et al., 2020) for QE. However, their performance is inferior to that of supervised models. In related areas such as automatic post-editing, parallel data has been used to create synthetic post-editing data (Negri et al., 2018), however this technique only compares machine-translated sentences to references. Our approach augments MT errors with additional errors via masked language model rewriting. We leverage noisy, mined comparable sentences obtained by weakly-supervised techniques (ElKishky et al., 2020b). These noisy bitexts have been mined from a variety of domains such as Wikipedia (Schwenk et al., 2019a) and large webcrawls (Schwenk et al., 2019b; El-Kishky et al., 2020a; El-Kishky and Guzm´an, 2020) and have been shown to be an invaluable source of training data for NMT models. Using this data is crucial to"
2021.eacl-main.50,N19-4009,0,0.0790376,"Missing"
2021.eacl-main.50,W12-3116,0,0.0753106,"Missing"
2021.eacl-main.50,2021.eacl-main.115,1,0.832831,"Missing"
2021.eacl-main.50,P01-1063,0,0.041196,"beginning and the end) as gt ∈ {OK, BAD}, where t ∈ [1, T + 1]. In traditional QE, data is collected by first translating source sentences using an MT model. Second, experts post-edit these translations. Third, the post-edits and machine translations are aligned in such a way that induces the minimum edit distance between the tokens of each. Finally, each mt is labelled as BAD if it should be deleted or substituted and each gt is labelled as BAD if at least a word should be inserted there. Sentence-level QE labels can be generated by computing the Human-targeted Translation Error Rate (HTER) (Snover and Brent, 2001; Snover et al., 2006), which is the minimum Approach to Data Synthesis As depicted in Figure 1, we synthesize data from mined Wikipedia datasets, where each example consists of a (source, target) sentence pair. We create candidate translations of source sentences in two ways: For the first approach, we apply the NMT model to translate each source sentence. For the second approach, we rewrite each reference target sentence using a masked language model (MLM), as shown in the MLM Rewrites block in Figure 1. The two approaches create two forms of translations. Then, by treating target sentences"
2021.eacl-main.50,2006.amta-papers.25,0,0.201519,"s gt ∈ {OK, BAD}, where t ∈ [1, T + 1]. In traditional QE, data is collected by first translating source sentences using an MT model. Second, experts post-edit these translations. Third, the post-edits and machine translations are aligned in such a way that induces the minimum edit distance between the tokens of each. Finally, each mt is labelled as BAD if it should be deleted or substituted and each gt is labelled as BAD if at least a word should be inserted there. Sentence-level QE labels can be generated by computing the Human-targeted Translation Error Rate (HTER) (Snover and Brent, 2001; Snover et al., 2006), which is the minimum Approach to Data Synthesis As depicted in Figure 1, we synthesize data from mined Wikipedia datasets, where each example consists of a (source, target) sentence pair. We create candidate translations of source sentences in two ways: For the first approach, we apply the NMT model to translate each source sentence. For the second approach, we rewrite each reference target sentence using a masked language model (MLM), as shown in the MLM Rewrites block in Figure 1. The two approaches create two forms of translations. Then, by treating target sentences as if they were post-e"
2021.eacl-main.50,P13-4014,1,0.921917,"Missing"
2021.eacl-main.50,2009.eamt-1.5,1,0.814798,"improvements achieved from Neural Machine Translation (NMT). However, even with improved performance, translation quality is not consistent across language pairs, domains, and sentences. This can be detrimental to end-user’s trust and can cause unintended consequences arising from poor translations. Thus, having metrics to assess the quality of translated content is crucial to ensure that only high-quality translations are provided to end-users or downstream tasks. Quality Estimation (QE) metrics aim to predict translation quality without access to reference translations (Blatz et al., 2004; Specia et al., 2009, 2013). State-of-the-art QE techniques have leveraged MT systems and language-specific human annotations as supervision, including direct assessment and post-editing (Kepler et al., 2019a; Fonseca et al., 2019; Sun et al., 2020). However, these annotations are costly and time-consuming, particularly for word-level QE, where each token needs a label. Some unsupervised approaches take inspiration from statistical MT (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018) or apply uncertainty quantification (Fomicheva et al., 2020) for QE. However, their performance is inferior to tha"
2021.eacl-main.50,2020.aacl-main.39,1,0.858128,"Missing"
2021.emnlp-demo.42,W12-3102,1,0.768315,"Missing"
2021.emnlp-demo.42,2020.acl-main.747,0,0.085337,"Missing"
2021.emnlp-demo.42,N19-1423,0,0.0739595,"Missing"
2021.emnlp-demo.42,W18-2501,0,0.0328651,"ls based on XLM-R into small bidirectional RNN-based models. 3.1 BiRNN-based Architecture deepQuest-py implements sentence-level models following the architecture proposed by Ive et al. (2018). In this approach, the original sentence and its translation are encoded independently using dedicated BiRNNs. To obtain predictions, these two representations are concatenated as the weighted sum of their word vectors, generated by an attention mechanism. Then, this joint representation is passed through a dense layer with sigmoid activation to generate the quality estimates. deepQuestpy uses AllenNLP (Gardner et al., 2018)3 as its backbone for the BiRNN model. 3.2 Inference Name TQXLM−R−Large TQDistilBERT BiRNN #params 561M 135M 18M Speed (secs.) RAM (MiB) Disk (M) 0.82 1.09 0.39 9,263.5 1,979.2 155.6 2140 517 132 Table 2: Efficiency. Inference speed and RAM for prediction are for 1 sentence on CPU (Intel Xeon Silver 4114 CPU @ 2.20GHz). Knowledge Distillation For cases where large size SotA QE models are not deployable, Gajbhiye et al. (2021) propose to use a KD approach to train more efficient and wellperforming models for sentence-level QE. The approach (illustrated in Figure 1) consists of three steps descr"
2021.emnlp-demo.42,C18-1266,1,0.933574,"(Specia et al., weight models for Quality Estimation (QE). 2013) and QuEst++ (Specia et al., 2015) were deepQuest-py provides access to (1) state-ofthe first ones, and included methods that relied the-art models based on pre-trained Transformon extracting linguistically-motivated features to ers for sentence-level and word-level QE; (2) train traditional machine learning models (e.g. suplight-weight and efficient sentence-level models implemented via knowledge distillation; port vector machines). With the advent of neuraland (3) a web interface for testing models based approaches, deepQuest (Ive et al., 2018) proand visualising their predictions. deepQuestvided a TensorFlow-based framework for RNNpy is available at https://github.com/ based sentence-level and document-level QE modsheffieldnlp/deepQuest-py under a els, inspired by the Predictor-Estimator approach CC BY-NC-SA licence. (Kim et al., 2017). OpenKiwi (Kepler et al., 2019) implements a common API for experiment1 Introduction ing with several feature-based and neural-based QE models. More recently, TransQuest (RanasQuality Estimation (QE) for Machine Translation (MT) aims to predict how good automatic transla- inghe et al., 2020b) release"
2021.emnlp-demo.42,P19-3020,0,0.0200373,"evel QE; (2) train traditional machine learning models (e.g. suplight-weight and efficient sentence-level models implemented via knowledge distillation; port vector machines). With the advent of neuraland (3) a web interface for testing models based approaches, deepQuest (Ive et al., 2018) proand visualising their predictions. deepQuestvided a TensorFlow-based framework for RNNpy is available at https://github.com/ based sentence-level and document-level QE modsheffieldnlp/deepQuest-py under a els, inspired by the Predictor-Estimator approach CC BY-NC-SA licence. (Kim et al., 2017). OpenKiwi (Kepler et al., 2019) implements a common API for experiment1 Introduction ing with several feature-based and neural-based QE models. More recently, TransQuest (RanasQuality Estimation (QE) for Machine Translation (MT) aims to predict how good automatic transla- inghe et al., 2020b) released state-of-the-art models for sentence-level QE based on pre-trained Transtions are without comparing them to gold-standard former architectures. references (Specia et al., 2009). This is useful in As shown in the latest WMT20 QE Shared real-world scenarios (e.g. computer-aided translaTask (Specia et al., 2020), systems are incr"
2021.emnlp-demo.42,W17-4763,0,0.0156272,"for sentence-level and word-level QE; (2) train traditional machine learning models (e.g. suplight-weight and efficient sentence-level models implemented via knowledge distillation; port vector machines). With the advent of neuraland (3) a web interface for testing models based approaches, deepQuest (Ive et al., 2018) proand visualising their predictions. deepQuestvided a TensorFlow-based framework for RNNpy is available at https://github.com/ based sentence-level and document-level QE modsheffieldnlp/deepQuest-py under a els, inspired by the Predictor-Estimator approach CC BY-NC-SA licence. (Kim et al., 2017). OpenKiwi (Kepler et al., 2019) implements a common API for experiment1 Introduction ing with several feature-based and neural-based QE models. More recently, TransQuest (RanasQuality Estimation (QE) for Machine Translation (MT) aims to predict how good automatic transla- inghe et al., 2020b) released state-of-the-art models for sentence-level QE based on pre-trained Transtions are without comparing them to gold-standard former architectures. references (Specia et al., 2009). This is useful in As shown in the latest WMT20 QE Shared real-world scenarios (e.g. computer-aided translaTask (Specia"
2021.emnlp-demo.42,2020.wmt-1.118,0,0.10677,"is predicted for each version of deepQuest that covers both large and original-translation pair. For example, 0-100 for di- light-weight neural QE models, with a particular rect assessments (DA, Graham et al., 2017), or 0-1 emphasis on knowledge distillation. The main fea382 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 382–389 August 1–6, 2021. ©2021 Association for Computational Linguistics tures of deepQuest-py are: • Implementation of state-of-the-art models for sentence-level (Ranasinghe et al., 2020a) and word-level (Lee, 2020) QE; • The first implementation of light-weight sentence-level QE models using knowledge distillation (Gajbhiye et al., 2021); • Easy-to-use command-line interface and API to train and test QE models in custom datasets, as well as those from several WMT QE Shared Tasks thanks to its integration with HuggingFace Datasets (Lhoest et al., 2021); and • An online tool to try out trained models, evaluate them and visualise their predictions. Different from existing open-source toolkits in the area, our aim is to provide access to neural QE models for both researchers (via a command-line interface an"
2021.emnlp-demo.42,P15-4020,1,0.769795,"nchego,1 Abiola Obamuyide,1 Amit Gajbhiye,1 Frédéric Blain,1,2 Marina Fomicheva,1 Lucia Specia1,3 1 University of Sheffield, 2 University of Wolverhampton, 3 Imperial College London {f.alva,a.obamuyide,a.gajbhiye,m.fomicheva}@sheffield.ac.uk f.blain@wlv.ac.uk, l.specia@imperial.ac.uk Abstract for human-targeted translation error rate (HTER, Snover et al., 2006). We introduce deepQuest-py, a framework for Few open-source software is available for imtraining and evaluation of large and lightplementing QE models. QuEst (Specia et al., weight models for Quality Estimation (QE). 2013) and QuEst++ (Specia et al., 2015) were deepQuest-py provides access to (1) state-ofthe first ones, and included methods that relied the-art models based on pre-trained Transformon extracting linguistically-motivated features to ers for sentence-level and word-level QE; (2) train traditional machine learning models (e.g. suplight-weight and efficient sentence-level models implemented via knowledge distillation; port vector machines). With the advent of neuraland (3) a web interface for testing models based approaches, deepQuest (Ive et al., 2018) proand visualising their predictions. deepQuestvided a TensorFlow-based framework"
2021.emnlp-demo.42,L18-1004,0,0.0348894,"Missing"
2021.emnlp-demo.42,2020.wmt-1.122,0,0.0322837,"Quest-py, a new a single continuous score is predicted for each version of deepQuest that covers both large and original-translation pair. For example, 0-100 for di- light-weight neural QE models, with a particular rect assessments (DA, Graham et al., 2017), or 0-1 emphasis on knowledge distillation. The main fea382 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 382–389 August 1–6, 2021. ©2021 Association for Computational Linguistics tures of deepQuest-py are: • Implementation of state-of-the-art models for sentence-level (Ranasinghe et al., 2020a) and word-level (Lee, 2020) QE; • The first implementation of light-weight sentence-level QE models using knowledge distillation (Gajbhiye et al., 2021); • Easy-to-use command-line interface and API to train and test QE models in custom datasets, as well as those from several WMT QE Shared Tasks thanks to its integration with HuggingFace Datasets (Lhoest et al., 2021); and • An online tool to try out trained models, evaluate them and visualise their predictions. Different from existing open-source toolkits in the area, our aim is to provide access to neural QE models for both researchers (vi"
2021.emnlp-demo.42,2020.coling-main.445,0,0.0371303,"Quest-py, a new a single continuous score is predicted for each version of deepQuest that covers both large and original-translation pair. For example, 0-100 for di- light-weight neural QE models, with a particular rect assessments (DA, Graham et al., 2017), or 0-1 emphasis on knowledge distillation. The main fea382 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 382–389 August 1–6, 2021. ©2021 Association for Computational Linguistics tures of deepQuest-py are: • Implementation of state-of-the-art models for sentence-level (Ranasinghe et al., 2020a) and word-level (Lee, 2020) QE; • The first implementation of light-weight sentence-level QE models using knowledge distillation (Gajbhiye et al., 2021); • Easy-to-use command-line interface and API to train and test QE models in custom datasets, as well as those from several WMT QE Shared Tasks thanks to its integration with HuggingFace Datasets (Lhoest et al., 2021); and • An online tool to try out trained models, evaluate them and visualise their predictions. Different from existing open-source toolkits in the area, our aim is to provide access to neural QE models for both researchers (vi"
2021.emnlp-demo.42,2006.amta-papers.25,0,0.0727004,"Missing"
2021.emnlp-demo.42,P13-4014,1,0.86662,"Missing"
2021.emnlp-demo.42,2009.eamt-1.5,1,0.686384,"ment-level QE modsheffieldnlp/deepQuest-py under a els, inspired by the Predictor-Estimator approach CC BY-NC-SA licence. (Kim et al., 2017). OpenKiwi (Kepler et al., 2019) implements a common API for experiment1 Introduction ing with several feature-based and neural-based QE models. More recently, TransQuest (RanasQuality Estimation (QE) for Machine Translation (MT) aims to predict how good automatic transla- inghe et al., 2020b) released state-of-the-art models for sentence-level QE based on pre-trained Transtions are without comparing them to gold-standard former architectures. references (Specia et al., 2009). This is useful in As shown in the latest WMT20 QE Shared real-world scenarios (e.g. computer-aided translaTask (Specia et al., 2020), systems are increastion or online translation of social media content), where users would benefit from knowing how con- ingly relying on large pre-trained models to achieve fident they should be of the generated translations. impressive results in the different proposed tasks. QE has received increased attention in the MT com- However, their considerable size could prevent their application in scenarios where fast inference munity, with Shared Tasks being orga"
2021.emnlp-main.474,P19-4007,0,0.0270368,"ature extraction. The proliferation of many-to-many NMT (Fan et al., 2021; Ko et al., 2021) has motivated similar multilingual QE models. These multilingual QE models have exploited large pre-trained contextualized multilingual language models to achieve 3 Background and hypothesis a previously-unseen level of correlation with human judgments in recent iterations of the WMT Current state of the art QE systems (Fomicheva QE shared task. For example, the top-performing et al., 2020b; Ranasinghe et al., 2020a; Sun et al., QE model at WMT 2019 (Kepler et al., 2019) is a 2020). are built on XLM-R (Conneau et al., 2019), neural predictor-estimator model based on multilin- a contextualized language model pre-trained on gual BERT (Devlin et al., 2019), while the best QE more than 2 terabytes of filtered CommonCrawl models at WMT 2020 (Ranasinghe et al., 2020a; data (Wenzek et al., 2020). As seen in Figure 1, the Fomicheva et al., 2020b) are regression models model concatenates a pair of source and translated built on XLM-R (Conneau et al., 2019). Sun et al. sentences with a separator token in between and (2020) find that these models generalize well across appends a special CLS token to the beginning languages"
2021.emnlp-main.474,N19-1423,0,0.028628,"els. These multilingual QE models have exploited large pre-trained contextualized multilingual language models to achieve 3 Background and hypothesis a previously-unseen level of correlation with human judgments in recent iterations of the WMT Current state of the art QE systems (Fomicheva QE shared task. For example, the top-performing et al., 2020b; Ranasinghe et al., 2020a; Sun et al., QE model at WMT 2019 (Kepler et al., 2019) is a 2020). are built on XLM-R (Conneau et al., 2019), neural predictor-estimator model based on multilin- a contextualized language model pre-trained on gual BERT (Devlin et al., 2019), while the best QE more than 2 terabytes of filtered CommonCrawl models at WMT 2020 (Ranasinghe et al., 2020a; data (Wenzek et al., 2020). As seen in Figure 1, the Fomicheva et al., 2020b) are regression models model concatenates a pair of source and translated built on XLM-R (Conneau et al., 2019). Sun et al. sentences with a separator token in between and (2020) find that these models generalize well across appends a special CLS token to the beginning languages and training a single multilingual QE of the concatenated string. It then converts the model is more effective than training a bili"
2021.emnlp-main.474,2020.emnlp-main.480,1,0.733843,"techniques for QE and find that, despite their popularity in other NLP tasks, they lead to poor performance in this regression setting. We observe that a full model parameterization is required to achieve SoTA results in a regression task. However, we argue that the level of expressiveness of a model in a continuous range is unnecessary given the downstream applications of QE, and show that reframing QE as a classification problem and evaluating QE models using classification metrics would better reflect their actual performance in real-world applications. 1 Introduction of NMT training data (El-Kishky et al., 2020b,a), QE has become an important tool in performing quality control on translations from models trained on noisy training data. The performance of a QE system is usually measured by the correlation between predicted QE and human-annotated QE scores. However, the predictions of QE models are primarily used to make binary decisions (Zhou et al., 2020): only translations above a certain QE threshold would be given to a human for post-edition in a translation company, or shown to the user in an online platform. Therefore, Pearson correlation might not be the best metric to evaluate the actual perf"
2021.emnlp-main.474,2020.wmt-1.116,1,0.842537,"Missing"
2021.emnlp-main.474,W19-5401,0,0.0957682,"ce of a QE system is usually measured by the correlation between predicted QE and human-annotated QE scores. However, the predictions of QE models are primarily used to make binary decisions (Zhou et al., 2020): only translations above a certain QE threshold would be given to a human for post-edition in a translation company, or shown to the user in an online platform. Therefore, Pearson correlation might not be the best metric to evaluate the actual performance of the QE models in real-world use cases. In recent iterations of the QE shared task at the Conference on Machine Translation (WMT) (Fonseca et al., 2019; Specia et al., 2020), the topperforming QE systems have been built on large multilingual contextualized language models that were pre-trained on huge amounts of multilingual text data. Further, these QE models are multilingual and work well in zero-shot scenarios (Sun et al., 2020). This characteristic makes them very appealing for real-life scenarios because it removes the need to train one bilingual model for every pair of languages. However, these neural QE models contain millions of parameters and as such their memory and disk footprints are very large. Moreover, at inference time they a"
2021.emnlp-main.474,2020.repl4nlp-1.18,0,0.0491008,"Missing"
2021.emnlp-main.474,P84-1044,0,0.126076,"Missing"
2021.emnlp-main.474,W19-5406,0,0.0278219,"Tuan et al., 2021), without the additional step of feature extraction. The proliferation of many-to-many NMT (Fan et al., 2021; Ko et al., 2021) has motivated similar multilingual QE models. These multilingual QE models have exploited large pre-trained contextualized multilingual language models to achieve 3 Background and hypothesis a previously-unseen level of correlation with human judgments in recent iterations of the WMT Current state of the art QE systems (Fomicheva QE shared task. For example, the top-performing et al., 2020b; Ranasinghe et al., 2020a; Sun et al., QE model at WMT 2019 (Kepler et al., 2019) is a 2020). are built on XLM-R (Conneau et al., 2019), neural predictor-estimator model based on multilin- a contextualized language model pre-trained on gual BERT (Devlin et al., 2019), while the best QE more than 2 terabytes of filtered CommonCrawl models at WMT 2020 (Ranasinghe et al., 2020a; data (Wenzek et al., 2020). As seen in Figure 1, the Fomicheva et al., 2020b) are regression models model concatenates a pair of source and translated built on XLM-R (Conneau et al., 2019). Sun et al. sentences with a separator token in between and (2020) find that these models generalize well across"
2021.emnlp-main.474,2021.acl-long.66,1,0.742287,"Missing"
2021.emnlp-main.474,D19-1445,0,0.0297417,"Missing"
2021.emnlp-main.474,2020.coling-main.287,0,0.0194598,"eacher encoder and the output of the target student encoder, and 2) the original objective function. We use sentence pairs in the MLQE dataset to train the student model and experiment with the following values for N: {2, 6, 12, 18, 23, 24}. 5 Experimental settings We report results on the MLQE-PE dataset using Pearson correlation for regression and F1 for classification. 5.1 QE dataset MLQE-PE is the publicly released multilingual QE dataset used for the WMT 2020 shared task on QE (Fomicheva et al., 2020a). This dataset was built Recent knowledge distillation (KD) methods (Jiao et al., 2019; Mao et al., 2020; Sanh et al., 2019) using source sentences extracted from Wikipedia and Reddit, translated to and from English with use larger BERT models (teacher) to supervise the training of smaller BERT models (student), typi- SoTA bilingual NMT systems that were trained on publicly available parallel corpora. It contains cally with the help of the same raw text data that seven language pairs: the high-resource Englishwas used by the teacher models. Given that XLM-R was trained on more than 2 terabytes of multilin- German (En-De), English-Chinese (En-Zh), and Russian-English (Ru-En); the medium-resource"
2021.emnlp-main.474,A94-1016,0,0.238353,"Missing"
2021.emnlp-main.474,2020.acl-main.202,0,0.0401991,"Missing"
2021.emnlp-main.474,P18-2124,0,0.0513506,"Missing"
2021.emnlp-main.474,2020.wmt-1.122,0,0.0262158,"ia et al., 2018; Fonseca et al., 2019; Specia et al., 2020; Tuan et al., 2021), without the additional step of feature extraction. The proliferation of many-to-many NMT (Fan et al., 2021; Ko et al., 2021) has motivated similar multilingual QE models. These multilingual QE models have exploited large pre-trained contextualized multilingual language models to achieve 3 Background and hypothesis a previously-unseen level of correlation with human judgments in recent iterations of the WMT Current state of the art QE systems (Fomicheva QE shared task. For example, the top-performing et al., 2020b; Ranasinghe et al., 2020a; Sun et al., QE model at WMT 2019 (Kepler et al., 2019) is a 2020). are built on XLM-R (Conneau et al., 2019), neural predictor-estimator model based on multilin- a contextualized language model pre-trained on gual BERT (Devlin et al., 2019), while the best QE more than 2 terabytes of filtered CommonCrawl models at WMT 2020 (Ranasinghe et al., 2020a; data (Wenzek et al., 2020). As seen in Figure 1, the Fomicheva et al., 2020b) are regression models model concatenates a pair of source and translated built on XLM-R (Conneau et al., 2019). Sun et al. sentences with a separator token in between"
2021.emnlp-main.474,2020.coling-main.445,0,0.0329996,"ia et al., 2018; Fonseca et al., 2019; Specia et al., 2020; Tuan et al., 2021), without the additional step of feature extraction. The proliferation of many-to-many NMT (Fan et al., 2021; Ko et al., 2021) has motivated similar multilingual QE models. These multilingual QE models have exploited large pre-trained contextualized multilingual language models to achieve 3 Background and hypothesis a previously-unseen level of correlation with human judgments in recent iterations of the WMT Current state of the art QE systems (Fomicheva QE shared task. For example, the top-performing et al., 2020b; Ranasinghe et al., 2020a; Sun et al., QE model at WMT 2019 (Kepler et al., 2019) is a 2020). are built on XLM-R (Conneau et al., 2019), neural predictor-estimator model based on multilin- a contextualized language model pre-trained on gual BERT (Devlin et al., 2019), while the best QE more than 2 terabytes of filtered CommonCrawl models at WMT 2020 (Ranasinghe et al., 2020a; data (Wenzek et al., 2020). As seen in Figure 1, the Fomicheva et al., 2020b) are regression models model concatenates a pair of source and translated built on XLM-R (Conneau et al., 2019). Sun et al. sentences with a separator token in between"
2021.emnlp-main.474,W18-6451,1,0.891775,"Missing"
2021.emnlp-main.474,2009.eamt-1.5,1,0.764099,"Missing"
2021.emnlp-main.474,2020.aacl-main.39,1,0.923674,"r post-edition in a translation company, or shown to the user in an online platform. Therefore, Pearson correlation might not be the best metric to evaluate the actual performance of the QE models in real-world use cases. In recent iterations of the QE shared task at the Conference on Machine Translation (WMT) (Fonseca et al., 2019; Specia et al., 2020), the topperforming QE systems have been built on large multilingual contextualized language models that were pre-trained on huge amounts of multilingual text data. Further, these QE models are multilingual and work well in zero-shot scenarios (Sun et al., 2020). This characteristic makes them very appealing for real-life scenarios because it removes the need to train one bilingual model for every pair of languages. However, these neural QE models contain millions of parameters and as such their memory and disk footprints are very large. Moreover, at inference time they are often more computationally expensive than the upstream neural machine translation (NMT) models, making them unsuitable for deployment in applications with low inference latency requirements or on devices with disk or memory constraints. In this paper we explore applying compressio"
2021.emnlp-main.474,D19-1441,0,0.0334585,"Missing"
2021.emnlp-main.474,D19-1374,0,0.0449147,"Missing"
2021.emnlp-main.474,W18-5446,0,0.0805585,"Missing"
2021.emnlp-main.474,2020.lrec-1.494,1,0.691615,"hypothesis a previously-unseen level of correlation with human judgments in recent iterations of the WMT Current state of the art QE systems (Fomicheva QE shared task. For example, the top-performing et al., 2020b; Ranasinghe et al., 2020a; Sun et al., QE model at WMT 2019 (Kepler et al., 2019) is a 2020). are built on XLM-R (Conneau et al., 2019), neural predictor-estimator model based on multilin- a contextualized language model pre-trained on gual BERT (Devlin et al., 2019), while the best QE more than 2 terabytes of filtered CommonCrawl models at WMT 2020 (Ranasinghe et al., 2020a; data (Wenzek et al., 2020). As seen in Figure 1, the Fomicheva et al., 2020b) are regression models model concatenates a pair of source and translated built on XLM-R (Conneau et al., 2019). Sun et al. sentences with a separator token in between and (2020) find that these models generalize well across appends a special CLS token to the beginning languages and training a single multilingual QE of the concatenated string. It then converts the model is more effective than training a bilingual pre-processed string into a sequence of embedding model for every language direction. Unfortunately, vectors using a pre-trained emb"
2021.emnlp-main.474,2020.emnlp-main.633,0,0.0151504,"ora. It contains cally with the help of the same raw text data that seven language pairs: the high-resource Englishwas used by the teacher models. Given that XLM-R was trained on more than 2 terabytes of multilin- German (En-De), English-Chinese (En-Zh), and Russian-English (Ru-En); the medium-resource gual text data, it would be computationally difficult Romanian–English (Ro-En) and Estonian–English to adapt the KD techniques to XLM-R. Instead, we experiment with a simplified KD setup inspired by (Et-En); and the low-resource Sinhala–English (SiEn) and Nepali–English (Ne-En). Each pair of senXu et al. (2020). tences was manually annotated for quality using a Module replacement We explore whether it is 0–100 direct assessment (DA) scheme as shown in effective to compress N encoder layers into a single table 2. A z-normalized version of these scores is encoder layer. As seen in Figure 2, we use the top used directly for regression. N layers of a fine-tuned QE model to supervise As previously mentioned, since the most comthe training of one encoder layer in a smaller QE mon use case of the QE is to make binary decisions model. For the student QE model, we randomly based on predicted QE scores (Zhou"
2021.emnlp-main.536,P19-1126,0,0.0384961,"Missing"
2021.emnlp-main.536,2021.eacl-main.233,0,0.127872,"cy. It is non-trivial to find an optimal translation strategy, as there is generally a rivalry between the two objectives, i.e. reading more source words before translating leads to better translation quality, but it in turn results in higher latency due to the longer time for reading. Conventional Wait-k policies (Ma et al., 2019) put a hard limitation over the buffer size k 1 , which guarantees low latency but weakens flexibility and scalability when handling long and complicated language pairs. Alternatively, reinforcement learning (RL) approaches (Gu et al., 2017; Satija and Pineau, 2016; Arthur et al., 2021) learn a dynamic policy using a combined reward of a quality metric like the BLEU score and AL (average lagging)2 . However, the poor sample efficiency make it very difficult to learn a robust SiMT model with RL. In this paper we propose a generative framework with a latent variable that dynamically decides between the actions of read or translate at every time step, enabling the formulation of SiMT as a structural sequence-to-sequence learning task. Figure 1 depicts the examples of possible translation paths of different models. Wait-k only explores one hypothesis, while adaptive wait-k ensem"
2021.emnlp-main.536,2020.emnlp-main.184,1,0.924428,"asets are chosen for testing BLEU (Papineni et al., 2002) and AL (average lagging) (Ma et al., 2019). For GSiMT models, we empirically fix λ = 3 for all the experiments, and use ζ as the free parameter to achieve different AL. For Multi30K (Elliott et al., 2016), we use all three language pairs EN→FR, EN→DE and EN→CZ with the image data from Flickr30k as extra modality and flickr2016 as test dataset. We build multimodal models with the goal of testing the generalisation ability of the generative models with extra modalities. To that end, we concatenate the object detection features applied in Caglayan et al. (2020) into the state representation Si,j and maintain the rest of the neural network the same as the unimodal SiMT. The other models (RL, Wait-k and Adpative Wait-k) incorporate the same features as well. Here, as the size of data is small, we apply a smaller Transformers with 4 layers, 4 heads, 512 model dimension and 1024 for linear connection. 4.2 Translation Quality & Latency Table 1 shows the SiMT performance for the benchmark models and our proposed generative models on the WMT15 DE→EN dataset. RL is our implementation of Gu et al. (2017) with policy gradient method. All the numbers for Wait-"
2021.emnlp-main.536,N13-1073,0,0.238471,"nsducer is not designed for SiMT. Because it is optimised by the crossentropy of target words, it naturally prefers read actions over translate actions in order to see more contexts before translation, which intuitively can result in better translation quality but high latency. Here, we propose to extend the neural transducer framework to modern Transformer-based translation models (Vaswani et al., 2017), and introduce a re-parameterised Poisson distribution to regularise the latency (i.e. how many source words are read before translating a target word). Inspired by the fast-alignment work by Dyer et al. (2013), the translation model generally favors word alignments distributed close to the diagonal. We hypothesise that the optimal sequence of translate actions in SiMT is also located close to the diagonal. Thus the Poisson prior acts as context-independent regularisation on the buffer size proportional to the distance between the current position and the diagonal. This ensures that the number of read source words will not grow indefinitely without translating any target words, while the soft boundary, due to the regularisation, still allows the model to consider complicated/long simultaneous transl"
2021.emnlp-main.536,W16-3210,1,0.887869,"Missing"
2021.emnlp-main.536,E17-1099,0,0.349888,"ween the translation quality and the latency. It is non-trivial to find an optimal translation strategy, as there is generally a rivalry between the two objectives, i.e. reading more source words before translating leads to better translation quality, but it in turn results in higher latency due to the longer time for reading. Conventional Wait-k policies (Ma et al., 2019) put a hard limitation over the buffer size k 1 , which guarantees low latency but weakens flexibility and scalability when handling long and complicated language pairs. Alternatively, reinforcement learning (RL) approaches (Gu et al., 2017; Satija and Pineau, 2016; Arthur et al., 2021) learn a dynamic policy using a combined reward of a quality metric like the BLEU score and AL (average lagging)2 . However, the poor sample efficiency make it very difficult to learn a robust SiMT model with RL. In this paper we propose a generative framework with a latent variable that dynamically decides between the actions of read or translate at every time step, enabling the formulation of SiMT as a structural sequence-to-sequence learning task. Figure 1 depicts the examples of possible translation paths of different models. Wait-k only explo"
2021.emnlp-main.536,D16-1073,0,0.0131231,"ck that hinders the applicability of RL in structural sequence-to-sequence learning. The proposed GSiMT model combines the merits of both the Wait-k policies and RL. Deep learning with structures has been explored in many NLP tasks, especially for sequence-tosequence learning. Kim et al. (2017) implements structural dependencies on attention networks, which gives the ability to attend to partial segmentations or subtrees without changing the sequence-to-sequence structure. Tran et al. (2016) parameterises the transition and emission probabilities of an HMM with explicit neural components, and Jiang et al. (2016) applies deep structural latent variables to implement the dependency model with valence (Klein and Manning, 2004) and integrates out all the structures in end-to-end learning. Our • A Transformer-based neural transducer model GSiMT model is based on neural transducer model. for simultaneous machine translation. Previously, Graves (2012) presents an RNN-based neural transducer for phoneme recognition, and Yu • Poisson prior for effectively balancing the et al. (2016) explores an LSTM-based neural transtranslation quality and latency. ducer for MT. The uni-directional variant model of • State-o"
2021.emnlp-main.536,P04-1061,0,0.0606634,"combines the merits of both the Wait-k policies and RL. Deep learning with structures has been explored in many NLP tasks, especially for sequence-tosequence learning. Kim et al. (2017) implements structural dependencies on attention networks, which gives the ability to attend to partial segmentations or subtrees without changing the sequence-to-sequence structure. Tran et al. (2016) parameterises the transition and emission probabilities of an HMM with explicit neural components, and Jiang et al. (2016) applies deep structural latent variables to implement the dependency model with valence (Klein and Manning, 2004) and integrates out all the structures in end-to-end learning. Our • A Transformer-based neural transducer model GSiMT model is based on neural transducer model. for simultaneous machine translation. Previously, Graves (2012) presents an RNN-based neural transducer for phoneme recognition, and Yu • Poisson prior for effectively balancing the et al. (2016) explores an LSTM-based neural transtranslation quality and latency. ducer for MT. The uni-directional variant model of • State-of-the-art SiMT results (BLEU & AL) Yu et al. (2016) is similar to our proposed GSiMT on benchmark datasets, and th"
2021.emnlp-main.536,P02-1040,0,0.109422,"rs with the same parameters in Vaswani et al. (2017) as the backbone. Instead of updating all the parameters from scratch, we pretrain the encoder and decoder (both are uni-directional Transformers) as consecutive NMT model for 10 epochs. Then we freeze the Transformers parameters, and apply 256 batch size and 1e-4 learning rate for training the generative models. On PyTorch (Paszke et al., 2019) platform, each epoch takes around 40 minutes with Adam (Kingma and Ba, 2014) on single V100 GPU4 . The checkpoints with best performance in 5 runs on development datasets are chosen for testing BLEU (Papineni et al., 2002) and AL (average lagging) (Ma et al., 2019). For GSiMT models, we empirically fix λ = 3 for all the experiments, and use ζ as the free parameter to achieve different AL. For Multi30K (Elliott et al., 2016), we use all three language pairs EN→FR, EN→DE and EN→CZ with the image data from Flickr30k as extra modality and flickr2016 as test dataset. We build multimodal models with the goal of testing the generalisation ability of the generative models with extra modalities. To that end, we concatenate the object detection features applied in Caglayan et al. (2020) into the state representation Si,j"
2021.emnlp-main.536,P16-1162,0,0.0185986,". It is worth mentioning that the Poisson prior distribution is only employed for regularising the training, but it is not required at testing time, as the translate action distributions have implicitly learned to translate the target words with low latency. Hence, the lengths of the sequences m and n are known during training, but they are not used at test time. During test, we simply use the average length ratio of the whole dataset. 4 Experiments 4.1 Datasets & Settings For WMT15 DE→EN, we follow the exactly the same preprocessing procedure as in (Ma et al., 2019; Zheng et al., 2020). BPE (Sennrich et al., 2016) is applied to achieve 35K vocabulary and we process 4.5M parallel corpus for training, 3K sentences of newstest-2013 for validation and 2,169 sentences of newstest-2015 for testing. Following (Ma et al., 2019; Zheng et al., 2020), we apply the base version of Transformers with the same parameters in Vaswani et al. (2017) as the backbone. Instead of updating all the parameters from scratch, we pretrain the encoder and decoder (both are uni-directional Transformers) as consecutive NMT model for 10 epochs. Then we freeze the Transformers parameters, and apply 256 batch size and 1e-4 learning rat"
2021.emnlp-main.536,W16-5907,0,0.0284719,"on word alignments as the oracle to improve the learning. However, the high variance of the estimator is still a bottleneck that hinders the applicability of RL in structural sequence-to-sequence learning. The proposed GSiMT model combines the merits of both the Wait-k policies and RL. Deep learning with structures has been explored in many NLP tasks, especially for sequence-tosequence learning. Kim et al. (2017) implements structural dependencies on attention networks, which gives the ability to attend to partial segmentations or subtrees without changing the sequence-to-sequence structure. Tran et al. (2016) parameterises the transition and emission probabilities of an HMM with explicit neural components, and Jiang et al. (2016) applies deep structural latent variables to implement the dependency model with valence (Klein and Manning, 2004) and integrates out all the structures in end-to-end learning. Our • A Transformer-based neural transducer model GSiMT model is based on neural transducer model. for simultaneous machine translation. Previously, Graves (2012) presents an RNN-based neural transducer for phoneme recognition, and Yu • Poisson prior for effectively balancing the et al. (2016) explo"
2021.emnlp-main.536,2020.acl-main.254,0,0.274926,"erage lagging (AL). Our contributions can be summarised: 2 Related Work Conventional SiMT methods are based on heuristic waiting criteria (Cho and Esipova, 2016) or fixed buffering strategy (Ma et al., 2019) to trade off the translation quality for lower latency. Although the heuristic approaches are simple and straightforward, they lack of scalability and cannot generalise well on longer sequences. There is also a bulk of work attempting to improve the attention mechanism (Arivazhagan et al., 2019) and re-translation strategies (Niehues et al., 2018) for better translation quality. Recently, Zheng et al. (2020) extends the fixed Wait-k policies into adaptive version and ensembles multiple models with lower latency to improve the performance, but one still needs to choose a hard boundary on the maximum value of k. By contrast, our GSiMT model considers all the possible paths with a soft boundary modelled by Poisson distribution, which leads to a more flexible balance between quality and latency. RL has been explored (Gu et al., 2017) to learn an agent that dynamically decides to read or translate conditioned on different translation contexts. Arthur et al. (2021) further applies extra knowledge on wo"
2021.findings-acl.443,Q17-1010,0,0.011125,"Missing"
2021.findings-acl.443,D18-2029,0,0.0254465,"Missing"
2021.findings-acl.443,P19-1106,0,0.199164,"Missing"
2021.findings-acl.443,N18-1149,0,0.0658452,"Missing"
2021.findings-acl.452,2020.acl-main.747,0,0.0742845,"Missing"
2021.findings-acl.452,W13-2305,0,0.0345617,"acted from Wikipedia translated to and from English for a total of six language pairs: English–German (En-De),3 English–Chinese (En-Zh), Romanian–English (Ro-En), Estonian– English (Et-En), Sinhala–English (Si-En) and Nepali–English (Ne-En). Each translation was produced with a SoTA Transformer-based NMT model and manually annotated for quality using Language Sentences Estonian Romanian Sinhala Nepalese English 25,176 372,690 139,406 85,343 1,563,519 Table 2: Number of sentences extracted from Wikipedia for data augmentation. an annotation scheme inspired by the Direct Assessment methodology (Graham et al., 2013). The scores are produced on a continuous scale indicating perceived translation quality in 0-100. For each language pair, this dataset contains partitions for training (7K), dev (1K), and test (1K). Distilled dataset. Monolingual data for data augmentation was sampled from Wikipedia following the procedure described in Fomicheva et al. (2020) to preserve the domain of the MLQE dataset. Specifically, we sampled documents from Wikipedia for English, Estonian, Romanian, Sinhalese and Nepalese and selected the top 100 documents containing the largest number of sentences that are: (i) in the inten"
2021.findings-acl.452,C18-1266,1,0.903172,"Missing"
2021.findings-acl.452,2020.findings-emnlp.372,0,0.042971,"nd prohibits deployment on client machines with limited resources. Making models based on pre-trained representations smaller and more usable in practice is an active area of research. One approach is Knowledge Distillation (KD), aiming to extract knowledge from a top-performing large model (the teacher) into a smaller (in terms of memory print, computational power and prediction latency) yet wellperforming model (the student) (Hinton et al., 2015; Gou et al., 2020). KD techniques have been used to make BERT and similar models smaller. For example, DistilBERT (Sanh et al., 2019) and TinyBERT (Jiao et al., 2020) follow the same general architecture as the teacher BERT, but with a reduced number of layers. However, these student models are also based on Transformers and, as such, they still have too large memory and disk footprints. For instance, the number of parameters in the multilingual DistilBERT-based TransQuest model for QE (Ranasinghe et al., 2020) is 135M. In this paper, we propose to distill the QE model directly, where the student architecture can be completely different from that of the teacher. Namely, we distill a large and powerful QE model based on XLM-Roberta into a small RNN-based mo"
2021.findings-acl.452,P19-3020,0,0.036983,"Missing"
2021.findings-acl.452,W17-4763,0,0.0289451,"on 2. We used a random subset of 100K sentences from Wikipedia to train the student model for each of the language pairs except for EtEn where the total amount of collected in-domain monolingual data is 25K. Models. As teachers, we use pre-trained models from TransQuest (TQTEACHER ), one of the winning submissions in the WMT2020 QE Shared Task, which we fine-tuned on the MLQE dataset. For noise filtering, we train five teacher models with random initialisation. As students, we use BiRNN models from DeepQuest (Ive et al., 2018). We also compare our results against the PredictorEstimator model (Kim et al., 2017; Kepler et al., 2019), the baseline at the WMT2020 QE Shared Task, and TransQuest models using multilingual DistilBERT.6 4 Results Table 3 shows the Pearson correlation with human judgments on the test partition of the MLQE dataset for different models and specifies the type of data used for training.7 The correlation for the student models (BiRNNSTUDENT∗ ) does not reach the performance of TQTEACHER . Smaller models may lack representation power for modeling cross-lingual tasks such as QE. Also, distillation for regression is more challenging, as discussed in Section 2. However, training on"
2021.findings-acl.452,N19-4009,0,0.0285837,"he length between 50 and 150 characters. Table 2 shows the total amount of sentences in the monolingual Wikipedia dataset collected for data augmentation. To test the impact of data domain on the performance of the student QE models, we also collect out-of-domain data for the Et-En language pair. The out-of-domain data is sampled from Common Crawl. We use the version of Common Crawl distributed by the WMT2018 News Translation Task4 . The total amount of sentences in this dataset is 100,779,314. To translate the data, we used the same MT models that generated the test data, built with fairseq (Ott et al., 2019) and made available by the WMT2020 QE Shared Task organisers.5 Sentences that were part of the training data for the MT models or part of the MLQE dataset were excluded. We generate quality predictions for the remaining sentences using the teacher models, as 2 Here we use ensemble only as a way of estimating the error in the predictions and leave distillation based on ensemble predictions to future work. 3 We skip this language pair as the performance of the teacher model for it is too weak. 4 http://www.statmt.org/wmt18/translati on-task.html 5 https://github.com/facebookresearch/ mlqe/tree/m"
2021.findings-acl.452,2020.coling-main.445,0,0.0826241,"nt, computational power and prediction latency) yet wellperforming model (the student) (Hinton et al., 2015; Gou et al., 2020). KD techniques have been used to make BERT and similar models smaller. For example, DistilBERT (Sanh et al., 2019) and TinyBERT (Jiao et al., 2020) follow the same general architecture as the teacher BERT, but with a reduced number of layers. However, these student models are also based on Transformers and, as such, they still have too large memory and disk footprints. For instance, the number of parameters in the multilingual DistilBERT-based TransQuest model for QE (Ranasinghe et al., 2020) is 135M. In this paper, we propose to distill the QE model directly, where the student architecture can be completely different from that of the teacher. Namely, we distill a large and powerful QE model based on XLM-Roberta into a small RNN-based model. Existing work along these lines has applied KD mainly to classification tasks (Tang et al., 2019; Sun et al., 2019). We instead explore this approach in the context of regression. In contrast to classification, where KD provides useful information on the output distribution of incorrect classes, for regression the teacher predictions are point"
2021.findings-acl.452,D19-1441,0,0.0645368,"Missing"
2021.findings-emnlp.271,P17-1074,0,0.0259986,"rds – noisy or not – with respect to the training. different levels of noise (number of noisy words n ∈ {1, 2, 4, 6, 10}) during hyperparameter tuning. See Appendix B for more details. Training and Evaluation We use ADAM (Kingma and Ba, 2015) as the optimizer and adopt the noam learning rate scheduler (Vaswani et al., 2017) with a warm-up of 8000 steps. The training batch size is 64. Models are evaluated using the METEOR score (Denkowski and Lavie, 2014), which is the main metric for multimodal machine translation (Barrault et al., 2018). For the evaluation of error correction, we use ERRANT (Bryant et al., 2017) to compute the F0.5 score. During evaluation, we select the checkpoint with the best performance on the development set and generate the translation and correction using beam search of size 12. All models are implemented using nmtpytorch4 and pysimt5 . Each model is run with three random seeds and the average results are reported. Each run takes approximately 2 hours to train on an RTX 2080 Ti GPU. 5 Results 5.1 Testing for Robustness to Noise We first evaluate the robustness of standard NMT and MMT models trained on clean data by testing on the noise-injected data. This setting represents re"
2021.findings-emnlp.271,2020.emnlp-main.184,1,0.9024,"texts. 1 Introduction jeune chanson jeune enfant young [v] Abstract young song Zhenhao Li, Marek Rei, Lucia Specia Language and Multimodal AI (LAMA) Lab, Imperial College London {zhenhao.li18, marek.rei, l.specia}@imperial.ac.uk Figure 1: As showed by Caglayan et al. (2019), multimodality can help translate unknown words, but fail when there is noise in the input. The misspelled word “song” is correctly translated as “enfant” (child) when it is replaced with an unknown token, but translated literally as “chanson” (song) otherwise. is incomplete (Caglayan et al., 2019; Imankulova et al., 2020; Caglayan et al., 2020). However, as exemplified by Caglayan et al. (2019) (Figure 1), an MMT model trained on clean data was not able to handle noise. When the word “son” was misspelled as “song”, the model disregarded the visual information and used the literal translation “chanson”. The MMT model attended to the relevant region in the image and generated the intended translation “enfant” only when the noise was masked by a placeholder in the input, imitating an out-ofvocabulary (OOV) example. Neural Machine Translation (NMT) has been shown to be very sensitive to noise (Belinkov and Bisk, 2018; Michel and Neubig,"
2021.findings-emnlp.271,N19-1422,1,0.790202,"on, we describe a novel error correction training regime that can be used as an auxiliary task to further improve translation robustness. Experiments on EnglishFrench and English-German translation show that both multimodal and error correction components improve model robustness to noisy texts, while still retaining translation quality on clean texts. 1 Introduction jeune chanson jeune enfant young [v] Abstract young song Zhenhao Li, Marek Rei, Lucia Specia Language and Multimodal AI (LAMA) Lab, Imperial College London {zhenhao.li18, marek.rei, l.specia}@imperial.ac.uk Figure 1: As showed by Caglayan et al. (2019), multimodality can help translate unknown words, but fail when there is noise in the input. The misspelled word “song” is correctly translated as “enfant” (child) when it is replaced with an unknown token, but translated literally as “chanson” (song) otherwise. is incomplete (Caglayan et al., 2019; Imankulova et al., 2020; Caglayan et al., 2020). However, as exemplified by Caglayan et al. (2019) (Figure 1), an MMT model trained on clean data was not able to handle noise. When the word “son” was misspelled as “song”, the model disregarded the visual information and used the literal translation"
2021.findings-emnlp.271,W16-2359,0,0.0243152,"l., 2020). In our case, the extra modality is given as visual features from an image network to complement the textual context. In standard MMT, these features can be fused with the 2 Background and Related Work textual representation by simple operations such Robust NMT Although NMT models can as concatenation (Caglayan et al., 2016), hidden achieve high performance on clean data, they are states initialization (Calixto and Liu, 2017), or via very brittle to non-standard inputs, such as noisy attention mechanisms (Libovický and Helcl, 2017; texts (Belinkov and Bisk, 2018). Different types of Calixto et al., 2016, 2017; Yao and Wan, 2020) and noisy data have been proposed to test translation latent variables (Calixto et al., 2019). robustness, e.g. synthetic word perturbations (BeRecent research has shown that the extra modallinkov and Bisk, 2018), grammatical errors (Anas- ity helps translation, especially when the input is intasopoulos et al., 2019), and user-generated texts complete (Caglayan et al., 2019, 2020; Imankulova from social platform (Michel and Neubig, 2018; Li et al., 2020) or ambiguous (Ive et al., 2019; Wu et al., 2019; Specia et al., 2020). et al., 2019b). Wu et al. (2019a) hinted at"
2021.findings-emnlp.271,D17-1105,0,0.0186055,"for the translation task. MMT Multimodal machine translation extends the framework of NMT by incorporating extra modalities, e.g. image (Specia et al., 2016a) or audio (Sulubacak et al., 2020). In our case, the extra modality is given as visual features from an image network to complement the textual context. In standard MMT, these features can be fused with the 2 Background and Related Work textual representation by simple operations such Robust NMT Although NMT models can as concatenation (Caglayan et al., 2016), hidden achieve high performance on clean data, they are states initialization (Calixto and Liu, 2017), or via very brittle to non-standard inputs, such as noisy attention mechanisms (Libovický and Helcl, 2017; texts (Belinkov and Bisk, 2018). Different types of Calixto et al., 2016, 2017; Yao and Wan, 2020) and noisy data have been proposed to test translation latent variables (Calixto et al., 2019). robustness, e.g. synthetic word perturbations (BeRecent research has shown that the extra modallinkov and Bisk, 2018), grammatical errors (Anas- ity helps translation, especially when the input is intasopoulos et al., 2019), and user-generated texts complete (Caglayan et al., 2019, 2020; Imankulo"
2021.findings-emnlp.271,P17-1175,0,0.0359492,"Missing"
2021.findings-emnlp.271,P19-1642,0,0.0184778,"context. In standard MMT, these features can be fused with the 2 Background and Related Work textual representation by simple operations such Robust NMT Although NMT models can as concatenation (Caglayan et al., 2016), hidden achieve high performance on clean data, they are states initialization (Calixto and Liu, 2017), or via very brittle to non-standard inputs, such as noisy attention mechanisms (Libovický and Helcl, 2017; texts (Belinkov and Bisk, 2018). Different types of Calixto et al., 2016, 2017; Yao and Wan, 2020) and noisy data have been proposed to test translation latent variables (Calixto et al., 2019). robustness, e.g. synthetic word perturbations (BeRecent research has shown that the extra modallinkov and Bisk, 2018), grammatical errors (Anas- ity helps translation, especially when the input is intasopoulos et al., 2019), and user-generated texts complete (Caglayan et al., 2019, 2020; Imankulova from social platform (Michel and Neubig, 2018; Li et al., 2020) or ambiguous (Ive et al., 2019; Wu et al., 2019; Specia et al., 2020). et al., 2019b). Wu et al. (2019a) hinted at the posThe most common approach to improve transla- sibility of multimodality helping NMT in dealing tion robustness is"
2021.findings-emnlp.271,P19-1425,0,0.0277514,"Missing"
2021.findings-emnlp.271,2020.acl-main.529,0,0.0670326,"Missing"
2021.findings-emnlp.271,P18-1163,0,0.0278474,"duce the types of noise injected and the error correction training method. In Section 4, we describe our experiment settings, with experiment results in Section 5, and further analysis in Section 6. data is often injected with different types of artificial noise, e.g. random word perturbations like character insertion/deletion/substitution (Belinkov and Bisk, 2018; Karpukhin et al., 2019; Passban et al., 2020; Xu et al., 2021), noise generated via back-translation (Berard et al., 2019; Vaibhav et al., 2019; Li and Specia, 2019), and adversarial examples generated by white-box generator model (Cheng et al., 2018, 2019, 2020). Even though this method has been shown to improve NMT performance on noisy data, the types of noise used thus far are not common in real data. For example, it would be highly unlikely for human authors to misspell the word “robust” as “zobust”, but such random transformations are used when synthesizing noisy training data for MT. In addition, back-translation paraphrases the texts to introduce noise, however such noise is less realistic as human-generated errors, which include mispellings and grammatical errors. In adversarial approaches for other NLP tasks, Ribeiro et al. (2020"
2021.findings-emnlp.271,W17-4718,1,0.93263,"cor ) = 1 |D| 1 |D| X − log P (y|x0 ; θmt ) (x0 ,y)∈D X − log P (x|x0 ; θcor ) (x0 ,x)∈D L(θ) = Lmt (θmt ) + λLcor (θcor ) (2) where λ ≥ 0 is the factor that controls the weight of the error correction loss, and D represents the noise-injected data consisting of triples in the form of (x, x0 , y). 4 4.1 Experiments Datasets We experiment with the Multi30K dataset (Elliott et al., 2016), using both the En-Fr and En-De language pairs. This is the standard dataset for MMT N Y and has been used in all open challenges on the P (y|x0 ; θmt ) = P (yt |y1:t−1 , x0 ; θmt ) topic (Specia et al., 2016b; Elliott et al., 2017a; t=1 Barrault et al., 2018). Following Caglayan et al. (1) M Y (2019), we use both the train and valid splits as our P (x|x0 ; θcor ) = P (xt |x1:t−1 , x0 ; θcor ) training set. The test2016-flickr set is used as our t=1 development set for checkpoint selection. For evalThe θmt represents parameters for the translation uation, we test the models on both test2017-flickr component and the θcor represents parameters and test2017-mscoco sets (Elliott et al., 2017b). We for the error correction component, with θmt = use a word-level vocabulary and build vocabular{θenc , θmt_dec }, θcor = {θenc ,"
2021.findings-emnlp.271,W19-5303,0,0.0386482,"Missing"
2021.findings-emnlp.271,D19-5543,1,0.891793,"e inputs often leading to mistranslations. To improve the robustness of NMT models, current research mostly focuses on adapting the model to noisy texts via methods such as fine-tuning (Michel and Neubig, 2018; Alam and Anastasopoulos, 2020), noiseGiven that the visual modality has been shown injection (Belinkov and Bisk, 2018; Cheng et al., to help predict unknown words, we investi2018; Karpukhin et al., 2019), and data augmenta- gate whether adding multimodal information to tion through back-translation (Berard et al., 2019; adaption-based methods would further improve Vaibhav et al., 2019; Li and Specia, 2019), etc. In translation robustness. To answer this question, these approaches, the translation model is trained we build MMT models in conjunction with noise or fine-tuned on the noisy data so that it can learn injection techniques and investigate their behaviour from the noise. However, methods using extra during training and inference on both noisy and context to help translate noisy texts have not been clean data. To further improve robustness, we exinvestigated. tend the current adversarial training method (i.e., Studies in Multimodal Machine Translation training NMT models on noisy texts) a"
2021.findings-emnlp.271,W16-3210,1,0.895537,"Missing"
2021.findings-emnlp.271,P17-2031,0,0.0192223,"ng extra modalities, e.g. image (Specia et al., 2016a) or audio (Sulubacak et al., 2020). In our case, the extra modality is given as visual features from an image network to complement the textual context. In standard MMT, these features can be fused with the 2 Background and Related Work textual representation by simple operations such Robust NMT Although NMT models can as concatenation (Caglayan et al., 2016), hidden achieve high performance on clean data, they are states initialization (Calixto and Liu, 2017), or via very brittle to non-standard inputs, such as noisy attention mechanisms (Libovický and Helcl, 2017; texts (Belinkov and Bisk, 2018). Different types of Calixto et al., 2016, 2017; Yao and Wan, 2020) and noisy data have been proposed to test translation latent variables (Calixto et al., 2019). robustness, e.g. synthetic word perturbations (BeRecent research has shown that the extra modallinkov and Bisk, 2018), grammatical errors (Anas- ity helps translation, especially when the input is intasopoulos et al., 2019), and user-generated texts complete (Caglayan et al., 2019, 2020; Imankulova from social platform (Michel and Neubig, 2018; Li et al., 2020) or ambiguous (Ive et al., 2019; Wu et al"
2021.findings-emnlp.271,2020.wmt-1.70,0,0.0193185,"slation quality on clean texts. 1 Introduction jeune chanson jeune enfant young [v] Abstract young song Zhenhao Li, Marek Rei, Lucia Specia Language and Multimodal AI (LAMA) Lab, Imperial College London {zhenhao.li18, marek.rei, l.specia}@imperial.ac.uk Figure 1: As showed by Caglayan et al. (2019), multimodality can help translate unknown words, but fail when there is noise in the input. The misspelled word “song” is correctly translated as “enfant” (child) when it is replaced with an unknown token, but translated literally as “chanson” (song) otherwise. is incomplete (Caglayan et al., 2019; Imankulova et al., 2020; Caglayan et al., 2020). However, as exemplified by Caglayan et al. (2019) (Figure 1), an MMT model trained on clean data was not able to handle noise. When the word “son” was misspelled as “song”, the model disregarded the visual information and used the literal translation “chanson”. The MMT model attended to the relevant region in the image and generated the intended translation “enfant” only when the noise was masked by a placeholder in the input, imitating an out-ofvocabulary (OOV) example. Neural Machine Translation (NMT) has been shown to be very sensitive to noise (Belinkov and Bisk,"
2021.findings-emnlp.271,W18-6326,0,0.0563372,"Missing"
2021.findings-emnlp.271,P19-1653,1,0.830617,"(Libovický and Helcl, 2017; texts (Belinkov and Bisk, 2018). Different types of Calixto et al., 2016, 2017; Yao and Wan, 2020) and noisy data have been proposed to test translation latent variables (Calixto et al., 2019). robustness, e.g. synthetic word perturbations (BeRecent research has shown that the extra modallinkov and Bisk, 2018), grammatical errors (Anas- ity helps translation, especially when the input is intasopoulos et al., 2019), and user-generated texts complete (Caglayan et al., 2019, 2020; Imankulova from social platform (Michel and Neubig, 2018; Li et al., 2020) or ambiguous (Ive et al., 2019; Wu et al., 2019; Specia et al., 2020). et al., 2019b). Wu et al. (2019a) hinted at the posThe most common approach to improve transla- sibility of multimodality helping NMT in dealing tion robustness is to train the model on noisy data, with natural noise stemming from the speech recogwhich is referred to as adversarial training. Since nition system used as a first step in their pipeline parallel data with noisy source sentences and clean approach to speech translations from videos. Their translations is difficult to obtain, the clean training results, however, were inconclusive. 1 Salesky e"
2021.findings-emnlp.271,D19-5506,0,0.248603,"ulary (OOV) example. Neural Machine Translation (NMT) has been shown to be very sensitive to noise (Belinkov and Bisk, 2018; Michel and Neubig, 2018; Ebrahimi et al., 2018), with even small perturbations in the inputs often leading to mistranslations. To improve the robustness of NMT models, current research mostly focuses on adapting the model to noisy texts via methods such as fine-tuning (Michel and Neubig, 2018; Alam and Anastasopoulos, 2020), noiseGiven that the visual modality has been shown injection (Belinkov and Bisk, 2018; Cheng et al., to help predict unknown words, we investi2018; Karpukhin et al., 2019), and data augmenta- gate whether adding multimodal information to tion through back-translation (Berard et al., 2019; adaption-based methods would further improve Vaibhav et al., 2019; Li and Specia, 2019), etc. In translation robustness. To answer this question, these approaches, the translation model is trained we build MMT models in conjunction with noise or fine-tuned on the noisy data so that it can learn injection techniques and investigate their behaviour from the noise. However, methods using extra during training and inference on both noisy and context to help translate noisy texts h"
2021.findings-emnlp.271,2021.emnlp-main.576,0,0.0415779,"al., 2019; Wu et al., 2019; Specia et al., 2020). et al., 2019b). Wu et al. (2019a) hinted at the posThe most common approach to improve transla- sibility of multimodality helping NMT in dealing tion robustness is to train the model on noisy data, with natural noise stemming from the speech recogwhich is referred to as adversarial training. Since nition system used as a first step in their pipeline parallel data with noisy source sentences and clean approach to speech translations from videos. Their translations is difficult to obtain, the clean training results, however, were inconclusive. 1 Salesky et al. (2021) investigate the robustness of Codes are available at https://github.com/ Nickeilf/Visual-Cues-Error-Correction open-vocabulary translation by representing texts 3154 clean edit-distance homophone keyboard a pink flower is starting to bloom . a pink flower is staring to loom . a pink flour is starting to bloom . a pink flower is starring to bloom . Table 1: An example of noise injected to the clean text. The noisy substitutes are marked in red. as images followed by optical character recognition to cover some cases of noise such as misspellings. This is an interesting but orthogonal area of re"
2021.findings-emnlp.271,W16-2346,1,0.834894,"Missing"
2021.findings-emnlp.271,2020.acl-main.400,0,0.029889,"extra modality is given as visual features from an image network to complement the textual context. In standard MMT, these features can be fused with the 2 Background and Related Work textual representation by simple operations such Robust NMT Although NMT models can as concatenation (Caglayan et al., 2016), hidden achieve high performance on clean data, they are states initialization (Calixto and Liu, 2017), or via very brittle to non-standard inputs, such as noisy attention mechanisms (Libovický and Helcl, 2017; texts (Belinkov and Bisk, 2018). Different types of Calixto et al., 2016, 2017; Yao and Wan, 2020) and noisy data have been proposed to test translation latent variables (Calixto et al., 2019). robustness, e.g. synthetic word perturbations (BeRecent research has shown that the extra modallinkov and Bisk, 2018), grammatical errors (Anas- ity helps translation, especially when the input is intasopoulos et al., 2019), and user-generated texts complete (Caglayan et al., 2019, 2020; Imankulova from social platform (Michel and Neubig, 2018; Li et al., 2020) or ambiguous (Ive et al., 2019; Wu et al., 2019; Specia et al., 2020). et al., 2019b). Wu et al. (2019a) hinted at the posThe most common ap"
2021.findings-emnlp.271,N16-1042,0,0.0304574,"oder ResNet-101 noisy: a man and too girls playing on they shore of the beach noisy: a man and too girls playing on they shore of the beach en: a man and two girls playing on the shore of the beach en: a man and two girls playing on the shore of the beach Figure 2: Illustration of the joint training of machine translation and error correction for NMT and MMT models. Solid lines: translation flow. Dotted lines: error correction flow. Left: NMT with error correction training. Right: MMT with error correction training. 3.2 Error Correction Training We introduce error correction (Ng et al., 2014; Yuan and Briscoe, 2016) as an auxiliary task to help improve the robustness against noisy inputs. For that, we add a second decoder to the MT architecture, which is only used for the error correction task. During training, the noisy sentence x0 is encoded by the encoder, which is shared between the translation and correction tasks, into hidden states h0 . The hidden state representation is then fed to both decoders. The translation decoder aims to generate a correct translation y while the correction decoder aims to recover the original source sentence x. This method is also compatible with the MMT model, where the"
2021.findings-emnlp.271,N19-1190,0,0.104658,"ll perturbations in the inputs often leading to mistranslations. To improve the robustness of NMT models, current research mostly focuses on adapting the model to noisy texts via methods such as fine-tuning (Michel and Neubig, 2018; Alam and Anastasopoulos, 2020), noiseGiven that the visual modality has been shown injection (Belinkov and Bisk, 2018; Cheng et al., to help predict unknown words, we investi2018; Karpukhin et al., 2019), and data augmenta- gate whether adding multimodal information to tion through back-translation (Berard et al., 2019; adaption-based methods would further improve Vaibhav et al., 2019; Li and Specia, 2019), etc. In translation robustness. To answer this question, these approaches, the translation model is trained we build MMT models in conjunction with noise or fine-tuned on the noisy data so that it can learn injection techniques and investigate their behaviour from the noise. However, methods using extra during training and inference on both noisy and context to help translate noisy texts have not been clean data. To further improve robustness, we exinvestigated. tend the current adversarial training method (i.e., Studies in Multimodal Machine Translation training NMT mo"
2021.mtsummit-up.22,2020.eamt-1.58,1,0.835182,"Missing"
2021.mtsummit-up.22,N19-1423,0,0.0106695,"ses. Quality Estimation Module: The Quality Gate incorporates QE models built using TransQuest (Ranasinghe et al., 2020), the winning toolkit in the WMT20 Quality Estimation Shared Task for sentence-level QE (Specia et al., 2020). In these models, the original sentence and its translation are concatenated using the [SEP] token, and then passed through a pre-trained Transformer-based language model to obtain a joint representation via the [CLS] token. This serves as input to a softmax layer that predicts translation quality. We trained language-specific models by fine-tuning Multilingual BERT (Devlin et al., 2019) with the dataset of Ive et al. (2020), which contains (source, MT output, human postedition, target) tuples of sentences in the legal domain. We chose this data since it is the closest to our application domain, and contains instances in the language pairs of our interest: 11,249 for English-Dutch (EN-NL) and 9,989 for English-French (EN-FR). In order to obtain gold QE scores, we used tercom (Snover et al., 2006) to compute a TER value for each sentence. We trained our models using the same data splits as Ive et al. (2020), obtaining better results than the ones originally reported with ensem"
2021.mtsummit-up.22,2020.amta-user.2,0,0.031494,"earson correlation), and cost (in terms of training and inference times). Different from theirs, our work relies on end-user translation acceptability as primary evaluation metric. 1 https://ape-quest.eu/ Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 2: MT Users and Providers Track Page 307 Finally, some work attempted to determine thresholds for metrics’ scores to identify ranges where post-editing productivity gains can be obtained (Parra Escartín and Arcedillo, 2015), or improvement in the quality of the raw MT output can be expected (Guerrero, 2020). However, they were based on post-hoc computations of TER (translation edit rate) or edit distance, respectively, instead of predicted QE scores as in our case. In addition, we experiment with thresholds of QE scores that benefit the overall translation workflow for different use cases and language pairs. 3 Quality Gate We describe the technical components of the Quality Gate, the translation workflows that it compares to, and the translation use cases we considered. 3.1 Core Technologies Machine Translation Module: The Quality Gate uses eTranslation2 as backend NMT service. This service prov"
2021.mtsummit-up.22,2020.lrec-1.455,1,0.837362,"Missing"
2021.mtsummit-up.22,D18-1512,0,0.0209086,"guage, three post-editors were hired, and each sentence was postedited once. Ratings were elicited for all MT outputs and their corresponding HPEs. Raters were professional translators that judged the quality of the sentences as Acceptable/Unacceptable for each use case. Raters were not informed of whether the sentences being judged were an MT output or HPE. For each target language and use case, two raters scored each translation (either MT or HPE) once. HPEs and ratings were collected using the in-house MT Evaluation tool of one of the consortium’s companies. Following recommended practice (Läubli et al., 2018; Toral et al., 2018), sentences were post-edited and rated within the document context of the source language, i.e. the preceding and the following sentences. For HPEs (Figure 1a) we also collected timestamps of when an editor started the editing job and of when the final job was delivered, at the sentence level. For collecting ratings (Figure 1b), the tool is flexible regarding the type of judgements that can be collected. In our case, we used binary ones for each use case. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 2: MT Users and P"
2021.mtsummit-up.22,Q17-1015,0,0.016051,"that suit each use case and target language, and demonstrate the potential benefits of adding QE to a translation workflow. 1 Introduction Quality Estimation (QE) for Machine Translation (MT) predicts how good or reliable automatic translations are without access to gold-standard references (Specia et al., 2009; Fonseca et al., 2019; Specia et al., 2020). This is especially useful in real-world settings, such as within a translation company, where it can improve post-editing efficiency by filtering out segments that require more effort to correct than to translate from scratch (Specia, 2011; Martins et al., 2017), or select high-quality segments to be published as they are (Soricut and Echihabi, 2010). However, while the utility of MT is widely accepted nowadays, thus far no research has looked into validating the utility of QE in practice, in a realistic setting. To address this gap, in this paper we ask ourselves the following questions: 1) Can QE make the translation process more efficient (i.e. faster and cheaper)? 2) What is the impact of a QE-based filter on the quality of the final translations? and 3) How does varying the threshold for this filter affect these two competing goals (efficiency a"
2021.mtsummit-up.22,2015.mtsummit-wptp.4,0,0.110336,"Missing"
2021.mtsummit-up.22,2020.coling-main.445,0,0.0345948,"ion workflow for different use cases and language pairs. 3 Quality Gate We describe the technical components of the Quality Gate, the translation workflows that it compares to, and the translation use cases we considered. 3.1 Core Technologies Machine Translation Module: The Quality Gate uses eTranslation2 as backend NMT service. This service provides state-of-the-art NMT systems for more than 24 languages, and is targeted mainly at European public administrations and small and medium-sized enterprises. Quality Estimation Module: The Quality Gate incorporates QE models built using TransQuest (Ranasinghe et al., 2020), the winning toolkit in the WMT20 Quality Estimation Shared Task for sentence-level QE (Specia et al., 2020). In these models, the original sentence and its translation are concatenated using the [SEP] token, and then passed through a pre-trained Transformer-based language model to obtain a joint representation via the [CLS] token. This serves as input to a softmax layer that predicts translation quality. We trained language-specific models by fine-tuning Multilingual BERT (Devlin et al., 2019) with the dataset of Ive et al. (2020), which contains (source, MT output, human postedition, target"
2021.mtsummit-up.22,2006.amta-papers.25,0,0.126305,"t representation via the [CLS] token. This serves as input to a softmax layer that predicts translation quality. We trained language-specific models by fine-tuning Multilingual BERT (Devlin et al., 2019) with the dataset of Ive et al. (2020), which contains (source, MT output, human postedition, target) tuples of sentences in the legal domain. We chose this data since it is the closest to our application domain, and contains instances in the language pairs of our interest: 11,249 for English-Dutch (EN-NL) and 9,989 for English-French (EN-FR). In order to obtain gold QE scores, we used tercom (Snover et al., 2006) to compute a TER value for each sentence. We trained our models using the same data splits as Ive et al. (2020), obtaining better results than the ones originally reported with ensembles of 5 models per language (Table 1). EN-NL Model Ive et al. (2020) Ours EN-FR r MAE r MAE 0.38 0.51 0.14 0.10 0.58 0.69 0.14 0.10 Table 1: Performance of QE models in terms of Pearson’s r correlation coefficient and Mean Absolute Error (MAE) in the test set of Ive et al. (2020). Whilst the performance of the models is moderate according to Pearson, the error is relatively low (0.1 in a 0-1 range), and thus we"
2021.mtsummit-up.22,P10-1063,0,0.0340375,"of adding QE to a translation workflow. 1 Introduction Quality Estimation (QE) for Machine Translation (MT) predicts how good or reliable automatic translations are without access to gold-standard references (Specia et al., 2009; Fonseca et al., 2019; Specia et al., 2020). This is especially useful in real-world settings, such as within a translation company, where it can improve post-editing efficiency by filtering out segments that require more effort to correct than to translate from scratch (Specia, 2011; Martins et al., 2017), or select high-quality segments to be published as they are (Soricut and Echihabi, 2010). However, while the utility of MT is widely accepted nowadays, thus far no research has looked into validating the utility of QE in practice, in a realistic setting. To address this gap, in this paper we ask ourselves the following questions: 1) Can QE make the translation process more efficient (i.e. faster and cheaper)? 2) What is the impact of a QE-based filter on the quality of the final translations? and 3) How does varying the threshold for this filter affect these two competing goals (efficiency and quality)? Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, Augu"
2021.mtsummit-up.22,2011.eamt-1.12,1,0.756295,"ish thresholds that suit each use case and target language, and demonstrate the potential benefits of adding QE to a translation workflow. 1 Introduction Quality Estimation (QE) for Machine Translation (MT) predicts how good or reliable automatic translations are without access to gold-standard references (Specia et al., 2009; Fonseca et al., 2019; Specia et al., 2020). This is especially useful in real-world settings, such as within a translation company, where it can improve post-editing efficiency by filtering out segments that require more effort to correct than to translate from scratch (Specia, 2011; Martins et al., 2017), or select high-quality segments to be published as they are (Soricut and Echihabi, 2010). However, while the utility of MT is widely accepted nowadays, thus far no research has looked into validating the utility of QE in practice, in a realistic setting. To address this gap, in this paper we ask ourselves the following questions: 1) Can QE make the translation process more efficient (i.e. faster and cheaper)? 2) What is the impact of a QE-based filter on the quality of the final translations? and 3) How does varying the threshold for this filter affect these two compet"
2021.mtsummit-up.22,2009.eamt-1.5,1,0.68364,"rld scenario, where we rely on end-user acceptability as quality metric. Using data in the public administration domain for English-Dutch and English-French, we experimented with two use cases: assimilation and dissemination. Results shed some light on how QE scores can be explored to establish thresholds that suit each use case and target language, and demonstrate the potential benefits of adding QE to a translation workflow. 1 Introduction Quality Estimation (QE) for Machine Translation (MT) predicts how good or reliable automatic translations are without access to gold-standard references (Specia et al., 2009; Fonseca et al., 2019; Specia et al., 2020). This is especially useful in real-world settings, such as within a translation company, where it can improve post-editing efficiency by filtering out segments that require more effort to correct than to translate from scratch (Specia, 2011; Martins et al., 2017), or select high-quality segments to be published as they are (Soricut and Echihabi, 2010). However, while the utility of MT is widely accepted nowadays, thus far no research has looked into validating the utility of QE in practice, in a realistic setting. To address this gap, in this paper"
2021.mtsummit-up.22,W18-6312,0,0.0203967,"tors were hired, and each sentence was postedited once. Ratings were elicited for all MT outputs and their corresponding HPEs. Raters were professional translators that judged the quality of the sentences as Acceptable/Unacceptable for each use case. Raters were not informed of whether the sentences being judged were an MT output or HPE. For each target language and use case, two raters scored each translation (either MT or HPE) once. HPEs and ratings were collected using the in-house MT Evaluation tool of one of the consortium’s companies. Following recommended practice (Läubli et al., 2018; Toral et al., 2018), sentences were post-edited and rated within the document context of the source language, i.e. the preceding and the following sentences. For HPEs (Figure 1a) we also collected timestamps of when an editor started the editing job and of when the final job was delivered, at the sentence level. For collecting ratings (Figure 1b), the tool is flexible regarding the type of judgements that can be collected. In our case, we used binary ones for each use case. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 2: MT Users and Providers Track Page 3"
2021.mtsummit-up.22,P15-2087,0,0.018653,"the human-only workflow, for all use cases and target languages. The gains are even greater when using oracle scores instead of predicted scores, signalling the benefits of improving this type of technology. This trade-off methodology for establishing QE thresholds proved helpful to demonstrate the benefits of incorporating QE in real-world computer-aided translation workflows (Section 6). 2 Related Work Previous studies on the benefits of QE in translation workflows compared translators’ productivity when post-editing selected MT outputs (based on QE scores) versus translating from scratch. Turchi et al. (2015) found that significant gains depend on the length of the source sentences and the quality of the MT output. Similarly, Parra Escartín et al. (2017) showed that translators spent less time post-editing sentences with “good” QE scores, i.e. scores that accurately predicted low PE effort. Different from these studies, we do not investigate impact on post-editor productivity, but rather whether it is possible to rely on QE scores to selectively bypass human post-edition and still achieve similar levels of translation quality. In addition, we experiment with state-of-the-art neural QE systems inst"
2021.naacl-main.14,C04-1046,0,0.235102,"ment algorithm,5 enables us to identify the source words that have led to those translation errors. QE suggestions are presented by red word highlighting (see Figure 1). We note that word-level error annotation is a hard and costly task. Thus, available data for building systems to predict word-level errors is scarce. To circumvent this issue we relied on a feature-based approach which exploited information from the neural MT system (i.e. a glass-box approach to QE) and did not require large amounts of data for training. Glass-box features have been successfully used for QE of statistical MT (Blatz et al., 2004; Specia et al., 2013) and have been recently shown to be effective for sentence-level QE of neural MT systems (Fomicheva et al., 2020). To accommodate for the different types of MT models used in this work, including a student model Czech 2, we did not use the full set of features from Fomicheva et al. (2020) but instead relied on simple subset of log-probability based features: • Log-probability of the word • Log-prob. of the previous word • Log-prob. of the next word • Average log-prob. of the translated sentence • Number of characters in the word We build a binary gradient boosting classif"
2021.naacl-main.14,N13-1073,0,0.0967535,"Missing"
2021.naacl-main.14,D19-5503,0,0.0406336,"Missing"
2021.naacl-main.14,2020.wmt-1.17,0,0.16429,"31.61 Table 1: Performance of utilized MT systems in BLEU score evaluated on WMT18 test set; higher is better. Machine Translation. We used three MT systems for Czech (differing in speed and training data size) and one for Estonian. All of the systems were trained in both directions: the forward systems translate from English, whereas the opposite direction is used as a backward translation cue. All the MT systems follow the Transformer model architecture (Vaswani et al., 2017) design, though student systems make use of the simplified simple recurrent unit and other modifications described in Germann et al. (2020). Table 1 shows how the MT systems performed in terms of BLEU score (Papineni et al., 2002) on the test set of WMT18 News task (Bojar et al., 2018). The Czech 3 system is the winning MT model of Czech–English News Translation in WMT 2019 (Popel et al., 2019), having been trained on 58M authentic sentence pairs and 65M backtranslated monolingual sentences.4 The training proposed by Germann et al. (2020) was used for a CPU-optimized student model Czech 2. It was created by the knowledge distillation (Kim and Rush, 2016) method on translations generated by Czech 3. Although it has been trained so"
2021.naacl-main.14,D16-1139,0,0.0245682,"the simplified simple recurrent unit and other modifications described in Germann et al. (2020). Table 1 shows how the MT systems performed in terms of BLEU score (Papineni et al., 2002) on the test set of WMT18 News task (Bojar et al., 2018). The Czech 3 system is the winning MT model of Czech–English News Translation in WMT 2019 (Popel et al., 2019), having been trained on 58M authentic sentence pairs and 65M backtranslated monolingual sentences.4 The training proposed by Germann et al. (2020) was used for a CPU-optimized student model Czech 2. It was created by the knowledge distillation (Kim and Rush, 2016) method on translations generated by Czech 3. Although it has been trained solely on synthetic data, its performance in the news domain falls behind the teacher only by 0.5 to 3.0 BLEU points, depending on the translation direction. We included it mainly due to its speed as shown in Section 4. The design of the Czech 1 system is identical to Czech 3. The only difference is that the former was trained only on a subsample of 5M sentence pairs from CzEng 1.7 (Bojar et al., 2016). This system was chosen to simulate performance on less resourceful language pairs. The Estonian system uses the same c"
2021.naacl-main.14,2020.iwslt-1.25,1,0.687222,"Missing"
2021.naacl-main.14,P02-1040,0,0.109419,"t; higher is better. Machine Translation. We used three MT systems for Czech (differing in speed and training data size) and one for Estonian. All of the systems were trained in both directions: the forward systems translate from English, whereas the opposite direction is used as a backward translation cue. All the MT systems follow the Transformer model architecture (Vaswani et al., 2017) design, though student systems make use of the simplified simple recurrent unit and other modifications described in Germann et al. (2020). Table 1 shows how the MT systems performed in terms of BLEU score (Papineni et al., 2002) on the test set of WMT18 News task (Bojar et al., 2018). The Czech 3 system is the winning MT model of Czech–English News Translation in WMT 2019 (Popel et al., 2019), having been trained on 58M authentic sentence pairs and 65M backtranslated monolingual sentences.4 The training proposed by Germann et al. (2020) was used for a CPU-optimized student model Czech 2. It was created by the knowledge distillation (Kim and Rush, 2016) method on translations generated by Czech 3. Although it has been trained solely on synthetic data, its performance in the news domain falls behind the teacher only by"
2021.naacl-main.14,W19-5337,1,0.901334,"Missing"
2021.naacl-main.14,underwood-etal-2014-evaluating,0,0.0267014,"to human professionals in specific settings (Hassan et al., 2018; Popel et al., 2020), it is far from reasonable to blindly believe that the output of MT systems is perfectly accurate. It should thus not be simply included in an email or another message without some means of verification. Feedback in this scenario is needed, which would tell users if the translation is correct and ideally even give instructions on how to improve it. A related area of interactive machine translation (IMT) focuses mainly on either post-editor scenarios (Martínez-Gómez et al., 2012; Sanchis-Trilles et al., 2014; Underwood et al., 2014; Alabau et al., 2016) or generally scenarios in which users are able to produce the translation themselves and the system only aims to speed it up or improve it (Santy et al., 2019). Outbound translation differs from common IMT scenarios by the fact that the user does not speak the target language, and hence operates on the MT result only in a limited way. The first work to deal with this task by Zouhar and Bojar (2020) focused on working with CzechGerman MT in context of asking and reformulating questions. A preliminary experiment on the effect of translation cues has been carried out by Zou"
2021.naacl-main.14,2020.lrec-1.860,1,0.894876,"responsibility to create the content in the way that it is correctly interpreted by a recipient lies on the authors of the message. The main issue is that the target language might be entirely unknown to them. Prototypically it is communication by email, filling in foreign language forms, or involving some other kind of interactive medium. The focus in this scenario is placed not only on producing high-quality translations but also on reassuring the author that the MT output is correct. One of the approaches to improving both quality and authors’ confidence, first employed in this scenario by Zouhar and Bojar (2020), is to provide cues that indicate the quality of MT output as well as suggest possible rephrasing of the source. They may include backward translation to the source language, highlighting of the potentially problematic parts of the input, or suggesting paraphrases. Except for preliminary work by Zouhar and Novák (2020), the impact of individual cues has not yet been properly explored. In this paper, we present the results of a new experiment on outbound translation. Building on the previous works, the focus was expanded to investigate the influence of different levels of performance of the un"
2021.naacl-main.14,D19-3018,0,0.0174502,"It should thus not be simply included in an email or another message without some means of verification. Feedback in this scenario is needed, which would tell users if the translation is correct and ideally even give instructions on how to improve it. A related area of interactive machine translation (IMT) focuses mainly on either post-editor scenarios (Martínez-Gómez et al., 2012; Sanchis-Trilles et al., 2014; Underwood et al., 2014; Alabau et al., 2016) or generally scenarios in which users are able to produce the translation themselves and the system only aims to speed it up or improve it (Santy et al., 2019). Outbound translation differs from common IMT scenarios by the fact that the user does not speak the target language, and hence operates on the MT result only in a limited way. The first work to deal with this task by Zouhar and Bojar (2020) focused on working with CzechGerman MT in context of asking and reformulating questions. A preliminary experiment on the effect of translation cues has been carried out by Zouhar and Novák (2020), but it was conducted on a much smaller scale both in terms of participants and annotators and with non-native speakers of English. This may have affected the re"
2021.naacl-main.14,P13-4014,1,0.685445,"Missing"
2021.naacl-main.14,tiedemann-2012-parallel,0,0.0914444,"Missing"
2021.naacl-main.252,D15-1075,0,0.0131808,".569 Avg 0.372 0.406 0.508 0.540 0.553 Table 3: Pearson Correlation with human scores for the WMT-17 with Roberta-Base in the SRC-MT setting. because the former significantly outperforms the latter (Conneau et al., 2019), as also shown by Reimers and Gurevych (2020) for crosslingual semantic textual similarity (STS) tasks (Cer et al., 2017). For a fair comparison with previous metrics like WMD0 , we replaced their original embeddings with XLM-Roberta-Base embeddings. For the semantic sentence embedding, we used XLM-Roberta-Base embeddings from Sentence Transformer, which were trained on SNLI (Bowman et al., 2015) + MultiNLI (Williams et al., 2018) and then fine-tuned on the STS benchmark training data. These sentence embeddings have been shown to provide good representations of the semantic relationship between two sentences, but they had not yet been tested for machine translation evaluation. Without using semantic embeddings, the performance of SSS is not consistent across different languages pairs given our experimental datasets (see Appendix A.1). XLM-Roberta-Large embeddings are not used in our experiments because they are not available in the pre-trained Sentence Transformer package yet. For mon"
2021.naacl-main.252,2020.wmt-1.116,1,0.833294,"Missing"
2021.naacl-main.252,J82-2005,0,0.733788,"Missing"
2021.naacl-main.252,W19-5356,1,0.935157,"ased evaluation metrics, recent setting (MT-reference translation), a well as work (Williams et al., 2018; Bowman et al., in the source-MT bilingual setting, where it 2015; Echizen’ya et al., 2019; Cer et al., 2017; performs on par with glass-box approaches to Echizen’ya et al., 2019) has investigated metrics quality estimation that rely on MT model inthat perform comparisons in the semantic space formation. rather than at the surface level. Notably, applica1 Introduction tions of Word Mover’s Distance (WMD; Kusner Automatically evaluating machine translation (MT) et al., 2015), such as WMDo (Chow et al., 2019), as well as other language generation tasks has been VIFIDEL (Madhyastha et al., 2019) and moverinvestigated for decades, with substantial progress score (Zhao et al., 2019), which compute similarity in recent years due to the advances of pretrained based on continuous word embeddings using precontextual word embeddings. The general goal of trained representations. These have been shown to such evaluation metrics is to estimate the semantic consistently outperform previous metrics on variequivalence between the input text (e.g. a source ous language generation evaluation tasks. sentence or a"
2021.naacl-main.252,P19-1264,0,0.125121,"d a complete turnaround. We made a turnaround. We have not made a complete turnaround. We have made an incomplete turnaround. Table 1: An example from the WMT-17 dataset. Given the reference (REF) sentence, BERTScore assigns higher similarity to its negated versions (MT3 and MT4) than to semantically similar variants (MT1 and MT2). Contrarily, SSS gives a very low score to MT3 and MT4. Their combination provides a more balanced score. results on a number of text generation tasks including summarization, machine translation, image captioning, and data-to-text generation, using BERT embeddings. Clark et al. (2019) present semantic metrics for text summarization based on the sentence mover’s similarity and ELMo embeddings. Chow et al. (2019) introduce a fluencybased word mover’s distance (WMDo) metric for machine translation evaluation using Word2Vec embeddings (Mikolov et al., 2013). Lo (2019) presents Yisi, a unified automatic semantic machine translation quality evaluation and estimation metric using BERT embeddings. There is also a bulk of work on metrics that take a step further to optimize their scores using machine learning algorithms trained on human scores for quality (Sellam et al., 2020; Ma e"
2021.naacl-main.252,P19-4007,0,0.152667,"lace human judgements. In order to understand how well they fare at this task, metrics are evaluated by how similar their scores are to human assigned judgements on held-out datasets. For absolute quality judgements, Pearson Correlation is the most popularly used metric for such a comparison (Mathur et al., 2020). Recent studies have showed that the new generation of automatic evaluation metrics, which instead of lexical overlap are based on word semantics using continuous word embedding, such as BERT (Devlin et al., 2019), ElMo (Peters et al., 2019), XLNet (Yang et al., 2019) or XLM-Roberta (Conneau et al., 2019), have significantly higher Pearson Correlation with the human judgements when comparing reference sentences with system generated sentences. Zhang et al. (2019) introduce BERTscore, an automatic evaluation metric based on contextual word embeddings, and tests it for text generation tasks such as machine translation and imaging captioning, using embeddings including BERT, XLMRoberta, and XLNet (more details in Section 3.2). Mathur et al. (2019) present supervised and unsupervised metrics which are based on BERT em1. We investigate and show the effectiveness of beddings for improving machine tr"
2021.naacl-main.252,N19-1423,0,0.110799,"ty levels, from characters to words to embedding vectors. The goal of such metrics is to replace human judgements. In order to understand how well they fare at this task, metrics are evaluated by how similar their scores are to human assigned judgements on held-out datasets. For absolute quality judgements, Pearson Correlation is the most popularly used metric for such a comparison (Mathur et al., 2020). Recent studies have showed that the new generation of automatic evaluation metrics, which instead of lexical overlap are based on word semantics using continuous word embedding, such as BERT (Devlin et al., 2019), ElMo (Peters et al., 2019), XLNet (Yang et al., 2019) or XLM-Roberta (Conneau et al., 2019), have significantly higher Pearson Correlation with the human judgements when comparing reference sentences with system generated sentences. Zhang et al. (2019) introduce BERTscore, an automatic evaluation metric based on contextual word embeddings, and tests it for text generation tasks such as machine translation and imaging captioning, using embeddings including BERT, XLMRoberta, and XLNet (more details in Section 3.2). Mathur et al. (2019) present supervised and unsupervised metrics which are base"
2021.naacl-main.252,N19-1186,0,0.0480465,"Missing"
2021.naacl-main.252,W04-1013,0,0.0485635,"ossible and quality scores from human-labeled data. In straightforward. A multitude of evaluation metrics this paper, we propose a more cost-effective, have been proposed following this approach, espeyet well performing unsupervised alternative cially for MT, the application we focus on in this SentSim: relying on strong pretrained mulpaper. These include the famous BLEU (Papineni tilingual word and sentence representations, et al., 2002) and METEOR (Banerjee and Lavie, we directly compare the source with the machine translated sentence, thus avoiding the 2005) for machine translation, ROUGE (Lin, 2004) need for both reference translations and lafor summarization, and CIDER (Vedantam et al., belled training data. The metric builds on 2014) for image captioning. These traditional metstate-of-the-art embedding-based approaches – rics are based on simple-word, n-gram matching namely BERTScore and Word Mover’s Dismechanisms or slight relaxations of these (e.g. syntance – by incorporating a notion of sentence onyms) which are computationally efficient, but semantic similarity. By doing so, it achieves suffer from various limitations. better correlation with human scores on different datasets. We"
2021.naacl-main.252,W17-4767,0,0.0624932,"Missing"
2021.naacl-main.252,W19-5358,0,0.0202784,"ically similar variants (MT1 and MT2). Contrarily, SSS gives a very low score to MT3 and MT4. Their combination provides a more balanced score. results on a number of text generation tasks including summarization, machine translation, image captioning, and data-to-text generation, using BERT embeddings. Clark et al. (2019) present semantic metrics for text summarization based on the sentence mover’s similarity and ELMo embeddings. Chow et al. (2019) introduce a fluencybased word mover’s distance (WMDo) metric for machine translation evaluation using Word2Vec embeddings (Mikolov et al., 2013). Lo (2019) presents Yisi, a unified automatic semantic machine translation quality evaluation and estimation metric using BERT embeddings. There is also a bulk of work on metrics that take a step further to optimize their scores using machine learning algorithms trained on human scores for quality (Sellam et al., 2020; Ma et al., 2017). They often perform even better, but the reliance on human scores for training, in addition to reference translations at inference time, makes them less applicable in practice. A separate strand of work that relies on contextual embeddings is that of Quality Estimation (M"
2021.naacl-main.252,W17-4768,0,0.018472,"019) present semantic metrics for text summarization based on the sentence mover’s similarity and ELMo embeddings. Chow et al. (2019) introduce a fluencybased word mover’s distance (WMDo) metric for machine translation evaluation using Word2Vec embeddings (Mikolov et al., 2013). Lo (2019) presents Yisi, a unified automatic semantic machine translation quality evaluation and estimation metric using BERT embeddings. There is also a bulk of work on metrics that take a step further to optimize their scores using machine learning algorithms trained on human scores for quality (Sellam et al., 2020; Ma et al., 2017). They often perform even better, but the reliance on human scores for training, in addition to reference translations at inference time, makes them less applicable in practice. A separate strand of work that relies on contextual embeddings is that of Quality Estimation (Moura et al., 2020; Fomicheva et al., 2020a; Ranasinghe et al., 2020; Specia et al., 2020). These are also trained on human judgements of quality, but machine translations are compared directly to the source sentences rather than against reference translations. In addition to embeddings for words, embeddings for full sentences"
2021.naacl-main.252,P19-1654,1,0.844461,"k (Williams et al., 2018; Bowman et al., in the source-MT bilingual setting, where it 2015; Echizen’ya et al., 2019; Cer et al., 2017; performs on par with glass-box approaches to Echizen’ya et al., 2019) has investigated metrics quality estimation that rely on MT model inthat perform comparisons in the semantic space formation. rather than at the surface level. Notably, applica1 Introduction tions of Word Mover’s Distance (WMD; Kusner Automatically evaluating machine translation (MT) et al., 2015), such as WMDo (Chow et al., 2019), as well as other language generation tasks has been VIFIDEL (Madhyastha et al., 2019) and moverinvestigated for decades, with substantial progress score (Zhao et al., 2019), which compute similarity in recent years due to the advances of pretrained based on continuous word embeddings using precontextual word embeddings. The general goal of trained representations. These have been shown to such evaluation metrics is to estimate the semantic consistently outperform previous metrics on variequivalence between the input text (e.g. a source ous language generation evaluation tasks. sentence or a document) and an output text that However, these metrics have two limitations: (i) has"
2021.naacl-main.252,2020.emnlp-main.365,0,0.282321,"avoiding the need of reference translations. We note that this is different from quality estimation (QE) metrics (Specia et al., 2013; Shah et al., 2015) , which also compare source and machine translated texts directly, but assume an additional step of supervised learning against human labels for quality. Second, we introduce Sentence Semantic Similarity (SSS) , an additional component to be combined with bag-of-embeddings distance metrics such as BERTScore. More specifically, we propose to explore semantic similarity at the sentence level – based on sentence embeddings (Sellam et al., 2020; Reimers and Gurevych, 2020; Thakur et al., 2020) – and linearly combine it with existing metrics that use word embeddings. By doing so, the resulting metrics have access to word and compositional semantics, leading with improved performance. The combination is a simple weighted sum, and does not require training data. As a motivational example, consider the case in Table 1, from the WMT-17 Metrics task (Zhang et al., 2019). When faced with MT sentences that contain a negated version of the reference (MT3 and MT4), token-level metrics such as BERTScore and WMD cannot correctly penalize these sentences since they match r"
2021.naacl-main.252,P19-1269,0,0.0239439,"d semantics using continuous word embedding, such as BERT (Devlin et al., 2019), ElMo (Peters et al., 2019), XLNet (Yang et al., 2019) or XLM-Roberta (Conneau et al., 2019), have significantly higher Pearson Correlation with the human judgements when comparing reference sentences with system generated sentences. Zhang et al. (2019) introduce BERTscore, an automatic evaluation metric based on contextual word embeddings, and tests it for text generation tasks such as machine translation and imaging captioning, using embeddings including BERT, XLMRoberta, and XLNet (more details in Section 3.2). Mathur et al. (2019) present supervised and unsupervised metrics which are based on BERT em1. We investigate and show the effectiveness of beddings for improving machine translation evallinearly combining sentence-level semantic uation. Zhao et al. (2019) introduce moverscore, similarity with different metrics using token- a metric which generates high-quality evaluation 3144 REF MT1 MT2 MT3 MT4 BERTScore SSS SSS + BERTScore 0.7975 0.7748 0.8296 0.8318 0.9578 0.8898 0.3878 0.4431 0.8111 0.7427 0.4832 0.5107 We have made a complete turnaround. We did a complete turnaround. We made a turnaround. We have not made a"
2021.naacl-main.252,2020.wmt-1.77,0,0.309195,"ese are evaluated either manually or automatically by comparison against one or multiple reference sentences. A multitude of metrics have been proposed for the latter, which perform comparisons at various granularity levels, from characters to words to embedding vectors. The goal of such metrics is to replace human judgements. In order to understand how well they fare at this task, metrics are evaluated by how similar their scores are to human assigned judgements on held-out datasets. For absolute quality judgements, Pearson Correlation is the most popularly used metric for such a comparison (Mathur et al., 2020). Recent studies have showed that the new generation of automatic evaluation metrics, which instead of lexical overlap are based on word semantics using continuous word embedding, such as BERT (Devlin et al., 2019), ElMo (Peters et al., 2019), XLNet (Yang et al., 2019) or XLM-Roberta (Conneau et al., 2019), have significantly higher Pearson Correlation with the human judgements when comparing reference sentences with system generated sentences. Zhang et al. (2019) introduce BERTscore, an automatic evaluation metric based on contextual word embeddings, and tests it for text generation tasks suc"
2021.naacl-main.252,2020.wmt-1.119,0,0.0802935,"Missing"
2021.naacl-main.252,P02-1040,0,0.114409,"Missing"
2021.naacl-main.252,D19-1005,0,0.0403034,"Missing"
2021.naacl-main.252,2020.wmt-1.122,0,0.485529,"machine translation quality evaluation and estimation metric using BERT embeddings. There is also a bulk of work on metrics that take a step further to optimize their scores using machine learning algorithms trained on human scores for quality (Sellam et al., 2020; Ma et al., 2017). They often perform even better, but the reliance on human scores for training, in addition to reference translations at inference time, makes them less applicable in practice. A separate strand of work that relies on contextual embeddings is that of Quality Estimation (Moura et al., 2020; Fomicheva et al., 2020a; Ranasinghe et al., 2020; Specia et al., 2020). These are also trained on human judgements of quality, but machine translations are compared directly to the source sentences rather than against reference translations. In addition to embeddings for words, embeddings for full sentences have been shown to work very well to measure semantic similarity. These are extracted using Transformer models that are specifically trained for capturing sentence semantic meanings using BERT, Roberta, and XLM-Roberta embeddings (Reimers and Gurevych, 2019; Reimers and Gurevych, 2020; Thakur et al., 2020) and provide state-of-art perfor"
2021.naacl-main.252,D19-1410,0,0.0716743,"beddings is that of Quality Estimation (Moura et al., 2020; Fomicheva et al., 2020a; Ranasinghe et al., 2020; Specia et al., 2020). These are also trained on human judgements of quality, but machine translations are compared directly to the source sentences rather than against reference translations. In addition to embeddings for words, embeddings for full sentences have been shown to work very well to measure semantic similarity. These are extracted using Transformer models that are specifically trained for capturing sentence semantic meanings using BERT, Roberta, and XLM-Roberta embeddings (Reimers and Gurevych, 2019; Reimers and Gurevych, 2020; Thakur et al., 2020) and provide state-of-art performance pretrained models for many languages.1 In this paper, we take inspiration from these lines 1 https://github.com/UKPLab/sentence-transformers of previous works to propose unsupervised metrics that combine word and sentence semantic similarity and show that this can be effective for both MTreference and source-MT comparisons. 3 Method In this section, we first describe in more detail the metrics that we have used in our experiments, namely semantic sentence cosine similarity, WMD and BERTScore. Then we presen"
2021.naacl-main.252,2020.acl-main.704,0,0.120552,"s multilingual BERT, avoiding the need of reference translations. We note that this is different from quality estimation (QE) metrics (Specia et al., 2013; Shah et al., 2015) , which also compare source and machine translated texts directly, but assume an additional step of supervised learning against human labels for quality. Second, we introduce Sentence Semantic Similarity (SSS) , an additional component to be combined with bag-of-embeddings distance metrics such as BERTScore. More specifically, we propose to explore semantic similarity at the sentence level – based on sentence embeddings (Sellam et al., 2020; Reimers and Gurevych, 2020; Thakur et al., 2020) – and linearly combine it with existing metrics that use word embeddings. By doing so, the resulting metrics have access to word and compositional semantics, leading with improved performance. The combination is a simple weighted sum, and does not require training data. As a motivational example, consider the case in Table 1, from the WMT-17 Metrics task (Zhang et al., 2019). When faced with MT sentences that contain a negated version of the reference (MT3 and MT4), token-level metrics such as BERTScore and WMD cannot correctly penalize these"
2021.naacl-main.252,2020.wmt-1.79,1,0.729296,"ity evaluation and estimation metric using BERT embeddings. There is also a bulk of work on metrics that take a step further to optimize their scores using machine learning algorithms trained on human scores for quality (Sellam et al., 2020; Ma et al., 2017). They often perform even better, but the reliance on human scores for training, in addition to reference translations at inference time, makes them less applicable in practice. A separate strand of work that relies on contextual embeddings is that of Quality Estimation (Moura et al., 2020; Fomicheva et al., 2020a; Ranasinghe et al., 2020; Specia et al., 2020). These are also trained on human judgements of quality, but machine translations are compared directly to the source sentences rather than against reference translations. In addition to embeddings for words, embeddings for full sentences have been shown to work very well to measure semantic similarity. These are extracted using Transformer models that are specifically trained for capturing sentence semantic meanings using BERT, Roberta, and XLM-Roberta embeddings (Reimers and Gurevych, 2019; Reimers and Gurevych, 2020; Thakur et al., 2020) and provide state-of-art performance pretrained model"
2021.naacl-main.252,P13-4014,1,0.86733,"Missing"
2021.naacl-main.252,N18-1101,0,0.0623808,"hes – rics are based on simple-word, n-gram matching namely BERTScore and Word Mover’s Dismechanisms or slight relaxations of these (e.g. syntance – by incorporating a notion of sentence onyms) which are computationally efficient, but semantic similarity. By doing so, it achieves suffer from various limitations. better correlation with human scores on different datasets. We show that it outperforms these In order to overcome the drawbacks of the traand other metrics in the standard monolingual ditional string-based evaluation metrics, recent setting (MT-reference translation), a well as work (Williams et al., 2018; Bowman et al., in the source-MT bilingual setting, where it 2015; Echizen’ya et al., 2019; Cer et al., 2017; performs on par with glass-box approaches to Echizen’ya et al., 2019) has investigated metrics quality estimation that rely on MT model inthat perform comparisons in the semantic space formation. rather than at the surface level. Notably, applica1 Introduction tions of Word Mover’s Distance (WMD; Kusner Automatically evaluating machine translation (MT) et al., 2015), such as WMDo (Chow et al., 2019), as well as other language generation tasks has been VIFIDEL (Madhyastha et al., 2019)"
2021.naacl-main.252,2020.clinicalnlp-1.3,0,0.0162683,"Semantic Sentence Similarity (SSS) A commonly used method to measure sentence similarity is using the cosine distance between the two vectors summarizing the sentences: i cos(θ) = The semantic distance calculated by WMD can be then defined as follows: WMD = min T ≥0 n X Tij c(i, j) (4) i,j=1 WMD, or the semantic distance between two text documents, can thus be computed by optimizing values in the flow matrix T . In other words, WMD corresponds to the minimal semantic distance to move semantically similar words (via their embeddings) from one text document to another. 3.2 BERTScore BERTScore (Zhang et al., 2020) is designed to evaluate semantic similarity between sentences in the same language, namely a reference sentence and a machine-generated sentence. Assume a reference sentence is denoted as x = (x1 , ...., xk ) and a candidate sentence is denoted as x ˆ = (xˆ1 , ...., xˆk ), BERTScore uses contextual embeddings such as BERT (Devlin et al., 2019) or ELMo (Peters et al., 2019) to represent word tokens in the sentences. It finds word matchings between the reference and candidate sentence using cosine similarity, which can be optionally reweighted by the inverse document frequency scores (IDF) of e"
2021.naacl-main.252,D19-1053,0,0.456831,"chizen’ya et al., 2019; Cer et al., 2017; performs on par with glass-box approaches to Echizen’ya et al., 2019) has investigated metrics quality estimation that rely on MT model inthat perform comparisons in the semantic space formation. rather than at the surface level. Notably, applica1 Introduction tions of Word Mover’s Distance (WMD; Kusner Automatically evaluating machine translation (MT) et al., 2015), such as WMDo (Chow et al., 2019), as well as other language generation tasks has been VIFIDEL (Madhyastha et al., 2019) and moverinvestigated for decades, with substantial progress score (Zhao et al., 2019), which compute similarity in recent years due to the advances of pretrained based on continuous word embeddings using precontextual word embeddings. The general goal of trained representations. These have been shown to such evaluation metrics is to estimate the semantic consistently outperform previous metrics on variequivalence between the input text (e.g. a source ous language generation evaluation tasks. sentence or a document) and an output text that However, these metrics have two limitations: (i) has been modified in some way (e.g. a translation they still rely on reference outputs, whi"
2021.repl4nlp-1.23,2020.wmt-1.122,0,0.0221479,"oposed Bayesian metalearning approach delivers improved predictive performance in both limited and full supervision settings. 1 Introduction Quality Estimation (QE) models aim to evaluate the output of Machine Translation (MT) systems at run-time, when no reference translations are available (Blatz et al., 2004; Specia et al., 2009). QE models can be applied for instance to improve translation productivity by selecting high-quality translations amongst several candidates. A number of approaches have been proposed for this task (Specia et al., 2009, 2015; Kim et al., 2017; Kepler et al., 2019; Ranasinghe et al., 2020), and a shared task 2 2.1 Background Model-Agnostic Meta-Learning The goal of meta-learning, also known as learning to learn (Schmidhuber, 1987; Thrun and Pratt, 223 Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 223–230 Bangkok, Thailand (Online), August 6, 2021. ©2021 Association for Computational Linguistics 1998), is to develop models that can learn more efficiently over time, by generalizing from knowledge of how to solve related tasks from a given distribution of tasks. Given a learner model fw , for instance a neural network parametrized by w ∈"
2021.repl4nlp-1.23,P19-3020,0,0.0255799,"Missing"
2021.repl4nlp-1.23,W17-4763,0,0.0171471,"haracteristics demonstrates that the proposed Bayesian metalearning approach delivers improved predictive performance in both limited and full supervision settings. 1 Introduction Quality Estimation (QE) models aim to evaluate the output of Machine Translation (MT) systems at run-time, when no reference translations are available (Blatz et al., 2004; Specia et al., 2009). QE models can be applied for instance to improve translation productivity by selecting high-quality translations amongst several candidates. A number of approaches have been proposed for this task (Specia et al., 2009, 2015; Kim et al., 2017; Kepler et al., 2019; Ranasinghe et al., 2020), and a shared task 2 2.1 Background Model-Agnostic Meta-Learning The goal of meta-learning, also known as learning to learn (Schmidhuber, 1987; Thrun and Pratt, 223 Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 223–230 Bangkok, Thailand (Online), August 6, 2021. ©2021 Association for Computational Linguistics 1998), is to develop models that can learn more efficiently over time, by generalizing from knowledge of how to solve related tasks from a given distribution of tasks. Given a learner model fw , fo"
2021.repl4nlp-1.23,2006.amta-papers.25,0,0.101095,"e models with access to all available training instances for each QE task. 225 PE ID Train Dev Test PE1 PE2 PE3 PE4 PE5 PE6 PE7 1440 2160 1444 1834 4866 1677 1567 360 540 361 459 1217 420 392 200 300 195 244 617 203 241 Total 14988 3749 2000 BERT (Devlin et al., 2019) trained with knowledge distillation (Buciluˇa et al., 2006; Hinton et al., 2015). It accepts as input the source and machine translation outputs concatenated as a single text, separated by a ‘[SEP]’ token and prepended with a ‘[CLS]’ token. The representation of the ‘[CLS]’ token is then passed to a linear layer to predict HTER (Snover et al., 2006) values as regression targets. (a) QT21 en-lv (nmt) PE ID Train Dev Test PE1 PE2 PE3 PE4 PE5 9952 3445 8770 4579 7651 2488 862 2193 1145 1913 559 193 537 276 435 Total 34397 8601 2000 Benchmark Approaches We compare the proposed approach with the following: MTLPRETRAIN is a baseline trained in classic multitask fashion for multiple epochs using data from all QE tasks. It is thereafter fine-tuned using each QE task’s training data before making predictions on its test set, in a similar fashion as the meta-learning approaches; REPTILE (Nichol and Schulman, 2018); Model-Agnostic MetaLearning (MAM"
2021.repl4nlp-1.23,P15-4020,1,0.882119,"Missing"
2021.repl4nlp-1.23,2021.acl-short.25,1,0.485675,"er particles have more weight in the aggregate. The second term of the equation can be understood as a repulsive force that prevents the particles from collapsing to a single point. For the case when the number of particles is one, the SVGD update procedure reduces to standard gradient ascent on the objective p(w) for any kernel with the property ∇w k (w, w) = 0, such as the RBF kernel. SVGD has been applied in a wide range of settings, including reinforcement learning (Liu et al., 2017; Haarnoja et al., 2017), uncertainty quantification (Zhu and Zabaras, 2018), and online continual learning (Obamuyide et al., 2021). 2.3 Stein Variational Gradient Descent with Matrix-Valued Kernels Let Hk denote a reproducing kernel Hilbert space (RKHS) H with kernel k. Wang et al. (2019) observed that the original SVGD as proposed in Liu and Wang (2016) searches for the optimal update direction φ in RKHS Hkd = Hk × · · · × Hk , a product of d copies of RKHS of scalar-valued functions, which does not allow the encoding of any potential correlations between different co-ordinates of φ. Wang et al. (2019) proposed Matrix-SVGD, which addressed this limitation by replacing Hkd with a more general RKHS of vector-valued functi"
2021.repl4nlp-1.23,2009.eamt-1.5,1,0.672615,"formance, we further propose an extension to a state-of-the-art Bayesian meta-learning approach which utilizes a matrix-valued kernel for Bayesian meta-learning of quality estimation. Experiments on data with varying number of users and language characteristics demonstrates that the proposed Bayesian metalearning approach delivers improved predictive performance in both limited and full supervision settings. 1 Introduction Quality Estimation (QE) models aim to evaluate the output of Machine Translation (MT) systems at run-time, when no reference translations are available (Blatz et al., 2004; Specia et al., 2009). QE models can be applied for instance to improve translation productivity by selecting high-quality translations amongst several candidates. A number of approaches have been proposed for this task (Specia et al., 2009, 2015; Kim et al., 2017; Kepler et al., 2019; Ranasinghe et al., 2020), and a shared task 2 2.1 Background Model-Agnostic Meta-Learning The goal of meta-learning, also known as learning to learn (Schmidhuber, 1987; Thrun and Pratt, 223 Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 223–230 Bangkok, Thailand (Online), August 6, 2021. ©2"
aziz-etal-2012-pet,P02-1040,0,\N,Missing
aziz-etal-2012-pet,P07-2045,0,\N,Missing
aziz-etal-2012-pet,W10-1703,0,\N,Missing
aziz-etal-2012-pet,2011.eamt-1.12,1,\N,Missing
aziz-etal-2012-pet,R11-1014,1,\N,Missing
C12-2020,P02-1020,0,0.0881139,"Missing"
C12-2020,C08-1118,0,0.0285419,"an, (Baroni and Bernardini, 2006) finds that features such as the distribution of function words, personal pronouns and adverbs are very relevant. (Pastor et al., 2008) explored the existence of the simplification universal – which states that translated texts are simpler than their source counterpart –, suggesting that this universal does affect Spanish translated texts. Also focusing on the simplification universal, the studies by (Ilisei et al., 2010; Ilisei and Inkpen, 2011) on Romanian and Spanish translationese use morphological and simplification-based features. A six-lingual study by (Halteren, 2008) using frequencies of word n-grams shows that it is possible to distinguish between translated and non-translated texts down to their respective original languages. This is followed by the work of (Lembersky et al., 2011, 2012) which uses statistical language models for each language. Furthermore, a study by (Volansky et al., 2012) explores the differences between original, manually translated and machine translated texts. The experiments on translation direction identification suggest that translated texts have lower lexical richness and higher number of frequent words. They point out that si"
C12-2020,W11-2123,0,0.0122417,"as applied as an alternative binary classifier: SVM-light-TK4 (Moschitti, 2006), wich uses tree kernels with (partial) syntactic trees as features. 3.3 Feature extraction and selection The datasets are pre-processed with sentence segmentation, tokenisation and lowercasing. The part-of-speech (POS) tags and lemmas of words and the syntactic trees of sentences are generated using the Stanford CoreNLP toolkit5 (Klein and Manning, 2003). Pre-defined lists of function words (Koppel and Ordan, 2011) and stopwords6 are used. N-gram language models (with n = 3 & 5) are built using the KenLM toolkit7 (Heafield, 2011). The corpus used to build such models consisted in a random selection of 1.7M segments extracted from the entire “original” collection of the PAN-PC-10 corpus, excluding all the documents containing one or more segments present in our two datasets. We then use these language models to calculate the scores for both plagiarised and original segments. Features that capture simplification, morphological, statistical and syntactic aspects of texts are investigated. Based on the simplification universal, we extract the following simplificationbased features: 2 We used the Jrip Weka implementation o"
C12-2020,P03-1054,0,0.00474156,"hims, 2006) is used with a linear kernel. Both classification and ranking models are trained and tested using 4-fold cross-validation. In addition, a structured prediction version of SVM was applied as an alternative binary classifier: SVM-light-TK4 (Moschitti, 2006), wich uses tree kernels with (partial) syntactic trees as features. 3.3 Feature extraction and selection The datasets are pre-processed with sentence segmentation, tokenisation and lowercasing. The part-of-speech (POS) tags and lemmas of words and the syntactic trees of sentences are generated using the Stanford CoreNLP toolkit5 (Klein and Manning, 2003). Pre-defined lists of function words (Koppel and Ordan, 2011) and stopwords6 are used. N-gram language models (with n = 3 & 5) are built using the KenLM toolkit7 (Heafield, 2011). The corpus used to build such models consisted in a random selection of 1.7M segments extracted from the entire “original” collection of the PAN-PC-10 corpus, excluding all the documents containing one or more segments present in our two datasets. We then use these language models to calculate the scores for both plagiarised and original segments. Features that capture simplification, morphological, statistical and"
C12-2020,P11-1132,0,0.0158034,"nd ranking models are trained and tested using 4-fold cross-validation. In addition, a structured prediction version of SVM was applied as an alternative binary classifier: SVM-light-TK4 (Moschitti, 2006), wich uses tree kernels with (partial) syntactic trees as features. 3.3 Feature extraction and selection The datasets are pre-processed with sentence segmentation, tokenisation and lowercasing. The part-of-speech (POS) tags and lemmas of words and the syntactic trees of sentences are generated using the Stanford CoreNLP toolkit5 (Klein and Manning, 2003). Pre-defined lists of function words (Koppel and Ordan, 2011) and stopwords6 are used. N-gram language models (with n = 3 & 5) are built using the KenLM toolkit7 (Heafield, 2011). The corpus used to build such models consisted in a random selection of 1.7M segments extracted from the entire “original” collection of the PAN-PC-10 corpus, excluding all the documents containing one or more segments present in our two datasets. We then use these language models to calculate the scores for both plagiarised and original segments. Features that capture simplification, morphological, statistical and syntactic aspects of texts are investigated. Based on the simp"
C12-2020,D11-1034,0,0.0173358,"universal – which states that translated texts are simpler than their source counterpart –, suggesting that this universal does affect Spanish translated texts. Also focusing on the simplification universal, the studies by (Ilisei et al., 2010; Ilisei and Inkpen, 2011) on Romanian and Spanish translationese use morphological and simplification-based features. A six-lingual study by (Halteren, 2008) using frequencies of word n-grams shows that it is possible to distinguish between translated and non-translated texts down to their respective original languages. This is followed by the work of (Lembersky et al., 2011, 2012) which uses statistical language models for each language. Furthermore, a study by (Volansky et al., 2012) explores the differences between original, manually translated and machine translated texts. The experiments on translation direction identification suggest that translated texts have lower lexical richness and higher number of frequent words. They point out that simplification-based features are very helpful, but alone they are not sufficient to distinguish original from translated texts. Although by nature plagiarised texts are very different from translated texts, we exploit ins"
C12-2020,E12-1026,0,0.0250815,"Missing"
C12-2020,E06-1015,0,0.0184329,"ary classification and Support Vector Machines (SVM) for ranking. RIPPER2 was selected as a good representative of symbolic classifiers: the rules produced can help identify relevant features for specific cases. SVM is one of the most robust and best performing algorithms in many language processing tasks. For ranking, the SVMrank algorithm3 (Joachims, 2006) is used with a linear kernel. Both classification and ranking models are trained and tested using 4-fold cross-validation. In addition, a structured prediction version of SVM was applied as an alternative binary classifier: SVM-light-TK4 (Moschitti, 2006), wich uses tree kernels with (partial) syntactic trees as features. 3.3 Feature extraction and selection The datasets are pre-processed with sentence segmentation, tokenisation and lowercasing. The part-of-speech (POS) tags and lemmas of words and the syntactic trees of sentences are generated using the Stanford CoreNLP toolkit5 (Klein and Manning, 2003). Pre-defined lists of function words (Koppel and Ordan, 2011) and stopwords6 are used. N-gram language models (with n = 3 & 5) are built using the KenLM toolkit7 (Heafield, 2011). The corpus used to build such models consisted in a random sel"
C12-2020,2008.amta-papers.5,0,0.0163453,"(Gellerstam, 1986), which hypothesises that translated texts tend to exhibit characteristics that are different from nontranslated texts. The theory was further explored by (Baker, 1993, 1996) and based on such a theory, research has been done for identifying specific properties that reflect these universals and using them to automatically test these universals. For example, on a corpus of original (non196 translated) and translated texts in Italian, (Baroni and Bernardini, 2006) finds that features such as the distribution of function words, personal pronouns and adverbs are very relevant. (Pastor et al., 2008) explored the existence of the simplification universal – which states that translated texts are simpler than their source counterpart –, suggesting that this universal does affect Spanish translated texts. Also focusing on the simplification universal, the studies by (Ilisei et al., 2010; Ilisei and Inkpen, 2011) on Romanian and Spanish translationese use morphological and simplification-based features. A six-lingual study by (Halteren, 2008) using frequencies of word n-grams shows that it is possible to distinguish between translated and non-translated texts down to their respective original"
C12-2020,C10-2115,0,0.0203321,"exhibit significant and measurable differences. We build models based on various linguistically and statistically-motivated features. The models are tested on manually simulated and artificially generated plagiarism cases. Each case consists of a segment of text. Well-known machine learning algorithms are used for two tasks: binary classification and ranking. These two variations of the approach are evaluated in the same way: computing the accuracy of each algorithm in categorising segments as original or plagiarised. 3.1 Corpus This study uses the PAN-PC-10 plagiarism detection task corpus (Potthast et al., 2010), which comprises books from the Project Gutenberg.1 Two datasets were extracted from this corpus, as shown in Table 1. The segments are extracted according to the annotation provided in the corpus: pre-defined labels for manually simulated and artificial plagiarism sequences of words. The Artificial Dataset is composed of a randomly selected subset of plagiarised texts that were generated automatically by three types of edits: (i) a set of text operations, which include replacing, shuffling, removing or inserting words at random, (ii) semantic word variations by replacing words by similar or"
C12-2112,H05-1103,0,0.0511358,"Missing"
C12-2112,levy-andrew-2006-tregex,0,0.0535524,"Missing"
C12-2112,W03-0203,1,0.839805,"Missing"
C16-1069,W10-1607,0,0.295739,"Missing"
C16-1069,P11-2087,0,0.158825,"Missing"
C16-1069,J96-2004,0,0.496536,"Missing"
C16-1069,E99-1042,0,0.174289,"Missing"
C16-1069,C96-2183,0,0.738103,"ses feature valuable new insight on the relationship between the non-natives’ notion of complexity and various morphological, semantic and lexical word properties. Some of our findings contradict long-standing misconceptions about word simplicity. The data produced in our studies consists of 211,564 annotations made by 1,100 volunteers, which we hope will guide forthcoming research on Text Simplification for non-native speakers of English. 1 Introduction Text Simplification is a useful application both to improve other Natural Language Processing tasks and to assist language-impaired readers (Chandrasekar et al., 1996). When a simplifier aims to help people, understanding their needs becomes very important. In Lexical Simplification – the task of replacing complex words and expressions with simpler alternatives – this has been shown to be the case (Rello et al., 2013b; Rello et al., 2013a; Rello et al., 2013c). They describe several user studies conducted with readers suffering from Dyslexia and outline the most recurring challenges faced by them, as well as the most effective ways to overcome these challenges. Given the widespread availability of content in English, non-native speakers of English become an"
C16-1069,P15-2011,0,0.113051,"Missing"
C16-1069,P14-2075,0,0.0543055,"Missing"
C16-1069,O13-1007,0,0.110547,"Missing"
C16-1069,P13-1151,0,0.0955934,"Missing"
C16-1069,P03-1054,0,0.0194155,"Missing"
C16-1069,S07-1009,0,0.0360823,"Missing"
C16-1069,S10-1002,0,0.0607105,"Missing"
C16-1069,W13-4813,1,0.936196,"Missing"
C16-1069,P15-4015,1,0.84938,"Missing"
C16-1069,W16-4912,0,0.09625,"Missing"
C16-1069,N15-2002,1,0.884135,"Missing"
C16-1069,P13-3015,0,0.169869,"Missing"
C16-1069,W13-2908,0,0.0352526,"Missing"
C16-1069,shardlow-2014-open,0,0.257223,"Missing"
C16-1069,S12-1046,1,0.927886,"Missing"
C16-1157,N15-1156,0,0.0255822,"Missing"
C16-1157,W10-1505,0,0.0137854,"a sentence, a target word, and candidate substitutions ranked by simplicity. This dataset has been widely used and hence allows the comparison of SubIMDB against state-of-the-art solutions for the task. For evaluation, we use Spearman (r) and Pearson (ρ) correlation, as well as the TRank metric proposed by Specia et al. (2012), which measures the rate with which a candidate substitution with the highest gold rank i.e. the simplest, was ranked first by the system. We compare the performance of all frequency norms described in Section 3 to Google 1T, a corpus composed of over 1 trillion words (Evert, 2010), and the winner system in the SemEval 2012 task, which 1675 Norm KF HAL Wiki SimpleWiki SUBTLEX Open2016 SubIMDB SubMOV SubSER SubFAM r 0.619 0.630 0.575 0.626 0.649 0.650 0.654 0.660 0.648 0.649 ρ 0.626 0.633 0.583 0.632 0.649 0.647 0.652 0.658 0.647 0.650 TRank 0.589 0.598 0.516 0.570 0.619 0.619 0.622 0.623 0.619 0.615 F-test ••• ••• ••• ••• ••• ••• ••• ••• ••• Norm SubCOM SubCHI SubFAM-M SubFAM-S SubCOM-M SubCOM-S SubCHI-M SubCHI-S Google 1T Best SemEval r 0.655 0.643 0.653 0.647 0.660 0.647 0.650 0.640 N/A N/A ρ 0.653 0.645 0.653 0.650 0.658 0.648 0.654 0.644 N/A N/A TRank 0.623 0.611 0."
C16-1157,P14-2075,0,0.046101,".1 (•), p &lt; 0.01 (••) or p &lt; 0.001 (• • •) (F-test). employs a Support Vector Machine ranker that uses a wide array of features (Jauhar and Specia, 2012). The results in Table 94 reveal that SubIMDB outperforms all baselines, including Google 1T and the former state-of-the-art for the task in TRank. Nonetheless, some SubIMDB subcorpora are even more effective than using our corpus in its entirety, despite being much smaller. Work in Text Simplification has, however, explored more than single-word frequency norms, considering for example raw n-gram frequencies and language model probabilities (Horn et al., 2014; BaezaYates et al., 2015; Paetzold and Specia, 2016b). Table 10 shows TRank scores obtained on the SemEval 2012 task when using 3-gram and 5-gram raw frequencies and language model probabilities extracted from various corpora. The 3-grams and 5-grams consist in a candidate substitution surrounded by one and two tokens, respectively. For probabilities, we trained 5-gram language models using SRILM (Stolcke, 2002). For the Kucera-Francis (KF) norms we use the Brown corpus (Francis and Kucera, 1979). The HAL corpus is not available for download and hence it could not be tested here. Table 10 sho"
C16-1157,S12-1066,1,0.883235,"FAM-M SubFAM-S SubCOM-M SubCOM-S SubCHI-M SubCHI-S Google 1T Best SemEval r 0.655 0.643 0.653 0.647 0.660 0.647 0.650 0.640 N/A N/A ρ 0.653 0.645 0.653 0.650 0.658 0.648 0.654 0.644 N/A N/A TRank 0.623 0.611 0.618 0.620 0.623 0.618 0.600 0.608 0.585 0.602 F-test • ••• ••• ••• ••• ••• ••• ••• - Table 9: Correlation and TRank scores for frequency norms with respect to simplicity. The fifth column indicates a statistically significant difference with SubIMDB given p &lt; 0.1 (•), p &lt; 0.01 (••) or p &lt; 0.001 (• • •) (F-test). employs a Support Vector Machine ranker that uses a wide array of features (Jauhar and Specia, 2012). The results in Table 94 reveal that SubIMDB outperforms all baselines, including Google 1T and the former state-of-the-art for the task in TRank. Nonetheless, some SubIMDB subcorpora are even more effective than using our corpus in its entirety, despite being much smaller. Work in Text Simplification has, however, explored more than single-word frequency norms, considering for example raw n-gram frequencies and language model probabilities (Horn et al., 2014; BaezaYates et al., 2015; Paetzold and Specia, 2016b). Table 10 shows TRank scores obtained on the SemEval 2012 task when using 3-gram"
C16-1157,P13-1151,0,0.055492,"s (SubFAM-S) Comedy movies (SubCOM-M) Comedy series (SubCOM-S) Children movies (SubCHI-M) Children series (SubCHI-S) Table 4: Subcorpora from SubIMDB used to predict lexical decision times We compare ours to six frequency norms: • KF: Oldest and most widely used frequency norm, calculated over the Brown corpus (Rudell, 1993; Francis and Kucera, 1979). • HAL: Hyperspace Analogue to Language word frequency norm, calculated over the HAL corpus, which contains over 131 million words from Usenet newsgroups (Burgess and Livesay, 1998). • Wiki: Word frequencies from Wikipedia, with 97 million words (Kauchak, 2013). • SimpleWiki: Word frequencies from Simple Wikipedia, with 9 million words (Kauchak, 2013). • SUBTLEX: Word frequencies from SUBTLEXus , with 51 million words (Brysbaert and New, 2009). • Open2016: Word frequencies from OpenSubtitles2016, with 2 billion words (Lison and Tiedemann, 2016). We regularise all norms using Equation 1, in which f is the frequency norm value of a word w. This transformation has shown to best represent the relationship between word frequencies and lexical decision times (Balota et al., 2004). norm(f (w)) = log10 (f (w) + 1) (1) We use the same lexical decision datase"
C16-1157,L16-1147,0,0.165703,"extracted is one of its most important defining traits. For example, the experiments of Brysbaert and New (2009) and Shardlow (2013) reveal that frequencies from spoken text have a much stronger correlation with psycholinguistic word properties than those from other sources. Their findings greatly highlight the potential of spoken language text, but there are very few examples of resources of this kind available for English. SUBTLEXus is a notable exception: it contains texts extracted from 8,388 subtitles of American movies, and is freely available for download. The OpenSubtitles2016 corpus (Lison and Tiedemann, 2016) is another example, featuring sentences extracted from numerous subtitle files aligned at sentence level across 60 languages. This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 1669 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1669–1679, Osaka, Japan, December 11-17 2016. However, since the subtitles in these corpora are not restricted with respect to genre or domain, their proficiency in capturing everyday language can also be"
C16-1157,N16-1050,1,0.892455,"e a poor performance for the Kucera-Francis coefficient. Despite its use in numerous previous contributions (Burgess and Livesay, 1998; Zevin and Seidenberg, 2002; Brysbaert and New, 2009), more modern resources proved more effective. We believe this is caused by the fact that these coefficients are calculated from a corpus that is very small when compared to the other resources presented in this paper. 4 Predicting Psycholinguistic Properties In addition to lexical decision times, other psycholinguistic properties of words have been studied in terms of their correlation with frequency norms (Paetzold and Specia, 2016a). In this experiment, we evaluate how well the norms described in Section 3 correlate with four psycholinguistic properties extracted from the MRC psycholinguistic Database: • Familiarity: Available for 9,392 words – frequency with which a word is seen, heard or used daily. • Age of Acquisition: Available for 3,503 words – age at which a word is learned. • Concreteness: Available for 8,228 words – how “palpable” the object the word refers to is. • Imagery: Available for 9,240 words – intensity with which a word arouses images. The results in Table 6 reveal that SubFAM-M (family movies) perfo"
C16-1157,W16-4912,0,0.0733417,"e a poor performance for the Kucera-Francis coefficient. Despite its use in numerous previous contributions (Burgess and Livesay, 1998; Zevin and Seidenberg, 2002; Brysbaert and New, 2009), more modern resources proved more effective. We believe this is caused by the fact that these coefficients are calculated from a corpus that is very small when compared to the other resources presented in this paper. 4 Predicting Psycholinguistic Properties In addition to lexical decision times, other psycholinguistic properties of words have been studied in terms of their correlation with frequency norms (Paetzold and Specia, 2016a). In this experiment, we evaluate how well the norms described in Section 3 correlate with four psycholinguistic properties extracted from the MRC psycholinguistic Database: • Familiarity: Available for 9,392 words – frequency with which a word is seen, heard or used daily. • Age of Acquisition: Available for 3,503 words – age at which a word is learned. • Concreteness: Available for 8,228 words – how “palpable” the object the word refers to is. • Imagery: Available for 9,240 words – intensity with which a word arouses images. The results in Table 6 reveal that SubFAM-M (family movies) perfo"
C16-1157,pak-paroubek-2010-twitter,0,0.0110531,"word frequency analysis is that the type of resource used as a corpus is often built for a specific communication purpose, such as news (Burgess and Livesay, 1998). This is however not representative of everyday language usage, particularly from a psycholinguistic perspective. The other extreme of the spectrum features resources compiled from user-generated content, such as micro-blogs. However, these resources often suffer from grammar errors and misspellings, excessive use of acronyms and shortenings, partly due to the constrains of the publication means (e.g. limited number of characters) (Pak and Paroubek, 2010). This is particularly concerning given that previous research has shown that the source from which a corpus was extracted is one of its most important defining traits. For example, the experiments of Brysbaert and New (2009) and Shardlow (2013) reveal that frequencies from spoken text have a much stronger correlation with psycholinguistic word properties than those from other sources. Their findings greatly highlight the potential of spoken language text, but there are very few examples of resources of this kind available for English. SUBTLEXus is a notable exception: it contains texts extrac"
C16-1157,P13-3015,0,0.0445307,"olinguistic perspective. The other extreme of the spectrum features resources compiled from user-generated content, such as micro-blogs. However, these resources often suffer from grammar errors and misspellings, excessive use of acronyms and shortenings, partly due to the constrains of the publication means (e.g. limited number of characters) (Pak and Paroubek, 2010). This is particularly concerning given that previous research has shown that the source from which a corpus was extracted is one of its most important defining traits. For example, the experiments of Brysbaert and New (2009) and Shardlow (2013) reveal that frequencies from spoken text have a much stronger correlation with psycholinguistic word properties than those from other sources. Their findings greatly highlight the potential of spoken language text, but there are very few examples of resources of this kind available for English. SUBTLEXus is a notable exception: it contains texts extracted from 8,388 subtitles of American movies, and is freely available for download. The OpenSubtitles2016 corpus (Lison and Tiedemann, 2016) is another example, featuring sentences extracted from numerous subtitle files aligned at sentence level"
C16-1157,S12-1046,1,0.945539,"cting word simplicity. In this experiment, we evaluate how well SubIMDB fairs against other corpora when employed as a solution to Lexical Simplification. As our test set, we use the one from the English Lexical Simplification task of SemEval 2012, which contains 1,710 instances composed of a sentence, a target word, and candidate substitutions ranked by simplicity. This dataset has been widely used and hence allows the comparison of SubIMDB against state-of-the-art solutions for the task. For evaluation, we use Spearman (r) and Pearson (ρ) correlation, as well as the TRank metric proposed by Specia et al. (2012), which measures the rate with which a candidate substitution with the highest gold rank i.e. the simplest, was ranked first by the system. We compare the performance of all frequency norms described in Section 3 to Google 1T, a corpus composed of over 1 trillion words (Evert, 2010), and the winner system in the SemEval 2012 task, which 1675 Norm KF HAL Wiki SimpleWiki SUBTLEX Open2016 SubIMDB SubMOV SubSER SubFAM r 0.619 0.630 0.575 0.626 0.649 0.650 0.654 0.660 0.648 0.649 ρ 0.626 0.633 0.583 0.632 0.649 0.647 0.652 0.658 0.647 0.650 TRank 0.589 0.598 0.516 0.570 0.619 0.619 0.622 0.623 0.61"
C16-2017,W10-1607,0,0.0172874,"texts according to the needs of individual users, and its enhancement module allows the user to search for a word’s definitions, synonyms, translations, and visual cues through related images. These utilities are brought together in an easy-to-use interface of a freely available web browser extension. 1 Introduction Readers who suffer from reading impairments find it difficult to understand certain types of texts which, to an average reader, would not pose any challenge. Low literacy readers and second language learners, for example, often have very limited vocabulary (Watanabe et al., 2009; Aluisio and Gasperin, 2010), while those with Dyslexia may have problems understanding the meaning of rare and/or long words (Ellis, 1993; Rello et al., 2013b). Other notable examples of such conditions are Aphasia and some forms of Autism, which can also hinder the patient’s capability of comprehending sentences made up of a large amount of words and/or complex syntactic constructs (Devlin and Tait, 1998; Barbu et al., 2015). Previous work has proposed a wide array of approaches that aim to adapt texts for these audiences. Text Simplification strategies are good examples of that. While Lexical Simplification approaches"
C16-2017,N15-3024,0,0.124057,"15). Previous work has proposed a wide array of approaches that aim to adapt texts for these audiences. Text Simplification strategies are good examples of that. While Lexical Simplification approaches handle vocabulary limitations by replacing complex words with simpler alternatives (Devlin and Tait, 1998; Paetzold and Specia, 2016a), Syntactic Simplification approaches address the problem of long, complex syntactic constructs by re-structuring them (Siddharthan, 2006; Paetzold and Specia, 2013). Text Enhancement approaches can also help: Devlin and Unthank (2006), Watanabe et al. (2009) and Azab et al. (2015) adorn the words of a text with definitions, images and synonyms in order to facilitate their comprehension. Rello et al. (2013a) reveal that while simplification tends to increase a document’s readability, enhancement tends to improve its comprehensibility. One important limitation of the state of the art Text Adaptation systems is that they are not available for download and/or use. Online demos are provided for some, but they only allow the processing of small snippets of text through online interfaces. Another limitation is that the adaptations made by these systems are not personalised i."
C16-2017,W13-4813,1,0.846484,"ing sentences made up of a large amount of words and/or complex syntactic constructs (Devlin and Tait, 1998; Barbu et al., 2015). Previous work has proposed a wide array of approaches that aim to adapt texts for these audiences. Text Simplification strategies are good examples of that. While Lexical Simplification approaches handle vocabulary limitations by replacing complex words with simpler alternatives (Devlin and Tait, 1998; Paetzold and Specia, 2016a), Syntactic Simplification approaches address the problem of long, complex syntactic constructs by re-structuring them (Siddharthan, 2006; Paetzold and Specia, 2013). Text Enhancement approaches can also help: Devlin and Unthank (2006), Watanabe et al. (2009) and Azab et al. (2015) adorn the words of a text with definitions, images and synonyms in order to facilitate their comprehension. Rello et al. (2013a) reveal that while simplification tends to increase a document’s readability, enhancement tends to improve its comprehensibility. One important limitation of the state of the art Text Adaptation systems is that they are not available for download and/or use. Online demos are provided for some, but they only allow the processing of small snippets of tex"
C16-2017,P15-4015,1,0.807253,"user must select a word they do not understand. The reading assistance wizard depicted in Figure 1b will then pop-up. Anita currently offers two types of adaptation: Simplification and Enhancement. 3 Simplification Module Anita’s simplification module attempts to replace the selected word with a simpler alternative. To do so, Anita first finds the sentence containing the selected word and then sends this information to the remote server where Anita’s Lexical Simplification engine is running. The engine runs a state-of-the-art Lexical Simplification system powered by the LEXenstein framework (Paetzold and Specia, 2015). The strategy used here has been shown to outperform all other simplifiers from previous work (Paetzold and Specia, 2016a). Upon receiving a simplification request for a word, Anita’s simplifier performs the following steps: 1. Generation: A context-aware word embeddings model trained over 7 billion words which accounts for grammatical information (Paetzold and Specia, 2016b) is used to produce candidate substitutions for the word. 2. Selection: The Unsupervised Boundary Ranking approach (Paetzold and Specia, 2016b) is used to select the candidates that best fit the context of the complex wor"
C16-2017,W16-4912,0,0.117325,"93; Rello et al., 2013b). Other notable examples of such conditions are Aphasia and some forms of Autism, which can also hinder the patient’s capability of comprehending sentences made up of a large amount of words and/or complex syntactic constructs (Devlin and Tait, 1998; Barbu et al., 2015). Previous work has proposed a wide array of approaches that aim to adapt texts for these audiences. Text Simplification strategies are good examples of that. While Lexical Simplification approaches handle vocabulary limitations by replacing complex words with simpler alternatives (Devlin and Tait, 1998; Paetzold and Specia, 2016a), Syntactic Simplification approaches address the problem of long, complex syntactic constructs by re-structuring them (Siddharthan, 2006; Paetzold and Specia, 2013). Text Enhancement approaches can also help: Devlin and Unthank (2006), Watanabe et al. (2009) and Azab et al. (2015) adorn the words of a text with definitions, images and synonyms in order to facilitate their comprehension. Rello et al. (2013a) reveal that while simplification tends to increase a document’s readability, enhancement tends to improve its comprehensibility. One important limitation of the state of the art Text Ada"
C16-3004,P12-3024,0,0.0270183,"Missing"
C16-3004,P10-1064,0,0.0826527,"Missing"
C16-3004,2015.iwslt-papers.4,1,0.838642,"Missing"
C16-3004,L16-1582,1,0.769073,"n entire document, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). It is most useful for gisting purposes, where post-editing is not an option. Two shared tasks on document-level QE were organised at WMT15 and WMT16. An open research question when it comes to document-level QE is to define effective quality labels for entire documents (Scarton et al., 2015). A few QE frameworks have been proposed in the last couple of years, namely (Gonz`alez et al., 2012; Specia et al., 2013; Servan et al., 2015; Logacheva et al., 2016; Specia et al., 2015). Q U E ST++ (Specia et al., 2015)1 is the most widely used one. It is a significantly refactored and expanded version of Q U E ST (Specia et al., 2013). It has been used as the official baseline system during all editions of the WMT shared task on QE (WMT12-WMT16) and is often the starting point upon which other participants build their systems, particularly for feature extraction. It has two main modules: feature extraction and 1 https://github.com/ghpaetzold/questplusplus 15 machine learning. The feature extraction module can operate at sentence, word and document-leve"
C16-3004,2014.eamt-1.21,1,0.770324,"word-level QE over the years. An application that can benefit from word-level QE is spotting errors (incorrect words) in a post-editing/revision scenario. A recent variant of this task is quality prediction at the level of phrases (Logacheva and L.Specia, 2015; Blain et al., 2016), where a phrase can be defined in different ways, e.g. using the segmentation from a statistical MT decoder in WMT16 (Bojar et al., 2016). Document-level QE has received much less attention than the other levels. This task consists in predicting a single quality label for an entire document, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). It is most useful for gisting purposes, where post-editing is not an option. Two shared tasks on document-level QE were organised at WMT15 and WMT16. An open research question when it comes to document-level QE is to define effective quality labels for entire documents (Scarton et al., 2015). A few QE frameworks have been proposed in the last couple of years, namely (Gonz`alez et al., 2012; Specia et al., 2013; Servan et al., 2015; Logacheva et al., 2016; Specia et al., 2015). Q U E ST++ (Specia et a"
C16-3004,W15-4916,1,0.786531,"Missing"
C16-3004,2015.iwslt-papers.11,0,0.0579597,"e quality label for an entire document, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). It is most useful for gisting purposes, where post-editing is not an option. Two shared tasks on document-level QE were organised at WMT15 and WMT16. An open research question when it comes to document-level QE is to define effective quality labels for entire documents (Scarton et al., 2015). A few QE frameworks have been proposed in the last couple of years, namely (Gonz`alez et al., 2012; Specia et al., 2013; Servan et al., 2015; Logacheva et al., 2016; Specia et al., 2015). Q U E ST++ (Specia et al., 2015)1 is the most widely used one. It is a significantly refactored and expanded version of Q U E ST (Specia et al., 2013). It has been used as the official baseline system during all editions of the WMT shared task on QE (WMT12-WMT16) and is often the starting point upon which other participants build their systems, particularly for feature extraction. It has two main modules: feature extraction and 1 https://github.com/ghpaetzold/questplusplus 15 machine learning. The feature extraction module can operate at sentence"
C16-3004,2014.eamt-1.22,1,0.76135,"arly popular in the area of Machine Translation (MT). With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency by filtering out segments which would require more effort or time to correct than to translate from scratch (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory (He et al., 2010), selecting the best translation from multiple MT systems (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Sentence-level QE represents the vast majority of existing work. It has been addressed as a supervised machine learning task using a variety of algorithms to train models from examples of sentence translations annotated with quality labels (e.g. 1 to 5 likert scores). This prediction level has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012 (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016). Wh"
C16-3004,P10-1063,0,0.198553,"these instances and quality predictions are then produced by the model. Figure 2: QE model prediction. QE is a reasonably new field, but over the last decade has become particularly popular in the area of Machine Translation (MT). With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency by filtering out segments which would require more effort or time to correct than to translate from scratch (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory (He et al., 2010), selecting the best translation from multiple MT systems (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Sentence-level QE represents the vast majority of existing work. It has been addressed as a supervised machine learning task using a variety of algorithms to train models from examples of sentence translations annotated with quality labels (e.g. 1 to 5 likert scores). This prediction level has been covered in shared tasks organised by the Workshop o"
C16-3004,P13-4014,1,0.892777,"Missing"
C16-3004,P15-4020,1,0.730678,"an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). It is most useful for gisting purposes, where post-editing is not an option. Two shared tasks on document-level QE were organised at WMT15 and WMT16. An open research question when it comes to document-level QE is to define effective quality labels for entire documents (Scarton et al., 2015). A few QE frameworks have been proposed in the last couple of years, namely (Gonz`alez et al., 2012; Specia et al., 2013; Servan et al., 2015; Logacheva et al., 2016; Specia et al., 2015). Q U E ST++ (Specia et al., 2015)1 is the most widely used one. It is a significantly refactored and expanded version of Q U E ST (Specia et al., 2013). It has been used as the official baseline system during all editions of the WMT shared task on QE (WMT12-WMT16) and is often the starting point upon which other participants build their systems, particularly for feature extraction. It has two main modules: feature extraction and 1 https://github.com/ghpaetzold/questplusplus 15 machine learning. The feature extraction module can operate at sentence, word and document-level and includes the ext"
C16-3004,2011.eamt-1.12,1,0.848372,"re used to train the QE model are extracted from these instances and quality predictions are then produced by the model. Figure 2: QE model prediction. QE is a reasonably new field, but over the last decade has become particularly popular in the area of Machine Translation (MT). With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency by filtering out segments which would require more effort or time to correct than to translate from scratch (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory (He et al., 2010), selecting the best translation from multiple MT systems (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Sentence-level QE represents the vast majority of existing work. It has been addressed as a supervised machine learning task using a variety of algorithms to train models from examples of sentence translations annotated with quality labels (e.g. 1 to 5 likert scores). This prediction leve"
C16-3004,2015.eamt-1.17,1,\N,Missing
C18-1266,P04-3031,0,0.113865,"of sentence i, and D is the size of the document in sentences,8 and (iii), a variant of W B LEU by weighting each sentence by its T F I DF score computed with regard P to its aligned reference (T B LEU). The numerator in the W B LEU equation is therefore replaced by: D i=1 T F I DF i B LEU i . Here, for each news document, we learn a T F I DF model on its reference in the target language, and compute the T F I DF score for each translated sentence, based on that model. 6 We performed Kolmogorov–Smirnov test for not normally distributed data. We compute B LEU scores with the NLP toolkit NLTK (Bird and Loper, 2004). For scoring documents, we used the corpus bleu() function. For sentence-level scores we used the sentence bleu() function with smoothing method 7. 8 According to Chen and Cherry (2014), W B LEU achieves a better correlation with human judgement than the original IBM corpus-level B LEU. 7 3151 Following Turchi et al. (2012), our intuition is that the document-level score should reflect the overall translation quality at sentence level, weighted by how important each individual sentence (important sentences have important words) is in that document. Data: We gathered all submissions at the WMT"
C18-1266,C04-1046,0,0.174729,"Missing"
C18-1266,W14-3302,1,0.83133,"Missing"
C18-1266,W15-3001,1,0.807105,"itecture for document-level QE. In addition, for the first time we apply QE models to the output of both statistical and neural MT systems for a series of European languages and highlight the new challenges resulting from the use of neural MT. 1 Introduction Quality Estimation (QE) (Blatz et al., 2004; Specia et al., 2009) aims at predicting the quality of machine translation (MT) without human intervention. Most recent work has focused on QE to predict sentencelevel post-editing (PE) effort, i.e. the process of manually correcting MT output to achieve publishable quality (Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016a; Bojar et al., 2017). In this case, QE indicates to what extent a MT sentence needs post-editing. Document-level QE, on the other hand, scores or ranks documents according to their quality for fully automated MT usage scenarios where no post-editing can be performed, e.g. MT for gisting of news articles online. Recently, neural methods have been successfully exploited to improve QE performance. These methods mostly rely on either complex architectures, require extensive pre-training, or need some feature engineering (Patel and M, 2016; Kim et al., 2017a; Martins et al., 2"
C18-1266,W14-3346,0,0.0189203,"eference (T B LEU). The numerator in the W B LEU equation is therefore replaced by: D i=1 T F I DF i B LEU i . Here, for each news document, we learn a T F I DF model on its reference in the target language, and compute the T F I DF score for each translated sentence, based on that model. 6 We performed Kolmogorov–Smirnov test for not normally distributed data. We compute B LEU scores with the NLP toolkit NLTK (Bird and Loper, 2004). For scoring documents, we used the corpus bleu() function. For sentence-level scores we used the sentence bleu() function with smoothing method 7. 8 According to Chen and Cherry (2014), W B LEU achieves a better correlation with human judgement than the original IBM corpus-level B LEU. 7 3151 Following Turchi et al. (2012), our intuition is that the document-level score should reflect the overall translation quality at sentence level, weighted by how important each individual sentence (important sentences have important words) is in that document. Data: We gathered all submissions at the WMT News shared tasks for various years. This is a task where each participating system is required to translate a set of news documents.9 This results in a large set of language pairs, as"
C18-1266,D14-1179,0,0.0184478,"Missing"
C18-1266,E17-2057,0,0.0138072,"). Shorter documents were extended with dummy sentences to fit to this length, which is a common practice in the field (Hewlett et al., 2017). Results Results of our experiments are reported in Tables 3 (baseline) and 4 (neural approaches). For both POSTECH and BI-RNN, we use only the last hidden states of the document decoder (Last) – a configuration that has been chosen empirically as the best performing among the configurations without attention, or the vector sum weighted by the attention mechanism (Att) as input to the output layer. 9 We considered using the document-level QE dataset in (Graham et al., 2017), however, the small number of documents (62) and language pairs (only one) made this resource less appealing for this work. 10 Individual submissions are not available but system combinations only 11 In (FILT) we have also discarded submissions for years 2008 and 2009, since official system-level scores are not available. 12 https://github.com/fredblain/docQE 13 http://www.statmt.org/wmt18/translation-task.html;http://www.statmt.org/wmt13/ translation-task.html 3152 set FILT ALL dev test FILT ALL dev test FILT ALL dev test FILT ALL dev test # docs 1147 1147 140 140 420 420 51 51 894 894 109 1"
C18-1266,W17-4763,0,0.244082,"jar et al., 2014; Bojar et al., 2015; Bojar et al., 2016a; Bojar et al., 2017). In this case, QE indicates to what extent a MT sentence needs post-editing. Document-level QE, on the other hand, scores or ranks documents according to their quality for fully automated MT usage scenarios where no post-editing can be performed, e.g. MT for gisting of news articles online. Recently, neural methods have been successfully exploited to improve QE performance. These methods mostly rely on either complex architectures, require extensive pre-training, or need some feature engineering (Patel and M, 2016; Kim et al., 2017a; Martins et al., 2017a; Jhaveri et al., 2018). In addition, these methods have only been developed for word, phrase and sentence-level QE. These cannot be directly used for document-level QE since this level requires to take into account the content of the document in its entirety. State-of-the-art document-level QE solutions still rely on non-neural methods, and extensive feature engineering (Scarton et al., 2016). In this paper we propose a neural framework that is able to accommodate any QE approach at a finegrained level (e.g. a sentence-level approach), and to generalize it to learn doc"
C18-1266,2005.mtsummit-papers.11,0,0.335219,"Missing"
C18-1266,P15-1107,0,0.0219369,"m. We apply the following attention function computing a normalized weight for each hidden state hj : exp(Wa h> j ) αj = PJ . >) exp(W h a k=1 k The resulting sentence vector is thus a weighted sum of word vectors: v = output layer takes this vector as input and produces real-value quality scores.2 3.2 (1) PJ j=1 αj hj . A sigmoid Document-level architecture Our document-level framework uses a bi-RNN encoder. RNNs have been successfully used for document representation (Lin et al., 2015) and applied to a series of downstream tasks such as topic labeling, summarization, and question answering (Li et al., 2015; Yang et al., 2016). The document-level quality predictor takes as input a set of sentence-level representations. The last hidden state of the decoder is the summary of an entire sequence. The sum, the maximum, or the average of hidden states for each sentence can then be provided to the output layer. Our assumption is that document-level QE scores are not a simple aggregations of sentence-level QE scores: they should reflect some notion of the importance of sentences within a document. To do so, we use an attention mechanism (Equation 1) to learn weights of different representations (differe"
C18-1266,D15-1106,0,0.0271276,"in a sentence. Thus, weights should be applied to those representations. Such weighting is provided by the attention mechanism. We apply the following attention function computing a normalized weight for each hidden state hj : exp(Wa h> j ) αj = PJ . >) exp(W h a k=1 k The resulting sentence vector is thus a weighted sum of word vectors: v = output layer takes this vector as input and produces real-value quality scores.2 3.2 (1) PJ j=1 αj hj . A sigmoid Document-level architecture Our document-level framework uses a bi-RNN encoder. RNNs have been successfully used for document representation (Lin et al., 2015) and applied to a series of downstream tasks such as topic labeling, summarization, and question answering (Li et al., 2015; Yang et al., 2016). The document-level quality predictor takes as input a set of sentence-level representations. The last hidden state of the decoder is the summary of an entire sequence. The sum, the maximum, or the average of hidden states for each sentence can then be provided to the output layer. Our assumption is that document-level QE scores are not a simple aggregations of sentence-level QE scores: they should reflect some notion of the importance of sentences wit"
C18-1266,Q17-1015,0,0.379901,"ojar et al., 2015; Bojar et al., 2016a; Bojar et al., 2017). In this case, QE indicates to what extent a MT sentence needs post-editing. Document-level QE, on the other hand, scores or ranks documents according to their quality for fully automated MT usage scenarios where no post-editing can be performed, e.g. MT for gisting of news articles online. Recently, neural methods have been successfully exploited to improve QE performance. These methods mostly rely on either complex architectures, require extensive pre-training, or need some feature engineering (Patel and M, 2016; Kim et al., 2017a; Martins et al., 2017a; Jhaveri et al., 2018). In addition, these methods have only been developed for word, phrase and sentence-level QE. These cannot be directly used for document-level QE since this level requires to take into account the content of the document in its entirety. State-of-the-art document-level QE solutions still rely on non-neural methods, and extensive feature engineering (Scarton et al., 2016). In this paper we propose a neural framework that is able to accommodate any QE approach at a finegrained level (e.g. a sentence-level approach), and to generalize it to learn document-level QE models."
C18-1266,W17-4764,0,0.19891,"ojar et al., 2015; Bojar et al., 2016a; Bojar et al., 2017). In this case, QE indicates to what extent a MT sentence needs post-editing. Document-level QE, on the other hand, scores or ranks documents according to their quality for fully automated MT usage scenarios where no post-editing can be performed, e.g. MT for gisting of news articles online. Recently, neural methods have been successfully exploited to improve QE performance. These methods mostly rely on either complex architectures, require extensive pre-training, or need some feature engineering (Patel and M, 2016; Kim et al., 2017a; Martins et al., 2017a; Jhaveri et al., 2018). In addition, these methods have only been developed for word, phrase and sentence-level QE. These cannot be directly used for document-level QE since this level requires to take into account the content of the document in its entirety. State-of-the-art document-level QE solutions still rely on non-neural methods, and extensive feature engineering (Scarton et al., 2016). In this paper we propose a neural framework that is able to accommodate any QE approach at a finegrained level (e.g. a sentence-level approach), and to generalize it to learn document-level QE models."
C18-1266,P02-1040,0,0.10233,"0.012 0.183 ±0.004 ρ Table 1: Performance on sentence-level predictions for the baseline with QuEst, the POSTECH architecture and our BI-RNN architecture for EN–DE and EN–LV (average and error margins over five runs). We highlight the best performing systems for a dataset. 4.2 Document-level predictions As introduced above, our framework relies on representations at sentence level to produce its predictions at document level. Therefore, we experiment with sentence-level representations from either the POSTECH or our BI-RNN predictor. The document-level labels we predict are variants of B LEU (Papineni et al., 2002): (i) document-level B LEU,7 (ii) the weighted average of sentence-level B LEU for all sentences in the document, where the weights correspond to the reference lengths: PD W B LEU d = i=1 len(Ri )B LEU i , PD i=1 len(Ri ) where B LEUi is the B LEU score of sentence i, and D is the size of the document in sentences,8 and (iii), a variant of W B LEU by weighting each sentence by its T F I DF score computed with regard P to its aligned reference (T B LEU). The numerator in the W B LEU equation is therefore replaced by: D i=1 T F I DF i B LEU i . Here, for each news document, we learn a T F I DF m"
C18-1266,W16-2389,0,0.0265817,"Missing"
C18-1266,W16-2391,1,0.899258,"lly exploited to improve QE performance. These methods mostly rely on either complex architectures, require extensive pre-training, or need some feature engineering (Patel and M, 2016; Kim et al., 2017a; Martins et al., 2017a; Jhaveri et al., 2018). In addition, these methods have only been developed for word, phrase and sentence-level QE. These cannot be directly used for document-level QE since this level requires to take into account the content of the document in its entirety. State-of-the-art document-level QE solutions still rely on non-neural methods, and extensive feature engineering (Scarton et al., 2016). In this paper we propose a neural framework that is able to accommodate any QE approach at a finegrained level (e.g. a sentence-level approach), and to generalize it to learn document-level QE models. We test the framework using a state of the art neural sentence-level QE approach (Kim et al., 2017b), which uses a complex architecture and requires resource-intensive pre-training, and a light-weight neural approach employing simple encoders and no pre-training. Our sentence-level prediction approach leads to comparable or better results than the state of the art at a much lower cost. Addition"
C18-1266,2009.eamt-1.5,1,0.914105,"Missing"
C18-1266,P15-4020,1,0.811086,"ranslations can be characterized by strongly focused attention connections. However, this internal information has not been proved to map directly into translation quality: a very weak correlation with human judgements in a small-scale assessment was reported. Therefore, this is the first time that experiments are performed with fully fledged, MT system-independent QE models for NMT. 3 A neural-based architecture for QE Our framework performs multi-level translation quality prediction, which has been shown to be successful in both traditional feature-engineered QE frameworks, such as QuEst++ (Specia et al., 2015), and neural QE architectures (Kim et al., 2017a; Martins et al., 2017a). In such architectures, the representations at a given level rely on representations from more fine-grained levels (i.e. sentences for document, and words for sentence). This is motivated by the nature of the task at hand: a document that is composed of high quality sentences is likely to have high quality as well. However, simply aggregating sentence-level predictions is not a good strategy, as a document needs to be cohesive and coherent as a whole, i.e. sentences cannot be considered completely in isolation, and thus t"
C18-1266,tiedemann-2012-parallel,0,0.0941006,"Missing"
C18-1266,2012.eamt-1.39,1,0.883618,"Missing"
C18-1266,N16-1174,0,0.0117727,"ollowing attention function computing a normalized weight for each hidden state hj : exp(Wa h> j ) αj = PJ . >) exp(W h a k=1 k The resulting sentence vector is thus a weighted sum of word vectors: v = output layer takes this vector as input and produces real-value quality scores.2 3.2 (1) PJ j=1 αj hj . A sigmoid Document-level architecture Our document-level framework uses a bi-RNN encoder. RNNs have been successfully used for document representation (Lin et al., 2015) and applied to a series of downstream tasks such as topic labeling, summarization, and question answering (Li et al., 2015; Yang et al., 2016). The document-level quality predictor takes as input a set of sentence-level representations. The last hidden state of the decoder is the summary of an entire sequence. The sum, the maximum, or the average of hidden states for each sentence can then be provided to the output layer. Our assumption is that document-level QE scores are not a simple aggregations of sentence-level QE scores: they should reflect some notion of the importance of sentences within a document. To do so, we use an attention mechanism (Equation 1) to learn weights of different representations (different sentences). The w"
D14-1131,W09-0437,0,0.0142068,"the leftmost uncovered source position (Lopez, 2009). This is a widely used strategy and it is in use in the Moses toolkit (Koehn et al., 2007).8 Nevertheless, the problem of finding the best 7 If d is a maximum from g and g(d) = f (d), then it is easy to show by contradiction that d is the actual maximum from f : if there existed d0 such that f (d0 ) &gt; f (d), then it follows that g(d0 ) ≥ f (d0 ) &gt; f (d) = g(d), and hence d would not be a maximum for g. 8 A distortion limit characterises a form of pruning that acts directly in the generative capacity of the model leading to induction errors (Auli et al., 2009). Limiting reordering like that lowers complexity to a polynomial function of I and an exponential function of the distortion limit. 1240 derivation under the model remains impracticable due to nonlocal parameterisation (namely, the n-gram LM component). The weighted set hD(x), f (d)i, which represents the objective, is a complex hypergraph which we cannot afford to construct. We propose to construct instead a simpler hypergraph for which optimisation by dynamic programming is feasible. This proxy rep resents the weighted set D(x), g (0) (d) , where g (0) (d) ≥ f (d) for every d ∈ D(x). Note t"
D14-1131,W13-2260,1,0.88383,"1997; Och et al., 2001), thus it is common to resort to heuristic techniques in order to find an approximation to d∗ (Koehn et al., 2003; Chiang, 2007). Evaluation exercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Other work has employed less common approximations to the model reducing its search space complexity (Kumar et al., 2006; Chang and Collins, 2011; Rush and Collins, 2011). These do not answer whether or not current decoding algorithms perform well at real translation tasks with state-of-the-art models. We propose an exact decoder for phrase-based SMT based on a coarse-to-fine search strategy (Dymetman et al., 2012). In a nutshell, we relax the decoding problem with respect to the Language Model (LM) component. This coarse view is incrementally refined based on evidence col1237 Proceedings of"
D14-1131,J93-2003,0,0.06433,"siting groups using a priority queue: if the top group contains a single hypothesis, the hypothesis is added to the beam, otherwise the group is partitioned and the parts are pushed back to the queue. More recently, Heafield et al. (2014) applied their beam filling algorithm to phrase-based decoding. 3.2 Exact optimisation Exact optimisation for monotone translation has been done using A∗ search (Tillmann et al., 1997) and finite-state operations (Kumar et al., 2006). Och et al. (2001) design near-admissible heuristics for A∗ and decode very short sentences (614 words) for a word-based model (Brown et al., 1993) with a maximum distortion strategy (d = 3). Zaslavskiy et al. (2009) frame phrase-based decoding as an instance of a generalised Travelling Salesman Problem (TSP) and rely on robust solvers to perform decoding. In this view, a salesman graph encodes the translation options, with each node representing a biphrase. Nonoverlapping constraints are imposed by the TSP solver, rather than encoded directly in the salesman graph. They decode only short sentences (17 words on average) using a 2-gram LM due to salesman graphs growing too large.4 Chang and Collins (2011) relax phrase-based models w.r.t."
D14-1131,D12-1103,1,0.792042,"esents a left-shift. The rule N ON - ADJACENT handles the remaining cases i &gt; l provided that the expansion skips at most d input words |r − i + 1 |≤ d. In the consequent, the window C is simply updated to record the translation of the input span i..i0 . In the nonadjacent case, a gap constraint imposes that the resulting item will require skipping no more than d positions before the leftmost uncovered word is translated |i0 − l + 1 |≤ d.10 Finally, note that deductions incorporate the weighted upperbound ω(·), rather than the true LM component ψ(·).11 4.3 LM upperbound and Max-ARPA Following Carter et al. (2012) we compute an upperbound on n-gram conditional probabilities by precomputing max-backoff weights stored in a “Max-ARPA” table, an extension of the ARPA format (Jurafsky and Martin, 2000). A standard ARPA table T stores entries 10 This constraint prevents items from becoming dead-ends where incomplete derivations require a reordering step larger than d. This is known to prevent many search errors in beam search (Chang and Collins, 2011). 11 Unlike Aziz et al. (2013), rather than unigrams only, we score all n-grams within a translation rule (including incomplete ones). 1241 hZ, Z.p, Z.bi, where"
D14-1131,J07-2003,0,0.49343,"d = he1 , e2 , . . . , el i is a sequence of l steps. Under this assumption, each step is assigned the weight w(e) = Λ·hh1 (e), h2 (e), . . . , hm (e)i. The set D is typically finite, however, it contains a very large number of structures — exponential (or even factorial, see §2) with the size of x — making exhaustive enumeration prohibitively slow. Only in very restricted cases combinatorial optimisation techniques are directly applicable (Tillmann et al., 1997; Och et al., 2001), thus it is common to resort to heuristic techniques in order to find an approximation to d∗ (Koehn et al., 2003; Chiang, 2007). Evaluation exercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Other work has employed less common approximations to the model reducing its search space complexity (Kumar et al., 2006; Chang and"
D14-1131,W13-2212,0,0.0177391,"t. the upperbound distribution) due to an extended context. LM 5 LM Experiments We used the dataset made available by the Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013) to train a German-English phrase-based system using the Moses toolkit (Koehn et al., 2007) in a standard setup. For phrase extraction, we used both Europarl (Koehn, 2005) and News Commentaries (NC) totalling about 2.2M sentences.15 For language modelling, in addition to the monolingual parts of Europarl 15 Pre-processing: tokenisation, truecasing and automatic compound-splitting (German only). Following Durrani et al. (2013), we set the maximum phrase length to 5. where w0 = λψ qLM (yγ0 ) qLM (γ0 ) Figure 4: Local intersection via LM right state refinement. The input is a hypergraph G = hV, Ei, a node v0 ∈ V singly identified by its carry [l0 , C0 , r0 , γ0 ] and a left-extension y for its LM context γE0 . The program copies most of the edges D w → v ∈ E. If a derivation goes through v0 uσ − and the string under v0 ends in yγ0 , the program refines and reweights it. and NC, we added News-2013 totalling about 25M sentences. We performed language model interpolation and batch-mira tuning (Cherry and Foster, 2012) u"
D14-1131,W12-6106,1,0.916844,"exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Other work has employed less common approximations to the model reducing its search space complexity (Kumar et al., 2006; Chang and Collins, 2011; Rush and Collins, 2011). These do not answer whether or not current decoding algorithms perform well at real translation tasks with state-of-the-art models. We propose an exact decoder for phrase-based SMT based on a coarse-to-fine search strategy (Dymetman et al., 2012). In a nutshell, we relax the decoding problem with respect to the Language Model (LM) component. This coarse view is incrementally refined based on evidence col1237 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1237–1249, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics lected via maximisation. A refinement increases the complexity of the model only slightly, hence dynamic programming remains feasible throughout the search until convergence. We test our decoding strategy with realistic models using stand"
D14-1131,D11-1003,0,0.642929,"ang, 2007). Evaluation exercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Other work has employed less common approximations to the model reducing its search space complexity (Kumar et al., 2006; Chang and Collins, 2011; Rush and Collins, 2011). These do not answer whether or not current decoding algorithms perform well at real translation tasks with state-of-the-art models. We propose an exact decoder for phrase-based SMT based on a coarse-to-fine search strategy (Dymetman et al., 2012). In a nutshell, we relax the decoding problem with respect to the Language Model (LM) component. This coarse view is incrementally refined based on evidence col1237 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1237–1249, c October 25-29, 2014, Doha, Qatar. 2014 Associa"
D14-1131,P01-1030,0,0.385025,"ues are directly applicable (Tillmann et al., 1997; Och et al., 2001), thus it is common to resort to heuristic techniques in order to find an approximation to d∗ (Koehn et al., 2003; Chiang, 2007). Evaluation exercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Other work has employed less common approximations to the model reducing its search space complexity (Kumar et al., 2006; Chang and Collins, 2011; Rush and Collins, 2011). These do not answer whether or not current decoding algorithms perform well at real translation tasks with state-of-the-art models. We propose an exact decoder for phrase-based SMT based on a coarse-to-fine search strategy (Dymetman et al., 2012). In a nutshell, we relax the decoding problem with respect to the Language Model (LM) component. This coarse view is incrementally refin"
D14-1131,N12-1047,0,0.0208928,"wing Durrani et al. (2013), we set the maximum phrase length to 5. where w0 = λψ qLM (yγ0 ) qLM (γ0 ) Figure 4: Local intersection via LM right state refinement. The input is a hypergraph G = hV, Ei, a node v0 ∈ V singly identified by its carry [l0 , C0 , r0 , γ0 ] and a left-extension y for its LM context γE0 . The program copies most of the edges D w → v ∈ E. If a derivation goes through v0 uσ − and the string under v0 ends in yγ0 , the program refines and reweights it. and NC, we added News-2013 totalling about 25M sentences. We performed language model interpolation and batch-mira tuning (Cherry and Foster, 2012) using newstest2010 (2,849 sentence pairs). For tuning we used cube pruning with a large beam size (k = 5000) and a distortion limit d = 4. Unpruned language models were trained using lmplz (Heafield et al., 2013b) which employs modified Kneser-Ney smoothing (Kneser and Ney, 1995). We report results on newstest2012. Our exact decoder produces optimal translation derivations for all the 3,003 sentences in the test set. Table 1 summarises the performance of our novel decoder for language models of order n = 3 to n = 5. For 3-gram LMs we also varied the distortion limit d (from 4 to 6). We report"
D14-1131,J99-4004,0,0.12781,"Missing"
D14-1131,P05-1033,0,0.0283693,"noverlapping constraints are imposed by the TSP solver, rather than encoded directly in the salesman graph. They decode only short sentences (17 words on average) using a 2-gram LM due to salesman graphs growing too large.4 Chang and Collins (2011) relax phrase-based models w.r.t. the non-overlapping constraints, which are replaced by soft penalties through Lagrangian multipliers, and intersect the LM component exhaustively. They do employ a maximum distortion limit (d = 4), thus the problem they tackle is no longer NP-complete. Rush and Collins (2011) relax a hierarchical phrase-based model (Chiang, 2005)5 w.r.t. the LM component. The translation forest and the language model trade their weights (through Lagrangian multipliers) so as to ensure agreement on what each component believes to be the maximum. In both approaches, when the dual converges to a compliant solution, the solution is guaranteed to be optimal. Other4 Exact decoding had been similarly addressed with Integer Linear Programming (ILP) in the context of word-based models for very short sentences using a 2-gram LM (Germann et al., 2001). Riedel and Clarke (2009) revisit that formulation and employ a cutting-plane algorithm (Dantzi"
D14-1131,N13-1116,0,0.108849,"rding to common traits and a fast-to-compute heuristic view of outside weights (cheapest way to complete a hypothesis) puts them to compete at a fairer level. Beam search exhausts a node’s possible expansions, scores them, and discards all but the k highest-scoring ones. This process is wasteful in that k is typically much smaller than the number of possible expansions. Cube pruning employs a priority queue at beam filling and computes k highscoring expansions directly in near best-first order. The parameter k is known as beam size and it controls the time-accuracy trade-off of the algorithm. Heafield et al. (2013a) move away from using the language model as a black-box and build a more involved beam filling algorithm. Even though they target approximate search, some of their ideas have interesting connections to ours (see §4). They group hypotheses that share partial language model state (Li and Khudanpur, 2008) reasoning over multiple hypotheses at once. They fill a beam in best-first order by iteratively visiting groups using a priority queue: if the top group contains a single hypothesis, the hypothesis is added to the beam, otherwise the group is partitioned and the parts are pushed back to the qu"
D14-1131,P13-2121,0,0.075138,"Missing"
D14-1131,P14-2022,0,0.0232288,"Missing"
D14-1131,P07-1019,0,0.0131256,"erised and contains all translation derivations (a translation lattice or forest), and one that re-ranks the first as a function of the interactions between translation steps. The model of translational equivalences parameterised only with φ is an instance of the former. An n-gram LM component is an instance of the latter. 2.1 Hypergraphs A backward-hypergraph, or simply hypergraph, is a generalisation of a graph where edges have multiple origins and one destination (Gallo et al., 1993). They can represent both finite-state and context-free weighted sets and they have been widely used in SMT (Huang and Chiang, 2007). A hypergraph is defined by a set of nodes (or ver3 Figure 1 can be seen as a specification for a weighted acyclic finite-state automaton whose states are indexed by [l, C, r] and transitions are labelled with biphrases. However, for generality of representation, we opt for using acyclic hypergraphs instead of automata (see §2.1). 1238 tices) V and a weighted set of edges hE, wi. An edge e connects a sequence of nodes in its tail t[e] ∈ V ∗ under a head node h[e] ∈ V and has weight w(e). A node v is a terminal node if it has no incoming edges, otherwise it is a nonterminal node. The node that"
D14-1131,E09-1044,0,0.0282357,"Missing"
D14-1131,J00-4006,0,0.0193412,"w C is simply updated to record the translation of the input span i..i0 . In the nonadjacent case, a gap constraint imposes that the resulting item will require skipping no more than d positions before the leftmost uncovered word is translated |i0 − l + 1 |≤ d.10 Finally, note that deductions incorporate the weighted upperbound ω(·), rather than the true LM component ψ(·).11 4.3 LM upperbound and Max-ARPA Following Carter et al. (2012) we compute an upperbound on n-gram conditional probabilities by precomputing max-backoff weights stored in a “Max-ARPA” table, an extension of the ARPA format (Jurafsky and Martin, 2000). A standard ARPA table T stores entries 10 This constraint prevents items from becoming dead-ends where incomplete derivations require a reordering step larger than d. This is known to prevent many search errors in beam search (Chang and Collins, 2011). 11 Unlike Aziz et al. (2013), rather than unigrams only, we score all n-grams within a translation rule (including incomplete ones). 1241 hZ, Z.p, Z.bi, where Z is an n-gram equal to the concatenation Pz of a prefix P with a word z, Z.p is the conditional probability p(z|P), and Z.b is a so-called “backoff” weight associated with Z. The condit"
D14-1131,J99-4005,0,0.273875,"m starts from its axioms and follows exhaustively deducing new items by combination of existing ones and no deduction happens twice. In Figure 1, a nonteminal item summarises partial derivation (or hypotheses). It is denoted by [C, r, γ] (also known as carry), where: C is a coverage vector, necessary to impose the non-overlapping constraint; r is the rightmost position most recently covered, necessary for the computation of δ; and γ is the last n − 1 words 1 Preventing phrases from overlapping requires an exponential number of constraints (the powerset of x) rendering the problem NP-complete (Knight, 1999). 2 Weighted logics have been extensively used to describe weighted sets (Lopez, 2009), operations over weighted sets (Chiang, 2007; Dyer and Resnik, 2010), and a variety of dynamic programming algorithms (Cohen et al., 2008).   I TEM {0, 1}I , [0, I + 1], ∆n−1 G OAL 1I , I + 1, EOS A XIOM hBOS → BOSi [0I , 0, BOS] : ψ(BOS) E XPAND   D i0 φr j 0 E j−1 − → yj xi − C, r, yj−n+1 Li0 ¯ h i k=i ck = 0 0 j C 0 , i0 , yj 0 −n+2 : w where c0k = ck if k &lt; i or k &gt; i0 else ¯ 1 0 j−1 w = φr ⊗ δ(r, i) ⊗ ψ(yjj |yj−n+1 ) ACCEPT  I  1 , r, γ r≤I [1I , I + 1, EOS] : δ(r, I + 1) ⊗ ψ(EOS|γ) Figure 1: Sp"
D14-1131,N03-1017,0,0.307503,"s independently and d = he1 , e2 , . . . , el i is a sequence of l steps. Under this assumption, each step is assigned the weight w(e) = Λ·hh1 (e), h2 (e), . . . , hm (e)i. The set D is typically finite, however, it contains a very large number of structures — exponential (or even factorial, see §2) with the size of x — making exhaustive enumeration prohibitively slow. Only in very restricted cases combinatorial optimisation techniques are directly applicable (Tillmann et al., 1997; Och et al., 2001), thus it is common to resort to heuristic techniques in order to find an approximation to d∗ (Koehn et al., 2003; Chiang, 2007). Evaluation exercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Other work has employed less common approximations to the model reducing its search space complexity (Kumar et al., 2"
D14-1131,P07-2045,0,0.0134732,"d), for some finite t. At which point dt is the optimum derivation d∗ from f and the sequence of upperbounds provides a proof of optimality.7 4.2 Model We work with phrase-based models in a standard parameterisation (Equation 2). However, to avoid having to deal with NP-completeness, we constrain reordering to happen only within a limited window given by a notion of distortion limit. We require that the last source word covered by any biphrase must be within d words from the leftmost uncovered source position (Lopez, 2009). This is a widely used strategy and it is in use in the Moses toolkit (Koehn et al., 2007).8 Nevertheless, the problem of finding the best 7 If d is a maximum from g and g(d) = f (d), then it is easy to show by contradiction that d is the actual maximum from f : if there existed d0 such that f (d0 ) &gt; f (d), then it follows that g(d0 ) ≥ f (d0 ) &gt; f (d) = g(d), and hence d would not be a maximum for g. 8 A distortion limit characterises a form of pruning that acts directly in the generative capacity of the model leading to induction errors (Auli et al., 2009). Limiting reordering like that lowers complexity to a polynomial function of I and an exponential function of the distortion"
D14-1131,2005.mtsummit-papers.11,0,0.0460421,"NE, which instead of copying them, creates new ones headed by a refined version of v0 . Finally, R EWEIGHT continues from the refined node with reweighted copies of the edges leaving v0 . The weight update represents a change in LM probability (w.r.t. the upperbound distribution) due to an extended context. LM 5 LM Experiments We used the dataset made available by the Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013) to train a German-English phrase-based system using the Moses toolkit (Koehn et al., 2007) in a standard setup. For phrase extraction, we used both Europarl (Koehn, 2005) and News Commentaries (NC) totalling about 2.2M sentences.15 For language modelling, in addition to the monolingual parts of Europarl 15 Pre-processing: tokenisation, truecasing and automatic compound-splitting (German only). Following Durrani et al. (2013), we set the maximum phrase length to 5. where w0 = λψ qLM (yγ0 ) qLM (γ0 ) Figure 4: Local intersection via LM right state refinement. The input is a hypergraph G = hV, Ei, a node v0 ∈ V singly identified by its carry [l0 , C0 , r0 , γ0 ] and a left-extension y for its LM context γE0 . The program copies most of the edges D w → v ∈ E. If a"
D14-1131,W08-0402,0,0.0190682,"t k is typically much smaller than the number of possible expansions. Cube pruning employs a priority queue at beam filling and computes k highscoring expansions directly in near best-first order. The parameter k is known as beam size and it controls the time-accuracy trade-off of the algorithm. Heafield et al. (2013a) move away from using the language model as a black-box and build a more involved beam filling algorithm. Even though they target approximate search, some of their ideas have interesting connections to ours (see §4). They group hypotheses that share partial language model state (Li and Khudanpur, 2008) reasoning over multiple hypotheses at once. They fill a beam in best-first order by iteratively visiting groups using a priority queue: if the top group contains a single hypothesis, the hypothesis is added to the beam, otherwise the group is partitioned and the parts are pushed back to the queue. More recently, Heafield et al. (2014) applied their beam filling algorithm to phrase-based decoding. 3.2 Exact optimisation Exact optimisation for monotone translation has been done using A∗ search (Tillmann et al., 1997) and finite-state operations (Kumar et al., 2006). Och et al. (2001) design nea"
D14-1131,E09-1061,0,0.624957,"existing ones and no deduction happens twice. In Figure 1, a nonteminal item summarises partial derivation (or hypotheses). It is denoted by [C, r, γ] (also known as carry), where: C is a coverage vector, necessary to impose the non-overlapping constraint; r is the rightmost position most recently covered, necessary for the computation of δ; and γ is the last n − 1 words 1 Preventing phrases from overlapping requires an exponential number of constraints (the powerset of x) rendering the problem NP-complete (Knight, 1999). 2 Weighted logics have been extensively used to describe weighted sets (Lopez, 2009), operations over weighted sets (Chiang, 2007; Dyer and Resnik, 2010), and a variety of dynamic programming algorithms (Cohen et al., 2008).   I TEM {0, 1}I , [0, I + 1], ∆n−1 G OAL 1I , I + 1, EOS A XIOM hBOS → BOSi [0I , 0, BOS] : ψ(BOS) E XPAND   D i0 φr j 0 E j−1 − → yj xi − C, r, yj−n+1 Li0 ¯ h i k=i ck = 0 0 j C 0 , i0 , yj 0 −n+2 : w where c0k = ck if k &lt; i or k &gt; i0 else ¯ 1 0 j−1 w = φr ⊗ δ(r, i) ⊗ ψ(yjj |yj−n+1 ) ACCEPT  I  1 , r, γ r≤I [1I , I + 1, EOS] : δ(r, I + 1) ⊗ ψ(EOS|γ) Figure 1: Specification for the weighted set of translation derivations in phrase-based SMT with u"
D14-1131,W01-1408,0,0.200059,"e steps in a derivation. That is, Hk (d) = e∈d hk (e), where hk is a (local) feature function that assesses steps independently and d = he1 , e2 , . . . , el i is a sequence of l steps. Under this assumption, each step is assigned the weight w(e) = Λ·hh1 (e), h2 (e), . . . , hm (e)i. The set D is typically finite, however, it contains a very large number of structures — exponential (or even factorial, see §2) with the size of x — making exhaustive enumeration prohibitively slow. Only in very restricted cases combinatorial optimisation techniques are directly applicable (Tillmann et al., 1997; Och et al., 2001), thus it is common to resort to heuristic techniques in order to find an approximation to d∗ (Koehn et al., 2003; Chiang, 2007). Evaluation exercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Othe"
D14-1131,P03-1021,0,0.00758593,"sk of producing a translation for an input string x = hx1 , x2 , . . . , xI i is typically associated with finding the best derivation d∗ compatible with the input under a linear model. In this view, a derivation is a structured output that represents a sequence of steps that covers the input producing a translation. Equation 1 illustrates this decoding process. d∗ = argmax f (d) d∈D(x) (1) The set D(x) is the space of all derivations compatible with x and supported by a model of translational equivalences (Lopez, 2008). The function f (d) = Λ · H(d) is a linear parameterisation of the model (Och, 2003). It assigns a real-valued score (or weight) to every derivation d ∈ D(x), where Λ ∈ Rm assigns a relative importance to different aspects of the derivation independently captured by m feature functions H(d) = hH1 (d), . . . , Hm (d)i ∈ Rm . The fully parameterised model can be seen as a discrete weighted set such that feature functions factorise P over the steps in a derivation. That is, Hk (d) = e∈d hk (e), where hk is a (local) feature function that assesses steps independently and d = he1 , e2 , . . . , el i is a sequence of l steps. Under this assumption, each step is assigned the weight"
D14-1131,D08-1012,0,0.0205634,"s incrementally refined to be closer to the goal until the maximum is found (or until the sampling performance exceeds a certain level). Figure 2 illustrates exact optimisation with OS∗ . Suppose f is a complex target goal distribution, such that we cannot optimise f , but we can assess f (d) for a given d. Let g (0) be an upperbound to f , i.e., g (0) (d) ≥ f (d) for all d ∈ D(x). Moreover, suppose that g (0) is simple enough to be optimised efficiently. The algorithm proceeds by solving d0 = argmaxd g (0) (d) and comput6 The intuition that a full intersection is wasteful is also present in (Petrov et al., 2008) in the context of approximate search. They start from a coarse distribution based on automatic word clustering which is refined in multiple passes. At each pass, hypotheses are pruned a posteriori on the basis of their marginal probabilities, and word clusters are further split. We work with upperbounds, rather than word clusters, with unpruned distributions, and perform exact optimisation. g(0) f* f1 g(1) f f0 d0 d1 d* D(x) Figure 2: Sequence of incrementally refined upperbound proposals. ing the quantity r0 = f (d0 )/g(0) (d0 ). If r0 were sufficiently close to 1, then g (0) (d0 ) would be"
D14-1131,N09-2002,0,0.0230745,"er NP-complete. Rush and Collins (2011) relax a hierarchical phrase-based model (Chiang, 2005)5 w.r.t. the LM component. The translation forest and the language model trade their weights (through Lagrangian multipliers) so as to ensure agreement on what each component believes to be the maximum. In both approaches, when the dual converges to a compliant solution, the solution is guaranteed to be optimal. Other4 Exact decoding had been similarly addressed with Integer Linear Programming (ILP) in the context of word-based models for very short sentences using a 2-gram LM (Germann et al., 2001). Riedel and Clarke (2009) revisit that formulation and employ a cutting-plane algorithm (Dantzig et al., 1954) reaching 30 words. 5 In hierarchical translation, reordering is governed by a synchronous context-free grammar and the underlying problem is no longer NP-complete. Exact decoding remains infeasible because the intersection between the translation forest and the target LM is prohibitively slow. 1239 wise, a subset of the constraints is explicitly added and the dual optimisation is repeated. They handle sentences above average length, however, resorting to compact rulesets (10 translation options per input segm"
D14-1131,P11-1008,0,0.0834511,"ercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Other work has employed less common approximations to the model reducing its search space complexity (Kumar et al., 2006; Chang and Collins, 2011; Rush and Collins, 2011). These do not answer whether or not current decoding algorithms perform well at real translation tasks with state-of-the-art models. We propose an exact decoder for phrase-based SMT based on a coarse-to-fine search strategy (Dymetman et al., 2012). In a nutshell, we relax the decoding problem with respect to the Language Model (LM) component. This coarse view is incrementally refined based on evidence col1237 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1237–1249, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Li"
D14-1131,P97-1037,0,0.461893,"ons factorise P over the steps in a derivation. That is, Hk (d) = e∈d hk (e), where hk is a (local) feature function that assesses steps independently and d = he1 , e2 , . . . , el i is a sequence of l steps. Under this assumption, each step is assigned the weight w(e) = Λ·hh1 (e), h2 (e), . . . , hm (e)i. The set D is typically finite, however, it contains a very large number of structures — exponential (or even factorial, see §2) with the size of x — making exhaustive enumeration prohibitively slow. Only in very restricted cases combinatorial optimisation techniques are directly applicable (Tillmann et al., 1997; Och et al., 2001), thus it is common to resort to heuristic techniques in order to find an approximation to d∗ (Koehn et al., 2003; Chiang, 2007). Evaluation exercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz"
D14-1131,P09-1038,1,0.891528,"a single hypothesis, the hypothesis is added to the beam, otherwise the group is partitioned and the parts are pushed back to the queue. More recently, Heafield et al. (2014) applied their beam filling algorithm to phrase-based decoding. 3.2 Exact optimisation Exact optimisation for monotone translation has been done using A∗ search (Tillmann et al., 1997) and finite-state operations (Kumar et al., 2006). Och et al. (2001) design near-admissible heuristics for A∗ and decode very short sentences (614 words) for a word-based model (Brown et al., 1993) with a maximum distortion strategy (d = 3). Zaslavskiy et al. (2009) frame phrase-based decoding as an instance of a generalised Travelling Salesman Problem (TSP) and rely on robust solvers to perform decoding. In this view, a salesman graph encodes the translation options, with each node representing a biphrase. Nonoverlapping constraints are imposed by the TSP solver, rather than encoded directly in the salesman graph. They decode only short sentences (17 words on average) using a 2-gram LM due to salesman graphs growing too large.4 Chang and Collins (2011) relax phrase-based models w.r.t. the non-overlapping constraints, which are replaced by soft penalties"
D14-1131,N10-1128,0,\N,Missing
D14-1131,W13-2201,1,\N,Missing
D14-1190,W13-2241,1,0.502038,"borrow statistical strength from other tasks; • The annotation scheme is subjective and very fine-grained, and is therefore heavily prone to bias and noise, both which can be modelled easily using GPs; • Finally, we also have the goal to learn a model that shows sound and interpretable correlations between emotions. 2 Multi-task Gaussian Process Regression Gaussian Processes (GPs) (Rasmussen and Williams, 2006) are a Bayesian kernelised framework considered the state-of-the-art for regression. They have been recently used successfully for translation quality prediction (Cohn and Specia, 2013; Beck et al., 2013; Shah et al., 2013) 1798 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1798–1803, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics and modelling text periodicities (Preotiuc-Pietro and Cohn, 2013). In the following we give a brief description on how GPs are applied in a regression setting. Given an input x, the GP regression assumes that its output y is a noise corrupted version of a latent function evaluation, y = f (x) + η, where η ∼ N (0, σn2 ) is the added white noise and the function f is drawn from"
D14-1190,P13-1004,1,0.574242,"ions and anti-correlations between emotions on a news headlines dataset. The proposed model outperforms both singletask baselines and other multi-task approaches. 1 Introduction Multi-task learning (Caruana, 1997) has been widely used in Natural Language Processing. Most of these learning methods are aimed for Domain Adaptation (Daum´e III, 2007; Finkel and Manning, 2009), where we hypothesize that we can learn from multiple domains by assuming similarities between them. A more recent use of multi-task learning is to model annotator bias and noise for datasets labelled by multiple annotators (Cohn and Specia, 2013). The settings mentioned above have one aspect in common: they assume some degree of positive correlation between tasks. In Domain Adaptation, we assume that some “general”, domainindependent knowledge exists in the data. For annotator noise modelling, we assume that a “ground truth” exists and that annotations are some noisy deviations from this truth. However, for some settings these assumptions do not necessarily hold and often tasks can be anti-correlated. For these cases, we need to employ multi-task methods that are able to learn these relations from data and correctly employ them when m"
D14-1190,P07-1033,0,0.315185,"Missing"
D14-1190,2013.mtsummit-papers.21,1,0.726565,"strength from other tasks; • The annotation scheme is subjective and very fine-grained, and is therefore heavily prone to bias and noise, both which can be modelled easily using GPs; • Finally, we also have the goal to learn a model that shows sound and interpretable correlations between emotions. 2 Multi-task Gaussian Process Regression Gaussian Processes (GPs) (Rasmussen and Williams, 2006) are a Bayesian kernelised framework considered the state-of-the-art for regression. They have been recently used successfully for translation quality prediction (Cohn and Specia, 2013; Beck et al., 2013; Shah et al., 2013) 1798 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1798–1803, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics and modelling text periodicities (Preotiuc-Pietro and Cohn, 2013). In the following we give a brief description on how GPs are applied in a regression setting. Given an input x, the GP regression assumes that its output y is a noise corrupted version of a latent function evaluation, y = f (x) + η, where η ∼ N (0, σn2 ) is the added white noise and the function f is drawn from a GP prior: f (x) ∼"
D14-1190,D08-1027,0,0.113416,"Missing"
D14-1190,S07-1013,0,0.269124,"Missing"
D14-1190,N09-1068,0,0.0127108,"ting multiple emotions in natural language sentences. Our model is based on a low-rank coregionalisation approach, which combines a vector-valued Gaussian Process with a rich parameterisation scheme. We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset. The proposed model outperforms both singletask baselines and other multi-task approaches. 1 Introduction Multi-task learning (Caruana, 1997) has been widely used in Natural Language Processing. Most of these learning methods are aimed for Domain Adaptation (Daum´e III, 2007; Finkel and Manning, 2009), where we hypothesize that we can learn from multiple domains by assuming similarities between them. A more recent use of multi-task learning is to model annotator bias and noise for datasets labelled by multiple annotators (Cohn and Specia, 2013). The settings mentioned above have one aspect in common: they assume some degree of positive correlation between tasks. In Domain Adaptation, we assume that some “general”, domainindependent knowledge exists in the data. For annotator noise modelling, we assume that a “ground truth” exists and that annotations are some noisy deviations from this tru"
D14-1190,D12-1054,0,0.136491,"For annotator noise modelling, we assume that a “ground truth” exists and that annotations are some noisy deviations from this truth. However, for some settings these assumptions do not necessarily hold and often tasks can be anti-correlated. For these cases, we need to employ multi-task methods that are able to learn these relations from data and correctly employ them when making predictions, avoiding negative knowledge transfer. An example of a problem that shows this behaviour is Emotion Analysis, where the goal is to automatically detect emotions in a text (Strapparava and Mihalcea, 2008; Mihalcea and Strapparava, 2012). This problem is closely related to Opinion Mining (Pang and Lee, 2008), with similar applications, but it is usually done at a more fine-grained level and involves the prediction of a set of labels (one for each emotion) instead of a single label. While we expect some emotions to have some degree of correlation, this is usually not the case for all possible emotions. For instance, we expect sadness and joy to be anti-correlated. We propose a multi-task setting for Emotion Analysis based on a vector-valued Gaussian Process (GP) approach known as coregionalisation ´ (Alvarez et al., 2012). The"
D14-1190,D13-1100,1,\N,Missing
D15-1125,W12-2703,0,0.0226373,"ically or grammatically related words are mapped to similar geometric locations in a high-dimensional continuous space. The probability distribution is thus much smoother and therefore the model has a better generalisation power on unseen events. The representations are learned in a continuous space to estimate the probabilities using neural networks with single (called shallow networks) or multiple (called deep networks) hidden layers. Deep neural networks have been shown to perform better than shallow ones due to their capability to learn higher-level, abstract representations of the input (Arisoy et al., 2012). In this paper, we explore the potential of these models in context of QE for MT. We obtain more robust features with CSLM and improve the overall prediction power for translation quality. The paper is organised as follows: In Section 2 we briefly present the related work. Section 3 describes the CSLM model training and its various settings. In Section 4 we propose the use of CSLM features for QE. In Section 5 we present our experiments along with their results. 2 Related Work For a detailed overview of various features and algorithms for QE, we refer the reader to the 1073 Proceedings of the"
D15-1125,W05-0909,0,0.0716489,"slator, with the post-editing time collected on a sentence-basis and used as label (in milliseconds). WMT15 (Task-1): Large English-Spanish news dataset containing source sentences, their machine translations by an online SMT system, and the post-editions of the translation by crowdsourced translators, with HTER used as label. IWSLT14: English-French dataset containing source language data from the 10-best (sentences) ASR system output. On the target side, the 1best MT translation is used. The ASR system leads to different source segments, which in turn lead to different translations. METEOR (Banerjee and Lavie, 2005) is used to label these alternative translations against a reference (human) translation. Both ASR and MT outputs come from a system submission in IWSLT 2014 (Ng et al., 2014). The ASR system is a multi-pass deep neural network tandem system with feature and model adaptation and rescoring. The MT system is a phrasebased SMT system produced using Moses. Dataset WMT12 WMT13 WMT14task1.1 WMT14task1.3 WMT15 IWSLT14 WMT12: English-Spanish news sentence translations produced by a Moses “baseline” statistical MT (SMT) system, and judged for perceived post-editing effort in 1–5 (highest-lowest), takin"
D15-1125,W14-3356,0,0.015393,"t the related work. Section 3 describes the CSLM model training and its various settings. In Section 4 we propose the use of CSLM features for QE. In Section 5 we present our experiments along with their results. 2 Related Work For a detailed overview of various features and algorithms for QE, we refer the reader to the 1073 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1073–1078, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. WMT12-14 shared tasks on QE (Callison-Burch et al., 2012; Bojar et al., 2013; Ling et al., 2014). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labour-intensive. Since their introduction in (Bengio et al., 2003), neural network language models have been successfully exploited in many speech and language processing problems, including automatic speech recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and machine tran"
D15-1125,H05-1026,0,0.26216,"Burch et al., 2012; Bojar et al., 2013; Ling et al., 2014). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labour-intensive. Since their introduction in (Bengio et al., 2003), neural network language models have been successfully exploited in many speech and language processing problems, including automatic speech recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and machine translation (Schwenk, 2012). Recently, (Banchs et al., 2015) used a Latent Semantic Indexing approach to model sentences as bag-of-words in a continuous space to measure cross language adequacy. (Tan et al., 2015) proposed to train models with deep regression for machine translation evaluation in a task to measure semantic similarity between sentences. They reported positive results on simple features; larger feature sets did not improve these results. In this paper, we propose to estimate the probabilities of source and target segments with continuous space langua"
D15-1125,C12-2104,0,0.195489,"ring the context of the text. QE performance usually differs depending on the language pair, the specific quality score being optimised (e.g., post-editing time vs translation adequacy) and the feature set. Features based on n-gram language models, despite their simplicity, are among those with the best performance in most QE tasks (Shah et al., 2013b). However, they may not generalise well due to the underlying discrete nature of words in n-gram modelling. Continuous Space Language Models (CSLM), on the other hand, have shown their potential to capture long distance dependencies among words (Schwenk, 2012; Mikolov et al., 2013). The assumption of these models is that semantically or grammatically related words are mapped to similar geometric locations in a high-dimensional continuous space. The probability distribution is thus much smoother and therefore the model has a better generalisation power on unseen events. The representations are learned in a continuous space to estimate the probabilities using neural networks with single (called shallow networks) or multiple (called deep networks) hidden layers. Deep neural networks have been shown to perform better than shallow ones due to their cap"
D15-1125,2013.mtsummit-papers.21,1,0.948535,"tically motivated features. They include features that summarise how the MT systems generate translations, as well as features that are oblivious to the systems. The majority of the features in the literature are extracted from each sentence pair in isolation, ignoring the context of the text. QE performance usually differs depending on the language pair, the specific quality score being optimised (e.g., post-editing time vs translation adequacy) and the feature set. Features based on n-gram language models, despite their simplicity, are among those with the best performance in most QE tasks (Shah et al., 2013b). However, they may not generalise well due to the underlying discrete nature of words in n-gram modelling. Continuous Space Language Models (CSLM), on the other hand, have shown their potential to capture long distance dependencies among words (Schwenk, 2012; Mikolov et al., 2013). The assumption of these models is that semantically or grammatically related words are mapped to similar geometric locations in a high-dimensional continuous space. The probability distribution is thus much smoother and therefore the model has a better generalisation power on unseen events. The representations ar"
D15-1125,P13-4014,1,0.896939,"Missing"
D15-1125,S15-2015,1,0.876008,"Missing"
D15-1125,N13-1090,0,0.00800323,"t of the text. QE performance usually differs depending on the language pair, the specific quality score being optimised (e.g., post-editing time vs translation adequacy) and the feature set. Features based on n-gram language models, despite their simplicity, are among those with the best performance in most QE tasks (Shah et al., 2013b). However, they may not generalise well due to the underlying discrete nature of words in n-gram modelling. Continuous Space Language Models (CSLM), on the other hand, have shown their potential to capture long distance dependencies among words (Schwenk, 2012; Mikolov et al., 2013). The assumption of these models is that semantically or grammatically related words are mapped to similar geometric locations in a high-dimensional continuous space. The probability distribution is thus much smoother and therefore the model has a better generalisation power on unseen events. The representations are learned in a continuous space to estimate the probabilities using neural networks with single (called shallow networks) or multiple (called deep networks) hidden layers. Deep neural networks have been shown to perform better than shallow ones due to their capability to learn higher"
D15-1125,P10-2041,0,0.0609381,"Missing"
D15-1125,W12-3102,1,\N,Missing
D15-1125,W13-2201,1,\N,Missing
D19-1318,E17-1050,0,0.419709,"Missing"
D19-1318,W18-6452,0,0.0158273,"as: Pr = Prgen Prd (wd ) + (1 − Prgen )at (3) where, Prd (wd ) is decoder’s probability of generating output word from the decoder vocabulary. We empirically observed that getting the new attention representation over the final layer is extremely important for obtaining a good estimate of the attention distribution. 3.3 Dual-source extension We extend copycat networks for the case of APE where a dual-source encoder-decoder architecture has been shown to obtain superior results (Junczys-Dowmunt and Grundkiewicz, 2018). As previously mentioned, most of the translations in current APE datasets (Chatterjee et al., 2018) already have optimal quality, thereby models that alter these already accurate translations are sub-optimal. Based on this observation we propose a dual-source extension to our copycat model. We add an extra encoder (Es ) that encodes the sentence in source language and 3229 (a) copycat network (b) Dual-source copycat (c) Dual-source double-attention copycat Figure 1: copycat architectures: (a) shows our copycat model for summarisation, where at is computed on the final decoder layer and the last encoder layer; (b) is the dual-source extension for APE with an additional encoder; and (c) furth"
D19-1318,P18-1063,0,0.0339834,"Missing"
D19-1318,P11-2031,0,0.0835346,"Missing"
D19-1318,D18-1443,0,0.0564202,"Pointer networks (Vinyals et al., 2015) are an extension of attentive recurrent neural network (RNN) architectures to use attention as a pointer to select which tokens of the input sequence should be copied to the output, as a switching mechanism between copying and generating new words. In this paper we propose copycat, a flexible pointer network framework for text-to-text generation tasks. It differs from previous work in the following main ways: it is based on transformer networks (Vaswani et al., 2017), which have been shown to achieve superior performance in text generation tasks. While Gehrmann et al. (2018b) also provide contrastive experiments using transformers, they however randomly choose one of the attention heads as the copy-distribution, while we let the networks learn through all layers and use the attention heads from the final layer of the decoder to learn the distribution; thereby the decision between copying input words and generating new words is made at the highest level of abstraction. Further, in our model, we do not use coverage penalty and achieve similar performance as Gehrmann et al. (2018b). For text summarisation, we show that our copycat model performs competitively with"
D19-1318,P16-1154,0,0.178472,"erform very well in monolingual variants of these tasks, such as text summarisation and text simplification (Nallapati et al., 2016a; Scarton and Specia, 2018). One limitation of such models is that they tend to be “over creative” when generating outputs, i.e. create a completely new output, which is not what they should do. Different strategies have been used to mitigate this issue, primarily in the context of abstractive text summarisation. The most prominent strategy is the use of pointer networks, which substantially outperform vanilla sequence to sequence models (Nallapati et al., 2016b; Gu et al., 2016; See et al., 2017). Pointer networks (Vinyals et al., 2015) are an extension of attentive recurrent neural network (RNN) architectures to use attention as a pointer to select which tokens of the input sequence should be copied to the output, as a switching mechanism between copying and generating new words. In this paper we propose copycat, a flexible pointer network framework for text-to-text generation tasks. It differs from previous work in the following main ways: it is based on transformer networks (Vaswani et al., 2017), which have been shown to achieve superior performance in text gene"
D19-1318,P18-1013,0,0.0284261,"Missing"
D19-1318,W18-6467,0,0.0703596,"ween generating a word from the decoder vocabulary or copying the word directly from the source sequence by using at as: Pr = Prgen Prd (wd ) + (1 − Prgen )at (3) where, Prd (wd ) is decoder’s probability of generating output word from the decoder vocabulary. We empirically observed that getting the new attention representation over the final layer is extremely important for obtaining a good estimate of the attention distribution. 3.3 Dual-source extension We extend copycat networks for the case of APE where a dual-source encoder-decoder architecture has been shown to obtain superior results (Junczys-Dowmunt and Grundkiewicz, 2018). As previously mentioned, most of the translations in current APE datasets (Chatterjee et al., 2018) already have optimal quality, thereby models that alter these already accurate translations are sub-optimal. Based on this observation we propose a dual-source extension to our copycat model. We add an extra encoder (Es ) that encodes the sentence in source language and 3229 (a) copycat network (b) Dual-source copycat (c) Dual-source double-attention copycat Figure 1: copycat architectures: (a) shows our copycat model for summarisation, where at is computed on the final decoder layer and the l"
D19-1318,P07-2045,0,0.01006,"Missing"
D19-1318,W04-1013,0,0.0291686,"he validation ROUGE-L or BLEU score, for summarisation and APE respectively. For summarisation, we use a randomly chosen validation subset of 2k abstracts. During decoding, we use a beam search of size 5 and tune the α parameter (length penalty) on the validation subset. During decoding for APE, we use a beam search of size 10. 5 Results In this section we present results of our experiments, first for summarisation (Section 5.1) and then for APE experiments (Section 5.2). 5.1 Summarisation Table 1 shows results of our summarisation experiments. We report F1-scores for a standard set of ROUGE (Lin, 2004) scores: ROUGE-1, ROUGE-2 and ROUGE-L. They measure unigram, bigram recall and the longest in-sequence common to the generated and the reference abstracts, respectively. The scores are measured with the pyrouge toolkit.4 We also report results for a range of conceptually different systems, including the LEAD-3 baseline (the top 3 sentences of the source). copycat performs on par with the CopyTransformer. Our generated sentences for both systems are of 62 tokens on average, with references of 58 tokens on average. To understand the model, we look at two aspects (i) the level of repetition withi"
D19-1318,K16-1028,0,0.0693346,"-of-the-art automated post-editing systems. More importantly, we show that it addresses a well-known limitation of automatic post-editing – overcorrecting translations – and that our novel mechanism for copying source language words improves the results. 1 Introduction Text-to-text generation is generally addressed using sequence to sequence neural models (Sutskever et al., 2014). While initially proposed for machine translation (Bahdanau et al., 2015), these models have been shown to perform very well in monolingual variants of these tasks, such as text summarisation and text simplification (Nallapati et al., 2016a; Scarton and Specia, 2018). One limitation of such models is that they tend to be “over creative” when generating outputs, i.e. create a completely new output, which is not what they should do. Different strategies have been used to mitigate this issue, primarily in the context of abstractive text summarisation. The most prominent strategy is the use of pointer networks, which substantially outperform vanilla sequence to sequence models (Nallapati et al., 2016b; Gu et al., 2016; See et al., 2017). Pointer networks (Vinyals et al., 2015) are an extension of attentive recurrent neural network"
D19-1318,L18-1004,0,0.043817,"Missing"
D19-1318,P02-1040,0,0.105334,"Missing"
D19-1318,N18-2102,0,0.0310881,"Missing"
D19-1318,D15-1044,0,0.111163,"Missing"
D19-1318,P18-2113,1,0.846574,"t-editing systems. More importantly, we show that it addresses a well-known limitation of automatic post-editing – overcorrecting translations – and that our novel mechanism for copying source language words improves the results. 1 Introduction Text-to-text generation is generally addressed using sequence to sequence neural models (Sutskever et al., 2014). While initially proposed for machine translation (Bahdanau et al., 2015), these models have been shown to perform very well in monolingual variants of these tasks, such as text summarisation and text simplification (Nallapati et al., 2016a; Scarton and Specia, 2018). One limitation of such models is that they tend to be “over creative” when generating outputs, i.e. create a completely new output, which is not what they should do. Different strategies have been used to mitigate this issue, primarily in the context of abstractive text summarisation. The most prominent strategy is the use of pointer networks, which substantially outperform vanilla sequence to sequence models (Nallapati et al., 2016b; Gu et al., 2016; See et al., 2017). Pointer networks (Vinyals et al., 2015) are an extension of attentive recurrent neural network (RNN) architectures to use a"
D19-1318,P17-1099,0,0.42764,"in monolingual variants of these tasks, such as text summarisation and text simplification (Nallapati et al., 2016a; Scarton and Specia, 2018). One limitation of such models is that they tend to be “over creative” when generating outputs, i.e. create a completely new output, which is not what they should do. Different strategies have been used to mitigate this issue, primarily in the context of abstractive text summarisation. The most prominent strategy is the use of pointer networks, which substantially outperform vanilla sequence to sequence models (Nallapati et al., 2016b; Gu et al., 2016; See et al., 2017). Pointer networks (Vinyals et al., 2015) are an extension of attentive recurrent neural network (RNN) architectures to use attention as a pointer to select which tokens of the input sequence should be copied to the output, as a switching mechanism between copying and generating new words. In this paper we propose copycat, a flexible pointer network framework for text-to-text generation tasks. It differs from previous work in the following main ways: it is based on transformer networks (Vaswani et al., 2017), which have been shown to achieve superior performance in text generation tasks. While"
D19-1318,P16-1162,0,0.787937,"is similar to the standard transformer architecture and comprises of 6 layers. It contains an additional sub-layer that performs multi-head attention over the outputs of the encoder block. Specifically, a decoding layer dli is the result of multi-head attention over the outputs of the encoder which in turn is a function of the encoder memory and the outputs from the previous layer: AD→E = f (ME , dli−1 ) where, the keys and values are the encoder outputs and the queries correspond to the decoder input. In all cases we tie the source and target vocabulary. We also use Byte Pair Encoding (BPE) (Sennrich et al., 2016) word segmentation approach to pre-process the data before we fix the vocabulary. This also helps us in reducing the OOV rate and makes the models less dependent on the vocabulary of the training data. We train all our models with the cross entropy loss. 3.2 part from their formulation. We first obtain an attention distribution over the source and decoder at each time step. We use a simple dot product at over output of the last decoder layer (Afin D→E ) and the contextualised encoder representations for each of the source language tokens (ME ): Pointer-generator Network We base our pointer-gen"
D19-1318,2013.mtsummit-papers.24,0,0.807772,"Missing"
D19-1318,W09-0441,0,0.45109,"Missing"
D19-1318,W18-6451,1,0.827902,"Missing"
D19-1318,W18-6471,0,0.0289865,"Missing"
D19-3009,N18-1063,0,0.14836,"Missing"
D19-3009,P17-1104,0,0.0138124,"pope (n) × rope (n) pope (n) + rope (n) 1 X = fope (n) k fope (n) = Fope Word-level Analysis and QE Features n=[1,..,k] Although Xu et al. (2016) indicate that only precision should be considered for the deletion operation, we follow the Java implementation that uses F1 score for all operations in corpus-level SARI. SAMSA measures structural simplicity (i.e. sentence splitting). This is in contrast to SARI, which is designed to evaluate simplifications involving paraphrasing. EASSE re-factors the original SAMSA implementation2 with some modifications: (1) an internal call to the TUPA parser (Hershcovich et al., 2017), which generates the semantic annotations for each original sentence; (2) a modified version of the monolingual word aligner (Sultan et al., 2014) that is compatible with Python 3, and uses Stanford CoreNLP (Manning et al., 2014)3 through their official Python interface; and (3) a single function call to get a SAMSA score instead of running a series of scripts. Quality Estimation Features Traditional automatic metrics used for SS rely on the existence and quality of references, and are often not enough to analyse the complex process of simplification. QE 1 https://github.com/mjpost/sacreBLEU"
D19-3009,Q14-1018,0,0.0285492,"ate that only precision should be considered for the deletion operation, we follow the Java implementation that uses F1 score for all operations in corpus-level SARI. SAMSA measures structural simplicity (i.e. sentence splitting). This is in contrast to SARI, which is designed to evaluate simplifications involving paraphrasing. EASSE re-factors the original SAMSA implementation2 with some modifications: (1) an internal call to the TUPA parser (Hershcovich et al., 2017), which generates the semantic annotations for each original sentence; (2) a modified version of the monolingual word aligner (Sultan et al., 2014) that is compatible with Python 3, and uses Stanford CoreNLP (Manning et al., 2014)3 through their official Python interface; and (3) a single function call to get a SAMSA score instead of running a series of scripts. Quality Estimation Features Traditional automatic metrics used for SS rely on the existence and quality of references, and are often not enough to analyse the complex process of simplification. QE 1 https://github.com/mjpost/sacreBLEU https://github.com/eliorsulem/SAMSA 3 https://stanfordnlp.github.io/ stanfordnlp/corenlp_client.html 2 4 https://github.com/mmautner/ readability 5"
D19-3009,P14-5010,0,0.00388726,"the Java implementation that uses F1 score for all operations in corpus-level SARI. SAMSA measures structural simplicity (i.e. sentence splitting). This is in contrast to SARI, which is designed to evaluate simplifications involving paraphrasing. EASSE re-factors the original SAMSA implementation2 with some modifications: (1) an internal call to the TUPA parser (Hershcovich et al., 2017), which generates the semantic annotations for each original sentence; (2) a modified version of the monolingual word aligner (Sultan et al., 2014) that is compatible with Python 3, and uses Stanford CoreNLP (Manning et al., 2014)3 through their official Python interface; and (3) a single function call to get a SAMSA score instead of running a series of scripts. Quality Estimation Features Traditional automatic metrics used for SS rely on the existence and quality of references, and are often not enough to analyse the complex process of simplification. QE 1 https://github.com/mjpost/sacreBLEU https://github.com/eliorsulem/SAMSA 3 https://stanfordnlp.github.io/ stanfordnlp/corenlp_client.html 2 4 https://github.com/mmautner/ readability 50 Figure 1: Example of automatic transformation annotations based on word alignment"
D19-3009,P12-1107,0,0.343006,"Missing"
D19-3009,W18-7005,1,0.908611,"Missing"
D19-3009,Q16-1029,0,0.497176,"rious metrics and features above and on how a particular SS output fares against reference simplifications. Through experiments, we show that these functionalities allow for better comparison and understanding of the performance of SS systems. 1 2 Introduction Sentence Simplification (SS) consists of modifying the content and structure of a sentence to improve its readability while retaining its original meaning. For automatic evaluation of a simplification output, it is common practice to use machine translation (MT) metrics (e.g. BLEU (Papineni et al., 2002)), simplicity metrics (e.g. SARI (Xu et al., 2016)), and readability metrics (e.g. FKGL (Kincaid et al., 1975)). Most of these metrics are available in individual code repositories, with particular software requirements that sometimes differ even in programming language (e.g. corpus-level SARI is implemented in Java, whilst sentence-level SARI is available in both Java and Python). Other metrics (e.g. SAMSA (Sulem et al., 2018b)) suffer from insufficient documentation or require executing multiple scripts with hard-coded paths, which prevents researchers from using them. 2.1 Package Overview Automatic Corpus-level Metrics Although human judge"
D19-3009,P14-1041,0,0.115191,"n TurkCorpus according to SARI. However, it gets the lowest SAMSA score, and the third to last BLEU score. PBSMT-R is the best in terms of these two metrics. Finally, across all metrics, the Reference stills gets the highest values, with significant differences from the top performing systems. Sentence Simplification Systems EASSE provides access to various SS system outputs that follow different approaches for the task. For instance, we include those that rely on phrasebased statistical MT, either by itself (e.g. PBSMTR (Wubben et al., 2012)), or coupled with semantic analysis, (e.g. Hybrid (Narayan and Gardent, 2014)). We also include SBSMT-SARI (Xu et al., 2016), which relies on syntax-based statistical MT; D RESS -L S (Zhang and Lapata, 2017), a neural model using the standard encoder-decoder architecture with attention combined with reinforcement learning; and DMASS-DCSS (Zhao et al., 2018), the current state-of-the-art in the TurkCorpus, which is based on the Transformer architecture (Vaswani et al., 2017). Word-level Transformations In order to better understand the previous results, we use the wordlevel annotations of text transformations (Table 3). Since SARI was design to evaluate mainly paraphras"
D19-3009,D17-1062,0,0.351654,"rate, those that display lexical simplifications, among others. Each of these aspects is illustrated with 10 instances. An example of the report can be viewed at https: //github.com/feralvam/easse/blob/ master/demo/report.gif. 3.2 3 Table 2: Comparison of systems’ performance based on automatic metrics. Automatic Metrics For illustration purposes, we compare systems’ outputs using BLEU and SARI in TurkCorpus (with 8 manual simplification references), and SAMSA in HSplit. For calculating Reference values in Table 2, we sample one of the 8 human references for each instance as others have done (Zhang and Lapata, 2017). When reporting SAMSA scores, we only use the first 70 sentences of TurkCorpus that also appear in HSplit.7 This allows us to compute Reference scores for instances that contain structural simplifications (i.e. sentence splits). We calculate SAMSA scores for each of the four manual simplifications in HSplit, and choose the highest as an upper-bound Reference value. The results for all three metrics are shown in Table 2. TurkCorpus Experiments We collected publicly available outputs of several SS systems (Sec. 3.1) to evaluate their performance using the functionalities available in EASSE. In"
D19-3009,D18-1355,0,0.106641,"ems. Sentence Simplification Systems EASSE provides access to various SS system outputs that follow different approaches for the task. For instance, we include those that rely on phrasebased statistical MT, either by itself (e.g. PBSMTR (Wubben et al., 2012)), or coupled with semantic analysis, (e.g. Hybrid (Narayan and Gardent, 2014)). We also include SBSMT-SARI (Xu et al., 2016), which relies on syntax-based statistical MT; D RESS -L S (Zhang and Lapata, 2017), a neural model using the standard encoder-decoder architecture with attention combined with reinforcement learning; and DMASS-DCSS (Zhao et al., 2018), the current state-of-the-art in the TurkCorpus, which is based on the Transformer architecture (Vaswani et al., 2017). Word-level Transformations In order to better understand the previous results, we use the wordlevel annotations of text transformations (Table 3). Since SARI was design to evaluate mainly paraphrasing transformations, the fact that SBSMTSARI is the best at performing replacements and second place in copying explains its high SARI score. DMASS-DCSS is second best in replacements, while PBSMT-R (which achieved the highest BLEU score) is the best at copying. Hybrid is the best"
D19-3009,I17-3001,1,0.857394,"for each operation (ope ∈ {add, del, keep}) and n-gram order, precision pope (n), recall rope (n) and F1 fope (n) scores are calculated. These are then averaged over the n-gram order to get the overall operation F1 score Fope : 2.2 Word-level Transformation Analysis EASSE includes algorithms to determine which specific text transformations a SS system performs more effectively. This is done based on word-level alignment and analysis. Since there is no available simplification dataset with manual annotations of the transformations performed, we re-use the annotation algorithms from MASSAlign (Paetzold et al., 2017). Given a pair of sentences (e.g. original and system output), the algorithms use word alignments to identify deletions, movements, replacements and copies (see Fig. 1). This process is prone to some errors: when compared to manual labels produced by four annotators in 100 original-simplified pairs, the automatic algorithms achieved a micro-averaged F1 score of 0.61 (Alva-Manchego et al., 2017). We generate two sets of automatic word-level annotations: (1) between the original sentences and their reference simplifications, and (2) between the original sentences and their automatic simplificati"
D19-3009,P02-1040,0,0.111759,"lly, EASSE generates easy-to-visualise reports on the various metrics and features above and on how a particular SS output fares against reference simplifications. Through experiments, we show that these functionalities allow for better comparison and understanding of the performance of SS systems. 1 2 Introduction Sentence Simplification (SS) consists of modifying the content and structure of a sentence to improve its readability while retaining its original meaning. For automatic evaluation of a simplification output, it is common practice to use machine translation (MT) metrics (e.g. BLEU (Papineni et al., 2002)), simplicity metrics (e.g. SARI (Xu et al., 2016)), and readability metrics (e.g. FKGL (Kincaid et al., 1975)). Most of these metrics are available in individual code repositories, with particular software requirements that sometimes differ even in programming language (e.g. corpus-level SARI is implemented in Java, whilst sentence-level SARI is available in both Java and Python). Other metrics (e.g. SAMSA (Sulem et al., 2018b)) suffer from insufficient documentation or require executing multiple scripts with hard-coded paths, which prevents researchers from using them. 2.1 Package Overview A"
D19-3009,C10-1152,0,0.234814,"crowdworkers to simplify 2,359 original sentences extracted from PWKP to collect multiple simplification references for each one. This dataset was then randomly split into tuning (2,000 instances) and test (359 instances) sets. The test set only contains 1-to-1 alignments, mostly with instances of paraphrasing and deletion. Each original sentence in TurkCorpus has 8 simplified references. As such, it is better suited for computing SARI and multireference BLEU scores. Access to Test Datasets EASSE provides access to three publicly available datasets for automatic SS evaluation (Table 1): PWKP (Zhu et al., 2010), TurkCorpus (Xu et al., 2016), and HSplit (Sulem et al., 2018a). All of them consist of the data from the original datasets, which are sentences extracted from English Wikipedia (EW) articles. EASSE can also evaluate system’s outputs in other custom datasets provided by the user. HSplit Sulem et al. (2018a) recognised that existing EW-based datasets did not contain sufficient instances of sentence splitting. As such, they collected four reference simplifications of this transformation for all 359 original sentences in the TurkCorpus test set. Even though SAMSA’s computation does not require a"
D19-3009,W18-6319,0,0.0340117,"Missing"
D19-3009,D18-1081,0,0.445546,"Missing"
D19-5543,W19-5361,0,0.145414,"Zhenhao Li Department of Computing Imperial College London, UK zhenhao.li18@imperial.ac.uk with different kinds of noise, e.g., typos, grammatical errors, emojis, spoken languages, etc. for two language pairs. In the WMT19 Robustness Task2 (Li et al., 2019), improving NMT robustness is treated as a domain adaption problem. The MTNT dataset is used as in-domain data, where models are trained with clean data and adapted to noisy data. Domain adaption is conducted in two main methods: fine tuning on in-domain data (Dabre and Sumita, 2019; Post and Duh, 2019) and mixed training with domain tags (Berard et al., 2019; Zheng et al., 2019). The size of the noisy data provided by the shared task is small, with only thousands of noisy sentence pairs on each direction. Hence most approaches participating in the task performed noisy data augmentation using back translation (Berard et al., 2019; Helcl et al., 2019; Zheng et al., 2019), with some approaches also directly adding synthetic noise (Berard et al., 2019). The robustness of an NMT model can be seen as denoising source sentences (e.g. dealing with typos, etc.) while keeping a similar level of informal language in the translations (e.g. keeping emojis/emo"
D19-5543,P19-1175,0,0.083342,"obustness but focus on techniques other than back translation. We follow the WMT19 Robustness Task and conduct experiments under constrained and unconstrained data settings on Fr↔En as language pairs. Under the constrained setting, we only use datasets provided by the shared task, and propose new data augmentation methods to generate noise from this data. We compare back translation (BT) (Sennrich et al., 2016a) with forward translation (FT) on noisy texts and find that pseudo-parallel data from forward translation can help improve more robustness. We also adapt the idea of fuzzy matches from Bulte and Tezcan (2019) to the MTNT case by finding similar sentences in a parallel corpus to augment the limited noisy data. Results show that the fuzzy match method can extend noisy parallel data and improve model performance on both noisy and clean texts. The proposed techniques substantially outperform the baseline. While they still lag behind the winning submission in the WMT19 shared task, the resulting models are trained on much smaller clean data but augmented noisy data, leading to faster and more efficient training. Under the unconstrained setting, we propose for the first time the use of speech datasets,"
D19-5543,D19-5506,0,0.059785,"Missing"
D19-5543,P17-4012,0,0.0293298,"/speech-totext/ 330 Corpus Gigaword UN Corpus Common Crawl Europarl News Commentary IWSLT MuST-C(en-fr) MTNT(en-fr) MTNT(fr-en) MTNT(en) MTNT(fr) Sentences 22.52M 12.89M 3.24M 2.01M 200k 236k 275k 36k 19k 81k 26k Words source target 575.67M 672.07M 316.22M 353.92M 70.73M 76.69M 50.26M 52.35M 4.46M 5.19M 4.16M 4.34M 5.09M 5.30M 841k 965k 661k 634k 3.41M 1.27M 3.3 Model Our baseline model, which only uses clean data for training, is the standard Transformer model (Vaswani et al., 2017) with default hyperparameters. The batch size is 4096 tokens (subwords). Our models are trained with OpenNMTpy (Klein et al., 2017) on a single GTX 1080 Ti for 5 epochs. We experimented with fine tuning on noisy data and mixed training with “domain” tags (Caswell et al., 2019; Kobus et al., 2016) indicating where the sentences are sourced from. We used different tags for clean data, MTNT parallel data, forward translation, back translation, ASR data, and fuzzy match data. Tags are added at the beginning of each source sentences. During the fine tuning on in-domain data, we continued the learning rate and model parameters. We stop the fine tuning when the perplexity on noisy validation set does not improve after 3 epochs."
D19-5543,2012.eamt-1.60,0,0.0160209,"similar sentences in a parallel corpus to augment the limited noisy data. Results show that the fuzzy match method can extend noisy parallel data and improve model performance on both noisy and clean texts. The proposed techniques substantially outperform the baseline. While they still lag behind the winning submission in the WMT19 shared task, the resulting models are trained on much smaller clean data but augmented noisy data, leading to faster and more efficient training. Under the unconstrained setting, we propose for the first time the use of speech datasets, in two forms: (a) the IWSLT (Cettolo et al., 2012) and MuSTC (Di Gangi et al., 2019) human transcripts as a source of spontaneous, clean speech data, and (b) automatically generated transcripts for the MuSTC dataset as another source of noise. We show that using informal language from spoken language datasets can also help to increase NMT robustness. This paper is structured as follows: Section 2 introduces the data augmentation methods for noisy texts, including the previously proposed methods and our approaches to data augmentation. Section 3 describes our experimental settings, including the datasets we used, the augmented data and the bas"
D19-5543,P19-1425,0,0.061464,"Missing"
D19-5543,P07-2045,0,0.00837981,"n transcripts S and transcript translated into another language T are provided. We used Google Speech-to-Text API4 and transcribed the audio files into automatic transcripts S ′ . The human and ASR transcripts of the audio (S and S ′ ) are treated as the source texts while the translations T are target texts. We formed a new set of parallel data (S ′ , T ) with ASR generated texts and the corresponding gold translations by humans. Looking into the ASR transcripts, we found that the ASR system tends to skip some sentences due For prepossessing, we first tokenized the data with Moses tokenizer (Koehn et al., 2007) and applied Byte Pair Encoding (BPE) (Sennrich et al., 2016b) to segment words into subwords. We experimented with a large vocabulary to include noisy as well as clean tokens and applied 50k merge operations for BPE. Upon evaluation, we detokenized our hypothesis files with the Moses detokenizer. We used multi-bleu-detok.perl to evaluate the BLEU score on the test sets. 5 https://wit3.fbk.eu/mt.php? release=2017-01-trnted 6 https://ict.fbk.eu/must-c/ 7 The data from IWSLT has the same sentences in both translation directions, so we reversed the En→Fr data on the Fr→En direction. The MuST-C da"
D19-5543,P18-1163,0,0.0274455,"elcl et al., 2019). Zheng et al. (2019) proposed an extension of back translation that generates noisy translations from clean monolingual data. Therefore, after reversing the direction, the noisy translations become the source, which would simulate the noisy source sentences from the MTNT parallel data. Synthetic noise is injected into clean data to form noisy parallel data in Belinkov and Bisk (2017); Karpukhin et al. (2019). However, rule-based synthetic noise injection is limited to certain types of noise. Adversarial methods are proposed to inject random noise into clean training data in Cheng et al. (2018, 2019). We explore the following new methods as alternative ways to augment noisy parallel data. tion for robustness but focus on techniques other than back translation. We follow the WMT19 Robustness Task and conduct experiments under constrained and unconstrained data settings on Fr↔En as language pairs. Under the constrained setting, we only use datasets provided by the shared task, and propose new data augmentation methods to generate noise from this data. We compare back translation (BT) (Sennrich et al., 2016a) with forward translation (FT) on noisy texts and find that pseudo-parallel d"
D19-5543,W19-5362,0,0.026537,"ral Machine Translation Robustness via Data Augmentation: Beyond Back Translation Zhenhao Li Department of Computing Imperial College London, UK zhenhao.li18@imperial.ac.uk with different kinds of noise, e.g., typos, grammatical errors, emojis, spoken languages, etc. for two language pairs. In the WMT19 Robustness Task2 (Li et al., 2019), improving NMT robustness is treated as a domain adaption problem. The MTNT dataset is used as in-domain data, where models are trained with clean data and adapted to noisy data. Domain adaption is conducted in two main methods: fine tuning on in-domain data (Dabre and Sumita, 2019; Post and Duh, 2019) and mixed training with domain tags (Berard et al., 2019; Zheng et al., 2019). The size of the noisy data provided by the shared task is small, with only thousands of noisy sentence pairs on each direction. Hence most approaches participating in the task performed noisy data augmentation using back translation (Berard et al., 2019; Helcl et al., 2019; Zheng et al., 2019), with some approaches also directly adding synthetic noise (Berard et al., 2019). The robustness of an NMT model can be seen as denoising source sentences (e.g. dealing with typos, etc.) while keeping a s"
D19-5543,W19-5303,0,0.118315,"Missing"
D19-5543,N19-1202,0,0.0343993,"Missing"
D19-5543,D18-1050,0,0.0227413,"s. 1 Introduction Neural Machine Translation (NMT) models trained on large, clean parallel corpora have reached impressive performance in translating clean texts following various architectures (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). Despite this success, NMT models still lack robustness when applied to noisy sentences. Belinkov and Bisk (2017) show that perturbations in characters could cause a significant decrease in translation quality. They point out that training on noisy data, which can be seen as adversarial training, might help to improve model robustness. Michel and Neubig (2018) propose the Machine Translation on Noisy Text (MTNT) dataset, which contains parallel sentence pairs with comments crawled from Reddit1 and manual translations. This dataset contains user-generated text 1 Lucia Specia Department of Computing Imperial College London, UK l.specia@imperial.ac.uk 2 http://www.statmt.org/wmt19/ robustness.html www.reddit.com 328 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 328–336 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics on monolingual data was used to generate noisy parallel sen"
D19-5543,W19-5365,0,0.0680442,"level of informal language in the translations (e.g. keeping emojis/emoticons). Based on this assumption, we believe that back translation of clean texts, although providing a large volume of extra data, is limited since it removes most types of noise from the translations. In addition to adapting models on noisy parallel data, other techniques have been used to improve performance, generally measured according to BLEU (Papineni et al., 2002) against clean references. For example, Berard et al. (2019) apply inline-casing by adding special tokens before each word to represent word casing. In (Murakami et al., 2019), placeholders are used to help to translate sentences with emojis. In this paper, we also explore data augmentaAbstract Neural Machine Translation (NMT) models have been proved strong when translating clean texts, but they are very sensitive to noise in the input. Improving NMT models robustness can be seen as a form of “domain” adaption to noise. The recently created Machine Translation on Noisy Text task corpus provides noisy-clean parallel data for a few language pairs, but this data is very limited in size and diversity. The state-of-the-art approaches are heavily dependent on large volum"
D19-5543,W19-5363,0,0.0366762,"Missing"
D19-5543,P02-1040,0,0.105149,"adding synthetic noise (Berard et al., 2019). The robustness of an NMT model can be seen as denoising source sentences (e.g. dealing with typos, etc.) while keeping a similar level of informal language in the translations (e.g. keeping emojis/emoticons). Based on this assumption, we believe that back translation of clean texts, although providing a large volume of extra data, is limited since it removes most types of noise from the translations. In addition to adapting models on noisy parallel data, other techniques have been used to improve performance, generally measured according to BLEU (Papineni et al., 2002) against clean references. For example, Berard et al. (2019) apply inline-casing by adding special tokens before each word to represent word casing. In (Murakami et al., 2019), placeholders are used to help to translate sentences with emojis. In this paper, we also explore data augmentaAbstract Neural Machine Translation (NMT) models have been proved strong when translating clean texts, but they are very sensitive to noise in the input. Improving NMT models robustness can be seen as a form of “domain” adaption to noise. The recently created Machine Translation on Noisy Text task corpus provide"
D19-5543,W19-5364,0,0.0390009,"Missing"
D19-5543,P16-1162,0,0.687028,"versarial methods are proposed to inject random noise into clean training data in Cheng et al. (2018, 2019). We explore the following new methods as alternative ways to augment noisy parallel data. tion for robustness but focus on techniques other than back translation. We follow the WMT19 Robustness Task and conduct experiments under constrained and unconstrained data settings on Fr↔En as language pairs. Under the constrained setting, we only use datasets provided by the shared task, and propose new data augmentation methods to generate noise from this data. We compare back translation (BT) (Sennrich et al., 2016a) with forward translation (FT) on noisy texts and find that pseudo-parallel data from forward translation can help improve more robustness. We also adapt the idea of fuzzy matches from Bulte and Tezcan (2019) to the MTNT case by finding similar sentences in a parallel corpus to augment the limited noisy data. Results show that the fuzzy match method can extend noisy parallel data and improve model performance on both noisy and clean texts. The proposed techniques substantially outperform the baseline. While they still lag behind the winning submission in the WMT19 shared task, the resulting"
D19-5543,W19-5367,0,0.189111,"t of Computing Imperial College London, UK zhenhao.li18@imperial.ac.uk with different kinds of noise, e.g., typos, grammatical errors, emojis, spoken languages, etc. for two language pairs. In the WMT19 Robustness Task2 (Li et al., 2019), improving NMT robustness is treated as a domain adaption problem. The MTNT dataset is used as in-domain data, where models are trained with clean data and adapted to noisy data. Domain adaption is conducted in two main methods: fine tuning on in-domain data (Dabre and Sumita, 2019; Post and Duh, 2019) and mixed training with domain tags (Berard et al., 2019; Zheng et al., 2019). The size of the noisy data provided by the shared task is small, with only thousands of noisy sentence pairs on each direction. Hence most approaches participating in the task performed noisy data augmentation using back translation (Berard et al., 2019; Helcl et al., 2019; Zheng et al., 2019), with some approaches also directly adding synthetic noise (Berard et al., 2019). The robustness of an NMT model can be seen as denoising source sentences (e.g. dealing with typos, etc.) while keeping a similar level of informal language in the translations (e.g. keeping emojis/emoticons). Based on thi"
D19-5543,W19-5368,0,0.0219086,"n on Noisy Text (MTNT) dataset, which contains parallel sentence pairs with comments crawled from Reddit1 and manual translations. This dataset contains user-generated text 1 Lucia Specia Department of Computing Imperial College London, UK l.specia@imperial.ac.uk 2 http://www.statmt.org/wmt19/ robustness.html www.reddit.com 328 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 328–336 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics on monolingual data was used to generate noisy parallel sentences (Murakami et al., 2019; Zhou et al., 2019; Berard et al., 2019; Helcl et al., 2019). Zheng et al. (2019) proposed an extension of back translation that generates noisy translations from clean monolingual data. Therefore, after reversing the direction, the noisy translations become the source, which would simulate the noisy source sentences from the MTNT parallel data. Synthetic noise is injected into clean data to form noisy parallel data in Belinkov and Bisk (2017); Karpukhin et al. (2019). However, rule-based synthetic noise injection is limited to certain types of noise. Adversarial methods are proposed to inject random noise into"
D19-5543,P16-1009,0,\N,Missing
D19-5543,W19-5366,0,\N,Missing
E17-1101,P14-2048,0,0.0178262,"al traits in translationese A large body of previous research has established that translations constitute an autonomic language variety: a special dialect of the target language, often re1075 ferred to as translationese (Gellerstam, 1986). Recent corpus-based investigations of translationese demonstrated that originals and translations are distinguishable by means of supervised and unsupervised classification (Baroni and Bernardini, 2006; Volansky et al., 2015; Rabinovich and Wintner, 2015). The identification of machinetranslated text has also been proven an easy task (Arase and Zhou, 2013; Aharoni et al., 2014). Previous work has investigated how gender artifacts are carried over into human translation in the context of social and gender studies, as well as cultural transfer (Simon, 2003; Von Flotow, 2010). Shlesinger et al. (2009) conducted a computational study exploring the implications of the translator’s gender on the final product. They conclude that “the computer could not be trained to accurately predict the gender of the translator”. Preservation of authorial style in literary translations was studied by Lynch (2014), identifying Russian authors of translated English literature, by using (s"
E17-1101,P13-1157,0,0.0255352,"erent domains. Authorial traits in translationese A large body of previous research has established that translations constitute an autonomic language variety: a special dialect of the target language, often re1075 ferred to as translationese (Gellerstam, 1986). Recent corpus-based investigations of translationese demonstrated that originals and translations are distinguishable by means of supervised and unsupervised classification (Baroni and Bernardini, 2006; Volansky et al., 2015; Rabinovich and Wintner, 2015). The identification of machinetranslated text has also been proven an easy task (Arase and Zhou, 2013; Aharoni et al., 2014). Previous work has investigated how gender artifacts are carried over into human translation in the context of social and gender studies, as well as cultural transfer (Simon, 2003; Von Flotow, 2010). Shlesinger et al. (2009) conducted a computational study exploring the implications of the translator’s gender on the final product. They conclude that “the computer could not be trained to accurately predict the gender of the translator”. Preservation of authorial style in literary translations was studied by Lynch (2014), identifying Russian authors of translated English"
E17-1101,D11-1120,0,0.0188716,"experimental results of personalized MT models. Gender classification A large body of research has been devoted to isolating distinguishing traits of male and female linguistic variations, both theoretically and empirically. Apart from content, male and female speech has been shown to exhibit stylistic and syntactic differences. Several studies demonstrated that literary texts and blog posts produced by male and female writers can be distinguished by means of automatic classification, using (content-independent) function words and ngrams of POS tags (Koppel et al., 2002; Schler et al., 2006; Burger et al., 2011). Although the tendencies of individual word usage are a subject of controversy, distributions of word categories across male and female English speech is nearly consensual: pronouns and verbs are more frequent in female texts, while nouns and numerals are more typical to male productions. Newman et al. (2008) carried out a comprehensive empirical study corroborating these findings with large and diverse datasets. However, little effort has been dedicated to investigating the variation of individual markers of demographic traits across different languages. Johannsen et al. (2015) conducted a l"
E17-1101,2012.eamt-1.60,0,0.00644988,"English EP data distributions across two dimensions: gender (left) and trans. status (right). TED talks (transcripts and translations): a collection of texts from a completely different genre, where demographic traits may manifest differently. Testing the potential benefits of personalized SMT models on these two very diverse datasets allows us to examine the robustness of our approach. We used the TED gender-annotated data from Mirkin et al. (2015).8 This corpus contains annotation of the speaker’s gender included in the English-French corpus of the IWSLT 2014 Evaluation Campaign’s MT track (Cettolo et al., 2012). We annotated 68 additional talks from the development and test sets of IWSLT 2014, 2015 and 2016. Using the full set, we split the TED parallel corpora by gender to obtain sub-corpora of 140K and 43K sentence pairs for male and female speakers, respectively. The sizes of the datasets used for training, tuning and testing of SMT models are shown in Table 3. Relatively large test sets are used for evaluation of the MT results for the sake of reliable per-outcome gender classification (§4.1). Although the size of the training/tuning/test sets in either direction for any language-pair is the sam"
E17-1101,P96-1041,0,0.0831521,"cifically, 370 chunks for each gender. Aiming to abstract away from content and capture instead stylistic and syntactic characteristics, we used as our feature set the combination of function words (FW)10 and (the top-1,000 most frequent) POS-trigrams. We employ 10-fold crossvalidation for evaluation of classification accuracy. 4.4 Downloaded from http://cm.xrce.xerox.com/. SMT setting We trained phrase-based SMT models with Moses (Koehn et al., 2007), an open source SMT system. KenLM (Heafield, 2011) was used for language modeling. We trained 5-gram language models with Kneser-Ney smoothing (Chen and Goodman, 1996). The models were tuned using Minimum Error Rate Tuning (MERT) (Och, 2003). Our preprocessing included cleaning (removal of empty, long and misaligned sentences), tokenization and punctuation normalization. The Stanford tokenizer (Manning et al., 2014) was used for tokenization and standard Moses scripts were used for other preprocessing tasks. We used BLEU (Papineni et al., 2002) to evaluate MT quality against one reference translation. 5 Personalized SMT models In order to investigate and improve gender traits transfer in MT, we devise and experiment with gender-aware SMT models. We demonstr"
E17-1101,P13-2061,0,0.0121704,"n (MT) Virtually no previous work in MT takes into account personal traits. State-of-the-art MT systems are built from examples of translations, where the general assumption is that the more data available to train models, the better, and a single model is usually produced. Exceptions to this assumption revolve around work on domain adaption, where systems are customized by using data that comes from a particular text domain (Hasler et al., 2014; Cuong and Sima’an, 2015); and work on data cleaning, where spurious data is removed from the training set to ensure the quality of the final models (Cui et al., 2013; Simard, 2014). Personal traits, sometimes well marked in the translation examples, are therefore not explicitly addressed. Learning from different, sometimes conflicting writing styles can hinder model performance and lead to translations that are unfaithful to the source text. Focusing on reader preferences, Mirkin and Meunier (2015) used a collaborative filtering approach from recommender systems, where a user’s preferred translation is predicted based on the preferences of similar users. However, the user preferences in this case refer to the overall choice between MT systems of a specifi"
E17-1101,N15-1043,0,0.0370924,"Missing"
E17-1101,W14-3358,0,0.0213763,"opic classification (Hovy, 2015), very little attention has been paid to translation. We provide here a brief summary of research relevant to our work. Machine translation (MT) Virtually no previous work in MT takes into account personal traits. State-of-the-art MT systems are built from examples of translations, where the general assumption is that the more data available to train models, the better, and a single model is usually produced. Exceptions to this assumption revolve around work on domain adaption, where systems are customized by using data that comes from a particular text domain (Hasler et al., 2014; Cuong and Sima’an, 2015); and work on data cleaning, where spurious data is removed from the training set to ensure the quality of the final models (Cui et al., 2013; Simard, 2014). Personal traits, sometimes well marked in the translation examples, are therefore not explicitly addressed. Learning from different, sometimes conflicting writing styles can hinder model performance and lead to translations that are unfaithful to the source text. Focusing on reader preferences, Mirkin and Meunier (2015) used a collaborative filtering approach from recommender systems, where a user’s preferred tra"
E17-1101,W11-2123,0,0.00745722,"t al., 2009). In all experiments we used (the maximal) equal amount of data from each category (M and F), specifically, 370 chunks for each gender. Aiming to abstract away from content and capture instead stylistic and syntactic characteristics, we used as our feature set the combination of function words (FW)10 and (the top-1,000 most frequent) POS-trigrams. We employ 10-fold crossvalidation for evaluation of classification accuracy. 4.4 Downloaded from http://cm.xrce.xerox.com/. SMT setting We trained phrase-based SMT models with Moses (Koehn et al., 2007), an open source SMT system. KenLM (Heafield, 2011) was used for language modeling. We trained 5-gram language models with Kneser-Ney smoothing (Chen and Goodman, 1996). The models were tuned using Minimum Error Rate Tuning (MERT) (Och, 2003). Our preprocessing included cleaning (removal of empty, long and misaligned sentences), tokenization and punctuation normalization. The Stanford tokenizer (Manning et al., 2014) was used for tokenization and standard Moses scripts were used for other preprocessing tasks. We used BLEU (Papineni et al., 2002) to evaluate MT quality against one reference translation. 5 Personalized SMT models In order to inv"
E17-1101,P15-1073,0,0.0123096,"se of the target language in both manual and automatic translation products. The main contributions of this paper are thus: (i) a new parallel corpus annotated with gender and age information, (ii) an in-depth assessment of the projection of gender traits in manual and automatic translation, and (iii) experiments showing that gender-personalized SMT systems better project gender traits while maintaining translation quality. 2 Related work While modeling of demographic traits has been proven beneficial in some NLP tasks such as sentiment analysis (Volkova et al., 2013) or topic classification (Hovy, 2015), very little attention has been paid to translation. We provide here a brief summary of research relevant to our work. Machine translation (MT) Virtually no previous work in MT takes into account personal traits. State-of-the-art MT systems are built from examples of translations, where the general assumption is that the more data available to train models, the better, and a single model is usually produced. Exceptions to this assumption revolve around work on domain adaption, where systems are customized by using data that comes from a particular text domain (Hasler et al., 2014; Cuong and S"
E17-1101,K15-1011,0,0.0117666,"er et al., 2006; Burger et al., 2011). Although the tendencies of individual word usage are a subject of controversy, distributions of word categories across male and female English speech is nearly consensual: pronouns and verbs are more frequent in female texts, while nouns and numerals are more typical to male productions. Newman et al. (2008) carried out a comprehensive empirical study corroborating these findings with large and diverse datasets. However, little effort has been dedicated to investigating the variation of individual markers of demographic traits across different languages. Johannsen et al. (2015) conducted a large-scale study on linguistic variation over age and gender across multiple languages in a social media domain. They showed that gender differences captured by shallow syntactic features were preserved across languages, when examined by linguistic categories. However, they did not study the distribution of individual gender markers across domains and languages. Our work demonstrates that while marker categories are potentially preserved, individual words typical to male and female language vary across languages and, more prominently, across different domains. Authorial traits in"
E17-1101,P07-2045,0,0.00586138,"hine classifiers with the default linear kernel (Hall et al., 2009). In all experiments we used (the maximal) equal amount of data from each category (M and F), specifically, 370 chunks for each gender. Aiming to abstract away from content and capture instead stylistic and syntactic characteristics, we used as our feature set the combination of function words (FW)10 and (the top-1,000 most frequent) POS-trigrams. We employ 10-fold crossvalidation for evaluation of classification accuracy. 4.4 Downloaded from http://cm.xrce.xerox.com/. SMT setting We trained phrase-based SMT models with Moses (Koehn et al., 2007), an open source SMT system. KenLM (Heafield, 2011) was used for language modeling. We trained 5-gram language models with Kneser-Ney smoothing (Chen and Goodman, 1996). The models were tuned using Minimum Error Rate Tuning (MERT) (Och, 2003). Our preprocessing included cleaning (removal of empty, long and misaligned sentences), tokenization and punctuation normalization. The Stanford tokenizer (Manning et al., 2014) was used for tokenization and standard Moses scripts were used for other preprocessing tasks. We used BLEU (Papineni et al., 2002) to evaluate MT quality against one reference tra"
E17-1101,C14-1037,0,0.025702,"ted text has also been proven an easy task (Arase and Zhou, 2013; Aharoni et al., 2014). Previous work has investigated how gender artifacts are carried over into human translation in the context of social and gender studies, as well as cultural transfer (Simon, 2003; Von Flotow, 2010). Shlesinger et al. (2009) conducted a computational study exploring the implications of the translator’s gender on the final product. They conclude that “the computer could not be trained to accurately predict the gender of the translator”. Preservation of authorial style in literary translations was studied by Lynch (2014), identifying Russian authors of translated English literature, by using (shallow) stylistic and syntactic features. Forsyth and Lam (2014) investigated authorial discriminability in translations of French originals into English, inspecting two distinct human translations, as well as automatic translation of the same sources. Our work, to the best of our knowledge, is the first to automatically identify speaker gender in manual, and more prominently, automatic translations over multiple domains and languagepairs, examining distribution of gender markers in source and target languages. 3 Europa"
E17-1101,P14-5010,0,0.00145718,"e employ 10-fold crossvalidation for evaluation of classification accuracy. 4.4 Downloaded from http://cm.xrce.xerox.com/. SMT setting We trained phrase-based SMT models with Moses (Koehn et al., 2007), an open source SMT system. KenLM (Heafield, 2011) was used for language modeling. We trained 5-gram language models with Kneser-Ney smoothing (Chen and Goodman, 1996). The models were tuned using Minimum Error Rate Tuning (MERT) (Och, 2003). Our preprocessing included cleaning (removal of empty, long and misaligned sentences), tokenization and punctuation normalization. The Stanford tokenizer (Manning et al., 2014) was used for tokenization and standard Moses scripts were used for other preprocessing tasks. We used BLEU (Papineni et al., 2002) to evaluate MT quality against one reference translation. 5 Personalized SMT models In order to investigate and improve gender traits transfer in MT, we devise and experiment with gender-aware SMT models. We demonstrate that despite their simplicity, these models lead to better preservation of gender traits, while not harming the general quality of the translations. 9 10 8 Classification setting http://nlp.stanford.edu/software/tagger.shtml We used the lists of fu"
E17-1101,D15-1238,1,0.726418,"omain adaption, where systems are customized by using data that comes from a particular text domain (Hasler et al., 2014; Cuong and Sima’an, 2015); and work on data cleaning, where spurious data is removed from the training set to ensure the quality of the final models (Cui et al., 2013; Simard, 2014). Personal traits, sometimes well marked in the translation examples, are therefore not explicitly addressed. Learning from different, sometimes conflicting writing styles can hinder model performance and lead to translations that are unfaithful to the source text. Focusing on reader preferences, Mirkin and Meunier (2015) used a collaborative filtering approach from recommender systems, where a user’s preferred translation is predicted based on the preferences of similar users. However, the user preferences in this case refer to the overall choice between MT systems of a specific reader, rather than a choice based on traits of the writer. Mirkin et al. (2015) motivated the need for personalization of MT models by showing that automatic translation does not preserve demographic and psychometric traits. They suggested treating the problem as a domain adaptation one, but did not provide experimental results of pe"
E17-1101,D15-1130,1,0.810976,"emographic trait (partially due to the absence of parallel data annotated for other traits). We evaluate the accuracy of automatic gender classification on original texts, on their manual translations and on their automatic translations generated through statistical machine translation (SMT). We show that while gender has a strong signal in originals, this signal is obfuscated in human and machine translation. Surprisingly, determining gender over manual translation is even harder than over SMT; this may be an artifact of the translation process itself or the human translators involved in it. Mirkin et al. (2015) were the first to show that authorial gender signals tend to vanish through both manual and automatic translation, using a small TED talks dataset. We use their data and extend it with a version of Europarl that we annotated with age and gender (§3). Furthermore, we conduct experiments with two language pairs, in both directions (§4). We also adopt a different classification methodology based on the finding that the translation process itself has a stronger signal than the author’s gender (§4.1). We then move on to assessing gender traits in SMT (§5). Since SMT systems typically do not take p"
E17-1101,P03-1021,0,0.0201708,"instead stylistic and syntactic characteristics, we used as our feature set the combination of function words (FW)10 and (the top-1,000 most frequent) POS-trigrams. We employ 10-fold crossvalidation for evaluation of classification accuracy. 4.4 Downloaded from http://cm.xrce.xerox.com/. SMT setting We trained phrase-based SMT models with Moses (Koehn et al., 2007), an open source SMT system. KenLM (Heafield, 2011) was used for language modeling. We trained 5-gram language models with Kneser-Ney smoothing (Chen and Goodman, 1996). The models were tuned using Minimum Error Rate Tuning (MERT) (Och, 2003). Our preprocessing included cleaning (removal of empty, long and misaligned sentences), tokenization and punctuation normalization. The Stanford tokenizer (Manning et al., 2014) was used for tokenization and standard Moses scripts were used for other preprocessing tasks. We used BLEU (Papineni et al., 2002) to evaluate MT quality against one reference translation. 5 Personalized SMT models In order to investigate and improve gender traits transfer in MT, we devise and experiment with gender-aware SMT models. We demonstrate that despite their simplicity, these models lead to better preservatio"
E17-1101,P02-1040,0,0.126265,"ting We trained phrase-based SMT models with Moses (Koehn et al., 2007), an open source SMT system. KenLM (Heafield, 2011) was used for language modeling. We trained 5-gram language models with Kneser-Ney smoothing (Chen and Goodman, 1996). The models were tuned using Minimum Error Rate Tuning (MERT) (Och, 2003). Our preprocessing included cleaning (removal of empty, long and misaligned sentences), tokenization and punctuation normalization. The Stanford tokenizer (Manning et al., 2014) was used for tokenization and standard Moses scripts were used for other preprocessing tasks. We used BLEU (Papineni et al., 2002) to evaluate MT quality against one reference translation. 5 Personalized SMT models In order to investigate and improve gender traits transfer in MT, we devise and experiment with gender-aware SMT models. We demonstrate that despite their simplicity, these models lead to better preservation of gender traits, while not harming the general quality of the translations. 9 10 8 Classification setting http://nlp.stanford.edu/software/tagger.shtml We used the lists of function words available at https://code.google.com/archive/p/stop-words. 1078 dataset EP TED language-pair en-fr & fr-en en-de & de-"
E17-1101,Q15-1030,1,0.83956,", individual words typical to male and female language vary across languages and, more prominently, across different domains. Authorial traits in translationese A large body of previous research has established that translations constitute an autonomic language variety: a special dialect of the target language, often re1075 ferred to as translationese (Gellerstam, 1986). Recent corpus-based investigations of translationese demonstrated that originals and translations are distinguishable by means of supervised and unsupervised classification (Baroni and Bernardini, 2006; Volansky et al., 2015; Rabinovich and Wintner, 2015). The identification of machinetranslated text has also been proven an easy task (Arase and Zhou, 2013; Aharoni et al., 2014). Previous work has investigated how gender artifacts are carried over into human translation in the context of social and gender studies, as well as cultural transfer (Simon, 2003; Von Flotow, 2010). Shlesinger et al. (2009) conducted a computational study exploring the implications of the translator’s gender on the final product. They conclude that “the computer could not be trained to accurately predict the gender of the translator”. Preservation of authorial style in"
E17-1101,2005.mtsummit-papers.11,0,0.00482647,"Lam (2014) investigated authorial discriminability in translations of French originals into English, inspecting two distinct human translations, as well as automatic translation of the same sources. Our work, to the best of our knowledge, is the first to automatically identify speaker gender in manual, and more prominently, automatic translations over multiple domains and languagepairs, examining distribution of gender markers in source and target languages. 3 Europarl with demographic info We created a resource1 based on the parallel corpus of the European Parliament (Europarl) Proceedings (Koehn, 2005). More specifically, we utilize the extension of its en-fr and en-de parallel versions (Rabinovich et al., 2015), where each sentence-pair is annotated with speaker name, the original language the sentence was uttered in, and the date of the corresponding session protocol. To extend speaker information with demographic properties, we used the Europarl website’s MEP information pages2 and applied a procedure of gender and age identification, as further detailed in §3.1. The final resource comprises en-fr and en-de parallel bilingual corpora where metadata of mem1 2 Available at http://cl.haifa."
E17-1101,J14-2003,0,0.00533819,"on, without harming the quality of the translation, thereby creating more personalized machine translation systems. 1 Raj Nath Patel C-DAC Mumbai Gulmohar Cross Road No. 9, Juhu Mumbai-400049, India patelrajnath@gmail.com Introduction Among many factors that mold the makeup of a text, gender and other authorial traits play a major role in our perception of the content we face. Many studies have shown that these traits can be identified by means of automatic classification methods. Classical examples include gender identification (Koppel et al., 2002), and authorship attribution and profiling (Seroussi et al., 2014). Most research, however, addressed texts in a single language, typically English. We investigate a related but different question: we are interested in understanding what happens to personality and demographic textual markers during the translation process. It is generally agreed that good translation goes beyond transformation of the original content, by preserving more subtle and implicit characteristics inferred by author’s personality, as well as era, geography, and various cultural and sociological aspects. In this work we explore whether translations preserve the stylistic characteristi"
E17-1101,2014.amta-researchers.6,0,0.0399392,"o previous work in MT takes into account personal traits. State-of-the-art MT systems are built from examples of translations, where the general assumption is that the more data available to train models, the better, and a single model is usually produced. Exceptions to this assumption revolve around work on domain adaption, where systems are customized by using data that comes from a particular text domain (Hasler et al., 2014; Cuong and Sima’an, 2015); and work on data cleaning, where spurious data is removed from the training set to ensure the quality of the final models (Cui et al., 2013; Simard, 2014). Personal traits, sometimes well marked in the translation examples, are therefore not explicitly addressed. Learning from different, sometimes conflicting writing styles can hinder model performance and lead to translations that are unfaithful to the source text. Focusing on reader preferences, Mirkin and Meunier (2015) used a collaborative filtering approach from recommender systems, where a user’s preferred translation is predicted based on the preferences of similar users. However, the user preferences in this case refer to the overall choice between MT systems of a specific reader, rathe"
E17-1101,D13-1187,0,0.0264891,"traits of the original language overshadow those of the target language in both manual and automatic translation products. The main contributions of this paper are thus: (i) a new parallel corpus annotated with gender and age information, (ii) an in-depth assessment of the projection of gender traits in manual and automatic translation, and (iii) experiments showing that gender-personalized SMT systems better project gender traits while maintaining translation quality. 2 Related work While modeling of demographic traits has been proven beneficial in some NLP tasks such as sentiment analysis (Volkova et al., 2013) or topic classification (Hovy, 2015), very little attention has been paid to translation. We provide here a brief summary of research relevant to our work. Machine translation (MT) Virtually no previous work in MT takes into account personal traits. State-of-the-art MT systems are built from examples of translations, where the general assumption is that the more data available to train models, the better, and a single model is usually produced. Exceptions to this assumption revolve around work on domain adaption, where systems are customized by using data that comes from a particular text dom"
E17-2006,P15-2011,0,0.513357,"Missing"
E17-2006,P14-2075,0,0.340551,"but most of them still adhere to the following pipeline: Complex Word Identification (CWI) to select words to simplify; Substitution Generation (SG) to produce candidate substitutions for each complex word; Substitution Selection (SS) to filter candidates that do not fit the context of the complex word; and Substitution Ranking (SR) to rank them according to their simplicity. The most effective LS approaches exploit Machine Learning techniques. In CWI, ensembles that use large corpora and thesauri dominate the top 10 systems in the CWI task of SemEval 2016 (Paetzold and Specia, 2016d). In SG, Horn et al. (2014) extract candidates from a parallel Wikipedia and Simple Wikipedia corpus, yielding major improvements over previous approaches 2 Hybrid Substitution Generation Our approach combines candidate substitutions from two sources: the Newsela corpus and retrofitted context-aware word embedding models. 34 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 34–40, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2.1 SG via Parallel Data their synonyms, then use the algorithm describ"
E17-2006,O13-1007,0,0.10872,"simplicity. Let M (ci , cj ) be the value estimated by our model for a pair of candidates ci and cj of a generated set C. During the ordering, we calculate the final score R(ci ) of all candidates ci (Eq. 1). X M (ci , cj ) Substitution Generation Evaluation Here we assess the performance of our SG approach in isolation (NNLS/SG), and when paired with our SS strategy (NNLS/SG+SS), as described in Sections 2 and 3. We compare them to the generators of all approaches featured in the benchmarks of Paetzold and Specia (2016a): Devlin (Devlin and Tait, 1998), Biran (Biran et al., 2011), Yamamoto (Kajiwara et al., 2013), Horn (Horn ˇ et al., 2014), Glavas (Glavaˇs and Stajner, 2015) and Paetzold (Paetzold and Specia, 2015; Paetzold and Specia, 2016f). These SG strategies extract candidates from WordNet, Wikipedia and Simple Wikipedia articles, Merriam dictionary, sentencealigned Wikipedia and Simple Wikipedia articles, typical word embeddings and context-aware word embeddings, respectively. They are all available in the LEXenstein framework (Paetzold and Specia, 2015). We use two common evaluation datasets for LS: BenchLS (Paetzold and Specia, 2016a), which contains 929 instances and is annotated by English"
E17-2006,P13-1151,0,0.106186,"an Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 34–40, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2.1 SG via Parallel Data their synonyms, then use the algorithm described in (Faruqui et al., 2015). We train a bag-of-words (CBOW) model (Mikolov et al., 2013b) of 1,300 dimensions with word2vec (Mikolov et al., 2013a) using a corpus of over 7 billion words that includes the SubIMDB corpus (Paetzold and Specia, 2016b), UMBC webbase2 , News Crawl3 , SUBTLEX (Brysbaert and New, 2009), Wikipedia and Simple Wikipedia (Kauchak, 2013). We retrofit the model over WordNet’s synonym relations only. We choose this model training configuration because it has been shown to perform best for LS in a recent extensive benchmarking (Paetzold, 2016). For each target word in the Newsela vocabulary we then generate as complementary candidate substitutions the three words in the model with the lowest cosine distances from the target word that have the same POS tag and are not a morphological variant. As demonstrated by Paetzold and Specia (2016a), in SG parallel corpora tend to yield higher Precision, but noticeably lower Recall than emb"
E17-2006,W11-2107,0,0.0487807,"To employ the Newsela corpus in SG, we first produce sentence alignments for all pairs of versions of a given article. To do so, we use paragraph and sentence alignment algorithms from (Paetzold and Specia, 2016g). They align paragraphs with sentences that have high TF-IDF similarity, concatenate aligned paragraphs, and finally align concatenated paragraphs at sentence-level using the TF-IDF similarity between them. Using this algorithm, we produce 550,644 sentence alignments. We then tag sentences using the Stanford Tagger (Toutanvoa and Manning, 2000), produce word alignments using Meteor (Denkowski and Lavie, 2011), and extract candidates using a strategy similar to that of Horn et al. (2014). First we consider all aligned complex-to-simple word pairs as candidates. Then we filter them by discarding pairs which: do not share the same POS tag, have at least one non-content word, have at least one proper noun, or share the same stem. After filtering, we inflect all nouns, verbs, adjectives and adverbs to all possible variants. We then complement the candidate substitutions from the Newsela corpus using the following word embeddings model. 2.2 3 We pair our generator with the Unsupervised Boundary Ranking"
E17-2006,P15-4015,1,0.903509,"Missing"
E17-2006,L16-1491,1,0.908722,"Missing"
E17-2006,N15-1184,0,0.0523727,"Missing"
E17-2006,C16-1157,1,0.896984,"Missing"
E17-2006,W00-1308,0,0.108991,"Missing"
E17-2006,Q15-1021,0,0.111984,"Computer Science University of Sheffield, UK {g.h.paetzold,l.specia}@sheffield.ac.uk Abstract (Devlin, 1999; Biran et al., 2011). Glavaˇs and ˇ Stajner (2015) and Paetzold and Specia (2016f) employ word embedding models to generate candidates, leading to even better results. In SR, the state-of-the-art performance is achieved by employing supervised approaches: SVMRank (Horn et al., 2014) and Boundary Ranking (Paetzold and Specia, 2015). Supervised approaches have the caveat of requiring annotated data, but as a consequence they can adapt to the needs of a specific target audience. Recently, (Xu et al., 2015) introduced the Newsela corpus, a new resource composed of thousands of news articles simplified by professionals. Their analysis reveals the potential use of this corpus in simplification, but thus far no simplifiers exist that exploit this resource. The scale of this corpus and the fact that it was created by professionals opens new avenues for research, including using Neural Network approaches, which have proved promising for many related problems. Neural Networks for supervised ranking have performed well in Information Retrieval (Burges et al., 2005), Medical Risk Evaluation (Caruana et"
E17-2006,S16-1149,1,0.892293,"Missing"
E17-2006,W16-4912,0,0.554948,"simplifiers are more sophisticated, but most of them still adhere to the following pipeline: Complex Word Identification (CWI) to select words to simplify; Substitution Generation (SG) to produce candidate substitutions for each complex word; Substitution Selection (SS) to filter candidates that do not fit the context of the complex word; and Substitution Ranking (SR) to rank them according to their simplicity. The most effective LS approaches exploit Machine Learning techniques. In CWI, ensembles that use large corpora and thesauri dominate the top 10 systems in the CWI task of SemEval 2016 (Paetzold and Specia, 2016d). In SG, Horn et al. (2014) extract candidates from a parallel Wikipedia and Simple Wikipedia corpus, yielding major improvements over previous approaches 2 Hybrid Substitution Generation Our approach combines candidate substitutions from two sources: the Newsela corpus and retrofitted context-aware word embedding models. 34 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 34–40, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2.1 SG via Parallel Data their synonyms, t"
E17-2006,C16-1069,1,0.859164,"their synonyms, then use the algorithm described in (Faruqui et al., 2015). We train a bag-of-words (CBOW) model (Mikolov et al., 2013b) of 1,300 dimensions with word2vec (Mikolov et al., 2013a) using a corpus of over 7 billion words that includes the SubIMDB corpus (Paetzold and Specia, 2016b), UMBC webbase2 , News Crawl3 , SUBTLEX (Brysbaert and New, 2009), Wikipedia and Simple Wikipedia (Kauchak, 2013). We retrofit the model over WordNet’s synonym relations only. We choose this model training configuration because it has been shown to perform best for LS in a recent extensive benchmarking (Paetzold, 2016). For each target word in the Newsela vocabulary we then generate as complementary candidate substitutions the three words in the model with the lowest cosine distances from the target word that have the same POS tag and are not a morphological variant. As demonstrated by Paetzold and Specia (2016a), in SG parallel corpora tend to yield higher Precision, but noticeably lower Recall than embedding models. We add only three candidates in order increase Recall without compromising the high Precision from the Newsela corpus. corpus1 The Newsela (version 2016-01-29.1) contains 1,911 news articles i"
E17-2006,shardlow-2014-open,0,0.350489,"Missing"
E17-2006,S12-1046,1,0.69357,"titution Ranking Evaluation We also compare our Neural Ranking SR approach (NNLS/SR) to the rankers of all aforementioned lexical simplifiers. The Devlin, Biran, Yamamoto, Horn, Glavas and Paetzold rankers exploit Kucera-Francis coefficients (Rudell, 1993), hand-crafted complexity metrics, a supervised SVM ranker, rank averaging and Boundary Ranking, respectively. In this experiment we disregard the step of Confidence Check, since we aim to analyse the performance of our ranking strategy alone. The datasets used are those introduced for the English Lexical Simplification task of SemEval 2012 (Specia et al., 2012), to which dozens of systems were submitted. The training and test sets are composed of 300 and 1,710 instances, respectively. Each instance is composed of a sentence, a target complex word, and a series of candidate substitutions ranked by simplicity. We use TRank, the official metric of the SemEval 2012 task, which measures the proportion of instances for which the candidate with the highest goldrank was ranked first, as well Pearson (p) correlation. While TRank best captures the reliability of • Precision: The proportion of instances in which the target word was either replaced by a gold ca"
I17-1030,E17-2006,1,0.875786,"Missing"
I17-1030,P02-1040,0,0.100406,"thermore, we use a more reliable (professionally created) corpus and our approach is more flexible as we do not rely on syntactic parse trees at test time. recent work that relies on the professionally edited Newsela corpus (Xu et al., 2015). Simple English Wikipedia Zhu et al. (2010) propose a syntax-based translation model for TS that learns operations over the parse trees of the complex sentences. They outperform several baselines in terms of Flesch index. Coster and Kauchak (2011b) train a phrase-based machine translation (PBMT) system and obtain significant improvements in terms of BLEU (Papineni et al., 2002) over a baseline. Coster and Kauchak (2011a) extend a PBMT model to include phrase deletion and outperform Coster and Kauchak (2011b). Wubben et al. (2012) also train a PBMT system for TS with a dissimilarity-based re-ranking heuristic, outperforming Zhu et al. (2010) in terms of BLEU. Narayan and Gardent (2014) built TS systems by combining discourse representation structures with a PBMT model, which outperforms previous approaches. Xu et al. (2016) modify a syntax-based MT system in order to use a new metric – SARI – for optimization and to include special rules for paraphrasing. Although th"
I17-1030,W03-1004,0,0.14258,"el we implement as a baseline in this paper is equivalent to that in Zhang et al. (2017) without the lexical constraints, while the statistical model is equivalent to the one in Coster and Kauchak (2011b). 3 corpus, which only aligns different versions of the same document, we first align sentences using the algorithms described in (Paetzold and Specia, 2016b). Their algorithms search for the best alignment path between the paragraphs and sentences of parallel documents based on TF-IDF cosine similarity and an incremental vicinity search range. They address limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011b; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for (semi-) supervised training, allowing long-distance alignment skips, and capturing N-to-N alignments. The alignments produced are categorized as: Simplification via End-to-End Models In addition to requiring large amounts of training data, MT-based approaches to TS are limited because of their black-box way of addressing the problem. As we are going to show in this section, standard end-to-end systems without special adaptation to TS do not succeed in learning alternative form"
I17-1030,E17-3017,0,0.038702,"Missing"
I17-1030,P16-2055,1,0.841416,"s are learned from a more informed labeled FA and JB contributed equally to this paper. 295 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 295–305, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP dataset of natural simplifications, and can then be applied in a controlled way, e.g., in adaptive simplification scenarios that prioritize different ways of simplifying (e.g. compression or sentence splitting) depending on a particular user’s needs. The only previous work on TS via explicitly predicting simplification operations is that by Bingel and Søgaard (2016), who create training data from comparable text to label entire syntactic units and train a sequence labeling model to predict deletions and phrase substitutions in a complex sentence. Our approach is different in that it captures a larger variety of operations in a more global fashion, by using sentence-wide word alignments rather than surface heuristics. Furthermore, we use a more reliable (professionally created) corpus and our approach is more flexible as we do not rely on syntactic parse trees at test time. recent work that relies on the professionally edited Newsela corpus (Xu et al., 20"
I17-1030,W11-1603,0,0.0232335,"hout the lexical constraints, while the statistical model is equivalent to the one in Coster and Kauchak (2011b). 3 corpus, which only aligns different versions of the same document, we first align sentences using the algorithms described in (Paetzold and Specia, 2016b). Their algorithms search for the best alignment path between the paragraphs and sentences of parallel documents based on TF-IDF cosine similarity and an incremental vicinity search range. They address limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011b; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for (semi-) supervised training, allowing long-distance alignment skips, and capturing N-to-N alignments. The alignments produced are categorized as: Simplification via End-to-End Models In addition to requiring large amounts of training data, MT-based approaches to TS are limited because of their black-box way of addressing the problem. As we are going to show in this section, standard end-to-end systems without special adaptation to TS do not succeed in learning alternative formulations of the original text. With a few exceptions (by the neural model), they tend to"
I17-1030,W16-2323,0,0.0609456,"Missing"
I17-1030,W11-1601,0,0.796852,"Bingel and Søgaard, 2016) or the identification of difficult words (Paetzold and Specia, 2016a), and typically encodes relevant simplification operations as discrete labels on tokens. The other variant includes more general, higher-level types of simplifications that often entail the rephrasing or re-structuring of sentences, with content added or removed. These “natural” simplifications are often created for end-users rather than for research purposes. Examples of the latter simplification resources include the Newsela (Xu et al., 2015) and Simple English Wikipedia corpora (Zhu et al., 2010; Coster and Kauchak, 2011b). These resources generally encode interdependencies between different types of simplification better than single-purpose resources and may thus seem favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS s"
I17-1030,N10-1063,0,0.0542761,"nt to that in Zhang et al. (2017) without the lexical constraints, while the statistical model is equivalent to the one in Coster and Kauchak (2011b). 3 corpus, which only aligns different versions of the same document, we first align sentences using the algorithms described in (Paetzold and Specia, 2016b). Their algorithms search for the best alignment path between the paragraphs and sentences of parallel documents based on TF-IDF cosine similarity and an incremental vicinity search range. They address limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011b; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for (semi-) supervised training, allowing long-distance alignment skips, and capturing N-to-N alignments. The alignments produced are categorized as: Simplification via End-to-End Models In addition to requiring large amounts of training data, MT-based approaches to TS are limited because of their black-box way of addressing the problem. As we are going to show in this section, standard end-to-end systems without special adaptation to TS do not succeed in learning alternative formulations of the original text. With a few excep"
I17-1030,P11-2117,0,0.827854,"Bingel and Søgaard, 2016) or the identification of difficult words (Paetzold and Specia, 2016a), and typically encodes relevant simplification operations as discrete labels on tokens. The other variant includes more general, higher-level types of simplifications that often entail the rephrasing or re-structuring of sentences, with content added or removed. These “natural” simplifications are often created for end-users rather than for research purposes. Examples of the latter simplification resources include the Newsela (Xu et al., 2015) and Simple English Wikipedia corpora (Zhu et al., 2010; Coster and Kauchak, 2011b). These resources generally encode interdependencies between different types of simplification better than single-purpose resources and may thus seem favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS s"
I17-1030,2006.amta-papers.25,0,0.237758,"Missing"
I17-1030,W13-2901,0,0.0320694,"Missing"
I17-1030,Q14-1018,0,0.0483654,"Missing"
I17-1030,P07-2045,0,0.00635879,"t audience rather than research) resource to date. • Identical: The alignment is one-to-one and the sentences are exactly the same (96,909 pairs across all adjacent levels). • 1-to-1: The alignment is one-to-one and the original-simplified sentences are different (130,790 pairs across all adjacent levels). • Split: The alignment is 1-to-N (42,545 pairs across all adjacent levels). • Join: The alignment is N-to-1 (7,962 pairs across all adjacent levels). Translation Models. We built two types of models using state-of-the-art MT-based approaches: a phrase-based statistical MT model using Moses (Koehn et al., 2007),3 and a neural MT model using Nematus (Sennrich et al., 2017).4 The Neural Text Simplification tool (NTS) made available by Nisioi et al. (2017) was also used for comparison.5 For our translation-based experiments, we consider two combinations of sentence alignments, using (i) only one-to-one alignments (1-to-1) (130,970 sentence pairs), and (ii) all alignments (all), i.e., the entire sentence-aligned corpus with identical, 1-to-1, split and join alignments (278,206 sentence pairs). The first type of data (1to-1) is the focus of this paper (see §4). The latter variant is included in the exper"
I17-1030,D11-1038,0,0.271991,"Missing"
I17-1030,P12-1107,0,0.636948,"Missing"
I17-1030,P14-1041,0,0.764692,"resources and may thus seem favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS systems that bear resemblance to MT models (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Zhang et al., 2017; Nisioi et al., 2017). Abstract Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotat"
I17-1030,Q15-1021,0,0.54195,"f.alva,g.h.paetzold,c.scarton,l.specia}@sheffield.ac.uk bingel@di.ku.dk (Bingel and Søgaard, 2016) or the identification of difficult words (Paetzold and Specia, 2016a), and typically encodes relevant simplification operations as discrete labels on tokens. The other variant includes more general, higher-level types of simplifications that often entail the rephrasing or re-structuring of sentences, with content added or removed. These “natural” simplifications are often created for end-users rather than for research purposes. Examples of the latter simplification resources include the Newsela (Xu et al., 2015) and Simple English Wikipedia corpora (Zhu et al., 2010; Coster and Kauchak, 2011b). These resources generally encode interdependencies between different types of simplification better than single-purpose resources and may thus seem favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extens"
I17-1030,P17-2014,0,0.294853,"m given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS systems that bear resemblance to MT models (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Zhang et al., 2017; Nisioi et al., 2017). Abstract Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These complex-simple parallel sentence pairs often differ to such a high degr"
I17-1030,Q16-1029,0,0.385239,"m favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS systems that bear resemblance to MT models (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Zhang et al., 2017; Nisioi et al., 2017). Abstract Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These compl"
I17-1030,D17-1062,0,0.328819,"earning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS systems that bear resemblance to MT models (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Zhang et al., 2017; Nisioi et al., 2017). Abstract Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These complex-simple parallel sente"
I17-1030,C10-1152,0,0.835255,"bingel@di.ku.dk (Bingel and Søgaard, 2016) or the identification of difficult words (Paetzold and Specia, 2016a), and typically encodes relevant simplification operations as discrete labels on tokens. The other variant includes more general, higher-level types of simplifications that often entail the rephrasing or re-structuring of sentences, with content added or removed. These “natural” simplifications are often created for end-users rather than for research purposes. Examples of the latter simplification resources include the Newsela (Xu et al., 2015) and Simple English Wikipedia corpora (Zhu et al., 2010; Coster and Kauchak, 2011b). These resources generally encode interdependencies between different types of simplification better than single-purpose resources and may thus seem favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural"
I17-3001,W03-1004,0,0.25572,"translators, simplifiers and summarizers that automate the process of adapting content. In order to do so, machine learning algorithms benefit from texts aligned at lower levels, such as paragraph, sentence, or even word levels. These alignments are however challenging to obtain since documents often do not even have the same number of sentences, i.e. they are comparable but not parallel. For monolingual texts, which are the focus of this paper, previous work has proposed different ways for obtaining sentence alignments: Xu et al. (2015) extract alignments based on a similarity metric, while Barzilay and Elhadad (2003) employ a more complex data-driven model, and Paetzold and Specia (2016) employ a vicinitydriven search method. However, we were not able to find any available and easy-to-use tool that allows one to align comparable documents at different levels of granularity. To solve that problem, we introduce MASSAlign: a user friendly tool that allows one to align monolingual comparable documents at both paragraph and sentence level, annotate words in aligned sentences with transformation labels, and also visualize the output produced. We introduce MASSAlign: a Python library for the alignment and annota"
I17-3001,W11-1603,0,0.0537678,"Missing"
I17-3001,N13-1073,0,0.0303595,"l. Some examples of operations include deletions, where words and/or phrases are discarded; and lexical simplifications, where words and/or phrases are replaced with more familiar alternatives. MASSAlign’s annotation module provides novel algorithms that automatically identify deletions, substitutions, re-orderings, and additions of words and phrases. The annotation module requires a pair of aligned sentences, their constituency parse trees, and the word alignments between them. To obtain word alignments, many consolidated tools can be employed, such as Giza++ (Och and Ney, 2003), fast align (Dyer et al., 2013), and the monolingual word aligner (Sultan et al., 2014). Our annotation algorithms only require that the word alignments be in 1-index Pharaoh format, which can be obtained from any of the previously mentioned tools. Our module first annotates word-level substitutions, deletions and additions: if two words are aligned and are not an exact match, the word in the original sentence receives a REPLACE tag; if a word in the original sentence is not aligned, it is annotated as a DELETE; and if a word in the modified sentence is not aligned, it is annotated as an ADD . There may be some cases of sub"
I17-3001,C16-1109,0,0.0844432,"Missing"
I17-3001,J03-1002,0,0.00979647,"performed at phrase or word-level. Some examples of operations include deletions, where words and/or phrases are discarded; and lexical simplifications, where words and/or phrases are replaced with more familiar alternatives. MASSAlign’s annotation module provides novel algorithms that automatically identify deletions, substitutions, re-orderings, and additions of words and phrases. The annotation module requires a pair of aligned sentences, their constituency parse trees, and the word alignments between them. To obtain word alignments, many consolidated tools can be employed, such as Giza++ (Och and Ney, 2003), fast align (Dyer et al., 2013), and the monolingual word aligner (Sultan et al., 2014). Our annotation algorithms only require that the word alignments be in 1-index Pharaoh format, which can be obtained from any of the previously mentioned tools. Our module first annotates word-level substitutions, deletions and additions: if two words are aligned and are not an exact match, the word in the original sentence receives a REPLACE tag; if a word in the original sentence is not aligned, it is annotated as a DELETE; and if a word in the modified sentence is not aligned, it is annotated as an ADD"
I17-3001,E17-2006,1,0.616628,"chical alignment approach, i.e. it exploits information from higher-level alignments to support and improve the quality of lower-level alignments. Moreover, the method can be used in documents that are not organized as a set of paragraphs: one can simply take each comparable document as a large paragraph and then apply the sentence-level alignment algorithm. The method is also entirely unsupervised and one can easily customize the alignment process by changing the similarity metric, the threshold α, or the sets of vicinities considered. Finally, this method has already been shown effective in Paetzold and Specia (2017), where it is used in the extraction of complex-to-simple word 2 We then proceed to labeling re-orderings (MOVE) by determining if the relative index of a word (considering preceding or following DELETE s and ADD s) in the original sentence changes in the modified one. Words that are kept, replaced or rewritten may be subject to re-orderings, such that a token may have more than one label (e.g. REPLACE and MOVE). For that, we extend the set of operations by the compound operations REPLACE + MOVE (RM) and REWRITE + MOVE (RWM). In order to capture operations that span across syntactic units, suc"
I17-3001,Q14-1018,0,0.0460055,"e words and/or phrases are discarded; and lexical simplifications, where words and/or phrases are replaced with more familiar alternatives. MASSAlign’s annotation module provides novel algorithms that automatically identify deletions, substitutions, re-orderings, and additions of words and phrases. The annotation module requires a pair of aligned sentences, their constituency parse trees, and the word alignments between them. To obtain word alignments, many consolidated tools can be employed, such as Giza++ (Och and Ney, 2003), fast align (Dyer et al., 2013), and the monolingual word aligner (Sultan et al., 2014). Our annotation algorithms only require that the word alignments be in 1-index Pharaoh format, which can be obtained from any of the previously mentioned tools. Our module first annotates word-level substitutions, deletions and additions: if two words are aligned and are not an exact match, the word in the original sentence receives a REPLACE tag; if a word in the original sentence is not aligned, it is annotated as a DELETE; and if a word in the modified sentence is not aligned, it is annotated as an ADD . There may be some cases of substitutions where two synonymous are not aligned. In orde"
I17-3001,Q15-1021,0,0.314955,"l Language Processing (NLP): it can be used in the training of automatic translators, simplifiers and summarizers that automate the process of adapting content. In order to do so, machine learning algorithms benefit from texts aligned at lower levels, such as paragraph, sentence, or even word levels. These alignments are however challenging to obtain since documents often do not even have the same number of sentences, i.e. they are comparable but not parallel. For monolingual texts, which are the focus of this paper, previous work has proposed different ways for obtaining sentence alignments: Xu et al. (2015) extract alignments based on a similarity metric, while Barzilay and Elhadad (2003) employ a more complex data-driven model, and Paetzold and Specia (2016) employ a vicinitydriven search method. However, we were not able to find any available and easy-to-use tool that allows one to align comparable documents at different levels of granularity. To solve that problem, we introduce MASSAlign: a user friendly tool that allows one to align monolingual comparable documents at both paragraph and sentence level, annotate words in aligned sentences with transformation labels, and also visualize the out"
I17-3007,D14-1082,0,0.0105669,"tep was to design general-purpose simplification rules which will later be specialised for the domain under consideration (PA). This solution led to the development of MUSST, which includes SS modules for three languages. MUSST is based on the framework proposed by Siddharthan (2004) and is available as an open source Python implementation. Our rules split conjoint clauses, relative clauses and appositive phrases, and change sentences from passive into active voice. These are arguably the most widely applicable simplification operations across languages. We use the Stanford dependency parser (Chen and Manning, 2014) for the three languages, which enabled us to build a consistent multilingual tool. MUSST is evaluated using corpora extracted from the SIMPATICO use cases data. Such corpora (one for each language) were checked and – where applicable – syntactically simplified by experts in the area. Inspired by the work of Gasperin et al. (2009), we also developed a complexity checker module in order to select sentences that should be simplified. In addition, we implemented a confidence model in order to predict whether or not a simplification produced by MUSST is good enough to be shown to the end-user. Dev"
I17-3007,W11-2123,0,0.0097727,"a sentence is “good enough” for a user, we trained a confidence model to classify a simplification as acceptable or not. Using the 292 sentences simplified by the English system and evaluated in Section 3, we built a confidence model for this language. The 70 sentences classified as incorrect (Section 3) were used as negative examples, whilst the remaining sentences received the positive label. As features, we used the same basic counts as for the complexity checker (Section 4.1) along with language model (LM) probabilities and perplexity and grammar checking on the simplifications. KenLM11 (Heafield, 2011) was used to extract LM features. A Python grammar checker was used for evaluating grammaticality12 . The model was trained using the Random Forest implementation from scikit-learn with 10fold cross-validation and achieved 0.80 of accuracy (F1/Precision/recall = 0.60/0.69/0.53), outperforming the MC classifier (accuracy = 0.61). For Italian and Spanish, we also experimented with the datasets presented in Section 3, but the performance is worse because of the significantly smaller training sets. Nevertheless, both models outperform the majority class baseline in terms of accuracy. For Italian,"
I17-3007,L16-1491,1,0.821273,"ing the lexical and/or syntactic complexity of a text (Siddharthan, 2004). It is common to divide this task in two subtasks: lexical simplification (LS) and syntactic simplification (SS). Whilst LS deals with the identification and replacement of difficult words or phrases, SS focuses on making complex syntactic constructions simpler. It is known, for instance, that passive voice constructions are more complex than active voice, and that long sentences with multiple clauses are more difficult to be understood than short sentences with a single clause. Several tools have been developed for LS (Paetzold and Specia, 2016). However, we are not aware of freely available tools for SS. 1 https://www.simpatico-project.eu/ 25 The Companion Volume of the IJCNLP 2017 Proceedings: System Demonstrations, pages 25–28, c Taipei, Taiwan, November 27 – December 1, 2017. 2017 AFNLP in Tint4 (Palmero Aprosio and Moretti, 2016) (an adapted version of CoreNLP for Italian). Figure 1 shows the parser output for the sentence “These organisations have been checked by us and should provide you with a quality service.”, as an example. The sentence is first sent to the Analysis module that will search for discourse markers. In this ca"
J19-3004,P09-1035,0,0.0478639,"Missing"
J19-3004,W05-0909,0,0.498639,"t-effective automatic metrics with reproducible outcomes were also needed for the building of such systems (i.e., parameter tuning). A number of metrics were proposed to measure distance or similarity against one or more human (reference) translations. Simplistic metrics borrowed from speech recognition such as word error rate (WER) and its position-independent variant (PER) were soon replaced by more elaborate metrics that reward similarity beyond word-level, notably BLEU (Papineni et al. 2002), or perform comparisons at stem and synonymy levels, rather than exact match only, namely, Meteor (Banerjee and Lavie 2005). Nearly three decades on, automatic metrics still play a critical role in MT research and development and, despite a handful of metrics proposed every year, the problem is far from solved. Evidence of that is the annual campaign run by the Conference on Machine Translation (WMT), which—among other tasks—invites researchers to submit new evaluation metrics that are benchmarked against human judgments in a Metrics Task (see Bojar, Graham, and Kamran [2017], Bojar et al. [2016c], Stanojevi´c et al. [2015], and Mach´acek and Bojar [2014] for the most recent task results). Starting in 2005, WMT ha"
J19-3004,C04-1046,0,0.408274,"trics, BLEU, Meteor and UPF-Cobalt, with the fluency-oriented features. UoW-ReVal. UoW-ReVal (Gupta, Orasan, and van Genabith 2015) uses a dependencytree Long Short-Term Memory (LSTM) network to represent both the MT output and the reference with a dense vector. The segment level scores are obtained from a neural network that takes into account both the distance and the Hadamard product of the two representations. Training is performed on WMT ranking judgments converted to similarity scores. QualityEstimation. Different from reference-based evaluation metrics, Quality Estimation (QE) metrics (Blatz et al. 2004; Specia et al. 2009) aim to predict the quality of a machine translated segment solely from information about the segment itself and its corresponding source segment (and optionally) information about the MT system that produced the translation. The problem is framed as a supervised machine learning task, where, given source-MT pairs annotated with a quality label, a number of features can be extracted and used to train a machine learning algorithm. Sentence-level QE has been covered as a shared task in the last six editions of the Conference on Machine Translation (WMT) (see Bojar et al. (20"
J19-3004,W17-4755,0,0.086292,"latz et al. 2004; Specia et al. 2009) aim to predict the quality of a machine translated segment solely from information about the segment itself and its corresponding source segment (and optionally) information about the MT system that produced the translation. The problem is framed as a supervised machine learning task, where, given source-MT pairs annotated with a quality label, a number of features can be extracted and used to train a machine learning algorithm. Sentence-level QE has been covered as a shared task in the last six editions of the Conference on Machine Translation (WMT) (see Bojar et al. (2017) for the state-of-the-art approaches and latest results). In our experiments we use the best performing system from the WMT17 QE estimation task, the POSTECH system (Kim, Lee, and Na 2017). This is a neural prediction system that relies on two components (each based on a bidirectional long short-term memory unit): a predictor, which extracts in unsupervised ways “quality” vector representations from good examples of translations (i.e., large parallel corpora of human translations) and an estimator, which use the quality vectors and human quality labels to build prediction models. 523 Computati"
J19-3004,W07-0718,0,0.211929,"the consistency of human assessments themselves and, therefore, raw human scores are used. 544 Fomicheva and Specia Taking MT Evaluation Metrics to Extremes Table 7 Average score difference for the WMT16 Direct Assessment data set for a given judge and across two different judges. Q1 Q2 Q∗2 All Inter-AA Intra-AA 33.721 23.541 23.739 25.279 17.721 11.936 12.087 13.450 We further compare the levels of inter- and intra-annotator agreement for lower and higher-quality translations. The kappa coefficient commonly used to calculate the consistency of human judgments in the context of MT evaluation (Callison-Burch et al. 2007) is not suitable for a continuous measurement scale. Instead, we used the method described in Graham et al. (2013) to compare evaluation consistency. Specifically, using system-level data from the WMT16 Metrics Task, we computed the average difference between the scores assigned to the same MT output by different judges and by the same judge. This was done separately for the segments, with the average score higher than or equal to 50 and with the average score lower than 50 (thus corresponding to high- and low-quality partitions, respectively). We also computed the average difference for the h"
J19-3004,W10-1703,0,0.0667131,"Missing"
J19-3004,P05-1022,0,0.0580359,"word forms – DP-HWCM(c) grammatical categories – DP-HWCM(r) grammatical relations • DP-Ol(*) Average lexical overlap between items according to their tree level • DP-Oc(*) Average lexical overlap between terminal nodes according to their grammatical category • DP-Or(*) Average lexical overlap between items according to their grammatical relationship CP-∗. CP metrics (Gim´enez and M`arquez 2010a, 2010b) analyze similarities between constituent parse trees associated with MT outputs and reference translations. Constituent trees are obtained using the Charniak-Johnson’s Max-Ent reranking parser (Charniak and Johnson 2005). The following measures are defined: • CP-Op(*) Average overlap between words belonging to the same part of speech. • CP-Oc(*) Average overlap between words belonging to constituents of the same type • CP-STMd This measure corresponds to the Syntactic Tree Matching defined by Liu and Gildea (2005), except that overlap is used instead of precision. Subtrees up to different d depths (d ∈ 4, 5, 6) are considered. SR∗. SR metrics (Gim´enez and M`arquez 2010a, 2010b) analyze similarities between MT outputs and reference translations by comparing the semantic roles (SRs) (i.e., arguments and adjunc"
J19-3004,2003.mtsummit-papers.9,0,0.12486,". Significant work has been dedicated to developing more advanced metrics, primarily by integrating different sources of information (synonyms, paraphrases, and syntactic and semantic analysis) and using learning techniques to appropriately combine them into a single score. The performance of evaluation metrics is typically assessed in terms of systemand sentence-level correlation with human judgments, mostly for the task of ranking alternative MT system translations for the same source segment. Existing work on metaevaluation has extensively discussed the limitations of n-gram–based metrics (Coughlin 2003; Culy and Riehemann 2003; Koehn and Monz 2006), whereas the studies examining the contributions of more advanced strategies, for example, the integration of linguistic information (Amigo´ et al. 2009), are more rare. Influential evaluation campaigns such as the WMT Metrics Task receive new metric submissions every year with many recent metrics reported to outperform standard metrics like BLEU (Mach´acek and Bojar 2014; Stanojevi´c et al. 2015; Bojar et al. 2016c; Bojar, Graham, and Kamran 2017). However, very little insight has been provided regarding where existing metrics succeed and where"
J19-3004,2003.mtsummit-papers.10,0,0.187583,"een dedicated to developing more advanced metrics, primarily by integrating different sources of information (synonyms, paraphrases, and syntactic and semantic analysis) and using learning techniques to appropriately combine them into a single score. The performance of evaluation metrics is typically assessed in terms of systemand sentence-level correlation with human judgments, mostly for the task of ranking alternative MT system translations for the same source segment. Existing work on metaevaluation has extensively discussed the limitations of n-gram–based metrics (Coughlin 2003; Culy and Riehemann 2003; Koehn and Monz 2006), whereas the studies examining the contributions of more advanced strategies, for example, the integration of linguistic information (Amigo´ et al. 2009), are more rare. Influential evaluation campaigns such as the WMT Metrics Task receive new metric submissions every year with many recent metrics reported to outperform standard metrics like BLEU (Mach´acek and Bojar 2014; Stanojevi´c et al. 2015; Bojar et al. 2016c; Bojar, Graham, and Kamran 2017). However, very little insight has been provided regarding where existing metrics succeed and where they fail, and why. Furth"
J19-3004,2010.amta-papers.20,0,0.0724535,"Missing"
J19-3004,W14-3348,0,0.0292637,"MT output and the reference translation. These are: BLEU. (Bilingual Evaluation Understudy) (Papineni et al. 2002). Measures the similarity between MT and the reference translation based on the number of matching word n-grams. Specifically, BLEU score is a product between n-gram precision and a brevity penalty that down-scales the score for the MT outputs that are shorter in length than the 519 Computational Linguistics Volume 45, Number 3 reference translation. In all the experiments with this metric, we use a smoothed version of BLEU as described by Lin and Och (2004b) with N = 4. Meteor. (Denkowski and Lavie 2014). Meteor aligns MT output to the reference translation using stems, synonyms, and paraphrases, besides exact word matching, and computes candidate-reference similarity based on the proportion of aligned words in the candidate and in the reference. Different weights are assigned to the word matches, depending on the type of lexical similarity, and to function and content words. Additionally, Meteor integrates a fragmentation penalty that penalizes the differences in word order. It is based on the number of chunks (sequential word matches) in candidatereference alignment. The final Meteor score"
J19-3004,L16-1437,1,0.896557,"Missing"
J19-3004,W15-3046,1,0.860011,"s. Throughout this article we use ChrF3 with β = 3, as suggested by Popovic (2015). Linguistic Representations. To overcome the limitations of the metrics based on lexical similarity, another family of evaluation metrics explores the use of different linguistic representations (morphological, syntactic, semantic, and discourse) for comparing the MT output against the reference translation. The motivation behind these metrics is, on the one hand, to abstract away from surface word forms and, on the other hand, to try to better assess the grammaticality of the MT output. UPF-Cobalt. UPF-Cobalt (Fomicheva et al. 2015; Fomicheva and Bel 2016) is an alignment-based metric that incorporates a syntactically informed context penalty to penalize the matches of lexically similar words that play different roles in the candidate and reference sentences. The sentence-level score combines the information on lexical similarity with the average context penalty. Word similarity is detected in various ways, including cosine similarity over distributed word representations. SP-∗. SP metrics (Gim´enez and M`arquez 2010a, 2010b) measure the similarities at the level of parts of speech, word lemmas, and base phrase chunks."
J19-3004,W16-2339,1,0.838167,"al. 2015) is a syntax-based metric that parses the reference translation with a standard parser and trains a new parser on the tree of the reference translation. This new parser is then used for scoring the MT output. DPMF uses an Fscore of unigrams in combination with the syntactic score. DPMF performs quite poorly as an individual metric. To boost performance DPMFComb (Yu et al. 2015) combines DPMF and the lexical, syntactic, and semantic metrics from the Asiya evaluation toolkit (Gim´enez and M`arquez 2010a) in a learning framework. Cobalt-F-comp and Metrics-F. Cobalt-F-comb and Metrics-F (Fomicheva et al. 2016) combine features extracted from UPF-Cobalt with reference-free features that capture translation fluency. Cobalt-F-comb combines various components of UPF-Cobalt with a series of fine-grained features intended to capture the number and scale of disfluent fragments contained in the MT outputs. Metrics-F is a combination of three evaluation metrics, BLEU, Meteor and UPF-Cobalt, with the fluency-oriented features. UoW-ReVal. UoW-ReVal (Gupta, Orasan, and van Genabith 2015) uses a dependencytree Long Short-Term Memory (LSTM) network to represent both the MT output and the reference with a dense v"
J19-3004,H93-1040,0,0.698837,"Missing"
J19-3004,W13-2305,0,0.388575,"e have seen the impact of the distribution of the scores in the data set on the overall correlation results. We return to this issue in Section 7. 5.3 Quality Levels and Evaluation Consistency Besides the inherent limitations of the evaluation metrics, a possible reason for lower correlation for low-quality MT outputs is a lack of consistency in manual evaluation. Manual evaluation is typically treated as the gold standard for assessing the performance of automatic evaluation metrics. However, MT quality assessment is known to be a complex task with low levels of agreement between annotators (Graham et al. 2013). One could hypothesize that evaluating poor quality translations is more demanding and harder for human annotators. They contain a higher number of errors of different types whose impact on quality can be difficult to determine (Denkowski and Lavie 2010). We test this hypothesis using the WMT16 DA data set (Section 4.1). The annotations in this data set were collected using Amazon Mechanical Turk, which often raises questions about the reliability of the data. However, as described in Graham, Mathur, and Baldwin (2015), a rigorous quality control was conducted in order to filter out 543 Compu"
J19-3004,N15-1124,0,0.055133,"Missing"
J19-3004,D15-1124,0,0.0555188,"Missing"
J19-3004,W17-4763,0,0.0516244,"Missing"
J19-3004,W06-3114,0,0.0760251,"developing more advanced metrics, primarily by integrating different sources of information (synonyms, paraphrases, and syntactic and semantic analysis) and using learning techniques to appropriately combine them into a single score. The performance of evaluation metrics is typically assessed in terms of systemand sentence-level correlation with human judgments, mostly for the task of ranking alternative MT system translations for the same source segment. Existing work on metaevaluation has extensively discussed the limitations of n-gram–based metrics (Coughlin 2003; Culy and Riehemann 2003; Koehn and Monz 2006), whereas the studies examining the contributions of more advanced strategies, for example, the integration of linguistic information (Amigo´ et al. 2009), are more rare. Influential evaluation campaigns such as the WMT Metrics Task receive new metric submissions every year with many recent metrics reported to outperform standard metrics like BLEU (Mach´acek and Bojar 2014; Stanojevi´c et al. 2015; Bojar et al. 2016c; Bojar, Graham, and Kamran 2017). However, very little insight has been provided regarding where existing metrics succeed and where they fail, and why. Furthermore, the performanc"
J19-3004,P04-1077,0,0.691612,"constituents, to rankings of up to 5 translations from different MT systems, to a 1–100 score per sentence according to its fluency or adequacy. Different types of correlation with human judgments are computed (Pearson r, Kendal τ, etc.), depending on the time of judgment and evaluation level (corpus or segment). 517 Computational Linguistics Volume 45, Number 3 For a recent summary over the various years of the meta-evaluation campaigns, we refer the reader to Bojar et al. (2016b). Initially, meta-evaluation focused on system-level analysis (Doddington 2002; Melamed, Green, and Turian 2003; Lin and Och 2004a). In this scenario, a single measurement is provided for a set of sentences generated by an MT system. For manual evaluation, this is usually an average of sentence-level scores, whereas for automatic evaluation system-level score is computed differently by different metrics. The correlation is then computed over such average measurements collected for multiple MT systems. System-level evaluation is useful for comparing the performance of different MT systems and is generally an easy task for MT evaluation metrics. In fact, according to the meta-evaluation shared tasks, such as the Metrics T"
J19-3004,C04-1072,0,0.634891,"constituents, to rankings of up to 5 translations from different MT systems, to a 1–100 score per sentence according to its fluency or adequacy. Different types of correlation with human judgments are computed (Pearson r, Kendal τ, etc.), depending on the time of judgment and evaluation level (corpus or segment). 517 Computational Linguistics Volume 45, Number 3 For a recent summary over the various years of the meta-evaluation campaigns, we refer the reader to Bojar et al. (2016b). Initially, meta-evaluation focused on system-level analysis (Doddington 2002; Melamed, Green, and Turian 2003; Lin and Och 2004a). In this scenario, a single measurement is provided for a set of sentences generated by an MT system. For manual evaluation, this is usually an average of sentence-level scores, whereas for automatic evaluation system-level score is computed differently by different metrics. The correlation is then computed over such average measurements collected for multiple MT systems. System-level evaluation is useful for comparing the performance of different MT systems and is generally an easy task for MT evaluation metrics. In fact, according to the meta-evaluation shared tasks, such as the Metrics T"
J19-3004,W05-0904,0,0.317111,"apers about the metrics themselves rarely attempt to provide a more detailed account of their performance. The first aspect of meta-evaluation discussed in this paper is how the level of translation quality affects the performance of evaluation metrics. The difficulties faced by the metrics change, depending on the quality of MT output. High-quality translation presents the problem of acceptable variation between the MT output and human reference (Gim´enez and M`arquez 2010b). Low-quality translation, on the other hand, requires an ability to assess the impact of different types of MT errors (Liu and Gildea 2005). However, hardly any rigorous meta-evaluation analysis has been performed that would indicate which problem is more damaging for the overall metrics performance. Besides very few exceptions, the analysis is limited to computing the correlation with human judgments. One notable exception is the work by Amigo´ et al. (2009). Following substantial research dedicated to the use of linguistic information in automatic MT evaluation, Amigo´ et al. (2009) analyze the benefits of introducing linguistic features into evaluation metrics. They introduce various meta-evaluation criteria to provide a bette"
J19-3004,W14-3336,0,0.0571849,"Missing"
J19-3004,N03-2021,0,0.100102,"Missing"
J19-3004,P10-2041,0,0.0328818,"Missing"
J19-3004,niessen-etal-2000-evaluation,0,0.288233,"candidatereference alignment. The final Meteor score is a parametrized combination of F-measure and fragmentation penalty. MPEDA. (Zhang et al. 2016). MPEDA is based on Meteor but uses a domain-specific paraphrase database instead of a general one to reduce noisy paraphrase matches. To extract domain-specific paraphrases, Zhang et al. (2016) first filter the large scale general monolingual corpus into a domain-specific sub-corpus using the M-L approach (Moore and Lewis 2010), and then exploit the Markov Network model to extract paraphrase tables from that sub-corpus. -WER. (Word Error Rate) (Nießen et al. 2000). WER is based on the edit distance defined as the minimum number of word substitutions, deletions, and insertions that need to be performed to convert MT output into the reference translation.1 -PER. (Position-independent Word Error Rate) (Tillmann et al. 1997). WER may be considered excessively strict for automatic MT evaluation as it does not allow any differences in word order. PER addressed this limitation by comparing MT and reference words without taking the word order into account. -TER. (Translation Edit Rate) (Snover et al. 2006). This metric is also based on edit distance. However,"
J19-3004,P09-1034,0,0.0799295,"Missing"
J19-3004,P02-1040,0,0.117616,"ifferent metrics is maintained even if the gold standard scores are based on different criteria. 1. Introduction The use of automatic evaluation is a common practice in the field of Machine Translation (MT). It allows for cost-effective quality assessment, making it possible to compare different approaches to MT, optimize parameters of statistical MT systems, and select models in neural MT systems. The most common approach to evaluation is based on the assumption that the closer the MT output is to a human reference translation, the higher its quality. For example, the well-known metric BLEU (Papineni et al. 2002) follows a simple strategy of counting the proportion of word n-grams in the MT output that are also found in one or more references. Submission received: 5 October 2018; revised version received: 19 March 2019; accepted for publication: 12 June 2019. https://doi.org/10.1162/COLI a 00356 © 2019 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 45, Number 3 BLEU has been severely criticized for several of its limitations, such as its poor performance at"
J19-3004,W15-3049,0,0.0211908,"ER, and -TERp-A to make the results more easily comparable with the rest of the metrics. 520 Fomicheva and Specia Taking MT Evaluation Metrics to Extremes considering stemming and discontinuous matchings (skip bigrams). We used five different variants from Lin and Och (2004a) implemented in Asiya toolkit:2 • ROUGE-n: for several N-gram lengths N ∈ [1, 4] • ROUGE-L: longest common subsequence • ROUGE-S: skip bigrams with no max-gap-length • ROUGE-SU: skip bigrams with no max-gap-length, including unigrams • ROUGE-W: weighted longest common subsequence with weighting factor w = 1.2 ChrF. ChrFβ (Popovic 2015, 2016) is a recently proposed evaluation metric that calculates the F-score of character n-grams of maximal length 6. The β parameter gives β times weight to recall. Using characters instead of words helps ameliorate the sparcity of word n-gram matches and better handle morphological differences. Throughout this article we use ChrF3 with β = 3, as suggested by Popovic (2015). Linguistic Representations. To overcome the limitations of the metrics based on lexical similarity, another family of evaluation metrics explores the use of different linguistic representations (morphological, syntactic,"
J19-3004,W16-2341,0,0.05193,"Missing"
J19-3004,J11-4002,0,0.0696361,"Missing"
J19-3004,W16-2323,0,0.0676027,"Missing"
J19-3004,P16-1159,0,0.02853,"in this work achieve a higher correlation with human assessments (and therefore, are more reliable) when evaluating the outputs of neural MT systems. In order to understand the reasons for that, we compared the number of different types of errors in the translations generated by statistical MT as opposed to the ones produced by neural MT. Neural MT contains more adequacy errors, which are more easily detected by evaluation metrics than the ones affecting translation fluency. This outcome encourages further work on using evaluation metrics for direct optimization of NMT model hyperparameters (Shen et al. 2016). Finally, we investigated to what extent the results obtained using different data sets vary in terms of how well metrics do. The performance of evaluation methods was tested on six data sets with different types of manual quality assessments. The rankings of the metrics in terms of their correlation with human judgments for the different data sets were compared. Testing the metrics using adequacy judgments generated using either discrete or continuous scale, as well as fluency judgments, produced very similar results, whereas using the judgments based on a post-editing effort criterion gener"
J19-3004,2006.amta-papers.25,0,0.591942,"tables from that sub-corpus. -WER. (Word Error Rate) (Nießen et al. 2000). WER is based on the edit distance defined as the minimum number of word substitutions, deletions, and insertions that need to be performed to convert MT output into the reference translation.1 -PER. (Position-independent Word Error Rate) (Tillmann et al. 1997). WER may be considered excessively strict for automatic MT evaluation as it does not allow any differences in word order. PER addressed this limitation by comparing MT and reference words without taking the word order into account. -TER. (Translation Edit Rate) (Snover et al. 2006). This metric is also based on edit distance. However, in contrast to WER and PER, in TER possible edits include shifts of words and word sequences. -TERp-A. (Snover et al. 2009). This metric enriches TER with stemming, synonyms, lookup, and paraphrase support. The metric is optimized for the adequacy criterion. NIST. (Doddington 2002). NIST differs from BLEU in two aspects. First, to handle the low co-occurrences for larger values of N, an arithmetic mean is used instead of a geometric mean when combining the precisions of n-gram matches. Second, the ngrams are weighted depending on their fre"
J19-3004,2011.eamt-1.12,1,0.845238,"Missing"
J19-3004,2011.mtsummit-papers.58,1,0.751964,"cted to use the subcategories whenever possible and to resort to the more general category level only in case of doubt. We note that very often annotators backed off to the most general category in this particular data set. 4.6 GALE Arabic–English Data Set Three Arabic newswire data sets produced as part of the DARPA GALE project are used: MT08, GALE09, and GALE10, containing 813, 683, and 1,089 sentences, respectively. Each data set was translated into English by two in-domain phrase-based SMT systems, system 1 and system 2, and annotated for adequacy in previous work for quality estimation (Specia et al. 2011). Translation adequacy annotations were provided by two Arabic–English professional translators, who judged the translations along with the source sentences. Each translation was annotated once (for each translation, one translator was randomly selected). A 4-point scale was used to answer the question To which degree does the translation convey the meaning of the original text?”: • 4 = Highly adequate • 3 = Fairly adequate 528 Fomicheva and Specia Taking MT Evaluation Metrics to Extremes Figure 5 Distribution of adequacy scores in the GALE data set. • 2 = Poorly adequate • 1 = Completely inad"
J19-3004,2009.eamt-1.5,1,0.882422,"and UPF-Cobalt, with the fluency-oriented features. UoW-ReVal. UoW-ReVal (Gupta, Orasan, and van Genabith 2015) uses a dependencytree Long Short-Term Memory (LSTM) network to represent both the MT output and the reference with a dense vector. The segment level scores are obtained from a neural network that takes into account both the distance and the Hadamard product of the two representations. Training is performed on WMT ranking judgments converted to similarity scores. QualityEstimation. Different from reference-based evaluation metrics, Quality Estimation (QE) metrics (Blatz et al. 2004; Specia et al. 2009) aim to predict the quality of a machine translated segment solely from information about the segment itself and its corresponding source segment (and optionally) information about the MT system that produced the translation. The problem is framed as a supervised machine learning task, where, given source-MT pairs annotated with a quality label, a number of features can be extracted and used to train a machine learning algorithm. Sentence-level QE has been covered as a shared task in the last six editions of the Conference on Machine Translation (WMT) (see Bojar et al. (2017) for the state-of-"
J19-3004,W14-3354,0,0.0516553,"Missing"
J19-3004,W15-3031,0,0.0532781,"Missing"
J19-3004,W05-0635,0,0.048193,"g to the same part of speech. • CP-Oc(*) Average overlap between words belonging to constituents of the same type • CP-STMd This measure corresponds to the Syntactic Tree Matching defined by Liu and Gildea (2005), except that overlap is used instead of precision. Subtrees up to different d depths (d ∈ 4, 5, 6) are considered. SR∗. SR metrics (Gim´enez and M`arquez 2010a, 2010b) analyze similarities between MT outputs and reference translations by comparing the semantic roles (SRs) (i.e., arguments and adjuncts) that occur in them. Sentences are automatically annotated using the SwiRL package (Surdeanu and Turmo 2005). The following measures are defined: • SR-Or(*): Average lexical overlap over semantic roles • SR-Mr(*): Average lexical matching over semantic roles • SR-Or: Average role overlap, i.e., overlap between semantic roles independently of their lexical realization Feature Combination. The most recent improvements in the performance of evaluation metrics is related to the use of machine learning techniques in order to combine a wide variety of features describing different aspects of MT quality. To be able to train on WMT ranking data and produce absolute scores at test time, most of the metrics d"
J19-3004,E17-1100,0,0.0424059,"Missing"
J19-3004,W16-2327,0,0.0217355,"simple text similarity are available for the MTSummit17 English–Latvian data set. WMT16 Direct Assessment Data Set. In the first experiment, we use the data from the WMT16 Direct Assessment Data Set (see Section 4.1) to compare the overall correlation between metric scores and human judgments for different MT paradigms. We selected all available data for three MT systems, corresponding to three different approaches: a phrase-based statistical MT system (PBMT), a syntax-based statistical MT (SYNTAX), and a neural MT system (NMT)—all these are University of Edinburgh’s systems, as described in Williams et al. (2016) and Sennrich, Haddow, and Birch (2016). The number of sentences with available direct assessment judgments for these systems is as follows, respectively: 231, 238, and 342 sentences. As shown in Table 8, all the metrics achieve consistently higher correlation on NMT outputs, although the difference is not significant for all the metrics due to the small size of the data set.22 According to the results from the previous section, our initial hypothesis was that the difference in correlation can be attributed to the fact that the quality of translations produced by NMT is generally higher (this"
J19-3004,W15-3053,0,0.0205257,"WMT ranking data and produce absolute scores at test time, most of the metrics described here 522 Fomicheva and Specia Taking MT Evaluation Metrics to Extremes (unless stated otherwise) use the learn-to-rank approach (Burges et al. 2005) for tuning the feature weights. BEER. BEER (Stanojevi´c and Sima’an 2014) is a trained evaluation metric with a linear model that combines lexical similarity features (precision, recall, and F-score over word and character n-gram matches) and features based on Permutation Trees (Zhang and Gildea 2007) to account for differences in word order. DPMFComb. DPMF (Yu et al. 2015) is a syntax-based metric that parses the reference translation with a standard parser and trains a new parser on the tree of the reference translation. This new parser is then used for scoring the MT output. DPMF uses an Fscore of unigrams in combination with the syntactic score. DPMF performs quite poorly as an individual metric. To boost performance DPMFComb (Yu et al. 2015) combines DPMF and the lexical, syntactic, and semantic metrics from the Asiya evaluation toolkit (Gim´enez and M`arquez 2010a) in a learning framework. Cobalt-F-comp and Metrics-F. Cobalt-F-comb and Metrics-F (Fomicheva"
J19-3004,W07-0404,0,0.0949609,"Missing"
J19-3004,W16-2343,0,0.0129378,"and paraphrases, besides exact word matching, and computes candidate-reference similarity based on the proportion of aligned words in the candidate and in the reference. Different weights are assigned to the word matches, depending on the type of lexical similarity, and to function and content words. Additionally, Meteor integrates a fragmentation penalty that penalizes the differences in word order. It is based on the number of chunks (sequential word matches) in candidatereference alignment. The final Meteor score is a parametrized combination of F-measure and fragmentation penalty. MPEDA. (Zhang et al. 2016). MPEDA is based on Meteor but uses a domain-specific paraphrase database instead of a general one to reduce noisy paraphrase matches. To extract domain-specific paraphrases, Zhang et al. (2016) first filter the large scale general monolingual corpus into a domain-specific sub-corpus using the M-L approach (Moore and Lewis 2010), and then exploit the Markov Network model to extract paraphrase tables from that sub-corpus. -WER. (Word Error Rate) (Nießen et al. 2000). WER is based on the edit distance defined as the minimum number of word substitutions, deletions, and insertions that need to be"
J19-3004,W16-2301,1,\N,Missing
J19-3004,W17-4717,1,\N,Missing
K16-1021,P13-2097,1,0.852049,"formation on the whole predictive distribution, unlike usual point estimatebased metrics. By assessing models using NLPD we can make better informed decisions about which model to employ for different settings. Furthermore, we showed how information in the predictive distribution can be used in asymmetric loss scenarios and how the proposed models can be beneficial in these settings. Uncertainty estimates can be useful in many other settings beyond the ones explored in this work. Active Learning can benefit from variance information in their query methods and it has shown to be useful for QE (Beck et al., 2013). Exploratory analysis is another avenue for future work, where error bars can provide further insights about the task, as shown in recent work (Nguyen and O’Connor, 2015). This kind of analysis can be useful for tracking post-editor behaviour and assessing cost estimates for translation projects, for instance. Our main goal in this paper was to raise awareness about how different modelling aspects should be taken into account when building QE models. Decision making can be risky using simple point estimates and we believe that uncertainty information can be beneficial in such scenarios by pro"
K16-1021,P13-1004,1,0.956005,"ng decision. For instance, in order to ensure good user experience for the human translator and maximise translation productivity, an MT segment could be forwarded for post-editing only if a QE model assigns a high quality score with low uncertainty (high confidence). Such a decision process is not possible with point estimates only. Good uncertainty estimates can be acquired from well-calibrated probability distributions over the quality predictions. In QE, arguably the most successful probabilistic models are Gaussian Processes (GPs) since they considered the state-ofthe-art for regression (Cohn and Specia, 2013; Hensman et al., 2013), especially in the low-data regimes typical for this task. We focus our analysis in this paper on GPs since other common models used in QE can only provide point estimates as predictions. Another reason why we focus on probabilistic models is because this lets us employ the ideas proposed by Qui˜nonero-Candela et al. (2006), which defined new evaluation metrics that take into account probability distributions over predictions. The remaining of this paper is organised as follows: Machine Translation Quality Estimation is a notoriously difficult task, which lessens its us"
K16-1021,D14-1190,1,0.928294,"Missing"
K16-1021,P15-1174,0,0.0195838,"r I allow more complex mappings to be learned but raise the risk of overfitting. Warped GPs provide an easy and elegant way to model response variables with non-Gaussian behaviour within the GP framework. In our experiments we explore models employing warping functions with up to 3 terms, which is the value recommended by Snelson et al. (2004). We also report results using the f (y) = log(y) warping function. 3 data. We also report two point estimate metrics on test data: Mean Absolute Error (MAE), the most commonly used evaluation metric in QE, and Pearson’s r, which has recently proposed by Graham (2015) as a more robust alternative. 3.1 Our experiments comprise datasets containing three different language pairs, where the label to predict is post-editing time: English-Spanish (en-es) This dataset was used in the WMT14 QE shared task (Bojar et al., 2014). It contains 858 sentences translated by one MT system and post-edited by a professional translator. Intrinsic Uncertainty Evaluation French-English (fr-en) Described in (Specia, 2011), this dataset contains 2, 525 sentences translated by one MT system and post-edited by a professional translator. Given a set of different probabilistic QE mod"
K16-1021,W14-3338,1,0.891871,"redictions for this task, they lack a probabilistic interpretation, which makes it hard to extract uncertainty estimates using them. Bootstrapping approaches like bagging (Abe and Mamitsuka, 1998) can be applied, but this requires setting and optimising hyperparameters like bag size and number of bootstraps. There is also no guarantee these estimates come from a well-calibrated probabilistic distribution. Gaussian Processes (GPs) (Rasmussen and Williams, 2006) is an alternative kernel-based framework that gives competitive results for point estimates (Cohn and Specia, 2013; Shah et al., 2013; Beck et al., 2014b). Unlike SVR, they explicitly model uncertainty in the data and in the predictions. This makes GPs very applicable when p(f |X ) = p(y|X, f )p(f ) , p(y|X) where X and y are the training inputs and response variables, respectively. For regression, we assume that each yi = f (xi ) + η, where η ∼ N (0, σn2 ) is added white noise. Having a Gaussian likelihood results in a closed form solution for the posterior. Training a GP involves the optimisation of model hyperparameters, which is done by maximising the marginal likelihood p(y|X) via gradient ascent. Predictive posteriors for unseen x∗ are"
K16-1021,P15-2030,1,0.837702,"to raise awareness about how different modelling aspects should be taken into account when building QE models. Decision making can be risky using simple point estimates and we believe that uncertainty information can be beneficial in such scenarios by providing more informed solutions. These ideas are not restricted to QE and we hope to see similar studies in other natural language applications in the future. Related Work Quality Estimation is generally framed as text regression task, similarly to many other applications such as movie revenue forecasting based on reviews (Joshi et al., 2010; Bitvai and Cohn, 2015) and detection of emotion strength in news headlines (Strapparava and Mihalcea, 2008; Beck et al., 2014a) and song lyrics (Mihalcea and Strapparava, 2012). In general, these applications are evaluated in terms of their point estimate predictions, arguably because not all of them employ probabilistic models. The NLPD is common and established metric used in the GP literature to evaluate new approaches. Examples include the original work on Warped GPs (Snelson et al., 2004), but also others like L´azaro-Gredilla (2012) and Chalupka et al. (2013). It has also been used to evaluate recent work on"
K16-1021,C04-1046,0,0.368786,"Missing"
K16-1021,N10-1038,0,0.0347264,"al in this paper was to raise awareness about how different modelling aspects should be taken into account when building QE models. Decision making can be risky using simple point estimates and we believe that uncertainty information can be beneficial in such scenarios by providing more informed solutions. These ideas are not restricted to QE and we hope to see similar studies in other natural language applications in the future. Related Work Quality Estimation is generally framed as text regression task, similarly to many other applications such as movie revenue forecasting based on reviews (Joshi et al., 2010; Bitvai and Cohn, 2015) and detection of emotion strength in news headlines (Strapparava and Mihalcea, 2008; Beck et al., 2014a) and song lyrics (Mihalcea and Strapparava, 2012). In general, these applications are evaluated in terms of their point estimate predictions, arguably because not all of them employ probabilistic models. The NLPD is common and established metric used in the GP literature to evaluate new approaches. Examples include the original work on Warped GPs (Snelson et al., 2004), but also others like L´azaro-Gredilla (2012) and Chalupka et al. (2013). It has also been used to"
K16-1021,2012.amta-wptp.2,1,0.832763,"n sentence basis for all datasets. Following common practice, we normalise the post-editing time by the length of the machine translated sentence to obtain postediting rates and use these as our response variables. Technically our approach could be used with any other numeric quality labels from the literature, including the commonly used Human Translation Error Rate (HTER) (Snover et al., 2006). Our decision to focus on post-editing time was based on the fact that time is a more complete measure of post-editing effort, capturing not only technical effort like HTER, but also cognitive effort (Koponen et al., 2012). Additionally, time is more directly applicable in real translation environments – where uncertainty estimates could be useful, as it relates directly to productivity measures. For model building, we use a standard set of 17 features from the QuEst framework (Specia et al., 2015). These features are used in the strong baseline models provided by the WMT n NLPD(ˆ y, y) = − Experimental Settings 1X log p(ˆ yi = yi |xi ). n i=1 where y ˆ is a set of test predictions, y is the set of true labels and n is the test set size. This metric has since been largely adopted by the ML community when evalua"
K16-1021,D12-1054,0,0.0317415,"g simple point estimates and we believe that uncertainty information can be beneficial in such scenarios by providing more informed solutions. These ideas are not restricted to QE and we hope to see similar studies in other natural language applications in the future. Related Work Quality Estimation is generally framed as text regression task, similarly to many other applications such as movie revenue forecasting based on reviews (Joshi et al., 2010; Bitvai and Cohn, 2015) and detection of emotion strength in news headlines (Strapparava and Mihalcea, 2008; Beck et al., 2014a) and song lyrics (Mihalcea and Strapparava, 2012). In general, these applications are evaluated in terms of their point estimate predictions, arguably because not all of them employ probabilistic models. The NLPD is common and established metric used in the GP literature to evaluate new approaches. Examples include the original work on Warped GPs (Snelson et al., 2004), but also others like L´azaro-Gredilla (2012) and Chalupka et al. (2013). It has also been used to evaluate recent work on uncertainty propagation methods for neural networks (Hern´andez-Lobato and Adams, 2015). Asymmetric loss functions are common in the econometrics literatu"
K16-1021,D15-1182,0,0.0681594,"Missing"
K16-1021,W12-3102,1,0.908405,"Missing"
K16-1021,2013.mtsummit-papers.21,1,0.907208,"erate competitive predictions for this task, they lack a probabilistic interpretation, which makes it hard to extract uncertainty estimates using them. Bootstrapping approaches like bagging (Abe and Mamitsuka, 1998) can be applied, but this requires setting and optimising hyperparameters like bag size and number of bootstraps. There is also no guarantee these estimates come from a well-calibrated probabilistic distribution. Gaussian Processes (GPs) (Rasmussen and Williams, 2006) is an alternative kernel-based framework that gives competitive results for point estimates (Cohn and Specia, 2013; Shah et al., 2013; Beck et al., 2014b). Unlike SVR, they explicitly model uncertainty in the data and in the predictions. This makes GPs very applicable when p(f |X ) = p(y|X, f )p(f ) , p(y|X) where X and y are the training inputs and response variables, respectively. For regression, we assume that each yi = f (xi ) + η, where η ∼ N (0, σn2 ) is added white noise. Having a Gaussian likelihood results in a closed form solution for the posterior. Training a GP involves the optimisation of model hyperparameters, which is done by maximising the marginal likelihood p(y|X) via gradient ascent. Predictive posteriors"
K16-1021,2006.amta-papers.25,0,0.0632672,". It was translated by one MT system for consistency we use a subset of 2, 828 instances post-edited by a single professional translator. As part of the process of creating these datasets, post-editing time was logged on an sentence basis for all datasets. Following common practice, we normalise the post-editing time by the length of the machine translated sentence to obtain postediting rates and use these as our response variables. Technically our approach could be used with any other numeric quality labels from the literature, including the commonly used Human Translation Error Rate (HTER) (Snover et al., 2006). Our decision to focus on post-editing time was based on the fact that time is a more complete measure of post-editing effort, capturing not only technical effort like HTER, but also cognitive effort (Koponen et al., 2012). Additionally, time is more directly applicable in real translation environments – where uncertainty estimates could be useful, as it relates directly to productivity measures. For model building, we use a standard set of 17 features from the QuEst framework (Specia et al., 2015). These features are used in the strong baseline models provided by the WMT n NLPD(ˆ y, y) = − E"
K16-1021,2009.eamt-1.5,1,0.909327,"Missing"
K16-1021,P15-4020,1,0.846462,"ty labels from the literature, including the commonly used Human Translation Error Rate (HTER) (Snover et al., 2006). Our decision to focus on post-editing time was based on the fact that time is a more complete measure of post-editing effort, capturing not only technical effort like HTER, but also cognitive effort (Koponen et al., 2012). Additionally, time is more directly applicable in real translation environments – where uncertainty estimates could be useful, as it relates directly to productivity measures. For model building, we use a standard set of 17 features from the QuEst framework (Specia et al., 2015). These features are used in the strong baseline models provided by the WMT n NLPD(ˆ y, y) = − Experimental Settings 1X log p(ˆ yi = yi |xi ). n i=1 where y ˆ is a set of test predictions, y is the set of true labels and n is the test set size. This metric has since been largely adopted by the ML community when evaluating GPs and other probabilistic models for regression (see Section 5 for some examples). As with other error metrics, lower values are better. Intuitively, if two models produce equally incorrect predictions but they have different uncertainty estimates, NLPD will penalise the ov"
K16-1021,2011.eamt-1.12,1,0.847639,"point estimate metrics on test data: Mean Absolute Error (MAE), the most commonly used evaluation metric in QE, and Pearson’s r, which has recently proposed by Graham (2015) as a more robust alternative. 3.1 Our experiments comprise datasets containing three different language pairs, where the label to predict is post-editing time: English-Spanish (en-es) This dataset was used in the WMT14 QE shared task (Bojar et al., 2014). It contains 858 sentences translated by one MT system and post-edited by a professional translator. Intrinsic Uncertainty Evaluation French-English (fr-en) Described in (Specia, 2011), this dataset contains 2, 525 sentences translated by one MT system and post-edited by a professional translator. Given a set of different probabilistic QE models, we are interested in evaluating the performance of these models, while also taking their uncertainty into account, particularly to distinguish among models with seemingly same or similar performance. A straightforward way to measure the performance of a probabilistic model is to inspect its negative (log) marginal likelihood. This measure, however, does not capture if a model overfit the training data. We can have a better generali"
L16-1356,2011.mtsummit-papers.17,1,0.779357,"s particularly true for morphologically rich languages. On this basis, our motivation is to make use of linguistic information to determine the sentence segmentation. Therefore, our phrases are extracted from the shallow syntactic structure of the sentence (Constant et al., 2011), the so-called chunks, based on TreeTagger (Schmid, 1994). In future work, we could use dependency structures to assess whether the labelling of a phrase is dependent on or influences other phrases to re-define error boundaries. These error dependencies are a well-known phenomenon in MT, as it has been identified in (Blain et al., 2011). S3: Decoder phrases This approach, described in the research on phrase-level QE of (Logacheva and Specia, 2015), considers phrases in the SMT sense: sequences of words which often occur together and can be translated as one instance. The idea is to reuse the phrase segmentation produced by the decoder, with two hypotheses: (i) MT errors are usually context-dependent, so by dealing with the whole phrase we provide the local context related to the choice of a given word in phrase-based SMT and can more easily detect a single error which spans over two or more words, (ii) detecting errors at th"
L16-1356,W15-3001,1,0.841081,"n was to mimic as much as possible annotators’ behaviour by producing a monolingual alignment between the raw machine translation and its post-edited version. We thus extract the phrases based on the edit path between these two sentences. Concretely, we first label as “BAD” every word in the MT output which has been marked as edited (inserted, substituted or moved) by the T ERCOM tool (Snover et al., 2006). All remaining words are labelled as “OK”. This is the standard procedure used currently to automatically generate labelled data from MT output and its post-edited version for wordlevel QE (Bojar et al., 2015). We add to this process then defining the final “OK” and “BAD” phrases as sequences of adjacent “OK” and “BAD” labels. Different from the two strategies described next, this strategy is guided by the word-level labels, rather than the types of errors, and as a side effect it produces particularly long phrases, especially for the “OK” sequences. Thus, even though a phrase may be correctly tagged as “BAD”, we lose the information on actual error boundaries in cases of multiple translation errors which happen to be consecutive, but are independent from each other. 2240 S2: Linguistically motivat"
L16-1356,P15-2026,0,0.012135,"incorrect positions. However, contrary to the word-level QE, for which the segmentation boundaries are self-defined and clear, QE at phrase-level implies that one needs to delimit sub-segments within the segment. This is not a trivial task as several alternatives can be used to define a phrase, but in our case the segmentation needs to be connected to the errors in the translation. QE at the phrase-level can reduce human post-editing effort by pinpointing the erroneous sequences that need to be fixed by the post-editor. It can also support automatic post-editing systems (McKeown et al., 2012; Chatterjee et al., 2015), by limiting the post-editing to sequences predicted as incorrect, and thus preventing risky edits that can make the translation even worse. The interest for automatic phrase-level segmentation (and labelling) is however not limited to QE. Given a human post-edition and its original machine translation, the combination of a monolingual alignment technique with an appropriate phrase segmentation would allow a more detailed analysis of the translation errors. In Section 2. we discuss three possible ways of automatically segmenting a translation into phrases and labelling them with binary labels"
L16-1356,P07-2045,0,0.00565319,"ular, on the training data of the SMT system used for decoding. If the data used for the MT system training and the sentences we are going to decode belong to different domains, there will be little intersection between the MT system’s phrase table and the decoded sentences. As a result, the vast majority of identified phrases will be one-word, which will reduce the phrase-level QE task to the word-level QE. For the target-source decoding strategy we used an SMT system trained on the English-French part of Europarl corpus (Koehn, 2005), built based on the Moses toolkit with standard settings (Koehn et al., 2007). Since our goldstandard sentences come from the LIG corpus, which was drawn from WMT test sets of different years (news domain), the system we used for decoding can be considered in-domain. 2.2. Phrase Labelling Our labelling strategy is based on comparing the MT sentences and their version post-edited by a human, as it is done for labelling of word-level QE training data. This is only possible for labelling datasets at “training time” or for evaluation / translation quality analysis. Another option would be to rely on humans to tag each phrase as “OK” or “BAD”, but this is costly and time co"
L16-1356,2005.mtsummit-papers.11,0,0.0130684,"ilarly to the source-target approach, it depends on the data, in particular, on the training data of the SMT system used for decoding. If the data used for the MT system training and the sentences we are going to decode belong to different domains, there will be little intersection between the MT system’s phrase table and the decoded sentences. As a result, the vast majority of identified phrases will be one-word, which will reduce the phrase-level QE task to the word-level QE. For the target-source decoding strategy we used an SMT system trained on the English-French part of Europarl corpus (Koehn, 2005), built based on the Moses toolkit with standard settings (Koehn et al., 2007). Since our goldstandard sentences come from the LIG corpus, which was drawn from WMT test sets of different years (news domain), the system we used for decoding can be considered in-domain. 2.2. Phrase Labelling Our labelling strategy is based on comparing the MT sentences and their version post-edited by a human, as it is done for labelling of word-level QE training data. This is only possible for labelling datasets at “training time” or for evaluation / translation quality analysis. Another option would be to rely"
L16-1356,2015.iwslt-papers.4,1,0.882232,"ls for some applications. However, phrase-level QE implies an intrinsic challenge: how to segment a machine translation into sequence of words (contiguous or not) that represent an error. We discuss three possible segmentation strategies to automatically extract erroneous phrases. We evaluate these strategies against annotations at phrase-level produced by humans, using a new dataset collected for this purpose. Keywords: Machine Translation, Post-Editing, Quality Estimation 1. Introduction We recently started to investigate Quality Estimation (QE) for Machine Translation (MT) at phrase-level (Logacheva and Specia, 2015) as a way to balance between word and sentence-level prediction, two well studied levels. Sentence-level QE generally aims to predict if a translation is either good enough or needs to be edited (and sometimes how much editing it needs). This is too coarse for certain tasks, for example, highlighting errors that need to be fixed. Word-level QE can help post-editors by highlighting words with errors, however, it is often hard to predict if an individual word is erroneous. Errors are generally interconnected within a segment, and it would be more beneficial for a post-editor if words belonging t"
L16-1356,2012.eamt-1.34,0,0.0608398,"Missing"
L16-1356,2006.amta-papers.25,0,0.0937755,"quently cooccur and are aligned with the same source word sequences. Therefore, we experimented with three segmentation strategies: S1: Phrases from edit distance metric Our first insight in terms of segmentation was to mimic as much as possible annotators’ behaviour by producing a monolingual alignment between the raw machine translation and its post-edited version. We thus extract the phrases based on the edit path between these two sentences. Concretely, we first label as “BAD” every word in the MT output which has been marked as edited (inserted, substituted or moved) by the T ERCOM tool (Snover et al., 2006). All remaining words are labelled as “OK”. This is the standard procedure used currently to automatically generate labelled data from MT output and its post-edited version for wordlevel QE (Bojar et al., 2015). We add to this process then defining the final “OK” and “BAD” phrases as sequences of adjacent “OK” and “BAD” labels. Different from the two strategies described next, this strategy is guided by the word-level labels, rather than the types of errors, and as a side effect it produces particularly long phrases, especially for the “OK” sequences. Thus, even though a phrase may be correctl"
L16-1356,E12-2021,0,0.0625259,"Missing"
L16-1356,W03-0419,0,0.0236995,"Missing"
L16-1491,P11-2087,0,0.181277,"us used for the Glavas generator, parsed with the Stanford Parser. Word vectors are trained using the Bag-ofWords (CBOW) architecture and 1300 vector dimensions. new resources have been used, such as aligned complex-tosimple parallel corpora (Paetzold, 2013; Paetzold and Specia, 2013; Horn et al., 2014) and word embedding models ˇ (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). 3.1. Systems We re-implemented the following SG systems for evaluation: • Devlin (Devlin and Tait, 1998): Extracts synonyms of complex words from WordNet 3.0 (Fellbaum, 1998). 3.2. Datasets and Metrics • Biran (Biran et al., 2011): Creates the Cartesian product between Wikipedia and Simple Wikipedia by simply pairing every word that appears in Wikipedia with every word in Simple Wikipedia. It then discards any pairs in which: As a gold-standard, we use the candidate substitutions in BenchLS. The evaluation metrics are: 1. At least one of the words is a stop-word, numeral or punctuation. 2. The words share the same lemma. 3. The words are not registered as synonyms or hypernyms in WordNet. • Precision: Proportion of generated substitutions that are in the gold-standard. • Yamamoto (Kajiwara et al., 2013): Given a comple"
L16-1491,W11-1603,0,0.0487713,"nd F1. Although the Precision obtained by the Belder selector is the highest, it comes at noticeable losses in Potential and Recall. 5. Substitution Ranking Substitution Ranking (SR) is the task of ranking candidates by their simplicity. The goal is to replace the complex word by its simplest candidate substitute. The most widely used SR strategy in the literature is metricbased ranking, in which candidates are ranked according to a manually crafted combination of features such as word frequency and length (Devlin and Tait, 1998; Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011; Bott and Saggion, 2011). Recently, however, more sophisticated supervised approaches have been explored, such as SVM rankers (Horn et al., 2014) and Boundary Ranking (Paetzold and Specia, 2015). 5.1. • Paetzold (Paetzold and Specia, 2015): Uses a supervised Boundary Ranking approach. It learns a ranking model from data using a binary classification setup inferred from the ranking examples. This strategy is the same one used by the Paetzold selector, but instead of learning the ranking model from training data obtained in unsupervised fashion, it learns the model from manually annotated data. 10 morphological, semant"
L16-1491,C12-1023,0,0.132126,"Missing"
L16-1491,J92-4003,0,0.0440002,"get complex word at least once in News Crawl. 3. The conditional probability of a candidate given the POS tag of the target word. To calculate this feature, we learn the probability distribution P (c|pt ), described in Equation 1, of all words in the News Crawl corpus. C(c, pt ) , p∈P C(c, p) • Belder (De Belder and Moens, 2010): Intersects the candidates generated for a target word with the words in a cluster in which the target word is included, as determined by a latent-variable language model. To replicate their approach, we learn 2, 000 word clusters using the Brown clustering algorithm (Brown et al., 1992). • Biran (Biran et al., 2011): Selects candidates using a word co-occurrence model. It first discards any candidates for which the cosine similarity between its co-occurrence vector and the co-occurrence vector of the sentence in which the target was found is smaller than a threshold value t1 . In order to avoid incoherent replacements, it then discards any candidates for which the cosine similarity between its common co-occurrence vector with the target word and the co-occurrence vector of the sentence is larger than a threshold value t2 . We train the co-occurrence model over the same corpu"
L16-1491,E99-1042,0,0.778639,"roduce BenchLS, a new dataset for the task. In the following Sections, we describe our dataset and present our experiments. 3. Substitution Generation Substitution Generation (SG) aims to produce candidate substitutions for complex words, which can be later ranked or filtered according to different criteria. This step does not take into account the ambiguity of words, i.e. it generates candidate substitutions for a word in all or any of its possible meanings. The most frequently used SG solution consists in extracting synonyms from linguistic databases, such as WordNet (Devlin and Tait, 1998; Carroll et al., 1999) or the UMLS database for medical content (Ong et al., 2007; Leroy et al., 2013). Recently, however, 3074 1 2 http://norvig.com/spell-correct.html http://www.statmt.org/wmt11/translation-task.html • Paetzold (Paetzold and Specia, 2016): Produces candidates using a context-aware word embeddings model. 10 candidates for each target word are retrieved with a model trained using the word2vec toolkit (Mikolov et al., 2013) over the same corpus used for the Glavas generator, parsed with the Stanford Parser. Word vectors are trained using the Bag-ofWords (CBOW) architecture and 1300 vector dimensions"
L16-1491,W10-1505,0,0.0123736,"systems require pre-generated candidates, in order to avoid any biases toward a given SG approach, we use candidates produced by generators altogether. 3076 Selector Lesk Aluisio Belder Biran Paetzold First Sense No Selection Pot. 0.337 0.916 0.297 0.478 0.851 0.207 0.940 Prec. 0.053 0.098 0.188 0.068 0.166 0.052 0.062 Rec. 0.075 0.398 0.057 0.185 0.284 0.036 0.438 F1 0.062 0.157 0.088 0.099 0.209 0.042 0.109 • Horn (Horn et al., 2014): Uses Support Vector Machines (Joachims, 2002) to learn a ranking model from data with several word and n-gram frequency features extracted from the Google 1T (Evert, 2010), Wikipedia and Simple Wikipedia corpora. ˇ • Glavas (Glavaˇs and Stajner, 2015): Ranks candidates according to several features, such as n-gram frequencies and word vector similarity with the target word, and then re-ranks them according to their average rankings. The word embeddings model used is the same one used by the Glavas generator, and n-gram frequencies were extracted from the Google 1T corˇ pus (Glavaˇs and Stajner, 2015). Table 3: SS benchmarking results 4.3. Results As illustrated in Table 3, only the Aluisio and Paetzold selectors have managed to obtain higher F1 scores than not"
L16-1491,P15-2011,0,0.376254,"Missing"
L16-1491,P14-2075,0,0.0677173,"is to replace the complex words in a text with simpler alternatives, without compromising its meaning or grammaticality. The LS task is often addressed as the series of steps in Figure 1, as introduced by (Devlin and Tait, 1998). Their work has inspired others to conceive new LS solutions for the aphasic (Carroll et al., 1998), dyslexic (Bott et al., 2012), illiterate (Watanabe et al., 2009), non-native English speakers (Paetzold, 2013; Paetzold and Specia, 2013), children (Kajiwara et al., 2013) and others. BenchLS: A New Dataset To create our dataset we combined two resources: the LexMTurk (Horn et al., 2014) and LSeval (De Belder and Moens, 2012) datasets. The instances in both datasets, 929 in total, contain a sentence, a target complex word, and several candidate substitutions ranked according to their simplicity. The candidates in both datasets were suggested and ranked by English speakers from the U.S. To increase its reliability, we applied the following corrections over each instance of our dataset: 1. Spelling Filtering: We discard any misspelled candidates using Norvig’s algorithm1 . We trained our spelling model over the News Crawl2 corpus. 2. Inflection Correction: We inflected all cand"
L16-1491,O13-1007,0,0.383694,"plification, Text Simplification, Evaluation Dataset 1. Introduction 2. The goal of a Lexical Simplification (LS) system is to replace the complex words in a text with simpler alternatives, without compromising its meaning or grammaticality. The LS task is often addressed as the series of steps in Figure 1, as introduced by (Devlin and Tait, 1998). Their work has inspired others to conceive new LS solutions for the aphasic (Carroll et al., 1998), dyslexic (Bott et al., 2012), illiterate (Watanabe et al., 2009), non-native English speakers (Paetzold, 2013; Paetzold and Specia, 2013), children (Kajiwara et al., 2013) and others. BenchLS: A New Dataset To create our dataset we combined two resources: the LexMTurk (Horn et al., 2014) and LSeval (De Belder and Moens, 2012) datasets. The instances in both datasets, 929 in total, contain a sentence, a target complex word, and several candidate substitutions ranked according to their simplicity. The candidates in both datasets were suggested and ranked by English speakers from the U.S. To increase its reliability, we applied the following corrections over each instance of our dataset: 1. Spelling Filtering: We discard any misspelled candidates using Norvig’s al"
L16-1491,P13-1151,0,0.0568814,"d the Stanford Parser. ˇ • Glavas (Glavaˇs and Stajner, 2015): Produces candidates using a word embeddings model. They retrieve the 10 words for which the embeddings vector has the highest cosine similarity with that of the target complex word, except for its morphological variants. Their model uses 200 vector dimensions and is trained with the GloVe toolkit (Pennington et al., 2014). We train their model over a corpus of 7 billion words which combines combines the SubIMDB corpus (Paetzold, 2015), UMBC webbase4 , News Crawl5 , SUBTLEX (Brysbaert and New, 2009), Wikipedia and Simple Wikipedia (Kauchak, 2013). As illustrated in Table 1, the Paetzold generator outperforms all others, including the supervised Horn generator, by a considerable margin in almost all metrics used, revealing the potential of context-aware embedding models for SG. In order to further highlight the importance of using context-aware as opposed to traditional embedding models, we have trained a modified version of the Glavas generator. ˇ Instead of using the model specified in (Glavaˇs and Stajner, 2015), it uses a model trained with the same settings specified for the context-aware model of the Paetzold generator: the Bag-o"
L16-1491,P03-1054,0,0.0163441,"ion metrics are: 1. At least one of the words is a stop-word, numeral or punctuation. 2. The words share the same lemma. 3. The words are not registered as synonyms or hypernyms in WordNet. • Precision: Proportion of generated substitutions that are in the gold-standard. • Yamamoto (Kajiwara et al., 2013): Given a complex word, it retrieves its definition from a dictionary, annotates it using a POS tagger, and then extracts as candidates any words that have the same POS tag as the complex word itself. This system queries the Merriam Dictionary3 , and tags definitions with the Stanford Parser (Klein and Manning, 2003). • Potential: Proportion of instances in which at least one of the candidates generated is in the gold-standard. • Recall: The proportion of gold-standard substitutions that are among the generated substitutions. • F1: Harmonic mean between Precision & Recall. 3.3. Results Generator Devlin Biran Yamamoto Horn Glavas Paetzold • Horn (Horn et al., 2014): Produces alignments for complex-to-simple parallel corpora, then extracts any hcomplex → simplei pairs of aligned words in which: 1. The complex word is not a stop-word. 2. The POS tag of both words are the same. 3. Neither word is a proper nou"
L16-1491,J03-1002,0,0.0139895,"ween Precision & Recall. 3.3. Results Generator Devlin Biran Yamamoto Horn Glavas Paetzold • Horn (Horn et al., 2014): Produces alignments for complex-to-simple parallel corpora, then extracts any hcomplex → simplei pairs of aligned words in which: 1. The complex word is not a stop-word. 2. The POS tag of both words are the same. 3. Neither word is a proper noun. Pot. 0.647 0.610 0.360 0.569 0.724 0.856 Prec. 0.133 0.130 0.032 0.235 0.142 0.180 Rec. 0.153 0.144 0.087 0.131 0.191 0.252 F1 0.143 0.136 0.047 0.168 0.163 0.210 Table 1: SG benchmarking results To produce alignments, we use GIZA++ (Och and Ney, 2003). The words are then inflected to all their morphological forms using Morph Adorner (Burns, 2013). For this system, we use the parallel Wikipedia and Simple Wikipedia corpus (Horn et al., 2014) and the Stanford Parser. ˇ • Glavas (Glavaˇs and Stajner, 2015): Produces candidates using a word embeddings model. They retrieve the 10 words for which the embeddings vector has the highest cosine similarity with that of the target complex word, except for its morphological variants. Their model uses 200 vector dimensions and is trained with the GloVe toolkit (Pennington et al., 2014). We train their m"
L16-1491,W13-4813,1,0.948707,"stems evaluated. Keywords: Lexical Simplification, Text Simplification, Evaluation Dataset 1. Introduction 2. The goal of a Lexical Simplification (LS) system is to replace the complex words in a text with simpler alternatives, without compromising its meaning or grammaticality. The LS task is often addressed as the series of steps in Figure 1, as introduced by (Devlin and Tait, 1998). Their work has inspired others to conceive new LS solutions for the aphasic (Carroll et al., 1998), dyslexic (Bott et al., 2012), illiterate (Watanabe et al., 2009), non-native English speakers (Paetzold, 2013; Paetzold and Specia, 2013), children (Kajiwara et al., 2013) and others. BenchLS: A New Dataset To create our dataset we combined two resources: the LexMTurk (Horn et al., 2014) and LSeval (De Belder and Moens, 2012) datasets. The instances in both datasets, 929 in total, contain a sentence, a target complex word, and several candidate substitutions ranked according to their simplicity. The candidates in both datasets were suggested and ranked by English speakers from the U.S. To increase its reliability, we applied the following corrections over each instance of our dataset: 1. Spelling Filtering: We discard any missp"
L16-1491,P15-4015,1,0.909843,"in total, contain a sentence, a target complex word, and several candidate substitutions ranked according to their simplicity. The candidates in both datasets were suggested and ranked by English speakers from the U.S. To increase its reliability, we applied the following corrections over each instance of our dataset: 1. Spelling Filtering: We discard any misspelled candidates using Norvig’s algorithm1 . We trained our spelling model over the News Crawl2 corpus. 2. Inflection Correction: We inflected all candidates to the tense of the target word using the Text Adorning module of LEXenstein (Paetzold and Specia, 2015; Burns, 2013). The resulting dataset – BenchLS – contains 929 instances, with an average of 7.37 candidate substitutions per complex word. We use it in all our experiments, as described in what follows. Figure 1: Lexical Simplification Pipeline Although various LS systems can be found in the literature, very little effort has been made to compare their performance. Apart from the work of (Shardlow, 2013) and (Specia et al., 2012), which provide brief benchmarkings of Complex Word Identification and Substitution Ranking, respectively, no other comparisons have been reported. To address this li"
L16-1491,W16-4912,0,0.328204,"x words, which can be later ranked or filtered according to different criteria. This step does not take into account the ambiguity of words, i.e. it generates candidate substitutions for a word in all or any of its possible meanings. The most frequently used SG solution consists in extracting synonyms from linguistic databases, such as WordNet (Devlin and Tait, 1998; Carroll et al., 1999) or the UMLS database for medical content (Ong et al., 2007; Leroy et al., 2013). Recently, however, 3074 1 2 http://norvig.com/spell-correct.html http://www.statmt.org/wmt11/translation-task.html • Paetzold (Paetzold and Specia, 2016): Produces candidates using a context-aware word embeddings model. 10 candidates for each target word are retrieved with a model trained using the word2vec toolkit (Mikolov et al., 2013) over the same corpus used for the Glavas generator, parsed with the Stanford Parser. Word vectors are trained using the Bag-ofWords (CBOW) architecture and 1300 vector dimensions. new resources have been used, such as aligned complex-tosimple parallel corpora (Paetzold, 2013; Paetzold and Specia, 2013; Horn et al., 2014) and word embedding models ˇ (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). 3.1. S"
L16-1491,N15-2002,1,0.819422,"r (Burns, 2013). For this system, we use the parallel Wikipedia and Simple Wikipedia corpus (Horn et al., 2014) and the Stanford Parser. ˇ • Glavas (Glavaˇs and Stajner, 2015): Produces candidates using a word embeddings model. They retrieve the 10 words for which the embeddings vector has the highest cosine similarity with that of the target complex word, except for its morphological variants. Their model uses 200 vector dimensions and is trained with the GloVe toolkit (Pennington et al., 2014). We train their model over a corpus of 7 billion words which combines combines the SubIMDB corpus (Paetzold, 2015), UMBC webbase4 , News Crawl5 , SUBTLEX (Brysbaert and New, 2009), Wikipedia and Simple Wikipedia (Kauchak, 2013). As illustrated in Table 1, the Paetzold generator outperforms all others, including the supervised Horn generator, by a considerable margin in almost all metrics used, revealing the potential of context-aware embedding models for SG. In order to further highlight the importance of using context-aware as opposed to traditional embedding models, we have trained a modified version of the Glavas generator. ˇ Instead of using the model specified in (Glavaˇs and Stajner, 2015), it uses"
L16-1491,D14-1162,0,0.0790735,"Missing"
L16-1491,W04-2013,0,0.0154432,"Missing"
L16-1491,P13-3015,0,0.0386882,"d our spelling model over the News Crawl2 corpus. 2. Inflection Correction: We inflected all candidates to the tense of the target word using the Text Adorning module of LEXenstein (Paetzold and Specia, 2015; Burns, 2013). The resulting dataset – BenchLS – contains 929 instances, with an average of 7.37 candidate substitutions per complex word. We use it in all our experiments, as described in what follows. Figure 1: Lexical Simplification Pipeline Although various LS systems can be found in the literature, very little effort has been made to compare their performance. Apart from the work of (Shardlow, 2013) and (Specia et al., 2012), which provide brief benchmarkings of Complex Word Identification and Substitution Ranking, respectively, no other comparisons have been reported. To address this limitation, we present a systematic benchmarking of LS systems. We innovate by comparing the performance of not only systems in their entirety, but also of system components individually, such as Substitution Generation, Selection and Ranking approaches. In addition, we introduce BenchLS, a new dataset for the task. In the following Sections, we describe our dataset and present our experiments. 3. Substitut"
L16-1491,S12-1046,1,0.846175,"Missing"
L16-1579,C04-1046,0,0.0689396,"are MT systems, to inform end-users or to assist in the translation process (such as in post-editing), appropriate evaluation methods and metrics need to be applied in order to provide reliable assessments. Automatic metrics that contrast system outputs against reference translations, such as BLEU (Papineni et al., 2002), are widely explored to compare MT systems and measure the progress of a given MT system over time. Quality Estimation (QE) is a different evaluation method that provides a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009). These metrics are useful to inform end-users and post-editors. Most work in QE focuses on sentence-level and word-level prediction (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). Estimation of quality at sentence and word levels are probably the most useful types of prediction for post-editing, since post-editors can benefit from smaller parts of the document that are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications a"
L16-1579,W14-3302,1,0.837552,"s that contrast system outputs against reference translations, such as BLEU (Papineni et al., 2002), are widely explored to compare MT systems and measure the progress of a given MT system over time. Quality Estimation (QE) is a different evaluation method that provides a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009). These metrics are useful to inform end-users and post-editors. Most work in QE focuses on sentence-level and word-level prediction (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). Estimation of quality at sentence and word levels are probably the most useful types of prediction for post-editing, since post-editors can benefit from smaller parts of the document that are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of end-users (such as gisting ) and where fully automated MT is needed (e.g. because the amount of data is unfeasible for human post-editing). While for sentence and word-level QE several quality labels"
L16-1579,W15-3001,1,0.926955,"em outputs against reference translations, such as BLEU (Papineni et al., 2002), are widely explored to compare MT systems and measure the progress of a given MT system over time. Quality Estimation (QE) is a different evaluation method that provides a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009). These metrics are useful to inform end-users and post-editors. Most work in QE focuses on sentence-level and word-level prediction (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). Estimation of quality at sentence and word levels are probably the most useful types of prediction for post-editing, since post-editors can benefit from smaller parts of the document that are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of end-users (such as gisting ) and where fully automated MT is needed (e.g. because the amount of data is unfeasible for human post-editing). While for sentence and word-level QE several quality labels have been proposed so"
L16-1579,W12-3102,1,0.817573,"to provide reliable assessments. Automatic metrics that contrast system outputs against reference translations, such as BLEU (Papineni et al., 2002), are widely explored to compare MT systems and measure the progress of a given MT system over time. Quality Estimation (QE) is a different evaluation method that provides a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009). These metrics are useful to inform end-users and post-editors. Most work in QE focuses on sentence-level and word-level prediction (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). Estimation of quality at sentence and word levels are probably the most useful types of prediction for post-editing, since post-editors can benefit from smaller parts of the document that are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of end-users (such as gisting ) and where fully automated MT is needed (e.g. because the amount of data is unfeasible for human post-editing). While for sentence a"
L16-1579,W11-2401,0,0.0269932,"es for the CREG corpus. between test takers could be calculated. Translation Methods Set 1 Document 1 4.1. Document 2 As previously mentioned, the reading comprehension questions are open questions, and thus any answer could be provided by the test takers. Another important detail is that these questions have different levels of complexity, meaning that some questions require more effort to be answered. Since our aim is to generate quality labels from the answers, information about the question complexity level is important. We therefore manually classified the questions using the classes in (Meurers et al., 2011), focusing on question forms and comprehension types (Day and Park, 2005). Document 3 Document 4 Document 5 Test takers Document 6 . . . Set m Document n-5 Document n-4 Document n-3 Question classification Question forms: these can be directly defined by the question structure and by the expected answer. The question forms available in the CREG corpus are: Document n-2 Document n-1 Document n • Yes/no questions: are simple questions that admit either yes or no as valid answers. Figure 1: Split of corpus in sets and translation approach. • Alternative questions: are a combination of yes/no ques"
L16-1579,P02-1040,0,0.101708,"t the reading comprehension test into document-level quality scores. Keywords: Machine Translation, Reading Comprehension, Quality Estimation 1. show similar quality scores. Introduction Evaluating Machine Translation (MT) systems outputs is a challenging task. Whether the evaluation goal is to compare MT systems, to inform end-users or to assist in the translation process (such as in post-editing), appropriate evaluation methods and metrics need to be applied in order to provide reliable assessments. Automatic metrics that contrast system outputs against reference translations, such as BLEU (Papineni et al., 2002), are widely explored to compare MT systems and measure the progress of a given MT system over time. Quality Estimation (QE) is a different evaluation method that provides a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009). These metrics are useful to inform end-users and post-editors. Most work in QE focuses on sentence-level and word-level prediction (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). Estimation of quality at sentence and word levels are prob"
L16-1579,2014.eamt-1.21,1,0.77146,"r parts of the document that are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of end-users (such as gisting ) and where fully automated MT is needed (e.g. because the amount of data is unfeasible for human post-editing). While for sentence and word-level QE several quality labels have been proposed so far (e.g. HTER (Snover et al., 2006), likert), there is a lack of studies in quality labels for document-level. Previous work use BLEU-style metrics as labels (Scarton and Specia, 2014; Soricut and Echihabi, 2010; Scarton, 2015) . The WMT15 QE shared task also followed this approach, using METEOR (Banerjee and Lavie, 2005) for a paragraph-level QE task (Bojar et al., 2015). However, as shown by Scarton et al. (2015), these metrics do not distinguish well among documents, i.e. most documents produced by the same or a similar MT system One issue of document-level quality labeling, noted by Scarton et al. (2015), is that the task of asking humans to assess documents is not trivial. While likert scores can be successfully applied for sentence and word levels, documentlevel qual"
L16-1579,W15-4916,1,0.866927,"Missing"
L16-1579,N15-2016,1,0.842109,"nstead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of end-users (such as gisting ) and where fully automated MT is needed (e.g. because the amount of data is unfeasible for human post-editing). While for sentence and word-level QE several quality labels have been proposed so far (e.g. HTER (Snover et al., 2006), likert), there is a lack of studies in quality labels for document-level. Previous work use BLEU-style metrics as labels (Scarton and Specia, 2014; Soricut and Echihabi, 2010; Scarton, 2015) . The WMT15 QE shared task also followed this approach, using METEOR (Banerjee and Lavie, 2005) for a paragraph-level QE task (Bojar et al., 2015). However, as shown by Scarton et al. (2015), these metrics do not distinguish well among documents, i.e. most documents produced by the same or a similar MT system One issue of document-level quality labeling, noted by Scarton et al. (2015), is that the task of asking humans to assess documents is not trivial. While likert scores can be successfully applied for sentence and word levels, documentlevel quality can not be evaluated in the same way. Is"
L16-1579,2006.amta-papers.25,0,0.482024,"uality at sentence and word levels are probably the most useful types of prediction for post-editing, since post-editors can benefit from smaller parts of the document that are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of end-users (such as gisting ) and where fully automated MT is needed (e.g. because the amount of data is unfeasible for human post-editing). While for sentence and word-level QE several quality labels have been proposed so far (e.g. HTER (Snover et al., 2006), likert), there is a lack of studies in quality labels for document-level. Previous work use BLEU-style metrics as labels (Scarton and Specia, 2014; Soricut and Echihabi, 2010; Scarton, 2015) . The WMT15 QE shared task also followed this approach, using METEOR (Banerjee and Lavie, 2005) for a paragraph-level QE task (Bojar et al., 2015). However, as shown by Scarton et al. (2015), these metrics do not distinguish well among documents, i.e. most documents produced by the same or a similar MT system One issue of document-level quality labeling, noted by Scarton et al. (2015), is that the task o"
L16-1579,P10-1063,0,0.0239598,"at are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of end-users (such as gisting ) and where fully automated MT is needed (e.g. because the amount of data is unfeasible for human post-editing). While for sentence and word-level QE several quality labels have been proposed so far (e.g. HTER (Snover et al., 2006), likert), there is a lack of studies in quality labels for document-level. Previous work use BLEU-style metrics as labels (Scarton and Specia, 2014; Soricut and Echihabi, 2010; Scarton, 2015) . The WMT15 QE shared task also followed this approach, using METEOR (Banerjee and Lavie, 2005) for a paragraph-level QE task (Bojar et al., 2015). However, as shown by Scarton et al. (2015), these metrics do not distinguish well among documents, i.e. most documents produced by the same or a similar MT system One issue of document-level quality labeling, noted by Scarton et al. (2015), is that the task of asking humans to assess documents is not trivial. While likert scores can be successfully applied for sentence and word levels, documentlevel quality can not be evaluated in"
L16-1579,2009.eamt-1.5,1,0.803854,"nform end-users or to assist in the translation process (such as in post-editing), appropriate evaluation methods and metrics need to be applied in order to provide reliable assessments. Automatic metrics that contrast system outputs against reference translations, such as BLEU (Papineni et al., 2002), are widely explored to compare MT systems and measure the progress of a given MT system over time. Quality Estimation (QE) is a different evaluation method that provides a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009). These metrics are useful to inform end-users and post-editors. Most work in QE focuses on sentence-level and word-level prediction (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). Estimation of quality at sentence and word levels are probably the most useful types of prediction for post-editing, since post-editors can benefit from smaller parts of the document that are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of"
L16-1579,W05-0909,0,\N,Missing
L16-1579,2015.eamt-1.17,1,\N,Missing
L16-1582,W14-3302,1,0.851768,"attempts to address the problem of QE was the workshop at John Hopkins University (Blatz et al., 2004). While the main focus of the workshop was sentencelevel QE, it also included research on word-level QE. In (Ueffing and Ney, 2007), word-level quality is defined as the word-level posterior probability, which is computed as a probability of a word occurring in an n-best list. Raybaud et al. (2009) rely on mutual information between the evaluated word and its source and target contexts, and an LM enhanced with linguistic features. More recently, the WMT shared task on QE (Bojar et al., 2013; Buck et al., 2014; Bojar et al., 2015) has spurred the development of wordlevel QE systems. The most successful QE models use features extracted from pseudo-references (Espl`a-Gomis et al., 2015) and n-best lists (Camargo de Souza et al., 2014) produced by MT systems. In (Luong et al., 2014) the feature set contains source and target contexts of the word, source and target POS-tags, LM scores, syntactic and semantic information. The most widely used training models are Conditional Random Fields (CRF) (Luong et al., 2014; Shah et al., 2015) or neural networks: feed-forward (Kreutzer et al., 2015) as well as rec"
L16-1582,W14-3340,0,0.0639111,"Missing"
L16-1582,W15-3036,0,0.116068,"Missing"
L16-1582,W15-3037,0,0.206249,"Bojar et al., 2013; Buck et al., 2014; Bojar et al., 2015) has spurred the development of wordlevel QE systems. The most successful QE models use features extracted from pseudo-references (Espl`a-Gomis et al., 2015) and n-best lists (Camargo de Souza et al., 2014) produced by MT systems. In (Luong et al., 2014) the feature set contains source and target contexts of the word, source and target POS-tags, LM scores, syntactic and semantic information. The most widely used training models are Conditional Random Fields (CRF) (Luong et al., 2014; Shah et al., 2015) or neural networks: feed-forward (Kreutzer et al., 2015) as well as recurrent (Camargo de Souza et al., 2014). Despite recent developments, word-level QE still lags behind other granularity levels in terms of performance. One of the reasons for that is the complexity of the task itself: word-level features are difficult to generalise, leading to sparsity in the training data. Another reason is the lack of toolkits that serve as a basis for researchers to develop upon. Despite many submissions to the WMT word-level QE task, only one tool is publicly available for QE at the word level, namely QuEst++ (Specia et al., 2015), which is an extension of a"
L16-1582,W14-3342,0,0.247048,"word-level posterior probability, which is computed as a probability of a word occurring in an n-best list. Raybaud et al. (2009) rely on mutual information between the evaluated word and its source and target contexts, and an LM enhanced with linguistic features. More recently, the WMT shared task on QE (Bojar et al., 2013; Buck et al., 2014; Bojar et al., 2015) has spurred the development of wordlevel QE systems. The most successful QE models use features extracted from pseudo-references (Espl`a-Gomis et al., 2015) and n-best lists (Camargo de Souza et al., 2014) produced by MT systems. In (Luong et al., 2014) the feature set contains source and target contexts of the word, source and target POS-tags, LM scores, syntactic and semantic information. The most widely used training models are Conditional Random Fields (CRF) (Luong et al., 2014; Shah et al., 2015) or neural networks: feed-forward (Kreutzer et al., 2015) as well as recurrent (Camargo de Souza et al., 2014). Despite recent developments, word-level QE still lags behind other granularity levels in terms of performance. One of the reasons for that is the complexity of the task itself: word-level features are difficult to generalise, leading t"
L16-1582,2009.eamt-1.15,0,0.0753507,"Missing"
L16-1582,W15-3041,1,0.804966,"features. More recently, the WMT shared task on QE (Bojar et al., 2013; Buck et al., 2014; Bojar et al., 2015) has spurred the development of wordlevel QE systems. The most successful QE models use features extracted from pseudo-references (Espl`a-Gomis et al., 2015) and n-best lists (Camargo de Souza et al., 2014) produced by MT systems. In (Luong et al., 2014) the feature set contains source and target contexts of the word, source and target POS-tags, LM scores, syntactic and semantic information. The most widely used training models are Conditional Random Fields (CRF) (Luong et al., 2014; Shah et al., 2015) or neural networks: feed-forward (Kreutzer et al., 2015) as well as recurrent (Camargo de Souza et al., 2014). Despite recent developments, word-level QE still lags behind other granularity levels in terms of performance. One of the reasons for that is the complexity of the task itself: word-level features are difficult to generalise, leading to sparsity in the training data. Another reason is the lack of toolkits that serve as a basis for researchers to develop upon. Despite many submissions to the WMT word-level QE task, only one tool is publicly available for QE at the word level, namely Q"
L16-1582,P15-4020,1,0.675881,"al networks: feed-forward (Kreutzer et al., 2015) as well as recurrent (Camargo de Souza et al., 2014). Despite recent developments, word-level QE still lags behind other granularity levels in terms of performance. One of the reasons for that is the complexity of the task itself: word-level features are difficult to generalise, leading to sparsity in the training data. Another reason is the lack of toolkits that serve as a basis for researchers to develop upon. Despite many submissions to the WMT word-level QE task, only one tool is publicly available for QE at the word level, namely QuEst++ (Specia et al., 2015), which is an extension of a previous, sentence-level version for QE at the document and word levels. It performs the extraction of word-level features used in (Luong et al., 2014), but not model learning, although it is distributed with several scripts for the sklearn library 1 . This tool is implemented in Java. In this paper, we present Marmot — a new tool for the extraction of word-level features and training of word-level QE models. Unlike QuEst++, this system is highly flexible and modular: it can easily be extended to include additional features and learning methods, and it includes a f"
L16-1582,P15-2087,0,0.0283639,"ity of an automatic translation without comparing it to a reference. This task is particularly important for many real-world applications of MT, where no reference translation is available. If MT is used for gisting, a user who can only speak the target language cannot judge the quality or reliability of its output. If an MT system is embedded into a computer-assisted translation (CAT) tool, quality estimation can increase the productivity of a user by filtering out translations that are too bad for editing or highlighting the phrases with low quality to make sure they are edited by the user (Turchi et al., 2015; O’Brien et al., 2014). Translation quality can be estimated at different levels of granularity, ranging from the word level to the document level. Thus far, the best results have been achieved in sentence-level QE. This is likely to be because the evaluation of the entire sentence allows the use of very many general features, such as language model (LM) scores and posterior translation probabilities. However, the more challenging task of estimating quality at the word level is also very important, both from an application perspective, and from a research point of view. Finegrained distinctio"
L16-1582,J07-1003,0,0.0410377,"rd level is also very important, both from an application perspective, and from a research point of view. Finegrained distinctions can be used to diagnose MT systems and to guide post-editors’ attention to erroneous parts of the sentence. In addition, sentence-level translation quality can be drastically affected by translation errors involving single words or phrases. One of the first attempts to address the problem of QE was the workshop at John Hopkins University (Blatz et al., 2004). While the main focus of the workshop was sentencelevel QE, it also included research on word-level QE. In (Ueffing and Ney, 2007), word-level quality is defined as the word-level posterior probability, which is computed as a probability of a word occurring in an n-best list. Raybaud et al. (2009) rely on mutual information between the evaluated word and its source and target contexts, and an LM enhanced with linguistic features. More recently, the WMT shared task on QE (Bojar et al., 2013; Buck et al., 2014; Bojar et al., 2015) has spurred the development of wordlevel QE systems. The most successful QE models use features extracted from pseudo-references (Espl`a-Gomis et al., 2015) and n-best lists (Camargo de Souza et"
L16-1649,P06-2103,0,0.138628,", v1 ) represents adjacent sentences, and c(·) is a function that counts how often a pattern (or a pair of patterns) was observed in the training data. m n Y Y 1 X c(ui , vj ) + α (3) p(D) = m i=1 c(ui ) + α|V | m n j=1 Y m n X Y p(vj |ui ) (4) n j=1 i=0 (um 1 ,v1 )∈D We resort to Expectation Maximisation (EM) to estimate the parameters in Equation 4 (Brown et al., 1993). Due to the convexity of IBM Model 1, EM is guaranteed to converge to a global optimum. Moreover, as we observe more data, this model converges to better parameters. A similar solution was proposed in a different context by (Soricut and Marcu, 2006) in their work on word co-occurrences. To avoid assigning 0 probability to documents containing unseen patterns, we modify the original training procedure to treat all the singletons as belonging to an unknown category (U NK), thus reserving probability mass for future unseen items. 3. Running the Toolkit Input The input data for the toolkit is raw text, with markup for document breaks. The syntax models take ptb marked up files, which can be derived from code included in the toolkit. Models available (u1 ,v1 )∈D To account for unseen syntactic patterns at test time, the model is smoothed by a"
L16-1649,J08-1001,0,0.223135,"Missing"
L16-1649,J93-2003,0,0.174796,"Section 2.2. we describe our entity graph implementation, which captures different aspects of lexical coherence from an entity grid, tracking connections between non-adjacent entities in the text. This model covers aspects of coherence that reflect the centrality and topic of the discourse. Following that we present our implementation of the syntax-based coherence model of Louis and Nenkova (2012), which captures syntactic patterns between adjacent sentences (Section 2.3.). Finally, in Section 2.4. we present our own extension of this model, a fully generative model incorporating IBM Model 1 (Brown et al., 1993) to model alignments over syntactic items in adjacent sentences. 2.1. Entity Grid Model As detailed in (Barzilay and Lapata, 2008; Elsner et al., 2007), the entity-based approach derives from the assumption that entities in a coherent text are distributed in a certain manner, as posed by various discourse theories, in par1 http://cs.brown.edu/melsner/manual.html ticular Centering Theory (Grosz et al., 1995). This theory holds that coherent texts are characterised by salient entities in strong grammatical roles, such as subject or object. Entity grids are constructed by identifying the discours"
L16-1649,P10-1020,0,0.154027,"ith a given syntactic role, namely, subject (S), object (O), or other (X). Transitions are observed by examining the grid vertically for each entity. We replicate the generative model of document coherence based on entity transitions introduced by Lapata (2005). Equation 1 shows this formulation, where m is the number of entities, n is the number of sentences in a document D and rs,e is the role taken by entity e in sentence s. m n 1 YY p(D) = p(rs,e |r(s−h),e . . . r(s−1),e ) (1) m · n e=1 s=1 This is because the syntactic patterns which hold for English, do not hold for German, for example (Cheung and Penn, 2010). 2.2. Entity Graph Model Guinaudeau and Strube (2013) converted a standard entity grid into a bipartite graph which tracks the occurrence of entities throughout the document, including between nonadjacent sentences. A local coherence score is calculated directly as the average outdegree of a projection, summing the shared edges of entities between two sentences. The general form of the coherence score assigned to a document D in this approach is shown in Equation 2. This is a centrality measure based on the average outdegree across the N sentences represented in a directed document graph. The"
L16-1649,N07-1055,0,0.0275142,"ons between non-adjacent entities in the text. This model covers aspects of coherence that reflect the centrality and topic of the discourse. Following that we present our implementation of the syntax-based coherence model of Louis and Nenkova (2012), which captures syntactic patterns between adjacent sentences (Section 2.3.). Finally, in Section 2.4. we present our own extension of this model, a fully generative model incorporating IBM Model 1 (Brown et al., 1993) to model alignments over syntactic items in adjacent sentences. 2.1. Entity Grid Model As detailed in (Barzilay and Lapata, 2008; Elsner et al., 2007), the entity-based approach derives from the assumption that entities in a coherent text are distributed in a certain manner, as posed by various discourse theories, in par1 http://cs.brown.edu/melsner/manual.html ticular Centering Theory (Grosz et al., 1995). This theory holds that coherent texts are characterised by salient entities in strong grammatical roles, such as subject or object. Entity grids are constructed by identifying the discourse entities in the documents under consideration, and constructing a 2D grid for each document, whereby each column corresponds to the entity, i.e. noun"
L16-1649,J95-2003,0,0.813958,"ptures syntactic patterns between adjacent sentences (Section 2.3.). Finally, in Section 2.4. we present our own extension of this model, a fully generative model incorporating IBM Model 1 (Brown et al., 1993) to model alignments over syntactic items in adjacent sentences. 2.1. Entity Grid Model As detailed in (Barzilay and Lapata, 2008; Elsner et al., 2007), the entity-based approach derives from the assumption that entities in a coherent text are distributed in a certain manner, as posed by various discourse theories, in par1 http://cs.brown.edu/melsner/manual.html ticular Centering Theory (Grosz et al., 1995). This theory holds that coherent texts are characterised by salient entities in strong grammatical roles, such as subject or object. Entity grids are constructed by identifying the discourse entities in the documents under consideration, and constructing a 2D grid for each document, whereby each column corresponds to the entity, i.e. noun, being tracked, and each row represents a particular sentence in the document. An entity transition is defined as a consecutive occurrence of an entity with a given syntactic role, namely, subject (S), object (O), or other (X). Transitions are observed by ex"
L16-1649,P13-1010,0,0.433436,"object (O), or other (X). Transitions are observed by examining the grid vertically for each entity. We replicate the generative model of document coherence based on entity transitions introduced by Lapata (2005). Equation 1 shows this formulation, where m is the number of entities, n is the number of sentences in a document D and rs,e is the role taken by entity e in sentence s. m n 1 YY p(D) = p(rs,e |r(s−h),e . . . r(s−1),e ) (1) m · n e=1 s=1 This is because the syntactic patterns which hold for English, do not hold for German, for example (Cheung and Penn, 2010). 2.2. Entity Graph Model Guinaudeau and Strube (2013) converted a standard entity grid into a bipartite graph which tracks the occurrence of entities throughout the document, including between nonadjacent sentences. A local coherence score is calculated directly as the average outdegree of a projection, summing the shared edges of entities between two sentences. The general form of the coherence score assigned to a document D in this approach is shown in Equation 2. This is a centrality measure based on the average outdegree across the N sentences represented in a directed document graph. The outdegree of a sentence si , denoted o(si ), is the t"
L16-1649,D12-1106,0,\N,Missing
L18-1553,W03-1004,0,0.239373,"a pair of documents: e.g. version 1 has a lower reading level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptions that can be made about the Newsela corpus, it is more efficient than more sophisticated approaches that perform exhaustive search over all"
L18-1553,P11-2087,0,0.024947,"Missing"
L18-1553,W11-1603,0,0.01484,"s a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptions that can be made about the Newsela corpus, it is more efficient than more sophisticated approaches that perform exhaustive search over all possible 3504 ˇ paragraph/sentence alignments (Stajner et al., 2017), while still offeri"
L18-1553,P11-2117,0,0.247664,"work. Keywords: text Simplification, simplification corpora, Newsela 1. Introduction Text Simplification (TS) consists in making texts more easily comprehensible. It can take many forms: Lexical Simplification (LS), in which complex words are replaced by simpler alternatives (Devlin, 1999), Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS"
L18-1553,W11-2107,0,0.0303121,"old 0.352 0.341 0.238 0.231 0.378 0.400 Table 4: Accuracy in the full pipeline evaluation Here we assess the potential of our corpus in LS. LS is commonly addressed as a pipeline of steps: candidates for a target complex word are produced via a Substitution Generation (SG) method, filtered with respect to the context of the complex word via a Substitution Selection (SS) method, and finally ordered for simplicity by a Substitution Ranking (SR) method. We use our aligned corpus for SG following the state of the art approach in (Horn et al., 2014). First, we produce word alignments using Meteor (Denkowski and Lavie, 2011) and extract complex-to-simple word correspondences. Then we filter word pairs with different POS tags, where the complex word is a stop word, or either word is a proper noun. Finally, we generate all possible inflections for nouns and verbs (Burns, 2013). We compare this approach to six other generators from a recent benchmark (Paetzold and Specia, 2016a): the Horn generator (Horn et al., 2014), which employs the approach described above over Wikipedia-Simple Wikipedia data, the Devlin (Devlin and Tait, 1998), Biran (Biran et ˇ al., 2011), Glavas (Glavaˇs and Stajner, 2015) and Paetzold (Paet"
L18-1553,W13-2901,0,0.129862,"e. It can take many forms: Lexical Simplification (LS), in which complex words are replaced by simpler alternatives (Devlin, 1999), Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 20"
L18-1553,P15-2011,0,0.0613146,"Missing"
L18-1553,P14-2075,0,0.0841375,"implification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 2011)) in comparison to bilingual corpora used with similar modelling techniques in MT and, more critically, covers a limited range of simplification operations, which are applied in ad hoc ways by volunteer editors. Xu et al. (2015) introduce a new resou"
L18-1553,P07-2045,0,0.00703543,"Missing"
L18-1553,W13-4813,1,0.897302,"s more easily comprehensible. It can take many forms: Lexical Simplification (LS), in which complex words are replaced by simpler alternatives (Devlin, 1999), Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence"
L18-1553,P15-4015,1,0.92596,"verbs (Burns, 2013). We compare this approach to six other generators from a recent benchmark (Paetzold and Specia, 2016a): the Horn generator (Horn et al., 2014), which employs the approach described above over Wikipedia-Simple Wikipedia data, the Devlin (Devlin and Tait, 1998), Biran (Biran et ˇ al., 2011), Glavas (Glavaˇs and Stajner, 2015) and Paetzold (Paetzold and Specia, 2016c) generators, which exploit WordNet, comparable complex-to-simple documents, typical word embeddings and context-aware word embeddings, respectively. All generators were implemented with the LEXenstein framework (Paetzold and Specia, 2015). We use the BenchLS dataset as our gold-standard dataset (Paetzold and Specia, 2016a). It is the largest dataset of its kind, with 929 instances, each composed by a sentence, a target complex word, and a set of gold substitutions given by humans. To compare the generators, we use standard metrics: Potential – the proportion of instances in which at least one of the candidates generated is in the gold-standard, Precision – the proportion of generated substitutions that are in the gold-standard, Recall – the proportion of goldstandard substitutions that are among the generated substitutions, an"
L18-1553,L16-1491,1,0.858958,"m 2 to 12, where 2 represents the lowest and 12 the highest level. Version identifiers capture the relationship between the reading levels of a pair of documents: e.g. version 1 has a lower reading level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptio"
L18-1553,N16-1050,1,0.859101,"m 2 to 12, where 2 represents the lowest and 12 the highest level. Version identifiers capture the relationship between the reading levels of a pair of documents: e.g. version 1 has a lower reading level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptio"
L18-1553,W16-4912,0,0.0361996,"m 2 to 12, where 2 represents the lowest and 12 the highest level. Version identifiers capture the relationship between the reading levels of a pair of documents: e.g. version 1 has a lower reading level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptio"
L18-1553,N10-1063,0,0.0372608,"ng level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptions that can be made about the Newsela corpus, it is more efficient than more sophisticated approaches that perform exhaustive search over all possible 3504 ˇ paragraph/sentence alignments"
L18-1553,P17-2016,0,0.0934999,"Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptions that can be made about the Newsela corpus, it is more efficient than more sophisticated approaches that perform exhaustive search over all possible 3504 ˇ paragraph/sentence alignments (Stajner et al., 2017), while still offering comparable alignment accuracy. The result of the alignment is a corpus with 19, 198 pairs of articles aligned at both paragraph (300, 475 pairs) and sentence (550, 644 pairs) levels. This is over three times larger than the Wikipedia–Simple Wikipedia corpus (Coster and Kauchak, 2011), making it the largest corpus of its kind. Columns 2 to 4 in Table 1 illustrate the number of paragraph and sentence alignments for all version pairs in the corpus. We categorise the sentence alignments according to four types of simplification: • None: Complex and simple sentences are ident"
L18-1553,D11-1038,0,0.0732401,"ch complex words are replaced by simpler alternatives (Devlin, 1999), Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 2011)) in comparison to bilingual corpora used with similar mo"
L18-1553,Q15-1021,0,0.457792,"hu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 2011)) in comparison to bilingual corpora used with similar modelling techniques in MT and, more critically, covers a limited range of simplification operations, which are applied in ad hoc ways by volunteer editors. Xu et al. (2015) introduce a new resource that allegedly addresses these limitations: the Newsela corpus (Newsela, 2016). Unlike Simple Wikipedia, the Newsela corpus was created by professional editors and targets a specific audience (students), which should make it a more reliable resource for TS. However,"
L18-1553,Q16-1029,0,0.0482634,"0.330 0.238 0.272 0.330 0.240 FLESCH-S 66.93 71.12 67.29 75.04 87.49 70.34 75.10 87.52 70.41 FLESCH-O 66.21 69.85 65.34 74.89 87.33 70.19 74.89 87.33 70.19 FLESCH-R 74.32 69.85 79.98 80.62 87.33 78.91 80.62 87.33 78.91 Table 5: Results for SMT-based simplifiers not a reliable metric when original, reference and simplified sentences are the same. For all cases where TER = 0, the SARI value was 0.330, which can be seem as a low value if the systems are producing an output equal to the reference. Since this metric was designed for cases where sentences should also be simplified (as explained in Xu et al. (2016)), the use of SARI for cases where the original sentences are already simple is not reliable. 6. Conclusions Upon studying the sentence-aligned Newsela corpus we found that: (i) it follows an expected TER distribution, with the lowest TER being between adjacent levels; (ii) the simplified sentences score as more readable than their original counterparts according to traditional readability metrics, and (iii) the corpus proved a more reliable source of complex-simple correspondences for LS and MT-based simplification than the Wikipedia-Simple Wikipedia corpus. We achieve some the highest perfor"
L18-1553,N10-1056,0,0.0550178,"ation (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 2011)) in comparison to bilingual corpora used with similar modelling techniques in MT and, more critically, covers a limited range of simplificat"
L18-1553,D17-1062,0,0.0627507,"Missing"
L18-1553,C10-1152,0,0.235267,"ification (TS) consists in making texts more easily comprehensible. It can take many forms: Lexical Simplification (LS), in which complex words are replaced by simpler alternatives (Devlin, 1999), Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015)"
L18-1602,W14-3348,0,0.0413292,"of the words in the reference, will be developed and tested in future. For now, we use this simple accuracy measure and demonstrate a potential application of the MLT Dataset. 8 7 https://github.com/sheffieldnlp/mlt Evaluating Machine Translation Systems For consistency, the system’s outputs undergo the same preprocessing steps in Section 2.1.1. 3812 3.1.1. Evaluating Machine Translation Systems In Elliott et al. (2017), the Multimodal and Text-only Machine Translation systems submitted to the shared task were evaluated and ranked using the Meteor metric and Human scoring. The Meteor metric (Denkowski and Lavie, 2014) calculates a sentence-level similarity score between 0 and 100 between the system output and the reference (human) translation, where 0 means no similarity and 100 means ‘perfect’ similarity. Similarity is computed as a function of the proportion of words that can be aligned between the system and human translations, allowing for different types of alignments (e.g. lemma, synonym). The overall Meteor score of a system is the mean of the sentence-level scores over the test set. Human scoring was carried out in Elliott et al. (2017) using bilingual Direct Assessment (Graham et al., 2017), where"
L18-1602,N13-1073,0,0.0120813,"olkit2 (Koehn et al., 1 2007). German sentences, which can contain compound words like ‘sonnenblumenkerne’ (sunflower seeds), are split/decompounded using pre-computed model of SEmantic COmpound Splitter (SECOS)3 (Riedl and Biemann, 2016). Since we are not interested in distinguishing morphological variants of the words, we also lemmatized4 all sentences in the respective languages, which reduced vocabulary size and led to better word alignment in the later step. 2.1.2. Word Alignment After the pre-processing step, the word tokens in the Multi30K parallel corpus are aligned using Fast Align5 (Dyer et al., 2013). Fast Align generates asymmetric word alignments depending on which language in the parallel corpus is treated as the source. We generate both alignments - ‘forward’ (where English is treated as the source language) and ‘reverse’ (where German or French is treated as the source language). To learn better word alignments, we train Fast Align on a larger corpus comprising of the Europarl parallel corpus6 (Koehn, 2005) in addition to the Multi30K parallel corpus for the English-German and English-French language pairs separately. The Europarl corpus also undergoes the same pre-processing steps i"
L18-1602,W16-3210,1,0.901433,"Missing"
L18-1602,W17-4718,1,0.840497,"luable multimodal and multilingual language resource with several potential uses including evaluation of lexical disambiguation within (Multimodal) Machine Translation systems. Keywords: Multimodal Machine Translation, Visual Sense Disambiguation, Multimodal Multilingual Language Resources 1. Introduction Multimodal Machine Translation is the task of translating text using information in other modalities (such as images) as auxiliary cues. It has been recently framed as a shared task as part of the last two editions of the Conference on Machine Translation (WMT16, WMT17) (Specia et al., 2016; Elliott et al., 2017). Within the Conference on Machine Translation, the task is defined as: Given an image and its description in the source language, the objective is to translate the description into a target language, where this process can be supported by information from the image, as depicted in Figure 1. (a) Ein Mann h¨alt ein Siegel (b) Ein Mann h¨alt einen Seehund Figure 2: Two different translations of “A man is holding a seal” depending on the visual context Figure 1: Multimodal Machine Translation Shared Task One of the main motivations to introduce multimodality in Machine Translation is the intuitio"
L18-1602,N16-1022,0,0.094204,"isual context. This modified version of Word Sense Disambiguation that uses visual context instead of textual context is called Visual Sense Disambiguation. In monolingual work, Visual Sense Disambiguation has previously been attempted for ambiguous nouns like the word ‘bank’ which could refer to a financial institution or a river bank (Barnard et al., 2003; Loeff et al., 2006; Saenko and Darrell, 2009; Chen et al., 2015). Recently, Visual Sense Disambiguation has also been attempted for ambiguous verbs like the word ‘play’ which could refer to playing a musical instrument or playing a sport (Gella et al., 2016). In Machine Translation, including Multimodal Machine 3810 Translation, disambiguation of word sense happens implicitly. For instance, in the same example “A man is holding a seal”, we would come to know whether the system disambiguated the correct sense of the word seal only indirectly from the translation produced by the system. The corresponding translation of the word seal in the target language (Siegel or Seehund in German) acts as a “sense label”. Further, in Multimodal Machine Translation, we would like know which modality (visual or textual) contributed to the disambiguation and to wh"
L18-1602,P07-2045,0,0.00698389,"Missing"
L18-1602,2005.mtsummit-papers.11,0,0.00918372,"better word alignment in the later step. 2.1.2. Word Alignment After the pre-processing step, the word tokens in the Multi30K parallel corpus are aligned using Fast Align5 (Dyer et al., 2013). Fast Align generates asymmetric word alignments depending on which language in the parallel corpus is treated as the source. We generate both alignments - ‘forward’ (where English is treated as the source language) and ‘reverse’ (where German or French is treated as the source language). To learn better word alignments, we train Fast Align on a larger corpus comprising of the Europarl parallel corpus6 (Koehn, 2005) in addition to the Multi30K parallel corpus for the English-German and English-French language pairs separately. The Europarl corpus also undergoes the same pre-processing steps in Section 2.1.1. before word alignment. 2.1.3. Automatic Filtering In this step we remove all the word alignments having stop words and select only those alignments which are to be found in both ‘forward’ and ‘reverse’ directions. In addition, we filter out the alignments between words with different Part-Of-Speech (POS) tags (using the NLP tool in footnote 4). Next, we remove all English words that get aligned to a"
L18-1602,P06-2071,0,0.234902,"ambiguation can be found in Navigli (2009) and Raganato et al. (2017). In standard Word Sense Disambiguation, words are disambiguated based on their textual context. However, in a multimodal setting we could also disambiguate words using visual context. This modified version of Word Sense Disambiguation that uses visual context instead of textual context is called Visual Sense Disambiguation. In monolingual work, Visual Sense Disambiguation has previously been attempted for ambiguous nouns like the word ‘bank’ which could refer to a financial institution or a river bank (Barnard et al., 2003; Loeff et al., 2006; Saenko and Darrell, 2009; Chen et al., 2015). Recently, Visual Sense Disambiguation has also been attempted for ambiguous verbs like the word ‘play’ which could refer to playing a musical instrument or playing a sport (Gella et al., 2016). In Machine Translation, including Multimodal Machine 3810 Translation, disambiguation of word sense happens implicitly. For instance, in the same example “A man is holding a seal”, we would come to know whether the system disambiguated the correct sense of the word seal only indirectly from the translation produced by the system. The corresponding translat"
L18-1602,D17-1120,0,0.0362146,"“Ein Mann h¨alt ein Siegel”, and (2) “Ein Mann h¨alt einen Seehund”. The images (Figure 2) could help a Multimodal Machine Translation system disambiguate the correct sense of the word seal and translate accordingly. Disambiguation of word senses, popularly known as Word Sense Disambiguation or Lexical Disambiguation, is a widely studied natural language processing task. Given an ambiguous word and its context, the objective is to assign the correct sense of the word based on a pre-defined sense inventory. A review of approaches to Word Sense Disambiguation can be found in Navigli (2009) and Raganato et al. (2017). In standard Word Sense Disambiguation, words are disambiguated based on their textual context. However, in a multimodal setting we could also disambiguate words using visual context. This modified version of Word Sense Disambiguation that uses visual context instead of textual context is called Visual Sense Disambiguation. In monolingual work, Visual Sense Disambiguation has previously been attempted for ambiguous nouns like the word ‘bank’ which could refer to a financial institution or a river bank (Barnard et al., 2003; Loeff et al., 2006; Saenko and Darrell, 2009; Chen et al., 2015). Rec"
L18-1602,N16-1075,0,0.0267664,"d French) by human translators (i is an integer index ranging from 1 to 31,014). From this sentence-level dataset, we extract the ambiguous words and their lexical translations using the following steps: Pre-processing → Word Alignment → Automatic Filtering → Human Filtering. 2.1.1. Pre-processing Sentences in all languages are lowercased and tokenized using scripts from the Moses toolkit2 (Koehn et al., 1 2007). German sentences, which can contain compound words like ‘sonnenblumenkerne’ (sunflower seeds), are split/decompounded using pre-computed model of SEmantic COmpound Splitter (SECOS)3 (Riedl and Biemann, 2016). Since we are not interested in distinguishing morphological variants of the words, we also lemmatized4 all sentences in the respective languages, which reduced vocabulary size and led to better word alignment in the later step. 2.1.2. Word Alignment After the pre-processing step, the word tokens in the Multi30K parallel corpus are aligned using Fast Align5 (Dyer et al., 2013). Fast Align generates asymmetric word alignments depending on which language in the parallel corpus is treated as the source. We generate both alignments - ‘forward’ (where English is treated as the source language) and"
L18-1602,W16-2346,1,0.9055,"Missing"
L18-1602,Q14-1006,0,0.0111618,"Machine Translation using the MLT Dataset. We build this resource for English to German and English to French translations. 2. Language Resource - MLT Dataset The MLT Dataset is a collection of 4-tuples of the form: {(xi , yi , xi , vi )}ni=1 (1) 1 where xi is an ambiguous word, xi is its textual context (a source sentence), vi is its visual context (an image), and yi is its translation that conforms with both the textual and visual contexts. 2.1. Generating the MLT Dataset We make use of the Multi30K dataset (Elliott et al., 2016; Elliott et al., 2017), an extension of the Flickr30K dataset (Young et al., 2014), which consists of 31,014 triples of the form (vi , xi , yi ) where vi is an image, xi is a description of the image in the source language (English) and yi is a translation of the description in the target language (German and French) by human translators (i is an integer index ranging from 1 to 31,014). From this sentence-level dataset, we extract the ambiguous words and their lexical translations using the following steps: Pre-processing → Word Alignment → Automatic Filtering → Human Filtering. 2.1.1. Pre-processing Sentences in all languages are lowercased and tokenized using scripts from"
L18-1685,P11-2087,0,0.00887031,"ification (TS) is the task of reducing lexical and/or structural complexity of texts (Siddharthan, 2004). It is common to divide this task in two: lexical simplification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan an"
L18-1685,bott-etal-2012-text,0,0.147002,"focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplificat"
L18-1685,W14-1206,0,0.0470791,"Missing"
L18-1685,W09-2105,1,0.911308,"Missing"
L18-1685,E99-1042,0,0.257767,"ntroduction Text simplification (TS) is the task of reducing lexical and/or structural complexity of texts (Siddharthan, 2004). It is common to divide this task in two: lexical simplification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et"
L18-1685,W11-1601,0,0.136429,"l et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not be sufficient to model operations for specifi"
L18-1685,P15-2011,0,0.0783328,"Missing"
L18-1685,P14-2075,0,0.0215017,"implification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations joint"
L18-1685,P14-1041,0,0.258899,"al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not be sufficient to model operations for specific domains. In terms of specific corpora for TS e"
L18-1685,P17-2014,0,0.261149,"Missing"
L18-1685,W13-4813,1,0.897804,"in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain"
L18-1685,L16-1491,1,0.915491,"004). It is common to divide this task in two: lexical simplification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches te"
L18-1685,N16-1050,1,0.921091,"004). It is common to divide this task in two: lexical simplification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches te"
L18-1685,W16-4912,0,0.0353036,"004). It is common to divide this task in two: lexical simplification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches te"
L18-1685,E17-2006,1,0.871376,"and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main pa"
L18-1685,I17-3007,1,0.734194,"ng passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish a"
L18-1685,E14-1076,0,0.018032,"de simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not b"
L18-1685,W11-2802,0,0.14289,"ement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela"
L18-1685,D11-1038,0,0.0952986,"S has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover"
L18-1685,P12-1107,0,0.437046,"Missing"
L18-1685,Q16-1029,0,0.334396,"proaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not be sufficient to model operations for specific domains. In terms of specific corpora for TS evaluation, only a"
L18-1685,D17-1062,0,0.0586889,"word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not be sufficient to model operations for specific domains. In terms of specific corpora for TS evaluation, only a few exist, mostly for E"
L18-1685,C10-1152,0,0.727031,"approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not be sufficient to mo"
logacheva-specia-2014-quality,ambati-etal-2010-active,0,\N,Missing
logacheva-specia-2014-quality,D10-1061,0,\N,Missing
logacheva-specia-2014-quality,W10-1723,0,\N,Missing
logacheva-specia-2014-quality,W07-1516,0,\N,Missing
logacheva-specia-2014-quality,P07-1052,0,\N,Missing
logacheva-specia-2014-quality,P02-1040,0,\N,Missing
logacheva-specia-2014-quality,N09-1047,0,\N,Missing
logacheva-specia-2014-quality,P10-1088,0,\N,Missing
logacheva-specia-2014-quality,P13-4014,1,\N,Missing
logacheva-specia-2014-quality,potet-etal-2012-collection,0,\N,Missing
logacheva-specia-2014-quality,2005.iwslt-1.7,0,\N,Missing
logacheva-specia-2014-quality,2009.mtsummit-papers.8,0,\N,Missing
logacheva-specia-2014-quality,P07-1033,0,\N,Missing
logacheva-specia-2014-quality,W13-3501,0,\N,Missing
N16-1050,P15-2011,0,0.145597,"Missing"
N16-1050,P14-2118,0,0.0590501,"ound that humans When quantified, these aspects can be used as features for various Natural Language Processing (NLP) tasks. The Lexical Simplification approach in (Jauhar and Specia, 2012) is an example. By combining various collocational features and psycholinguistic measures extracted from the MRC Psycholinguistic Database (Coltheart, 1981), they trained a ranker (Joachims, 2002) that reached first place in the English Lexical Simplification task at SemEval 2012. Semantic Classification tasks have also benefited from the use of such features: by combining Concreteness with other features, (Hill and Korhonen, 2014) reached the state-of-theart performance in Semantic Composition (denotative/connotative) and Semantic Modification (intersective/subsective) prediction. Despite the evident usefulness of psycholinguistic properties of words, resources describing such properties are rare. The most extensively developed resource for English is the MRC Psycholinguistic Database (Section 2). However, it is far from complete, most likely due to the inherent cost of manually entering such properties. In this paper we propose a method to automatically infer these missing properties. We train regressors by performing"
N16-1050,P14-2075,0,0.0519683,"vised ranking algorithms capture word simplicity. Using the parameters described in Section 4, we train bootstrappers for these two properties using all instances in the MRC Database as seeds. We then train three rankers with (W) and without (W/O) psycholinguistic features: • Age of Acquisition: 700 word vector dimensions with a CBOW model, and ζ = 0.7. • Concreteness: 1,100 word vector dimensions with a Skip-Gram model, and ζ = 0.7. • Imagery: 1,100 word vector dimensions with a Skip-Gram model, and ζ = 0.7. 438 5 Psycholinguistic Features for LS 4 http://ghpaetzold.github.io/subimdb • Horn (Horn et al., 2014): Uses an SVM ranker trained on various n-gram probability features. ˇ • Glavas (Glavaˇs and Stajner, 2015): Ranks candidates using various collocational and semantic metrics, and then re-ranks them according to their average rankings. • Paetzold (Paetzold and Specia, 2015): Ranks words according to their distance to a decision boundary learned from a classification setup inferred from ranking examples. Uses n-gram frequencies as features. We use data from the English Lexical Simplification task of SemEval 2012 to assess systems’ performance. The goal of the task is to rank words in different"
N16-1050,S12-1066,1,0.939075,"life. Other examples of psycholinguistic properties, such as Familiarity and Concreteness, influence one’s proficiency in word recognition and text comprehension. The experiments in (Connine et al., 1990; Morrel-Samuels and Krauss, 1992) show that words with high Familiarity yield lower reaction times in both visual and auditory lexical decision, and require less hand gesticulation in order to be described. Begg and Paivio (1969) found that humans When quantified, these aspects can be used as features for various Natural Language Processing (NLP) tasks. The Lexical Simplification approach in (Jauhar and Specia, 2012) is an example. By combining various collocational features and psycholinguistic measures extracted from the MRC Psycholinguistic Database (Coltheart, 1981), they trained a ranker (Joachims, 2002) that reached first place in the English Lexical Simplification task at SemEval 2012. Semantic Classification tasks have also benefited from the use of such features: by combining Concreteness with other features, (Hill and Korhonen, 2014) reached the state-of-theart performance in Semantic Composition (denotative/connotative) and Semantic Modification (intersective/subsective) prediction. Despite the"
N16-1050,P13-1151,0,0.0512709,"revious work for this task, in these experiments, we compare the performance of our bootstrapping strategy to various baselines. For training, we use the Ridge regression algorithm (Tikhonov, 1963). As features, our regressor uses the word’s raw embedding values, along with the following 15 lexical features: • Word’s length and number of syllables, as determined by the Morph Adorner module of LEXenstein (Paetzold and Specia, 2015). • Word’s frequency in the Brown (Francis and Kucera, 1979), SUBTLEX (Brysbaert and New, 2009), SubIMDB (Paetzold and Specia, 2016), Wikipedia and Simple Wikipedia (Kauchak, 2013) corpora. • Number of senses, synonyms, hypernyms and hyponyms for word in WordNet (Fellbaum, 1998). • Minimum, maximum and average distance between the word’s senses in WordNet and the thesaurus’ root sense. • Number of images found for word in the Getty Images database1 . We train our embedding models using word2vec (Mikolov et al., 2013a) over a corpus of 7 billion words composed by the SubIMDB corpus, UMBC webbase2 , News Crawl3 , SUBTLEX (Brysbaert and New, 2009), Wikipedia and Simple Wikipedia (Kauchak, 2013). We use 5-fold cross-validation to optimise parameters: ζ, embeddings model arc"
N16-1050,N13-1090,0,0.245663,"one instance was added to S, go to step 2, otherwise, return the resulting classifier. One critical difference between this approach and ours is that our task requires regression algorithms instead of classifiers. In classification, the prediction confidence c is often calculated as the maximum signed distance between an instance and the estimated hyperplanes. There is, however, no analogous confidence estimation technique for regression problems. We address this problem by using word embedding models. Embedding models have been proved effective in capturing linguistic regularities of words (Mikolov et al., 2013b). In order to exploit these regularities, we assume that the quality of a regressor’s prediction on an instance is directly proportional to how similar the instance is to the ones in the labelled set. Since the input for the regressors are words, we compute the similarity between a test word and the words in the labelled dataset as the maximum cosine similarity between the test word’s vector and the vectors in the labelled set. Let M be an embeddings model trained over vocabulary V , S a set of training seeds, ζ a minimum confidence threshold, sim(w, S, M ) the maximum cosine similarity betw"
N16-1050,P15-4015,1,0.939037,"resources, we were only able to predict the Familiarity, Age of Acquisition, Concreteness and Imagery values of the remaining 85,942 words in MRC. 4 Evaluation Since we were not able to find previous work for this task, in these experiments, we compare the performance of our bootstrapping strategy to various baselines. For training, we use the Ridge regression algorithm (Tikhonov, 1963). As features, our regressor uses the word’s raw embedding values, along with the following 15 lexical features: • Word’s length and number of syllables, as determined by the Morph Adorner module of LEXenstein (Paetzold and Specia, 2015). • Word’s frequency in the Brown (Francis and Kucera, 1979), SUBTLEX (Brysbaert and New, 2009), SubIMDB (Paetzold and Specia, 2016), Wikipedia and Simple Wikipedia (Kauchak, 2013) corpora. • Number of senses, synonyms, hypernyms and hyponyms for word in WordNet (Fellbaum, 1998). • Minimum, maximum and average distance between the word’s senses in WordNet and the thesaurus’ root sense. • Number of images found for word in the Getty Images database1 . We train our embedding models using word2vec (Mikolov et al., 2013a) over a corpus of 7 billion words composed by the SubIMDB corpus, UMBC webbas"
N16-1050,W16-4912,0,0.102431,"words in MRC. 4 Evaluation Since we were not able to find previous work for this task, in these experiments, we compare the performance of our bootstrapping strategy to various baselines. For training, we use the Ridge regression algorithm (Tikhonov, 1963). As features, our regressor uses the word’s raw embedding values, along with the following 15 lexical features: • Word’s length and number of syllables, as determined by the Morph Adorner module of LEXenstein (Paetzold and Specia, 2015). • Word’s frequency in the Brown (Francis and Kucera, 1979), SUBTLEX (Brysbaert and New, 2009), SubIMDB (Paetzold and Specia, 2016), Wikipedia and Simple Wikipedia (Kauchak, 2013) corpora. • Number of senses, synonyms, hypernyms and hyponyms for word in WordNet (Fellbaum, 1998). • Minimum, maximum and average distance between the word’s senses in WordNet and the thesaurus’ root sense. • Number of images found for word in the Getty Images database1 . We train our embedding models using word2vec (Mikolov et al., 2013a) over a corpus of 7 billion words composed by the SubIMDB corpus, UMBC webbase2 , News Crawl3 , SUBTLEX (Brysbaert and New, 2009), Wikipedia and Simple Wikipedia (Kauchak, 2013). We use 5-fold cross-validation"
N16-1050,N15-2002,1,0.839505,"Paetzold (Paetzold and Specia, 2015): Ranks words according to their distance to a decision boundary learned from a classification setup inferred from ranking examples. Uses n-gram frequencies as features. We use data from the English Lexical Simplification task of SemEval 2012 to assess systems’ performance. The goal of the task is to rank words in different contexts according to their simplicity. The training and test sets contain 300 and 1,710 instances, respectively. The official metric from the task – TRank (Specia et al., 2012) – is used to measure systems’ performance. As discussed in (Paetzold, 2015), this metric best represents LS performance in practice. The results in Table 2 show that the addition of our features lead to performance increases with all rankers. Performing F-tests over the rankings estimated for the simplest candidate in each instance, we have found these differences to be statistically significant (p < 0.05). Using our features, the Paetzold ranker reaches the best published results for the dataset, significantly superior to the best system in SemEval (Jauhar and Specia, 2012). Ranker Best SemEval Horn Glavas Paetzold TRank W/O W 0.602 0.625 0.635 0.623 0.636 0.653 0.6"
N16-1050,S12-1046,1,0.875178,"semantic metrics, and then re-ranks them according to their average rankings. • Paetzold (Paetzold and Specia, 2015): Ranks words according to their distance to a decision boundary learned from a classification setup inferred from ranking examples. Uses n-gram frequencies as features. We use data from the English Lexical Simplification task of SemEval 2012 to assess systems’ performance. The goal of the task is to rank words in different contexts according to their simplicity. The training and test sets contain 300 and 1,710 instances, respectively. The official metric from the task – TRank (Specia et al., 2012) – is used to measure systems’ performance. As discussed in (Paetzold, 2015), this metric best represents LS performance in practice. The results in Table 2 show that the addition of our features lead to performance increases with all rankers. Performing F-tests over the rankings estimated for the simplest candidate in each instance, we have found these differences to be statistically significant (p < 0.05). Using our features, the Paetzold ranker reaches the best published results for the dataset, significantly superior to the best system in SemEval (Jauhar and Specia, 2012). Ranker Best SemE"
N16-1050,P95-1026,0,0.660134,"te-of-theart performance in Semantic Composition (denotative/connotative) and Semantic Modification (intersective/subsective) prediction. Despite the evident usefulness of psycholinguistic properties of words, resources describing such properties are rare. The most extensively developed resource for English is the MRC Psycholinguistic Database (Section 2). However, it is far from complete, most likely due to the inherent cost of manually entering such properties. In this paper we propose a method to automatically infer these missing properties. We train regressors by performing bootstrapping (Yarowsky, 1995) over the existing features in the MRC database, exploiting word embedding models and other linguistic resources for that (Section 3). This approach outperform various strong baselines (Section 4) and the resulting properties lead to significant improvements when used in Lexical Simplification models (Section 5). 435 Proceedings of NAACL-HLT 2016, pages 435–440, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2 The MRC Psycholinguistic Database 3. Predict values for a set of unlabelled instances U . Introduced by Coltheart (1981), the MRC (Machine Read"
N16-1069,W13-2241,1,0.906874,"g models for individual translators is a sensible decision when large amounts of data are available, the multitask learning approach can outperform these models by learning from data by multiple annotators. Additionally, besides having translators as “tasks”, we address the problem of learning from data for multiple language pairs. We devise our multitaslk approach within the Bayesian non-parametric machine learning framework of Gaussian Processes (Rasmussen and Williams, 2006). Gaussian Processes have shown very good results for quality estimation in previous 559 work (Cohn and Specia, 2013; Beck et al., 2013; Shah et al., 2013). Our datasets – annotated for postediting distance – contain nearly 100K data points, two orders of magnitude larger than those used in previous work. To cope with scalability issues resulting from the size of these datasets, we apply a sparse version of Gaussian Processes. We perform extensive experiments on this large-scale data aiming to answer the following research questions: • What is the best approach to build models to be used by individual translators? How much data is necessary to build independent models (one per translator) that can be as accurate as (or better"
N16-1069,D14-1190,1,0.87043,"ion environment. For that, they adapted an online passive-aggressive algorithm (Cavallanti et al., 2010) to the multitask scenario. While their setting is interesting and could be considered more challenging because of the online adaptation requirements, ours is different as we can take advantage of already having collected large volumes of data. 560 Multitask learning has also been used for other classification and regression tasks in language processing, mostly for domain adaptation (Daume III, 2007; Finkel and Manning, 2009), but also more recently for tasks such as multi-emotion analysis (Beck et al., 2014), where the each emotion explaining a text is defined as a task. However, in all previous work the focus has been on addressing task variance coupled with data scarcity, which makes them different from the work we describe in this paper. 3 Gaussian Processes Gaussian Processes (GPs) (Rasmussen and Williams, 2006) are a Bayesian non-parametric machine learning framework considered the stateof-the-art for regression. GPs have been used successfully for MT quality prediction (Cohn and Specia, 2013; Beck et al., 2013; Shah et al., 2013), among other tasks. GPs assume the presence of a latent funct"
N16-1069,W15-3001,1,0.923711,"ing the postediting of MT output. QE models can provide translators with information on how much editing/time will be necessary to fix a given segment, or on whether it is worth editing it at all, as opposed to translating it from scratch. For this application, models are learnt from quality annotations that reflect post-editing effort, for instance, 1-5 judgements on estimated post-editing effort (Callison-Burch et al., 2012) or actual post-editing effort measured as post-editing time (Bojar et al., 2013) or edit distance between the MT output and its post-edited version (Bojar et al., 2014; Bojar et al., 2015). Multitask learning has been proven a useful technique in a number of Natural Language Processing applications where data is scarce and naturally diverse. Examples include learning from data of different domains and learning from labels provided by multiple annotators. Tasks in these scenarios would be the domains or the annotators. When faced with limited data for each task, a framework for the learning of tasks in parallel while using a shared representation is clearly helpful: what is learned for a given task can be transferred to other tasks while the peculiarities of each task are still"
N16-1069,P13-1004,1,0.525106,"eedings of NAACL-HLT 2016, pages 558–567, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics translators on the same data points, as was done in the first shared task on the topic (Callison-Burch et al., 2012); (ii) models are built for individual translators by collecting labelled data for each translator (Shah and Specia, 2014); and (iii) models are built using multitask learning techniques (Caruana, 1997) to put together annotations from multiple translators while keeping track of the translators’ identification to account for their individual biases (Cohn and Specia, 2013; de Souza et al., 2015). The first approach is sensible because, in the limit, the models built should reflect the “average” strategies/preferences of translators. However, its cost makes it prohibitive. The second approach can lead to very accurate models but it requires sufficient training data for each translator, and that all translators are known at model building time. The last approach is very attractive. It is a transfer learning (a.k.a. domain-adaptation) approach that allows the modelling of data from each individual translator while also modelling correlations between translators s"
N16-1069,P07-1033,0,0.18237,"Missing"
N16-1069,C14-1040,0,0.09383,"Missing"
N16-1069,2014.amta-workshop.2,0,0.484567,"Missing"
N16-1069,P15-1022,0,0.0679008,"Missing"
N16-1069,N09-1068,0,0.0692845,"Missing"
N16-1069,W12-3123,0,0.021436,"d machine learning problem using various features indicating fluency, adequacy and complexity of the source-target text pair, and annotations on translation quality given by human translators. Various kernel-based regression and classification algorithms have been explored to learn prediction models. One of the biggest challenges in this field is to deal with the inherent subjectivity of quality labels given by humans. Explicit judgements (e.g. the 1-5 point scale) are affected the most, with previous work showing that translators’ perception of post-editing effort differs from actual effort (Koponen, 2012). However, even objective annotations of actual post-editing effort are subject to natural variance. Take, for example, post-editing time as a label: Different annotators have different typing speeds and may require more or less time to deal with the same edits depending on their level of experience, familiarity with the domain, etc. Post-editing distance also varies across translators as there are often multiple ways of producing a good quality translation from an MT output, even when strict guidelines are given. In order to address variance among multiple translators, three strategies have b"
N16-1069,2011.eamt-1.2,0,0.109265,"Missing"
N16-1069,2014.eamt-1.22,1,0.559783,"ucing a good quality translation from an MT output, even when strict guidelines are given. In order to address variance among multiple translators, three strategies have been applied: (i) models are built by averaging annotations from multiple 558 Proceedings of NAACL-HLT 2016, pages 558–567, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics translators on the same data points, as was done in the first shared task on the topic (Callison-Burch et al., 2012); (ii) models are built for individual translators by collecting labelled data for each translator (Shah and Specia, 2014); and (iii) models are built using multitask learning techniques (Caruana, 1997) to put together annotations from multiple translators while keeping track of the translators’ identification to account for their individual biases (Cohn and Specia, 2013; de Souza et al., 2015). The first approach is sensible because, in the limit, the models built should reflect the “average” strategies/preferences of translators. However, its cost makes it prohibitive. The second approach can lead to very accurate models but it requires sufficient training data for each translator, and that all translators are"
N16-1069,2013.mtsummit-papers.21,1,0.952221,"dual translators is a sensible decision when large amounts of data are available, the multitask learning approach can outperform these models by learning from data by multiple annotators. Additionally, besides having translators as “tasks”, we address the problem of learning from data for multiple language pairs. We devise our multitaslk approach within the Bayesian non-parametric machine learning framework of Gaussian Processes (Rasmussen and Williams, 2006). Gaussian Processes have shown very good results for quality estimation in previous 559 work (Cohn and Specia, 2013; Beck et al., 2013; Shah et al., 2013). Our datasets – annotated for postediting distance – contain nearly 100K data points, two orders of magnitude larger than those used in previous work. To cope with scalability issues resulting from the size of these datasets, we apply a sparse version of Gaussian Processes. We perform extensive experiments on this large-scale data aiming to answer the following research questions: • What is the best approach to build models to be used by individual translators? How much data is necessary to build independent models (one per translator) that can be as accurate as (or better than) models using"
N16-1069,2006.amta-papers.25,0,0.178727,"Missing"
N16-1069,P13-4014,1,0.891075,"Missing"
N18-1198,W11-0326,0,\N,Missing
N18-1198,W14-3348,0,\N,Missing
N18-1198,S14-1015,0,\N,Missing
N18-2069,N15-1053,0,0.0713271,"Missing"
N18-2069,W16-3203,0,0.0310197,"k. We also measure the upperbound performance of our models using gold standard annotations. Our analysis reveals that the simpler model performs well even without image information, suggesting that the dataset contains strong linguistic bias. 1 Introduction Models tackling vision-to-language (V2L) tasks, for example Image Captioning (IC) and Visual Question Answering (VQA), have demonstrated impressive results in recent years in terms of automatic metric scores. However, whether or not these models are actually learning to address the tasks they are designed for is questionable. For example, Hodosh and Hockenmaier (2016) showed that IC models do not understand images sufficiently, as reflected by the generated captions. As a consequence, in the last few years many diagnostic tasks and datasets have been proposed aiming at investigating the capabilities of such models in more detail to determine whether and how these models are capable of exploiting visual and/or linguistic information (Shekhar et al., 2017b; John1. A model (Section 3) for foiled captions classification using a simple and interpretable 433 Proceedings of NAACL-HLT 2018, pages 433–438 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association"
N18-2069,N16-3020,0,0.120998,"Missing"
N18-2069,W17-6938,0,0.135115,"al., 2015; Chen et al., 2015; Gao et al., 2015; Yu et al., 2015; Zhu et al., 2016). FOIL (Shekhar et al., 2017b) is one such dataset. It was proposed to evaluate the ability of V2L models in understanding the interplay of objects and their attributes in the images and their relations in an image captioning framework. This is done by replacing a word in MSCOCO (Lin et al., 2014) captions with a ‘foiled’ word that is semantically similar or related to the original word (substituting dog with cat), thus rendering the image caption unfaithful to the image content, while yet linguistically valid. Shekhar et al. (2017b) report poor performance for V2L models in classifying captions as foiled (or not). They suggested that their models (using image embeddings as input) are very poor at encoding structured visuallinguistic information to spot the mismatch between a foiled caption and the corresponding content depicted in the image. In this paper, we focus on the foiled captions classification task (Section 2), and propose the use of explicit object detections as salient image cues for solving the task. In contrast to methods from previous work that make use of word based information extracted from captions (H"
N18-2069,P17-1024,0,0.41494,"al., 2015; Chen et al., 2015; Gao et al., 2015; Yu et al., 2015; Zhu et al., 2016). FOIL (Shekhar et al., 2017b) is one such dataset. It was proposed to evaluate the ability of V2L models in understanding the interplay of objects and their attributes in the images and their relations in an image captioning framework. This is done by replacing a word in MSCOCO (Lin et al., 2014) captions with a ‘foiled’ word that is semantically similar or related to the original word (substituting dog with cat), thus rendering the image caption unfaithful to the image content, while yet linguistically valid. Shekhar et al. (2017b) report poor performance for V2L models in classifying captions as foiled (or not). They suggested that their models (using image embeddings as input) are very poor at encoding structured visuallinguistic information to spot the mismatch between a foiled caption and the corresponding content depicted in the image. In this paper, we focus on the foiled captions classification task (Section 2), and propose the use of explicit object detections as salient image cues for solving the task. In contrast to methods from previous work that make use of word based information extracted from captions (H"
N18-5015,W14-3348,0,0.0310595,"cted using powerful search and selection functions and results can be visualised with graphical representations of the scores and distributions. 1 Introduction Automatic evaluation of Machine Translation (MT) hypotheses is key for system development and comparison. Even though human assessment ultimately provides more reliable and insightful information, automatic evaluation is faster, cheaper, and often considered more consistent. Many metrics have been proposed for MT that compare system translations against human references, with the most popular being BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006), and, more recently, BEER (Stanojevic and Sima’an, 2014). These and other automatic metrics are often criti2 Related Tools Several tools have been developed to visualise the output of MT evaluation metrics that go beyond displaying just single scores and/or a few statistics. Despite its criticisms and limitations, BLEU is still regarded as the de facto evaluation metric used for rating and comparing MT systems. It was one of the earliest metrics to assert a high enough correlation with human judgments. Interactive BLEU (iBleu) (Madnani, 2011) is 71 Proceedings of NA"
N18-5015,P02-1040,0,0.112123,"ed sentences can easily be inspected using powerful search and selection functions and results can be visualised with graphical representations of the scores and distributions. 1 Introduction Automatic evaluation of Machine Translation (MT) hypotheses is key for system development and comparison. Even though human assessment ultimately provides more reliable and insightful information, automatic evaluation is faster, cheaper, and often considered more consistent. Many metrics have been proposed for MT that compare system translations against human references, with the most popular being BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006), and, more recently, BEER (Stanojevic and Sima’an, 2014). These and other automatic metrics are often criti2 Related Tools Several tools have been developed to visualise the output of MT evaluation metrics that go beyond displaying just single scores and/or a few statistics. Despite its criticisms and limitations, BLEU is still regarded as the de facto evaluation metric used for rating and comparing MT systems. It was one of the earliest metrics to assert a high enough correlation with human judgments. Interactive BLEU (iBleu) (Ma"
N18-5015,W14-3354,0,0.0586956,"Missing"
N18-5015,zhang-etal-2004-interpreting,0,0.110555,"Missing"
N19-1422,W18-6402,1,0.82762,"d into the model. 1 Pranava Madhyastha Imperial College London pranava@imperial.ac.uk Introduction Multimodal Machine Translation (MMT) aims at designing better translation systems which take into account auxiliary inputs such as images. Initially organized as a shared task within the First Conference on Machine Translation (WMT16) (Specia et al., 2016), MMT has so far been studied using the Multi30K dataset (Elliott et al., 2016), a multilingual extension of Flickr30K (Young et al., 2014) with translations of the English image descriptions into German, French and Czech (Elliott et al., 2017; Barrault et al., 2018). The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y and Helcl, 2017; Helcl et al., 2018) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016; Gr¨onroos et al., 2018). Nevertheless, the conclusion about the contributio"
N19-1422,W17-4746,1,0.83507,"Missing"
N19-1422,W18-6438,1,0.873621,"Missing"
N19-1422,W16-2359,0,0.254545,"ts such as images. Initially organized as a shared task within the First Conference on Machine Translation (WMT16) (Specia et al., 2016), MMT has so far been studied using the Multi30K dataset (Elliott et al., 2016), a multilingual extension of Flickr30K (Young et al., 2014) with translations of the English image descriptions into German, French and Czech (Elliott et al., 2017; Barrault et al., 2018). The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y and Helcl, 2017; Helcl et al., 2018) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016; Gr¨onroos et al., 2018). Nevertheless, the conclusion about the contribution of the visual modality is still unclear: Gr¨onroos et al. (2018) consider their multimodal gains “modest” and attribute the largest gain to the usage of external parallel corpora. Lala et al. (2018) observe that the"
N19-1422,D17-1105,0,0.67641,"T has so far been studied using the Multi30K dataset (Elliott et al., 2016), a multilingual extension of Flickr30K (Young et al., 2014) with translations of the English image descriptions into German, French and Czech (Elliott et al., 2017; Barrault et al., 2018). The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y and Helcl, 2017; Helcl et al., 2018) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016; Gr¨onroos et al., 2018). Nevertheless, the conclusion about the contribution of the visual modality is still unclear: Gr¨onroos et al. (2018) consider their multimodal gains “modest” and attribute the largest gain to the usage of external parallel corpora. Lala et al. (2018) observe that their multimodal word-sense disambiguation approach is not significantly different than the monomodal counterpart. The organizers of the latest"
N19-1422,D14-1179,0,0.0654888,"Missing"
N19-1422,P11-2031,0,0.0378663,"encoder/decoder outputs, respectively (Srivastava et al., 2014). The weights are decayed with a factor of 1e−5. We use ADAM (Kingma and Ba, 2014) with a learning rate of 4e−4 and mini-batches of 64 samples. The gradients are clipped if the total norm exceeds 1 (Pascanu et al., 2013). The training is early-stopped if dev set METEOR (Denkowski and Lavie, 2014) does not improve for ten epochs. All experiments are conducted with nmtpytorch1 (Caglayan et al., 2017b). 4 We train all systems three times each with different random initialization in order to perform significance testing with multeval (Clark et al., 2011). Throughout the section, we always report the mean over three runs (and the standard deviation) of the considered metrics. We decode the translations with a beam size of 12. github.com/lium-lst/nmtpytorch We first present test2017 METEOR scores for the baseline NMT and MMT systems, when trained on the full dataset D (Table 2). The first column indicates that, although MMT models perform slightly better on average, they are not significantly better than the baseline NMT. We now introduce and discuss the results obtained under the proposed degradation schemes. Please refer to Table 5 and the ap"
N19-1422,W14-3348,0,0.0692332,"encoder and decoder GRUs have 400 hidden units and are initialized with 0 except the multimodal INIT system. All embeddings are 200-dimensional and the decoder embeddings are tied (Press and Wolf, 2016). A dropout of 0.4 and 0.5 is applied on source embeddings and encoder/decoder outputs, respectively (Srivastava et al., 2014). The weights are decayed with a factor of 1e−5. We use ADAM (Kingma and Ba, 2014) with a learning rate of 4e−4 and mini-batches of 64 samples. The gradients are clipped if the total norm exceeds 1 (Pascanu et al., 2013). The training is early-stopped if dev set METEOR (Denkowski and Lavie, 2014) does not improve for ten epochs. All experiments are conducted with nmtpytorch1 (Caglayan et al., 2017b). 4 We train all systems three times each with different random initialization in order to perform significance testing with multeval (Clark et al., 2011). Throughout the section, we always report the mean over three runs (and the standard deviation) of the considered metrics. We decode the translations with a beam size of 12. github.com/lium-lst/nmtpytorch We first present test2017 METEOR scores for the baseline NMT and MMT systems, when trained on the full dataset D (Table 2). The first c"
N19-1422,D18-1329,0,0.450816,"ion about the contribution of the visual modality is still unclear: Gr¨onroos et al. (2018) consider their multimodal gains “modest” and attribute the largest gain to the usage of external parallel corpora. Lala et al. (2018) observe that their multimodal word-sense disambiguation approach is not significantly different than the monomodal counterpart. The organizers of the latest edition of the shared task concluded that the multimodal integration schemes explored so far resulted in marginal changes in terms of automatic metrics and human evaluation (Barrault et al., 2018). In a similar vein, Elliott (2018) demonstrated that MMT models can translate without significant performance losses even in the presence of features from unrelated images. These empirical findings seem to indicate that images are ignored by the models and hint at the fact that this is due to representation or modeling limitations. We conjecture that the most plausible reason for the linguistic dominance is that – at least in Multi30K – the source text is sufficient to perform the translation, eventually preventing the visual information from intervening in the learning process. To investigate this hypothesis, we introduce sev"
N19-1422,W17-4718,1,0.847898,"way they are integrated into the model. 1 Pranava Madhyastha Imperial College London pranava@imperial.ac.uk Introduction Multimodal Machine Translation (MMT) aims at designing better translation systems which take into account auxiliary inputs such as images. Initially organized as a shared task within the First Conference on Machine Translation (WMT16) (Specia et al., 2016), MMT has so far been studied using the Multi30K dataset (Elliott et al., 2016), a multilingual extension of Flickr30K (Young et al., 2014) with translations of the English image descriptions into German, French and Czech (Elliott et al., 2017; Barrault et al., 2018). The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y and Helcl, 2017; Helcl et al., 2018) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016; Gr¨onroos et al., 2018). Nevertheless, the conclusi"
N19-1422,W16-3210,1,0.827773,"Missing"
N19-1422,W18-6439,0,0.123297,"Missing"
N19-1422,W18-6441,0,0.340492,"Missing"
N19-1422,W16-2360,0,0.228592,"image descriptions into German, French and Czech (Elliott et al., 2017; Barrault et al., 2018). The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y and Helcl, 2017; Helcl et al., 2018) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016; Gr¨onroos et al., 2018). Nevertheless, the conclusion about the contribution of the visual modality is still unclear: Gr¨onroos et al. (2018) consider their multimodal gains “modest” and attribute the largest gain to the usage of external parallel corpora. Lala et al. (2018) observe that their multimodal word-sense disambiguation approach is not significantly different than the monomodal counterpart. The organizers of the latest edition of the shared task concluded that the multimodal integration schemes explored so far resulted in marginal changes in terms of automatic metrics and human eva"
N19-1422,P07-2045,0,0.00557821,"rating the visual modality would likely deteriorate in terms of metrics. 3 Experimental Setup Dataset. We conduct experiments on the English→French part of Multi30K. The models are trained on the concatenation of the train and val sets (30K sentences) whereas test2016 (dev) and test2017 (test) are used for early-stopping and model evaluation, respectively. For entity masking, we revert to the default Flickr30K splits and perform the model evaluation on test2016, since test2017 is not annotated for entities. We use word-level vocabularies of 9,951 English and 11,216 French words. We use Moses (Koehn et al., 2007) scripts to lowercase, normalize and tokenize the sentences with hyphen splitting. The hyphens are stitched back prior to evaluation. Visual Features. We use a ResNet-50 CNN (He et al., 2016) trained on ImageNet (Deng et al., 2009) as image encoder. Prior to feature extraction, we center and standardize the images using ImageNet statistics, resize the shortest edge to 256 pixels and take a center crop of size 256x256. We extract spatial features of size 2048x8x8 from the final convolutional layer and apply L2 normalization along the depth dimension (Caglayan et al., 2018). For the non-attentiv"
N19-1422,W18-6442,1,0.870079,"et al., 2016; Calixto et al., 2016; Libovick´y and Helcl, 2017; Helcl et al., 2018) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016; Gr¨onroos et al., 2018). Nevertheless, the conclusion about the contribution of the visual modality is still unclear: Gr¨onroos et al. (2018) consider their multimodal gains “modest” and attribute the largest gain to the usage of external parallel corpora. Lala et al. (2018) observe that their multimodal word-sense disambiguation approach is not significantly different than the monomodal counterpart. The organizers of the latest edition of the shared task concluded that the multimodal integration schemes explored so far resulted in marginal changes in terms of automatic metrics and human evaluation (Barrault et al., 2018). In a similar vein, Elliott (2018) demonstrated that MMT models can translate without significant performance losses even in the presence of features from unrelated images. These empirical findings seem to indicate that images are ignored by the"
N19-1422,P17-2031,0,0.3724,"Missing"
N19-1422,W17-4751,0,0.0226414,"ed using the Multi30K dataset (Elliott et al., 2016), a multilingual extension of Flickr30K (Young et al., 2014) with translations of the English image descriptions into German, French and Czech (Elliott et al., 2017; Barrault et al., 2018). The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y and Helcl, 2017; Helcl et al., 2018) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016; Gr¨onroos et al., 2018). Nevertheless, the conclusion about the contribution of the visual modality is still unclear: Gr¨onroos et al. (2018) consider their multimodal gains “modest” and attribute the largest gain to the usage of external parallel corpora. Lala et al. (2018) observe that their multimodal word-sense disambiguation approach is not significantly different than the monomodal counterpart. The organizers of the latest edition of the sh"
N19-1422,Q14-1006,0,\N,Missing
N19-1422,W16-2346,1,\N,Missing
N19-1422,W17-4752,1,\N,Missing
P06-3010,H05-1097,0,0.128464,"Missing"
P06-3010,dorr-katsova-1998-lexical,0,0.017704,"of MT systems, provided that such module is developed following specific requirements of MT, e.g., employing multilingual sense repositories. Differences between monolingual and multilingual WSD are very significant for MT, since it is concerned only with the ambiguities that 2 Related work Many approaches have been proposed for WSD, but only a few are designed for specific applications, such as MT. Existing multilingual approaches can be classified as (a) knowledge-based approaches, which make use of linguistic knowledge manually codified or extracted from lexical resources (Pedersen, 1997; Dorr and Katsova, 1998); (b) corpus-based approaches, which make use of knowledge automatically acquired from text using machine learning algorithms (Lee, 2002; Vickrey et al., 2005); and (c) hybrid approaches, which employ techniques from the two other approaches (Zinovjeva, 2000). 55 Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 55–60, c Sydney, July 2006. 2006 Association for Computational Linguistics Hybrid approaches potentially explore the advantages of both other strategies, yielding accurate and comprehensive systems. However, they are quite rare, even in monolingual contexts (Stevenson"
P06-3010,lee-2002-classification,0,0.0261338,"nces between monolingual and multilingual WSD are very significant for MT, since it is concerned only with the ambiguities that 2 Related work Many approaches have been proposed for WSD, but only a few are designed for specific applications, such as MT. Existing multilingual approaches can be classified as (a) knowledge-based approaches, which make use of linguistic knowledge manually codified or extracted from lexical resources (Pedersen, 1997; Dorr and Katsova, 1998); (b) corpus-based approaches, which make use of knowledge automatically acquired from text using machine learning algorithms (Lee, 2002; Vickrey et al., 2005); and (c) hybrid approaches, which employ techniques from the two other approaches (Zinovjeva, 2000). 55 Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 55–60, c Sydney, July 2006. 2006 Association for Computational Linguistics Hybrid approaches potentially explore the advantages of both other strategies, yielding accurate and comprehensive systems. However, they are quite rare, even in monolingual contexts (Stevenson and Wilks, 2001, e.g.), and they are not able to integrate and use knowledge coming from corpus and other resources during the learning"
P06-3010,C92-4189,0,\N,Missing
P06-3010,J01-3001,0,\N,Missing
P07-1006,2006.iwslt-evaluation.5,0,0.0240444,"translation for an ambiguous source word. There is not always a direct relation between the possible senses for a word in a (monolingual) lexicon and its translations to a particular language, so this represents a different task to WSD against a (monolingual) lexicon (Hutchins and Somers, 1992). Although it has been argued that WSD does not yield better translation quality than a machine translation system alone, it has been recently shown that a WSD module that is developed following specific multilingual requirements can significantly improve the performance of a machine translation system (Carpuat et al., 2006). This paper focuses on the application of our approach to the translation of verbs in English to Portuguese translation, specifically for a set of 10 mainly light and highly ambiguous verbs. We also experiment with a monolingual task by using the verbs from Senseval-3 lexical sample task. We explore knowledge from 12 syntactic, semantic and pragmatic sources. In principle, the proposed approach could also be applied to any lexical disambiguation task by customizing the sense reposi42 tory and knowledge sources. In the remainder of this paper we first present related approaches to WSD and disc"
P07-1006,W04-0824,0,0.025698,"olingual task (Senseval-3 verbs with fine-grained sense distinctions and using the evaluation system provided by Senseval). It also shows the average accuracy of the most frequent sense and accuracies reported on the same set of verbs by the best systems submitted by the sites which participated in this task. Syntalex-3 (Mohammad and Pedersen, 2004) is based on an ensemble of bagged decision trees with narrow context part-of-speech features and bigrams. CLaC1 (Lamjiri et al., 2004) uses a Naive Bayes algorithm with a dynamically adjusted context window around the target word. Finally, MC-WSD (Ciaramita and Johnson, 2004) is a multi-class averaged perceptron classifier using syntactic and narrow context features, with one component trained on the data provided by Senseval and other trained on WordNet glosses. System Majority sense Syntalex-3 CLaC1 MC-WSD Aleph a small number of rules (from 6, for verbs with a few examples, to 88) and all knowledge sources are used across different rules and verbs. In general, results from both multilingual and monolingual tasks demonstrate that the hypothesis put forward in Section 1, that ILP’s ability to generate expressive rules which combine and integrate a wide range of k"
P07-1006,P96-1006,0,0.287988,"2 Related Work WSD approaches can be classified as (a) knowledge-based approaches, which make use of linguistic knowledge, manually coded or extracted from lexical resources (Agirre and Rigau, 1996; Lesk 1986); (b) corpus-based approaches, which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models (Yarowsky, 1995; Schütze 1998); and (c) hybrid approaches, which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge (Ng and Lee 1996; Stevenson and Wilks, 2001). Hybrid approaches can combine advantages from both strategies, potentially yielding accurate and comprehensive systems, particularly when deep knowledge is explored. Linguistic knowledge is available in electronic resources suitable for practical use, such as WordNet (Fellbaum, 1998), dictionaries and parsers. However, the use of this information has been hampered by the limitations of the modeling techniques that have been explored so far: using deep sources of domain knowledge is beyond the capabilities of such techniques, which are in general based on attribute"
P07-1006,P05-1006,0,0.0608579,"Missing"
P07-1006,W04-0833,0,0.0295984,"Missing"
P07-1006,P93-1016,0,0.0121142,"igram(snt1, back, as). has_bigram(snt1, such, a). … KS3. Narrow context containing 5 content words to the right and left of the verb, identified using POS tags, represented by has_narrow(snt, word_position, word): has_narrow(snt1, 1st_word_left, mind). has_narrow(snt1, 1st_word_right, back). … KS4. POS tags of 5 words to the right and left of the verb, represented by has_pos(snt, word_position, pos): has pos(snt1, 1st_word_left, nn). has pos(snt1, 1st_word_right, rb). … Table 1. Verbs and possible senses in our corpus Both corpora were lemmatized and part-of-speech (POS) tagged using Minipar (Lin, 1993) and 44 KS5. 11 collocations of the verb: 1st preposition to the right, 1st and 2nd words to the left and right, 1st noun, 1st adjective, and 1st verb to the left and right. These are represented using definitions of the form has_collocation(snt, type, collocation): has_collocation(snt1, 1st_prep_right, back). has_collocation(snt1, 1st_noun_left, mind).… KS6. Subject and object of the verb obtained using Minipar and represented by has_rel(snt, type, word): has_rel(snt1, subject, i). has_rel(snt1, object, nil). … KS7. Grammatical relations not including the target verb also identified using Min"
P07-1006,W04-0807,0,0.181853,"lation. Sense ambiguity has been recognized as one of the most important obstacles Maria das Graças V. Nunes NILC/ICMC University of São Paulo Caixa Postal 668, 13560-970 São Carlos, SP, Brazil gracan@icmc.usp.br to successful language understanding since the early 1960’s and many techniques have been proposed to solve the problem. Recent approaches focus on the use of various lexical resources and corpus-based techniques in order to avoid the substantial effort required to codify linguistic knowledge. These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al., 2004 for an overview of state-ofthe-art systems). However, current approaches rely on limited knowledge representation and modeling techniques: traditional machine learning algorithms and attribute-value vectors to represent disambiguation instances. This has made it difficult to exploit deep knowledge sources in the generation of the disambiguation models, that is, knowledge that goes beyond simple features extracted directly from the corpus, like bags-of-words and collocations, or provided by shallow natural language tools like part-of-speech taggers. In this paper we present a novel approach fo"
P07-1006,W04-0839,0,0.0150978,"s for both Aleph and the other learning algorithms. Values that yielded the best average predictive accuracy in the training sets were assumed to be optimal and used to evaluate the test sets. 4.2 Monolingual task Table 3 shows the average accuracy obtained by Aleph in the monolingual task (Senseval-3 verbs with fine-grained sense distinctions and using the evaluation system provided by Senseval). It also shows the average accuracy of the most frequent sense and accuracies reported on the same set of verbs by the best systems submitted by the sites which participated in this task. Syntalex-3 (Mohammad and Pedersen, 2004) is based on an ensemble of bagged decision trees with narrow context part-of-speech features and bigrams. CLaC1 (Lamjiri et al., 2004) uses a Naive Bayes algorithm with a dynamically adjusted context window around the target word. Finally, MC-WSD (Ciaramita and Johnson, 2004) is a multi-class averaged perceptron classifier using syntactic and narrow context features, with one component trained on the data provided by Senseval and other trained on WordNet glosses. System Majority sense Syntalex-3 CLaC1 MC-WSD Aleph a small number of rules (from 6, for verbs with a few examples, to 88) and all"
P07-1006,W97-0213,0,0.0198365,"efits from more specific knowledge sources, such as the verb’s relation to other items in the sentence (for example, by analysing the semantic type of its subject and object). Consequently, we believe that the disambiguation of verbs is task to which ILP is particularly wellsuited. Therefore, this paper focuses on the disambiguation of verbs, which is an interesting task since much of the previous work on WSD has concentrated on the disambiguation of nouns. WSD is usually approached as an independent task, however, it has been argued that different applications may have specific requirements (Resnik and Yarowsky, 1997). For example, in machine translation, WSD, or translation disambiguation, is responsible for identifying the correct translation for an ambiguous source word. There is not always a direct relation between the possible senses for a word in a (monolingual) lexicon and its translations to a particular language, so this represents a different task to WSD against a (monolingual) lexicon (Hutchins and Somers, 1992). Although it has been argued that WSD does not yield better translation quality than a machine translation system alone, it has been recently shown that a WSD module that is developed fo"
P07-1006,J98-1004,0,0.029006,"itations (Section 2). We then describe some basic concepts on ILP and our application of this technique to WSD (Section 3). Finally, we described our experiments and their results (Section 4). 2 Related Work WSD approaches can be classified as (a) knowledge-based approaches, which make use of linguistic knowledge, manually coded or extracted from lexical resources (Agirre and Rigau, 1996; Lesk 1986); (b) corpus-based approaches, which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models (Yarowsky, 1995; Schütze 1998); and (c) hybrid approaches, which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge (Ng and Lee 1996; Stevenson and Wilks, 2001). Hybrid approaches can combine advantages from both strategies, potentially yielding accurate and comprehensive systems, particularly when deep knowledge is explored. Linguistic knowledge is available in electronic resources suitable for practical use, such as WordNet (Fellbaum, 1998), dictionaries and parsers. However, the use of this information has been hampered by the li"
P07-1006,P06-3010,1,0.851114,"s a subject B that is a proper noun (nnp) or a personal pronoun (prp). 4 Experiments and results To assess the performance of the approach the model produced for each verb was tested on the corresponding set of test cases by applying the rules in a decision-list like approach, i.e., retaining the order in which they were produced and backing off to the most frequent sense in the training set to classify cases that were not covered by any of the rules. All the knowledge sources were made available to be used by the inference engine, since previous experiments showed that they are all relevant (Specia, 2006). In what follows we present the results and discuss each task. 4.1 with three learning algorithms frequently used for WSD, which rely on knowledge represented as attribute-value vectors: C4.5 (decision-trees), Naive Bayes and Support Vector Machine (SVM)1. In order to represent all knowledge sources in attribute-value vectors, KS2, KS7, KS9 and KS10 had to be pre-processed to be transformed into binary attributes. For example, in the case of selectional restrictions (KS9), one attribute was created for each possible sense of the verb and a true/false value was assigned to it depending on whet"
P07-1006,J01-3001,1,0.881631,"SD approaches can be classified as (a) knowledge-based approaches, which make use of linguistic knowledge, manually coded or extracted from lexical resources (Agirre and Rigau, 1996; Lesk 1986); (b) corpus-based approaches, which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models (Yarowsky, 1995; Schütze 1998); and (c) hybrid approaches, which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge (Ng and Lee 1996; Stevenson and Wilks, 2001). Hybrid approaches can combine advantages from both strategies, potentially yielding accurate and comprehensive systems, particularly when deep knowledge is explored. Linguistic knowledge is available in electronic resources suitable for practical use, such as WordNet (Fellbaum, 1998), dictionaries and parsers. However, the use of this information has been hampered by the limitations of the modeling techniques that have been explored so far: using deep sources of domain knowledge is beyond the capabilities of such techniques, which are in general based on attribute-value vector representation"
P07-1006,C96-1005,0,\N,Missing
P07-1006,P95-1026,0,\N,Missing
P09-1089,W07-0734,0,0.0147474,"ets and oceanfront homes”. Here, chalets are replaced by houses or units (depending on the model), providing a translation that would be acceptable by most readers. Other incorrect translations occurred when the unknown term was part of a phrase, for example, troughs replaced with depressions in peaks Automatic MT Evaluation Although automatic MT evaluation metrics are less appropriate for capturing the variations generated by our method, to ensure that there was no degradation in the system-level scores according to such metrics we also measured the models’ performance using BLEU and METEOR (Agarwal and Lavie, 2007). The version of METEOR we used on the target language (French) considers the stems of the words, instead of surface forms only, but does not make use of WordNet synonyms. We evaluated the performance of the top models of Table 1, as well as of a baseline SMT system that left unknown terms untranslated, on the sample of 1,014 manually annotated sentences. As shown in Table 3, all models resulted in improvement with respect to the original sentences (base797 (Dyer et al., 2008) in order to allow a compact representation of alternative inputs to an SMT system. This is an approach that we intend"
P09-1089,N06-1003,0,0.45233,"Missing"
P09-1089,W08-0309,0,0.0202988,"Missing"
P09-1089,D08-1021,0,0.0212981,"s can be exploited to validate the application of a rule to a text. In such models, an explicit Word Sense Disambiguation (WSD) is not necessarily required; rather, an implicit sense-match is sought after (Dagan et al., 2006). Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule. However, in general, entailment rule application addresses other aspects of context matching as well (Szpektor et al., 2008). al., 2006; Cohn and Lapata, 2007; Zhao et al., 2008; Callison-Burch, 2008; Guzm´an and Garrido, 2008). The procedure to extract paraphrases in these approaches is similar to standard phrase extraction in SMT systems, and therefore a large amount of additional parallel corpus is required. Moreover, as discussed in Section 5, when unknown texts are not from the same domain as the SMT training corpus, it is likely that paraphrases found through such methods will yield misleading translations. Bond et al. (2008) use grammars to paraphrase the whole source sentence, covering aspects like word order and minor lexical variations (tenses etc.), but not content words. The p"
P09-1089,P07-1092,0,0.0147012,"mouse to mark your answers”. Context-models can be exploited to validate the application of a rule to a text. In such models, an explicit Word Sense Disambiguation (WSD) is not necessarily required; rather, an implicit sense-match is sought after (Dagan et al., 2006). Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule. However, in general, entailment rule application addresses other aspects of context matching as well (Szpektor et al., 2008). al., 2006; Cohn and Lapata, 2007; Zhao et al., 2008; Callison-Burch, 2008; Guzm´an and Garrido, 2008). The procedure to extract paraphrases in these approaches is similar to standard phrase extraction in SMT systems, and therefore a large amount of additional parallel corpus is required. Moreover, as discussed in Section 5, when unknown texts are not from the same domain as the SMT training corpus, it is likely that paraphrases found through such methods will yield misleading translations. Bond et al. (2008) use grammars to paraphrase the whole source sentence, covering aspects like word order and minor lexical variations (t"
P09-1089,P04-1036,0,0.00728935,"e also applied several combinations of source models, such as LSA combined with LMS, to take advantage of their complementary strengths. Additionally, we assessed our method with sourceonly models, by setting the number of sentences to be selected by the source model to one (k = 1). Scoring source texts We test our proposed method using several context-models shown to perform reasonably well in previous work: • FREQ: The first model we use is a contextindependent baseline. A common useful heuristic to pick an entailment rule is to select the candidate with the highest frequency in the corpus (Mccarthy et al., 2004). In this model, a rule’s score is the normalized number of occurrences of its RHS in the training corpus, ignoring the context of the LHS. 5 5.1 Manual Evaluation To evaluate the translations produced using the various source and target models and the different rule-sets, we rely mostly on manual assessment, since automatic MT evaluation metrics like BLEU do not capture well the type of semantic variations • LSA: Latent Semantic Analysis (Deerwester et al., 1990) is a well-known method for rep2 Results http://opennlp.sourceforge.net 795 Model 1 2 3 4 5 6 7 –:SMT NB:SMT LSA:SMT NB:– LMS:LMT FR"
P09-1089,P06-1057,1,0.819303,"lment rules, i.e. rules without variables. Various resources for lexical rules are available, and the prominent one is WordNet (Fellbaum, 1998), which has been used in virtually all TE systems (Giampiccolo et al., 2007). Typically, a rule application is valid only under specific contexts. For example, mouse ⇒ rodent should not be applied to “Use the mouse to mark your answers”. Context-models can be exploited to validate the application of a rule to a text. In such models, an explicit Word Sense Disambiguation (WSD) is not necessarily required; rather, an implicit sense-match is sought after (Dagan et al., 2006). Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule. However, in general, entailment rule application addresses other aspects of context matching as well (Szpektor et al., 2008). al., 2006; Cohn and Lapata, 2007; Zhao et al., 2008; Callison-Burch, 2008; Guzm´an and Garrido, 2008). The procedure to extract paraphrases in these approaches is similar to standard phrase extraction in SMT systems, and therefore a large amount of additional parallel corpus is"
P09-1089,D08-1022,0,0.0101575,"Missing"
P09-1089,W09-0404,0,0.0103699,"ossible, to generate more general texts for translation. Our approach, based on the textual entailment framework, considers the newly generated texts as entailed from the original one. Monolingual semantic resources such as WordNet can provide entailment rules required for both these symmetric and asymmetric entailment relations. Textual Entailment (TE) has recently become a prominent paradigm for modeling semantic inference, capturing the needs of a broad range of text understanding applications (Giampiccolo et al., 2007). Yet, its application to SMT has been so far limited to MT evaluation (Pado et al., 2009). TE defines a directional relation between two texts, where the meaning of the entailed text (hypothesis, h) can be inferred from the meaning of the entailing text, t. Under this paradigm, paraphrases are a special case of the entailment relation, when the relation is symmetric (the texts entail each other). Otherwise, we say that one text directionally entails the other. A common practice for proving (or generating) h from t is to apply entailment rules to t. An entailment rule, denoted LHS ⇒ RHS, specifies an entailment relation between two text fragments (the Left- and Right- Hand Sides),"
P09-1089,P08-1115,0,0.0258391,"he system-level scores according to such metrics we also measured the models’ performance using BLEU and METEOR (Agarwal and Lavie, 2007). The version of METEOR we used on the target language (French) considers the stems of the words, instead of surface forms only, but does not make use of WordNet synonyms. We evaluated the performance of the top models of Table 1, as well as of a baseline SMT system that left unknown terms untranslated, on the sample of 1,014 manually annotated sentences. As shown in Table 3, all models resulted in improvement with respect to the original sentences (base797 (Dyer et al., 2008) in order to allow a compact representation of alternative inputs to an SMT system. This is an approach that we intend to explore in future work, as a way to efficiently handle the different source language alternatives generated by entailment rules. However, since most current MT systems do not accept such type of inputs, we consider the results on pruning by source-side context models as broadly relevant. and troughs, a problem that also strongly affects paraphrasing. In another case, movement was the hypernym chosen to replace labor in labor movement, yielding an awkward text for translatio"
P09-1089,P02-1040,0,0.104704,"ord order and minor lexical variations (tenses etc.), but not content words. The paraphrases are added to the source side of the corpus and the corresponding target sentences are duplicated. This, however, may yield distorted probability estimates in the phrase table, since these were not computed from parallel data. The main use of monolingual paraphrases in MT to date has been for evaluation. For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al., 2002). 2.3 3 Textual Entailment and Entailment Rules Textual Entailment for Statistical Machine Translation Previous solutions for handling unknown terms in a source text s augment the SMT system’s phrase table based on multilingual corpora. This allows indirectly paraphrasing s, when the SMT system chooses to use a paraphrase included in the table and produces a translation with the corresponding target phrase for the unknown term. We propose using monolingual paraphrasing methods and resources for this task to obtain a more extensive set of rules for paraphrasing the source. These rules are then"
P09-1089,eck-etal-2008-communicating,0,0.0802498,"Missing"
P09-1089,H05-1095,1,0.693883,"setting of these experiments is described in what follows. Preliminary analysis confirmed (as expected) that readers prefer translations of paraphrases, when available, over translations of directional entailments. This consideration is therefore taken into account in the proposed method. The input is a text unit to be translated, such as a sentence or paragraph, with one or more unknown terms. For each unknown term we first fetch a list of candidate rules for paraphrasing (e.g. synonyms), where the unknown term is the LHS. For SMT data To produce sentences for our experiments, we use Matrax (Simard et al., 2005), a standard phrase-based SMT system, with the exception that it allows gaps in phrases. We use approximately 1M sentence pairs from the English-French 794 Europarl corpus for training, and then translate a test set of 5,859 English sentences from the News corpus into French. Both resources are taken from the shared translation task in WMT-2008 (Callison-Burch et al., 2008). Hence, we compare our method in a setting where the training and test data are from different domains, a common scenario in the practical use of MT systems. Of the 5,859 translated sentences, 2,494 contain unknown terms (c"
P09-1089,W07-1401,1,0.667804,"cal selection and ordering. This phenomenon is demonstrated in the following sentences, where the translation of the English sentence (1) is acceptable only when the unknown word (in bold) is replaced with a translatable paraphrase (3): candidates before supplying them to the translation engine, thus improving translation efficiency. Second, the ranking may be combined with target language information in order to choose the best translation, thus improving translation quality. We position the problem of generating alternative texts for translation within the Textual Entailment (TE) framework (Giampiccolo et al., 2007). TE provides a generic way for handling language variability, identifying when the meaning of one text is entailed by the other (i.e. the meaning of the entailed text can be inferred from the meaning of the entailing one). When the meanings of two texts are equivalent (paraphrase), entailment is mutual. Typically, a more general version of a certain text is entailed by it. Hence, through TE we can formalize the generation of both equivalent and more general texts for the source text. When possible, a paraphrase is used. Otherwise, an alternative text whose meaning is entailed by the original"
P09-1089,P08-1078,1,0.677078,"should not be applied to “Use the mouse to mark your answers”. Context-models can be exploited to validate the application of a rule to a text. In such models, an explicit Word Sense Disambiguation (WSD) is not necessarily required; rather, an implicit sense-match is sought after (Dagan et al., 2006). Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule. However, in general, entailment rule application addresses other aspects of context matching as well (Szpektor et al., 2008). al., 2006; Cohn and Lapata, 2007; Zhao et al., 2008; Callison-Burch, 2008; Guzm´an and Garrido, 2008). The procedure to extract paraphrases in these approaches is similar to standard phrase extraction in SMT systems, and therefore a large amount of additional parallel corpus is required. Moreover, as discussed in Section 5, when unknown texts are not from the same domain as the SMT training corpus, it is likely that paraphrases found through such methods will yield misleading translations. Bond et al. (2008) use grammars to paraphrase the whole source sentence, covering aspects like word ord"
P09-1089,W06-2907,1,0.899223,"Missing"
P09-1089,E06-1006,0,0.130232,"Missing"
P09-1089,P08-1089,0,0.035166,"ers”. Context-models can be exploited to validate the application of a rule to a text. In such models, an explicit Word Sense Disambiguation (WSD) is not necessarily required; rather, an implicit sense-match is sought after (Dagan et al., 2006). Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule. However, in general, entailment rule application addresses other aspects of context matching as well (Szpektor et al., 2008). al., 2006; Cohn and Lapata, 2007; Zhao et al., 2008; Callison-Burch, 2008; Guzm´an and Garrido, 2008). The procedure to extract paraphrases in these approaches is similar to standard phrase extraction in SMT systems, and therefore a large amount of additional parallel corpus is required. Moreover, as discussed in Section 5, when unknown texts are not from the same domain as the SMT training corpus, it is likely that paraphrases found through such methods will yield misleading translations. Bond et al. (2008) use grammars to paraphrase the whole source sentence, covering aspects like word order and minor lexical variations (tenses etc.), but no"
P09-1089,P08-2015,0,0.0376735,"Missing"
P09-1089,N06-1058,0,0.0361947,"t is likely that paraphrases found through such methods will yield misleading translations. Bond et al. (2008) use grammars to paraphrase the whole source sentence, covering aspects like word order and minor lexical variations (tenses etc.), but not content words. The paraphrases are added to the source side of the corpus and the corresponding target sentences are duplicated. This, however, may yield distorted probability estimates in the phrase table, since these were not computed from parallel data. The main use of monolingual paraphrases in MT to date has been for evaluation. For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al., 2002). 2.3 3 Textual Entailment and Entailment Rules Textual Entailment for Statistical Machine Translation Previous solutions for handling unknown terms in a source text s augment the SMT system’s phrase table based on multilingual corpora. This allows indirectly paraphrasing s, when the SMT system chooses to use a paraphrase included in the table and produces a translation with the corresponding target phrase for the"
P09-1089,P97-1017,0,0.266109,"Missing"
P09-1089,E03-1076,0,0.130127,"Missing"
P09-1089,D07-1092,0,0.0249287,"Missing"
P09-1089,S07-1009,0,0.0137349,"Missing"
P09-1089,2008.iwslt-papers.2,0,\N,Missing
P13-1004,P11-1022,0,0.0273313,"such as professional translators. Examples of applications of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort and time to correct than translating from scratch (Specia et al., 2009), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting subsegments that need revision (Bach et al., 2011). QE is generally addressed as a machine learning task using a variety of linear and kernel-based regression or classification algorithms to induce models from examples of translations described through a number of features and annotated for quality. For an overview of various algorithms and features we refer the reader to the WMT12 shared task on QE (Callison-Burch et al., 2012). While initial work used annotations derived Our approach is based on Gaussian Processes (GPs) (Rasmussen and Williams, 2006), a kernelised Bayesian non-parametric learning framework. We develop multi-task learning mo"
P13-1004,P02-1040,0,0.0972589,"Missing"
P13-1004,C04-1046,0,0.116879,"Missing"
P13-1004,W11-2103,0,0.0227561,"previously post-edited translation by a fourth translator. In an attempt to accommodate for systematic biases among annotators, the final effort score was computed as the weighted average between the three PE-effort scores, with more weight given to the judges with higher standard deviation from their own mean score. This resulted in scores spread more evenly in the [1, 5] range. WPTP12: This dataset was distributed by Koponen et al. (2012). It contains 299 English sentences translated into Spanish using two or more of eight MT systems randomly selected from all system submissions for WMT11 (Callison-Burch et al., 2011). These MT systems range from online and customised SMT systems to commercial rule-based systems. Translations were post-edited by humans while time was recorded. The labels are the number of seconds spent by a translator editing a sentence normalised by source sentence length. The post-editing was done by eight native speakers of Spanish, including five professional translators and three translation students. Only 20 translations were edited by all eight annotators, with the remaining translations randomly distributed amongst them. The resulting dataset contains 1, 624 instances, which were r"
P13-1004,W12-3102,1,0.890395,"Missing"
P13-1004,quirk-2004-training,0,0.0538761,"listic framework and have been successfully adapted for multi-task learning in a number of ways, e.g., by learning multi-task correlations (Bonilla et al., 2008), modelling per-task variance (Groot et al., 2011) or perannotator biases (Rogers et al., 2010). Our method builds on the work of Bonilla et al. (2008) by 1 We are not strictly the first, Polajnar et al. (2011) used GPs for text classification. 33 from automatic MT evaluation metrics (Blatz et al., 2004) such as BLEU (Papineni et al., 2002) at training time, it soon became clear that human labels result in significantly better models (Quirk, 2004). Current work at sentence level is thus based on some form of human supervision. As typical of subjective annotation tasks, QE datasets should contain multiple annotators to lead to models that are representative. Therefore, work in QE faces all common issues regarding variability in annotators’ judgements. The following are a few other features that make our datasets particularly interesting: • In order to minimise annotation costs, translation instances are often spread among annotators, such that each instance is only labelled by one or a few judges. In fact, for a sizeable dataset (thousa"
P13-1004,P07-1033,0,0.0527429,"Missing"
P13-1004,D08-1027,0,0.174664,"Missing"
P13-1004,P10-1064,0,0.0227248,"Missing"
P13-1004,P10-1063,0,0.0150805,"at providing an estimate on the quality of each translated segment – typically a sentence – without access to reference translations. Work in this area has become increasingly popular in recent years as a consequence of the widespread use of MT among realworld users such as professional translators. Examples of applications of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort and time to correct than translating from scratch (Specia et al., 2009), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting subsegments that need revision (Bach et al., 2011). QE is generally addressed as a machine learning task using a variety of linear and kernel-based regression or classification algorithms to induce models from examples of translations described through a number of features and annotated for quality. For an overview of various algorithms and features we refer the reader to the WMT12 shared t"
P13-1004,2009.eamt-1.5,1,0.906119,"epartment of Computer Science University of Sheffield Sheffield, United Kingdom {t.cohn,l.specia}@sheffield.ac.uk Abstract – and yet equally valid – truths. Particularly in highly subjective annotation tasks, the differences between annotators cannot be captured by simple models such as scaling all instances of a certain annotator by a factor. They can originate from a number of nuanced aspects. This is the case, for example, of annotations on the quality of sentences generated using machine translation (MT) systems, which are often used to build quality estimation models (Blatz et al., 2004; Specia et al., 2009) – our application of interest. In addition to annotators’ own perceptions and expectations with respect to translation quality, a number of factors can affect their judgements on specific sentences. For example, certain annotators may prefer translations produced by rulebased systems as these tend to be more grammatical, while others would prefer sentences produced by statistical systems with more adequate lexical choices. Likewise, some annotators can be biased by the complexity of the source sentence: lengthy sentences are often (subconsciously) assumed to be of low quality by some annotato"
P13-1004,2012.amta-wptp.2,1,0.102762,"ies an estimated percentage of the MT output that needs to be corrected. The post-editing effort scores were produced independently by three professional translators based on a previously post-edited translation by a fourth translator. In an attempt to accommodate for systematic biases among annotators, the final effort score was computed as the weighted average between the three PE-effort scores, with more weight given to the judges with higher standard deviation from their own mean score. This resulted in scores spread more evenly in the [1, 5] range. WPTP12: This dataset was distributed by Koponen et al. (2012). It contains 299 English sentences translated into Spanish using two or more of eight MT systems randomly selected from all system submissions for WMT11 (Callison-Burch et al., 2011). These MT systems range from online and customised SMT systems to commercial rule-based systems. Translations were post-edited by humans while time was recorded. The labels are the number of seconds spent by a translator editing a sentence normalised by source sentence length. The post-editing was done by eight native speakers of Spanish, including five professional translators and three translation students. Onl"
P13-1004,2011.eamt-1.12,1,0.862114,"ators can be biased by the complexity of the source sentence: lengthy sentences are often (subconsciously) assumed to be of low quality by some annotators. An extreme case is the judgement of quality through post-editing time: annotators have different typing speeds, as well as levels of expertise in the task of post-editing, proficiency levels in the language pair, and knowledge of the terminology used in particular sentences. These variations result in time measurements that are not comparable across annotators. Thus far, the use of post-editing time has been done on an per-annotator basis (Specia, 2011), or simply averaged across multiple translators (Plitt and Masselot, 2010), both strategies far from ideal. Overall, these myriad of factors affecting quality judgements make the modelling of multiple annotators a very challenging problem. This problem is exacerbated when annotations are provided by non-professional annotators, e.g., through crowdsourcing – a common strategy used Annotating linguistic data is often a complex, time consuming and expensive endeavour. Even with strict annotation guidelines, human subjects often deviate in their analyses, each bringing different biases, interpret"
P13-2097,2010.jec-1.5,1,0.743001,"ation is focused on feature engineering and feature selection, with some recent work on devising more reliable and less subjective quality labels. Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic metric (e.g. NIST (Doddington, 2002)) as the quality label. Quirk (2004) showed that small datasets manually annotated by humans for quality can result in models that outperform those trained on much larger, automatically labelled sets. Since quality labels are subjective to the annotators’ judgements, Specia and Farzindar (2010) evaluated the performance of QE models using HTER (Snover et al., 2006) as the quality score, i.e., the edit distance between the MT output and its post-edited version. Specia (2011) compared the performance of models based on labels for 543 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543–548, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 3.2 post-editing effort, post-editing time, and HTER. In terms of learning algorithms, by and large most approaches use Support Vector Machines, particularly regression-b"
P13-2097,2009.mtsummit-papers.16,1,0.933478,"Missing"
P13-2097,2011.mtsummit-papers.58,1,0.843038,"Missing"
P13-2097,2011.eamt-1.12,1,0.81515,"e study on QE for MT: 91 features were proposed and used to train predictors based on an automatic metric (e.g. NIST (Doddington, 2002)) as the quality label. Quirk (2004) showed that small datasets manually annotated by humans for quality can result in models that outperform those trained on much larger, automatically labelled sets. Since quality labels are subjective to the annotators’ judgements, Specia and Farzindar (2010) evaluated the performance of QE models using HTER (Snover et al., 2006) as the quality score, i.e., the edit distance between the MT output and its post-edited version. Specia (2011) compared the performance of models based on labels for 543 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543–548, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 3.2 post-editing effort, post-editing time, and HTER. In terms of learning algorithms, by and large most approaches use Support Vector Machines, particularly regression-based approaches. For an overview on various feature sets and machine learning algorithms, we refer the reader to a recent shared task on the topic (Callison-Burch et al., 2012). Previ"
P13-2097,W06-2209,0,0.034761,"g time, and HTER. In terms of learning algorithms, by and large most approaches use Support Vector Machines, particularly regression-based approaches. For an overview on various feature sets and machine learning algorithms, we refer the reader to a recent shared task on the topic (Callison-Burch et al., 2012). Previous work use supervised learning methods (“passive learning” following the AL terminology) to train QE models. On the other hand, AL has been successfully used in a number of natural language applications such as text classification (Lewis and Gale, 1994), named entity recognition (Vlachos, 2006) and parsing (Baldridge and Osborne, 2004). See Olsson (2009) for an overview on AL for natural language processing as well as a comprehensive list of previous work. 3 3.1 Query Methods The core of an AL setting is how the learner will gather new instances to add to its training data. In our setting, we use a pool-based strategy, where the learner queries an instance pool and selects the best instance according to an informativeness measure. The learner then asks an “oracle” (in this case, the human expert) for the true label of the instance and adds it to the training data. Query methods use"
P13-2097,quirk-2004-training,0,0.0356732,"nd subject to inconsistencies due to the subjectivity of the task. To avoid inconsistencies because of disagreements among annotators, it is often recommended that a QE model is trained 2 Related Work Most research work on QE for machine translation is focused on feature engineering and feature selection, with some recent work on devising more reliable and less subjective quality labels. Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic metric (e.g. NIST (Doddington, 2002)) as the quality label. Quirk (2004) showed that small datasets manually annotated by humans for quality can result in models that outperform those trained on much larger, automatically labelled sets. Since quality labels are subjective to the annotators’ judgements, Specia and Farzindar (2010) evaluated the performance of QE models using HTER (Snover et al., 2006) as the quality score, i.e., the edit distance between the MT output and its post-edited version. Specia (2011) compared the performance of models based on labels for 543 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543"
P13-2097,D08-1112,0,0.0661861,"uery Methods The core of an AL setting is how the learner will gather new instances to add to its training data. In our setting, we use a pool-based strategy, where the learner queries an instance pool and selects the best instance according to an informativeness measure. The learner then asks an “oracle” (in this case, the human expert) for the true label of the instance and adds it to the training data. Query methods use different criteria to predict how informative an instance is. We experiment with two of them: Uncertainty Sampling (US) (Lewis and Gale, 1994) and Information Density (ID) (Settles and Craven, 2008). In the following, we denote M (x) the query score with respect to method M . According to the US method, the learner selects the instance that has the highest labelling variance according to its model: Experimental Settings U S(x) = V ar(y|x) Datasets The ID method considers that more dense regions of the query space bring more useful information, leveraging the instance uncertainty and its similarity to all the other instances in the pool: !β U 1 X ID(x) = V ar(y|x) × sim(x, x(u) ) U We perform experiments using four MT datasets manually annotated for quality: English-Spanish (en-es): 2, 25"
P13-2097,2006.amta-papers.25,0,0.0422342,"work on devising more reliable and less subjective quality labels. Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic metric (e.g. NIST (Doddington, 2002)) as the quality label. Quirk (2004) showed that small datasets manually annotated by humans for quality can result in models that outperform those trained on much larger, automatically labelled sets. Since quality labels are subjective to the annotators’ judgements, Specia and Farzindar (2010) evaluated the performance of QE models using HTER (Snover et al., 2006) as the quality score, i.e., the edit distance between the MT output and its post-edited version. Specia (2011) compared the performance of models based on labels for 543 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543–548, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 3.2 post-editing effort, post-editing time, and HTER. In terms of learning algorithms, by and large most approaches use Support Vector Machines, particularly regression-based approaches. For an overview on various feature sets and machine lea"
P13-2097,W04-3202,0,\N,Missing
P13-2097,W12-3102,1,\N,Missing
P13-2097,P07-2045,0,\N,Missing
P13-2097,C04-1046,0,\N,Missing
P13-4014,P11-1022,0,0.0616038,"ent; • Selecting among alternative translations produced by different MT systems; • Deciding whether the translation can be used for self-training of MT systems. Work in QE for MT started in the early 2000’s, inspired by the confidence scores used in Speech Recognition: mostly the estimation of word posterior probabilities. Back then it was called confi79 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79–84, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2010), and highlighting sub-segments that need revision (Bach et al., 2011). QE is generally addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of translations described through a number of features and annotated for quality. For an overview of various algorithms and features we refer the reader to the WMT12 shared task on QE (Callison-Burch et al., 2012). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engine"
P13-4014,C04-1046,0,0.524049,"Missing"
P13-4014,W12-3102,1,0.870241,"then it was called confi79 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79–84, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2010), and highlighting sub-segments that need revision (Bach et al., 2011). QE is generally addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of translations described through a number of features and annotated for quality. For an overview of various algorithms and features we refer the reader to the WMT12 shared task on QE (Callison-Burch et al., 2012). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labourintensive. Different language pairs or optimisation against specific quality scores (e.g., post-editing time vs translation adequacy) can benefit from very different feature sets. Q U E ST, our framework for quality estimation, provides a wide range of feature extractors fr"
P13-4014,P10-1064,0,0.0219866,"Missing"
P13-4014,P02-1040,0,0.0950837,"Missing"
P13-4014,P10-1063,0,0.045366,"also be useful for end-users reading translations for gisting, particularly those who cannot read the source language. QE nowadays focuses on estimating more interpretable metrics. “Quality” is defined according to the application: post-editing, gisting, etc. A number of positive results have been reported. Examples include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for postediting (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., We describe Q U E ST, an open source framework for machine translation quality estimation. The framework allows the extraction of several quality indicators from source segments, their translations, external resources (corpora, language models, topic models, etc.), as well as language tools (parsers, part-of-speech tags, etc.). It also provides machine learning algorithms to build quality estimation models. We benc"
P13-4014,2009.eamt-1.5,1,0.793882,"roblem by using more complex metrics that go beyond matching the source segment with previously translated data. QE can also be useful for end-users reading translations for gisting, particularly those who cannot read the source language. QE nowadays focuses on estimating more interpretable metrics. “Quality” is defined according to the application: post-editing, gisting, etc. A number of positive results have been reported. Examples include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for postediting (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., We describe Q U E ST, an open source framework for machine translation quality estimation. The framework allows the extraction of several quality indicators from source segments, their translations, external resources (corpora, language models, topic models, etc.), as well as language tools"
P13-4014,2011.eamt-1.12,1,0.842332,"complex metrics that go beyond matching the source segment with previously translated data. QE can also be useful for end-users reading translations for gisting, particularly those who cannot read the source language. QE nowadays focuses on estimating more interpretable metrics. “Quality” is defined according to the application: post-editing, gisting, etc. A number of positive results have been reported. Examples include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for postediting (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., We describe Q U E ST, an open source framework for machine translation quality estimation. The framework allows the extraction of several quality indicators from source segments, their translations, external resources (corpora, language models, topic models, etc.), as well as language tools (parsers, part"
P15-4015,P14-2075,0,0.17849,"ed in WordNet. We define Substitution Generation (SG) as the task of producing candidate substitutions for complex words, which is normally done regardless of the context of the complex word. Previous work commonly addresses this task by querying general domain thesauri such as WordNet (Fellbaum, 1998), or domain specific ones such as UMLS (Bodenreider, 2004). Examples of work resorting to this strategy are (Devlin and Tait, 1998) and (Carroll et al., 1999). Recent work focuses on learning substitutions from sentence-aligned parallel corpora of complex-simple texts (Paetzold and Specia, 2013; Horn et al., 2014). LEXenstein’s SG module offers support for five approaches. All approaches use LEXenstein’s Text Adorning module to create substitutions for all possible inflections of verbs and nouns. Each approach is represented by one of the following Python classes: 2.2 Substitution Selection (SS) is the task of selecting which substitutions – from a given list – can replace a complex word in a given sentence without altering its meaning. Most work addresses this task referring to the context of the complex word by employing Word Sense Disambiguation (WSD) approaches (Sedding and Kazakov, 2004; Nunes et"
P15-4015,O13-1007,0,0.29547,"module to create substitutions for all possible inflections of verbs and nouns. Each approach is represented by one of the following Python classes: 2.2 Substitution Selection (SS) is the task of selecting which substitutions – from a given list – can replace a complex word in a given sentence without altering its meaning. Most work addresses this task referring to the context of the complex word by employing Word Sense Disambiguation (WSD) approaches (Sedding and Kazakov, 2004; Nunes et al., 2013), or by discarding substitutions which do not share the same POS tag of the target complex word (Kajiwara et al., 2013; Paetzold and Specia, 2013). LEXenstein’s SS module provides access to three approaches. All approaches require as input a dictionary of substitutions generated by a given approach and a dataset in the VICTOR format (as in Victor Frankenstein (Shelley, 2007)). As output, they produce a set of selected substitutions for each entry in the VICTOR dataset. The VICTOR format is structured as illustrated in Example 1, where Si is the ith sentence in the dataset, wi a target complex word in the hi th position of Si , cji a substitution candidate and rij its simplicity ranking. Each bracketed compone"
P15-4015,P13-1151,0,0.194569,"rmat, lists of stop and basic words, as well as language models and lexica built over Wikipedia and Simple Wikipedia. 3 Experiments In this Section, we discuss the results obtained in four benchmarking experiments. SelectorEvaluator: Provides evaluation metrics for SS methods. It requires a gold-standard in the VICTOR format and a set of selected substitutions. It returns the Potential, Precision and Fmeasure of the SS approach, as defined above. 3.1 Substitution Generation In this experiment we evaluate all SG approaches in LEXenstein. For the KauchakGenerator, we use the corpus provided by (Kauchak, 2013), composed of 150, 569 complex-to-simple parallel sentences, parsed by the Stanford Parser (Klein and Manning, 1965). From the the same corpus, we build the required vocabularies and language models for the BiranGenerator. We used the LexMturk dataset as the gold-standard (Horn et al., 2014), which is composed by 500 sentences, each with a single target complex word and 50 substitutions suggested by turkers. The results are presented in Table 1. The results in Table 1 show that the method of (Horn et al., 2014) yields the best F-Measure results, although combining the output of all generation"
P15-4015,P00-1056,0,0.144076,"set. The VICTOR format is structured as illustrated in Example 1, where Si is the ith sentence in the dataset, wi a target complex word in the hi th position of Si , cji a substitution candidate and rij its simplicity ranking. Each bracketed component is separated by a tabulation marker.   hS1 i hw1 i hh1 i r11 :c11 · · ·hrn1 :cn1 i   ..   (1) . 1 1 hSm i hwm i hhm i rm :cm · · ·hrnm :cnm i KauchakGenerator (Horn et al., 2014) Automatically extracts substitutions from parallel corpora. It requires a set of tagged parallel sentences and the word alignments between them in Pharaoh format (Och and Ney, 2000). It produces a dictionary of complex-to-simple substitutions filtered by the criteria described in (Horn et al., 2014). BiranGenerator (Biran et al., 2011) Filters substitutions based on the Cartesian product between vocabularies of complex and simple words. It requires vocabularies of complex and simple words, as well as two language models trained over complex and simple corpora. It produces a dictionary linking words to a set of synonyms and hypernyms filtered by the criteria described in (Biran et al., 2011). LEXenstein includes two resources for training/testing in the VICTOR format: the"
P15-4015,W13-4813,1,0.844781,"to their synonyms, as listed in WordNet. We define Substitution Generation (SG) as the task of producing candidate substitutions for complex words, which is normally done regardless of the context of the complex word. Previous work commonly addresses this task by querying general domain thesauri such as WordNet (Fellbaum, 1998), or domain specific ones such as UMLS (Bodenreider, 2004). Examples of work resorting to this strategy are (Devlin and Tait, 1998) and (Carroll et al., 1999). Recent work focuses on learning substitutions from sentence-aligned parallel corpora of complex-simple texts (Paetzold and Specia, 2013; Horn et al., 2014). LEXenstein’s SG module offers support for five approaches. All approaches use LEXenstein’s Text Adorning module to create substitutions for all possible inflections of verbs and nouns. Each approach is represented by one of the following Python classes: 2.2 Substitution Selection (SS) is the task of selecting which substitutions – from a given list – can replace a complex word in a given sentence without altering its meaning. Most work addresses this task referring to the context of the complex word by employing Word Sense Disambiguation (WSD) approaches (Sedding and Kaza"
P15-4015,W04-2013,0,0.0585151,"and Specia, 2013; Horn et al., 2014). LEXenstein’s SG module offers support for five approaches. All approaches use LEXenstein’s Text Adorning module to create substitutions for all possible inflections of verbs and nouns. Each approach is represented by one of the following Python classes: 2.2 Substitution Selection (SS) is the task of selecting which substitutions – from a given list – can replace a complex word in a given sentence without altering its meaning. Most work addresses this task referring to the context of the complex word by employing Word Sense Disambiguation (WSD) approaches (Sedding and Kazakov, 2004; Nunes et al., 2013), or by discarding substitutions which do not share the same POS tag of the target complex word (Kajiwara et al., 2013; Paetzold and Specia, 2013). LEXenstein’s SS module provides access to three approaches. All approaches require as input a dictionary of substitutions generated by a given approach and a dataset in the VICTOR format (as in Victor Frankenstein (Shelley, 2007)). As output, they produce a set of selected substitutions for each entry in the VICTOR dataset. The VICTOR format is structured as illustrated in Example 1, where Si is the ith sentence in the dataset,"
P15-4015,P11-2087,0,0.569157,"on of Si , cji a substitution candidate and rij its simplicity ranking. Each bracketed component is separated by a tabulation marker.   hS1 i hw1 i hh1 i r11 :c11 · · ·hrn1 :cn1 i   ..   (1) . 1 1 hSm i hwm i hhm i rm :cm · · ·hrnm :cnm i KauchakGenerator (Horn et al., 2014) Automatically extracts substitutions from parallel corpora. It requires a set of tagged parallel sentences and the word alignments between them in Pharaoh format (Och and Ney, 2000). It produces a dictionary of complex-to-simple substitutions filtered by the criteria described in (Horn et al., 2014). BiranGenerator (Biran et al., 2011) Filters substitutions based on the Cartesian product between vocabularies of complex and simple words. It requires vocabularies of complex and simple words, as well as two language models trained over complex and simple corpora. It produces a dictionary linking words to a set of synonyms and hypernyms filtered by the criteria described in (Biran et al., 2011). LEXenstein includes two resources for training/testing in the VICTOR format: the LexMTurk (Horn et al., 2014) and the SemEval corpus (Specia et al., 2012). Each approach in the SS module is represented by one of the following Python cla"
P15-4015,S12-1046,1,0.906795,"Missing"
P15-4015,E99-1042,0,0.781784,"the most effective approach to Lexical Simplification. 1 Introduction The goal of a Lexical Simplification (LS) approach is to replace complex words and expressions in a given text, often a sentence, with simpler alternatives of equivalent meaning in context. Although very intuitive, this is a challenging task since the substitutions must preserve both the original meaning and the grammaticality of the sentence being simplified. The LS task has been gaining significant attention since the late 1990’s, thanks to the positive influence of the early work presented by (Devlin and Tait, 1998) and (Carroll et al., 1999). More recently, the LS task at SemEval-2012 (Specia et al., 2012) has given LS wider visibility. Participants had the opportunity to compare their approaches in the task of ranking candidate substitutions, all of which were already known to fit the context, according to their “simplicity”. Despite its growth in popularity, the inexistence of tools to support the process and help researchers to build upon has been hampering progress in the area. We were only able to find one tool for LS: a Figure 1: Lexical Simplification Pipeline LEXenstein was devised to facilitate performance comparisons am"
P15-4015,P03-1054,0,\N,Missing
P15-4015,P94-1019,0,\N,Missing
P15-4015,S12-1066,1,\N,Missing
P15-4020,W14-3340,0,0.0963817,"Missing"
P15-4020,2014.eamt-1.21,1,0.93638,"al., 2013; Bojar et al., 2014). QE at other textual levels have received much less attention. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). While certain sentences are perfect in isolation, their combination in context may lead to an incoherent document. Conversely, while a sentence can be poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a good quality document. Feature engineering is a challenge given the little availability of tools to extract discourse-wide information. In addition, no datasets with human-created labels are available and thus scores produced by automatic metr"
P15-4020,W15-4916,1,0.841275,"Missing"
P15-4020,2014.eamt-1.22,1,0.821768,"hine learning algorithms to build and test quality estimation models. Results on recent datasets show that Q U E ST++ achieves state-of-the-art performance. 1 Introduction Quality Estimation (QE) of Machine Translation (MT) have become increasingly popular over the last decade. With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting the best translation (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of sentence translations annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source f"
P15-4020,P10-1063,0,0.153973,"on. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). While certain sentences are perfect in isolation, their combination in context may lead to an incoherent document. Conversely, while a sentence can be poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a good quality document. Feature engineering is a challenge given the little availability of tools to extract discourse-wide information. In addition, no datasets with human-created labels are available and thus scores produced by automatic metrics have to be used as approximation (Scarton et al., 2015). Some applications require fine-g"
P15-4020,P13-4014,1,0.872058,"Missing"
P15-4020,2011.eamt-1.12,1,0.122579,"a higher level (e.g. sentences). Q U E ST++ allows the extraction of a variety of features, and provides machine learning algorithms to build and test quality estimation models. Results on recent datasets show that Q U E ST++ achieves state-of-the-art performance. 1 Introduction Quality Estimation (QE) of Machine Translation (MT) have become increasingly popular over the last decade. With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting the best translation (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of sentence translations annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012. While standard"
P15-4020,W14-3339,0,0.0335233,"esources and preprocessing steps so that extractors for new features can be easily added. The basic functioning of the feature extraction module requires raw text files with the source and translation texts, and a few resources (where available) such as the MT source training corpus and source and target language models (LMs). Configuration files are used to indicate paths for resources and the features that should be extracted. For its main resources (e.g. LMs), if a resource is missing, Q U E ST++ can generate it automatically. 3.1 Word level We explore a range of features from recent work (Bicici and Way, 2014; Camargo de Souza et al., 2014; Luong et al., 2014; Wisniewski et al., 2014), totalling 40 features of seven types: Figure 1 depicts the architecture of Q U E ST++ . Document and Paragraph classes are used for document-level feature extraction. A Document is a group of Paragraphs, which in turn is a group of Sentences. Sentence is used for both word- and sentence-level feature extraction. A Feature Processing Module was created for each level. Each processing level is independent and can deal with the peculiarities of its type of feature. Target context These are features that explore the con"
P15-4020,C04-1046,0,0.539275,"uality Prediction with Q U E ST++ Lucia Specia, Gustavo Henrique Paetzold and Carolina Scarton Department of Computer Science University of Sheffield, UK {l.specia,ghpaetzold1,c.scarton}@sheffield.ac.uk extraction toolkits are available for that: A SIYA1 and Q U E ST2 (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of participants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014). QE at other textual levels have received much less attention. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). While ce"
P15-4020,D12-1097,0,0.0149439,"at always predicts “Unintelligible” for Multi-Class, “Fluency” for Level 1, and “Bad” for the Binary setup. Document level Results The F-1 scores for the WMT14 datasets are given in Tables 1–4, for Q U E ST++ and systems that oficially participated in the task. The results show that Q U E ST++ was able to outperform all participating systems in WMT14 except for the English-Spanish baseline in the Binary and Level 1 tasks. The results in Table 5 also highlight the importance of selecting an adequate learning algorithm in CRF models. Our document-level features follow from those in the work of (Wong and Kit, 2012) on MT evaluation and (Scarton and Specia, 2014) for documentlevel QE. Nine features are extracted, in addition to aggregated values of sentence-level features for the entire document: • content words/lemmas/nouns repetition in S/T , • ratio of content words/lemmas/nouns in S/T , 4 System Q U E ST++ Baseline LIG/BL LIG/FS FBK-1 FBK-2 LIMSI RTM-1 RTM-2 Experiments In what follows, we evaluate Q U E ST++’s performance for the three prediction levels and various datasets. 4.1 Word-level QE Binary 0.502 0.525 0.441 0.444 0.487 0.426 0.473 0.350 0.328 Level 1 0.392 0.404 0.317 0.317 0.372 0.385 − 0"
P15-4020,P11-1022,0,\N,Missing
P15-4020,W14-3344,0,\N,Missing
P15-4020,2015.eamt-1.17,1,\N,Missing
P15-4021,J90-2002,0,0.311473,"es on the automated mark-up of WAs, as typically produced by WA tools. Different from most previous work, it presents the alignment information graphically in a WA matrix that can be easily understood by users, as opposed to text connected by lines. The key features of the tool are the ability to visualise WA matrices for multiple parallel aligned sentences simultaneously in a single place, coupled with powerful search and selection components to find and inspect particular sentences as required. 1 Introduction Automatically generated WA of parallel sentences, as introduced by the IBM models (Brown et al., 1990), is a mapping between source words and target words. It plays a vital role in Statistical Machine Translation (SMT) as the initial step to generate translation rules in most state of the art SMT approaches. It is also widely classed as a valuable linguistic resource for multilingual text processing in general. Accurate WAs form the basis for constructing probabilistic word or phrase-based translation dictionaries, as well as the generation of more elaborate translation rules, such as hierarchical 2 Visualising Word Alignments With the continuing attention given to SMT and the overarching impo"
P15-4021,P08-4006,0,0.432198,"in Figure 1, but in the grid style. Single blocks show mappings between individual elements (e.g. ‘Mary’ and ‘Maria’) whereas multiple blocks appearing in the same row or column tend to show phrases mapping to single words or other phrases (e.g. ‘did not’ maps to ‘no’). As can be seen from Figures 1, 2 and 3 the same information is clearly presented in two different formats, both of which are more intuitive than showing text and word position numbers only. The tools described so far are static and only show visual representations of WAs. Tools such as Yawat (Yet Another Word Alignment Tool) (Germann, 2008) and the SWIFT Aligner (Gilmanov et al., 2014), however, allow the direct manipulation and editing of WAs via graphical interfaces. Picaro – a simple command-line alignment visualisation tool (Riesa, 2011) – uses the grid style to display information. It also has an online demo web page1 that allows for the demonstration of the tool within a browser for a single parallel sentence. Although Picaro is a relatively simple tool, the visual presentation of the grid format on the demonstration web page is clear and is ideal for 1 122 http://nlg.isi.edu/demos/picaro/ quickly understanding WAs. Our re"
P15-4021,gilmanov-etal-2014-swift,0,0.34635,"ingle blocks show mappings between individual elements (e.g. ‘Mary’ and ‘Maria’) whereas multiple blocks appearing in the same row or column tend to show phrases mapping to single words or other phrases (e.g. ‘did not’ maps to ‘no’). As can be seen from Figures 1, 2 and 3 the same information is clearly presented in two different formats, both of which are more intuitive than showing text and word position numbers only. The tools described so far are static and only show visual representations of WAs. Tools such as Yawat (Yet Another Word Alignment Tool) (Germann, 2008) and the SWIFT Aligner (Gilmanov et al., 2014), however, allow the direct manipulation and editing of WAs via graphical interfaces. Picaro – a simple command-line alignment visualisation tool (Riesa, 2011) – uses the grid style to display information. It also has an online demo web page1 that allows for the demonstration of the tool within a browser for a single parallel sentence. Although Picaro is a relatively simple tool, the visual presentation of the grid format on the demonstration web page is clear and is ideal for 1 122 http://nlg.isi.edu/demos/picaro/ quickly understanding WAs. Our research in SMT requires the use of this type of"
P15-4021,P13-2060,0,0.0253741,"es are useful tools, if the user is looking for more specific sentences then they can use searches combined with basic regular expressions (RE). Figure 5 is an example of WAs returned using the RE search term ‘if.*, then’ which is being used to examine sentences containing the if/then conditional. Using the RE search term ‘if.*, then’ matches any sentence that contains: ‘if’ followed by any number of characters (.*) followed by a comma and space and finally a ‘then’. Being able to use REs makes the search very flexible and helps to pinpoint specific examples. Phrase Pairs (Minimal Bi-phrases) Koehn (2013) describes the idea of extracting phrase pairs from word alignments for phrase-based SMT. The reasoning is that if a phrase pair has been identified, it can then be used as evidence for the translation of future occurrences of the phrase. Figure 7 shows an example where ‘assumes that’ has been mapped to ‘geht davon aus , dass’. Using this idea we enabled our software to highlight phrase pairs in order to better evaluate the WAs not just for single words, but also for entire phrases. The input file remains the same, but when the optional Using Browser Features 124 Figure 7: A WA grid showing a"
P15-4021,smith-jahr-2000-cairo,0,0.866537,"Missing"
P15-4021,tiedemann-2006-isa,0,0.064452,"Missing"
P16-2013,J08-4004,0,0.033756,"ences (Diff. ref.), same-reference (Same ref.) and source-based evaluation (Source) As shown in Table 1, the agreement is consistently lower for annotators using different references. In other words, the same MT outputs systematically receive different scores when differ1 In MT evaluation, agreement is usually computed using standard k both for ranking different translations and for scoring translations on an interval-level scale. We note, however, that weighted k is more appropriate for scoring, since it allows the use of weights to describe the closeness of the agreement between categories (Artstein and Poesio, 2008). 79 ent human translations are used for their evaluation. Here and in what follows, the differences between the results for the same-reference annotator group and different-reference annotator group were found to be statistically significant with pvalue &lt; 0.01. The agreement between annotators using the source sentences is slightly lower than in the monolingual, same-reference scenario, but it is higher than in the case of the different-reference group. This may be an indication that referencebased evaluation is an easier task for annotators, perhaps because in this case they are not required"
P16-2013,P02-1040,0,0.0972104,"for the MT outputs using each of the references and selected the sentences with the highest standard deviation between the scores. chine Translation (Callison-Burch et al., 2007; Bojar et al., 2015)), manual assessment is expected to consider both MT fluency and adequacy, with a human (reference) translation commonly used as a proxy for the source text to allow for adequacy judgement by monolingual judges. The reference bias problem has been extensively discussed in the context of automatic MT evaluation. Evaluation systems based on stringlevel comparison, such as the well known BLEU metric (Papineni et al., 2002) heavily penalize potentially acceptable variations between MT and human reference. A variety of methods have been proposed to address this issue, from using multiple references (Dreyer and Marcu, 2012) to referencefree evaluation (Specia et al., 2010). Research in manual evaluation has focused on overcoming annotator bias, i.e. the preferences and expectations of individual annotators with respect to translation quality that lead to low levels of inter-annotator agreement (Cohn and Specia, 2013; Denkowski and Lavie, 2010; Graham et al., 2013; Guzm´an et al., 2015). The problem of reference bi"
P16-2013,W07-0718,0,0.071206,"acceptable linguistic variation by allowing for synonym and paraphrase matching, the resulting score is only 0.33, which shows that, not surprisingly, human translations vary substantially. To make the annotation process feasible given the resources available, we selected a subset of 100 source sentences for the experiment. To ensure variable levels of similarity between the MT and each of the references, we computed sentencelevel Meteor scores for the MT outputs using each of the references and selected the sentences with the highest standard deviation between the scores. chine Translation (Callison-Burch et al., 2007; Bojar et al., 2015)), manual assessment is expected to consider both MT fluency and adequacy, with a human (reference) translation commonly used as a proxy for the source text to allow for adequacy judgement by monolingual judges. The reference bias problem has been extensively discussed in the context of automatic MT evaluation. Evaluation systems based on stringlevel comparison, such as the well known BLEU metric (Papineni et al., 2002) heavily penalize potentially acceptable variations between MT and human reference. A variety of methods have been proposed to address this issue, from usin"
P16-2013,1994.amta-1.25,0,0.718197,"Missing"
P16-2013,P13-1004,1,0.851623,"valuation. Evaluation systems based on stringlevel comparison, such as the well known BLEU metric (Papineni et al., 2002) heavily penalize potentially acceptable variations between MT and human reference. A variety of methods have been proposed to address this issue, from using multiple references (Dreyer and Marcu, 2012) to referencefree evaluation (Specia et al., 2010). Research in manual evaluation has focused on overcoming annotator bias, i.e. the preferences and expectations of individual annotators with respect to translation quality that lead to low levels of inter-annotator agreement (Cohn and Specia, 2013; Denkowski and Lavie, 2010; Graham et al., 2013; Guzm´an et al., 2015). The problem of reference bias, however, has not been examined in previous work. By contrast to automatic MT evaluation, monolingual quality assessment is considered unproblematic, since human annotators are supposed to recognize meaning-preserving variations between the MT output and a given human reference. However, as will be shown in what follows, manual evaluation is also strongly affected by biases due to specific reference translations. 3 3.2 We developed a simple online interface to collect human judgments. Our eva"
P16-2013,2010.amta-papers.20,0,0.0182928,"ystems based on stringlevel comparison, such as the well known BLEU metric (Papineni et al., 2002) heavily penalize potentially acceptable variations between MT and human reference. A variety of methods have been proposed to address this issue, from using multiple references (Dreyer and Marcu, 2012) to referencefree evaluation (Specia et al., 2010). Research in manual evaluation has focused on overcoming annotator bias, i.e. the preferences and expectations of individual annotators with respect to translation quality that lead to low levels of inter-annotator agreement (Cohn and Specia, 2013; Denkowski and Lavie, 2010; Graham et al., 2013; Guzm´an et al., 2015). The problem of reference bias, however, has not been examined in previous work. By contrast to automatic MT evaluation, monolingual quality assessment is considered unproblematic, since human annotators are supposed to recognize meaning-preserving variations between the MT output and a given human reference. However, as will be shown in what follows, manual evaluation is also strongly affected by biases due to specific reference translations. 3 3.2 We developed a simple online interface to collect human judgments. Our evaluation task was based on t"
P16-2013,W14-3348,0,0.0688176,"on Challenge (Przybocki et al., 2008), and annual Workshops on Statistical Ma77 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 77–82, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ing. All teams used the same translation guidelines, which emphasize faithfulness to the source sentence as one of the main requirements. We note that even in such a scenario, human translations differ from each other. We measured the average similarity between the four references in the dataset using the Meteor evaluation metric (Denkowski and Lavie, 2014). Meteor scores range between 0 and 1 and reflect the proportion of similar words occurring in similar order. This metric is normally used to compare the MT output with a human reference, but it can also be applied to measure similarity between any two translations. We computed Meteor for all possible combinations between the four available references and took the average score. Even though Meteor covers certain amount of acceptable linguistic variation by allowing for synonym and paraphrase matching, the resulting score is only 0.33, which shows that, not surprisingly, human translations vary"
P16-2013,N12-1017,0,0.0846644,"manual assessment is expected to consider both MT fluency and adequacy, with a human (reference) translation commonly used as a proxy for the source text to allow for adequacy judgement by monolingual judges. The reference bias problem has been extensively discussed in the context of automatic MT evaluation. Evaluation systems based on stringlevel comparison, such as the well known BLEU metric (Papineni et al., 2002) heavily penalize potentially acceptable variations between MT and human reference. A variety of methods have been proposed to address this issue, from using multiple references (Dreyer and Marcu, 2012) to referencefree evaluation (Specia et al., 2010). Research in manual evaluation has focused on overcoming annotator bias, i.e. the preferences and expectations of individual annotators with respect to translation quality that lead to low levels of inter-annotator agreement (Cohn and Specia, 2013; Denkowski and Lavie, 2010; Graham et al., 2013; Guzm´an et al., 2015). The problem of reference bias, however, has not been examined in previous work. By contrast to automatic MT evaluation, monolingual quality assessment is considered unproblematic, since human annotators are supposed to recognize"
P16-2013,W13-2305,0,0.0882533,"comparison, such as the well known BLEU metric (Papineni et al., 2002) heavily penalize potentially acceptable variations between MT and human reference. A variety of methods have been proposed to address this issue, from using multiple references (Dreyer and Marcu, 2012) to referencefree evaluation (Specia et al., 2010). Research in manual evaluation has focused on overcoming annotator bias, i.e. the preferences and expectations of individual annotators with respect to translation quality that lead to low levels of inter-annotator agreement (Cohn and Specia, 2013; Denkowski and Lavie, 2010; Graham et al., 2013; Guzm´an et al., 2015). The problem of reference bias, however, has not been examined in previous work. By contrast to automatic MT evaluation, monolingual quality assessment is considered unproblematic, since human annotators are supposed to recognize meaning-preserving variations between the MT output and a given human reference. However, as will be shown in what follows, manual evaluation is also strongly affected by biases due to specific reference translations. 3 3.2 We developed a simple online interface to collect human judgments. Our evaluation task was based on the adequacy criterion"
P16-2013,W15-3059,0,0.0378317,"Missing"
P16-2095,W15-3001,1,0.93314,"ers of an error cannot be solved unambiguously even by human annotators (Wisniewski et al., 2013). In order to take into account partially correct phrases (e.g. a 4-word “BAD” phrase where the first word was tagged as “OK” by a system and the remaining words were correctly tagged as “BAD”), we compute the number of true positives as the sum of percentages of words with correctly predicted tags for every “OK” phrase. The number of true negatives is defined analogously. Sequence correlation. The sequence correlation score was used as a secondary evaluation metric in the QE shared task at WMT15 (Bojar et al., 2015). Analogously to the phrase-level F1 -score, it is based on the intersection of spans of correct and incorrect words. It also weights the phrases to give them equal importance and penalises the difference in the number of phrases between the reference and the hypothesis. 3 Metrics comparison One of the most reliable ways of comparing metrics is to measure their correlation with human judgements. However, for the word-level QE task, asking humans to rate a system labelling or to compare the outputs of two or more QE systems is a very expensive process. A practical way of getting the human judge"
P16-2095,W03-0413,0,0.464531,"stem outputs and synthetically generated datasets and suggest a reliable alternative to the F1 -BAD score — the multiplication of F1 -scores for different classes. Other metrics have lower discriminative power and are biased by unfair labellings. 1 Introduction Quality estimation (QE) of machine translation (MT) is a task of determining the quality of an automatically translated text without any oracle (reference) translation. This task has lately been receiving significant attention: from confidence estimation (i.e. estimation of how confident a particular MT system is on a word or a phrase (Gandrabur and Foster, 2003)) it evolved to systemindependent QE and is performed at the word level (Luong et al., 2014), sentence level (Shah et al., 2013) and document level (Scarton et al., 2015). The emergence of a large variety of approaches to QE led to need for reliable ways to compare them. The evaluation metrics that have been used to compare the performance of systems participating in QE shared tasks1 have received some criticisms. Graham (2015) shows that Pearson correlation better suits for the evaluation of sentence-level QE systems than mean absolute error (MAE), often used for this purpose. Pearson correla"
P16-2095,P15-1174,0,0.0847917,"lately been receiving significant attention: from confidence estimation (i.e. estimation of how confident a particular MT system is on a word or a phrase (Gandrabur and Foster, 2003)) it evolved to systemindependent QE and is performed at the word level (Luong et al., 2014), sentence level (Shah et al., 2013) and document level (Scarton et al., 2015). The emergence of a large variety of approaches to QE led to need for reliable ways to compare them. The evaluation metrics that have been used to compare the performance of systems participating in QE shared tasks1 have received some criticisms. Graham (2015) shows that Pearson correlation better suits for the evaluation of sentence-level QE systems than mean absolute error (MAE), often used for this purpose. Pearson correlation evaluates how well a system captures 2 Metrics One of the reasons word-level QE is a challenging problem is the fact that “OK” and “BAD” labels are not equally important: we are generally more interested in finding incorrect words than in assigning a suitable category to every single word. An ideal metric should be oriented towards the recall for the “BAD” class. However, the case of F1 -BAD score shows that this is not th"
P16-2095,W14-3342,0,0.0871545,"score — the multiplication of F1 -scores for different classes. Other metrics have lower discriminative power and are biased by unfair labellings. 1 Introduction Quality estimation (QE) of machine translation (MT) is a task of determining the quality of an automatically translated text without any oracle (reference) translation. This task has lately been receiving significant attention: from confidence estimation (i.e. estimation of how confident a particular MT system is on a word or a phrase (Gandrabur and Foster, 2003)) it evolved to systemindependent QE and is performed at the word level (Luong et al., 2014), sentence level (Shah et al., 2013) and document level (Scarton et al., 2015). The emergence of a large variety of approaches to QE led to need for reliable ways to compare them. The evaluation metrics that have been used to compare the performance of systems participating in QE shared tasks1 have received some criticisms. Graham (2015) shows that Pearson correlation better suits for the evaluation of sentence-level QE systems than mean absolute error (MAE), often used for this purpose. Pearson correlation evaluates how well a system captures 2 Metrics One of the reasons word-level QE is a ch"
P16-2095,W15-3040,1,0.353033,"s have lower discriminative power and are biased by unfair labellings. 1 Introduction Quality estimation (QE) of machine translation (MT) is a task of determining the quality of an automatically translated text without any oracle (reference) translation. This task has lately been receiving significant attention: from confidence estimation (i.e. estimation of how confident a particular MT system is on a word or a phrase (Gandrabur and Foster, 2003)) it evolved to systemindependent QE and is performed at the word level (Luong et al., 2014), sentence level (Shah et al., 2013) and document level (Scarton et al., 2015). The emergence of a large variety of approaches to QE led to need for reliable ways to compare them. The evaluation metrics that have been used to compare the performance of systems participating in QE shared tasks1 have received some criticisms. Graham (2015) shows that Pearson correlation better suits for the evaluation of sentence-level QE systems than mean absolute error (MAE), often used for this purpose. Pearson correlation evaluates how well a system captures 2 Metrics One of the reasons word-level QE is a challenging problem is the fact that “OK” and “BAD” labels are not equally impor"
P16-2095,2013.mtsummit-papers.21,1,0.738223,"res for different classes. Other metrics have lower discriminative power and are biased by unfair labellings. 1 Introduction Quality estimation (QE) of machine translation (MT) is a task of determining the quality of an automatically translated text without any oracle (reference) translation. This task has lately been receiving significant attention: from confidence estimation (i.e. estimation of how confident a particular MT system is on a word or a phrase (Gandrabur and Foster, 2003)) it evolved to systemindependent QE and is performed at the word level (Luong et al., 2014), sentence level (Shah et al., 2013) and document level (Scarton et al., 2015). The emergence of a large variety of approaches to QE led to need for reliable ways to compare them. The evaluation metrics that have been used to compare the performance of systems participating in QE shared tasks1 have received some criticisms. Graham (2015) shows that Pearson correlation better suits for the evaluation of sentence-level QE systems than mean absolute error (MAE), often used for this purpose. Pearson correlation evaluates how well a system captures 2 Metrics One of the reasons word-level QE is a challenging problem is the fact that “"
P16-2095,2013.mtsummit-papers.15,0,0.0433376,"ect) words. Precision is the percentage of correctly identified spans among all the spans found by a system, recall is the percentage of correctly identified spans among the spans in the test data. However, in NER the correct borders of a named entity are of big importance, because failure to identify them results in an incorrect entity. On the other hand, the actual borders of an error span in QE are not as important: the primary goal is to identify the erroneous region in the sentence, the task of finding the exact borders of an error cannot be solved unambiguously even by human annotators (Wisniewski et al., 2013). In order to take into account partially correct phrases (e.g. a 4-word “BAD” phrase where the first word was tagged as “OK” by a system and the remaining words were correctly tagged as “BAD”), we compute the number of true positives as the sum of percentages of words with correctly predicted tags for every “OK” phrase. The number of true negatives is defined analogously. Sequence correlation. The sequence correlation score was used as a secondary evaluation metric in the QE shared task at WMT15 (Bojar et al., 2015). Analogously to the phrase-level F1 -score, it is based on the intersection o"
P16-2095,C00-2137,0,0.361002,"racy or speed. One such a downstream task can be computer-assisted translation, where the user translates a sentence having automatic translation as a draft, and word-level quality labels can highlight incorrect parts in a sentence. Improvements in productivity could show the degree of usefulness of the quality labels in this case. However, such an experiment is also very expensive to be performed. Therefore, we consider indirect ways of comparing the metrics’ reliability based on prelabelled gold-standard test sets. 586 3.1 Comparison on real systems pair of systems with randomisation tests (Yeh, 2000) with Bonferroni correction (Abdi, 2007). In order to evaluate the metrics’ performance we compute the system distinction coefficient d — the probability of two systems being significantly different, which is defined as the ratio between the number of significantly different pairs of systems and all pairs of systems. We also compute d for the top half and for the bottom half of the ranked systems list separately in order to check how well each metric can discriminate between better performing and worse performing systems.3 The results are shown in Table 1. For every synthetic dataset we show t"
P16-2095,W14-3302,1,\N,Missing
P18-2113,I17-1030,1,0.775628,"ingly, the Newsela corpus has a feature that has been ignored thus far: Each instance in the corpus was created for readers with a certain school grade level. Each original article has a label indicating its corresponding grade level (from 12 to 2), and may have various simplified versions, each for a different grade level. For example, a level 12 article may have simplified counterparts for levels 8 and 4. In other words, the corpus contains instances where the same input leads to different outputs. Disregarding this factor may lead to suboptimal models. To avoid this problem, previous work (Alva-Manchego et al., 2017; Zhang and Lapata, 2017; Scarton et al., 2018b) has used subsets of the corpus with only certain combinations of complex-simplified article pairs, e.g. adjacent or non-adjacent pairs. This however reduces the amount of data available for training. 2 System architecture Our approach follows that of Johnson et al. (2017), a multilingual MT approach that adds an artificial token to encode the target language to the beginning of each source sentence in the parallel corpus. With this modified version of the corpus, a single encoder-decoder architecture is used to deal with different language pairs"
P18-2113,W13-4813,1,0.918544,"Missing"
P18-2113,C16-1069,1,0.864667,"and 12 BLEU and SARI points, respectively), including when training data for the specific complex-simple combination of grade levels is not available, i.e. zero-shot learning. 1 Introduction Text simplification (TS) is the task of modifying an original text into a simpler version of it. One of the main parameters for defining a suitable simplification is the target audience. Examples include elderly, children, cognitively impaired users, nonnative speakers and low-literacy readers. Traditionally, work on TS has been divided in lexical simplification (LS) and syntactic simplification (SS). LS (Paetzold, 2016) deals with the identification and replacement of complex words or phrases. SS (Siddharthan, 2011) performs 1 https://newsela.com/data, v.2016-01-29. 712 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 712–718 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics • Many-to-one: joining – 2+ original sentences are aligned to a single simplified sentence. at training time can still be generated during testing. We show that our zero-shot learning models perform virtually as well as our grade/operati"
P18-2113,W11-1601,0,0.250409,"Carolina Scarton and Lucia Specia Department of Computer Science, University of Sheffield Regent Court, 211 Portobello Street, Sheffield, S1 4DP, UK {c.scarton,l.specia}@sheffield.ac.uk Abstract structural transformations such as changing a sentence from passive to active voice. However, most recent approaches learn transformations from corpora, addressing simplification at lexical and syntactic levels altogether. These include either learning tree-based transformations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013) or using machine translation (MT)-based techniques (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012; Narayan and Gardent, 2014; Nisioi et al., 2017; Zhang and Lapata, 2017). This paper uses the latter type of technique, which treats TS as a monolingual MT task, where an original text is “translated” into its simplified version. In order to build MT-based models, a parallel corpus of original texts with their simplified counterparts is needed. For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) (Zhu et al., 2010) and the Newsela Article Corpus.1 The former is a collection of original Wikipedia articles and their simplified versions create"
P18-2113,P11-2117,0,0.162146,"Carolina Scarton and Lucia Specia Department of Computer Science, University of Sheffield Regent Court, 211 Portobello Street, Sheffield, S1 4DP, UK {c.scarton,l.specia}@sheffield.ac.uk Abstract structural transformations such as changing a sentence from passive to active voice. However, most recent approaches learn transformations from corpora, addressing simplification at lexical and syntactic levels altogether. These include either learning tree-based transformations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013) or using machine translation (MT)-based techniques (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012; Narayan and Gardent, 2014; Nisioi et al., 2017; Zhang and Lapata, 2017). This paper uses the latter type of technique, which treats TS as a monolingual MT task, where an original text is “translated” into its simplified version. In order to build MT-based models, a parallel corpus of original texts with their simplified counterparts is needed. For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) (Zhu et al., 2010) and the Newsela Article Corpus.1 The former is a collection of original Wikipedia articles and their simplified versions create"
P18-2113,Q17-1024,0,0.0978286,"Missing"
P18-2113,P02-1040,0,0.100965,"cted between 0-{1,2,3,4}, 1{2,3,4}, 2-{3,4} and 3-4, where available. Our corpus is also larger than the ones used in (AlvaManchego et al., 2017; Scarton et al., 2018b) and (Zhang and Lapata, 2017). While the former use only adjacent levels (e.g. 0-1, 1-2) and the latter only non-adjacent levels (e.g. 0-2, 1-4), we make use of the full dataset. As baseline we trained a model using OpenNMT and the same hyperparameters as described in §2 on the entire Newsela corpus but without artificial tokens (s2s model). The state-of-theFigure 1: Neural model architecture. We evaluate our models with BLEU3 (Papineni et al., 2002) (a proxy for grammaticality assessment), SARI (Xu et al., 2016)4 (a proxy for simplicity assessment) and Flesch Reading Ease5 (a 2 Torch version: http://opennmt.net/OpenNMT/ The multi-blue.perl script from https://github. com/moses-smt/mosesdecoder 4 https://github.com/cocoxu/ simplification 5 https://github.com/mmautner/ readability 3 714 art model is represented by NTS, which was also trained on the entire corpus using a similar OpenNMT architecture with the same hyperparameters but additional pre-trained word embeddings as described in Nisioi et al. (2017).6 As shown in Table 2 the NTS sys"
P18-2113,L18-1685,1,0.921882,"n ignored thus far: Each instance in the corpus was created for readers with a certain school grade level. Each original article has a label indicating its corresponding grade level (from 12 to 2), and may have various simplified versions, each for a different grade level. For example, a level 12 article may have simplified counterparts for levels 8 and 4. In other words, the corpus contains instances where the same input leads to different outputs. Disregarding this factor may lead to suboptimal models. To avoid this problem, previous work (Alva-Manchego et al., 2017; Zhang and Lapata, 2017; Scarton et al., 2018b) has used subsets of the corpus with only certain combinations of complex-simplified article pairs, e.g. adjacent or non-adjacent pairs. This however reduces the amount of data available for training. 2 System architecture Our approach follows that of Johnson et al. (2017), a multilingual MT approach that adds an artificial token to encode the target language to the beginning of each source sentence in the parallel corpus. With this modified version of the corpus, a single encoder-decoder architecture is used to deal with different language pairs. Based on the tokens, the source sentences ar"
P18-2113,D15-1166,0,0.0530278,"the rust of the fence near Sasabe. dusty handprints could be seen on the fence near Sasabe. Table 1: Examples of artificial tokens used. fied to grade level 4 or grade level 2. Since the reference for grade level 4 is a copy of the original, the operation token for this case is <identical>. For level 2 the reference is a rewrite and, therefore, the operation token is <elaboration>. We use OpenNMT2 as our encoder-decoder architecture. Both encoder and decoder have two LSTM layers, hidden states of size 500 and dropout = 0.3. Global attention combined with input-feeding is used, as describe in (Luong et al., 2015). A model is trained for each dataset constructed with different artificial tokens for 13 epochs. The best model is selected according to perplexity on the development set. Figure 1 shows the architecture of the neural network, including attention and input-feeding. In this example, <token> represents the artificial token added to the pre-processed data. proxy for readability assessment). According to Xu et al. (2016), BLEU shows high correlation with human scores for grammaticality and meaning preservation, whilst SARI shows high correlation with human scores for simplicity. Although previous"
P18-2113,L18-1553,1,0.922898,"n ignored thus far: Each instance in the corpus was created for readers with a certain school grade level. Each original article has a label indicating its corresponding grade level (from 12 to 2), and may have various simplified versions, each for a different grade level. For example, a level 12 article may have simplified counterparts for levels 8 and 4. In other words, the corpus contains instances where the same input leads to different outputs. Disregarding this factor may lead to suboptimal models. To avoid this problem, previous work (Alva-Manchego et al., 2017; Zhang and Lapata, 2017; Scarton et al., 2018b) has used subsets of the corpus with only certain combinations of complex-simplified article pairs, e.g. adjacent or non-adjacent pairs. This however reduces the amount of data available for training. 2 System architecture Our approach follows that of Johnson et al. (2017), a multilingual MT approach that adds an artificial token to encode the target language to the beginning of each source sentence in the parallel corpus. With this modified version of the corpus, a single encoder-decoder architecture is used to deal with different language pairs. Based on the tokens, the source sentences ar"
P18-2113,P14-1041,0,0.138993,"Computer Science, University of Sheffield Regent Court, 211 Portobello Street, Sheffield, S1 4DP, UK {c.scarton,l.specia}@sheffield.ac.uk Abstract structural transformations such as changing a sentence from passive to active voice. However, most recent approaches learn transformations from corpora, addressing simplification at lexical and syntactic levels altogether. These include either learning tree-based transformations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013) or using machine translation (MT)-based techniques (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012; Narayan and Gardent, 2014; Nisioi et al., 2017; Zhang and Lapata, 2017). This paper uses the latter type of technique, which treats TS as a monolingual MT task, where an original text is “translated” into its simplified version. In order to build MT-based models, a parallel corpus of original texts with their simplified counterparts is needed. For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) (Zhu et al., 2010) and the Newsela Article Corpus.1 The former is a collection of original Wikipedia articles and their simplified versions created by volunteers. The latter consists of news arti"
P18-2113,I17-3007,1,0.831656,"ation: concatenation of the two above tokens. Different from the grade level, which can be available at test time simply by knowing the intended reader of the text, information about the operations to be performed, which we extracted from the parallel corpus, will not be available at test time. We use gold labels extracted from the parallel corpus for an oracle experiment but also use a classifier that predicts the operations for the test set based on those in the training data. We built a simple Naive Bayes classifier using the scikit-learn toolkit (Pedregosa et al., 2011) and nine features (Scarton et al., 2017): • number of tokens / punctuation / content words / clauses, • ratio of the number of verbs / nouns / adjectives / adverbs / connectives to the number of content words. Table 1 shows examples of the tokens used when an original instance is marked to be simpliWe propose a way of making use of this information to build more informed TS models that are aware of different types of target audiences, while still making use of the full dataset for learning. Inspired by the work of Johnson et al. (2017) for MT, we add to each original instance an artificial token that represents the target grade leve"
P18-2113,P17-2014,0,0.442451,"y of Sheffield Regent Court, 211 Portobello Street, Sheffield, S1 4DP, UK {c.scarton,l.specia}@sheffield.ac.uk Abstract structural transformations such as changing a sentence from passive to active voice. However, most recent approaches learn transformations from corpora, addressing simplification at lexical and syntactic levels altogether. These include either learning tree-based transformations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013) or using machine translation (MT)-based techniques (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012; Narayan and Gardent, 2014; Nisioi et al., 2017; Zhang and Lapata, 2017). This paper uses the latter type of technique, which treats TS as a monolingual MT task, where an original text is “translated” into its simplified version. In order to build MT-based models, a parallel corpus of original texts with their simplified counterparts is needed. For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) (Zhu et al., 2010) and the Newsela Article Corpus.1 The former is a collection of original Wikipedia articles and their simplified versions created by volunteers. The latter consists of news articles professionally s"
P18-2113,P17-2016,0,0.0447413,"erparts is needed. For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) (Zhu et al., 2010) and the Newsela Article Corpus.1 The former is a collection of original Wikipedia articles and their simplified versions created by volunteers. The latter consists of news articles professionally simplified for various specific audiences following the US school grade system. To build simplification models, the pairs of articles in these corpora have been aligned at the level of smaller units using standard algorithms (Coster and Kauchak, 2011b; Paetzold and ˇ Specia, 2016; Stajner et al., 2017). Based on the number of sentences involved in these alignments, one can categorise alignments into four types of coarse-grained simplification operations: • Identical: an original sentence is aligned to itself, i.e. no simplification is performed. • Elaboration: an original sentence is aligned to a single, rewritten simplified sentence. • One-to-many: splitting – an original sentence is aligned to 2+ simplified sentences. Text simplification (TS) is a monolingual text-to-text transformation task where an original (complex) text is transformed into a target (simpler) text. Most recent work is"
P18-2113,D11-1038,0,0.174376,"Missing"
P18-2113,P12-1107,0,0.468692,"Missing"
P18-2113,Q15-1021,0,0.211546,"e. at training time can still be generated during testing. We show that our zero-shot learning models perform virtually as well as our grade/operationinformed models (§4). To the best of our knowledge, this is the first work to build TS models for specific target audiences and to explore zero-shot learning for this application. We hereafter refer to the unit of simplification, i.e. one or more original or simplified sentences, as instances. The Newsela corpus is seen as having higher quality than W-SW because its simplifications are created by professionals, following well defined guidelines (Xu et al., 2015). It is also larger which is preferable for training corpus-based models. More interestingly, the Newsela corpus has a feature that has been ignored thus far: Each instance in the corpus was created for readers with a certain school grade level. Each original article has a label indicating its corresponding grade level (from 12 to 2), and may have various simplified versions, each for a different grade level. For example, a level 12 article may have simplified counterparts for levels 8 and 4. In other words, the corpus contains instances where the same input leads to different outputs. Disrega"
P18-2113,Q16-1029,0,0.324751,"architecture. Both encoder and decoder have two LSTM layers, hidden states of size 500 and dropout = 0.3. Global attention combined with input-feeding is used, as describe in (Luong et al., 2015). A model is trained for each dataset constructed with different artificial tokens for 13 epochs. The best model is selected according to perplexity on the development set. Figure 1 shows the architecture of the neural network, including attention and input-feeding. In this example, <token> represents the artificial token added to the pre-processed data. proxy for readability assessment). According to Xu et al. (2016), BLEU shows high correlation with human scores for grammaticality and meaning preservation, whilst SARI shows high correlation with human scores for simplicity. Although previous work have also relied on human judgements of grammaticality, meaning preservation and simplicity, in our case such a type of evaluation is infeasible: we would need to involve judges with specific grade levels or rely on professionals who are experts in grade level-specific simplification to make such assessments. 3 Reader-specific TS models Our version of the Newsela corpus has 550, 644 instance pairs (11M original"
P18-2113,D17-1062,0,0.597037,"Court, 211 Portobello Street, Sheffield, S1 4DP, UK {c.scarton,l.specia}@sheffield.ac.uk Abstract structural transformations such as changing a sentence from passive to active voice. However, most recent approaches learn transformations from corpora, addressing simplification at lexical and syntactic levels altogether. These include either learning tree-based transformations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013) or using machine translation (MT)-based techniques (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012; Narayan and Gardent, 2014; Nisioi et al., 2017; Zhang and Lapata, 2017). This paper uses the latter type of technique, which treats TS as a monolingual MT task, where an original text is “translated” into its simplified version. In order to build MT-based models, a parallel corpus of original texts with their simplified counterparts is needed. For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) (Zhu et al., 2010) and the Newsela Article Corpus.1 The former is a collection of original Wikipedia articles and their simplified versions created by volunteers. The latter consists of news articles professionally simplified for various spe"
P18-2113,C10-1152,0,0.271894,"Target Audiences Carolina Scarton and Lucia Specia Department of Computer Science, University of Sheffield Regent Court, 211 Portobello Street, Sheffield, S1 4DP, UK {c.scarton,l.specia}@sheffield.ac.uk Abstract structural transformations such as changing a sentence from passive to active voice. However, most recent approaches learn transformations from corpora, addressing simplification at lexical and syntactic levels altogether. These include either learning tree-based transformations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013) or using machine translation (MT)-based techniques (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012; Narayan and Gardent, 2014; Nisioi et al., 2017; Zhang and Lapata, 2017). This paper uses the latter type of technique, which treats TS as a monolingual MT task, where an original text is “translated” into its simplified version. In order to build MT-based models, a parallel corpus of original texts with their simplified counterparts is needed. For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) (Zhu et al., 2010) and the Newsela Article Corpus.1 The former is a collection of original Wikipedia articles and their"
P19-1653,W18-6402,1,0.885643,"machine translation (MMT) is an area of research that addresses the task of translating texts using context from an additional modality, generally static images. The assumption is that the visual context can help ground the meaning of the text and, as a consequence, generate more adequate translations. Current work has focused on datasets of images paired with their descriptions, which are crowdsourced in English and then translated into different languages, namely the Multi30K dataset (Elliott et al., 2016). Results from the most recent evaluation campaigns in the area (Elliott et al., 2017; Barrault et al., 2018) have shown that visual information can be helpful, as humans generally prefer translations generated by multimodal models than by their text-only counterparts. However, previous work has also shown that images are only needed in very specific cases (Lala et al., 2018). This is also the case for humans. Frank et al. (2018) (see Figure 1) concluded that visual information is needed by humans in the presence of the following: incorrect or ambiguous source words and gender-neutral words that need to be marked for gender in the target language. In an experiment where human translators were asked t"
P19-1653,W17-4746,0,0.116921,"Missing"
P19-1653,N19-1422,1,0.746529,"rocess and uses visual information only at this stage, and (ii) in addition to convolutional filters we use objectlevel visual information. The latter has only been explored to generate a single global representation (Gr¨onroos et al., 2018) and used for example to initialise the encoder (Huang et al., 2016). We note that translation refinement is different translation re-ranking from a text-only model based on image representation (Shah et al., 2016; Hitschler et al., 2016; Lala et al., 2018), since the latter assumes that the correct translation can already be produced by a text-only model. Caglayan et al. (2019) investigate the importance and the contribution of multimodality for MMT. They perform careful experiments by using input degradation and observe that, specially under limited textual context, multimodal models exploit the visual input to generate better translations. Caglayan et al. (2019) also show that MMT systems exploit visual cues and obtain correct translations even with typographical errors in the source sentences. In this paper, we build upon this idea and investigate the potential of visual cues for refining translation. Translation refinement: The idea of treating machine translati"
P19-1653,D17-1105,0,0.182115,"se RNN-based sequence to sequence models (Bahdanau et al., 2015) enhanced with a single, global image vector, extracted as one of the layers of a CNN trained for object classification (He et al., 2016), often the penultimate or final layer. The image representation is integrated into the MT models by initialising the encoder or decoder (Elliott et al., 2015; Caglayan et al., 2017; Madhyastha et al., 2017); element-wise multiplication with the source word annotations (Caglayan et al., 2017); or projecting the image representation and encoder context to a common space to initialise the decoder (Calixto and Liu, 2017). Elliott and K´ad´ar (2017) and Helcl et al. (2018) instead model the source sentence and reconstruct the image representation jointly via multi-task learning. An alternative way of exploring image representations is to have an attention mechanism 6526 (Bahdanau et al., 2015) on the output of the last convolutional layer of a CNN (Xu et al., 2015). The layer represents the activation of K different convolutional filters on evenly quantised N × N spatial regions of the image. Caglayan et al. (2017) learn the attention weights for both source text and visual encoders, while Calixto et al. (2017"
P19-1653,P17-1175,0,0.173101,"Missing"
P19-1653,I17-1014,0,0.188142,"Missing"
P19-1653,W19-3821,0,0.0348707,"Missing"
P19-1653,W18-6439,0,0.0766196,"Missing"
P19-1653,D12-1108,0,0.0138747,"odal models exploit the visual input to generate better translations. Caglayan et al. (2019) also show that MMT systems exploit visual cues and obtain correct translations even with typographical errors in the source sentences. In this paper, we build upon this idea and investigate the potential of visual cues for refining translation. Translation refinement: The idea of treating machine translation as a two step approach dates back to statistical models, e.g. in order to improve a draft sentence-level translation by exploring document-wide context through hill-climbing for local refinements (Hardmeier et al., 2012). Iterative refinement approaches have also been proposed that start with a draft translation and then predict discrete substitutions based on an attention mechanism (Novak et al., 2016), or using nonautoregressive methods with a focus on speeding up decoding (Lee et al., 2018). Translation refinement can also be done through learning a separate model for automatic post-editing (Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Chatterjee et al., 2018), but this requires additional training data with draft translations and their correct version. An interesting approach is that of d"
P19-1653,D17-1095,0,0.127598,"Missing"
P19-1653,W14-3348,0,0.0633926,"re-processed the initial dataset to remove noise. We also add the gender-marked pronouns he, she, her and his to the person word list. 6529 (base+sum); (c) base with AIF using spacial (base+att) or object based (base+obj) image features; (d) standard deliberation model (del); (e) deliberation models enriched with image information: del+sum, del+att and del+obj. source degradation (Section 5.1) and then in the setup with various source degradation strategies (Section 5.2). 4.4 Table 2 shows the results of our main experiments on the 2016 and 2018 test sets for French and German. We use Meteor (Denkowski and Lavie, 2014) as the main metric, as in the WMT tasks (Barrault et al., 2018). We compare our transformer baseline to transformer models enriched with image information, as well as to the deliberation models, with or without image information. We first note that our multimodal models achieve the state of the art performance for transformer networks (constrained models) on the English-German dataset, as compared to (Helcl et al., 2018). Second, our deliberation models lead to significant improvements over this baseline across test sets (average ∆METEOR = 1, ∆BLEU = 1). Transformer-based models enriched with"
P19-1653,W17-4718,1,0.890615,"troduction Multimodal machine translation (MMT) is an area of research that addresses the task of translating texts using context from an additional modality, generally static images. The assumption is that the visual context can help ground the meaning of the text and, as a consequence, generate more adequate translations. Current work has focused on datasets of images paired with their descriptions, which are crowdsourced in English and then translated into different languages, namely the Multi30K dataset (Elliott et al., 2016). Results from the most recent evaluation campaigns in the area (Elliott et al., 2017; Barrault et al., 2018) have shown that visual information can be helpful, as humans generally prefer translations generated by multimodal models than by their text-only counterparts. However, previous work has also shown that images are only needed in very specific cases (Lala et al., 2018). This is also the case for humans. Frank et al. (2018) (see Figure 1) concluded that visual information is needed by humans in the presence of the following: incorrect or ambiguous source words and gender-neutral words that need to be marked for gender in the target language. In an experiment where human"
P19-1653,W18-6441,0,0.240265,"Missing"
P19-1653,P16-1227,0,0.0319564,"ation. However, we differ in two main aspects (Section 3): (i) our approach explores additional textual context through a second pass decoding process and uses visual information only at this stage, and (ii) in addition to convolutional filters we use objectlevel visual information. The latter has only been explored to generate a single global representation (Gr¨onroos et al., 2018) and used for example to initialise the encoder (Huang et al., 2016). We note that translation refinement is different translation re-ranking from a text-only model based on image representation (Shah et al., 2016; Hitschler et al., 2016; Lala et al., 2018), since the latter assumes that the correct translation can already be produced by a text-only model. Caglayan et al. (2019) investigate the importance and the contribution of multimodality for MMT. They perform careful experiments by using input degradation and observe that, specially under limited textual context, multimodal models exploit the visual input to generate better translations. Caglayan et al. (2019) also show that MMT systems exploit visual cues and obtain correct translations even with typographical errors in the source sentences. In this paper, we build upon"
P19-1653,W16-2360,0,0.1081,"ach is learnt independently. Helcl et al. (2018) is the closest to our work: we also use a doubly-attentive transformer architecture and explore spatial visual information. However, we differ in two main aspects (Section 3): (i) our approach explores additional textual context through a second pass decoding process and uses visual information only at this stage, and (ii) in addition to convolutional filters we use objectlevel visual information. The latter has only been explored to generate a single global representation (Gr¨onroos et al., 2018) and used for example to initialise the encoder (Huang et al., 2016). We note that translation refinement is different translation re-ranking from a text-only model based on image representation (Shah et al., 2016; Hitschler et al., 2016; Lala et al., 2018), since the latter assumes that the correct translation can already be produced by a text-only model. Caglayan et al. (2019) investigate the importance and the contribution of multimodality for MMT. They perform careful experiments by using input degradation and observe that, specially under limited textual context, multimodal models exploit the visual input to generate better translations. Caglayan et al. ("
P19-1653,I17-1013,0,0.0234732,"ates back to statistical models, e.g. in order to improve a draft sentence-level translation by exploring document-wide context through hill-climbing for local refinements (Hardmeier et al., 2012). Iterative refinement approaches have also been proposed that start with a draft translation and then predict discrete substitutions based on an attention mechanism (Novak et al., 2016), or using nonautoregressive methods with a focus on speeding up decoding (Lee et al., 2018). Translation refinement can also be done through learning a separate model for automatic post-editing (Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Chatterjee et al., 2018), but this requires additional training data with draft translations and their correct version. An interesting approach is that of deliberation networks, which jointly train an encoder and first and second stage decoders (Xia et al., 2017). The second stage decoder has access to both left and right side context and this has been shown to improve translation (Xia et al., 2017; Hassan et al., 2018). We follow this approach as it offers a very flexible framework to incorporate additional information in the second stage decoder. 3 Model We base our model on the transforme"
P19-1653,W18-6442,1,0.862188,"r in two main aspects (Section 3): (i) our approach explores additional textual context through a second pass decoding process and uses visual information only at this stage, and (ii) in addition to convolutional filters we use objectlevel visual information. The latter has only been explored to generate a single global representation (Gr¨onroos et al., 2018) and used for example to initialise the encoder (Huang et al., 2016). We note that translation refinement is different translation re-ranking from a text-only model based on image representation (Shah et al., 2016; Hitschler et al., 2016; Lala et al., 2018), since the latter assumes that the correct translation can already be produced by a text-only model. Caglayan et al. (2019) investigate the importance and the contribution of multimodality for MMT. They perform careful experiments by using input degradation and observe that, specially under limited textual context, multimodal models exploit the visual input to generate better translations. Caglayan et al. (2019) also show that MMT systems exploit visual cues and obtain correct translations even with typographical errors in the source sentences. In this paper, we build upon this idea and inves"
P19-1653,D18-1149,0,0.0285391,"he potential of visual cues for refining translation. Translation refinement: The idea of treating machine translation as a two step approach dates back to statistical models, e.g. in order to improve a draft sentence-level translation by exploring document-wide context through hill-climbing for local refinements (Hardmeier et al., 2012). Iterative refinement approaches have also been proposed that start with a draft translation and then predict discrete substitutions based on an attention mechanism (Novak et al., 2016), or using nonautoregressive methods with a focus on speeding up decoding (Lee et al., 2018). Translation refinement can also be done through learning a separate model for automatic post-editing (Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Chatterjee et al., 2018), but this requires additional training data with draft translations and their correct version. An interesting approach is that of deliberation networks, which jointly train an encoder and first and second stage decoders (Xia et al., 2017). The second stage decoder has access to both left and right side context and this has been shown to improve translation (Xia et al., 2017; Hassan et al., 2018). We follow"
P19-1653,P17-2031,0,0.0819461,"age representation jointly via multi-task learning. An alternative way of exploring image representations is to have an attention mechanism 6526 (Bahdanau et al., 2015) on the output of the last convolutional layer of a CNN (Xu et al., 2015). The layer represents the activation of K different convolutional filters on evenly quantised N × N spatial regions of the image. Caglayan et al. (2017) learn the attention weights for both source text and visual encoders, while Calixto et al. (2017); Delbrouck and Dupont (2017) combine both attentions independently via a gating scalar, and Libovick´y and Helcl (2017); Helcl et al. (2018) apply a hierarchical attention distribution over two projected vectors where the attention for each is learnt independently. Helcl et al. (2018) is the closest to our work: we also use a doubly-attentive transformer architecture and explore spatial visual information. However, we differ in two main aspects (Section 3): (i) our approach explores additional textual context through a second pass decoding process and uses visual information only at this stage, and (ii) in addition to convolutional filters we use objectlevel visual information. The latter has only been explore"
P19-1653,W17-4752,1,0.854693,"ns for transformer-based architectures (Section 3.3). 2 Related work MMT: Approaches to MMT vary with regards to how they represent images and how they incorporate this information in the models. Initial approaches use RNN-based sequence to sequence models (Bahdanau et al., 2015) enhanced with a single, global image vector, extracted as one of the layers of a CNN trained for object classification (He et al., 2016), often the penultimate or final layer. The image representation is integrated into the MT models by initialising the encoder or decoder (Elliott et al., 2015; Caglayan et al., 2017; Madhyastha et al., 2017); element-wise multiplication with the source word annotations (Caglayan et al., 2017); or projecting the image representation and encoder context to a common space to initialise the decoder (Calixto and Liu, 2017). Elliott and K´ad´ar (2017) and Helcl et al. (2018) instead model the source sentence and reconstruct the image representation jointly via multi-task learning. An alternative way of exploring image representations is to have an attention mechanism 6526 (Bahdanau et al., 2015) on the output of the last convolutional layer of a CNN (Xu et al., 2015). The layer represents the activatio"
P19-1653,W18-5455,1,0.77433,"18), where the decoder block now contains an additional cross-attention sub-layer AD0 →V which attends to the visual information (V). The keys and values correspond to the visual information. Within the deliberation network framework, based on the previously discussed observation (Section 1) that images are only needed in a small number of cases, we propose to add visual crossattention only to the second-pass decoder block (see Figure 2). 3.3 Image features Motivated by previous work that indicates the importance of structured information from images (Caglayan et al., 2017; Wang et al., 2018; Madhyastha et al., 2018), we focus on structural forms of image representations, including the spatially aware feature maps from CNNs and information extracted from automatic object detectors. Spatial image features: We use spatial feature maps from the last convolutional layer of a pretrained ResNet-50 (He et al., 2016) CNN-based image classifier for every image.3 These feature maps contain output activations for various filters while preserving spatial information. They have been used in various vision to language tasks including image captioning (Xu et al., 2015) and multimodal machine translation (Section 2). Our"
P19-1653,E12-1076,0,0.0455155,"of these features into the deliberation network is shown in Figure 2, setup (b). We use the the AIF setup and refer to models that use the representation as att. Object-based image features: We use a bag-of-objects representation where the objects are obtained using an off-shelf object detector (Kuznetsova et al., 2018) based on the Open Images dataset. This representations is a sparse 545-dimensional vector with the frequency of each (545) given object in an image. This is inspired by previous research that investigates the potential of object-based information for vision to language tasks (Mitchell et al., 2012; Wang et al., 2018). We 3 Provided at http://statmt.org/wmt18/ multimodal-task.html. 6528 use the the AIC setup and refer to models that use the representation as sum. Object-based embedding features: The bagof-objects representations makes it hard to exploit object-to-object similarity, since visual representations of different objects can be very different. To mitigate this, we propose a simple extension using bag-of-object embeddings. We represent each object using the pre-trained GLoVebased (Pennington et al., 2014) 50-dimensional word vectors for their categories (e.g. woman). We use the"
P19-1653,C16-1172,0,0.0566867,"Missing"
P19-1653,D14-1162,0,0.0830904,"ates the potential of object-based information for vision to language tasks (Mitchell et al., 2012; Wang et al., 2018). We 3 Provided at http://statmt.org/wmt18/ multimodal-task.html. 6528 use the the AIC setup and refer to models that use the representation as sum. Object-based embedding features: The bagof-objects representations makes it hard to exploit object-to-object similarity, since visual representations of different objects can be very different. To mitigate this, we propose a simple extension using bag-of-object embeddings. We represent each object using the pre-trained GLoVebased (Pennington et al., 2014) 50-dimensional word vectors for their categories (e.g. woman). We use the the AIF based setup and refer to models that use the representation as obj (Figure 2 setup (a)). 4 Experimental settings 4.1 Data We build and test our MMT models on the Multi30K dataset (Elliott et al., 2016). Each image in Multi30K contains one English (EN) description taken from Flickr30K (Young et al., 2014) and human translations into German (DE), French (FR) and Czech (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). The dataset contains 29,000 instances for training, 1,014 for development, and 1"
P19-1653,W16-2363,1,0.795787,"atial visual information. However, we differ in two main aspects (Section 3): (i) our approach explores additional textual context through a second pass decoding process and uses visual information only at this stage, and (ii) in addition to convolutional filters we use objectlevel visual information. The latter has only been explored to generate a single global representation (Gr¨onroos et al., 2018) and used for example to initialise the encoder (Huang et al., 2016). We note that translation refinement is different translation re-ranking from a text-only model based on image representation (Shah et al., 2016; Hitschler et al., 2016; Lala et al., 2018), since the latter assumes that the correct translation can already be produced by a text-only model. Caglayan et al. (2019) investigate the importance and the contribution of multimodality for MMT. They perform careful experiments by using input degradation and observe that, specially under limited textual context, multimodal models exploit the visual input to generate better translations. Caglayan et al. (2019) also show that MMT systems exploit visual cues and obtain correct translations even with typographical errors in the source sentences. In t"
P19-1653,W16-2346,1,0.899669,"Missing"
P19-1653,D18-1334,0,0.0467768,"Missing"
P19-1653,W18-1819,0,0.0289858,"ation networks, which jointly train an encoder and first and second stage decoders (Xia et al., 2017). The second stage decoder has access to both left and right side context and this has been shown to improve translation (Xia et al., 2017; Hassan et al., 2018). We follow this approach as it offers a very flexible framework to incorporate additional information in the second stage decoder. 3 Model We base our model on the transformer architecture (Vaswani et al., 2017) for neural machine translation. Our implementation is a multilayer encoder-decoder architecture that uses the tensor2tensor1 (Vaswani et al., 2018) library. The encoder and decoder blocks are as follows: Encoder Block (E): The encoder block comprises of 6 layers, with each containing two sublayers of multi-head self-attention mechanism followed by a fully connected feed forward neural network. We follow the standard implementation and employ residual connections between each layer, as well as layer normalisation. The output of the encoder forms the encoder memory which consists of contextualised representations for each of the source tokens (ME ). Decoder Block (D): The decoder block also comprises of 6 layers. It contains an additional"
P19-1653,N18-1198,1,0.929641,"in Helcl et al. (2018), where the decoder block now contains an additional cross-attention sub-layer AD0 →V which attends to the visual information (V). The keys and values correspond to the visual information. Within the deliberation network framework, based on the previously discussed observation (Section 1) that images are only needed in a small number of cases, we propose to add visual crossattention only to the second-pass decoder block (see Figure 2). 3.3 Image features Motivated by previous work that indicates the importance of structured information from images (Caglayan et al., 2017; Wang et al., 2018; Madhyastha et al., 2018), we focus on structural forms of image representations, including the spatially aware feature maps from CNNs and information extracted from automatic object detectors. Spatial image features: We use spatial feature maps from the last convolutional layer of a pretrained ResNet-50 (He et al., 2016) CNN-based image classifier for every image.3 These feature maps contain output activations for various filters while preserving spatial information. They have been used in various vision to language tasks including image captioning (Xu et al., 2015) and multimodal machine tr"
P19-1653,Q14-1006,0,0.235002,"visual representations of different objects can be very different. To mitigate this, we propose a simple extension using bag-of-object embeddings. We represent each object using the pre-trained GLoVebased (Pennington et al., 2014) 50-dimensional word vectors for their categories (e.g. woman). We use the the AIF based setup and refer to models that use the representation as obj (Figure 2 setup (a)). 4 Experimental settings 4.1 Data We build and test our MMT models on the Multi30K dataset (Elliott et al., 2016). Each image in Multi30K contains one English (EN) description taken from Flickr30K (Young et al., 2014) and human translations into German (DE), French (FR) and Czech (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for test. We only experiment with German and French, which are languages for which we have in-house expertise for the type of analysis we present. In addition to the official Multi30K test set (test 2016), we also use the test set from the latest WMT evaluation competition, test 2018 (Barrault et al., 2018).4 4.2 Degradation of source In addition to using the Multi30K dataset as i"
P19-1653,N18-2003,0,0.0480834,"Missing"
P19-1654,W14-3348,0,0.0736386,"e string matching. This is done by building on the Word Mover’s Distance (WMD) metric, which measures the distance between two texts in a word embeddings space. Another contribution is the extension of WMD to allow for multiple references to be used to model object importance, i.e. an approach for consensus within WMD. We evaluate the performance of VIFIDEL against human judgments on two popular IDG datasets (§4). 2 Background Various IDG metrics have either been adapted from other fields or proposed specifically for IDG. Examples of the former include BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014) from machine translation evaluation, and ROUGE (Lin, 2004) (more specifically ROUGEL (Lin and Och, 2004)) from text summarisation evaluation. Metrics designed specifically for evaluating image descriptions include CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016) and BAST (Ellebracht et al., 2015). One main weakness of most metrics (BLEU, ROUGE, Meteor, CIDEr) is that they rely on exact string matching to measure the surface-level, ngram overlap between candidate texts and human references. This can result in data spasity problems, especially with limited references. Meteor partial"
P19-1654,W15-2806,1,0.520626,"hin WMD. We evaluate the performance of VIFIDEL against human judgments on two popular IDG datasets (§4). 2 Background Various IDG metrics have either been adapted from other fields or proposed specifically for IDG. Examples of the former include BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014) from machine translation evaluation, and ROUGE (Lin, 2004) (more specifically ROUGEL (Lin and Och, 2004)) from text summarisation evaluation. Metrics designed specifically for evaluating image descriptions include CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016) and BAST (Ellebracht et al., 2015). One main weakness of most metrics (BLEU, ROUGE, Meteor, CIDEr) is that they rely on exact string matching to measure the surface-level, ngram overlap between candidate texts and human references. This can result in data spasity problems, especially with limited references. Meteor partially addresses this by matching synonyms from dictionaries and paraphrase tables, but it is constrained to the availability of such dictionaries, potted plant + 0.55 Distance = 2.201 VIFIDEL = exp(2.201) = 0.11 Figure 2: An illustration of VIFIDEL as a visual fidelity metric for IDG. Bright yellow arrows indic"
P19-1654,P16-2013,1,0.831928,"her than surface forms. VIFIDEL also differs from previous approaches using WMD for image description evaluation (Kilickaya et al., 2017), where the metric is only computed using the description and the single closest reference. One of the problems in such an approach is the biased choice of reference: in this case, the reference with the smallest WMD distance from the system description. A better reference may be available, e.g. mentioning more image content, which would lead lower system scores in an unbiased evaluation. This has been a common problem in metrics based on a single reference (Fomicheva and Specia, 2016). Another contribution in this paper is therefore the incorporation of an object importance model into the WMD framework using human references. Our approach rewards candidate descriptions that mention objects depicted in the image (i.e. faithful to image content), and that the objects are also mentioned frequently across all references (i.e. they mention important objects). Intuitively, the WMD cost function (Eq. 1) is replaced with a weighted Euclidean distance. These weights are derived from human descriptions. While the original cost function captures the faithfulness of candidate descript"
P19-1654,E17-2068,0,0.0100803,"y sit at a dinner table. 3. A table full of people that are eating at a restaurant. 4. Co-workers often get together after a long day at work. 5. A group of people that are sitting around a table. VIFIDEL Objects Table 4: Ablation study on the number of detected objects: one detected object and fifteen detected objects. Effect of word representations: We also studied the effect of various pre-trained embeddings and found that the pre-trained model of word2vec 300-dimensional CBOW embeddings (Mikolov et al., 2013) is slightly better than GLoVe (Pennington et al., 2014) and FastText embeddings (Joulin et al., 2017). This could be because of the amount of data on which these were trained. FastText embeddings had similar performance as word2vec embeddings even when only trained on the Wikipedia as corpus. For consistency, we used the word2vec embeddings pre-trained on Google News. 5 person 0.75 person, dining-table 0.83 person, dining-table, umbrella, handbag, bottle 0.74 person, car, backpack, umbrella, handbag, bottle, dining-table, cup, fork, knife 0.73 person, car, backpack, umbrella, handbag, bottle, wine-glass, cup, fork, knife, spoon, bowl, broccoli, chair, dining-table 0.70 Figure 5: VIFIDEL score"
P19-1654,D14-1162,0,0.0893772,"n table with food. 2. The nine people smile as they sit at a dinner table. 3. A table full of people that are eating at a restaurant. 4. Co-workers often get together after a long day at work. 5. A group of people that are sitting around a table. VIFIDEL Objects Table 4: Ablation study on the number of detected objects: one detected object and fifteen detected objects. Effect of word representations: We also studied the effect of various pre-trained embeddings and found that the pre-trained model of word2vec 300-dimensional CBOW embeddings (Mikolov et al., 2013) is slightly better than GLoVe (Pennington et al., 2014) and FastText embeddings (Joulin et al., 2017). This could be because of the amount of data on which these were trained. FastText embeddings had similar performance as word2vec embeddings even when only trained on the Wikipedia as corpus. For consistency, we used the word2vec embeddings pre-trained on Google News. 5 person 0.75 person, dining-table 0.83 person, dining-table, umbrella, handbag, bottle 0.74 person, car, backpack, umbrella, handbag, bottle, dining-table, cup, fork, knife 0.73 person, car, backpack, umbrella, handbag, bottle, wine-glass, cup, fork, knife, spoon, bowl, broccoli, ch"
P19-1654,W11-0326,0,0.030302,"hese human reference descriptions are not available, VIFIDEL can still reliably evaluate system descriptions. The metric achieves high correlation with human judgments on two well-known datasets and is competitive with metrics that depend on and rely exclusively on human references. 1 Introduction A popular task at the intersection of computer vision and natural language is image description generation (IDG), i.e. the task of generating as output a sentence describing the visual content of a given input image. While a variety of methods have been proposed for this task (Kulkarni et al., 2011; Li et al., 2011; Vinyals et al., 2015), its evaluation is still an understudied problem. Evaluation of IDG is currently performed in two ways: (i) human judgment; (ii) automatic metrics. Human judgments evaluate either the overall quality of descriptions or specific criteria in isolation (relevance, fluency, etc.). Such methods, however, can be subjective and expensive to scale. Automatic metrics address the scalability issue by comparing candidate descriptions against human-authored reference descriptions. These metrics conflate various criteria implicitly into a single evaluation assumption, i.e. a good de"
P19-1654,W10-0721,0,0.0401083,"e penalty scores are then used as weights to compute the cost c0 (book, encyclopedias|I) between the object label book from the image and the word encyclopedias in the candidate description. 4.2 Accuracy on PASCAL-50S 0 In this section, we focus on the PASCAL-50S dataset and tackle the binary forced-choice task of predicting: “which description is more similar to A: B or C?”, as proposed by Vedantam et al. (2015). We focus on the variant comparing two machine generated captions. The dataset contains multiple crowdsourced image description for each of 1,000 images from the UIUC PASCAL dataset (Rashtchian et al., 2010). Evaluation of system outputs as relative rankings has long been established as the best practice in many fields where language outputs are produced and no single correct output exists. The WMT yearly evaluation campaigns for machine translation (Bojar et al., 2016), for example, have argued that relative ranking leads to more reliable judgments than absolute scores. We therefore consider our findings on this dataset as the most important. For the binary forced-choice task, Vedantam et al. (2015) collected 48 descriptions A per image, and formed pairs of descriptions B and C from machine gene"
P19-1654,W04-1013,0,0.131469,"MD) metric, which measures the distance between two texts in a word embeddings space. Another contribution is the extension of WMD to allow for multiple references to be used to model object importance, i.e. an approach for consensus within WMD. We evaluate the performance of VIFIDEL against human judgments on two popular IDG datasets (§4). 2 Background Various IDG metrics have either been adapted from other fields or proposed specifically for IDG. Examples of the former include BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014) from machine translation evaluation, and ROUGE (Lin, 2004) (more specifically ROUGEL (Lin and Och, 2004)) from text summarisation evaluation. Metrics designed specifically for evaluating image descriptions include CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016) and BAST (Ellebracht et al., 2015). One main weakness of most metrics (BLEU, ROUGE, Meteor, CIDEr) is that they rely on exact string matching to measure the surface-level, ngram overlap between candidate texts and human references. This can result in data spasity problems, especially with limited references. Meteor partially addresses this by matching synonyms from dictionaries an"
P19-1654,P04-1077,0,0.152538,"e between two texts in a word embeddings space. Another contribution is the extension of WMD to allow for multiple references to be used to model object importance, i.e. an approach for consensus within WMD. We evaluate the performance of VIFIDEL against human judgments on two popular IDG datasets (§4). 2 Background Various IDG metrics have either been adapted from other fields or proposed specifically for IDG. Examples of the former include BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014) from machine translation evaluation, and ROUGE (Lin, 2004) (more specifically ROUGEL (Lin and Och, 2004)) from text summarisation evaluation. Metrics designed specifically for evaluating image descriptions include CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016) and BAST (Ellebracht et al., 2015). One main weakness of most metrics (BLEU, ROUGE, Meteor, CIDEr) is that they rely on exact string matching to measure the surface-level, ngram overlap between candidate texts and human references. This can result in data spasity problems, especially with limited references. Meteor partially addresses this by matching synonyms from dictionaries and paraphrase tables, but it is constrained to"
P19-1654,W18-5455,1,0.845159,"ocuments dα and dβ contains a combined vocabulary from dα and dβ resulting in a square transport matrix T of dimensionality N ×N . Kusner et al. (2015) note that WMD is a special case of Earth Mover’s Distance (Rubner et al., 2000), popular in the computer vision community, or Wasserstein’s Distance (Datta et al., 2008), popular in the optimal transport community. PN 3.2 Using objects as image information In this paper, we explore image information in the form of explicit object detections, both using gold or predicted object instances for a given image. Previous works (Yin and Ordonez, 2017; Madhyastha et al., 2018; Wang et al., 2018) have found explicit object detections to be informative for image description generation. Thus, we base our intuition on the hypothesis that a thorough and true description of the image should consist of information about objects and their interactions (frequencies, etc.) in the environment. VIFIDEL has the capacity to capture these. While objects represent one important type of semantic image information, VIFIDEL can potentially incorporate other semantic image information including attributes, actions, positions, scenes, relations between objects, and more finegrained in"
P19-1654,W15-4722,1,0.799116,"scale or adapt to different languages and domains. Word Mover’s Distance (WMD) (Kusner et al., 2015) has also been proposed as an IDG metric (Kilickaya et al., 2017). WMD finds optimal alignments between word embeddings in candidate and reference descriptions instead of performing n-gram matching to address data sparseness issues. VIFIDEL is inspired by WMD, but goes beyond using reference texts by comparing candidate texts against image content (§3.2). Using images for IDG evaluation. As far as we are aware, no previous work uses images for evaluating IDG. The closest related work is that by Wang and Gaizauskas (2015), who propose an f -measure-based metric to evaluate the task of selecting relevant object instances to be mentioned in a description. The metric computes the overlap between selected object instances and objects mentioned in references, averaged over multiple references. The averaging process implicitly captures consensus over which objects should be mentioned (§3.3), i.e. objects mentioned in more references should be more important than those mentioned in fewer. Their work, however, requires manual correspondence annotations between bounding box instances and object mentions in descriptions"
P19-1654,N18-1198,1,0.844788,"ns a combined vocabulary from dα and dβ resulting in a square transport matrix T of dimensionality N ×N . Kusner et al. (2015) note that WMD is a special case of Earth Mover’s Distance (Rubner et al., 2000), popular in the computer vision community, or Wasserstein’s Distance (Datta et al., 2008), popular in the optimal transport community. PN 3.2 Using objects as image information In this paper, we explore image information in the form of explicit object detections, both using gold or predicted object instances for a given image. Previous works (Yin and Ordonez, 2017; Madhyastha et al., 2018; Wang et al., 2018) have found explicit object detections to be informative for image description generation. Thus, we base our intuition on the hypothesis that a thorough and true description of the image should consist of information about objects and their interactions (frequencies, etc.) in the environment. VIFIDEL has the capacity to capture these. While objects represent one important type of semantic image information, VIFIDEL can potentially incorporate other semantic image information including attributes, actions, positions, scenes, relations between objects, and more finegrained information such as co"
P19-1654,D17-1017,0,0.0273707,"s distribution of the documents dα and dβ contains a combined vocabulary from dα and dβ resulting in a square transport matrix T of dimensionality N ×N . Kusner et al. (2015) note that WMD is a special case of Earth Mover’s Distance (Rubner et al., 2000), popular in the computer vision community, or Wasserstein’s Distance (Datta et al., 2008), popular in the optimal transport community. PN 3.2 Using objects as image information In this paper, we explore image information in the form of explicit object detections, both using gold or predicted object instances for a given image. Previous works (Yin and Ordonez, 2017; Madhyastha et al., 2018; Wang et al., 2018) have found explicit object detections to be informative for image description generation. Thus, we base our intuition on the hypothesis that a thorough and true description of the image should consist of information about objects and their interactions (frequencies, etc.) in the environment. VIFIDEL has the capacity to capture these. While objects represent one important type of semantic image information, VIFIDEL can potentially incorporate other semantic image information including attributes, actions, positions, scenes, relations between objects"
Q15-1033,P11-1022,0,0.0277316,"0.4777 VBP WHNP NN JJ SQ 0.4653 0.4508 0.4274 0.4021 0.4000 Table 3: Top 15 symbols sorted according to their obtained λ values in the SASSTKfull model with fixed α. The numbers are the corresponding λ values, averaged over all six emotions. ples of applications include filtering machine translated sentences that would require more post-editing effort than translation from scratch (Specia et al., 2009), selecting the best translation from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al., 2011). While various quality metrics exist, here we focus on post-editing time prediction. Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to test our models. Datasets We use two publicly available datasets containing post-edited machine translated sentences. Both are composed of a set of source sentences, their machine translated outputs and the correspo"
Q15-1033,D14-1190,1,0.899543,"Missing"
Q15-1033,W14-3338,1,0.917946,"riances into account, grid search would still need around 10 times more computation 4 For specific details on the SVM models used in all experiments performed in this paper we refer the reader to Appendix A. 467 Emotion Analysis The goal of Emotion Analysis is to automatically detect emotions in a text (Strapparava and Mihalcea, 2008). This problem is closely related to Opinion Mining (Pang and Lee, 2008), with similar applications, but it is usually done at a more fine-grained level and involves the prediction of a set of labels for each text (one for each emotion) instead of a single label. Beck et al. (2014a) used a multi-task GP for this task with a bag-of-words feature representation. In theory, it is possible to combine their multi-task kernel with our tree kernels, but to keep the focus of the experiments on testing tree kernel approaches, here we use independently trained models, one per emotion. Dataset We use the dataset provided by the “Affective Text” shared task in SemEval2007 (Strapparava and Mihalcea, 2007), which is composed of 1000 news headlines annotated in terms of six emotions: Anger, Disgust, Fear, Joy, Sadness and Surprise. For each emotion, a score between 0 and 100 is given"
Q15-1033,C04-1046,0,0.231901,"Missing"
Q15-1033,P13-1004,1,0.946875,"of these issues, but have several limitations (see §6 for details). Our proposed approach for model selection relies on Gaussian Processes (GPs) (Rasmussen and Williams, 2006), a widely used Bayesian kernel machine. GPs allow efficient and fine-grained model selection by maximizing the evidence on the training data using gradient-based methods, dropping the requirement for development data. As a Bayesian procedure, GPs also naturally balance between model capacity and generalization. GPs have been shown to achieve state of the art performance in various regression tasks (Hensman et al., 2013; Cohn and Specia, 2013). Therefore, we base our approach on this framework. While prediction performance is important to consider (as we show in our experiments), we are 461 Transactions of the Association for Computational Linguistics, vol. 3, pp. 461–473, 2015. Action Editor: Stefan Riezler. Submission batch: 2/2015; Revision batch 7/2015; Published 8/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. mainly interested in two other significant aspects that are enabled by our approach: • Gradient-based methods are more efficient than grid search for high dimensional space"
Q15-1033,W12-3112,1,0.856136,"rresponding λ values, averaged over all six emotions. ples of applications include filtering machine translated sentences that would require more post-editing effort than translation from scratch (Specia et al., 2009), selecting the best translation from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al., 2011). While various quality metrics exist, here we focus on post-editing time prediction. Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to test our models. Datasets We use two publicly available datasets containing post-edited machine translated sentences. Both are composed of a set of source sentences, their machine translated outputs and the corresponding post-editing time. • French-English (fr-en): This dataset, described in (Specia, 2011), contains 2524 French sentences translated into English and postedited by a novice translator. • Eng"
Q15-1033,2011.eamt-1.32,1,0.860626,"he numbers are the corresponding λ values, averaged over all six emotions. ples of applications include filtering machine translated sentences that would require more post-editing effort than translation from scratch (Specia et al., 2009), selecting the best translation from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al., 2011). While various quality metrics exist, here we focus on post-editing time prediction. Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to test our models. Datasets We use two publicly available datasets containing post-edited machine translated sentences. Both are composed of a set of source sentences, their machine translated outputs and the corresponding post-editing time. • French-English (fr-en): This dataset, described in (Specia, 2011), contains 2524 French sentences translated into English and postedited b"
Q15-1033,P10-1064,0,0.0293227,"Missing"
Q15-1033,D14-1219,0,0.0166756,"re space. Also, their method does not take into account the underlying learning algorithm. Another recent approach proposed for model selection is random search (Bergstra and Bengio, 2012). Like grid search, it has the drawback of not employing gradient information, as it is designed for any kind of hyperparameters (including categorical ones). Structural kernels have been successfully employed in a number of NLP tasks. The original SSTK proposed by Collins and Duffy (2001) was used to rerank the output of syntactic parsers. Recently, this reranking idea was also applied to discourse parsing (Joty and Moschitti, 2014). Other tree kernel applications include Semantic Role Labelling (Moschitti et al., 2008) and Relation Extraction (Plank and Moschitti, 2013). String kernels were mostly used in Text Classification (Lodhi et al., 2002; Cancedda et al., 2003), while graph kernels have been used for recognizing Textual Entailment (Zanzotto and Dell’Arciprete, 2009). However, these previous works focused on frequentist methods like SVM or voted perceptron while we employ a Bayesian approach. Gaussian Processes are a major framework in machine learning nowadays: applications include Robotics (Ko et al., 2007), Geo"
Q15-1033,P14-5010,0,0.00433599,"on. Grid search is embarassingly parallelizable since each grid point can run in a different core. However, the GP optimization can also benefit from multiple cores by running each kernel computation inside the Gram matrix in parallel. To keep the comparisons simpler, the results shown in this section use a single core but all experiments in §5 employ parallelization in the Gram matrix computation level (for both SVM and GP models). 5 NLP Experiments Our experiments with NLP data address two regression tasks: Emotion Analysis and Quality Estimation. For both tasks, we use the Stanford parser (Manning et al., 2014) to obtain constituency trees for all sentences. Also, rather than using data official splits, we perform 5-fold cross-validation in order to obtain more reliable results. 5.1 Figure 4: Results from performance experiments. The x axis corresponds to wall clock time in seconds and it is in log scale. The y axis shows RMSE on the test set. The blue dashed line corresponds to the RMSE value obtained after L-BFGS converged. Error bars are obtained by measuring one standard deviation over the 20 runs made in each experiment. We can see that optimizing the GP model is consistently much faster than d"
Q15-1033,J08-2003,0,0.0337766,"Another recent approach proposed for model selection is random search (Bergstra and Bengio, 2012). Like grid search, it has the drawback of not employing gradient information, as it is designed for any kind of hyperparameters (including categorical ones). Structural kernels have been successfully employed in a number of NLP tasks. The original SSTK proposed by Collins and Duffy (2001) was used to rerank the output of syntactic parsers. Recently, this reranking idea was also applied to discourse parsing (Joty and Moschitti, 2014). Other tree kernel applications include Semantic Role Labelling (Moschitti et al., 2008) and Relation Extraction (Plank and Moschitti, 2013). String kernels were mostly used in Text Classification (Lodhi et al., 2002; Cancedda et al., 2003), while graph kernels have been used for recognizing Textual Entailment (Zanzotto and Dell’Arciprete, 2009). However, these previous works focused on frequentist methods like SVM or voted perceptron while we employ a Bayesian approach. Gaussian Processes are a major framework in machine learning nowadays: applications include Robotics (Ko et al., 2007), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004). Only very r"
Q15-1033,E06-1015,0,0.586128,"0 pr(n1 ) 6= pr(n2 )    λ pr(n1 ) = pr(n2 ) ∧ ∆(n1 , n2 ) =  preterm(n1 )    λg(n , n ) otherwise, 1 2 where pr(n) is the grammar production at node n and preterm(n) returns true if n is a pre-terminal node. The function g is defined as follows: g(n1 , n2 ) = |n1 | Y (α + ∆(cin1 , cin2 )) , (3) i=1 where |n |is the number of children of node n and cin is the ith child of node n. This recursive definition is calculated efficiently by employing dynamic programming to cache intermediate ∆ results. Equation 3 also adds another hyperparameter, α. This hyperparameter was introduced by Moschitti (2006b)3 as a way to select between two different tree kernels. If α = 1, we get the original SSTK, if α = 0, then we obtain the Subtree Kernel, which only allows fragments with terminal symbols 3 In his original formulation, this hyperparameter was named σ but here we use α to not confuse it with the GP noise hyperparameter. as leaves. We can also interpret the Subtree Kernel as a “sparse” version of the SSTK, where the “nonsubtree” fragments have their weights equal to zero. Even though fragment weights are affected by both kernel hyperparameters, previous work did not discuss their effects. The"
Q15-1033,W10-2926,0,0.0265089,"t contains complete grammar rules (see Figure 1 for an example). Consider the set of nodes in the two trees as N1 and N2 respectively. We define Ii (n) as an indicator function that returns 1 if fragment fi ∈ F has root n and 0 otherwise. A SSTK can then be defined as: X X k(t1 , t2 ) = ∆(n1 , n2 ) , (2) n1 ∈N1 n2 ∈N2 where ∆(n1 , n2 ) = |F | X λ s(i) 2 Ii (n1 )Ii (n2 ) i=1 and s(i) is the number of fragments in i with at least one child2 . The formulation in Equation 2 is the same as the one shown in Equation 1, except that we are now restricting the weights w(f ) to be a function of a 2 See Pighin and Moschitti (2010) for details and a proof on this derivation. 463 Tree S A B A B a b A B A B A B A B a S b Fragments S S a S b a b Figure 1: An example tree and the respective set of tree fragments defined by a SSTK. hyperparameter λ. The original goal of λ is to act as a decay factor that penalizes contributions from larger fragments cf smaller ones (and therefore, it should be in the [0, 1] interval). Without this factor, the resulting distribution over tree pairs is skewed, giving extremely large values when trees are equal and rapidly decreasing for small differences over fragment counts. The decay factor"
Q15-1033,P13-1147,0,0.0315825,"ion is random search (Bergstra and Bengio, 2012). Like grid search, it has the drawback of not employing gradient information, as it is designed for any kind of hyperparameters (including categorical ones). Structural kernels have been successfully employed in a number of NLP tasks. The original SSTK proposed by Collins and Duffy (2001) was used to rerank the output of syntactic parsers. Recently, this reranking idea was also applied to discourse parsing (Joty and Moschitti, 2014). Other tree kernel applications include Semantic Role Labelling (Moschitti et al., 2008) and Relation Extraction (Plank and Moschitti, 2013). String kernels were mostly used in Text Classification (Lodhi et al., 2002; Cancedda et al., 2003), while graph kernels have been used for recognizing Textual Entailment (Zanzotto and Dell’Arciprete, 2009). However, these previous works focused on frequentist methods like SVM or voted perceptron while we employ a Bayesian approach. Gaussian Processes are a major framework in machine learning nowadays: applications include Robotics (Ko et al., 2007), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004). Only very recently they have been successfully employed in a fe"
Q15-1033,D13-1100,1,0.910314,"Missing"
Q15-1033,S14-2138,0,0.0219346,"Arciprete, 2009). However, these previous works focused on frequentist methods like SVM or voted perceptron while we employ a Bayesian approach. Gaussian Processes are a major framework in machine learning nowadays: applications include Robotics (Ko et al., 2007), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004). Only very recently they have been successfully employed in a few NLP tasks such as translation quality estimation (Cohn and Specia, 2013; Beck et al., 2014b), detection of temporal patterns in text (Preot¸iuc-Pietro and Cohn, 2013), semantic similarity (Rios and Specia, 2014) and emotion analysis (Beck et al., 2014a). In terms of feature 471 representations, previous work focused on the vectorial inputs and applied well-known kernels for these inputs, e.g. the RBF kernel. As shown on §5.2, our approach is orthogonal to these previous ones, since kernels can be easily combined in different ways. It is important to note that we are not the first ones to combine GPs with kernels on structured inputs. Driessens et al. (2006) employed a combination of GPs and graph kernels for reinforcement learning. However, unlike our approach, they did not attempt model selection, e"
Q15-1033,2009.eamt-1.5,1,0.873699,"de a quality prediction for new, unseen machine translated texts (Blatz et al., 2004; Bojar et al., 2014). ExamJJR PRP$ WDT RBR VBG 0.8333 0.6933 0.6578 0.5445 0.5163 WHADVP QP JJS NNS . 0.5004 0.5001 0.4996 0.4961 0.4777 VBP WHNP NN JJ SQ 0.4653 0.4508 0.4274 0.4021 0.4000 Table 3: Top 15 symbols sorted according to their obtained λ values in the SASSTKfull model with fixed α. The numbers are the corresponding λ values, averaged over all six emotions. ples of applications include filtering machine translated sentences that would require more post-editing effort than translation from scratch (Specia et al., 2009), selecting the best translation from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al., 2011). While various quality metrics exist, here we focus on post-editing time prediction. Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to te"
Q15-1033,P13-4014,1,0.861222,"Missing"
Q15-1033,2011.eamt-1.12,1,0.842829,"ction. Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to test our models. Datasets We use two publicly available datasets containing post-edited machine translated sentences. Both are composed of a set of source sentences, their machine translated outputs and the corresponding post-editing time. • French-English (fr-en): This dataset, described in (Specia, 2011), contains 2524 French sentences translated into English and postedited by a novice translator. • English-Spanish (en-es): This dataset was used in the WMT14 Quality Estimation shared task (Bojar et al., 2014), containing 858 sentences translated from English into Spanish and post-edited by an expert translator. For each dataset, post-editing times are first divided by the translation output length (obtaining the post-editing time per word) and then mean normalized. 469 Models Since our data consists of pairs of trees, our models in this task use a pair of tree kernels. We combine these two ke"
Q15-1033,D09-1010,0,0.0764496,"Missing"
Q15-1033,S07-1013,0,\N,Missing
R11-1014,2005.mtsummit-papers.11,0,0.00530899,"revised to guarantee the largest possible set of 1-1 correspondences and also to correct mistakes that resulted from the particularities of aligning subtitles. After the correction of the sentence alignment, four episodes were randomly chosen and kept aside as our test data. Statistics about the resulting sentence-aligned parallel corpus are reported in Table 1. Armstrong et al. (2006) train an EBMT system in two scenarios: i) using a homogenous corpus compiled exclusively with DVD subtitles, and ii) using a heterogenous corpus compiled with a mix of subtitles and sentences from the Europarl (Koehn, 2005). The results show that a homogenous setting leads to better translations. Flanagan (2009) extends the work of Armstrong et al. (2006) by using larger parallel corpora of subtitles from multiple genres. A subjective evaluation querying users who watched movies containing the translated subtitles in terms of intelligibility and acceptability was performed. Results show an average performance (∼ 3 on a 1-6 scale). Melero et al. (2006) combine a black-box MT system and a TM using a corpus of newspaper articles and United Nation texts to translate subtitles. They find that MT+TM performs significa"
R11-1014,2006.tc-1.10,0,0.559222,"and iv) XML-like tags. Subsequently, the corpus was automatically aligned at the sentence level using heuristics aimed at maximizing the time overlap between the source and target subtitles. The sentence alignment was revised to guarantee the largest possible set of 1-1 correspondences and also to correct mistakes that resulted from the particularities of aligning subtitles. After the correction of the sentence alignment, four episodes were randomly chosen and kept aside as our test data. Statistics about the resulting sentence-aligned parallel corpus are reported in Table 1. Armstrong et al. (2006) train an EBMT system in two scenarios: i) using a homogenous corpus compiled exclusively with DVD subtitles, and ii) using a heterogenous corpus compiled with a mix of subtitles and sentences from the Europarl (Koehn, 2005). The results show that a homogenous setting leads to better translations. Flanagan (2009) extends the work of Armstrong et al. (2006) by using larger parallel corpora of subtitles from multiple genres. A subjective evaluation querying users who watched movies containing the translated subtitles in terms of intelligibility and acceptability was performed. Results show an av"
R11-1014,P02-1040,0,0.100466,"etting leads to better translations. Flanagan (2009) extends the work of Armstrong et al. (2006) by using larger parallel corpora of subtitles from multiple genres. A subjective evaluation querying users who watched movies containing the translated subtitles in terms of intelligibility and acceptability was performed. Results show an average performance (∼ 3 on a 1-6 scale). Melero et al. (2006) combine a black-box MT system and a TM using a corpus of newspaper articles and United Nation texts to translate subtitles. They find that MT+TM performs significantly better than MT in terms of BLEU (Papineni et al., 2002) in an English-Spanish task. For EnglishCzech they compare HT against PE in terms of time. The comparison is somewhat inconclusive as the HT and PE were compared using different texts and a single human translator. Volk (2008) uses a large proprietary corpus of subtitles (5 million sentences) to train an SMT system. The author reports BLEU: i) using a single reference, and ii) using the translations produced by six post-editors. The author finds that SMT outputs can still be acceptable translations even though they do not exactly match the HT as long as they lie within 5 keystrokes, distance f"
R11-1014,W00-0506,0,0.317615,"Missing"
R11-1014,2011.eamt-1.12,1,0.262575,"produced by six post-editors. The author finds that SMT outputs can still be acceptable translations even though they do not exactly match the HT as long as they lie within 5 keystrokes, distance from it. Similarly to prior work we compile a corpus of DVD subtitles in order to perform in-domain subtitle translations. We train our own SMT model and compare it against other MT approaches and a TM. Our main goal is to demonstrate that, regardless of the MT/TM strategy, PE is faster than HT without a loss in quality. For that, we design a comprehensive evaluation: i) objectively in terms of time (Specia, 2011), ii) subjectively using well specified scoring guidelines (Specia, 2011), and iii) automatically in terms of BLEU using single and multiple references. As a by-product, a comparison between different translation approaches is performed. 3 Corpus en tokens pt tokens Sentence pairs Training 720,845 613,201 76,295 Test 17,796 14,000 2,379 Table 1: Token and sentence numbers in the parallel corpus 4 Experiments This section describes how the effort to translate subtitles from scratch was compared to the effort to post-edit translations automatically obtained through different tools. 4.1 Systems W"
R11-1014,P07-2045,0,\N,Missing
R11-1031,P98-2127,0,0.0253836,"preferences. act 2 attribute 1 communication 1 food 1,2 motive 1 living thing plant 2 quantity shape 1,2 substance 1 tool 1 solid abstraction animal body part event 3 group natural object person 1,2 possession relation 1 state 2 time device 1 liquid thing Aiming at producing an SRL system with features that can be easily extracted for different languages and also to provide additional lexical information, we expanded chunks’ heads with similar words. For every head word on its base form, regardless its part-of-speech, we selected the 10-most similar words from Lin’s distributional thesaurus (Lin, 1998). Lin’s thesaurus is an automatically constructed resource that maps words to similar concepts in terms of a distributional lexical similarity metric. The last column in Figure 2 exemplifies similar words retrieved for some chunks. 3.3 artifact cognition feeling location physical object phenomenon process 6 relation 2,3,6 state 6 vehicle 1 garment 1 physical entity Motivated by VerbNet’s (Kipper et al., ) selectional restrictions, we manually selected the 38 categories listed in Figure 1 and mapped them into the WordNet lexicon. We chose general hypernyms in order to avoid fine-grained sense d"
R11-1031,W05-0629,0,0.0202164,"n, or semantic role labels, to the syntactic annotation of the Penn Treebank. The test set used was CoNLL-2005 (Carreras and M`arquez, 2005), which has predicate-argument information for approximately 2.5K sentences from the Wall Street Journal (WSJ) (in-domain evaluation) and 450 sentences from Brown corpus (out-of-domain evaluation). Table 1 presents the overall results for the SRL taskon the in-domain test set (WSJ), and Table 2 presents the same analysis on the out-of-domain test set (Brown). They also show CoNLL 2005’s baseline (Carreras and M`arquez, 2005) and a similar chunk-based SRL (Mitsumori et al., 2005). The figures refer to the weighted average of the performance in correctly classifying target predicates (V), their core arguments (A0 to A5) and their modifiers. Tables 1 and 2 show that the proposed lexicalized features yielded an important gain in Templates The CRF++ toolkit allows the definition of templates over the basic feature space, that is, rules that combine multiple features. Templates are expanded token-by-token, that is, for every CRF token the original feature set is used to create additional features. Templates can be based on features only, referred to as unigram templates, o"
R11-1031,J05-1004,0,0.0478113,"10-most Similar Words: as described in 3.2, henceforth referred to as 10sim 3.4 Results We experimented with different configurations of features in order to understand the impact of their contribution. The baseline model (B) contains all features apart from the selectional preferences and the 10-most similar words, the main contributions of this paper. We added the selectional preferences (B+sp) and the most similar words (B+10sim) separately, and built a final model containing all the features (B+10sim+sp), as described in Section 3. Training was performed using the whole Proposition Bank (Palmer et al., 2005) (except Section 23, which is part of the test set). The Proposition Bank adds a layer of predicate-argument information, or semantic role labels, to the syntactic annotation of the Penn Treebank. The test set used was CoNLL-2005 (Carreras and M`arquez, 2005), which has predicate-argument information for approximately 2.5K sentences from the Wall Street Journal (WSJ) (in-domain evaluation) and 450 sentences from Brown corpus (out-of-domain evaluation). Table 1 presents the overall results for the SRL taskon the in-domain test set (WSJ), and Table 2 presents the same analysis on the out-of-doma"
R11-1031,N04-1030,0,0.240475,"s available. 1 Introduction Identifying the relations that words or groups of words have with verbs in a sentence constitutes an important step for many applications in Natural Language Processing (NLP). This is addressed by the field of Semantic Role Labeling (SRL). SRL has been shown to contribute to many NLP applications, such as Information Extraction, Question Answering and Machine Translation. Most of the SRL approaches operate via two consecutive steps: i) the identification of the arguments of a target predicate and ii) the classification of those arguments (Gildea and Jurafsky, 2002; Pradhan et al., 2004). Alternatively, graph models can rely on the sequential nature of the shallow 226 Proceedings of Recent Advances in Natural Language Processing, pages 226–232, Hissar, Bulgaria, 12-14 September 2011. extracted from other sources of structured information such as DBpedia1 . The paper is structured as follows: in Section 2 we give an overview of the related work; in Section 3 we describe the proposed system; in Section 4 we present the results of our experiments. Finally, in Section 5 we present our conclusions and some directions for future work. 2 lemmas and named entities. Wordforms and lemm"
R11-1031,J08-2006,0,0.12042,"While parse trees allow a set of very informative path-based, structural features, chunks can provide more reliable annotations. Hacioglu et al. (2004) propose the use of base phrases as data representation using Support Vector Machines in order to perform a single argument classification step. Roth and tau Yih (2005) use the same sort of representation with Conditional Random Fields (CRF) as learning algorithm, motivated by the sequential nature of the task. Cohn and Blunsom (2005) use CRF to perform SRL in a single identification/classification step based on features from constituent trees. Pradhan et al. (2008) point out the lack of semantic features as the bottleneck in argument role classification, a task closely-related to that of word sense disambiguation. Shallow lexical features such as word forms and word lemmas are very sparse. Although named-entity categories have been proposed to alleviate this sparsity problem, they only apply to a fraction of the arguments’ words. In this paper we propose the addition of other forms of lexical knowledge in order to address this problem. The proposed SRL system tags data in a joint identification/classification step using CRF as the learning algorithm. Th"
R11-1031,W97-0209,0,0.123426,"ns and some directions for future work. 2 lemmas and named entities. Wordforms and lemmas make very sparse features; while more general features such as named-entities generalize just a fraction of all the nouns that verbs might take as arguments. To improve argument classification, Zapirain et al. (2010) propose to merge selectional preferences into a state-of-the-art SRL system. They define selectional preference as a similarity score between the predicate, the argument role and the constituent head word. The similarity is computed using different strategies: i) Resnik’s similarity measure (Resnik, 1997) based on WordNet (Miller et al., 1990), and ii) different corpus-based distributional similarity metrics, considering both first and second order similarities. They report consistent gains on argument classification by combining models based on different similarity metrics. In this work we propose to add lexical information in a different fashion. Instead of measuring the similarity between the argument head word and the predicate we: i) understand selectional preferences as categories, such as the usual named-entities, however covering any sort of noun; ii) provide additional evidence of lex"
R11-1031,W05-0620,0,0.0941713,"Missing"
R11-1031,W05-0622,0,0.136825,"eously (Roth and tau Yih, 2005; Cohn and Blunsom, 2005). Features for SRL are usually extracted from chunks or constituent parse trees. While parse trees allow a set of very informative path-based, structural features, chunks can provide more reliable annotations. Hacioglu et al. (2004) propose the use of base phrases as data representation using Support Vector Machines in order to perform a single argument classification step. Roth and tau Yih (2005) use the same sort of representation with Conditional Random Fields (CRF) as learning algorithm, motivated by the sequential nature of the task. Cohn and Blunsom (2005) use CRF to perform SRL in a single identification/classification step based on features from constituent trees. Pradhan et al. (2008) point out the lack of semantic features as the bottleneck in argument role classification, a task closely-related to that of word sense disambiguation. Shallow lexical features such as word forms and word lemmas are very sparse. Although named-entity categories have been proposed to alleviate this sparsity problem, they only apply to a fraction of the arguments’ words. In this paper we propose the addition of other forms of lexical knowledge in order to address"
R11-1031,W04-3212,0,0.0311835,", however covering any sort of noun; ii) provide additional evidence of lexical similarity by expanding the head of any base phrase to its 10-most similar concepts retrieved from a distributional thesaurus. Related Work In most previous work, improvements in SRL come from new features used either in the argument identification or in the argument classification step. It is common to train different binary classifiers to perform each of the two steps separately (Gildea and Jurafsky, 2002; Pradhan et al., 2004). In the first step chunks are identified as potential arguments of a given predicate. Xue and Palmer (2004) apply syntax-driven heuristics in order to prune unlikely candidates. In the second step, the selected arguments are individually labeled with semantic roles. Pradhan et al. (2004) use features such as the role of the preceding argument in order to create a dependency between the classification of different arguments. Hacioglu et al. (2004) propose a single identification/classification step using SVM by labeling chunks within a window centered in the predicated from left to right. The authors propose to label base phrases instead of constituents in a full parse tree. They also change the dat"
R11-1031,J02-3001,0,0.432526,"ittle syntactic knowledge is available. 1 Introduction Identifying the relations that words or groups of words have with verbs in a sentence constitutes an important step for many applications in Natural Language Processing (NLP). This is addressed by the field of Semantic Role Labeling (SRL). SRL has been shown to contribute to many NLP applications, such as Information Extraction, Question Answering and Machine Translation. Most of the SRL approaches operate via two consecutive steps: i) the identification of the arguments of a target predicate and ii) the classification of those arguments (Gildea and Jurafsky, 2002; Pradhan et al., 2004). Alternatively, graph models can rely on the sequential nature of the shallow 226 Proceedings of Recent Advances in Natural Language Processing, pages 226–232, Hissar, Bulgaria, 12-14 September 2011. extracted from other sources of structured information such as DBpedia1 . The paper is structured as follows: in Section 2 we give an overview of the related work; in Section 3 we describe the proposed system; in Section 4 we present the results of our experiments. Finally, in Section 5 we present our conclusions and some directions for future work. 2 lemmas and named entit"
R11-1031,N10-1058,0,0.0524722,"Missing"
R11-1031,W04-2416,0,0.0258518,"ument classification step. It is common to train different binary classifiers to perform each of the two steps separately (Gildea and Jurafsky, 2002; Pradhan et al., 2004). In the first step chunks are identified as potential arguments of a given predicate. Xue and Palmer (2004) apply syntax-driven heuristics in order to prune unlikely candidates. In the second step, the selected arguments are individually labeled with semantic roles. Pradhan et al. (2004) use features such as the role of the preceding argument in order to create a dependency between the classification of different arguments. Hacioglu et al. (2004) propose a single identification/classification step using SVM by labeling chunks within a window centered in the predicated from left to right. The authors propose to label base phrases instead of constituents in a full parse tree. They also change the data representation of the roles to IOB2 notation which is more adequate to shallow parsing. In the proposed representation, the features of base phrases include those that can be extracted from their head words as well as some chunk oriented features (e.g the distance of the chunk to the predicate). Cohn and Blunsom (2005) approach induces an"
R11-1031,N07-1070,0,\N,Missing
R11-1031,C98-2122,0,\N,Missing
R11-1102,I05-2026,0,0.0119872,"accuracy. Due to the very short time given to participants to process the corpus for the official competitions, little effort has been made in these competitions to further explore NLP techniques. Outside of these competitions, lexical resources with synonymy information have been used in a few approaches. Similar to our work, the idea is to generalise the words in the texts by considering synonyms when searching for lexical matching between suspicious and source texts, in addition to exact matching of words. The use of a lexical thesaurus such as WordNet (Fellbaum, 1998) was investigated by Nahnsen et al. (2005). The paper described the use of lexical resources in text similarity detection, which involved the use of cosine similarity on n-grams of lexical chains, with word sense disambiguation applied to nouns, verbs and adjectives. They computed tf-idf of the disambiguated words as a similarity measure but if the WSD process is not accurate, it would affect the similarity scores. Another research by Chen et al. (2010) has concluded that using WordNet to perform synonym recognition can help determine whether a sentence pair contains similar words. They measure the similarity by comparing the synonyms"
R11-1102,C10-2115,0,\N,Missing
S07-1099,W04-0807,0,\N,Missing
S10-1024,2005.mtsummit-papers.11,0,0.034221,"recently been proposed using standard WSD features to learn models using translations instead of senses (Specia et al., 2007; Carpuat and Wu, 2007; Chan and Ng, 2007). In such approaches, the global WSD score is added as a feature to statistical MT systems, 117 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 117–122, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics 2 Resources 2.1 The process to generate the corpus-based dictionary for USPwlv is described in Section 4. Parallel corpus The English-Spanish part of Europarl (Koehn, 2005), a parallel corpus from the European Parliament proceedings, was used as a source of sentence level aligned data. The nearly 1.7M sentence pairs of English-Spanish translations, as provided by the Fourth Workshop on Machine Translation (WMT091 ), sum up to approximately 48M tokens in each language. Europarl was used both to train the SMT system and to generate dictionaries based on inter-language mutual information. 2.2 2.3 Pre-processing techniques The Europarl parallel corpus was tokenized and lowercased using standard tools provided by the WMT09 competition. Additionally, the sentences tha"
S10-1024,2009.eamt-1.15,0,0.0784297,"ovided by a dictionary. USPwlv System For each source word occurring in the context of a specific sentence, this system uses a linear combination of features to rank the options from an automatically built English-Spanish dictionary. For the best subtask, the translation ranked first is chosen, while for the oot subtask, the 10 best ranked translations are used without repetition. The building of the dictionary, the features used and the learning scheme are described in what follows. Dictionary Building The dictionary building is based on the concept of inter-language Mutual Information (MI) (Raybaud et al., 2009). It consists in detecting which words in a source-language sentence trigger the appearance of other words in its target-language translation. The inter-language MI in Equation 3 can be defined for pairs of source (s) and target (t) words by observing their occurrences at the sentence level in a parallel, sentence aligned corpus. Both simple (Equation 1) and joint distributions (Equation 2) were built based on the English-Spanish Europarl corpus using its Lemma.pos version (Section 2.3). pl (x) = countl (x) T otal (1) fen,es (s, t) (2) T otal   pen,es (s, t) (3) M I(s, t) = pen,es (s, t)log"
S10-1024,P07-1006,1,0.845329,"Substitution task in Semeval-2010 (Mihalcea et al., 2010) is to find the best (best subtask) Spanish translation or the 10-best (oot subtask) translations for 100 different English source words depending on their context of occurrence. Source words include nouns, adjectives, adverbs and verbs. 1, 000 occurrences of such words are given along with a short context (a sentence). This task resembles that of Word Sense Disambiguation (WSD) within Machine Translation (MT). A few approaches have recently been proposed using standard WSD features to learn models using translations instead of senses (Specia et al., 2007; Carpuat and Wu, 2007; Chan and Ng, 2007). In such approaches, the global WSD score is added as a feature to statistical MT systems, 117 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 117–122, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics 2 Resources 2.1 The process to generate the corpus-based dictionary for USPwlv is described in Section 4. Parallel corpus The English-Spanish part of Europarl (Koehn, 2005), a parallel corpus from the European Parliament proceedings, was used as a source of sentence level aligned dat"
S10-1024,D07-1007,0,\N,Missing
S10-1024,S10-1002,0,\N,Missing
S12-1046,S12-1067,0,0.0303565,"yntactic complexity of documents that are similar to the given context, candidate length, and letter-wise recognizability of candidate as measured by a trigram LM. The first feature sets for co-training combines the syntactic complexity, character trigram LM and basic word length features, resulting in 29 features against the remaining 21. EMNLPCPH-ORD2: This is a variant of the EMNLPCPH-ORD1 system where the first feature set pools all syntactic complexity features and Wikipedia-based features (28 features) against all the remaining 22 features in the second group. SB-mmSystem: The approach (Amoia and Romanelli, 2012) builds on the baseline definition of simplicity using word frequencies but attempt at defining a more linguistically motivated notion of simplicity based on lexical semantics considerations. It adopts different strategies depending on the syntactic complexity of the substitute. For one-word substitutes or common collocations, the system uses its frequency from Wordnet as a metric. In the case of multi-words substitutes the system uses “relevance” rules that apply (de)compositional semantic criteria and attempts to identify a unique content word in the substitute that might better approximate"
S12-1046,P11-2087,0,0.397661,"iolation, but enormity should be dropped as it does not fit the context appropriately. Finally, the system would determine the simplest of these substitutes, e.g., cruelty, and use it to replace the complex word, yielding the sentence: “Hitler committed terrible cruelties during the second World War.”. Different from other subtasks of Text Simplification like Syntactic Simplification, which have been relatively well studied, Lexical Simplification has received less attention. Although a few recent attempts explicitly address dependency on context (de Belder et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Specia, 2010), most approaches are contextindependent (Candido et al., 2009; Devlin and Tait, 1998). In addition, a general deeper understanding 347 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 347–355, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics of the problem is yet to be gained. As a first attempt to address this problem in the shape of a shared task, the English Simplification task at SemEval-2012 focuses on the third component, which we believe is the core of the Lexical Simplification problem. The SemEval-2012 share"
S12-1046,W11-2103,0,0.0278906,"varies from individual to individual, we carefully chose a group of annotators in an attempt to capture as much of a common notion of simplicity as possible. For practical reasons, we selected annotators with high proficiency levels in English as second language learners - all with a university first degree in different subjects. The Trial dataset was annotated by four people while the Test dataset was annotated by five people. In both cases each annotator tagged the complete dataset. Inter-annotator agreement was computed using an adaptation of the kappa index with pairwise rank comparisons (Callison-Burch et al., 2011). This is also the primary evaluation metric for participating systems in the shared task, and it is covered in more detail in Section 4. The inter-annotator agreement was computed for each pair of annotators and averaged over all possible pairs for a final agreement score. On the Trial dataset, a kappa index of 0.386 was found, while for the Test dataset, a kappa index of 0.398 was found. It may be noted that certain annotators disagreed considerably with all others. For example, on the Test set, if annotations from one judge are removed, the average inter-annotator agreement rises to 0.443."
S12-1046,W09-2105,1,0.505463,"Missing"
S12-1046,S12-1066,1,0.513793,"tensive and nonfree resource such as the Web1T corpus makes a difference over other free and lightweight resources. UNT-SaLSA: The only resource SaLSA depends on is the Web1T data, and in particular only 3-grams from this corpus. It leverages the context provided with the dataset by replacing the target placeholder one by one with each of the substitutes and their inflections thus building sets of 3-grams for each substitute in a given instance. The score of any substitute is then the sum of the 3-gram frequencies of all the generated 3-grams for that substitute. UOW-SHEF-SimpLex: The system (Jauhar and Specia, 2012) uses a linear weighted ranking function composed of three features to produce a ranking. These include a context sensitive n-gram frequency model, a bag-of-words model and a feature composed of simplicity oriented psycholinguistic features. These three features are combined using an SVM ranker that is trained and tuned on the Trial dataset. 6.2 Pairwise kappa The official task results and the ranking of the systems are shown in Table 3. Firstly, it is worthwhile to note that all the top ranking systems include features that use frequency as a surrogate measure for lexical simplicity. This ind"
S12-1046,S12-1068,0,0.0205478,"he other hand, performs very strongly, in spite of its simplistic approach, which is entirely agnostic to context. In fact it surpasses the average inter-annotator agreement on both Trial and Test datasets. Indeed, the scores on the Test set approach the best inter-annotator agreement scores between any two annotators. L-Sub Gold Random Simple Freq. Trial 0.050 0.016 0.397 Test 0.106 0.012 0.471 Table 2: Baseline kappa scores on trial and test sets 6 6.1 Results and Discussion Participants Five sites submitted one or more systems to the task, totaling nine systems: ANNLOR-lmbing: This system (Ligozat et al., 2012) relies on language models probabilities, and builds on the principle of the Simple Frequency baseline. While the baseline uses Google n-grams to rank substitutes, this approach uses Microsoft Web n-grams in the same way. Additionally characteristics, such as the contexts of each term to be substituted, were integrated into the system. Microsoft Web N-gram Service was used to obtain log likelihood probabilities for text units, composed of the lexical item and 4 words to the left and right from the surrounding context. ANNLOR-simple: The system (Ligozat et al., 2012) is based on Simple English"
S12-1046,S07-1009,0,0.309906,"regardless of its context. 1 Introduction Lexical Simplification is a subtask of Text Simplification (Siddharthan, 2006) concerned with replacing words or short phrases by simpler variants in a context aware fashion (generally synonyms), which can be understood by a wider range of readers. It generally envisages a certain human target audience that may find it difficult or impossible to understand complex words or phrases, e.g., children, people with poor literacy levels or cognitive disabilities, or second language learners. It is similar in many respects to the task of Lexical Substitution (McCarthy and Navigli, 2007) in that it involves determining adequate substitutes in context, but in this case on the basis of a predefined criterion: simplicity. As an example take the sentence: “Hitler committed terrible atrocities during the second World War.” The system would first identify complex words, e.g. atrocities, then search for substitutes that might adequately replace it. A thesaurus lookup would yield the following synonyms: abomination, cruelty, enormity and violation, but enormity should be dropped as it does not fit the context appropriately. Finally, the system would determine the simplest of these su"
S12-1046,S12-1069,0,0.0648761,"requency from Wordnet as a metric. In the case of multi-words substitutes the system uses “relevance” rules that apply (de)compositional semantic criteria and attempts to identify a unique content word in the substitute that might better approximate the whole expression. The expression is then assigned the frequency associated to this content word for the ranking. After POS tagging and sense disambiguating all substitutes, hand-written rules are used to decompose the meaning of a complex phrase and identify the most relevant word conveying the semantics of the whole. UNT-SimpRank: The system (Sinha, 2012) uses external resources, including the Simple English Wikipedia corpus, a set of Spoken English dialogues, transcribed into machine readable form, WordNet, and unigram frequencies (Google Web1T data). SimpRank scores each substitute by a sum of its unigram frequency, its Rank 1 frequency in the Simple English Wikipedia, its frequency in the spoken corpus, the inverse of its length, and the number of senses the substitute has in WordNet. For a given context, the substitutes are then reverse-ranked based on their simplicity scores. UNT-SimpRankLight: This is a variant of SimpRank which does not"
S12-1046,N10-1056,0,0.284082,"ruelty, enormity and violation, but enormity should be dropped as it does not fit the context appropriately. Finally, the system would determine the simplest of these substitutes, e.g., cruelty, and use it to replace the complex word, yielding the sentence: “Hitler committed terrible cruelties during the second World War.”. Different from other subtasks of Text Simplification like Syntactic Simplification, which have been relatively well studied, Lexical Simplification has received less attention. Although a few recent attempts explicitly address dependency on context (de Belder et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Specia, 2010), most approaches are contextindependent (Candido et al., 2009; Devlin and Tait, 1998). In addition, a general deeper understanding 347 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 347–355, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics of the problem is yet to be gained. As a first attempt to address this problem in the shape of a shared task, the English Simplification task at SemEval-2012 focuses on the third component, which we believe is the core of the Lexical Simplification problem. Th"
S12-1066,P11-2087,0,0.170766,"d with textual simplicity and selected the three that seemed the most promising on an evaluation with the trial dataset. These include contextual and psycholinguistic components. When combined using an SVM 1 Developed by co-organizers of the shared task Related Work Lexical Simplification has received considerably less interest in the NLP community as compared with Syntactic Simplification. However, there are a number of notable works related to the topic. In particular Yatskar et al. (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. Biran et al. (2011) extend this base methodology to apply lexical simplification to input sentences. De Belder and Moens (2010), in contrast, provide a more general architecture for the task, with scope for possible extension to other languages. These studies and others have envisaged a range of different target user groups including children (De Belder and Moens, 2010), people with low literacy levels (Aluisio et al., 2008) and aphasic readers (Carroll et al., 1998). The current work differs from previous research in that it envisages a stand-alone lexical simplification system based on linguistically motivated"
S12-1066,S07-1091,0,0.0138079,"mplexity) in different contexts. A blind application of n-gram frequency searching on the shared task’s dataset, however, gives suboptimal results because of two main factors: 1. Inconsistently lemmatized candidates. {smart} 2. Blind replacement of even correctly lemmatized forms in context producing ungrammatical results. a system is required to produce a ranking, e.g.: System: {intelligent} {bright} {clever, smart} Note that ties were permitted and that all candidates needed to be included in the system rankings. 4 4.1 The SimpLex Lexical Simplification System In an approach similar to what Hassan et al. (2007) used for Lexical Substitution, SimpLex ranks candidates based on a weighted linear scoring function, which has the generalized form: X 1 s (cn,i ) = m r (cn,i ) m∈M where cn,i is the candidate substitute to be scored, and each rm is a standalone ranking function that attributes to each candidate its rank based on its uniquely associated features. Based on this scoring, candidates for context are ranked in descending order of scores. In the development of the system we experimented with a number of these features including ranking based on word length, number of syllables, scoring with a 2-ste"
S12-1066,S07-1009,0,0.0529578,"first overall on the Lexical Simplification task. This paper describes SimpLex,1 a Lexical Simplification system that participated in the English Lexical Simplification shared task at SemEval-2012. It operates on the basis of a linear weighted ranking function composed of context sensitive and psycholinguistic features. The system outperforms a very strong baseline, and ranked first on the shared task. 1 2 Introduction Lexical Simplification revolves around replacing words by their simplest synonym in a context aware fashion. It is similar in many respects to the task of Lexical Substitution (McCarthy and Navigli, 2007) in that it involves elements of selectional preference on the basis of a central predefined criterion (simplicity in the current case), as well as sensitivity to context. Lexical Simplification envisages principally a human target audience, and can greatly benefit children, second language learners, people with low literacy levels or cognitive disabilities, and in general facilitate the dissemination of knowledge to wider audiences. We experimented with a number of features that we posited might be inherently linked with textual simplicity and selected the three that seemed the most promising"
S12-1066,S12-1046,1,0.574477,"Missing"
S12-1066,N10-1056,0,0.0656446,"mination of knowledge to wider audiences. We experimented with a number of features that we posited might be inherently linked with textual simplicity and selected the three that seemed the most promising on an evaluation with the trial dataset. These include contextual and psycholinguistic components. When combined using an SVM 1 Developed by co-organizers of the shared task Related Work Lexical Simplification has received considerably less interest in the NLP community as compared with Syntactic Simplification. However, there are a number of notable works related to the topic. In particular Yatskar et al. (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. Biran et al. (2011) extend this base methodology to apply lexical simplification to input sentences. De Belder and Moens (2010), in contrast, provide a more general architecture for the task, with scope for possible extension to other languages. These studies and others have envisaged a range of different target user groups including children (De Belder and Moens, 2010), people with low literacy levels (Aluisio et al., 2008) and aphasic readers (Carroll et al., 1998). The current work differ"
S12-1100,W10-1703,0,0.0694003,"Missing"
S12-1100,W04-3205,0,0.0753091,"Missing"
S12-1100,W10-1751,0,0.027057,"d Entities and TINE (Rios et al., 2011). Named entities are matched by type and content: while the type has to match exactly, the content is compared with the assistance of a distributional thesaurus. TINE is a metric proposed to measure adequacy in machine translation and favors similar semantic frames. TINE attempts to align verb predicates, assuming a oneto-one correspondence between semantic roles, and considering ontologies for inexact alignment. The surface realization of the arguments is compared using a distributional thesaurus and the cosine similarity metric. Finally, we use METEOR (Denkowski and Lavie, 2010), also a common metric for machine translation evaluation, that also computes inexact word overlap as at way of measuring the impact of our semantic metrics. The lexical and syntactic metrics complement the semantic metrics in dealing with the phenomena observed in the task’s dataset. For instance, from the MSRvid dataset: S1 Two men are playing football. S2 Two men are practicing football. In this case, as typical of paraphrasing, the situation and participants are the same while the surface realization differs, but playing can be considered similar to practicing. From the SMT-eur dataset: S3"
S12-1100,P98-2127,0,0.0491073,"Missing"
S12-1100,P02-1040,0,0.085034,"clusive. 1 Introduction We describe the UOW submissions to the Semantic Textual Similarity (STS) task at SemEval-2012. Our systems are based on combining similarity scores as features using a regression algorithm to predict the degree of semantic equivalence between a pair of sentences. We train the regression algorithm with different classes of similarity metrics: i) lexical, ii) syntactic and iii) semantic. The lexical similarity metrics are: i) cosine similarity using a bag-ofwords representation, and ii) precision, recall and F-measure of content words. The syntactic metric computes BLEU (Papineni et al., 2002), a machine translation evaluation metric, over a labels of basephrases (chunks). Two semantic metrics are used: a metric based on the preservation of Named Entities and TINE (Rios et al., 2011). Named entities are matched by type and content: while the type has to match exactly, the content is compared with the assistance of a distributional thesaurus. TINE is a metric proposed to measure adequacy in machine translation and favors similar semantic frames. TINE attempts to align verb predicates, assuming a oneto-one correspondence between semantic roles, and considering ontologies for inexact"
S12-1100,W11-2112,1,0.725819,"sion algorithm to predict the degree of semantic equivalence between a pair of sentences. We train the regression algorithm with different classes of similarity metrics: i) lexical, ii) syntactic and iii) semantic. The lexical similarity metrics are: i) cosine similarity using a bag-ofwords representation, and ii) precision, recall and F-measure of content words. The syntactic metric computes BLEU (Papineni et al., 2002), a machine translation evaluation metric, over a labels of basephrases (chunks). Two semantic metrics are used: a metric based on the preservation of Named Entities and TINE (Rios et al., 2011). Named entities are matched by type and content: while the type has to match exactly, the content is compared with the assistance of a distributional thesaurus. TINE is a metric proposed to measure adequacy in machine translation and favors similar semantic frames. TINE attempts to align verb predicates, assuming a oneto-one correspondence between semantic roles, and considering ontologies for inexact alignment. The surface realization of the arguments is compared using a distributional thesaurus and the cosine similarity metric. Finally, we use METEOR (Denkowski and Lavie, 2010), also a comm"
S12-1100,C98-2122,0,\N,Missing
S12-1100,W11-2100,0,\N,Missing
S15-2015,S13-1004,0,0.0316499,"submission between Saarland University and University of Sheffield to the STS English shared task at SemEval2015. We have submitted three models that use Machine Translation (MT) evaluation metrics as features to build supervised regressors that predict the similarity scores for the STS task. We introduce two variants of a novel deep regressor architecture and a classical baseline regression system that uses MT evaluation metrics as input features. Related Work Previously, research teams have applied MT evaluation metrics for the STS task with increasingly better results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). Rios et al. (2012) trained a Support Vector Regressor scoring a Pearson correlation mean of 0.3825 (Baseline1 : 0.4356). Barr´onCede˜no et al. (2013) also used a Support Vector Regressor and did better than the baseline at 0.4037 mean score (Baseline: 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 201"
S15-2015,W05-0909,0,0.143784,"STS English Task: • ModelX: Deep Regression framework with the full feature set from n-gram overlaps, Shallow Parsing and METEOR. For SP-POS, SP-LEMMA and SP-IOB, we use the NIST-like measure where we not only consider the individual POS, LEMMA or IOB tags but an accumulated score over a sequence of 1-5 ngrams, e.g. SP-POS(DT+NN,DT+NN+VBZ, ...) or SP-LEMMA(a+dog,a+dog+jump, ...). 5 Experiments and Results METEOR METEOR aligns the translation to a reference translation first then it uses unigram mapping to match words at their surface forms, word stems, synonym matches and paraphrase matches (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). Different from the n-gram and shallow parsing features, METEOR makes a distinction between content words and function words and the precision and recall is measured by weighing them differently. 87 • ModelY: Bayesian Ridge Regressor with the full feature set • ModelZ: Deep Regression framework with only METEOR features For the hidden regressors layer of the deep regression models, we have used the multivariate linear, logistic, Bayesian ridge, elastic net, random sample consensus and support vector (radial basis function kernel) regressors.2 The final layer regres"
S15-2015,S13-1020,0,0.218511,"Missing"
S15-2015,S13-1034,1,0.905276,"Missing"
S15-2015,S14-2085,0,0.112676,"Missing"
S15-2015,N10-1031,0,0.0555248,"X: Deep Regression framework with the full feature set from n-gram overlaps, Shallow Parsing and METEOR. For SP-POS, SP-LEMMA and SP-IOB, we use the NIST-like measure where we not only consider the individual POS, LEMMA or IOB tags but an accumulated score over a sequence of 1-5 ngrams, e.g. SP-POS(DT+NN,DT+NN+VBZ, ...) or SP-LEMMA(a+dog,a+dog+jump, ...). 5 Experiments and Results METEOR METEOR aligns the translation to a reference translation first then it uses unigram mapping to match words at their surface forms, word stems, synonym matches and paraphrase matches (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). Different from the n-gram and shallow parsing features, METEOR makes a distinction between content words and function words and the precision and recall is measured by weighing them differently. 87 • ModelY: Bayesian Ridge Regressor with the full feature set • ModelZ: Deep Regression framework with only METEOR features For the hidden regressors layer of the deep regression models, we have used the multivariate linear, logistic, Bayesian ridge, elastic net, random sample consensus and support vector (radial basis function kernel) regressors.2 The final layer regressor is a Bayesian ridge regr"
S15-2015,W14-3351,0,0.173018,"Missing"
S15-2015,C12-2044,0,0.0694697,": 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013; Bic¸ici and Way, 2014). These previous approaches have trained a different system for each subcorpus provided by the task organizers. We have chosen to combine the different subcorpora since MT evaluation metrics are expected to be robust against text types and domains (Han et al., 2012; Pad´o et al., 2009). Much of the previous work on using MT evaluation metrics is based on improving the regressors through algorithm choice, feature selection and parameters tuning. We introduce a novel architecture of hybrid supervised machine learning, Deep Regression, which attempts to combine different regressors and automating feature selection by means of dimensionality reduction. 1 Refers to the token cosine baseline (baseline-tokencos) from the task organizers. 85 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 85–89, c Denver, Colorado, Jun"
S15-2015,S14-2102,0,0.348269,"ce two variants of a novel deep regressor architecture and a classical baseline regression system that uses MT evaluation metrics as input features. Related Work Previously, research teams have applied MT evaluation metrics for the STS task with increasingly better results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). Rios et al. (2012) trained a Support Vector Regressor scoring a Pearson correlation mean of 0.3825 (Baseline1 : 0.4356). Barr´onCede˜no et al. (2013) also used a Support Vector Regressor and did better than the baseline at 0.4037 mean score (Baseline: 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013; Bic¸ici and Way, 2014). These previous approaches have trained a different system for each subcorpus provided by the task organizers. We have chosen to combine the different subcorpora since MT evaluation metrics are expected to be robust against text types and domains (Han et al., 2012; Pad´o et al., 2"
S15-2015,P09-1034,0,0.064364,"Missing"
S15-2015,P02-1040,0,0.0962895,"ch metric comprises several features that compute the translation quality by comparing every translation against one or several reference translations. We consider three sets of features: n-gram overlaps, Shallow Parsing metrics and METEOR. These metrics correspond to the lexical, syntactic and semantic levels respectively. 4.1 N -gram Overlaps Gonz`alez et al. (2014) reintroduces the notion of language independent metrics relying on n-gram overlaps. This is similar to the BLEU metric that calculates the geometric mean of n-gram precision by comparing the translation against its reference(s) (Papineni et al., 2002) without the brevity penalty. Different from BLEU, the n-gram overlaps are computed as similarity coefficients instead of taking the crude proportion of overlap n-gram. n -gramoverlap = sim n -gramtrans ∩ n -gramref Figure 1: Deep Regression Architecture. Figure 1 presents the Deep Regression architecture where the inputs are fed into the different hidden regressors and unlike traditional neural network, each regressor produces a discrete output with a different cost function unlike the consistent activation function in neural nets. Different from ensemble learning, the voting/selection determ"
S15-2015,S12-1100,1,0.739167,"University of Sheffield to the STS English shared task at SemEval2015. We have submitted three models that use Machine Translation (MT) evaluation metrics as features to build supervised regressors that predict the similarity scores for the STS task. We introduce two variants of a novel deep regressor architecture and a classical baseline regression system that uses MT evaluation metrics as input features. Related Work Previously, research teams have applied MT evaluation metrics for the STS task with increasingly better results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). Rios et al. (2012) trained a Support Vector Regressor scoring a Pearson correlation mean of 0.3825 (Baseline1 : 0.4356). Barr´onCede˜no et al. (2013) also used a Support Vector Regressor and did better than the baseline at 0.4037 mean score (Baseline: 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013; Bic¸ici and Way, 2014). These previous"
S15-2015,W00-0726,0,0.036069,"Missing"
S15-2015,1992.tmi-1.7,0,0.226955,"input is reduced to the number of hidden regressors and the input for the last layer regressors is a latent layer in the higher dimensional space. Within a standard neural net, every node in the latent layer is influenced by all the perceptrons in the previous layer. In contrast, each latent dimen86  We use 16 features of n-gram overlap by considering both the cosine similarity and Jaccard Index in calculating the n-gram overlaps for character and token n-gram from the order of bigrams to 5-grams. In addition, we use the ratio of n-gram lengths and the Jaccard similarity of pseudo-cognates (Simard et al., 1992) as the 17th and 18th n-gram overlap features. 4.2 Shallow Parsing The Shallow Parsing (SP) metric measures the syntactic similarities by computing the overlaps between the translation and the reference translation at the Parts-Of-Speech (POS), word lemmas and base phrase chunks level. The purpose of the SP metric is to capture the proportion of lexical items correctly translated according to their shallow syntactic realization. The base phrase chunks are tagged using the BIOS toolkit (Surdeanu et al., 2005) and POS tagging and lemmatization are achieved using SVMTool (Gim´enez and M`arquez, 2"
S15-2015,S14-2010,0,\N,Missing
S15-2015,S12-1051,0,\N,Missing
S16-1085,E99-1042,0,0.0319809,"dictor of word complexity. 1 Introduction Complex Word Identification (CWI) is the task of deciding which words should be simplified in a given text. It is commonly connected with the task of Lexical Simplification (LS), which has as goal to replace complex words and expressions with simpler alternatives. In the usual LS pipeline, which was first introduced by (Shardlow, 2014), CWI is the first step. An effective CWI strategy can prevent LS approaches from replacing simple words, and hence prevent them from making grammatical and/or semantic errors. Early LS approaches (Devlin and Tait, 1998; Carroll et al., 1999) do not include CWI. As shown in (Paetzold and Specia, 2013; Shardlow, 2014), ignoring this step can considerably decrease the quality of the output produced by a simplifier. CWI has been gaining popularity in recent research. The LS approach in (Horn et al., 2014) employs an implicit CWI strategy in which a target word is only deemed complex if the LS model can find a candidate substitution which is simpler. Their results, however, show that the approach is unable to find simplifications for one third of the complex words in the dataset. (Shardlow, 2013b) presents the CW corpus: the first dat"
S16-1085,P13-1151,0,0.035208,"h word in a given sentence. In the following we provide more details on the sentences used and the annotation process. 3.1 Data Sources We selected 9,200 sentences to be annotated, after filtering out cases with spurious characters, HTML 561 CW Corpus (Shardlow, 2013b): composed of 731 sentences from the Simple English Wikipedia in which exactly one word had been simplified by Wikipedia editors from the standard English Wikipedia. Commonly used for the training and evaluation of Complex Word Identification systems. 231 sentences that conformed to our criteria were extracted. Simple Wikipedia (Kauchak, 2013): composed of 167,689 sentences from the Simple English Wikipedia, each aligned to an equivalent sentence in the standard English Wikipedia. We selected a set of 8,700 sentences from the Simple Wikipedia version that conformed to our criteria and were aligned to an identical sentence in Wikipedia. The goal was to evaluate the ability of the Wikipedia (human) editors in identifying complex words for readers of the Simple Wikipedia. 3.2 Annotation Process 400 non-native speakers of English participated in the experiment, mostly university students or staff. Volunteers provided anonymous informat"
S16-1085,padro-stanilovsky-2012-freeling,0,0.0331487,"evaluate the ability of the Wikipedia (human) editors in identifying complex words for readers of the Simple Wikipedia. 3.2 Annotation Process 400 non-native speakers of English participated in the experiment, mostly university students or staff. Volunteers provided anonymous information about their native language, age, education level and English proficiency level according to CEFR (Common European Framework of Reference for Languages). They were asked to judge whether or not they could understand the meaning of each content word (nouns, verbs, adjectives and adverbs, as tagged by Freeling (Padr and Stanilovsky, 2012)) in a set of sentences, each of which was judged independently. Volunteers were instructed to annotate all words that they could not understand individually, even if they could comprehend the meaning of the sentence as a whole. A subset of 200 sentences was split into 20 subsets of 10 sentences, and each subset was annotated by a total of 20 volunteers. The remaining 9,000 sentences were split into 300 subsets of 30 sentences, each of which was annotated by a single volunteer. 4 Analysis A total of 35,958 distinct words were annotated (232,481 in total). Out of these, 3,854 distinct words (6,"
S16-1085,W13-4813,1,0.927707,"Identification (CWI) is the task of deciding which words should be simplified in a given text. It is commonly connected with the task of Lexical Simplification (LS), which has as goal to replace complex words and expressions with simpler alternatives. In the usual LS pipeline, which was first introduced by (Shardlow, 2014), CWI is the first step. An effective CWI strategy can prevent LS approaches from replacing simple words, and hence prevent them from making grammatical and/or semantic errors. Early LS approaches (Devlin and Tait, 1998; Carroll et al., 1999) do not include CWI. As shown in (Paetzold and Specia, 2013; Shardlow, 2014), ignoring this step can considerably decrease the quality of the output produced by a simplifier. CWI has been gaining popularity in recent research. The LS approach in (Horn et al., 2014) employs an implicit CWI strategy in which a target word is only deemed complex if the LS model can find a candidate substitution which is simpler. Their results, however, show that the approach is unable to find simplifications for one third of the complex words in the dataset. (Shardlow, 2013b) presents the CW corpus: the first dataset for CWI. Although a relevant contribution, this datase"
S16-1085,P13-3015,0,0.280809,"proaches (Devlin and Tait, 1998; Carroll et al., 1999) do not include CWI. As shown in (Paetzold and Specia, 2013; Shardlow, 2014), ignoring this step can considerably decrease the quality of the output produced by a simplifier. CWI has been gaining popularity in recent research. The LS approach in (Horn et al., 2014) employs an implicit CWI strategy in which a target word is only deemed complex if the LS model can find a candidate substitution which is simpler. Their results, however, show that the approach is unable to find simplifications for one third of the complex words in the dataset. (Shardlow, 2013b) presents the CW corpus: the first dataset for CWI. Although a relevant contribution, this dataset contains only 731 instances extracted automatically from the Simple English Wikipedia edits, which raises concerns about its reliability and applicability. The results obtained by Shardlow (2013a) highlight some of the issues of the dataset. They use the CW corpus to compare the performance of three solutions to CWI: a Threshold-Based approach, a Support Vector Machine (SVM), and a “Simplify Everything” approach. In their experiments, the “Simplify Everything” approach achieves higher Accuracy,"
S16-1085,W13-2908,0,0.0792963,"proaches (Devlin and Tait, 1998; Carroll et al., 1999) do not include CWI. As shown in (Paetzold and Specia, 2013; Shardlow, 2014), ignoring this step can considerably decrease the quality of the output produced by a simplifier. CWI has been gaining popularity in recent research. The LS approach in (Horn et al., 2014) employs an implicit CWI strategy in which a target word is only deemed complex if the LS model can find a candidate substitution which is simpler. Their results, however, show that the approach is unable to find simplifications for one third of the complex words in the dataset. (Shardlow, 2013b) presents the CW corpus: the first dataset for CWI. Although a relevant contribution, this dataset contains only 731 instances extracted automatically from the Simple English Wikipedia edits, which raises concerns about its reliability and applicability. The results obtained by Shardlow (2013a) highlight some of the issues of the dataset. They use the CW corpus to compare the performance of three solutions to CWI: a Threshold-Based approach, a Support Vector Machine (SVM), and a “Simplify Everything” approach. In their experiments, the “Simplify Everything” approach achieves higher Accuracy,"
S16-1085,shardlow-2014-open,0,0.101234,"re submitted from 21 distinct teams, and nine baselines were provided. The results highlight the effectiveness of Decision Trees and Ensemble methods for the task, but ultimately reveal that word frequencies remain the most reliable predictor of word complexity. 1 Introduction Complex Word Identification (CWI) is the task of deciding which words should be simplified in a given text. It is commonly connected with the task of Lexical Simplification (LS), which has as goal to replace complex words and expressions with simpler alternatives. In the usual LS pipeline, which was first introduced by (Shardlow, 2014), CWI is the first step. An effective CWI strategy can prevent LS approaches from replacing simple words, and hence prevent them from making grammatical and/or semantic errors. Early LS approaches (Devlin and Tait, 1998; Carroll et al., 1999) do not include CWI. As shown in (Paetzold and Specia, 2013; Shardlow, 2014), ignoring this step can considerably decrease the quality of the output produced by a simplifier. CWI has been gaining popularity in recent research. The LS approach in (Horn et al., 2014) employs an implicit CWI strategy in which a target word is only deemed complex if the LS mod"
S16-1095,S13-1004,0,0.0692141,"cutting open a box.”, an STS system predicts a real number similarity score on a scale of 0 (no relation) to 5 (semantic equivalence). This paper presents a collaborative submission between Saarland University and University of Sheffield to the STS English shared task at SemEval2016. We have submitted three supervised models that predict the similarity scores for the STS task using Machine Translation (MT) evaluation metrics as regression features. 2 Related Work Previous approaches have applied MT evaluation metrics for the STS task with progressively improving results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). At the pilot English STS-2012 task, Rios et al. (2012) trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Su"
S16-1095,1983.tc-1.13,0,0.706143,"Missing"
S16-1095,N10-1031,0,0.113645,"ls that predict the similarity scores for the STS task using Machine Translation (MT) evaluation metrics as regression features. 2 Related Work Previous approaches have applied MT evaluation metrics for the STS task with progressively improving results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). At the pilot English STS-2012 task, Rios et al. (2012) trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 2005) and semantic roles, discourse representation and explicit sema"
S16-1095,W14-3351,0,0.058462,"Missing"
S16-1095,D15-1124,1,0.873884,"Missing"
S16-1095,S14-2102,0,0.554526,"Missing"
S16-1095,S14-2003,0,0.013839,"LEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 2005) and semantic roles, discourse representation and explicit semantic analysis (Gabrilovich and Markovitch, 2007) annotations of the text snippets. These similarity measures are packaged in the Asiya toolkit (Gim´enez and M`arquez, 2010). They scored 0.4037 mean score and performed better than the Takelab baseˇ c et al., 2012) at 0.3639. line (Sari´ At the SemEval-2014 Cross-level Semantic Similarity task (Jurgens et al., 2014; Jurgens et al., 2015), participating teams submitted similarity scores for text of different granularity. Huang and Chang (2014) used a linear regressor solely with MT evalu1 Refers to the token cosine (baseline-tokencos) in STS-2012. 628 Proceedings of SemEval-2016, pages 628–633, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics baseline system ation metrics (BLEU, METEOR, ROUGE) to compute the similarity scores between paragraphs and sentences. They scored 0.792 beating the lowest common substring baseline which scored 0.613. In the SemEval-2015 Eng"
S16-1095,P04-1077,0,0.0772826,"2) trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 2005) and semantic roles, discourse representation and explicit semantic analysis (Gabrilovich and Markovitch, 2007) annotations of the text snippets. These similarity measures are packaged in the Asiya toolkit (Gim´enez and M`arquez, 2010). They scored 0.4037 mean score and performed better than the Takelab baseˇ c et al., 2012) at 0.3639. line (Sari´ At the SemEval-2014 Cross-level Semantic Similarity task (Jurgens et al., 2014; Jurgens et al.,"
S16-1095,W05-0904,0,0.0464141,"ineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 2005) and semantic roles, discourse representation and explicit semantic analysis (Gabrilovich and Markovitch, 2007) annotations of the text snippets. These similarity measures are packaged in the Asiya toolkit (Gim´enez and M`arquez, 2010). They scored 0.4037 mean score and performed better than the Takelab baseˇ c et al., 2012) at 0.3639. line (Sari´ At the SemEval-2014 Cross-level Semantic Similarity task (Jurgens et al., 2014; Jurgens et al., 2015), participating teams submitted similarity scores for text of different granularity. Huang and Chang (2014) used a linear regressor solely with MT ev"
S16-1095,W12-3129,0,0.0199825,"ompute the similarity scores between paragraphs and sentences. They scored 0.792 beating the lowest common substring baseline which scored 0.613. In the SemEval-2015 English STS and Twitter similarity tasks, Bertero and Fung (2015) trained a neural network classifier using (i) lexical similarity features based on WordNet (Miller, 1995), (ii) neural auto-encoders (Socher et al., 2011), syntactic features based on parse tree edit distance (Zhang and Shasha, 1989; Wan et al., 2006) and (iii) MT evaluation metrics, viz. BLEU, TER, SEPIA (Habash and Elkholy, 2008), BADGER (Parker, 2008) and MEANT (Lo et al., 2012). For the classic English STS task in SemEval2015, Tan et al. (2015) used a range of MT evaluation metrics based on lexical (surface ngram overlaps), syntactic (shallow parsing similarity) and semantic features (METEOR variants) to train a Bayesian ridge regressor. Their best system achieved 0.7275 mean Pearson correlation outperforming the token-cos baseline which scored 0.5871 while the top system (Sultan et al., 2015) achieved 0.8015. Another notable mention of MT technology in the STS tasks is the use of referential translation machines to predict and derive features instead of using MT ev"
S16-1095,P02-1040,0,0.114057,"task at SemEval2016. We have submitted three supervised models that predict the similarity scores for the STS task using Machine Translation (MT) evaluation metrics as regression features. 2 Related Work Previous approaches have applied MT evaluation metrics for the STS task with progressively improving results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). At the pilot English STS-2012 task, Rios et al. (2012) trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 200"
S16-1095,S12-1100,1,0.813789,"ation) to 5 (semantic equivalence). This paper presents a collaborative submission between Saarland University and University of Sheffield to the STS English shared task at SemEval2016. We have submitted three supervised models that predict the similarity scores for the STS task using Machine Translation (MT) evaluation metrics as regression features. 2 Related Work Previous approaches have applied MT evaluation metrics for the STS task with progressively improving results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). At the pilot English STS-2012 task, Rios et al. (2012) trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Li"
S16-1095,S12-1060,0,0.060836,"Missing"
S16-1095,2006.amta-papers.25,0,0.0633173,"xical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 2005) and semantic roles, discourse representation and explicit semantic analysis (Gabrilovich and Markovitch, 2007) annotations of the text snippets. These similarity measures are packaged in the Asiya toolkit (Gim´enez and M`arquez, 2010). They scored 0.4037 mean score and performed better than the Takelab baseˇ c et al., 2012) at 0.3639. line (Sari´ At the SemEval-2014 Cross-level Semantic Similarity task (Jurgens et al., 2014; Jurgens et al., 2015), participating teams submitted similarity scor"
S16-1095,W14-3354,0,0.0234987,"feature set, we use 52 shallow parsing features described in (Tan et al., 2015); they measure the similarity coefficients from the n-gram overlaps of the lexicalized shallow parsing (aka chunking) annotations. As for semantics, we use 44 similarity coefficients from Named Entity (NE) annotation overlaps between two texts. After some feature analysis, we found that 22 out of the 44 NE n-gram overlap features and 1 of the shallow parsing features have extremely low variance across all sentence pairs in the training data. We removed these features before training our models. 3.1.2 BEER Features Stanojevic and Simaan (2014) presents an MT evaluation metric that uses character n-gram overlaps, the Kendall tau distance of the monotonic word order (Isozaki et al., 2010; Birch and Osborne, 2010) and abstract ordering patterns from tree factorization of permutations (Zhang and Gildea, 2007). While Asiya features are agnostic to word classes, BEER differentiates between function words and non-function words when calculating its adequacy features. 3.1.3 METEOR Features METEOR first aligns the translation to its reference, then it uses the unigram mapping to see whether they match based on their surface forms, Linear Bo"
S16-1095,S15-2027,0,0.0913526,"e tree edit distance (Zhang and Shasha, 1989; Wan et al., 2006) and (iii) MT evaluation metrics, viz. BLEU, TER, SEPIA (Habash and Elkholy, 2008), BADGER (Parker, 2008) and MEANT (Lo et al., 2012). For the classic English STS task in SemEval2015, Tan et al. (2015) used a range of MT evaluation metrics based on lexical (surface ngram overlaps), syntactic (shallow parsing similarity) and semantic features (METEOR variants) to train a Bayesian ridge regressor. Their best system achieved 0.7275 mean Pearson correlation outperforming the token-cos baseline which scored 0.5871 while the top system (Sultan et al., 2015) achieved 0.8015. Another notable mention of MT technology in the STS tasks is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013; Bic¸ici and Way, 2014; Bicici, 2015). 3 Approach Following the success of systems that use MT evaluation metrics, we train three regression models using an array of MT metrics based on lexical, syntactic and semantic features. 3.1 Feature Matrix Machine translation evaluation metrics utilize various degrees of lexical, syntactic and semantic information. Each metric consi"
S16-1095,P15-1150,0,0.057508,"erently. We use all four variants of METEOR: exact, stem, synonym and paraphrase. gressor (XGBoost) (Chen and He, 2015; Chen and Guestrin, 2015). They were trained using all features described in Section 3. We have released the MT metrics annotations of the STS data and implementation of systems on https://github.com/alvations/stasis /blob/master/notebooks/ARMOR.ipynb 3.1.4 ReVal Features 4 ReVal (Gupta et al., 2015) is a deep neural net based metric which uses the cosine similarity score between the Tree-based Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Tai et al., 2015) dense vector space representations of two sentences. 3.2 Models We annotated the STS 2012 to 2015 datasets with the features as described in Section 3.1 and submitted three models to the SemEval-2016 English STS Task using (i) a linear regressor (Linear), (ii) boosted tree regressor (Boosted) (Friedman, 2001) and (iii) eXtreme Gradient Boosted tree re630 Results Table 1 presents the official results for our submissions to the English STS task. The bottom part of the table presents the median and the best correlation results across all participating teams for the respective domains. Our baseli"
S16-1095,S15-2015,1,0.847418,"Missing"
S16-1095,U06-1019,0,0.0223859,"California, June 16-17, 2016. 2016 Association for Computational Linguistics baseline system ation metrics (BLEU, METEOR, ROUGE) to compute the similarity scores between paragraphs and sentences. They scored 0.792 beating the lowest common substring baseline which scored 0.613. In the SemEval-2015 English STS and Twitter similarity tasks, Bertero and Fung (2015) trained a neural network classifier using (i) lexical similarity features based on WordNet (Miller, 1995), (ii) neural auto-encoders (Socher et al., 2011), syntactic features based on parse tree edit distance (Zhang and Shasha, 1989; Wan et al., 2006) and (iii) MT evaluation metrics, viz. BLEU, TER, SEPIA (Habash and Elkholy, 2008), BADGER (Parker, 2008) and MEANT (Lo et al., 2012). For the classic English STS task in SemEval2015, Tan et al. (2015) used a range of MT evaluation metrics based on lexical (surface ngram overlaps), syntactic (shallow parsing similarity) and semantic features (METEOR variants) to train a Bayesian ridge regressor. Their best system achieved 0.7275 mean Pearson correlation outperforming the token-cos baseline which scored 0.5871 while the top system (Sultan et al., 2015) achieved 0.8015. Another notable mention o"
S16-1095,W07-0404,0,0.0167353,"d Entity (NE) annotation overlaps between two texts. After some feature analysis, we found that 22 out of the 44 NE n-gram overlap features and 1 of the shallow parsing features have extremely low variance across all sentence pairs in the training data. We removed these features before training our models. 3.1.2 BEER Features Stanojevic and Simaan (2014) presents an MT evaluation metric that uses character n-gram overlaps, the Kendall tau distance of the monotonic word order (Isozaki et al., 2010; Birch and Osborne, 2010) and abstract ordering patterns from tree factorization of permutations (Zhang and Gildea, 2007). While Asiya features are agnostic to word classes, BEER differentiates between function words and non-function words when calculating its adequacy features. 3.1.3 METEOR Features METEOR first aligns the translation to its reference, then it uses the unigram mapping to see whether they match based on their surface forms, Linear Boosted XGBoost Median Best answer-answer 0.31539 0.37717 0.47716 0.48018 0.69235 headlines 0.76551 0.77183 0.78848 0.76439 0.82749 plagiarism 0.82063 0.81529 0.83212 0.78949 0.84138 postediting 0.83329 0.84528 0.84960 0.81241 0.86690 question-question 0.73987 0.66825"
S16-1095,S15-2010,0,\N,Missing
S16-1095,S14-2085,0,\N,Missing
S16-1095,W05-0909,0,\N,Missing
S16-1095,S12-1051,0,\N,Missing
S16-1149,W07-1007,0,0.0237372,"ge the members of a given target audience. It is part of the usual Lexical Simplification pipeline, which is illustrated in Figure 1. As shown by the results obtained by (Paetzold and Specia, 2013) and (Shardlow, 2014), ignoring the step of Complex Word Identification in Lexical Simplification can lead simplifiers to neglect challenging words, as well as to replace simple words with inappropriate alternatives. Various strategies have been devised to address CWI and most of them are very simple in nature. For example, to identify complex words, the lexical simplifier for the medical domain in (Elhadad and Sutaria, 2007) uses a Lexicon-Based approach that exploits the UMLS (Bodenreider, 2004) database: if a medical expression is among the technical terms registered in UMLS, then it is complex. The complexity identifier for the lexical simplifier in (Keskis¨arkk¨a, 2012), for Swedish, uses a threshold over word frequencies to distinguish complex from simple words. Recently, however, more sophisticated approaches have been used. (Shardlow, 2013) presents a CWI benchmarking that compares the performance of a Threshold-Based strategy, a Support Vector Machine (SVM) model trained over various features, and a “simp"
S16-1149,N06-1058,0,0.00985943,"series of sub-systems, to avoid confusion, we henceforth refer to these subsystems as “voters”. 3.1 Features Our voters use a total of 69 features. They can be divided in four categories: • Binary: If a target word is part of a certain vocabulary, then it receives label 1, otherwise, 0. We extract vocabularies from Simple Wikipedia (Kauchak, 2013), Ogden’s Basic English (Ogden, 1968) and SubIMDB (Paetzold, 2015). 971 • Lexical: Includes word length, number of syllables, number of senses, synonyms, hypernyms and hyponyms in WordNet (Fellbaum, 1998), and language model probability in Wikipedia (Kauchak and Barzilay, 2006), Simple Wikipedia and SubIMDB. • Collocational: Language model probabilities of all n-gram combinations with windows w < 3 to the left and right of the target complex word in Wikipedia, SUBTLEX (Brysbaert and New, 2009), Simple Wikipedia and SubIMDB. • Nominal: Includes the word itself, its POS tag, both word and POS tag n-gram combinations with windows w < 3 to the left and right, and the word’s language model backoff behavior (Uhrik and Ward, 1997) according to a 5-gram language model trained over Simple Wikipedia with SRILM (Stolcke and others, 2002). In order for language model probabilit"
S16-1149,P13-1151,0,0.032053,"classes. In what follows, we described the features and settings used in the creation of our two CWI systems: SV000gg-Hard and SV000gg-Soft. While SV000gg-Hard uses basic Hard Voting, SV000ggSoft uses Performance-Oriented Soft Voting. Since both of them combine a series of sub-systems, to avoid confusion, we henceforth refer to these subsystems as “voters”. 3.1 Features Our voters use a total of 69 features. They can be divided in four categories: • Binary: If a target word is part of a certain vocabulary, then it receives label 1, otherwise, 0. We extract vocabularies from Simple Wikipedia (Kauchak, 2013), Ogden’s Basic English (Ogden, 1968) and SubIMDB (Paetzold, 2015). 971 • Lexical: Includes word length, number of syllables, number of senses, synonyms, hypernyms and hyponyms in WordNet (Fellbaum, 1998), and language model probability in Wikipedia (Kauchak and Barzilay, 2006), Simple Wikipedia and SubIMDB. • Collocational: Language model probabilities of all n-gram combinations with windows w < 3 to the left and right of the target complex word in Wikipedia, SUBTLEX (Brysbaert and New, 2009), Simple Wikipedia and SubIMDB. • Nominal: Includes the word itself, its POS tag, both word and POS ta"
S16-1149,W13-4813,1,0.846746,"its prediction confidence, allowing for completely heterogeneous systems to be combined. Our performance comparison shows that our voting techniques outperform traditional Soft Voting, as well as other systems submitted to the shared task, ranking first and second overall. 1 Figure 1: Lexical Simplification pipeline Introduction In Complex Word Identification (CWI), the goal is to find which words in a given text may challenge the members of a given target audience. It is part of the usual Lexical Simplification pipeline, which is illustrated in Figure 1. As shown by the results obtained by (Paetzold and Specia, 2013) and (Shardlow, 2014), ignoring the step of Complex Word Identification in Lexical Simplification can lead simplifiers to neglect challenging words, as well as to replace simple words with inappropriate alternatives. Various strategies have been devised to address CWI and most of them are very simple in nature. For example, to identify complex words, the lexical simplifier for the medical domain in (Elhadad and Sutaria, 2007) uses a Lexicon-Based approach that exploits the UMLS (Bodenreider, 2004) database: if a medical expression is among the technical terms registered in UMLS, then it is com"
S16-1149,P15-4015,1,0.841177,"(Brysbaert and New, 2009), Simple Wikipedia and SubIMDB. • Nominal: Includes the word itself, its POS tag, both word and POS tag n-gram combinations with windows w < 3 to the left and right, and the word’s language model backoff behavior (Uhrik and Ward, 1997) according to a 5-gram language model trained over Simple Wikipedia with SRILM (Stolcke and others, 2002). In order for language model probabilities to be calculated, we train a 5-gram language model for each of the aforementioned corpora using SRILM (Stolcke and others, 2002). Nominal features were obtained with the help of LEXenstein (Paetzold and Specia, 2015). 3.2 Voters We train a total of 21 voters which we have grouped in three categories: • Lexicon-Based (LB): If a word is present in a given vocabulary of simple words, then it is simple, otherwise, it is complex. We train one Lexicon-Based voter for each binary feature described in the previous Section. • Threshold-Based (TB): Given a certain feature, learns the threshold t which best separates complex and simple words. In order to learn t, it first calculates the feature value for all instances in the training data and obtains its minimum and maximum. It then divides the interval into 10, 000"
S16-1149,N15-2002,1,0.843404,"used in the creation of our two CWI systems: SV000gg-Hard and SV000gg-Soft. While SV000gg-Hard uses basic Hard Voting, SV000ggSoft uses Performance-Oriented Soft Voting. Since both of them combine a series of sub-systems, to avoid confusion, we henceforth refer to these subsystems as “voters”. 3.1 Features Our voters use a total of 69 features. They can be divided in four categories: • Binary: If a target word is part of a certain vocabulary, then it receives label 1, otherwise, 0. We extract vocabularies from Simple Wikipedia (Kauchak, 2013), Ogden’s Basic English (Ogden, 1968) and SubIMDB (Paetzold, 2015). 971 • Lexical: Includes word length, number of syllables, number of senses, synonyms, hypernyms and hyponyms in WordNet (Fellbaum, 1998), and language model probability in Wikipedia (Kauchak and Barzilay, 2006), Simple Wikipedia and SubIMDB. • Collocational: Language model probabilities of all n-gram combinations with windows w < 3 to the left and right of the target complex word in Wikipedia, SUBTLEX (Brysbaert and New, 2009), Simple Wikipedia and SubIMDB. • Nominal: Includes the word itself, its POS tag, both word and POS tag n-gram combinations with windows w < 3 to the left and right, an"
S16-1149,P13-3015,0,0.121076,"devised to address CWI and most of them are very simple in nature. For example, to identify complex words, the lexical simplifier for the medical domain in (Elhadad and Sutaria, 2007) uses a Lexicon-Based approach that exploits the UMLS (Bodenreider, 2004) database: if a medical expression is among the technical terms registered in UMLS, then it is complex. The complexity identifier for the lexical simplifier in (Keskis¨arkk¨a, 2012), for Swedish, uses a threshold over word frequencies to distinguish complex from simple words. Recently, however, more sophisticated approaches have been used. (Shardlow, 2013) presents a CWI benchmarking that compares the performance of a Threshold-Based strategy, a Support Vector Machine (SVM) model trained over various features, and a “simplify everything” baseline. (Shardlow, 2013)’s SVM model has shown promising results, but CWI approaches do not tend to explore Machine Learning techniques and, in particular, their combination. As an effort to fill this gap, in this paper we describe our contributions to the Complex Word Identification task of SemEval 2016. We introduce two systems, SV000gg-Hard and SV000gg-Soft, both of which use straightforward Ensemble Metho"
S16-1149,shardlow-2014-open,0,0.415258,"wing for completely heterogeneous systems to be combined. Our performance comparison shows that our voting techniques outperform traditional Soft Voting, as well as other systems submitted to the shared task, ranking first and second overall. 1 Figure 1: Lexical Simplification pipeline Introduction In Complex Word Identification (CWI), the goal is to find which words in a given text may challenge the members of a given target audience. It is part of the usual Lexical Simplification pipeline, which is illustrated in Figure 1. As shown by the results obtained by (Paetzold and Specia, 2013) and (Shardlow, 2014), ignoring the step of Complex Word Identification in Lexical Simplification can lead simplifiers to neglect challenging words, as well as to replace simple words with inappropriate alternatives. Various strategies have been devised to address CWI and most of them are very simple in nature. For example, to identify complex words, the lexical simplifier for the medical domain in (Elhadad and Sutaria, 2007) uses a Lexicon-Based approach that exploits the UMLS (Bodenreider, 2004) database: if a medical expression is among the technical terms registered in UMLS, then it is complex. The complexity"
S17-2001,S17-2013,0,0.019824,"Missing"
S17-2001,S17-2031,0,0.0137096,"Missing"
S17-2001,P98-1013,0,0.169413,"Missing"
S17-2001,S15-2045,1,0.888131,"elated but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversati"
S17-2001,L16-1662,0,0.0083775,"ubmission uses sentence IC exclusively. Another ensembles IC with Sultan et al. (2015)’s alignment method, while a third ensembles IC with cosine similarity of summed word embeddings with an IDF weighting scheme. Sentence IC in isolation outperforms all systems except those from ECNU. Combining sentence IC with word embedding similarity performs best. CompiLIG (Ferrero et al., 2017) The best Spanish-English performance on SNLI sentences was achieved by CompiLIG using features including: cross-lingual conceptual similarity using DBNary (Serasset, 2015), cross-language MultiVec word embeddings (Berard et al., 2016), and Brychcin and Svoboda (2016)’s improvements to Sultan et al. (2015)’s method. LIM-LIG (Nagoudi et al., 2017) Using only weighted word embeddings, LIM-LIG took second place on Arabic.17 Arabic word embeddings are summed into sentence embeddings using uniform, POS and IDF weighting schemes. Sentence similarity is computed by cosine similarity. POS and IDF outperform uniform weighting. Combining the IDF and POS weights by multiplication is reported by LIM-LIG to achieve r 0.7667, higher than all submitted Arabic (track 1) systems. HCTI (Shao, 2017) Third place overall is obtained by HCTI wit"
S17-2001,S17-2030,0,0.0302147,"Missing"
S17-2001,S17-2021,0,0.0336317,"Missing"
S17-2001,S16-1081,1,0.903281,"n an English sentence and its Arabic machine translation5 where they perform post-editing to correct errors. Spanish translation is completed by a University of Sheffield graduate student who is a native Spanish speaker and fluent in English. Turkish translations are obtained from SDL.6 3.4 Crowdsourced Annotations Crowdsourced annotation is performed on Amazon Mechanical Turk.8 Annotators examine the STS pairings of English SNLI sentences. STS labels are then transferred to the translated pairs for crosslingual and non-English tracks. The annotation instructions and template are identical to Agirre et al. (2016). Labels are collected in batches of 20 pairs with annotators paid $1 USD per batch. Five annotations are collected per pair. The MTurk master9 qualification is required to perform the task. Gold scores average the five individual annotations. This section describes the preparation of the evaluation data. For SNLI data, this includes the selection of sentence pairs, annotation of pairs with STS labels and the translation of the original English sentences. WMT quality estimation data is directly annotated with STS labels. 3.3 Annotation 4.1 Table 2 summarizes the evaluation data by track. The s"
S17-2001,W14-3302,1,0.742822,"Missing"
S17-2001,S12-1051,1,0.784858,"nd paraphrase detection in that it captures gradations of meaning overlap rather than making binary classifications of particular relationships. While semantic relatedness expresses a graded semantic relationship as well, it is non-specific about the nature of the relationship with contradictory material still being a candidate for a high score (e.g., “night” and “day” are highly related but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep lear"
S17-2001,S17-2012,0,0.0153726,"Missing"
S17-2001,S17-2032,0,0.0142311,"Missing"
S17-2001,D15-1075,0,0.136451,"s/missing. John said he is considered a witness but not a suspect. “He is not a suspect anymore.” John said. The two sentences are not equivalent, but share some details. They flew out of the nest in groups. They flew into the nest together. The two sentences are not equivalent, but are on the same topic. The woman is playing the violin. The young lady enjoys listening to the guitar. The two sentences are completely dissimilar. The black dog is running through the snow. A race car driver is driving his car through the mud. Evaluation Data The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) is the primary evaluation data source with the exception that one of the 3 Previous years of the STS shared task include more data sources. This year the task draws from two data sources and includes a diverse set of languages and language-pairs. 4 HTER is the minimal number of edits required for correction of a translation divided by its length after correction. pilot track on cross-lingual Spanish-English STS. The English tracks attracted the most participation and have the largest use of the evaluation data in ongoing research. 2 Track 1 2 3 4a 4b 5 6 Language(s) Arabic (ar-ar) Arabic-Engl"
S17-2001,S16-1089,0,0.0248981,"Missing"
S17-2001,S17-2015,0,0.0563979,"fication of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://translate.google.com 5 Team ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) BIT (Wu et al., 2017)* BIT (Wu et al., 2017)* BIT (Wu et al., 2017) HCTI (Shao, 2017) MITRE (Henderson et al., 2017) MITRE (Henderson et al., 2017) FCICU (Hassan et al., 2017) neobility (Zhuang and Chang, 2017) FCICU (Hassan et al., 2017) STS-UHH (Kohail et al., 2017) RTV HCTI (Shao, 2017) RTV MatrusriIndia STS-UHH (Kohail et al., 2017) SEF@UHH (Duma and Menzel, 2017) SEF@UHH (Duma and Menzel, 2017) RTV SEF@UHH (Duma and Menzel, 2017) neobility (Zhuang and Chang, 2017) neobility (Zhuang and Chang, 2017) MatrusriIndia NLPProxem UMDeep (Barrow and Peskov, 2017) NLPProxem UMDeep (Barrow and Peskov, 2017) Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* NLPProxem RTM (Bic¸ici"
S17-2001,D15-1181,0,0.0245131,"Missing"
S17-2001,N16-1108,0,0.0260604,"Missing"
S17-2001,S16-1170,0,0.0147665,"Missing"
S17-2001,D17-1070,0,0.281621,"Missing"
S17-2001,N16-1162,0,0.0181244,"Missing"
S17-2001,C04-1051,0,0.847641,"Missing"
S17-2001,N06-2015,0,0.0472258,"Missing"
S17-2001,S17-2024,0,0.0294688,"Missing"
S17-2001,S17-2019,0,0.0294083,"Missing"
S17-2001,P15-1162,0,0.0488426,"Missing"
S17-2001,S12-1061,0,0.675418,"ationship with contradictory material still being a candidate for a high score (e.g., “night” and “day” are highly related but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarizat"
S17-2001,marelli-etal-2014-sick,0,0.0431263,"from English STS shared tasks (2012-2017). 2 5 4 3 2 1 0 Table 1: Similarity scores with explanations and English examples from Agirre et al. (2013). cross-lingual tracks explores data from the WMT 2014 quality estimation task (Bojar et al., 2014).3 Sentences pairs in SNLI derive from Flickr30k image captions (Young et al., 2014) and are labeled with the entailment relations: entailment, neutral, and contradiction. Drawing from SNLI allows STS models to be evaluated on the type of data used to assess textual entailment methods. However, since entailment strongly cues for semantic relatedness (Marelli et al., 2014), we construct our own sentence pairings to deter gold entailment labels from informing evaluation set STS scores. Track 4b investigates the relationship between STS and MT quality estimation by providing STS labels for WMT quality estimation data. The data includes Spanish translations of English sentences from a variety of methods including RBMT, SMT, hybrid-MT and human translation. Translations are annotated with the time required for human correction by post-editing and Human-targeted Translation Error Rate (HTER) (Snover et al., 2006).4 Participants are not allowed to use the gold qualit"
S17-2001,P16-1089,0,0.0231566,"Missing"
S17-2001,S17-2025,0,0.0295389,"Missing"
S17-2001,H92-1116,0,0.16712,"Missing"
S17-2001,W16-1609,0,0.0245746,"Arabic, Spanish and Turkish. The primary evaluation criteria combines performance on all of the different language conditions except English-Turkish, which was run as a surprise language track. Even with this departure from prior years, the task attracted 31 teams producing 84 submissions. STS shared task data sets have been used extensively for research on sentence level similarity and semantic representations (i.a., Arora et al. (2017); Conneau et al. (2017); Mu et al. (2017); Pagliardini et al. (2017); Wieting and Gimpel (2017); He and Lin (2016); Hill et al. (2016); Kenter et al. (2016); Lau and Baldwin (2016); Wieting et al. (2016b,a); He et al. (2015); Pham et al. (2015)). To encourage the use of a common evaluation set for assessing new methods, we present the STS Benchmark, a publicly available selection of data from English STS shared tasks (2012-2017). 2 5 4 3 2 1 0 Table 1: Similarity scores with explanations and English examples from Agirre et al. (2013). cross-lingual tracks explores data from the WMT 2014 quality estimation task (Bojar et al., 2014).3 Sentences pairs in SNLI derive from Flickr30k image captions (Young et al., 2014) and are labeled with the entailment relations: entailment"
S17-2001,P17-2099,0,0.0191097,"Missing"
S17-2001,S17-2029,0,0.0284965,"Missing"
S17-2001,S17-2017,0,0.0940842,"jerva and Ostling, 2017) ¨ ResSim (Bjerva and Ostling, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) hjpwhu hjpwhu compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) FCICU (Hassan et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* L2F/INESC-ID (Fialho et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) MatrusriIndia NRC* NRC OkadaNaoya ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017) QLUT (Meng et al., 2017)* QLUT (Meng et al., 2017) QLUT (Meng et al., 2017)* SIGMA SIGMA SIGMA SIGMA PKU 2 SIGMA PKU 2 SIGMA PKU 2 STS-UHH (Kohail et al., 2017) UCSC-NLP UdL (Al-Natsheh et al., 2017) UdL (Al-Natsheh et al., 2017)* UdL (Al-Natsheh et al., 2017) Primary 73.16 70.44 69.40 67.89 67.03 66.62 65.98 65.90"
S17-2001,P10-1023,0,0.039078,"2017) Fourth place overall is MITRE that, like ECNU, takes an ambitious feature engineering approach complemented by deep learning. Ensembled components inˇ c clude: alignment similarity; TakeLab STS (Sari´ et al., 2012b); string similarity measures such as matching n-grams, summarization and MT metrics (BLEU, WER, PER, ROUGE); a RNN and recurrent convolutional neural networks (RCNN) over word alignments; and a BiLSTM that is state-ofthe-art for textual entailment (Chen et al., 2016). FCICU (Hassan et al., 2017) Fifth place overall is FCICU that computes a sense-base alignment using BabelNet (Navigli and Ponzetto, 2010). BabelNet synsets are multilingual allowing non-English and cross-lingual pairs to be processed similarly to English pairs. Alignment similarity scores are used with two runs: one that combines the scores within a string kernel and another that uses them with a weighted variant of Sultan et al. (2015)’s method. Both runs average the Babelnet based scores with soft-cardinality (Jimenez et al., 2012b). BIT (Wu et al., 2017) Second place overall is achieved by BIT primarily using sentence information content (IC) informed by WordNet and BNC word frequencies. One submission uses sentence IC exclu"
S17-2001,S17-2022,0,0.0368865,"Missing"
S17-2001,N18-1049,0,0.148448,"Missing"
S17-2001,S17-2014,0,0.250083,"ow and Peskov, 2017) Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* NLPProxem RTM (Bic¸ici, 2017)* UMDeep (Barrow and Peskov, 2017) RTM (Bic¸ici, 2017)* RTM (Bic¸ici, 2017)* ¨ ResSim (Bjerva and Ostling, 2017) ¨ ResSim (Bjerva and Ostling, 2017) ¨ ResSim (Bjerva and Ostling, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) hjpwhu hjpwhu compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) FCICU (Hassan et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* L2F/INESC-ID (Fialho et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) MatrusriIndia NRC* NRC OkadaNaoya ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017)"
S17-2001,P14-5010,0,0.0119011,"Missing"
S17-2001,D14-1162,0,0.109043,"Missing"
S17-2001,P15-1094,0,0.0153938,"Missing"
S17-2001,S12-1060,0,0.0229108,"Missing"
S17-2001,C16-1009,0,0.0254292,"lect methods are highlighted below. As directed by the SemEval workshop organizers, the CodaLab research platform hosts the task.11 6.4 Rankings Baseline The baseline is the cosine of binary sentence vectors with each dimension representing whether an individual word appears in a sentence.12 For crosslingual pairs, non-English sentences are translated into English using state-of-the-art machine translation.13 The baseline achieves an average correlation of 53.7 with human judgment on tracks 1-5 and would rank 23rd overall out the 44 system submissions that participated in all tracks. 14 e.g., Reimers et al. (2016) report success using STS labels with alternative metrics such as normalized Cumulative Gain (nCG), normalized Discounted Cumulative Gain (nDCG) and F1 to more accurately predict performance on the downstream tasks: text reuse detection, binary classification of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://transla"
S17-2001,S16-1091,0,0.00933941,"2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 tea"
S17-2001,Q15-1025,0,0.02311,"Missing"
S17-2001,D16-1157,0,0.0328236,"Missing"
S17-2001,P16-2068,0,0.0331584,"Missing"
S17-2001,P17-1190,0,0.0172847,"Missing"
S17-2001,S17-2007,0,0.0512581,"ormalized Discounted Cumulative Gain (nDCG) and F1 to more accurately predict performance on the downstream tasks: text reuse detection, binary classification of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://translate.google.com 5 Team ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) BIT (Wu et al., 2017)* BIT (Wu et al., 2017)* BIT (Wu et al., 2017) HCTI (Shao, 2017) MITRE (Henderson et al., 2017) MITRE (Henderson et al., 2017) FCICU (Hassan et al., 2017) neobility (Zhuang and Chang, 2017) FCICU (Hassan et al., 2017) STS-UHH (Kohail et al., 2017) RTV HCTI (Shao, 2017) RTV MatrusriIndia STS-UHH (Kohail et al., 2017) SEF@UHH (Duma and Menzel, 2017) SEF@UHH (Duma and Menzel, 2017) RTV SEF@UHH (Duma and Menzel, 2017) neobility (Zhuang and Chang, 2017) neobility (Zhuang and Chang, 2017) MatrusriIndia NLPProxem UMDeep (Barrow and Peskov, 2017) NLPProxem UMDeep (Barrow and Peskov, 2017) Lump (Espa˜n"
S17-2001,S17-2016,0,0.204045,"Missing"
S17-2001,S15-2001,0,0.0578438,"elated but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversati"
S17-2001,Q14-1006,0,0.0320307,"Lin (2016); Hill et al. (2016); Kenter et al. (2016); Lau and Baldwin (2016); Wieting et al. (2016b,a); He et al. (2015); Pham et al. (2015)). To encourage the use of a common evaluation set for assessing new methods, we present the STS Benchmark, a publicly available selection of data from English STS shared tasks (2012-2017). 2 5 4 3 2 1 0 Table 1: Similarity scores with explanations and English examples from Agirre et al. (2013). cross-lingual tracks explores data from the WMT 2014 quality estimation task (Bojar et al., 2014).3 Sentences pairs in SNLI derive from Flickr30k image captions (Young et al., 2014) and are labeled with the entailment relations: entailment, neutral, and contradiction. Drawing from SNLI allows STS models to be evaluated on the type of data used to assess textual entailment methods. However, since entailment strongly cues for semantic relatedness (Marelli et al., 2014), we construct our own sentence pairings to deter gold entailment labels from informing evaluation set STS scores. Track 4b investigates the relationship between STS and MT quality estimation by providing STS labels for WMT quality estimation data. The data includes Spanish translations of English sentences f"
S17-2001,2006.amta-papers.25,0,0.0602904,"e entailment strongly cues for semantic relatedness (Marelli et al., 2014), we construct our own sentence pairings to deter gold entailment labels from informing evaluation set STS scores. Track 4b investigates the relationship between STS and MT quality estimation by providing STS labels for WMT quality estimation data. The data includes Spanish translations of English sentences from a variety of methods including RBMT, SMT, hybrid-MT and human translation. Translations are annotated with the time required for human correction by post-editing and Human-targeted Translation Error Rate (HTER) (Snover et al., 2006).4 Participants are not allowed to use the gold quality estimation annotations to inform STS scores. Task Overview STS is the assessment of pairs of sentences according to their degree of semantic similarity. The task involves producing real-valued similarity scores for sentence pairs. Performance is measured by the Pearson correlation of machine scores with human judgments. The ordinal scale in Table 1 guides human annotation, ranging from 0 for no meaning overlap to 5 for meaning equivalence. Intermediate values reflect interpretable levels of partial overlap in meaning. The annotation scale"
S17-2001,S17-2023,0,0.0172414,"Missing"
S17-2001,S17-2018,0,0.0148193,"Missing"
S17-2001,S15-2027,0,0.00981403,"Missing"
S17-2001,S17-2028,0,0.624183,"abels with alternative metrics such as normalized Cumulative Gain (nCG), normalized Discounted Cumulative Gain (nDCG) and F1 to more accurately predict performance on the downstream tasks: text reuse detection, binary classification of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://translate.google.com 5 Team ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) BIT (Wu et al., 2017)* BIT (Wu et al., 2017)* BIT (Wu et al., 2017) HCTI (Shao, 2017) MITRE (Henderson et al., 2017) MITRE (Henderson et al., 2017) FCICU (Hassan et al., 2017) neobility (Zhuang and Chang, 2017) FCICU (Hassan et al., 2017) STS-UHH (Kohail et al., 2017) RTV HCTI (Shao, 2017) RTV MatrusriIndia STS-UHH (Kohail et al., 2017) SEF@UHH (Duma and Menzel, 2017) SEF@UHH (Duma and Menzel, 2017) RTV SEF@UHH (Duma and Menzel, 2017) neobility (Zhuang and Chang, 2017) neobility (Zhuang and Chang, 2017) MatrusriIndia NLPProxem UMDeep (Barrow a"
S17-2001,S14-2010,1,\N,Missing
S17-2001,S17-2027,0,\N,Missing
S17-2001,W17-4759,0,\N,Missing
shah-etal-2014-efficient,W12-3102,1,\N,Missing
shah-etal-2014-efficient,P11-1022,0,\N,Missing
shah-etal-2014-efficient,P10-1064,0,\N,Missing
shah-etal-2014-efficient,J03-1002,0,\N,Missing
shah-etal-2014-efficient,2009.eamt-1.5,1,\N,Missing
shah-etal-2014-efficient,P13-4014,1,\N,Missing
shah-etal-2014-efficient,2011.eamt-1.12,1,\N,Missing
shah-etal-2014-efficient,potet-etal-2012-collection,0,\N,Missing
shah-etal-2014-efficient,W13-2201,1,\N,Missing
shah-etal-2014-efficient,P10-1063,0,\N,Missing
specia-etal-2010-dataset,quirk-2004-training,0,\N,Missing
specia-etal-2010-dataset,W08-0332,0,\N,Missing
specia-etal-2010-dataset,P02-1040,0,\N,Missing
specia-etal-2010-dataset,W09-0401,0,\N,Missing
specia-etal-2010-dataset,H05-1095,1,\N,Missing
specia-etal-2010-dataset,W06-3118,0,\N,Missing
specia-etal-2010-dataset,D09-1107,0,\N,Missing
specia-etal-2010-dataset,2009.eamt-smart.4,0,\N,Missing
specia-etal-2010-dataset,W07-0734,0,\N,Missing
specia-etal-2010-dataset,C04-1072,0,\N,Missing
specia-etal-2010-dataset,C04-1046,0,\N,Missing
specia-etal-2010-dataset,W07-0718,0,\N,Missing
specia-etal-2010-dataset,2009.eamt-1.5,1,\N,Missing
specia-etal-2010-dataset,W08-0309,0,\N,Missing
specia-etal-2010-dataset,2005.mtsummit-papers.11,0,\N,Missing
specia-etal-2010-dataset,W04-3250,0,\N,Missing
W06-0508,P93-1016,0,0.0436154,"imilarity metrics (e.g., past tense). 2.1) If there is more that one possible matching, check whether any of them is a substring of the term. For example, the instance name for “Enrico Motta” is a substring of the term “Motta”, and thus it should be preferred. Figure 3. Examples of linguistic triples for the newsletter in Figure 2 Jape patterns are based on shallow syntactic information only, and therefore they are not able to capture certain potentially relevant triples. To overcome this limitation, we employ a parser as a complementary resource to produce linguistic triples. We use Minipar (Lin, 1993), which produces functional relations for the components in a sentence, including subject and object relations with respect to a verb. This allows capturing some implicit relations, such as indirect objects and long distance dependence relations. Minipar’s representation is converted into a triple format and therefore the intermediate representation provided by both GATE and Minipar consists of triples of the type: &lt;noun_phrase, verbal_expression, noun_phrase&gt;. 3.3 Identifying entities and relations For example, the similarity values returned for the term “vanessa” with instances potentially r"
W06-0508,A00-2030,0,0.0225044,"omposed by subject-verb-object (SVO) tuples. Interesting work has been done on the unsupervised automatic detection of relations from a small number of seed patterns. These are used as a starting point to bootstrap the pattern learning process, by means of semantic similarity measures (Yangarber, 2000; Stevenson, 2004). Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al, 2000), or clustering of semantically similar syntactic dependencies, according to their selectional restrictions (Gamallo et al., 2002). In corpus-based approaches, many variations are found concerning the machine learning techniques used to produce classifiers to judge relation as relevant or non-relevant. (Roth and Yih, 2002), e.g., use probabilistic classifiers with constraints induced between relations and entities, such as selectional restrictions. Based on instances represented by a pair of entities and their position in a shallow parse tree, (Zelenko et al., 2003) use support vector machines"
W06-0508,P05-3014,0,0.0161474,"tting all its synonyms in WordNet, RSS verifies that two of them match possible relations in the ontology between a person and an organization: “direct” and “lead”. In this case, the WSD module disambiguates the sense of “head” as “direct”. 3.5 3.6 Disambiguating relations The ambiguity arising when more than one possible relation exists for a pair of entities is a problem neglected in most of the current work on relation extraction. In our architecture, when the RSS finds more than one possible relation, we choose one relation by using the word sense disambiguation (WSD) system SenseLearner (Mihalcea and Csomai, 2005). SenseLearner is supervised WSD system to disambiguate all open class words in any given text, after being trained on a small data set, according to global models for word categories. The current distribution includes two default models for verbs, which were trained on a corpus containing 200,000 content words of journalistic texts tagged with their WordNet senses. Since SenseLeaner requires a sense tagged corpus in order to be trained to specific domains and there is not such a corpus for our domain, we use one of the default training models. This is a contextual model that relies on the fir"
W06-0508,P05-1052,0,0.019869,"., 2002). In corpus-based approaches, many variations are found concerning the machine learning techniques used to produce classifiers to judge relation as relevant or non-relevant. (Roth and Yih, 2002), e.g., use probabilistic classifiers with constraints induced between relations and entities, such as selectional restrictions. Based on instances represented by a pair of entities and their position in a shallow parse tree, (Zelenko et al., 2003) use support vector machines and voted perceptron algorithms with a specialized kernel model. Also using kernel methods and support vector machines, (Zhao and Grishman, 2005) combine clues from different levels of syntactic information and applies composite kernels to integrate the individual kernels. Similarly to our proposal, the framework presented by (Iria and Ciravegna, 2005) aims at the automation of semantic annotations according to ontologies. Several supervised algorithms can be used on the training data represented through a canonical graph-based data model. The framework includes a shallow linguistic processing step, in which corpora are analyzed and a representation is produced according to the data model, and a classification step, where classifiers r"
W06-0508,C02-1151,0,\N,Missing
W06-0508,A00-1039,0,\N,Missing
W06-2505,J96-2004,0,0.0638774,"Missing"
W06-2505,P05-1048,0,0.0333307,"articular applications. 1 Introduction Word Sense Disambiguation (WSD) is concerned with the choice of the most appropriate sense of an ambiguous word given its context. The applications for which WSD has been thought to be helpful include Information Retrieval, Information Extraction, and Machine Translation (MT) (Ide and Verónis, 1998). The usefulness of WSD for MT, particularly, has been recently subject of debate, with conflicting results. Vickrey et al. (2005), e.g., show that the inclusion of a WSD module significantly improves the performance of their statistical MT system. Conversely, Carpuat and Wu (2005) found that WSD does not yield significantly better translation quality than a statistical MT system alone. In this latter work, however, the WSD module was not specifically designed for MT: it is based on the use of monolingual methods to identify the source language senses, which are then mapped into the target language translations. In fact, although it has been agreed that WSD is more useful when it is meant for a specific application (Wilks and Stevenson, 1998; Kilgarriff, 1997; Resnik and Yarowsky, 1997), little has been done on the development of WSD modules specifically for particular"
W06-2505,P02-1033,0,0.0353418,"similar senses. Ide (1999), for example, analyzes translations of English words into four different languages, in order to check if the different senses of an English word are lexicalized by different words in all the other languages. A parallel aligned corpus is used and the translated senses are mapped into WordNet senses. She uses this information to determine a set of monolingual sense distinctions that is potentially useful for NLP applications. In subsequent work (Ide et al., 2002), seven languages and clustering techniques are employed to create sense groups based on the translations. Diab and Resnik (2002) use multilingual information to create an English sense tagged corpus to train a monolingual WSD approach. An English sense inventory and a parallel corpus automatically produced by an MT system are employed. Sentence and word alignment systems are used to assign the word correspondences between the two languages. After grouping all the words that correspond to translations of a single word in the target language, all their possible senses are considered as candidates. The sense that maximizes the semantic similarity of the word with the others in the group is chosen. Similarly, Ng et al. (20"
W06-2505,W99-0508,0,0.0249968,"They are motivated by the argument that the senses of a word should be determined based on the distinctions that are lexicalized in a second language (Resnik and Yarowsky, 1997). In general, the assumptions behind these approaches are the following: (1) If a source language word is translated differently into a second language, it might be ambiguous and the different translations can indicate the senses in the source language. 34 (2) If two distinct source language words are translated as the same word into a second language, it often indicates that the two are being used with similar senses. Ide (1999), for example, analyzes translations of English words into four different languages, in order to check if the different senses of an English word are lexicalized by different words in all the other languages. A parallel aligned corpus is used and the translated senses are mapped into WordNet senses. She uses this information to determine a set of monolingual sense distinctions that is potentially useful for NLP applications. In subsequent work (Ide et al., 2002), seven languages and clustering techniques are employed to create sense groups based on the translations. Diab and Resnik (2002) use"
W06-2505,H05-1097,0,0.108904,"at the traditional monolingual WSD strategies are not suitable for multilingual applications, we intend to motivate the development of WSD methods for particular applications. 1 Introduction Word Sense Disambiguation (WSD) is concerned with the choice of the most appropriate sense of an ambiguous word given its context. The applications for which WSD has been thought to be helpful include Information Retrieval, Information Extraction, and Machine Translation (MT) (Ide and Verónis, 1998). The usefulness of WSD for MT, particularly, has been recently subject of debate, with conflicting results. Vickrey et al. (2005), e.g., show that the inclusion of a WSD module significantly improves the performance of their statistical MT system. Conversely, Carpuat and Wu (2005) found that WSD does not yield significantly better translation quality than a statistical MT system alone. In this latter work, however, the WSD module was not specifically designed for MT: it is based on the use of monolingual methods to identify the source language senses, which are then mapped into the target language translations. In fact, although it has been agreed that WSD is more useful when it is meant for a specific application (Wilk"
W06-2505,H94-1046,0,0.051065,"tigated the ambiguity in the translation of the English verb “to have” into Hindi. 11 translation patterns were identified for the 19 senses of the verb, according to the various target syntactic structures and/or target words for the verb. They argued that differences in both these aspects do not depend only on the sense of the verb. Out of the 14 senses analyzed, six had 2-5 different translations each. Bentivogli et al. (2004) proposed an approach to create an Italian sense tagged corpus (MultiSemCor) based on the transference of the annotations from the English sense tagged corpus SemCor (Miller et al., 1994), by means of wordalignment methods. A gold standard corpus was created by manually transferring senses in SemCor to the Italian words in a translated version of that corpus. From a total of 1,054 English words, 155 annotations were considered nontransferable to their corresponding Italian words, mainly due to the lack of synonymy at the lexical level. Miháltz (2005) manually mapped senses from the English in a sense tagged corpus to Hungarian translations, in order to carry out WSD between these languages. Out of 43 ambiguous nouns, 38 had all or most of their English senses mapped into the s"
W06-2505,P03-1058,0,0.0893779,"Resnik (2002) use multilingual information to create an English sense tagged corpus to train a monolingual WSD approach. An English sense inventory and a parallel corpus automatically produced by an MT system are employed. Sentence and word alignment systems are used to assign the word correspondences between the two languages. After grouping all the words that correspond to translations of a single word in the target language, all their possible senses are considered as candidates. The sense that maximizes the semantic similarity of the word with the others in the group is chosen. Similarly, Ng et al. (2003) employ EnglishChinese parallel word aligned corpora to identify a repository of senses for English. The English word senses are manually defined, based on the WordNet senses, and then revised in the light of the Chinese translations. For example, if two occurrences of a word with two different senses in WordNet are translated into the same Chinese word, they will be considered to have the same English sense. In general, these approaches rely on the two previously mentioned assumptions about the interaction between translations and word senses. Although these assumptions can be useful when usi"
W06-2505,C04-1053,0,\N,Missing
W06-2505,W02-0808,0,\N,Missing
W09-2105,W03-1602,0,0.0579795,"potential change of meaning. The writer then chooses their preferred simplification. This system ensures accurate output, but requires human intervention at every step. Our system, on the other hand, is autonomous, even though the user is able to undo any undesirable simplification or to choose alternative simplifications. These alternative simplifications may be produced in two cases: i) to compose a new subject in simplifications involving relatives and appositions and ii) to choose among one of the coordinate or subordinate simplifications when there is ambiguity regarding to conjunctions. Inui et al. (2003) proposes a rule-based system for text simplification aimed at deaf people. The authors create readability assessments based on questionnaires answered by teachers about the deaf. With approximately one thousand manually created rules, the authors generate several paraphrases for each sentence and train a classifier to select the simpler ones. Promising results are obtained, although different types of errors on the paraphrase generation are encountered, such as problems with verb conjugation and regency. In our work we produce alternative simplifications only in the two cases explained above."
W09-2105,2005.jeptalnrecital-court.15,0,0.0397125,"on. Siddharthan's system deals with nonfinite clauses which are not handled by our system at this stage. Lal and Ruger’s (2002) created a bayesian summarizer with a built-in lexical simplification module, based on WordNet and MRC psycholinguistic database3. The system focuses on schoolchildren and provides background information about people and locations in the text, which are retrieved from databases. Our rule-based simplification system only replaces discourse markers for more common ones using lexical resources built in our project, instead of inserting additional information in the text. Max (2005, 2006) applies text simplification in the writing process by embedding an interactive text simplification system into a word processor. At the user’s request, an automatic parser analyzes an individual sentence and the system applies handcrafted rewriting rules. The resulting suggested simplifications are ranked by a score of syntactic complexity and potential change of meaning. The writer then chooses their preferred simplification. This system ensures accurate output, but requires human intervention at every step. Our system, on the other hand, is autonomous, even though the user is able to"
W10-1001,W09-2105,1,0.77245,"Missing"
W10-1001,dias-da-silva-etal-2008-automatic,0,0.0263079,"Missing"
W10-1001,N07-1058,0,0.509808,"more appropriate: those producing nominal, ordinal or interval scales of measurement, and (v) providing an application for the automatic assessment of reading difficulty. Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al., 2003). The following studies address readability assessment for specific audiences: learners of English as second language (Schwarm and Ostendorf, 2005; Heilman et al., 2007), people with intellectual disabilities (Feng et al., 2009), and people with 4 cognitive impairment caused by Alzheimer (Roark at al, 2007). Sheehan et al. (2007) focus on models for literary and expository texts, given that traditional metrics like Flesch-Kincaid Level score tend to overpredict the difficulty of literary texts and underpredict the difficulty of expository texts. Heilman et al. (2008) investigate an appropriate scale of measurement for reading difficulty – nominal, ordinal, or interval – by comparing the effectiveness of statistical models for each type of data. Petersen and O"
W10-1001,W08-0909,0,0.823408,"e. With our readability assessment tool, the author is able to automatically check the complexity/readability level of the original text, as well as modified versions of such text produced as he/she applies simplification operations offered by SIMPLIFICA, until the text reaches the expected level, adequate for the target reader. In this paper we present such readability assessment tool, developed as part of the PorSimples project, and discuss its application within the authoring tool. Different from previous work, the tool does not model text difficulty according to linear grade levels (e.g., Heilman et al., 2008), but instead maps the text into the three levels of literacy defined by INAF: rudimentary, basic or advanced. Moreover, it uses a more comprehensive set of features, different learning techniques and targets a new language and application, as we discuss in Section 4. More specifically, we address the following research questions: 1. Given some training material, is it possible to detect the complexity level of Portuguese texts, which corresponds to the different literacy levels defined by INAF? 2. What is the best way to model this problem and which features are relevant? We experiment with n"
W10-1001,W08-0911,0,0.115784,"Missing"
W10-1001,D08-1020,0,0.197,"lity Assessment Recent work on readability assessment for the English language focus on: (i) the feature set used to capture the various aspects of readability, to evaluate the contribution of lexical, syntactic, semantic and discursive features; (ii) the audience of the texts the readability measurement is intended to; (iii) the genre effects on the calculation of text difficult; (iv) the type of learning technique which is more appropriate: those producing nominal, ordinal or interval scales of measurement, and (v) providing an application for the automatic assessment of reading difficulty. Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al., 2003). The following studies address readability assessment for specific audiences: learners of English as second language (Schwarm and Ostendorf, 2005; Heilman et al., 2007), people with intellectual disabilities (Feng et al., 2009), and people with 4 cognitive impairment caused by Alzheimer (Roark at al, 2007). Sheehan et al. (2007) focus on mode"
W10-1001,W07-1001,0,0.0352639,"Missing"
W10-1001,P05-1065,0,0.40269,"f learning technique which is more appropriate: those producing nominal, ordinal or interval scales of measurement, and (v) providing an application for the automatic assessment of reading difficulty. Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al., 2003). The following studies address readability assessment for specific audiences: learners of English as second language (Schwarm and Ostendorf, 2005; Heilman et al., 2007), people with intellectual disabilities (Feng et al., 2009), and people with 4 cognitive impairment caused by Alzheimer (Roark at al, 2007). Sheehan et al. (2007) focus on models for literary and expository texts, given that traditional metrics like Flesch-Kincaid Level score tend to overpredict the difficulty of literary texts and underpredict the difficulty of expository texts. Heilman et al. (2008) investigate an appropriate scale of measurement for reading difficulty – nominal, ordinal, or interval – by comparing the effectiveness of statistical models for each type"
W10-1001,E09-1027,0,\N,Missing
W11-2112,W05-0909,0,0.381717,"urus (Lin, 1998) . In the remainder of this paper we describe some related work (Section 2), present our metric - TINE - (Section 3) and its performance compared to previous work (Section 4) as well as some further improvements. We then provide an analysis of these results and discuss the limitations of the metric (Section 5) and present conclusions and future work (Section 6). 2 Related Work A few metrics have been proposed in recent years to address the problem of measuring whether a hypothesis and a reference translation share the same meaning. The most well-know metric is probably METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). METEOR is based on a generalized concept of unigram matching between the hypothesis and the reference translation. Alignments are based on exact, stem, synonym, and paraphrase matches between words and phrases. However, the structure of the sentences is not considered. Wong and Kit (2010) measure word choice and word order by the matching of words based on surface forms, stems, senses and semantic similarity. The informativeness of matched and unmatched words is also weighted. Liu et al. (2010) propose to match bags of unigrams, bigrams and trigrams considering bo"
W11-2112,W10-1703,0,0.157528,"(vh = order,vr = book): Ah = {A0, A1, AM-LOC} Ar = {A0, A1, AM-LOC} 7. different word forms: expand the representation: HA1 = {ski, stays} and RA1 = {ski, holidays} expand to: HA1 = {{ski},{stays, remain... journey...}} RA1 = {{ski},{holidays, vacations, trips... journey...}} argument score = 0.5 8. similarly to HAM −LOC and RAM −LOC argument score = 0.72 10. A(H, R) = 0.74 2 Results We set the weights α and β by experimental testing to α = 1 and β = 0.25. The lexical component weight is prioritized because it has shown a good average Kendall’s tau correlation (0.23) on a development dataset (Callison-Burch et al., 2010). Table 1 shows the correlation of the lexical component with human judgments for a number of language pairs. Metric Lexical 6. exact matches: HA0 = {people} and RA0 = {people} argument score = 1 1+0.5+0.72 3 4 Table 1: Kendall’s tau segment-level correlation of the lexical component with human judgments 5. Ah ∩ Ar = {A0, A1, AM-LOC} 9. verb score (order, book) = for and reserve (something for someone else) in advance, etc.). Thus, a WordNet-based similarity measure would require disambiguating segments, an additional step and a possible source of errors. Second, a thresholds would need to be"
W11-2112,W05-0620,0,0.0644847,"Missing"
W11-2112,W04-3205,0,0.020109,"will penalize less the matching than POW. That is specially interesting when core arguments 118 get merged with modifiers due to bad semantic role labeling (e.g. [A0 I] [T bought] [A1 something to eat yesterday] instead of [A0 I] [T bought] [A1 something to eat] [AM-TMP yesterday]). P A(H, R) = v∈V verb score(Hv , Rv ) |Vr | (3) In the adequacy component, V is the set of verbs aligned between H and R, and |Vr |is the number of verbs in R. Hereafter the indexes h and r stand for hypothesis and reference translations, respectively. Verbs are aligned using VerbNet (Schuler, 2006) and VerbOcean (Chklovski and Pantel, 2004). A verb in the hypothesis vh is aligned to a verb in the reference vr if they are related according to the following heuristics: (i) the pair of verbs share at least one class in VerbNet; or (ii) the pair of verbs holds a relation in VerbOcean. For example, in VerbNet the verbs spook and terrify share the same class amuse-31.1, and in VerbOcean the verb dress is related to the verb wear. P verb score(Hv , Rv ) = a∈Ar ∩At arg score(Ha , Ra ) |Ar | (4) The similarity between the arguments of a verb pair (vh , vr ) in V is measured as defined in Equation (4), where Ah and At are the sets of labe"
W11-2112,W10-1751,0,0.419678,"s (core arguments and modifiers), and iii) matching predicate arguments using distributional semantics. TINE’s performance is comparable to that of previous metrics at segment level for several language pairs, with average Kendall’s tau correlation from 0.26 to 0.29. We show that the addition of the shallow-semantic component improves the performance of simple lexical matching strategies and metrics such as BLEU. 1 A number of other metrics have been proposed to address these limitations, for example, by allowing for the matching of synonyms or paraphrases of content words, such as in METEOR (Denkowski and Lavie, 2010). Other attempts have been made to capture whether the reference translation and hypothesis translations share the same meaning using shallow semantics, i.e., Semantic Role Labeling (Gim´enez and M´arquez, 2007). However, these are limited to the exact matching of semantic roles and their fillers. Introduction The automatic evaluation of Machine Translation (MT) is a long-standing problem. A number of metrics have been proposed in the last two decades, mostly measuring some form of matching between the MT output (hypothesis) and one or more human (reference) translations. However, most of thes"
W11-2112,W07-0738,0,0.211147,"Missing"
W11-2112,W10-1750,0,0.0470798,"Missing"
W11-2112,W10-1753,0,0.0330133,"Missing"
W11-2112,P98-2127,0,0.163445,"the semantic structure of the sentence and the content of the semantic elements. The metric uses SRLs such as in (Gim´enez and M´arquez, 2007). However, it analyses the content of predicates and arguments seeking for either exact or “similar” matches. The 116 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 116–122, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics inexact matching is based on the use of ontologies such as VerbNet (Schuler, 2006) and distributional semantics similarity metrics, such as Dekang Lin’s thesaurus (Lin, 1998) . In the remainder of this paper we describe some related work (Section 2), present our metric - TINE - (Section 3) and its performance compared to previous work (Section 4) as well as some further improvements. We then provide an analysis of these results and discuss the limitations of the metric (Section 5) and present conclusions and future work (Section 6). 2 Related Work A few metrics have been proposed in recent years to address the problem of measuring whether a hypothesis and a reference translation share the same meaning. The most well-know metric is probably METEOR (Banerjee and Lav"
W11-2112,W10-1754,0,0.0159783,"the same meaning. The most well-know metric is probably METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). METEOR is based on a generalized concept of unigram matching between the hypothesis and the reference translation. Alignments are based on exact, stem, synonym, and paraphrase matches between words and phrases. However, the structure of the sentences is not considered. Wong and Kit (2010) measure word choice and word order by the matching of words based on surface forms, stems, senses and semantic similarity. The informativeness of matched and unmatched words is also weighted. Liu et al. (2010) propose to match bags of unigrams, bigrams and trigrams considering both recall and precision and F-measure giving more importance to recall, but also using WordNet synonyms. Tratz and Hovy (2008) use transformations in order to match short syntactic units defined as Basic Elements (BE). The BE are minimal-length syntactically well defined units. For example, nouns, verbs, adjectives and adverbs can be considered BE-Unigrams, while a BE-Bigram could be formed from a syntactic relation (e.g. subject+verb, verb+object). BEs can be lexically different, but semantically similar. Pad´o et al. (200"
W11-2112,N06-1006,0,0.0829905,"Missing"
W11-2112,P02-1040,0,0.0817216,"Missing"
W11-2112,W10-1755,0,0.0150449,"5) and present conclusions and future work (Section 6). 2 Related Work A few metrics have been proposed in recent years to address the problem of measuring whether a hypothesis and a reference translation share the same meaning. The most well-know metric is probably METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). METEOR is based on a generalized concept of unigram matching between the hypothesis and the reference translation. Alignments are based on exact, stem, synonym, and paraphrase matches between words and phrases. However, the structure of the sentences is not considered. Wong and Kit (2010) measure word choice and word order by the matching of words based on surface forms, stems, senses and semantic similarity. The informativeness of matched and unmatched words is also weighted. Liu et al. (2010) propose to match bags of unigrams, bigrams and trigrams considering both recall and precision and F-measure giving more importance to recall, but also using WordNet synonyms. Tratz and Hovy (2008) use transformations in order to match short syntactic units defined as Basic Elements (BE). The BE are minimal-length syntactically well defined units. For example, nouns, verbs, adjectives an"
W11-2112,C98-2122,0,\N,Missing
W11-2136,P02-1040,0,0.0802485,"Missing"
W11-2136,W05-0908,0,0.073685,"Missing"
W11-2136,E09-3008,0,0.015054,"ent corpus. The parallel corpus was then tokenized and truecased. Additionally, for en-de, compound splitting of the German side of the corpus was performed using a frequency based method described in (Koehn and Knight, 2003). This method helps alleviate sparsity, reducing the size of the vocabulary by decomposing compounds into their base words. Recasing and detokenization, along with compound merging of the translations into German, were handled at post-processing stage. Compound merging was performed by finding the most likely sequences of words to be merged into previously seen compounds (Stymne, 2009). 3.1 Source Language Annotation For rule extraction, training and test, the English side of the corpus was annotated with Semantic Role Labels (SRL) using the toolkit SENNA2 , which also outputs POS and base-phrase (without prepositional attachment) tags. The resulting source language annotation was used to produce trees in order to build a tree-to-string model in Moses. 1 http://www.speech.sri.com/projects/ srilm/ 2 http://ml.nec-labs.com/senna/ S NP PRP he VBZ intends VP TO to NP VB donate DT this NN money PP TO to NP NN charity O PUNC , O CC but NP PRP he VBZ has VP RB VBD not decided NP W"
W11-2136,W06-3119,0,0.488725,"cialized vocabulary also leads to spurious ambiguity (Chiang, 2005). Syntax-based models are hierarchical models whose rules are constrained by syntactic information.The syntactic constraints have an impact in the rule extraction process, reducing drastically the number of rules available to the system. While this may be helpful to reduce ambiguity, it can lead to poorer performance (Ambati and Lavie, 2008). Motivated by the fact that syntactically constraining a hierarchical model can decrease translation quality, some attempts to overcome the problems at rule extraction time have been made. Venugopal and Zollmann (2006) propose a heuristic method to relax parse trees known as Syntax Augmented Machine Translation (SAMT). Significant gains are obtained by grouping nonterminals under categories when they do not span across syntactic constituents. Hoang and Koehn (2010) propose a soft syntaxbased model which combines the precision of a syntax-constrained model with the coverage of an 316 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 316–322, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics unconstrained hierarchical model. Instead of having h"
W11-2136,N09-2004,0,0.0563643,"nd, UK, July 30–31, 2011. 2011 Association for Computational Linguistics unconstrained hierarchical model. Instead of having heuristic strategies to combine nonterminals in a parse tree, whenever a rule cannot be retrieved because it does not span a constituent, the extraction procedure falls back to the hierarchical approach, retrieving a rule with unlabeled nonterminals. Performance gains are reported over standard hierarchical models using both full parse trees and shallow syntax. Moving beyond syntactic information, some attempts have recently been made to add semantic annotations to SMT. Wu and Fung (2009) present a two-pass model to incorporate semantic information to the phrase-based SMT pipeline. The method performs conventional translation in a first step, followed by a constituent reordering step seeking to maximize the cross-lingual match of the semantic role labels of the translation and source sentences. Liu and Gildea (2010) add features extracted from the source sentences annotated with semantic role labels in a tree-to-string SMT model. They modify a syntax-based SMT system in order to penalize/reward role reordering and role deletion. The input sentence is parsed for semantic roles"
W11-2136,C10-1081,0,\N,Missing
W11-2136,W10-1715,0,\N,Missing
W11-2136,2010.amta-papers.7,0,\N,Missing
W11-2136,W10-1761,0,\N,Missing
W11-2136,E03-1076,0,\N,Missing
W11-2136,P07-2045,0,\N,Missing
W11-2136,W07-0733,0,\N,Missing
W11-2136,P05-1033,0,\N,Missing
W11-2136,2009.iwslt-papers.4,0,\N,Missing
W11-2315,2009.mtsummit-btm.1,0,\N,Missing
W11-2315,W10-1607,1,\N,Missing
W11-2315,W03-1602,0,\N,Missing
W11-2315,P08-1040,0,\N,Missing
W11-2315,W10-0406,0,\N,Missing
W11-2315,daelemans-etal-2004-automatic,0,\N,Missing
W11-2315,E99-1042,0,\N,Missing
W11-4533,J93-2003,0,0.0169584,"wercase and tokenize them; 2. For every pt document we replace the original words by their n-best translations according to our bilingual dictionary; 3. We represent documents as distributional profiles (DP), that is, a vector that contains a word w and its frequency f in the document: profile : w 7→ f ; 4. For every possible document pair we compute the cosine similarity of their DPs. 5. We align each en document to its best pt candidate if the similarity score is above a given threshold. We compiled bilingual dictionaries using the small corpus Fapesp-v1 running 5 iterations of IBM Model 1 [Brown et al. 1993]. IBM Model 1 estimates a word-based translation probability distribution from a sentence aligned corpus. We set the number of best translations from the dictionary to 3 and the similarity metric threshold to 0.15 after a short round of evaluation described in Section 3. The same procedure was used to align pt-es articles. We obtained 2, 675 pt-en and 2, 668 pt-es parallel texts. 2.3. Sentence Alignment We use an implementation of TCA specially written for aligning Brazilian Portuguese to a foreign text [Caseli and Nunes 2003]. While the most desired alignment type is the substitution (1-1),"
W11-4533,P05-1033,0,0.042189,"n automatically aligned at document- and sentence-level. The resulting corpora contain about 2,700 parallel documents totaling over 150,000 aligned sentences each. The quality of the corpora and their usefulness are tested in an experiment with machine translation. 1. Introduction Parallel corpora are collections of texts that are mutual translations. Machine Translation (MT) systems have recently become very popular due to the latest advances in the field of Statistical Machine Translation (SMT). The most advanced SMT approaches, such as phrase-based [Koehn et al. 2003] and hierarchical SMT [Chiang 2005], learn how to perform translation from parallel corpora. A popular way of building a parallel corpus is collecting large amounts of data from the web. The alignment between documents, sentences and words allows the development of resources such as dictionaries and SMT systems. [Espl`a-Gomis and Forcada 2010] present Bitextor, a language-independent tool to harvest parallel documents from multilingual web sites. It uses features from the HTML structure of the documents and their URLs to perform document alignment. [Aziz et al. 2008] describe the creation of a Brazilian Portuguese-Spanish (pt-"
W11-4533,W08-0318,0,0.0169663,"language, we split the corpus in one training, two developments and two test sets as shown in Table 1 (the last column points to the HTML documents in the website). 236 Table 1. Splitting of the Fapesp-v2 for training and test purposes Sentences Issues HTML pointer Set pt-en pt-es training 150,968 146,755 April, 2005 - March, 2011 19 - 945 dev 1,375 1,302 June, 2003 118 1,608 1,601 June, 2010 934 dev-test test-a 1,314 1,201 July, 2003 119 1,447 1,379 July, 2010 935 test-b We trained a PBSMT model using the Moses toolkit [Koehn et al. 2007] following its “baseline” settings and truecased data [Koehn et al. 2008]. For comparison purposes, we trained a model using the old version of the training set (18, 232 sentence pairs): pt-en Fapesp-v1 (UoW-fapesp-v1), and another using the new version: pt-en Fapesp-v2 (UoW-fapesp-v2). In both experiments we used the Fapesp-v2 dev for tuning the features of the PBSMT model. The systems were tested using 3 test sets: i) Fapesp-v1 test (667 sentences), ii) Fapesp-v2 test-a, and iii) Fapesp-v2 test-b. We assessed the system’s performances in terms of BLEU [Papineni et al. 2002] and its case-sensitive version BLEU-c. We also compared the translations against those pr"
W11-4533,P07-2045,0,0.00951855,"T) systems and evaluated their quality using automatic metrics. For each language, we split the corpus in one training, two developments and two test sets as shown in Table 1 (the last column points to the HTML documents in the website). 236 Table 1. Splitting of the Fapesp-v2 for training and test purposes Sentences Issues HTML pointer Set pt-en pt-es training 150,968 146,755 April, 2005 - March, 2011 19 - 945 dev 1,375 1,302 June, 2003 118 1,608 1,601 June, 2010 934 dev-test test-a 1,314 1,201 July, 2003 119 1,447 1,379 July, 2010 935 test-b We trained a PBSMT model using the Moses toolkit [Koehn et al. 2007] following its “baseline” settings and truecased data [Koehn et al. 2008]. For comparison purposes, we trained a model using the old version of the training set (18, 232 sentence pairs): pt-en Fapesp-v1 (UoW-fapesp-v1), and another using the new version: pt-en Fapesp-v2 (UoW-fapesp-v2). In both experiments we used the Fapesp-v2 dev for tuning the features of the PBSMT model. The systems were tested using 3 test sets: i) Fapesp-v1 test (667 sentences), ii) Fapesp-v2 test-a, and iii) Fapesp-v2 test-b. We assessed the system’s performances in terms of BLEU [Papineni et al. 2002] and its case-sen"
W11-4533,N03-1017,0,0.00949287,"ual Brazilian magazine. The texts are then automatically aligned at document- and sentence-level. The resulting corpora contain about 2,700 parallel documents totaling over 150,000 aligned sentences each. The quality of the corpora and their usefulness are tested in an experiment with machine translation. 1. Introduction Parallel corpora are collections of texts that are mutual translations. Machine Translation (MT) systems have recently become very popular due to the latest advances in the field of Statistical Machine Translation (SMT). The most advanced SMT approaches, such as phrase-based [Koehn et al. 2003] and hierarchical SMT [Chiang 2005], learn how to perform translation from parallel corpora. A popular way of building a parallel corpus is collecting large amounts of data from the web. The alignment between documents, sentences and words allows the development of resources such as dictionaries and SMT systems. [Espl`a-Gomis and Forcada 2010] present Bitextor, a language-independent tool to harvest parallel documents from multilingual web sites. It uses features from the HTML structure of the documents and their URLs to perform document alignment. [Aziz et al. 2008] describe the creation of"
W11-4533,P02-1040,0,0.0880041,"g the Moses toolkit [Koehn et al. 2007] following its “baseline” settings and truecased data [Koehn et al. 2008]. For comparison purposes, we trained a model using the old version of the training set (18, 232 sentence pairs): pt-en Fapesp-v1 (UoW-fapesp-v1), and another using the new version: pt-en Fapesp-v2 (UoW-fapesp-v2). In both experiments we used the Fapesp-v2 dev for tuning the features of the PBSMT model. The systems were tested using 3 test sets: i) Fapesp-v1 test (667 sentences), ii) Fapesp-v2 test-a, and iii) Fapesp-v2 test-b. We assessed the system’s performances in terms of BLEU [Papineni et al. 2002] and its case-sensitive version BLEU-c. We also compared the translations against those produced by Google Translate, an off-the-shelf, out-of-domain PBSMT system. Table 2 shows that the model trained using our automatically compiled corpus significantly outperforms both Google Translate and a model trained on the previous, smaller version of the corpus, UoW-fapesp-v1, in all test sets. PBSMT system Test set fapesp-v1 fapesp-v2-a fapesp-v2-b Table 2. Performance on the test sets UoW-fapesp-v1 UoW-fapesp-v2 Google BLEU BLEU-c BLEU BLEU-c BLEU BLEU-c 39.99 37.94 57.09 54.25 37.62 36.97 43.24 40"
W12-3102,W10-1703,1,0.557844,"Missing"
W12-3102,W11-2103,1,0.709065,"- ANNOTATOR AGREEMENT P (A) 0.567 0.576 0.595 0.598 0.540 0.504 0.568 0.519 0.568 0.601 P (A) 0.660 0.566 0.733 0.732 0.792 0.566 0.719 0.634 0.671 0.722 P (E) 0.405 0.383 0.401 0.394 0.408 0.398 0.406 0.388 0.396 0.362 κ 0.272 0.312 0.323 0.336 0.222 0.176 0.272 0.214 0.284 0.375 P (E) 0.405 0.383 0.401 0.394 0.408 0.398 0.406 0.388 0.396 0.362 κ 0.428 0.296 0.554 0.557 0.648 0.279 0.526 0.401 0.455 0.564 Table 3: Inter- and intra-annotator agreement rates for the WMT12 manual evaluation. For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al. (2011), Table 7). Agreement rates vary widely across languages. For inter-annotator agreements, the range is 0.176 to 0.336, while intra-annotator agreement ranges from 0.279 to 0.648. We note in particular the low agreement rates among judgments in the English-Spanish task, which is reflected in the relative lack of statistical significance Table 4. The agreement rates for this year were somewhat lower than last year. 3.3 Results of the Translation Task We used the results of the manual evaluation to analyze the translation quality of the different systems that were submitted to the workshop. In ou"
W12-3102,D09-1030,1,0.149941,"Missing"
W12-3102,W12-3103,0,0.0434403,"Missing"
W12-3102,W11-2107,0,0.311039,"Missing"
W12-3102,W12-3131,0,0.0259292,"Missing"
W12-3102,W12-3133,0,0.0166865,"Missing"
W12-3102,W12-3134,1,0.0973948,"Missing"
W12-3102,W12-3135,0,0.0202653,"Missing"
W12-3102,W12-3111,0,0.0311487,"Missing"
W12-3102,W12-3136,0,0.0403021,"Missing"
W12-3102,W12-3112,0,0.176358,"Missing"
W12-3102,2011.eamt-1.32,0,0.0209858,"with only two linear equations. System “SDLLW SVM” uses a 20-feature set and an SVM epsilon regression model with radial basis function kernel with parameters C, gamma, and epsilon tuned on a development set (305 training instances). The model was trained with 10-fold cross validation and the tuning process was restarted several times using different starting points and step sizes to avoid overfitting. The final model was selected based on its performance on the development set and the number of support vectors. UU (R, S): System “UU best” uses the 17 baseline features, plus 82 features from Hardmeier (2011) (with some redundancy and some overlap with baseline features), and constituency trees over input sentences generated by the Stanford parser and dependency trees over both input and output sentences generated by the MaltParser. System “UU bltk” uses only the 17 baseline features plus constituency and dependency trees as above. The machine learning component in both cases is SVM regression (SVMlight software). For the ranking task, 29 the ranking induced by the regression output is used. The system uses polynomial kernels of degree 2 (UU best) and 3 (UU bltk) as well as two different types of"
W12-3102,W12-3137,0,0.0533752,"Missing"
W12-3102,W12-3106,0,0.0413119,"Missing"
W12-3102,W12-3145,0,0.0522389,"Missing"
W12-3102,W12-3146,0,0.0998404,"Missing"
W12-3102,W09-0415,0,0.0172168,"Missing"
W12-3102,W12-3150,1,0.188192,"Missing"
W12-3102,W12-3119,0,0.030516,"Missing"
W12-3102,W12-3151,0,0.046244,"Missing"
W12-3102,W12-3117,0,\N,Missing
W12-3102,W12-3138,0,\N,Missing
W12-3102,W12-3130,0,\N,Missing
W12-3102,W12-3148,0,\N,Missing
W12-3102,W12-3114,0,\N,Missing
W12-3102,W12-3107,0,\N,Missing
W12-3102,W12-3139,1,\N,Missing
W12-3102,W11-2108,0,\N,Missing
W12-3102,W09-0401,1,\N,Missing
W12-3102,W12-3140,0,\N,Missing
W12-3102,W12-3132,0,\N,Missing
W12-3102,W12-3109,0,\N,Missing
W12-3102,W10-1711,0,\N,Missing
W12-3102,2010.iwslt-evaluation.22,0,\N,Missing
W12-3102,W07-0718,1,\N,Missing
W12-3102,W06-3114,1,\N,Missing
W12-3102,2009.eamt-1.5,1,\N,Missing
W12-3102,W12-3144,0,\N,Missing
W12-3102,W11-2158,0,\N,Missing
W12-3102,W12-3110,1,\N,Missing
W12-3102,W08-0309,1,\N,Missing
W12-3102,W12-3105,0,\N,Missing
W12-3102,W12-3149,0,\N,Missing
W12-3102,2011.eamt-1.12,1,\N,Missing
W12-3102,W04-3250,1,\N,Missing
W12-3102,W12-3142,0,\N,Missing
W12-3102,W11-2113,0,\N,Missing
W12-3102,W12-3143,0,\N,Missing
W12-3102,W11-2145,0,\N,Missing
W12-3102,W12-3147,0,\N,Missing
W12-3102,W12-3101,0,\N,Missing
W12-3102,W12-3115,0,\N,Missing
W12-3102,W11-2101,0,\N,Missing
W12-3102,W12-3113,0,\N,Missing
W12-3102,W12-3104,0,\N,Missing
W12-3102,W12-3108,0,\N,Missing
W12-3102,W12-3118,1,\N,Missing
W12-3110,P11-1022,0,0.283211,"Missing"
W12-3110,C04-1046,0,0.629683,"usions and a brief discussion of future work. 2 al. (2011) focused on learning patterns of linguistic information (such as sequences of part-of-speech tags) to predict sub-sentence errors. Finally, Pighin and M`arquez (2011) modelled the expected projections of semantic roles from the input text into the translations. Related Work 3 Reference-free MT quality assessment was initially approached as a Confidence Estimation task, strongly biased towards exploiting data from a Statistical MT (SMT) system and the translation process to model the confidence of the system in the produced translation. Blatz et al. (2004) attempted sentence-level assessment using a set of 91 features (from the SMT system input and translation texts) and automatic annotations such as NIST and WER. Experiments on classification and regression using different machine learning techniques produced not very encouraging results. More successful experiments were later run by Quirk (2004) in a similar setting but using a smaller dataset with human quality judgments. Specia et al. (2009a) used Partial Least Squares regression to jointly address feature selection and model learning using a similar set of features and datasets annotated w"
W12-3110,2011.eamt-1.32,0,0.342254,"e as glass-box features (i.e. those from the MT system). Later work using black-box features only focused on finding an appropriate threshold for discriminating ‘good’ from ‘bad’ translations for postediting purposes (Specia et al., 2009b) and investigating more objective ways of obtaining human annotation, such as post-editing time (Specia, 2011). Recent approaches have started exploiting linguistic information with promising results. Specia et al. (2011), for instance, used part-of-speech (PoS) tagging, chunking, dependency relations and named entities for English-Arabic quality estimation. Hardmeier (2011) explored the use of constituency and dependency trees for English-Swedish/Spanish quality estimation. Focusing on word-error detection through the estimation of WER, Xiong et al. (2010) used PoS tags of neighbouring words and a link grammar parser to detect words that are not connected to the rest of the sentence. Work by Bach et 97 Method Our work focuses on the use of a wide range of linguistic information for representing different aspects of translation quality to complement shallow, system-independent features that have been proved to perform well in previous work. 3.1 Linguistic feature"
W12-3110,P07-2045,0,0.00316133,"address the QE problem as a regression task by building SVM models with an epsilon regressor and a radial basis function kernel using the LibSVM toolkit (Chang and Lin, 2011). Values for the cost, epsilon and gamma parameters were optimized using 5-fold cross validation on the training set. 99 Baseline WLV-SHEF FS WLV-SHEF BL MAE ↓ 0.69 0.69 0.72 RMSE ↓ 0.82 0.85 0.86 Pearson ↑ 0.562 0.514 0.490 Table 1: Scoring performance The training sets distributed for the shared task comprised 1, 832 English sentences taken from news texts and their Spanish translations produced by an SMT system, Moses (Koehn et al., 2007), which had been trained on a concatenation of Europarl and news-commentaries data (from WMT-10). Translations were accompanied by a quality score derived from an average of three human judgments of postediting effort using a 1-5 scale. The models built for each of these two feature sets were evaluated using the official test set of 422 sentences produced in the same fashion as the training set. Two sub-tasks were considered: (i) scoring translations using the 1-5 quality scores, and (ii) ranking translations from best to worse. While quality scores were directly predicted by our models, sente"
W12-3110,padro-etal-2010-freeling,0,0.0164232,"Missing"
W12-3110,P02-1040,0,0.0986459,"o be carefully combined with other features in order to produce better results. 1 Introduction Estimating the quality of automatic translations is becoming a subject of increasing interest within the Machine Translation (MT) community for a number of reasons, such as helping human translators post-editing MT, warning users about non-reliable translations or combining output from multiple MT systems. Different from most classic approaches for measuring the progress of an MT system or comparing MT systems, which assess quality by contrasting system output to reference translations such as BLEU (Papineni et al., 2002), Quality Estimation (QE) is a more challenging task, aimed at MT systems in use, and therefore without access to reference translations. We test this hypothesis as part of the WMT-12 shared task on quality estimation. The system submitted to this task (WLV-SHEF) integrates linguistic information to a strong baseline system using only shallow statistics from the input and translation texts, with no explicit information from the MT system that produced the translations. A variant also tests the addition of linguistic information to a larger set of shallow features. The quality estimation proble"
W12-3110,W11-1001,0,0.330465,"Missing"
W12-3110,quirk-2004-training,0,0.137555,"ality assessment was initially approached as a Confidence Estimation task, strongly biased towards exploiting data from a Statistical MT (SMT) system and the translation process to model the confidence of the system in the produced translation. Blatz et al. (2004) attempted sentence-level assessment using a set of 91 features (from the SMT system input and translation texts) and automatic annotations such as NIST and WER. Experiments on classification and regression using different machine learning techniques produced not very encouraging results. More successful experiments were later run by Quirk (2004) in a similar setting but using a smaller dataset with human quality judgments. Specia et al. (2009a) used Partial Least Squares regression to jointly address feature selection and model learning using a similar set of features and datasets annotated with both automatic and human scores. Black-box features (i.e. those extracted from the input and translation texts only) were as discriminative as glass-box features (i.e. those from the MT system). Later work using black-box features only focused on finding an appropriate threshold for discriminating ‘good’ from ‘bad’ translations for posteditin"
W12-3110,2009.eamt-1.5,1,0.57631,"owards exploiting data from a Statistical MT (SMT) system and the translation process to model the confidence of the system in the produced translation. Blatz et al. (2004) attempted sentence-level assessment using a set of 91 features (from the SMT system input and translation texts) and automatic annotations such as NIST and WER. Experiments on classification and regression using different machine learning techniques produced not very encouraging results. More successful experiments were later run by Quirk (2004) in a similar setting but using a smaller dataset with human quality judgments. Specia et al. (2009a) used Partial Least Squares regression to jointly address feature selection and model learning using a similar set of features and datasets annotated with both automatic and human scores. Black-box features (i.e. those extracted from the input and translation texts only) were as discriminative as glass-box features (i.e. those from the MT system). Later work using black-box features only focused on finding an appropriate threshold for discriminating ‘good’ from ‘bad’ translations for postediting purposes (Specia et al., 2009b) and investigating more objective ways of obtaining human annotati"
W12-3110,2009.mtsummit-papers.16,1,0.805574,"owards exploiting data from a Statistical MT (SMT) system and the translation process to model the confidence of the system in the produced translation. Blatz et al. (2004) attempted sentence-level assessment using a set of 91 features (from the SMT system input and translation texts) and automatic annotations such as NIST and WER. Experiments on classification and regression using different machine learning techniques produced not very encouraging results. More successful experiments were later run by Quirk (2004) in a similar setting but using a smaller dataset with human quality judgments. Specia et al. (2009a) used Partial Least Squares regression to jointly address feature selection and model learning using a similar set of features and datasets annotated with both automatic and human scores. Black-box features (i.e. those extracted from the input and translation texts only) were as discriminative as glass-box features (i.e. those from the MT system). Later work using black-box features only focused on finding an appropriate threshold for discriminating ‘good’ from ‘bad’ translations for postediting purposes (Specia et al., 2009b) and investigating more objective ways of obtaining human annotati"
W12-3110,2011.mtsummit-papers.58,1,0.830508,"tasets annotated with both automatic and human scores. Black-box features (i.e. those extracted from the input and translation texts only) were as discriminative as glass-box features (i.e. those from the MT system). Later work using black-box features only focused on finding an appropriate threshold for discriminating ‘good’ from ‘bad’ translations for postediting purposes (Specia et al., 2009b) and investigating more objective ways of obtaining human annotation, such as post-editing time (Specia, 2011). Recent approaches have started exploiting linguistic information with promising results. Specia et al. (2011), for instance, used part-of-speech (PoS) tagging, chunking, dependency relations and named entities for English-Arabic quality estimation. Hardmeier (2011) explored the use of constituency and dependency trees for English-Swedish/Spanish quality estimation. Focusing on word-error detection through the estimation of WER, Xiong et al. (2010) used PoS tags of neighbouring words and a link grammar parser to detect words that are not connected to the rest of the sentence. Work by Bach et 97 Method Our work focuses on the use of a wide range of linguistic information for representing different aspe"
W12-3110,2011.eamt-1.12,1,0.751728,"hypothesis as part of the WMT-12 shared task on quality estimation. The system submitted to this task (WLV-SHEF) integrates linguistic information to a strong baseline system using only shallow statistics from the input and translation texts, with no explicit information from the MT system that produced the translations. A variant also tests the addition of linguistic information to a larger set of shallow features. The quality estimation problem is modelled as a supervised regression task using Support Vector Machines (SVM), which has been shown to achieve good performance in previous work (Specia, 2011). Linguistic features are computed using a number of auxiliary resources such as parsers and monolingual corpora. The remainder of this paper is organised as follows. Section 2 gives an overview of previous work 96 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 96–103, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics on quality estimation, Section 3 describes the set of linguistic features proposed in this paper, along with general experimental settings, Section 4 presents our evaluation and Section 5 provides conclusions and a brie"
W12-3110,taule-etal-2008-ancora,0,0.013412,"Missing"
W12-3110,P10-1062,0,0.346038,"slations for postediting purposes (Specia et al., 2009b) and investigating more objective ways of obtaining human annotation, such as post-editing time (Specia, 2011). Recent approaches have started exploiting linguistic information with promising results. Specia et al. (2011), for instance, used part-of-speech (PoS) tagging, chunking, dependency relations and named entities for English-Arabic quality estimation. Hardmeier (2011) explored the use of constituency and dependency trees for English-Swedish/Spanish quality estimation. Focusing on word-error detection through the estimation of WER, Xiong et al. (2010) used PoS tags of neighbouring words and a link grammar parser to detect words that are not connected to the rest of the sentence. Work by Bach et 97 Method Our work focuses on the use of a wide range of linguistic information for representing different aspects of translation quality to complement shallow, system-independent features that have been proved to perform well in previous work. 3.1 Linguistic features Non-linguistic features, such as sentence length or n-gram statistics, are limited in their scope since they can only account for very shallow aspects of a translation. They convey no"
W13-2201,W13-2205,0,0.0583032,"Missing"
W13-2201,S13-1034,0,0.0277746,"Missing"
W13-2201,C12-1008,0,0.0091484,"ignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011), and pseudo-reference METEOR score; the most successful set, Feature Set 33 combines those 24 features with the 17 baseline features. For English-Spanish, LogReg was used with L2 Regularisation (Lin et al., 2007) and"
W13-2201,W13-2206,0,0.0913052,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2240,0,0.0186069,"Missing"
W13-2201,W13-2242,0,0.220429,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2207,0,0.0357036,"Missing"
W13-2201,W11-2104,0,0.00810596,"Missing"
W13-2201,P10-2016,1,0.826191,"Missing"
W13-2201,W13-2208,1,0.743011,"Missing"
W13-2201,P11-1022,0,0.112625,"the intended application. In Task 2 we tested binary word-level classification in a post-editing setting. If such annotation is presented through a user interface we imagine that words marked as incorrect would be hidden from the editor, highlighted as possibly wrong or that a list of alternatives would we generated. With respect to the poor improvements over trivial baselines, we consider that the results for word-level prediction could be mostly connected to limitations of the datasets provided, which are very small for word-level prediction, as compared to successful previous work such as (Bach et al., 2011). Despite the limited amount of training data, several systems were able to predict dubious words (binary variant of the task), showing that this can be a promising task. Extending the granularity even further by predicting the actual editing action necessary for a word yielded less positive results than the binary setting. We cannot directly compare sentence- and word-level results. However, since sentence-level predictions can benefit from more information available and therefore more signal on which the prediction is based, the natural conclusion is that, if there is a choice in the predict"
W13-2201,W13-2209,0,0.034542,"Missing"
W13-2201,W13-2241,1,0.0862471,"Missing"
W13-2201,W07-0718,1,0.697635,"to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated both automatically and manually."
W13-2201,W11-2103,1,0.5768,"Missing"
W13-2201,W13-2243,1,0.754887,"Missing"
W13-2201,W13-2213,0,0.0301468,"Missing"
W13-2201,P05-1022,0,0.0401846,"parses of the source and target sentence, the positions of the phrases with the lowest and highest probability and future cost estimate in the translation, the counts of phrases in the decoding graph whose probability or whether the future cost estimate is higher/lower than their standard deviation, counts of verbs and determiners, etc. The second submission (pls8) was trained with Partial Least Squares regression (Stone and Brooks, 1990) including more glass-box features. The following NLP tools were used in feature extraction: the Brown English Wall-StreetJournal-trained statistical parser (Charniak and Johnson, 2005), a Lexical Functional Grammar parser (XLE), together with a hand-crafted Lexical Functional Grammar, the English ParGram grammar (Kaplan et al., 2004), and the TreeTagger part-of-speech tagger (Schmidt, 1994) with off-the-shelf publicly available pre-trained tagging models for English and Spanish. For pseudoreference features, the Bing, Moses and Systran translation systems were used. The Mallet toolkit (McCallum, 2002) was used to build the topic models and features based on a grammar checker were extracted with LanguageTool.16 FBK-Uedin (T1.1, T1.3): The submissions explored features built"
W13-2201,W13-2210,0,0.0366902,"Missing"
W13-2201,W13-2212,1,0.1814,"Missing"
W13-2201,W13-2214,0,0.0302242,"Missing"
W13-2201,W13-2217,0,0.0313645,"Missing"
W13-2201,W13-2215,0,0.0174795,"Missing"
W13-2201,W13-2253,0,0.055006,"Missing"
W13-2201,W13-2246,0,0.0627442,"Missing"
W13-2201,N06-1058,0,0.0556187,"of the official test set size, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run ma"
W13-2201,W13-2248,0,0.0488662,"Missing"
W13-2201,2012.iwslt-papers.5,1,0.674636,"ts are somewhat artificially more diverse; in narrow domains, source sentences can repeat and even appear verbatim in the training data, and in natural test sets with multiple references, short sentences can receive several identical translations. For each probe, we measure the Spearman’s rank correlation coefficient ρ of the ranks proposed by BLEU or NIST and the manual ranks. We use the same implementation as applied in the WMT13 Shared Metrics Task (Mach´acˇ ek and Bojar, 2013). Note that the WMT13 metrics task still uses the WMT12 evaluation method ignoring ties, not the expected wins. As Koehn (2012) shows, the two methods do not differ much. Overall, the correlation is strongly impacted by Figure 5: Correlation of BLEU and WMT13 manual ranks for English→Czech translation Figure 6: Correlation of NIST and WMT13 manual ranks for English→Czech translation the particular choice of test sentences and reference translations. By picking sentences randomly, similarly or equally sized test sets can reach different correlations. Indeed, e.g. for a test set of about 1500 distinct sentences selected from the 3000-sentence official test set (1 reference translation), we obtain correlations for BLEU b"
W13-2201,W13-2202,1,0.761984,"Missing"
W13-2201,W06-3114,1,0.635019,"ntence, ranking of up to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated bot"
W13-2201,W13-2219,0,0.0360343,"Missing"
W13-2201,W13-2220,0,0.017834,"Missing"
W13-2201,W13-2255,0,0.0136612,"Missing"
W13-2201,W12-3113,0,0.012942,"Missing"
W13-2201,W13-2247,0,0.0208658,"Missing"
W13-2201,W13-2221,1,0.762444,"Missing"
W13-2201,W13-2228,0,0.0270572,"Missing"
W13-2201,W13-2224,0,0.0611782,"Missing"
W13-2201,2013.mtsummit-papers.21,1,0.577491,"ve learning to reduce the training set size (and therefore the annotation effort). The initial set features contains all black box and glass box features available within the Q U E ST framework (Specia et al., 2013) for the dataset at hand (160 in total for Task 1.1, and 80 for Task 1.3). The query selection strategy for active learning is based on the informativeness of the instances using Information Density, a measure that leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is. To perform feature selection, following (Shah et al., 2013) features are ranked by the Gaussian Process 23 algorithm according to their learned length scales, which can be interpreted as the relevance of such feature for the model. This information was used for feature selection by discarding the lowest ranked (least useful) ones. based on empirical results found in (Shah et al., 2013), the top 25 features for both models were selected and used to retrain the same regression algorithm. Semantic Roles could bring marginally better accuracy. TCD-CNGL (T1.1) and TCD-DCU-CNGL (T1.3): The system is based on features which are commonly used for style classi"
W13-2201,2012.amta-papers.13,0,0.0208135,"Missing"
W13-2201,W13-2250,0,0.0320711,"Missing"
W13-2201,W13-2225,0,0.0376682,"Missing"
W13-2201,P13-1135,1,0.207781,"Missing"
W13-2201,W13-2226,1,0.759686,"Missing"
W13-2201,2006.amta-papers.25,0,0.295541,"ation Based on the data of Task 1.3, we define Task 2, a word-level annotation task for which participants are asked to produce a label for each token that indicates whether the word should be changed by a post-editor or kept in the final translation. We consider the following two sets of labels for prediction: Sentence-level Quality Estimation Task 1.1 Predicting Post-editing Distance This task is similar to the quality estimation task in WMT12, but with one important difference in the scoring variant: instead of using the post-editing effort scores in the [1, 2, 3, 4, 5] range, we use HTER (Snover et al., 2006) as quality score. This score is to be interpreted as the minimum edit distance between the machine translation and its manually post-edited version, and its range is [0, 1] (0 when no edit needs to be made, and 1 when all words need to be edited). Two variants of the results could be submitted in the shared task: • Binary classification: a keep/change label, the latter meaning that the token should be corrected in the post-editing process. • Multi-class classification: a label specifying the edit action that should be performed on the token (keep as is, delete, or substitute). 6.3 Datasets Ta"
W13-2201,W13-2227,0,0.0389834,"Missing"
W13-2201,W09-0441,0,0.0271208,"ze, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly p"
W13-2201,W13-2230,0,0.0166145,"Missing"
W13-2201,W12-3118,1,0.609118,"1.1, T1.3): The submissions explored features built on MT engine resources including automatic word alignment, n-best candidate translation lists, back-translations and word posterior probabilities. Information about word alignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011"
W13-2201,P13-2135,0,0.148019,"the difference between selecting the best or the worse translation. 19 ID CMU CNGL DCU DCU-SYMC DFKI FBK-UEdin LIG LIMSI LORIA SHEF TCD-CNGL TCD-DCU-CNGL UMAC UPC Participating team Carnegie Mellon University, USA (Hildebrand and Vogel, 2013) Centre for Next Generation Localization, Ireland (Bicici, 2013b) Dublin City University, Ireland (Almaghout and Specia, 2013) Dublin City University & Symantec, Ireland (Rubino et al., 2013b) German Research Centre for Artificial Intelligence, Germany (Avramidis and Popovic, 2013) Fondazione Bruno Kessler, Italy & University of Edinburgh, UK (Camargo de Souza et al., 2013) Laboratoire d’Informatique Grenoble, France (Luong et al., 2013) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Singh et al., 2013) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois and Smaili, 2013) University of Sheffield, UK (Beck et al., 2013) Trinity College Dublin & CNGL, Ireland (Moreau and Rubino, 2013) Trinity College Dublin, Dublin City University & CNGL, Ireland (Moreau and Rubino, 2013) University of Macau, China (Han et al., 2013) Universitat Politecnica de Catalunya, Spain (Formiga et al., 2013b) Ta"
W13-2201,2011.eamt-1.12,1,0.741789,"ar (CCG) features: CCG supertag language model perplexity and log probability, the number of maximal CCG constituents in the translation output which are the highestprobability minimum number of CCG constituents that span the translation output, the percentage of CCG argument mismatches between each subsequent CCG supertags, the percentage of CCG argument mismatches between each subsequent CCG maximal categories and the minimum number of phrases detected in the translation output. A second submission uses the aforementioned CCG features combined with 80 features from Q U E ST as described in (Specia, 2011). For the CCG features, the C&C parser was used to parse the translation output. Moses was used to build the phrase table from the SMT training corpus with maximum phrase length set to 7. The language model of supertags was built using the SRILM toolkit. As learning algorithm, Logistic Regression as provided by the SCIKIT- LEARN toolkit was used. The training data was prepared by converting each ranking of translation outputs to a set of pairwise comparisons according to the approach proposed by Avramidis et al. (2011). The rankings were generated back from pairwise comparisons predicted by th"
W13-2201,P13-4014,1,0.118757,"Missing"
W13-2201,W13-2229,0,0.0371208,"Missing"
W13-2201,tantug-etal-2008-bleu,0,0.0131442,"a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly picking the test set size (number of distinct sentences) and the number of distinct references per sentence. Note that such test sets are s"
W13-2201,W10-1751,0,\N,Missing
W13-2201,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2201,W13-2249,0,\N,Missing
W13-2201,N04-1013,0,\N,Missing
W13-2201,W02-1001,0,\N,Missing
W13-2201,W12-3102,1,\N,Missing
W13-2201,P12-3024,0,\N,Missing
W13-2201,W09-0401,1,\N,Missing
W13-2201,W13-2222,0,\N,Missing
W13-2201,W10-1711,0,\N,Missing
W13-2201,2010.iwslt-evaluation.22,0,\N,Missing
W13-2201,W10-1703,1,\N,Missing
W13-2201,W08-0309,1,\N,Missing
W13-2201,W13-2218,0,\N,Missing
W13-2201,P13-1004,1,\N,Missing
W13-2201,2013.mtsummit-papers.9,0,\N,Missing
W13-2201,W13-2216,1,\N,Missing
W13-2201,W13-2244,0,\N,Missing
W13-2241,P13-1004,1,0.421716,"Missing"
W13-2241,P07-2045,0,0.00303498,"Missing"
W13-2241,D08-1112,0,0.0127566,"a two-step procedure where we first optimise a model with all the SE length scales tied to the same value (which is equivalent to an isotropic model) and we used the resulting values as starting point for the ARD optimisation. 2 339 http://sheffieldml.github.io/GPy/ only simulate AL) and then adds it to the training set. Our procedure started with 50 instances for task 1.1 and 20 instances for task 1.3, given its reduced training set size. We optimised the Gaussian Process hyperparameters every 20 new instances, for both tasks. As a measure of informativeness we used Information Density (ID) (Settles and Craven, 2008). This measure leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is: ID(x) = V ar(y|x) × !β U 1 X (u) sim(x, x ) U Figure 1: Test error and test confidence curves for HTER prediction (task 1.1) using the WMT12 training and test sets. u=1 The β parameter controls the relative importance of the density term. In our experiments, we set it to 1, giving equal weights to variance and density. The U term is the number of instances in the query pool. The variance values V ar(y|x) are given by the GP prediction while the similar"
W13-2241,2013.mtsummit-papers.21,1,0.684818,"(x, x0 )) which is parameterized by a mean function (here, 0) and a covariance kernel function k(x, x0 ). Each http://www.quest.dcs.shef.ac.uk 338 All our models were trained using the GPy2 toolkit, an open source implementation of GPs written in Python. response value is then generated from the function evaluated at the corresponding input, yi = f (xi )+ η, where η ∼ N (0, σn2 ) is added white-noise. Prediction is formulated as a Bayesian inference under the posterior: Z p(y∗ |x∗ , D) = p(y∗ |x∗ , f )p(f |D) 3.1 Feature Selection To perform feature selection, we followed the approach used in Shah et al. (2013) and ranked the features according to their learned length scales (from the lowest to the highest). The length scales of a feature can be interpreted as the relevance of such feature for the model. Therefore, the outcome of a GP model using an ARD kernel can be viewed as a list of features ranked by relevance, and this information can be used for feature selection by discarding the lowest ranked (least useful) ones. For task 1.1, we performed this feature selection over all 160 features mentioned in Section 2. For task 1.3, we used a subset of the 80 most general BB features as in (Shah et al."
W13-2241,2006.amta-papers.25,0,0.234895,"Missing"
W13-2241,2009.mtsummit-papers.16,1,0.860974,"of the available instances for training. These results give evidence that Gaussian Processes achieve the state of the art performance as a modelling approach for translation quality estimation, and that carefully selecting features and instances for the problem can further improve or at least maintain the same performance levels while making the problem less resource-intensive. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Callison-burch et al., 2012). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence. The WMT13 QE shared task defined a group of tasks related to QE. In this paper, we present 2 Features In our experiments, we used a set of 160 features which are grouped into black box (BB) and glass box (GB) features. They were extracted using the 337 Proceedings of the Eighth Workshop on Statistic"
W13-2241,P13-4014,1,0.477368,"Missing"
W13-2241,P13-2097,1,0.862588,"Missing"
W13-2241,C04-1046,0,0.0592042,"Missing"
W13-2241,W12-3102,1,\N,Missing
W13-3522,P07-2045,0,0.00736798,"-best paraphrases of each method and distributed them amongst the evaluators. We considered two evaluation scenarios: multi: the paraphrasing model with multilingual constraints introduced in this paper. Gold-standard translations: the English translation as found in Europarl was taken as sense tag, using automatic word-alignments to identify the English phrase that constrains the sense of the Spanish phrase. CCB: the model in (Bannard and CallisonBurch, 2005) which does not explicitly perform any sense disambiguation. SMT translations: a phrase-based SMT system built using the Moses toolkit (Koehn et al., 2007) and the whole Spanish-English dataset (except the sentences in the test set) was used to translated the Spanish sentences. Instead of gold-standard translations as a quasiperfect sense annotation (quasi because the word-alignment is still automatic and thus prone to errors), the phrase-based SMT system plays the role of a sense annotation module predicting the “sense” tags. CCB-wsd: an extended model in (Bannard and Callison-Burch, 2005) using English phrases as sense tags for pivoting. Using each of these three models, we paraphrased the 258 samples in our test set, retrieving the 3-best par"
W13-3522,P05-1074,0,0.442645,"13. 2013 Association for Computational Linguistics the meaning of the source in the triangulated translation, as different languages are likely to realise ambiguities differently. Although their findings apply to generating translation candidates, the input phrases are not constrained to specific senses, and as a consequence multiple translations, which are valid in different contexts but not generally interchangeable, are mixed together in the same distribution. In SMT the target Language Model (LM) helps selecting the adequate translation candidate in context. Callison-Burch (2008) extends (Bannard and Callison-Burch, 2005) by adding syntactic constraints to the model. Paraphrase extraction is done by pivoting using word-alignment information, as before, but sentences are syntactically annotated and paraphrases are restricted to those with the same syntactic category. This addresses categorial ambiguity by preventing that words with a given category (e.g. a noun) are paraphrased by words with other categories (e.g., a verb). However, the approach does not solve the more complex issue of polysemous paraphrases: words with the same category but different meanings, such as the noun bank as financial institution and"
W13-3522,N06-1003,0,0.0480386,"Missing"
W13-3522,2005.mtsummit-papers.11,0,0.0750779,"e paraphrased. Ambiguity is determined on the basis of the number of synsets in the Spanish WordNet. We note that this information was only used to bias the selection of the phrases, i.e., WordNet is not used in the proposed approach. Experimental settings 4.1 Resources La idea de conceder a la Uni´on Europea su propia competencia fiscal - la palabra clave es el “impuesto por Europa” - est´a siendo debatida. The idea of granting the EU its own tax competence - the keyword is the “Europe tax” - is being discussed. The source of bilingual data used in the experiments is the Europarl collection (Koehn, 2005). We paraphrase Spanish (es) phrases using their corresponding English (en) phrases as sense tags and nine European languages as pivots: German (de), Dutch (nl), Danish (da), Swedish (sv), Finnish (fi), French (fr), Italian (it), Portuguese (pt) and Greek (el). The tools provided along with the corpus were used to extract the sentence aligned parallel data as shown in Table 1. The sentence aligned parallel data is first wordaligned using GIZA++ in both source-target and target-source directions, followed by the application of traditional symmetrisation heuristics (Och and Ney, 2003). These ali"
W13-3522,W10-1754,0,0.0141708,"mables 0.16 ∗ kind 0.12 especie 0.09 ∗ amable 0.08 tipo 0.07 ∗ Table 5: Top paraphrases of forma annotated by the English words way, form, means and kind. Starred phrases denote inadequate candidates. 5.3 Potential applications tilingual constraints offer more control over ambiguities, therefore potentially leading to more accurate phrase pairs added to the phrase-table. In what follows we discuss two applications which we believe could directly benefit from the paraphrase extraction approach proposed in this paper. 6 MT evaluation metrics such as METEOR (Denkowski and Lavie, 2010) and TESLA (Liu et al., 2010) already use paraphrases of n-grams in the machine translated sentence in an attempt to match more of the reference translation’s ngrams. TESLA, in particular, uses paraphrases constrained by a single pivot language as sense tag as originally proposed in (Bannard and CallisonBurch, 2005). Metrics like METEOR, which use paraphrases simply as a repository with extra options for the n-gram matching, could be extended to use the word-alignment between the source sentence and the translation to constrain the translated phrases while paraphrasing them with multilingual constraints. In this case the"
W13-3522,D08-1021,0,0.103433,"LM component in getting the senses of the paraphrases right. Results The evaluation was performed by seven native speakers of Spanish who judged a total of 5, 110 sentences containing one paraphrased input phrase each. We used 40 overlapping judgements across annotators to measure inter-annotator agreement. The average inter-annotator agreement in terms of Cohen’s Kappa (Cohen, 1960) is 0.54 ± 0.15 for meaning judgements, 0.63 ± 0.16 for grammar judgements and 0.62 ± 0.20 for correctness judgements. These figures are similar or superior to those reported in (Bannard and Callison-Burch, 2005; Callison-Burch, 2008), which we consider particularly encouraging as in our case we have seven instead of only two annotators. In Tables 2, 3 and 4 we report the performance of the three models in terms of precision, recall and F1 , with p-values &lt; 0.01 based on the t-test for statistical significance. 5.1 G 23 9 43 Table 3: Performance (F1 ) in correctly retrieving the best paraphrase in context using gold-standard translations without the 5-gram LM component. Table 2: Performance in retrieving paraphrases in context using gold-standard translations for sense tags and a 5-gram LM component. 5 M 33 19 64 To show t"
W13-3522,J10-3003,0,0.0214049,"g preserving in context. This paper is structured as follows: Section 2 describes additional previous work on paraphrase extraction and pivoting. Section 3 presents the proposed model. Section 4 introduces our experimental settings, while Section 5 shows the results of a series of experiments. 2 Related work 3 In addition to the well-known approach by (Bannard and Callison-Burch, 2005), the following previous approaches using pivot languages for paraphrasing can be mentioned. For a recent and comprehensive survey on a number of datadriven paraphrase generation methods, we refer the reader to (Madnani and Dorr, 2010). Cohn and Lapata (2007) make use of multiple parallel corpora to improve Statistical Machine Translation (SMT) by triangulation for languages with little or no source-target parallel data available. Translation tables are learnt by pivoting through languages for which source-pivot and pivot-target bilingual corpora can be found. Multiple pivot languages were found useful to preserve Paraphrasing through multilingual constraints Our approach to paraphrasing can be applied to both individual words or sequences of words of any length, conditioned only on sufficient evidence of these segments in"
W13-3522,D07-1007,0,0.0250728,"is, a valid translation of the input phrase in a language of interest, here referred to as target language. Treating the target language vocabulary as a sense repository is a good strategy from both theoretical and practical perspectives: it has been shown that monolingual sense distinctions can be effectively captured by translations into second languages, especially as language family distance increases (Resnik and Yarowsky, 1999; Specia et al., 2006). These translations can be easily captured given the availability of bilingual parallel data and robust automatic word-alignment techniques (Carpuat and Wu, 2007; Chan et al., 2007). Figure 1 illustrates the proposed model to produce sense tagged paraphrases. We start the process at e1 and we need to make sure that the pivot phrases f ∈ F align back to the input language, producing the paraphrase e2 , and to the target language, producing the sense tag q. To avoid computing the distribution p(e2 , q|f ) – which would require a trilingual parallel corpus – we assume that e2 and q are conditionally independent on f : p(e2 , q|f ) e2 ⊥ ⊥q|f = itory, in addition to bilingual parallel corpora between the input language and the pivot languages, our model re"
W13-3522,W07-0716,0,0.0434365,"Missing"
W13-3522,P07-1005,0,0.0311003,"on of the input phrase in a language of interest, here referred to as target language. Treating the target language vocabulary as a sense repository is a good strategy from both theoretical and practical perspectives: it has been shown that monolingual sense distinctions can be effectively captured by translations into second languages, especially as language family distance increases (Resnik and Yarowsky, 1999; Specia et al., 2006). These translations can be easily captured given the availability of bilingual parallel data and robust automatic word-alignment techniques (Carpuat and Wu, 2007; Chan et al., 2007). Figure 1 illustrates the proposed model to produce sense tagged paraphrases. We start the process at e1 and we need to make sure that the pivot phrases f ∈ F align back to the input language, producing the paraphrase e2 , and to the target language, producing the sense tag q. To avoid computing the distribution p(e2 , q|f ) – which would require a trilingual parallel corpus – we assume that e2 and q are conditionally independent on f : p(e2 , q|f ) e2 ⊥ ⊥q|f = itory, in addition to bilingual parallel corpora between the input language and the pivot languages, our model requires bilingual par"
W13-3522,D09-1040,0,0.0448874,"Missing"
W13-3522,P09-1089,1,0.859292,"ond morphological variants of the original text is a challenging problem and has been shown to be useful in many natural language applications. These include i) expanding the set of reference translations for Machine Translation (MT) evaluation (Denkowski and Lavie, 2010; Liu et al., 2010) and parameter optimisation (Madnani et al., 2007), where multiple reference translations are important to accommodate for valid variations of system translations; ii) addressing the problem of out-of-vocabulary words or phrases in MT, either by replacing these by paraphrases that are known to the MT system (Mirkin et al., 2009) or by exX f ∈F p(f |e1 )p(e2 |f ) (1) Equation 1 allows paraphrases to be extracted by using multiple pivot languages such that these languages help discard inadequate paraphrases resulting from ambiguous pivot phrases. However in this formulation all senses of the input phrase are mixed together in a single distribution. For example, for the Spanish input phrase acabar con, both paraphrases superar (overcome) and eliminar (eliminate) may be adequate depending on the context, however they are not generally interchangeable. In (Bannard and Callison-Burch, 1 The distributions p(f |e) and p(e|f"
W13-3522,P07-1092,0,0.0160503,"This paper is structured as follows: Section 2 describes additional previous work on paraphrase extraction and pivoting. Section 3 presents the proposed model. Section 4 introduces our experimental settings, while Section 5 shows the results of a series of experiments. 2 Related work 3 In addition to the well-known approach by (Bannard and Callison-Burch, 2005), the following previous approaches using pivot languages for paraphrasing can be mentioned. For a recent and comprehensive survey on a number of datadriven paraphrase generation methods, we refer the reader to (Madnani and Dorr, 2010). Cohn and Lapata (2007) make use of multiple parallel corpora to improve Statistical Machine Translation (SMT) by triangulation for languages with little or no source-target parallel data available. Translation tables are learnt by pivoting through languages for which source-pivot and pivot-target bilingual corpora can be found. Multiple pivot languages were found useful to preserve Paraphrasing through multilingual constraints Our approach to paraphrasing can be applied to both individual words or sequences of words of any length, conditioned only on sufficient evidence of these segments in a parallel corpus. We us"
W13-3522,J03-1002,0,0.00593946,"parl collection (Koehn, 2005). We paraphrase Spanish (es) phrases using their corresponding English (en) phrases as sense tags and nine European languages as pivots: German (de), Dutch (nl), Danish (da), Swedish (sv), Finnish (fi), French (fr), Italian (it), Portuguese (pt) and Greek (el). The tools provided along with the corpus were used to extract the sentence aligned parallel data as shown in Table 1. The sentence aligned parallel data is first wordaligned using GIZA++ in both source-target and target-source directions, followed by the application of traditional symmetrisation heuristics (Och and Ney, 2003). These aligned corpora are used for paraphrase extraction, except for a subset of them used in the creation of a test set (Section 4.2). 4.2 Figure 3: Example of context selected for the phrase clave. tag to avoid selecting simpler, categorial ambiguities). Figure 2 lists the selected words and phrases in their base forms. The bilingual corpus was queried for sentences containing at least one of the 50 phrases listed in Figure 2, or any of their morphological variants. The resulting sentences were then grouped on the basis of whether or not they shared the same English translation. To find th"
W13-3522,W10-1751,0,0.0509239,"Missing"
W13-3522,P02-1040,0,0.0970258,"Missing"
W13-3522,N03-1017,0,0.00925389,"Missing"
W13-3522,P07-1059,0,0.0741067,"Missing"
W13-3522,W06-2505,1,0.73993,"millions of sentence pairs the pivot phrases that will lead to adequate paraphrases. In our approach a sense tag consists in a phrase in a foreign language, that is, a valid translation of the input phrase in a language of interest, here referred to as target language. Treating the target language vocabulary as a sense repository is a good strategy from both theoretical and practical perspectives: it has been shown that monolingual sense distinctions can be effectively captured by translations into second languages, especially as language family distance increases (Resnik and Yarowsky, 1999; Specia et al., 2006). These translations can be easily captured given the availability of bilingual parallel data and robust automatic word-alignment techniques (Carpuat and Wu, 2007; Chan et al., 2007). Figure 1 illustrates the proposed model to produce sense tagged paraphrases. We start the process at e1 and we need to make sure that the pivot phrases f ∈ F align back to the input language, producing the paraphrase e2 , and to the target language, producing the sense tag q. To avoid computing the distribution p(e2 , q|f ) – which would require a trilingual parallel corpus – we assume that e2 and q are condition"
W13-4813,I11-1053,0,0.0392347,"Missing"
W13-4813,J96-2004,0,0.0479818,"Missing"
W13-4813,P96-1041,0,0.168474,"of the tree transformations, we use 133K pairs of original-simple sentences from the Simple Wikipedia corpus [Coster and Kauchak 2011] parsed by the Stanford constituency parser [Klein and Manning 2003]. Language Model For this experiment a 3-gram language model is used to rank candidate simplifications. The model is trained on a superset of the Simple Wikipedia corpus containing 710K simple sentences. We use the SRILM toolkit [Stolcke et al. 2011] to build the 3-gram language model. The motivation to use a 3-gram rather than a 4 or 5-gram language model is based on the studies presented at [Chen and Goodman 1996], which suggests that language models of higher order tend to lead to data sparsity when the corpus is either noisy or not large enough. Test Corpus The corpus to be simplified in our tests contains 130 original sentences randomly selected from a held-out portion of the parallel Simple Wikipedia corpus. All sentences have a simpler version in the corpus which is different from the original version, and vary between 50 and 240 characters in length. Evaluation Metrics We evaluate the simplification both automatically by the string matching metric BLEU [Papineni et al. 2002], and manually using"
W13-4813,P11-2117,0,0.0516376,"2.3. Ranking Module This is a simple module that scores all candidate simplifications for a given complex sentence produced by the lexical or syntactic simplification modules. The scores are then used to rank the candidates such that the top scoring candidate can be selected. At this stage, we score candidates using their predicted perplexity based on a language model of simplified sentences, as described in the next section. 3. Experimental Settings Training Corpus For the extraction of the tree transformations, we use 133K pairs of original-simple sentences from the Simple Wikipedia corpus [Coster and Kauchak 2011] parsed by the Stanford constituency parser [Klein and Manning 2003]. Language Model For this experiment a 3-gram language model is used to rank candidate simplifications. The model is trained on a superset of the Simple Wikipedia corpus containing 710K simple sentences. We use the SRILM toolkit [Stolcke et al. 2011] to build the 3-gram language model. The motivation to use a 3-gram rather than a 4 or 5-gram language model is based on the studies presented at [Chen and Goodman 1996], which suggests that language models of higher order tend to lead to data sparsity when the corpus is either no"
W13-4813,W11-2107,0,0.0301312,"lification approach uses as a basis possible transformations learned with the Tree Transducer Toolkit (T3) [Cohn and Lapata 2009]. The approach is composed by three main components: a training module, a simplification module, and a ranking module. 2.1. Training Module This module is responsible for generating a large number of candidate transformation rules, some of which will be purely syntactic or purely lexical, while others will be lexico-syntactic. It takes as input a parsed version of a parallel corpus of complex and simple sentences aligned at the word level (produced using Meteor 1.4 [Denkowski and Lavie 2011]). It uses the harvest function in T3 to extract a synchronous tree-substitution grammar which describes tree transformations. The harvest function extracts the maximally general (smallest) pairs of tree fragments that are consistent with the word-alignment. In Table 1 we show examples of transformations produced by this function using the Simple Wikipedia corpus, where # indicates indexes of place holders for any type of syntactic subtree. By inspecting Table 1, one will notice that the tree transformations can take very distinct forms. The first example shows a transformation which removes"
W13-4813,P03-1054,0,0.0240031,"simplifications for a given complex sentence produced by the lexical or syntactic simplification modules. The scores are then used to rank the candidates such that the top scoring candidate can be selected. At this stage, we score candidates using their predicted perplexity based on a language model of simplified sentences, as described in the next section. 3. Experimental Settings Training Corpus For the extraction of the tree transformations, we use 133K pairs of original-simple sentences from the Simple Wikipedia corpus [Coster and Kauchak 2011] parsed by the Stanford constituency parser [Klein and Manning 2003]. Language Model For this experiment a 3-gram language model is used to rank candidate simplifications. The model is trained on a superset of the Simple Wikipedia corpus containing 710K simple sentences. We use the SRILM toolkit [Stolcke et al. 2011] to build the 3-gram language model. The motivation to use a 3-gram rather than a 4 or 5-gram language model is based on the studies presented at [Chen and Goodman 1996], which suggests that language models of higher order tend to lead to data sparsity when the corpus is either noisy or not large enough. Test Corpus The corpus to be simplified in"
W13-4813,P02-1040,0,0.0856712,"tudies presented at [Chen and Goodman 1996], which suggests that language models of higher order tend to lead to data sparsity when the corpus is either noisy or not large enough. Test Corpus The corpus to be simplified in our tests contains 130 original sentences randomly selected from a held-out portion of the parallel Simple Wikipedia corpus. All sentences have a simpler version in the corpus which is different from the original version, and vary between 50 and 240 characters in length. Evaluation Metrics We evaluate the simplification both automatically by the string matching metric BLEU [Papineni et al. 2002], and manually using five human subjects. Our system produces two simplified versions for each sentence, resulting in 260 simplified sentences to be evaluated. As part of the manual evaluation, each evaluator is assigned 100 sentences: the syntactically and lexically simplified versions of 50 originally complex sentences. 30 of these sentences are common to all evaluators so that inter-annotator agreement can be computed. Evaluators are asked to answer three questions for each syntactically or lexically simplified sentence: • Is it simpler than the original sentence? • Is it grammatical? • Do"
W13-4813,S12-1046,1,0.817339,"h is proposed in [Woodsend and Lapata 2011] using a quasi-synchronous grammar and integer programming to generate syntactic simplification rules. These are generally precise, but at the cost of low coverage. Most work on lexical simplification is based on synonym substitution using frequency-related or readability statistics, and also context models based mostly on bagof-words to identify whether a candidate substitution fits the context [Keskisarkka 2012, Kandula et al. 2010, Carroll et al. 1998]. For a comprehensive overview, we refer the reader to the SemEval-2012 shared task on the topic [Specia et al. 2012]. Overall, lexical simplification work disregards explicit syntactic cues. To overcome these limitations we investigate the learning of tree transduction rules to produce larger volumes of both lexical and lexicalized syntactic simplification rules. This approach has the potential to produce rule sets with high coverage and variability, and at the same time make them more specific by lexicalising some of the rule components of syntactic simplification rules. 2. Simplification Approach Our simplification approach uses as a basis possible transformations learned with the Tree Transducer Toolkit"
W13-4813,D11-1038,0,0.0605627,"c Fortaleza, CE, Brazil, October 21–23, 2013. 2013 Sociedade Brasileira de Computa¸c˜ ao Efforts on using statistical and machine learning techniques to address syntactic simplification include several approaches inspired by phrase- or tree-based statistical models for machine translation [Specia 2010, Zhu et al. 2010, Wubben et al. 2012], which learn limited transformations such as short-distance reordering. [Bach et al. 2011] design templates for subject-verb-object simplification, which limit the application of rules to cases matching the templates. A more general approach is proposed in [Woodsend and Lapata 2011] using a quasi-synchronous grammar and integer programming to generate syntactic simplification rules. These are generally precise, but at the cost of low coverage. Most work on lexical simplification is based on synonym substitution using frequency-related or readability statistics, and also context models based mostly on bagof-words to identify whether a candidate substitution fits the context [Keskisarkka 2012, Kandula et al. 2010, Carroll et al. 1998]. For a comprehensive overview, we refer the reader to the SemEval-2012 shared task on the topic [Specia et al. 2012]. Overall, lexical simp"
W13-4813,P12-1107,0,0.112594,"Missing"
W13-4813,C10-1152,0,0.170855,"Missing"
W14-0312,2005.iwslt-1.7,0,0.0244841,"om l.specia@sheffield.ac.uk Introduction Active learning (AL) is a technique for the automatic selection of data which is most useful for model building. In the context of machine translation (MT), AL is particularly important as the acquisition of data often has a high cost, i.e. new source texts need to be translated manually. Thus it is beneficial to select for manual translation sentences which can lead to better translation quality. The majority of AL methods for MT is based on the (dis)similarity of sentences with respect to the training data, with particular focus on domain adaptation. Eck et al. (2005) suggest a TF-IDF metric to choose sentences with words absent in the training corpus. Ambati et al. (2010) propose a metric of informativeness relying on unseen ngrams. Bloodgood and Callison-Burch (2010) use ngram frequency and coverage of the additional data as selection criteria. Their technique solicits translations for phrases instead of entire sentences, which saves user effort and leads to quality improvements even if the initial dataset is already sizeable. A recent trend is to select source sentences based on an estimate of the quality of their translation by a baseline MT system. It"
W14-0312,P02-1040,0,0.0955572,"the new batch. The process is repeated until the pool is empty, with subsequent steps using the MT system trained on the previous step as a baseline. The performance of each MT system is measured in terms of BLEU scores. We use the following AL strategies: Table 1: Datasets 3.2 • QuEst: our method described in section 2. Post-editions versus references In order to compare the impact of post-editions and reference translations on MT quality, we added these two variants of translations to baseline MT systems of different sizes, including the entire News Commentary corpus. The figures for BLEU (Papineni et al., 2002) scores in Table 2 show that adding post-editions results in significantly better quality than adding the same number of reference translations3 . This effect can be seen even when the additional data corresponds to only a small fraction of the training data. In addition, it does not seem to matter which MT system produced the translations which were then post-edited in the post-edition corpus. Even if the output of a third-party system was used (as in our case), it improves the quality of machine translations for unseen data. We assume that since posteditions tend to be closer to original sen"
W14-0312,N09-1047,0,0.0534082,"Missing"
W14-0312,2009.mtsummit-papers.8,0,0.0245348,"ich go beyond those used in previous work, covering information from both source and target sentences. Another important novel feature in our work is the addition of real post-editions to the MT training data, as opposed to simulated post-editions (human reference translations) as in previous work on AL for MT. As we show in section 3.2, adding post-editions leads to superior translation quality improvements. Additionally, this is a suitable solution for “human in the loop” settings, as postediting automatically translated sentences tends to be faster and easier than translation from scratch (Koehn and Haddow, 2009). Also, different from previous work, we do not focus on domain adaptation: our experiments involve only in-domain data. Compared to previous work on confidencedriven AL, our approach has led to better results, but these proved to be highly dependent on a sentence length bias. However, an oracle-based selecThe paper presents experiments with active learning methods for the acquisition of training data in the context of machine translation. We propose a confidencebased method which is superior to the state-of-the-art method both in terms of quality and complexity. Additionally, we discovered th"
W14-0312,W07-0737,0,0.0274888,". As it was already noted by Bloodgood and Callison-Burch (2010), measuring the amount of added data in sentences can significantly contort the real annotation cost (the cost of acquisition of new translations). So we switch to length-independent representation. length-independent representation can still be seen to perform worse than both our strategy and random selection. Results of Length and Error strategies (plotted separately in figure 5 for readability) are very close and both underperform our QuEst-based strategy and random selection of data. Here our experience echoes the results of (Mohit and Hwa, 2007), where the authors propose the idea of difficult to translate phrases. It is assumed that extending an MT system with phrases that can cause difficulties during translation is more effective than simply adding new data and re-building the system. Due to the lack of time and human annotators, the authors extracted difficult phrases automatically using a set of features: alignment features, syntactic features, model score, etc. Conversely, we had the human-generated information on what segments have been translated incorrectly. We assumed that the use of this knowledge as part of our AL strateg"
W14-0312,ambati-etal-2010-active,0,0.0236258,"of data which is most useful for model building. In the context of machine translation (MT), AL is particularly important as the acquisition of data often has a high cost, i.e. new source texts need to be translated manually. Thus it is beneficial to select for manual translation sentences which can lead to better translation quality. The majority of AL methods for MT is based on the (dis)similarity of sentences with respect to the training data, with particular focus on domain adaptation. Eck et al. (2005) suggest a TF-IDF metric to choose sentences with words absent in the training corpus. Ambati et al. (2010) propose a metric of informativeness relying on unseen ngrams. Bloodgood and Callison-Burch (2010) use ngram frequency and coverage of the additional data as selection criteria. Their technique solicits translations for phrases instead of entire sentences, which saves user effort and leads to quality improvements even if the initial dataset is already sizeable. A recent trend is to select source sentences based on an estimate of the quality of their translation by a baseline MT system. It is assumed 78 Workshop on Humans and Computer-assisted Translation, pages 78–83, c Gothenburg, Sweden, 26"
W14-0312,W10-1723,0,0.020359,"e of the pool of additional data (10,000) poses a limitation. To examine improvements obtained by adding fractions of up to only 9,000 sentences, we took a small random subset of the WMT13 data for these experiments (Table 1). Although these figures may seem small, the settings are realistic for many language pairs and text domains where larger data sets are simply not available. We should also note that all the data used in our experiments belongs to the same domain: the LIG SMT system which produced sentences for the post-editions corpus was trained on Europarl and News commentary datasets (Potet et al., 2010), but the post-edited sentences themselves were taken from news test sets released for WMT shared tasks in different years. Our baseline system is trained on a fraction of the news commentary corpus. Finally, we tune and test all our systems on WMT shared task news news datasets (those which do not overlap with the post-editions corpus). Experiments and results Datasets and MT settings For the AL data selection experiment, two datasets are necessary: parallel sentences to train an initial, baseline MT system, and an additional pool of parallel sentences to select from. Our goal was to study po"
W14-0312,D10-1061,0,0.268825,"Translation Varvara Logacheva University of Sheffield Sheffield, United Kingdom v.logacheva@sheffield.ac.uk Abstract that if a sentence has been translated well with the existing data, it will not contribute to improving the translation quality. If however a sentence has been translated erroneously, it might have words or phrases that are absent or incorrectly represented. Haffari et al. (2009) train a classifier to define the sentences to select. The classifier uses a set of features of the source sentences and their automatic translations: n-grams and phrases frequency, MT model score, etc. Ananthakrishnan et al. (2010) build a pairwise classifier that ranks sentences according to the proportion of n-grams they contain that can cause errors. For quality estimation, Banerjee et al. (2013) train language models of well and badly translated sentences. The usefulness of a sentence is measured as the difference of its perplexities in these two language models. In this research we also explore a quality-based AL technique. Compared to its predecessors, our method is based on a more complex and therefore potentially more reliable quality estimation framework. It uses wider range of features, which go beyond those u"
W14-0312,potet-etal-2012-collection,0,0.0670151,"Missing"
W14-0312,2013.mtsummit-papers.13,0,0.0739522,"Missing"
W14-0312,2006.amta-papers.25,0,0.0237985,"Active selection strategy Our AL sentence selection strategy relies on quality estimation (QE). QE is aimed at predicting the quality of a translated text (in this case, a sentence) without resorting to reference translations. It considers features of the source and machine translated texts, and an often small number (a few hundreds) of examples of translations labelled for quality by humans to train a machine learning algorithm to predict such quality labels for new data. We use the open source QE framework QuEst (Specia et al., 2013). In our settings it was trained to predict an HTER score (Snover et al., 2006) for each sentence, i.e., the edit distance between the automatic translation and its human post-edited version. QuEst can extract a wide range of features. In our experiments we use only the 17 socalled baseline features, which have been shown to perform well in evaluation campaigns (Bojar et al., 2013): number of tokens in sentences, average token length, language model probabilities for source and target sentences, average number of translations per source word, percentage of higher and lower frequency n-grams in source sentence based on MT training corpus, number of punctuation marks in so"
W14-0312,P10-1088,0,0.108096,"tion (MT), AL is particularly important as the acquisition of data often has a high cost, i.e. new source texts need to be translated manually. Thus it is beneficial to select for manual translation sentences which can lead to better translation quality. The majority of AL methods for MT is based on the (dis)similarity of sentences with respect to the training data, with particular focus on domain adaptation. Eck et al. (2005) suggest a TF-IDF metric to choose sentences with words absent in the training corpus. Ambati et al. (2010) propose a metric of informativeness relying on unseen ngrams. Bloodgood and Callison-Burch (2010) use ngram frequency and coverage of the additional data as selection criteria. Their technique solicits translations for phrases instead of entire sentences, which saves user effort and leads to quality improvements even if the initial dataset is already sizeable. A recent trend is to select source sentences based on an estimate of the quality of their translation by a baseline MT system. It is assumed 78 Workshop on Humans and Computer-assisted Translation, pages 78–83, c Gothenburg, Sweden, 26 April 2014. 2014 Association for Computational Linguistics the baseline system and post-edited by"
W14-0312,P10-1000,0,0.219244,"Missing"
W14-0312,P13-4014,1,0.885749,"Missing"
W14-0312,W13-2201,1,\N,Missing
W14-1214,W03-1004,0,0.102798,"e English Wikipedia (SimpleEW)1 , which we consider a crowdsourced text simplification corpus. Coster and Kauchak (2011) paired articles from these two versions and automatically extracted parallel paragraphs and sentences from them (ParallelSEW). The first task was accomplished in a straightforward way, given that corresponding articles have the same title as unique identification. The paragraph alignment was performed selecting paragraphs when their normalised TFIDF weighted cosine distance reached a minimum threshold. Sentence alignment was performed using monolingual alignment techniques (Barzilay and Elhadad, 2003) based on a dynamic programming algorithm. In total, 137, 000 sentences were found to be parallel. The resulting parallel corpora contains transformation operations of various types, including rewording, reordering, insertion and deletion. In our experiments we analyse the distribution of these operations and perform some further analysis on their nature. Most studies on data-driven Text Simplification have focused on the learning of the operations, with no or little qualitative analysis of the Text Simplification corpora used (Yasseri et al., 2012). As in any other area, the quality of machin"
W14-1214,C96-2183,0,0.670593,"o complex to be processed by a large proportion of the population. Adapting texts into their simpler variants is an expensive task. Work on automating this process only started in recent years. However, already in the 1920’s Lively and Pressey (1923) created a method to distinguish simple from complex texts based on readability measures. Using such measures, publishers were able to grade texts according to reading levels (Klare and Buck, 1954) so that readers could focus on texts that were appropriate to them. The first attempt to automate the process of simplification of texts was devised by Chandrasekar et al. (1996). This pioneer work has shown that it was possible to simplify texts automatically through hand-crafted linguistic rules. In further work, Chandrasekar et al. (1997) developed a method to extract these rules from data. Siddharthan (2002) defines Text Simplification as any method or process that simplifies text while maintaining its information. Instead of handcrafted rules, recent methodologies are mostly data-driven, i.e., based on the induction of simplification rules from parallel corpora of complex segments and their corresponding simpler variants. Specia (2010) and Zhu et al. (2010) model"
W14-1214,D11-1038,0,0.0643864,"(2002) defines Text Simplification as any method or process that simplifies text while maintaining its information. Instead of handcrafted rules, recent methodologies are mostly data-driven, i.e., based on the induction of simplification rules from parallel corpora of complex segments and their corresponding simpler variants. Specia (2010) and Zhu et al. (2010) model the task using the Statistical Machine Translation framework, where simplified sentences are considered the “target language”. Yatskar et al. (2010) construct a simplification model based on edits in the Simple English Wikipedia. Woodsend and Lapata (2011) adopt a quasi-synchronous grammar with optimisation via integer linear programming. This research focuses the corpus used by most of Abstract We present a study on the text simplification operations undertaken collaboratively by Simple English Wikipedia contributors. The aim is to understand whether a complex-simple parallel corpus involving this version of Wikipedia is appropriate as data source to induce simplification rules, and whether we can automatically categorise the different operations performed by humans. A subset of the corpus was first manually analysed to identify its transforma"
W14-1214,P11-2117,0,0.195777,"ting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 123–130, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics previous data-driven Text Simplification work: the parallel corpus of the main and simple English Wikipedia. Following the collaborative nature of Wikipedia, a subset of the Main English Wikipedia (MainEW) has been edited by volunteers to make the texts more readable to a broader audience. This resulted in the Simple English Wikipedia (SimpleEW)1 , which we consider a crowdsourced text simplification corpus. Coster and Kauchak (2011) paired articles from these two versions and automatically extracted parallel paragraphs and sentences from them (ParallelSEW). The first task was accomplished in a straightforward way, given that corresponding articles have the same title as unique identification. The paragraph alignment was performed selecting paragraphs when their normalised TFIDF weighted cosine distance reached a minimum threshold. Sentence alignment was performed using monolingual alignment techniques (Barzilay and Elhadad, 2003) based on a dynamic programming algorithm. In total, 137, 000 sentences were found to be para"
W14-1214,N10-1056,0,0.0833459,"ther work, Chandrasekar et al. (1997) developed a method to extract these rules from data. Siddharthan (2002) defines Text Simplification as any method or process that simplifies text while maintaining its information. Instead of handcrafted rules, recent methodologies are mostly data-driven, i.e., based on the induction of simplification rules from parallel corpora of complex segments and their corresponding simpler variants. Specia (2010) and Zhu et al. (2010) model the task using the Statistical Machine Translation framework, where simplified sentences are considered the “target language”. Yatskar et al. (2010) construct a simplification model based on edits in the Simple English Wikipedia. Woodsend and Lapata (2011) adopt a quasi-synchronous grammar with optimisation via integer linear programming. This research focuses the corpus used by most of Abstract We present a study on the text simplification operations undertaken collaboratively by Simple English Wikipedia contributors. The aim is to understand whether a complex-simple parallel corpus involving this version of Wikipedia is appropriate as data source to induce simplification rules, and whether we can automatically categorise the different o"
W14-1214,C10-1152,0,0.185161,"ndrasekar et al. (1996). This pioneer work has shown that it was possible to simplify texts automatically through hand-crafted linguistic rules. In further work, Chandrasekar et al. (1997) developed a method to extract these rules from data. Siddharthan (2002) defines Text Simplification as any method or process that simplifies text while maintaining its information. Instead of handcrafted rules, recent methodologies are mostly data-driven, i.e., based on the induction of simplification rules from parallel corpora of complex segments and their corresponding simpler variants. Specia (2010) and Zhu et al. (2010) model the task using the Statistical Machine Translation framework, where simplified sentences are considered the “target language”. Yatskar et al. (2010) construct a simplification model based on edits in the Simple English Wikipedia. Woodsend and Lapata (2011) adopt a quasi-synchronous grammar with optimisation via integer linear programming. This research focuses the corpus used by most of Abstract We present a study on the text simplification operations undertaken collaboratively by Simple English Wikipedia contributors. The aim is to understand whether a complex-simple parallel corpus in"
W14-3302,bojar-etal-2014-hindencorp,1,0.801715,"Missing"
W14-3302,W13-2242,0,0.189283,"Missing"
W14-3302,W14-3305,0,0.0227897,"Missing"
W14-3302,W14-3326,1,0.729814,"Missing"
W14-3302,W14-3313,0,0.0328356,"Missing"
W14-3302,W14-3342,0,0.0770851,"Missing"
W14-3302,2012.iwslt-papers.5,1,0.897779,"Each system SJ in the pool {Sj } is represented by an associated relative ability µj and a variance σa2 (fixed across all systems) which serve as the parameters of a Gaussian distribution. Samples from this distribution represent the quality of sentence translations, with higher quality samples having higher values. Pairwise annotations (S1 , S2 , π) are generated according to the following process: Method 1: Expected Wins (EW) Introduced for WMT13, the E XPECTED W INS has an intuitive score demonstrated to be accurate in ranking systems according to an underlying model of “relative ability” (Koehn, 2012a). The idea is to gauge the probability that a system Si will be ranked better than another system randomly chosen from a pool of opponents {Sj : j 6= i}. If we define the function win(A, B) as the number of times system A is ranked better than system B, 19 1. Select two systems S1 and S2 from the pool of systems {Sj } This score is then used to sort the systems and produce the ranking. 2. Draw two “translations”, adding random 2 to simulate Gaussian noise with variance σobs the subjectivity of the task and the differences among annotators: 3.4 Method Selection We have three methods which, pr"
W14-3302,W06-3114,1,0.176114,"estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2014. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013). This year we conducted four official tasks: a translation task, a quality estimation task, a metrics task1 and a medical translation task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Hindi, and Russian. The Hindi translation tasks were new this year, providing a lesser resourced data condition on a challenging languag"
W14-3302,W12-3101,0,0.0505246,"Missing"
W14-3302,P02-1040,0,0.10401,"inuous-space language model is also used in a post-processing step for each system. POSTECH submitted a phrase-based SMT system and query translation system for the DE–EN language pair in both subtasks. They analysed three types of query formation, generated query translation candidates using term-to-term dictionaries and a phrase-based system, and then scored them using a co-occurrence word frequency measure to select the best candidate. UEDIN applied the Moses phrase-based system to 5.5 Results MT quality in the Medical Translation Task is evaluated using automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), PER (Tillmann et al., 1997), and CDER (Leusch et al., 2006). BLEU scores are reported as percentage and all error rates are reported as one minus the original value, also as percentage, so that all metrics are in the 0-100 range, and higher scores indicate better translations. The main reason for not conducting human evaluation, as it happens in the standard Trans45 original ID BLEU normalized truecased normalized lowercased 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER Czech→English CUNI 29.64 29.79±1.07 47.45±1.15 61.64±1.06 52.18±0.98 31.68±1.14 49.84±1.10 64.38±1."
W14-3302,W14-3328,0,0.0383178,"Missing"
W14-3302,W14-3301,1,0.485672,"guage pair by translating newspaper articles and provided training data. 2.1 2.2 As in past years we provided parallel corpora to train translation models, monolingual corpora to train language models, and development sets to tune system parameters. Some training corpora were identical from last year (Europarl4 , United Nations, French-English 109 corpus, CzEng, Common Crawl, Russian-English Wikipedia Headlines provided by CMU), some were updated (Russian-English parallel data provided by Yandex, News Commentary, monolingual data), and a new corpus was added (HindiEnglish corpus, Bojar et al. (2014)), Hindi-English Wikipedia Headline corpus). Some statistics about the training materials are given in Figure 1. Test data The test data for this year’s task was selected from news stories from online sources, as before. However, we changed our method to create the test sets. In previous years, we took equal amounts of source sentences from all six languages involved (around 500 sentences each), and translated them into all other languages. While this produced a multi-parallel test corpus that could be also used for language pairs (such as Czech-Russian) that we did not include in the evaluati"
W14-3302,W14-3330,0,0.0464303,"Missing"
W14-3302,W14-3320,0,0.0387654,"Missing"
W14-3302,W14-3317,0,0.0337314,"Missing"
W14-3302,W14-3343,1,0.726267,"Finally, with respect to out-of-domain (different 41 It is interesting to mention the indirect use of human translations by USHEFF for Tasks 1.1-1.3: given a translation for a source segment, all other translations for the same segment were used as pseudo-references. Apart from when this translation was actually the human translation, the human translation was effectively used as a reference. While this reference was mixed with 23 other pseudo-references (other machine translations) for the feature computations, these features led to significant gains in performance over the baseline features Scarton and Specia (2014). We believe that more investigation is needed for human translation quality prediction. Tasks dedicated to this type of data at both sentence- and word-level in the next editions of this shared task would be a possible starting point. The acquisition of such data is however much more costly, as it is arguably hard to find examples of low quality human translation, unless specific settings, such as translation learner corpora, are considered. text domain and MT system) test data, for Task 1.1, none of the papers submitted included experiments. (Shah and Specia, 2014) applied the models trained"
W14-3302,W02-1001,0,\N,Missing
W14-3302,E06-1031,0,\N,Missing
W14-3302,W12-3102,1,\N,Missing
W14-3302,P13-2135,0,\N,Missing
W14-3302,W14-3311,0,\N,Missing
W14-3302,W14-3314,0,\N,Missing
W14-3302,W09-0401,1,\N,Missing
W14-3302,W14-3319,0,\N,Missing
W14-3302,W14-3344,0,\N,Missing
W14-3302,W14-3327,0,\N,Missing
W14-3302,W07-0718,1,\N,Missing
W14-3302,P11-1132,0,\N,Missing
W14-3302,W13-2248,0,\N,Missing
W14-3302,W14-3307,0,\N,Missing
W14-3302,W10-1703,1,\N,Missing
W14-3302,W14-3323,0,\N,Missing
W14-3302,P13-4014,1,\N,Missing
W14-3302,W08-0309,1,\N,Missing
W14-3302,W14-3340,1,\N,Missing
W14-3302,W14-3312,0,\N,Missing
W14-3302,P13-1139,0,\N,Missing
W14-3302,W14-3310,1,\N,Missing
W14-3302,P13-1004,1,\N,Missing
W14-3302,W14-3304,0,\N,Missing
W14-3302,W14-3321,0,\N,Missing
W14-3302,W14-3308,0,\N,Missing
W14-3302,uresova-etal-2014-multilingual,1,\N,Missing
W14-3302,W14-3336,1,\N,Missing
W14-3302,W14-3331,0,\N,Missing
W14-3302,W14-3332,0,\N,Missing
W14-3302,W14-3325,0,\N,Missing
W14-3302,W14-3329,0,\N,Missing
W14-3302,W14-3315,0,\N,Missing
W14-3302,W13-2201,1,\N,Missing
W14-3302,W14-3339,0,\N,Missing
W14-3302,W14-3322,1,\N,Missing
W14-3302,W14-3303,0,\N,Missing
W14-3302,W14-3318,0,\N,Missing
W14-3302,W14-3341,0,\N,Missing
W14-3302,W11-2101,1,\N,Missing
W14-3302,W14-3338,1,\N,Missing
W14-3338,W13-2241,1,0.665279,"higher number of hyperparameters to optimise, and was therefore more prone to overfitting. We plan to investigate this issue further but a possible cause could be bad starting values for the hyperparameters. Speed-up – 3.59x 12.39x Table 3: Wall clock times and speed-ups for GPs training and prediction: full versus sparse GPs. 4 Table 6 shows results for Task 1.3. In this task, the standard GP model outperformed the baseline, with the sparse GP model following very closely. These figures represent significant improvements compared to our submission to the same task in last year’s shared task (Beck et al., 2013), where we were not able to beat the baseline. The main differences between last year’s and this year’s models are the use of additional datasets and a higher number of features (25 vs. 40). The competitive results for the sparse GP models are very promising because they show we can combine multiple datasets to improve post-editing time prediction while employing a sparse model to cope with speed issues. Official Results and Discussion Table 4 shows the results for Task 1.1. Using standard GPs we obtained improved results over the baseline for English-Spanish and EnglishGerman only, with parti"
W14-3338,C04-1046,0,0.112821,"ibe our systems for the WMT14 Shared Task on Quality Estimation (subtasks 1.1, 1.2 and 1.3). Our submissions use the framework of Multi-task Gaussian Processes, where we combine multiple datasets in a multi-task setting. Due to the large size of our datasets we also experiment with Sparse Gaussian Processes, which aim to speed up training and prediction by providing sensible sparse approximations. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Bojar et al., 2013). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence. The WMT 2014 QE shared task defined a group of tasks related to QE. In this paper, we describe our submissions for subtasks 1.1, 1.2 and 1.3. Our models are based on Gaussian Processes (GPs) (Rasmussen and Williams, 2006), a non-parametric kernelised probabilistic framework. We propose t"
W14-3338,P13-1004,1,0.922592,"s a Bayesian inference under the posterior: Z p(y∗ |x∗ , D) = p(y∗ |x∗ , f )p(f |D), where kdata is a kernel on the input points, d and d0 are task or metadata information for each input and B ∈ RD×D is the multi-task matrix, which encodes task covariances. For task 1.1, we consider each language pair as a different task, while for tasks 1.2 and 1.3 we use additional datasets for the same language pair (en-es), treating each dataset as a different task. To perform the learning procedure the multitask matrix should be parameterised in a sensible way. We follow the parameterisations proposed by Cohn and Specia (2013), which we briefly describe here: f where x∗ is a test input, y∗ is the test response value and D is the training set. The predictive posterior can be solved analitically, resulting in: y∗ ∼ N (kT∗ (K + σn2 I)−1 y, k(x∗ , x∗ ) − kT∗ (K + σn2 I)−1 k∗ ), where k∗ = [k(x∗ , x1 )k(x∗ , x2 ) . . . k(x∗ , xn )]T is the vector of kernel evaluations between the training set and the test input and K is the kernel matrix over the training inputs (the Gram matrix). The kernel function encodes the covariance (similarity) between each input pair. While a variety of kernel functions are available, here we f"
W14-3338,2013.mtsummit-papers.21,1,0.946618,"y∗ is the test response value and D is the training set. The predictive posterior can be solved analitically, resulting in: y∗ ∼ N (kT∗ (K + σn2 I)−1 y, k(x∗ , x∗ ) − kT∗ (K + σn2 I)−1 k∗ ), where k∗ = [k(x∗ , x1 )k(x∗ , x2 ) . . . k(x∗ , xn )]T is the vector of kernel evaluations between the training set and the test input and K is the kernel matrix over the training inputs (the Gram matrix). The kernel function encodes the covariance (similarity) between each input pair. While a variety of kernel functions are available, here we followed previous work in QE using GP (Cohn and Specia, 2013; Shah et al., 2013) and employed a squared exponential (SE) kernel with automatic relevance determination (ARD): ! F 1 X xi − x0i 0 2 k(x, x ) = σf exp − , 2 li Independent: B = I. In this setting each task is modelled independently. This is not strictly equivalent to independent model training because the tasks share the same data kernel (and the same hyperparameters); Pooled: B = 1. Here the task identity is ignored. This is equivalent to pooling all datasets in a single task model; Combined: B = 1 + αI. This setting leverages between independent and pooled models. Here, α &gt; 0 is treated as an hyperparameter;"
W14-3338,2006.amta-papers.25,0,0.0117924,"rman-English (de-en). Each contains a different number of source sentences and their human translations, as well as 2-3 versions of machine translations: by a statistical (SMT) system, a rule-based system (RBMT) system and, for en-es/de only, a hybrid system. Source sentences were extracted from tests sets of WMT13 and WMT12, and the translations were produced by top MT systems of each type and a human translator. Labels range from 1 to 3, with 1 indicating a perfect translation and 3, a low quality translation. The purpose of task 1.2 is to predict HTER scores (Human Translation Error Rate) (Snover et al., 2006) using a dataset composed of 896 English-Spanish sentences translated by a MT system and post-edited by a professional translator. Finally, task 1.3 aims at predicting post-editing time, using a subset of 650 sentences from the Task 1.2 dataset. For each task, participants can submit two types of results: scoring and ranking. For scoring, evaluation is made in terms of Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). For ranking, DeltaAvg and Spearman’s rank correlation were used as evaluation metrics. We describe our systems for the WMT14 Shared Task on Quality Estimation (subtask"
W14-3338,2009.mtsummit-papers.16,1,0.852189,"the WMT14 Shared Task on Quality Estimation (subtasks 1.1, 1.2 and 1.3). Our submissions use the framework of Multi-task Gaussian Processes, where we combine multiple datasets in a multi-task setting. Due to the large size of our datasets we also experiment with Sparse Gaussian Processes, which aim to speed up training and prediction by providing sensible sparse approximations. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Bojar et al., 2013). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence. The WMT 2014 QE shared task defined a group of tasks related to QE. In this paper, we describe our submissions for subtasks 1.1, 1.2 and 1.3. Our models are based on Gaussian Processes (GPs) (Rasmussen and Williams, 2006), a non-parametric kernelised probabilistic framework. We propose to combine multiple da"
W14-3338,P13-4014,1,0.916299,"Missing"
W14-3338,2011.eamt-1.12,1,0.933628,"Missing"
W14-3338,W13-2201,1,\N,Missing
W14-3343,P04-1077,0,0.295785,"ramework. For QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) use pseudo-references in the same way to extract a BLEU feature for sentence-level prediction. Feature analysis on a number of datasets showed that this feature contributed the most across all datasets. Louis and Nenkova (2013) apply pseudoreferences for summary evaluation. They use six systems classified as “best systems”, “mediocre systems” or “worst systems” to make the comparison, with ROUGE (Lin and Och, 2004) as quality score. They also experiment with a combination of the “best systems” and the “worst systems”. The use of only “best systems” led to the best results. Examples of “bad summaries” are said not to be very useful because a summary close to the worst systems outputs can mean that either it is bad or it is too different from the best systems outputs in terms of content. Albrecht and Hwa (2008) use pseudo-references to improve MT evaluation by combining them with a single human reference. They show that the use of pseudo-references imThis paper presents the use of consensus among Machine"
W14-3343,J13-2002,0,0.0248237,"od that uses sentence-level prediction models for document-level QE. They also use a pseudo-references-based feature (based in BLEU) and claim that this feature is one of the most powerful in the framework. For QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) use pseudo-references in the same way to extract a BLEU feature for sentence-level prediction. Feature analysis on a number of datasets showed that this feature contributed the most across all datasets. Louis and Nenkova (2013) apply pseudoreferences for summary evaluation. They use six systems classified as “best systems”, “mediocre systems” or “worst systems” to make the comparison, with ROUGE (Lin and Och, 2004) as quality score. They also experiment with a combination of the “best systems” and the “worst systems”. The use of only “best systems” led to the best results. Examples of “bad summaries” are said not to be very useful because a summary close to the worst systems outputs can mean that either it is bad or it is too different from the best systems outputs in terms of content. Albrecht and Hwa (2008) use ps"
W14-3343,P02-1040,0,0.0900983,"etrics are used in such settings as a way of predicting translation quality. While reference translations are not available for QE, previous work has explored the so called pseudoreferences (Soricut and Echihabi, 2010; Soricut et al., 2012; Soricut and Narsale, 2012; Shah et al., 2013). Pseudo-references are alternative translations produced by MT systems different from the system that we intend to predict quality for (Albrecht and Hwa, 2008). These can be used to provide additional features to train QE models. Such features are normally figures resulting from automatic metrics (such as BLEU, Papineni et al. (2002)) computed between pseudo-references and the output of the given MT system. Soricut and Echihabi (2010) explore pseudoreferences for document-level QE prediction to 1 http://www.statmt.org/wmt12/ 342 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 342–347, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics compared to the baselines provided. Section 4 presents some conclusions. proves the correlation with human judgements. Soricut and Echihabi (2010) claim that pseudoreferences should be produced by systems as different as po"
W14-3343,W08-0330,0,0.119763,"sets. Louis and Nenkova (2013) apply pseudoreferences for summary evaluation. They use six systems classified as “best systems”, “mediocre systems” or “worst systems” to make the comparison, with ROUGE (Lin and Och, 2004) as quality score. They also experiment with a combination of the “best systems” and the “worst systems”. The use of only “best systems” led to the best results. Examples of “bad summaries” are said not to be very useful because a summary close to the worst systems outputs can mean that either it is bad or it is too different from the best systems outputs in terms of content. Albrecht and Hwa (2008) use pseudo-references to improve MT evaluation by combining them with a single human reference. They show that the use of pseudo-references imThis paper presents the use of consensus among Machine Translation (MT) systems for the WMT14 Quality Estimation shared task. Consensus is explored here by comparing the MT system output against several alternative machine translations using standard evaluation metrics. Figures extracted from such metrics are used as features to complement baseline prediction models. The hypothesis is that knowing whether the translation of interest is similar or dissim"
W14-3343,2013.mtsummit-papers.21,1,0.94296,"MT system training data is also used as pseudo-references to compute training data-based features. The use of pseudoreferences has been shown to outperform strong baseline results. Soricut and Narsale (2012) propose a method that uses sentence-level prediction models for document-level QE. They also use a pseudo-references-based feature (based in BLEU) and claim that this feature is one of the most powerful in the framework. For QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) use pseudo-references in the same way to extract a BLEU feature for sentence-level prediction. Feature analysis on a number of datasets showed that this feature contributed the most across all datasets. Louis and Nenkova (2013) apply pseudoreferences for summary evaluation. They use six systems classified as “best systems”, “mediocre systems” or “worst systems” to make the comparison, with ROUGE (Lin and Och, 2004) as quality score. They also experiment with a combination of the “best systems” and the “worst systems”. The use of only “best systems” led to the best results. Examples of “bad su"
W14-3343,W05-0909,0,0.0928187,"onsensual information was extracted by using systems submitted to the WMT translation shared tasks of both years. Therefore, for each source sentence in the WMT12/13 data, all translations produced by the participating MT systems of that year were used as pseudo-references. The uedin system outputs for both WMT13 and WMT12 were not considered, since the datasets in Tasks 1.2 and 1.3 were created from translations generated by this system.2 The Asyia Toolkit3 (Gim´enez and M`arquez, 2010) was used to extract the automatic metrics considered as features. BLEU, TER (Snover et al., 2006), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin and Och, 2004) are used in all task variants. For Tasks 1.2 and 1.3 we also use metrics based on syntactic similarities from shallow and dependency parser information (metrics SPOc(*) and DPmHWCM c1, respectively, in Asyia). BLEU is a precision-oriented metric that compares n-grams (n=1-4 in our case) from reference documents against n-grams of the MT output, measuring how close the output of a system is to one or more references. TER (Translation Error Rate) measures the minimum number of edits required to transform the MT output into the closest reference document. METEOR (Me"
W14-3343,2006.amta-papers.25,0,0.278824,"air had 954 English sentences and 3,816 Spanish sentences. In the source file, the English sentences were repeated in batches of 954 sentences. Based on that, we assumed that in the target file each set of 954 translations in sequence corresponded to a given MT system (or human). For each system (human translation is considered as a system, since we do not know the order of the translations), we calculate the consensual information considering the other 2-3 systems available as pseudo-references. The quality scores for Task 1.2 and Task 1.3 were computed as HTER (Human Translation Error Rate (Snover et al., 2006)) and post-editing time, respectively, for both scoring and ranking. This paper describes the use of consensual information for the WMT14 QE shared task (USHEFF-consensus system), simulating a scenario where we do not know the quality of the pseudo-references, nor the characteristics of any MT systems (the system of interest or the systems which generated the pseudo-references). We participated in all variants of Task 1, sentence-level QE, for both for scoring and ranking. Section 2 explains how we extracted consensual information for all tasks. Section 3 shows our official results 343 3 The d"
W14-3343,W12-3121,0,0.0871667,"Specia Department of Computer Science, University of Sheffield Regent Court, 211 Portobello, Sheffield, S1 4DP, UK {c.scarton,l.specia}@sheffield.ac.uk Abstract rank outputs from an MT system. The pseudoreferences-based features are BLEU scores extracted by comparing the output of the MT system under investigation and the output of an offthe-shelf MT system, for both the target and the source texts. The statistical MT system training data is also used as pseudo-references to compute training data-based features. The use of pseudoreferences has been shown to outperform strong baseline results. Soricut and Narsale (2012) propose a method that uses sentence-level prediction models for document-level QE. They also use a pseudo-references-based feature (based in BLEU) and claim that this feature is one of the most powerful in the framework. For QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) use pseudo-references in the same way to extract a BLEU feature for sentence-level prediction. Feature analysis on a number of datasets showed that this feature contributed the most across"
W14-3343,W12-3118,0,0.192802,"Missing"
W14-3343,P13-4014,1,0.918348,"Missing"
W14-3343,P10-1063,0,\N,Missing
W15-2507,W12-3156,0,0.226705,"data for quality estimation purposes, but these are at the word level, and the process was guided by post-editing data. They derived an error distribution of MT output by inspecting post editing data. We do not have a similar way of inducing a distribution of errors for coherence. A large amount of post editings of entire documents would be needed, and it still be difficult to isolate which of the edits relate to coherence errors. There has been previous work in the area of lexical cohesion in MT (Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012). Lexical cohesion is part of coherence, as it looks at the linguistic elements which hold a text together. However, there has been very little work in the wider area of coherence as a whole. Besides lexical cohesion, another discourse related phenomenon that has been addressed in MT is reference resolution. As detailed in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010). More recent work includes that of Guillou (2012), which highlights the differences of coreference dep"
W15-2507,2010.iwslt-papers.10,0,0.364938,"n the area of lexical cohesion in MT (Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012). Lexical cohesion is part of coherence, as it looks at the linguistic elements which hold a text together. However, there has been very little work in the wider area of coherence as a whole. Besides lexical cohesion, another discourse related phenomenon that has been addressed in MT is reference resolution. As detailed in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010). More recent work includes that of Guillou (2012), which highlights the differences of coreference depending on the language pair. Since then Hardmeier et al. (2013b) have used a new approach for anaphora resolution via neural networks which achieves comparable results to a standard anaphora resolutions system, but without annotated data. Recently work has begun on negation in MT, particularly by Wetzel and Bond (2012; Fancellu and Webber (2014). There is also work focusing on evaluation against reference translations (Guzm´an et al., 2014) based on the comparison"
W15-2507,P13-4033,0,0.418241,"ta artificially manipulated to contain errors of coherence common in MT output. Such a corpus could then be used as a benchmark for coherence models in MT, and potentially as training data for coherence models in supervised settings. 1 Introduction Discourse information has only recently started to attract attention in MT, particularly in Statistical Machine Translation (SMT), the focus of this paper. Most decoders work on a sentence by sentence basis, isolated from context, due to both modelling and computational complexity. An exception are approaches to multi-pass decoding, such as Docent (Hardmeier et al., 2013a). Our work focuses on an issue which has not yet been much explored in MT, that of coherence. Coherence is undeniably a cognitive process, and we will limit our remit to the extent that this process is guided by linguistic elements discernible in the discourse. While it does include cohesion, it is wider in terms of describing how a text becomes semantically meaningful overall, and additionally spans the entire document. We are interested in capturing aspects of coherence as defined by Grosz and Sidner (1986), based on the attentional state, intentional structure and linguistic 52 Proceeding"
W15-2507,P12-2023,0,0.0308778,"pair. Since then Hardmeier et al. (2013b) have used a new approach for anaphora resolution via neural networks which achieves comparable results to a standard anaphora resolutions system, but without annotated data. Recently work has begun on negation in MT, particularly by Wetzel and Bond (2012; Fancellu and Webber (2014). There is also work focusing on evaluation against reference translations (Guzm´an et al., 2014) based on the comparison between discourse trees in MT versus reference. This information was found to improve evaluation of MT output. Drawing from research on topic modelling (Eidelman et al., 2012), where lexical probabilities conditioned on topics are computed, Xiong and Zhang (2013) attempt to improve coherence based using topic information. They determine the topic of the source sentence and project it onto the target as a feature to ensure the decoder selects the appropriate words. They observed slight improvements in terms of general standard metrics, indicating perhaps that these metrics fail to account for discourse improvements. As far as we aware, no attempts have been made to create a corpus exhibiting incoherence, other than by shuffling ordered sentences. There has been work"
W15-2507,D13-1037,0,0.413157,"ta artificially manipulated to contain errors of coherence common in MT output. Such a corpus could then be used as a benchmark for coherence models in MT, and potentially as training data for coherence models in supervised settings. 1 Introduction Discourse information has only recently started to attract attention in MT, particularly in Statistical Machine Translation (SMT), the focus of this paper. Most decoders work on a sentence by sentence basis, isolated from context, due to both modelling and computational complexity. An exception are approaches to multi-pass decoding, such as Docent (Hardmeier et al., 2013a). Our work focuses on an issue which has not yet been much explored in MT, that of coherence. Coherence is undeniably a cognitive process, and we will limit our remit to the extent that this process is guided by linguistic elements discernible in the discourse. While it does include cohesion, it is wider in terms of describing how a text becomes semantically meaningful overall, and additionally spans the entire document. We are interested in capturing aspects of coherence as defined by Grosz and Sidner (1986), based on the attentional state, intentional structure and linguistic 52 Proceeding"
W15-2507,P11-2022,0,0.144231,"n Karin Sim Smith§ , Wilker Aziz† and Lucia Specia§ §Department of Computer Science, University of Sheffield, UK {kmsimsmith1,l.specia}@sheffield.ac.uk †Institute for Logic, Language and Computation University of Amsterdam, The Netherlands w.aziz@uva.nl Abstract structure of discourse. As a result, we believe that a coherent discourse should have a context and a focus, be characterised by appropriate coherence relations, and structured in a logical manner. Previous computational models for assessing coherence in a monolingual context have covered entity transitions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Burstein et al., 2010; Guinaudeau and Strube, 2013), syntactic patterns (Louis and Nenkova, 2012), discourse relations (Lin et al., 2011), distributed sentence representations (Li and Hovy, 2014) and lexical chains (Somasundaran et al., 2014). For evaluation, these studies in coherence have typically used automatically summarized texts, or texts with sentences artificially shuffled as their ‘incoherent’ data. The latter is an example of artificially created labelled data, distorting the ordered logic of the text and thus affecting some aspects of coherence. However, it is inadequate for our"
W15-2507,E14-1063,0,0.126945,"in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010). More recent work includes that of Guillou (2012), which highlights the differences of coreference depending on the language pair. Since then Hardmeier et al. (2013b) have used a new approach for anaphora resolution via neural networks which achieves comparable results to a standard anaphora resolutions system, but without annotated data. Recently work has begun on negation in MT, particularly by Wetzel and Bond (2012; Fancellu and Webber (2014). There is also work focusing on evaluation against reference translations (Guzm´an et al., 2014) based on the comparison between discourse trees in MT versus reference. This information was found to improve evaluation of MT output. Drawing from research on topic modelling (Eidelman et al., 2012), where lexical probabilities conditioned on topics are computed, Xiong and Zhang (2013) attempt to improve coherence based using topic information. They determine the topic of the source sentence and project it onto the target as a feature to ensure the decoder selects the appropriate words. They obse"
W15-2507,W05-0820,0,0.0193956,"13) The reference translation has a clausal pattern which is more cohesive to the English reader. coherent and incoherent texts. This will be done in a flexible manner, such that the incoherent documents can be created for a variety of (coherent) input texts. Moreover they can be created for specific types of errors. The quality of MT output varies greatly from one language pair and MT system to another. For example, the output from a French-English MT system trained in very large collections is superior to that of, for example, an English-Finnish system trained on smaller quantities of data (Koehn and Monz, 2005; Bojar et al., 2015).The errors encountered also vary, depending on the language pair, in particular for aspects such as discourse markers and syntax. Some of these error patterns are more relevant for particular language pairs, e.g. negation for French-English, which is otherwise a wellperforming language pair. We propose to inject errors programmatically in a systematic manner, as detailed below. Negation MT systems often miss the focus of the negation. This results in incorrectly transferred negations that affect coherence (Wetzel and Bond, 2012; Fancellu and Webber, 2014). src: ‘Aucun dir"
W15-2507,W15-1301,0,0.0138909,"ersions of the corpus with different proportions of errors will be created. We will inject errors systematically and incrementally to vary the degree and location of the errors. The errors will be introduced systematically via pattern-matching, and as highlighted by Brockett et al. (2006), may not be distributed in a natural way. Artificially generating coherence errors Significant work has already been done in the areas of coreference resolution (Michal, 2011; Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2013b; Guillou, 2012) and negation (Wetzel and Bond, 2012; Fancellu and Webber, 2015; Fancellu and Webber, 2014) in MT. In our corpus we will focus on less studied issues and limit ourselves to targeting coherence more specifically than cohesion. The proposed framework will take as input well-formed documents that are determined ‘coherent’ (i.e. grammatically correct and coherent) and then artificially distort them in ways (detailed below) that directly affect coherence in the manner that an MT system would. The resulting texts will make a corpus of ‘incoherent’ texts for assessing the ability of models to discriminate between 4.2 Error Injection We will inject errors of the"
W15-2507,W10-1737,0,0.0627839,"Missing"
W15-2507,D14-1218,0,0.0970977,"rsity of Amsterdam, The Netherlands w.aziz@uva.nl Abstract structure of discourse. As a result, we believe that a coherent discourse should have a context and a focus, be characterised by appropriate coherence relations, and structured in a logical manner. Previous computational models for assessing coherence in a monolingual context have covered entity transitions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Burstein et al., 2010; Guinaudeau and Strube, 2013), syntactic patterns (Louis and Nenkova, 2012), discourse relations (Lin et al., 2011), distributed sentence representations (Li and Hovy, 2014) and lexical chains (Somasundaran et al., 2014). For evaluation, these studies in coherence have typically used automatically summarized texts, or texts with sentences artificially shuffled as their ‘incoherent’ data. The latter is an example of artificially created labelled data, distorting the ordered logic of the text and thus affecting some aspects of coherence. However, it is inadequate for our task, as MT preserves the sentence ordering, but suffers from other aspects of incoherence. Moreover, while the MT output can potentially be considered ‘incoherent’, it contains a multitude of prob"
W15-2507,E14-3013,0,0.0178387,"iong and Zhang (2013) attempt to improve coherence based using topic information. They determine the topic of the source sentence and project it onto the target as a feature to ensure the decoder selects the appropriate words. They observed slight improvements in terms of general standard metrics, indicating perhaps that these metrics fail to account for discourse improvements. As far as we aware, no attempts have been made to create a corpus exhibiting incoherence, other than by shuffling ordered sentences. There has been work in other areas to introduce errors in correct texts. For example, Felice and Yuan (2014) and Brockett et al. (2006) inject grammatical errors common to non-native speakers of English in good quality texts. Felice and Yuan (2014) use existing corrected corpora to derive the error distribution, while Brockett et al. (2006) adopt a de3 Issues of incoherence in MT systems Current MT approaches suffer from a lack of linguistic information at various stages (modelling, decoding, pruning) causing the lack of coherence in the output. Below we describe a number of issues that are generally viewed as coherence issues which MT approaches deal poorly with and which have also been the subject"
W15-2507,P11-1100,0,0.271648,"Missing"
W15-2507,J86-3001,0,0.333161,"onal complexity. An exception are approaches to multi-pass decoding, such as Docent (Hardmeier et al., 2013a). Our work focuses on an issue which has not yet been much explored in MT, that of coherence. Coherence is undeniably a cognitive process, and we will limit our remit to the extent that this process is guided by linguistic elements discernible in the discourse. While it does include cohesion, it is wider in terms of describing how a text becomes semantically meaningful overall, and additionally spans the entire document. We are interested in capturing aspects of coherence as defined by Grosz and Sidner (1986), based on the attentional state, intentional structure and linguistic 52 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 52–58, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. 2 Existing work terministic approach based on hand-crafted rules. Logacheva and Specia (2015) inject various types of errors to generate negative data for quality estimation purposes, but these are at the word level, and the process was guided by post-editing data. They derived an error distribution of MT output by inspecting post editing dat"
W15-2507,W15-4907,1,0.809712,"ernible in the discourse. While it does include cohesion, it is wider in terms of describing how a text becomes semantically meaningful overall, and additionally spans the entire document. We are interested in capturing aspects of coherence as defined by Grosz and Sidner (1986), based on the attentional state, intentional structure and linguistic 52 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 52–58, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. 2 Existing work terministic approach based on hand-crafted rules. Logacheva and Specia (2015) inject various types of errors to generate negative data for quality estimation purposes, but these are at the word level, and the process was guided by post-editing data. They derived an error distribution of MT output by inspecting post editing data. We do not have a similar way of inducing a distribution of errors for coherence. A large amount of post editings of entire documents would be needed, and it still be difficult to isolate which of the edits relate to coherence errors. There has been previous work in the area of lexical cohesion in MT (Wong and Kit, 2012; Xiong et al., 2013a; Xio"
W15-2507,E12-3001,0,0.265322,"2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012). Lexical cohesion is part of coherence, as it looks at the linguistic elements which hold a text together. However, there has been very little work in the wider area of coherence as a whole. Besides lexical cohesion, another discourse related phenomenon that has been addressed in MT is reference resolution. As detailed in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010). More recent work includes that of Guillou (2012), which highlights the differences of coreference depending on the language pair. Since then Hardmeier et al. (2013b) have used a new approach for anaphora resolution via neural networks which achieves comparable results to a standard anaphora resolutions system, but without annotated data. Recently work has begun on negation in MT, particularly by Wetzel and Bond (2012; Fancellu and Webber (2014). There is also work focusing on evaluation against reference translations (Guzm´an et al., 2014) based on the comparison between discourse trees in MT versus reference. This information was found to"
W15-2507,P13-1010,0,0.0930757,"§ §Department of Computer Science, University of Sheffield, UK {kmsimsmith1,l.specia}@sheffield.ac.uk †Institute for Logic, Language and Computation University of Amsterdam, The Netherlands w.aziz@uva.nl Abstract structure of discourse. As a result, we believe that a coherent discourse should have a context and a focus, be characterised by appropriate coherence relations, and structured in a logical manner. Previous computational models for assessing coherence in a monolingual context have covered entity transitions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Burstein et al., 2010; Guinaudeau and Strube, 2013), syntactic patterns (Louis and Nenkova, 2012), discourse relations (Lin et al., 2011), distributed sentence representations (Li and Hovy, 2014) and lexical chains (Somasundaran et al., 2014). For evaluation, these studies in coherence have typically used automatically summarized texts, or texts with sentences artificially shuffled as their ‘incoherent’ data. The latter is an example of artificially created labelled data, distorting the ordered logic of the text and thus affecting some aspects of coherence. However, it is inadequate for our task, as MT preserves the sentence ordering, but suff"
W15-2507,D12-1106,0,0.472344,"heffield, UK {kmsimsmith1,l.specia}@sheffield.ac.uk †Institute for Logic, Language and Computation University of Amsterdam, The Netherlands w.aziz@uva.nl Abstract structure of discourse. As a result, we believe that a coherent discourse should have a context and a focus, be characterised by appropriate coherence relations, and structured in a logical manner. Previous computational models for assessing coherence in a monolingual context have covered entity transitions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Burstein et al., 2010; Guinaudeau and Strube, 2013), syntactic patterns (Louis and Nenkova, 2012), discourse relations (Lin et al., 2011), distributed sentence representations (Li and Hovy, 2014) and lexical chains (Somasundaran et al., 2014). For evaluation, these studies in coherence have typically used automatically summarized texts, or texts with sentences artificially shuffled as their ‘incoherent’ data. The latter is an example of artificially created labelled data, distorting the ordered logic of the text and thus affecting some aspects of coherence. However, it is inadequate for our task, as MT preserves the sentence ordering, but suffers from other aspects of incoherence. Moreove"
W15-2507,P14-1065,0,0.0355369,"Missing"
W15-2507,W12-4203,0,0.488354,"resolution. As detailed in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010). More recent work includes that of Guillou (2012), which highlights the differences of coreference depending on the language pair. Since then Hardmeier et al. (2013b) have used a new approach for anaphora resolution via neural networks which achieves comparable results to a standard anaphora resolutions system, but without annotated data. Recently work has begun on negation in MT, particularly by Wetzel and Bond (2012; Fancellu and Webber (2014). There is also work focusing on evaluation against reference translations (Guzm´an et al., 2014) based on the comparison between discourse trees in MT versus reference. This information was found to improve evaluation of MT output. Drawing from research on topic modelling (Eidelman et al., 2012), where lexical probabilities conditioned on topics are computed, Xiong and Zhang (2013) attempt to improve coherence based using topic information. They determine the topic of the source sentence and project it onto the target as a feature to ensure the decoder selects the"
W15-2507,W12-0117,0,0.338111,"p¨ater an den Folgen der schweren Verletzungen gestorben.’ mt: ‘The victim was later at the consequences of the serious injuries died.’ ref: ‘The victim later died as a result of the serious injuries.’ (Bojar et al., 2014). This can affect the understanding of the sentence, the overall logic of it in the context of the surrounding sentences, or simply require a reread which itself is indicative of impaired coherence. Discourse connectives Discourse connectives are vital for the correct understanding of discourse. Yet in MT systems these can be incorrect or missing (Meyer and Pol´akov´a, 2013; Meyer and Popescu-Belis, 2012; Meyer et al., 2011; Steele, 2015). In particular, where discourse connectives are ambiguous, e.g. those which can be temporal 54 src: ‘Bereits im Jahr 1925 wurde in Polen eine Eisenbahn-Draisine gebaut, f¨ur die ein Raketenantrieb geplant war. Der Autor des Entwurfs und die Details dieses Vorhabens blieben leider unbekannt.’ mt: ‘Already in 1925 a railway trolley was built in Poland, for which a rocket was planned. The author of the design and the details of the project remained unfortunately unknown.’ ref: In 1925, Poland had already built a handcar which was supposed to be fitted with a ro"
W15-2507,D12-1097,0,0.442224,"d-crafted rules. Logacheva and Specia (2015) inject various types of errors to generate negative data for quality estimation purposes, but these are at the word level, and the process was guided by post-editing data. They derived an error distribution of MT output by inspecting post editing data. We do not have a similar way of inducing a distribution of errors for coherence. A large amount of post editings of entire documents would be needed, and it still be difficult to isolate which of the edits relate to coherence errors. There has been previous work in the area of lexical cohesion in MT (Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012). Lexical cohesion is part of coherence, as it looks at the linguistic elements which hold a text together. However, there has been very little work in the wider area of coherence as a whole. Besides lexical cohesion, another discourse related phenomenon that has been addressed in MT is reference resolution. As detailed in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010)"
W15-2507,W11-2022,0,0.129008,"en Verletzungen gestorben.’ mt: ‘The victim was later at the consequences of the serious injuries died.’ ref: ‘The victim later died as a result of the serious injuries.’ (Bojar et al., 2014). This can affect the understanding of the sentence, the overall logic of it in the context of the surrounding sentences, or simply require a reread which itself is indicative of impaired coherence. Discourse connectives Discourse connectives are vital for the correct understanding of discourse. Yet in MT systems these can be incorrect or missing (Meyer and Pol´akov´a, 2013; Meyer and Popescu-Belis, 2012; Meyer et al., 2011; Steele, 2015). In particular, where discourse connectives are ambiguous, e.g. those which can be temporal 54 src: ‘Bereits im Jahr 1925 wurde in Polen eine Eisenbahn-Draisine gebaut, f¨ur die ein Raketenantrieb geplant war. Der Autor des Entwurfs und die Details dieses Vorhabens blieben leider unbekannt.’ mt: ‘Already in 1925 a railway trolley was built in Poland, for which a rocket was planned. The author of the design and the details of the project remained unfortunately unknown.’ ref: In 1925, Poland had already built a handcar which was supposed to be fitted with a rocket engine. Unfortu"
W15-2507,D13-1163,0,0.212259,"acheva and Specia (2015) inject various types of errors to generate negative data for quality estimation purposes, but these are at the word level, and the process was guided by post-editing data. They derived an error distribution of MT output by inspecting post editing data. We do not have a similar way of inducing a distribution of errors for coherence. A large amount of post editings of entire documents would be needed, and it still be difficult to isolate which of the edits relate to coherence errors. There has been previous work in the area of lexical cohesion in MT (Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012). Lexical cohesion is part of coherence, as it looks at the linguistic elements which hold a text together. However, there has been very little work in the wider area of coherence as a whole. Besides lexical cohesion, another discourse related phenomenon that has been addressed in MT is reference resolution. As detailed in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010). More recent work i"
W15-2507,potet-etal-2012-collection,0,0.0699691,"Missing"
W15-2507,C14-1090,0,0.0496657,"ziz@uva.nl Abstract structure of discourse. As a result, we believe that a coherent discourse should have a context and a focus, be characterised by appropriate coherence relations, and structured in a logical manner. Previous computational models for assessing coherence in a monolingual context have covered entity transitions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Burstein et al., 2010; Guinaudeau and Strube, 2013), syntactic patterns (Louis and Nenkova, 2012), discourse relations (Lin et al., 2011), distributed sentence representations (Li and Hovy, 2014) and lexical chains (Somasundaran et al., 2014). For evaluation, these studies in coherence have typically used automatically summarized texts, or texts with sentences artificially shuffled as their ‘incoherent’ data. The latter is an example of artificially created labelled data, distorting the ordered logic of the text and thus affecting some aspects of coherence. However, it is inadequate for our task, as MT preserves the sentence ordering, but suffers from other aspects of incoherence. Moreover, while the MT output can potentially be considered ‘incoherent’, it contains a multitude of problems, which are not all due to lack of coherenc"
W15-2507,P98-2202,0,0.0661022,"e elements, in terms of cue words, and their organisation. A comparison of the discourse connectives in the MT and the Human Translation (HT) will be established, and where these differ, a syntactic check is made automatically (Pitler and Nenkova, ) to establish if the connective is a synonym or incorrect. We can also refer to the discourse connectives in the original source text, and automatically check, for example, if the correct sense of the connective has been transferred. These can be identified from a list compiled from appropriate resources (e.g. DiMLex for German, LexConn for French)(Stede and Umbach, 1998; Roze and Danlos, 2012) and a list of problematic ones derived e.g. from work by (Meyer and Popescu-Belis, 2012; Meyer and Pol´akov´a, 2013) for French. 5 Conclusion We can parse the discourse tree structure and extract grammatical information using the Stanford parser2 and POS tagger3 , before distorting the parse tree by swapping nodes at the relevant level. References We have introduced our initiative for artificially generating a corpus with coherence errors from well-formed data that specifically simulate coherence issues in MT. Other possible direction could be to use an nbest list, tak"
W15-2507,N15-2015,0,0.11992,"orben.’ mt: ‘The victim was later at the consequences of the serious injuries died.’ ref: ‘The victim later died as a result of the serious injuries.’ (Bojar et al., 2014). This can affect the understanding of the sentence, the overall logic of it in the context of the surrounding sentences, or simply require a reread which itself is indicative of impaired coherence. Discourse connectives Discourse connectives are vital for the correct understanding of discourse. Yet in MT systems these can be incorrect or missing (Meyer and Pol´akov´a, 2013; Meyer and Popescu-Belis, 2012; Meyer et al., 2011; Steele, 2015). In particular, where discourse connectives are ambiguous, e.g. those which can be temporal 54 src: ‘Bereits im Jahr 1925 wurde in Polen eine Eisenbahn-Draisine gebaut, f¨ur die ein Raketenantrieb geplant war. Der Autor des Entwurfs und die Details dieses Vorhabens blieben leider unbekannt.’ mt: ‘Already in 1925 a railway trolley was built in Poland, for which a rocket was planned. The author of the design and the details of the project remained unfortunately unknown.’ ref: In 1925, Poland had already built a handcar which was supposed to be fitted with a rocket engine. Unfortunately, both th"
W15-2507,W10-2602,0,0.0432997,"pes of errors to generate negative data for quality estimation purposes, but these are at the word level, and the process was guided by post-editing data. They derived an error distribution of MT output by inspecting post editing data. We do not have a similar way of inducing a distribution of errors for coherence. A large amount of post editings of entire documents would be needed, and it still be difficult to isolate which of the edits relate to coherence errors. There has been previous work in the area of lexical cohesion in MT (Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012). Lexical cohesion is part of coherence, as it looks at the linguistic elements which hold a text together. However, there has been very little work in the wider area of coherence as a whole. Besides lexical cohesion, another discourse related phenomenon that has been addressed in MT is reference resolution. As detailed in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010). More recent work includes that of Guillou (2012), which h"
W15-2507,N10-1099,0,\N,Missing
W15-2507,P06-1032,0,\N,Missing
W15-2507,J08-1001,0,\N,Missing
W15-2507,C98-2197,0,\N,Missing
W15-2507,W15-3001,1,\N,Missing
W15-2507,2015.eamt-1.8,1,\N,Missing
W15-2507,W13-2201,1,\N,Missing
W15-2507,W13-3306,0,\N,Missing
W15-3001,W05-0909,0,0.0473932,"asks 1 and 2 provide the same dataset with English-Spanish translations generated by the statistical machine translation (SMT) system, while Task 3 provides two different datasets, for two language pairs: English-German (EN-DE) and German-English (DE-EN) translations taken from all participating systems in WMT13 (Bojar et al., 2013). These datasets were annotated with different labels for quality: for Tasks 1 and 2, the labels were automatically derived from the post-editing of the machine translation output, while for Task 3, scores were computed based on reference translations using Meteor (Banerjee and Lavie, 2005). Any external resource, including additional quality estimation training data, could be used by participants (no distinction between open and close tracks was made). As presented in Section 4.1, participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 4.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scor"
W15-3001,2011.mtsummit-papers.35,0,0.348896,"Missing"
W15-3001,W13-2241,1,0.851171,"Missing"
W15-3001,P13-2097,1,0.801912,"Missing"
W15-3001,W13-2242,0,0.0386206,"Missing"
W15-3001,W14-3339,0,0.0452066,"Missing"
W15-3001,W15-3035,0,0.054515,"Missing"
W15-3001,W11-2103,1,0.524514,"Missing"
W15-3001,W13-2201,1,0.499374,"Missing"
W15-3001,W14-3302,1,0.498006,"Missing"
W15-3001,W14-3340,1,0.54686,"Missing"
W15-3001,W15-3007,0,0.0166876,"Missing"
W15-3001,W15-3006,1,0.827804,"Missing"
W15-3001,W07-0718,1,0.664541,"om 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (Stanojevi´c et al., 2015a,b)."
W15-3001,2014.amta-researchers.13,0,0.0200349,"Missing"
W15-3001,W08-0309,1,0.406319,"2. machine translation and automatic evaluation or prediction of translation quality. 2 Overview of the Translation Task The recurring task of the workshop examines translation between English and other languages. As in the previous years, the other languages include German, French, Czech and Russian. Finnish replaced Hindi as the special language this year. Finnish is a lesser resourced language compared to the other languages and has challenging morphological properties. Finnish represents also a different language family that we had not tackled since we included Hungarian in 2008 and 2009 (Callison-Burch et al., 2008, 2009). We created a test set for each language pair by translating newspaper articles and provided training data, except for French, where the test set was drawn from user-generated comments on the news articles. 2.1 2.3 We received 68 submissions from 24 institutions. The participating institutions and their entry names are listed in Table 2; each system did not necessarily appear in all translation tasks. We also included 1 commercial off-the-shelf MT system and 6 online statistical MT systems, which we anonymized. For presentation of the results, systems are treated as either constrained"
W15-3001,W15-3025,1,0.914094,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W10-1703,1,0.163282,"Missing"
W15-3001,P15-2026,1,0.797338,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W12-3102,1,0.571688,"ator agreement, both for inter- and intra-annotator agreement scores. not included to make the graphs viewable). The plots cleary suggest that a fair comparison of systems of different kinds cannot rely on automatic scores. Rule-based systems receive a much lower BLEU score than statistical systems (see for instance English–German, e.g., PROMT- RULE). The same is true to a lesser degree for statistical syntax-based systems (see English–German, UEDIN - SYNTAX ) and online systems that were not tuned to the shared task (see Czech–English, CU TECTO vs. the cluster of tuning task systems TT*). 4 (Callison-Burch et al., 2012; Bojar et al., 2013, 2014), with tasks including both sentence and word-level estimation, using new training and test datasets, and an additional task: document-level prediction. The goals of this year’s shared task were: • Advance work on sentence- and wordlevel quality estimation by providing larger datasets. • Investigate the effectiveness of quality labels, features and learning methods for documentlevel prediction. Quality Estimation Task • Explore differences between sentence-level and document-level prediction. The fourth edition of the WMT shared task on quality estimation (QE) of mac"
W15-3001,W15-3008,0,0.0135172,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W09-0401,1,0.251315,"Missing"
W15-3001,W15-3009,0,0.0460438,"Missing"
W15-3001,W15-3010,0,0.035772,"Missing"
W15-3001,P10-4002,0,0.00861255,"t sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features:"
W15-3001,W15-3011,0,0.0373266,"Missing"
W15-3001,W15-3036,0,0.0738999,"Missing"
W15-3001,W15-3012,0,0.0168435,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W15-4903,0,0.0623526,"Missing"
W15-3001,W15-3013,1,0.843414,"Missing"
W15-3001,W11-2123,0,0.0240468,"27,101 5,966 8,816 SRC 13,701 3,765 5,307 Lemmas TGT PE 7,624 7,689 2,810 2,819 3,778 3,814 Table 18: Data statistics. and classifying each word of a sentence as good or bad. An automatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee e"
W15-3001,W08-0509,0,0.0268564,"ere collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specific challenges to the participating systems. As discussed in Section 5.4, the results of this pilot task can be partially explained in light of such challenges. This dataset, however, has three major advantages that made it sui"
W15-3001,W15-3014,0,0.0344469,"Missing"
W15-3001,P15-1174,0,0.0105686,"415 for test. Since no human annotation exists for the quality of entire paragraphs (or documents), Meteor against reference translations was used as quality label for this task. Meteor was calculated using its implementation within the Asyia toolkit, with the following settings: exact match, tokenised and case insensitive (Gim´enez and M`arquez, 2010). guage pairs. All systems were significantly better than the baseline. However, the difference between the baseline system and all submissions was much lower in the scoring evaluation than in the ranking evaluation. Following the suggestion in (Graham, 2015), Table 16 shows an alternative ranking of systems considering Pearson’s r correlation results. The alternative ranking differs from the official ranking in terms of MAE: for EN-DE, RTMDCU/RTM-FS-SVR is no longer in the winning group, while for DE-EN, USHEF/QUEST-DISCBO and USAAR-USHEF/BFF did not show statistically significant difference against the baseline. However, as with Task 1 these results are the same as the official ones in terms of DeltaAvg. 4.6 Discussion In what follows, we discuss the main findings of this year’s shared task based on the goals we had previously identified for it."
W15-3001,W04-3250,1,0.485885,"f the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara et al. (2011). The APE systems are built-in an incremental manner. At each stage of the APE pipeline, the best configuration of a component is decided and then used in the next stage. The APE pipeline begins with the selection of the best language model from several language models trained on different types and quantities of data. The next stage addresses the possible"
W15-3001,2005.mtsummit-papers.11,1,0.0759255,"(Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features: – The order of the highest order n-gram which starts or ends with the target token. – Backoff behaviour of the n-grams (ti−2 , ti−1 , ti ), (ti−1 , ti , ti+1 ), (ti , ti+1 , ti+2 ), where"
W15-3001,W15-3039,1,0.80659,"Missing"
W15-3001,J10-4005,0,0.0619684,"Missing"
W15-3001,J82-2005,0,0.818658,"Missing"
W15-3001,W14-3342,0,0.0353204,"t although the system is referred to as “baseline”, it is in fact a strong system. It has proved robust across a range of language pairs, MT systems, and text domains for predicting various forms of post-editing effort (Callison-Burch et al., 2012; Bojar et al., 2013, 2014). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool9 . For the baseline system we used a number of features that have been found the most informative in previous research on word-level quality estimation. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 25 features: Baseline systems Sentence-level baseline system: For Task 1, Q U E ST7 (Specia et al., 2013) was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), but the length of a sentence might influence the probability of a word being incorrect. • Number of tokens in the source and target sentences. • Ave"
W15-3001,W13-2248,0,0.0824189,"Missing"
W15-3001,W06-3114,1,0.427665,"translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (S"
W15-3001,W15-3015,0,0.0443946,"Missing"
W15-3001,W15-3016,0,0.0442638,"Missing"
W15-3001,W15-3037,0,0.0800739,"Missing"
W15-3001,P03-1021,0,0.0587969,"utomatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara"
W15-3001,padro-stanilovsky-2012-freeling,0,0.011024,"o rankings are not identical, none of the systems was particularly penalized by the case sensitive evaluation. Indeed, individual differences in the two modes are always close to the same value (∼ 0.7 TER difference) measured for the two baselines. USAAR-SAPE. The USAAR-SAPE system (Pal et al., 2015b) is designed with three basic components: corpus preprocessing, hybrid word alignment and a state-of-the-art phrase-based SMT system integrated with the hybrid word alignment. The preprocessing of the training corpus is carried out by stemming the Spanish MT output and the PE data using Freeling (Padr and Stanilovsky, 2012). The hybrid word alignment method combines different kinds of word alignment: GIZA++ word alignment with the 31 ID Baseline FBK Primary LIMSI Primary USAAR-SAPE LIMSI Contrastive Abu-MaTran Primary FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Avg. TER 22.913 23.228 23.331 23.426 23.573 23.639 23.649 23.839 24.715 ID Baseline LIMSI Primary FBK Primary USAAR-SAPE Abu-MaTran Primary LIMSI Contrastive FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Table 20: Official results for the WMT15 Automatic Post-editing task – average TER (↓) case sensitive. Table 21: Official"
W15-3001,W15-3038,0,0.0668224,"Missing"
W15-3001,W13-2814,0,0.0155219,"ject (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniews"
W15-3001,W07-0734,0,0.0277649,"Abu-MaTran FBK LIMSI USAAR-SAPE Participating team Abu-MaTran Project (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of"
W15-3001,W15-3017,0,0.0328586,"Missing"
W15-3001,W15-3026,0,0.0488223,"Missing"
W15-3001,W15-3040,1,0.889327,"HIDDEN Participating team Dublin City University, Ireland and University of Sheffield, UK (Logacheva et al., 2015) Heidelberg University, Germany (Kreutzer et al., 2015) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois, 2015) Dublin City University, Ireland (Bicici et al., 2015) Shenyang Aerospace University, China (Shang et al., 2015) University of Sheffield Team 1, UK (Shah et al., 2015) Alicant University, Spain (Espl`a-Gomis et al., 2015a) Ghent University, Belgium (Tezcan et al., 2015) University of Sheffield, UK and Saarland University, Germany (Scarton et al., 2015a) University of Sheffield, UK (Scarton et al., 2015a) Undisclosed Table 7: Participants in the WMT15 quality estimation shared task. one from the official training data. Pseudoreferences were produced by three online systems. These features measure the intersection between n-gram sets of the target sentence and of the pseudo-references. Three sets of features were extracted from each online system, and a fourth feature was extracted measuring the inter-agreement among the three online systems and the target system. and fine-tuned for the quality estimation classification task by back-propagat"
W15-3001,W15-4916,1,0.80858,"Missing"
W15-3001,P02-1040,0,0.108957,"ndia (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniewski et al., 2015). The first one is based on the approach of Simard et al. (2007) and considers the APE task as a monolingual translation between a translation hypothesis and its post-edition. This straightforward approach does not succeed in imp"
W15-3001,2012.eamt-1.34,0,0.0799883,"Missing"
W15-3001,W15-3023,0,0.0267499,"Missing"
W15-3001,W15-3018,0,0.0412931,"Missing"
W15-3001,W15-3041,1,0.847113,"Missing"
W15-3001,potet-etal-2012-collection,0,0.011237,"Missing"
W15-3001,W15-3042,0,0.0858502,"Missing"
W15-3001,W15-3019,0,0.0362545,"Missing"
W15-3001,N07-1064,0,0.644928,"nsitive) are reported in Tables 20 and 21. • The target (TGT) is a tokenized Spanish translation of the source, produced by an unknown MT system; • The human post-edition (PE) is a manuallyrevised version of the target. PEs were collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specifi"
W15-3001,W15-3022,0,0.0629217,"Missing"
W15-3001,P15-4020,1,0.0689693,"://github.com/lspecia/quest 14 http://scikit-learn.org/ https://github.com/qe-team/marmot • Target token, its left and right contexts of one word. Document-level baseline system: For Task 3, the baseline features for sentence-level prediction were used. These are aggregated by summing or averaging their values for the entire document. Features that were summed: number of tokens in the source and target sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boole"
W15-3001,W15-3027,0,0.0625354,"Missing"
W15-3001,P13-4014,1,0.762876,"Missing"
W15-3001,2013.mtsummit-papers.15,0,0.055239,"these rules is based on an analysis of the most frequent error corrections and aims at: i) predicting word case; ii) predicting exclamation and interrogation marks; and iii) predicting verbal endings. Experiments with this approach show that this system also hurts translation quality. An in-depth analysis revealed that this negative result is mainly explained by two reasons: i) most of the post-edition operations are nearly unique, which makes very difficult to generalize from a small amount of data; and ii) even when they are not, the high variability of post-editing, already pointed out by Wisniewski et al. (2013), results in predicting legitimate corrections that have not been made by the annotators, therefore preventing from improving over the baseline. 5.3 Results The official results achieved by the participating systems are reported in Tables 20 and 21. The seven runs submitted are sorted based on the average TER they achieve on test data. Table 20 shows the results computed in case sensitive mode, while Table 21 provides scores computed in the case insensitive mode. Both rankings reveal an unexpected outcome: none of the submitted runs was able to beat the baselines (i.e. average TER scores of 22"
W15-3001,W15-3032,1,0.810291,"Missing"
W15-3001,W15-3031,1,0.376914,"Missing"
W15-3001,W15-3020,1,0.808362,"Missing"
W15-3001,W15-3043,0,0.0443554,"Missing"
W15-3001,W15-3021,0,0.038175,"Missing"
W15-3001,P07-2045,1,\N,Missing
W15-3001,W15-3004,0,\N,Missing
W15-3001,2015.eamt-1.4,0,\N,Missing
W15-3001,N06-1014,0,\N,Missing
W15-3001,2015.eamt-1.17,1,\N,Missing
W15-3001,2012.iwslt-papers.12,0,\N,Missing
W15-3020,W13-2205,0,0.0554806,"Missing"
W15-3020,D13-1174,0,0.0428603,"Missing"
W15-3020,P10-4002,0,0.212667,"Missing"
W15-3020,hajlaoui-etal-2014-dcep,0,0.0219022,"Missing"
W15-3020,W05-0820,0,0.0997647,"Missing"
W15-3020,2009.mtsummit-papers.9,0,0.0150512,"e capitalised. Clearly this would not be acceptable and so a ratio of capitalised ‘The’ versus lower-case ‘the’ was recorded and if it was over a set limit then all occurrences of ‘the’ would be capitalised or else none would be. By itself this still has a number of limitations, but it was surprisingly accurate in this case, improving our original BLEU-cased score by nearly +2.0. Filtering We attempted an experiment with filtering, based on research proving that the translation direction of the training data makes a significant difference for both the translation model and the language model (Kurokawa et al., 2009; Lembersky et al., 2013; Lembersky et al., 2012). This research indicates a qualitative improvement with much less data. It would seem logical that training on translated data already incorporates some of the crosslingual transfer which is performed by a human translator, and therefore is valuable to capture. To this end we constructed a directional corpus, filtering the whole of the Europarl for excerpts which were originally in the Finnish language. We did this by tracking the ‘language’ attribute in the markup to filter out any contributions which had originally been in Finnish. Once we ha"
W15-3020,J12-4004,0,0.0160237,"able and so a ratio of capitalised ‘The’ versus lower-case ‘the’ was recorded and if it was over a set limit then all occurrences of ‘the’ would be capitalised or else none would be. By itself this still has a number of limitations, but it was surprisingly accurate in this case, improving our original BLEU-cased score by nearly +2.0. Filtering We attempted an experiment with filtering, based on research proving that the translation direction of the training data makes a significant difference for both the translation model and the language model (Kurokawa et al., 2009; Lembersky et al., 2013; Lembersky et al., 2012). This research indicates a qualitative improvement with much less data. It would seem logical that training on translated data already incorporates some of the crosslingual transfer which is performed by a human translator, and therefore is valuable to capture. To this end we constructed a directional corpus, filtering the whole of the Europarl for excerpts which were originally in the Finnish language. We did this by tracking the ‘language’ attribute in the markup to filter out any contributions which had originally been in Finnish. Once we had filtered these out we matched them with corresp"
W15-3020,J13-4007,0,0.015776,"this would not be acceptable and so a ratio of capitalised ‘The’ versus lower-case ‘the’ was recorded and if it was over a set limit then all occurrences of ‘the’ would be capitalised or else none would be. By itself this still has a number of limitations, but it was surprisingly accurate in this case, improving our original BLEU-cased score by nearly +2.0. Filtering We attempted an experiment with filtering, based on research proving that the translation direction of the training data makes a significant difference for both the translation model and the language model (Kurokawa et al., 2009; Lembersky et al., 2013; Lembersky et al., 2012). This research indicates a qualitative improvement with much less data. It would seem logical that training on translated data already incorporates some of the crosslingual transfer which is performed by a human translator, and therefore is valuable to capture. To this end we constructed a directional corpus, filtering the whole of the Europarl for excerpts which were originally in the Finnish language. We did this by tracking the ‘language’ attribute in the markup to filter out any contributions which had originally been in Finnish. Once we had filtered these out we"
W15-3020,D10-1015,0,0.0265589,"ogically rich languages in MT, FinnishEnglish has previously featured as a language pair, in the 2005 shared task (Koehn and Monz, 2005). Chahuneau et al. (2013) experimented specifically with models into morphologically rich languages, we opted to do from Finnish, as a morphologically rich language, into English. Their approach is, however, more systematic, deploying a morphological grammar. Another approach, used by Ammar et al. (2013) is that of synthetic translation options, supplementing the phrase tables to compensate for the sparseness in translating from/to highly inflected languages. Luong et al. (2010) also investigate morphemelevel extraction, but integrate this into the decoding process itself, instead of the pre-processing step we have. They also incorporate unsupervised morphological analysis and do not rely on language-specific tools, whereas we used a Finnish parser for our morphological analysis This paper provides an overview of the Sheffield University submission to the WMT15 Translation Task for the FinnishEnglish language pair. The submitted translations were created from a system built using the CDEC decoder. Finnish is a morphologically rich language with elements such as nouns"
W15-3020,D11-1034,0,\N,Missing
W15-3020,W14-3315,0,\N,Missing
W15-3039,2005.mtsummit-papers.11,0,0.0275159,"need to be converted into one-hot vector representation. The one-hot representation of a categorical feature is the representation of every possible feature • Lexical features: – target token – target token’s left and right contexts of 1 word • Alignment features: – source word aligned to the target token – source word’s left and right contexts of 1 word • Boolean dictionary features: – – – – Alternative system 4 The pseudo-reference used for this feature extraction is the automatic translation generated by an English-Spanish phrase-based statistical MT system trained on the Europarl corpus (Koehn, 2005) using Moses system with standard settings (http://www.statmt.org/moses/?n= Moses.Baseline). 5 POS tagging was performed with TreeTagger tool http://www.cis.uni-muenchen.de/˜schmid/ tools/TreeTagger/ 6 http://scikit-learn.org/ target token is a stopword target token is a punctuation mark target token is a proper noun target token is a number • Target language model features: 331 value from a domain D as a vector of 0s and a single 1. The length of such vector is |D |(length of the set of possible values of the feature), every position in the vector corresponds to a value from D. Each instance"
W15-3039,W14-3342,0,0.0252085,"h. The training data consists of 11,271 sentences, the development and test sets have 1,000 and 1,817 sentences, respectively. The post-editions and tags for the test data were not made available until after the end of the evaluation period. 2.1 • Source language model features: – order of the highest order ngram which ends with the source token – order of the highest order ngram which starts with the source token • Boolean pseudo-reference feature: 1 if the token is contained in the pseudo-reference, 0 otherwise4 Features • Part-of-speech features5 : We used a subset of features described by Luong et al. (2014), mainly the features that were listed as the most informative. This corresponds to the baseline feature set released for the shared task. The full list of features is the following: – POS of the target token – POS of the source token • WordNet features: – Number of senses for the target token – Number of senses for the source token • Word count features: – source and target token counts – source and target token count ratio 2.2 We performed additional experiments with a reduced feature set which does not contain lexical and alignment features. These features were excluded in order to enable t"
W15-3039,2006.amta-papers.25,0,0.0131273,"en – order of the highest order ngram which starts with the target token – backoff behaviour of the ngram (ti−2 , ti−1 , ti ), where ti is the target token (backoff behaviour is computed as described in Raybaud et al. (2011)) – backoff behavior of the ngram (ti−1 , ti , ti+1 ) – backoff behavior of the ngram (ti , ti+1 , ti+2 ) translations into the target language, the manual post-editions (corrections) of the automatic translations, and the word-level tags for the automatic translations. The tags were acquired by aligning the machine translations with their post-editions using the TER tool (Snover et al., 2006). Unchanged words were assigned the label “GOOD”, words which were substituted with another word or deleted by a posteditor were assigned the label “BAD”. The “BAD” labels thus correspond to the “addition” and “substitution” edit operations in the word-level string alignment between the MT hypothesis and the post-edited segment. The dataset contains automatic translations from English into Spanish. The training data consists of 11,271 sentences, the development and test sets have 1,000 and 1,817 sentences, respectively. The post-editions and tags for the test data were not made available until"
W15-3039,W13-2201,1,\N,Missing
W15-3040,2014.eamt-1.21,1,0.963963,"rt documents) becomes necessary. This paper presents the University of Sheffield (USHEF) and University of Saarland (USAAR) submissions to the Task 3 of the WMT15 QE shared task: paragraph-level scoring and ranking. We submitted systems for both language pairs: English-German (EN-DE) and German-English (DE-EN). Little previous research has been done to address document-level QE. Soricut and Echihabi (2010) proposed document-aware features in order to rank machine translated documents. Soricut and Narsale (2012) use sentence-level features and predictions to improve document-level QE. Finally, Scarton and Specia (2014) and Scarton (2015) introduced discourse-aware features, which are combined with baseline features adapted from sentence-level work, in order to predict the quality of full documents. Previous work led to some improvements over the baselines used. However, several problems remain to be addressed for improving document-level QE, such as the choice of quality label, as discussed by Scarton et We present the results of the USHEF and USAAR-USHEF submissions for the WMT15 shared task on document-level quality estimation. The USHEF submissions explored several document and discourse-aware features."
W15-3040,W05-0909,0,0.179643,"those provided by the organisers were used. Tasks we participate in Task 3 (paragraph-level QE) in both subtasks, scoring and ranking. The evaluation for the scoring task was done using Mean Absolute Error (MAE) and the evaluation for the ranking task was done by DeltaAvg (official metrics of the competition). Data the official data of Task 3 - WMT15 QE shared task consist of 1215 paragraphs for ENDE and DE-EN, extracted from the corpora of WMT13 machine translation shared task (Bojar et al., 2013). For training, 800 paragraphs were used and, for test, 415 paragraphs were considered. METEOR (Banerjee and Lavie, 2005) was used as quality labels. Feature combination we experimented with different feature sets: • baseline (17 baseline features only) • baseline + discourse repetition features2 • baseline + document-aware features • baseline + discourse-aware features • all features. Features selected for EN-DE only: • LM probability of target document • LM perplexity of target document (with and without sentence markers) • type/token ration • average number of translations per source word in the document (threshold: prob &gt;0.2/0.5) Backward feature selection3 in order to perform feature selection, we used the"
W15-3040,W15-4916,1,0.869812,"Missing"
W15-3040,C04-1046,0,0.0396634,"eral metrics have been proposed so far comparing the MT outputs to human translations (references) in terms of ngrams matches (such as BLEU (Papineni et al., 2002)) or error rates (such as TER (Snover et al., 2006)). However, in some scenarios, human references are not available. For example, the use of machine translation in a workflow where good enough translations are given to humans for post-editing. Another example is machine translation for gisting by users of online systems. Quality Estimation (QE) approaches aim to predict the quality of MT outputs without relying on human references (Blatz et al., 2004; Specia et al., 2009). Features from source (original document) and target (MT outputs) and, when available, from the MT system are used to train supervised machine learning models (classifiers or regressors). A number of data points need to be annotated for quality (by humans or automatically) for training, using a given quality metric. 336 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 336–341, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. al. (2015). Our approach focuses on extracting various features and building mod"
W15-3040,N15-2016,1,0.900849,"y. This paper presents the University of Sheffield (USHEF) and University of Saarland (USAAR) submissions to the Task 3 of the WMT15 QE shared task: paragraph-level scoring and ranking. We submitted systems for both language pairs: English-German (EN-DE) and German-English (DE-EN). Little previous research has been done to address document-level QE. Soricut and Echihabi (2010) proposed document-aware features in order to rank machine translated documents. Soricut and Narsale (2012) use sentence-level features and predictions to improve document-level QE. Finally, Scarton and Specia (2014) and Scarton (2015) introduced discourse-aware features, which are combined with baseline features adapted from sentence-level work, in order to predict the quality of full documents. Previous work led to some improvements over the baselines used. However, several problems remain to be addressed for improving document-level QE, such as the choice of quality label, as discussed by Scarton et We present the results of the USHEF and USAAR-USHEF submissions for the WMT15 shared task on document-level quality estimation. The USHEF submissions explored several document and discourse-aware features. The USAARUSHEF subm"
W15-3040,2006.amta-papers.25,0,0.0992842,"ch to select the best features from the official baseline. Results show slight improvements over the baseline with the use of discourse features. More interestingly, we found that a model of comparable performance can be built with only three features selected by the exhaustive search procedure. 1 Introduction Evaluating the quality of Machine Translation (MT) systems outputs is a challenging topic. Several metrics have been proposed so far comparing the MT outputs to human translations (references) in terms of ngrams matches (such as BLEU (Papineni et al., 2002)) or error rates (such as TER (Snover et al., 2006)). However, in some scenarios, human references are not available. For example, the use of machine translation in a workflow where good enough translations are given to humans for post-editing. Another example is machine translation for gisting by users of online systems. Quality Estimation (QE) approaches aim to predict the quality of MT outputs without relying on human references (Blatz et al., 2004; Specia et al., 2009). Features from source (original document) and target (MT outputs) and, when available, from the MT system are used to train supervised machine learning models (classifiers o"
W15-3040,P10-1063,0,0.0579964,"f each word or sentence individually is not as important as the quality of the review as a whole. Therefore, predicting the quality of the whole document (or paragraph, considering paragraph as short documents) becomes necessary. This paper presents the University of Sheffield (USHEF) and University of Saarland (USAAR) submissions to the Task 3 of the WMT15 QE shared task: paragraph-level scoring and ranking. We submitted systems for both language pairs: English-German (EN-DE) and German-English (DE-EN). Little previous research has been done to address document-level QE. Soricut and Echihabi (2010) proposed document-aware features in order to rank machine translated documents. Soricut and Narsale (2012) use sentence-level features and predictions to improve document-level QE. Finally, Scarton and Specia (2014) and Scarton (2015) introduced discourse-aware features, which are combined with baseline features adapted from sentence-level work, in order to predict the quality of full documents. Previous work led to some improvements over the baselines used. However, several problems remain to be addressed for improving document-level QE, such as the choice of quality label, as discussed by S"
W15-3040,W12-3121,0,0.015232,"s a whole. Therefore, predicting the quality of the whole document (or paragraph, considering paragraph as short documents) becomes necessary. This paper presents the University of Sheffield (USHEF) and University of Saarland (USAAR) submissions to the Task 3 of the WMT15 QE shared task: paragraph-level scoring and ranking. We submitted systems for both language pairs: English-German (EN-DE) and German-English (DE-EN). Little previous research has been done to address document-level QE. Soricut and Echihabi (2010) proposed document-aware features in order to rank machine translated documents. Soricut and Narsale (2012) use sentence-level features and predictions to improve document-level QE. Finally, Scarton and Specia (2014) and Scarton (2015) introduced discourse-aware features, which are combined with baseline features adapted from sentence-level work, in order to predict the quality of full documents. Previous work led to some improvements over the baselines used. However, several problems remain to be addressed for improving document-level QE, such as the choice of quality label, as discussed by Scarton et We present the results of the USHEF and USAAR-USHEF submissions for the WMT15 shared task on docu"
W15-3040,W12-3102,1,0.794963,"Missing"
W15-3040,2009.eamt-1.5,1,0.80626,"en proposed so far comparing the MT outputs to human translations (references) in terms of ngrams matches (such as BLEU (Papineni et al., 2002)) or error rates (such as TER (Snover et al., 2006)). However, in some scenarios, human references are not available. For example, the use of machine translation in a workflow where good enough translations are given to humans for post-editing. Another example is machine translation for gisting by users of online systems. Quality Estimation (QE) approaches aim to predict the quality of MT outputs without relying on human references (Blatz et al., 2004; Specia et al., 2009). Features from source (original document) and target (MT outputs) and, when available, from the MT system are used to train supervised machine learning models (classifiers or regressors). A number of data points need to be annotated for quality (by humans or automatically) for training, using a given quality metric. 336 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 336–341, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. al. (2015). Our approach focuses on extracting various features and building models with different com"
W15-3040,P13-4014,1,0.869316,"Missing"
W15-3040,P13-1048,0,0.0414372,"umber of EDU (elementary discourse units) breaks in the source (target) document • number of RST (Rhetorical Structure Theory) Nucleus relations in the source/target document • number of RST Satellite relations in the source/target document. In order to extract the last set of features we use existing NLP tools: For identifying pronouns, we use the output of Charniak’s parser (Charniak, 2000) (we count the P RP tags). Discourse connectives are automatically extracted by the parser of Pitler and Nenkova (2009). RST trees and EDUs are extracted by the discourse parser and discourse segmenter of Joty et al. (2013). 3 toolkit (Pedregosa et al., 2011), to rank the features. Once this feature ranking is produced, we apply a backward feature selection approach. Starting with the features with lower positition in the rank, the method consists in consistently eliminate features, aiming to obtain a feature set that better fit the predictions. For both EN-DE and DE-EN, 38 features were selected. The set of features selected for both languages is: • LM probability of source document • LM perplexity of source document • average trigram frequency in quartile 1/2/3/4 of frequency in a corpus of the source language"
W15-3040,P15-4020,1,0.846447,"ge of nouns in the source and target documents • ratio of percentage of verbs in the source and target documents • ratio of percentage of pronouns in the source and target documents • number of dependencies with aligned constituents normalised by the total number of dependencies (maximum between source and target) • number of sentences (source and target should be the same). Document-level features Along with the official baseline features, we use two different sets of features. The first set contains document-aware features, based on QuEst features for sentence-level QE (Specia et al., 2013; Specia et al., 2015). The second set are features that encompass discourse information, following previous work of Scarton and Specia (2014) and Scarton (2015). 2.1 Document-aware features The 17 baseline features made available by the organisers are the same baseline features used for sentence-level QE, adapted for documentlevel.1 However, as part of the QuEst framework, other sentence-level features can be easily adapted for document-level QE. Our complete set of document-aware features include: • ratio of number of tokens in source and target (and in target and source) • absolute difference between number toke"
W15-3040,P02-1040,0,0.108728,"AARUSHEF submissions used an exhaustive search approach to select the best features from the official baseline. Results show slight improvements over the baseline with the use of discourse features. More interestingly, we found that a model of comparable performance can be built with only three features selected by the exhaustive search procedure. 1 Introduction Evaluating the quality of Machine Translation (MT) systems outputs is a challenging topic. Several metrics have been proposed so far comparing the MT outputs to human translations (references) in terms of ngrams matches (such as BLEU (Papineni et al., 2002)) or error rates (such as TER (Snover et al., 2006)). However, in some scenarios, human references are not available. For example, the use of machine translation in a workflow where good enough translations are given to humans for post-editing. Another example is machine translation for gisting by users of online systems. Quality Estimation (QE) approaches aim to predict the quality of MT outputs without relying on human references (Blatz et al., 2004; Specia et al., 2009). Features from source (original document) and target (MT outputs) and, when available, from the MT system are used to trai"
W15-3040,P09-2004,0,0.0467811,"cument-level evaluation purposes. We considered the discourse-aware features presented in Scarton and Specia (2014), which are already implemented in the QuEst framework (called herein as discourse repetition features): • word/lemma/noun repetition in the source/target document • ratio of word/lemma/noun repetition between source and target documents. Other discourse features were also explored (following the work of Scarton (2015)): • number of pronouns in the source/target document • number of discourse connectives in the source/target document • number of pronouns of each type according to Pitler and Nenkova (2009)’s classification: 1 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 337 Expansion, Temporal, Contingency, Comparison and Non-discourse • number of EDU (elementary discourse units) breaks in the source (target) document • number of RST (Rhetorical Structure Theory) Nucleus relations in the source/target document • number of RST Satellite relations in the source/target document. In order to extract the last set of features we use existing NLP tools: For identifying pronouns, we use the output of Charniak’s parser (Charniak, 2000) (we count the P RP tags). Discourse c"
W15-3041,N13-1090,0,0.603059,"s which are language-independent, require minimal resources, and can be extracted in unsupervised ways with the use of neural networks. Word embeddings have shown their potential in modelling long distance dependencies in data, including syntactic and semantic information. For instance, neural network language models (Bengio et al., 2003) have been successfully explored in many problems including Automatic Speech Recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and Machine Translation (Schwenk, 2012). While neural network language models predict the next word given a preceding context, (Mikolov et al., 2013b) proposed a neural network framework to predict the word given the left and right contexts, or to predict the word’s left and right contexts in a given sentence. Recently, it has been shown that these distributed vector representations (or word embeddings) can be exploited across languages to predict translations (Mikolov et al., 2013a). The word representations are learned from large monolingual data independently for source and target languages. A small seed dictionary is used to learn mapping from the source into the target space. In this paper, we investigate the use of such resources in"
W15-3041,J03-1002,0,0.00477454,"dited by crowdsourced translators, and HTER labels were computed using the TER tool (settings: tokenised, case insensitive, exact matching only, with scores capped to 1). 4.2.2 Word alignment training To extract word embedding features, as explained in Section 3, we need word-to-word alignments between source and target data. As word-level alignments between the source and target corpora were not made available by WMT, we first aligned the QE datasets with a bilingual word-level alignment model trained on the same data used for the word2vec modelling step, with the help of the GIZA++ toolkit (Och and Ney, 2003). Working on target side, we refined the resulting n-m targetto-source word alignments to a set of 1-m alignments by filtering potential spurious source-side candidates out. To do so, the decision was based on the lexical probabilities extracted from the previous alignment training step. Hence, each target4.1.2 Feature set We extracted the following features: • AF: 80 black-box features using the QuEst framework (Specia et al., 2013; Shah et al., 2013a) as described in Shah et al. (2013b). • CSLM: A feature for each source and target sentence using CSLM as described in Section 2. • FS(AF): Top"
W15-3041,H05-1026,0,0.173581,"from segment pairs in isolation, ignoring contextual clues from other segments in the text. The focus of our contributions this year is to introduce a new set of features which are language-independent, require minimal resources, and can be extracted in unsupervised ways with the use of neural networks. Word embeddings have shown their potential in modelling long distance dependencies in data, including syntactic and semantic information. For instance, neural network language models (Bengio et al., 2003) have been successfully explored in many problems including Automatic Speech Recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and Machine Translation (Schwenk, 2012). While neural network language models predict the next word given a preceding context, (Mikolov et al., 2013b) proposed a neural network framework to predict the word given the left and right contexts, or to predict the word’s left and right contexts in a given sentence. Recently, it has been shown that these distributed vector representations (or word embeddings) can be exploited across languages to predict translations (Mikolov et al., 2013a). The word representations are learned from large monolingual data independently for source and"
W15-3041,C12-2104,0,0.035367,"her segments in the text. The focus of our contributions this year is to introduce a new set of features which are language-independent, require minimal resources, and can be extracted in unsupervised ways with the use of neural networks. Word embeddings have shown their potential in modelling long distance dependencies in data, including syntactic and semantic information. For instance, neural network language models (Bengio et al., 2003) have been successfully explored in many problems including Automatic Speech Recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and Machine Translation (Schwenk, 2012). While neural network language models predict the next word given a preceding context, (Mikolov et al., 2013b) proposed a neural network framework to predict the word given the left and right contexts, or to predict the word’s left and right contexts in a given sentence. Recently, it has been shown that these distributed vector representations (or word embeddings) can be exploited across languages to predict translations (Mikolov et al., 2013a). The word representations are learned from large monolingual data independently for source and target languages. A small seed dictionary is used to le"
W15-3041,2013.mtsummit-papers.21,1,0.922831,"extracted word embeddings for all words in the Task 2 training, development and test sets from these models to be used as features. These distributed numerical representations of words as features aim at locating each word as a point in a 500-dimensional space. Inspired by the work of (Mikolov et al., 2013a), we extracted another feature by mapping the source space onto a target space using a seed dictionary (trained with Europarl + Newscommentary + News-crawl). A given word and 2 http://www-lium.univ-lemans.fr/cslm/ 343 https://code.google.com/p/word2vec/ (GPs) by the procedure described in (Shah et al., 2013b). its continuous vector representation a could be mapped to the other language space by computing z = M a, where M is a transformation matrix learned with stochastic gradient descent. The assumption is that the vector representations of similar words in different languages are related by a linear transformation because of similar geometric arrangements. The words whose representation are closest to a in the target language space, using cosine similarity, are considered as potential translations for a given word in the source language. Since the goal of QE is not to translate content, but to"
W15-3041,W13-2241,1,0.886632,"late content, but to measure the quality of translations, we take the source-to-target similarity scores as a feature itself. To calculate it, we first learn word alignments (see Section 4.2.2), and then compute the similarity scores between target word and the source word aligned to it. 4 4.1.3 We use the Support Vector Machines implementation in the scikit-learn toolkit (Pedregosa et al., 2011) to perform regression (SVR) on each feature set with either linear or RBF kernels and parameters optimised using grid search. We also apply GPs with similar settings to those in our WMT13 submission (Beck et al., 2013) using GPy toolkit 3 . For models with feature selection, we train a GP, select the top 20 features according to the produced feature ranking, and then retrain a SparseGP on the full training set using these 20 features and 50 inducing points. To evaluate the prediction models we use Mean Absolute Error (MAE), its squared version – Root Mean Squared Error (RMSE), and Spearman’s Correlation. Experiments We present experiments on the WMT15 QE Tasks 1 and 2, with CSLM features for Task 1, and word embedding features for Task 2. 4.1 Learning algorithms 4.2 Task 2 4.2.1 Dataset The data for this is"
W15-3041,P13-4014,1,0.89127,"Missing"
W15-3041,P15-4020,1,0.79936,"Missing"
W15-3041,P10-2041,0,\N,Missing
W15-4907,W14-3342,0,0.018695,"E ST toolkit (Specia et al., 2013). It trains QE models using sklearn1 versions of Support Vector Machine (SVM) classiﬁer (for ternary classiﬁcation task, Section 4.4) and SVM regression (for HTER prediction, Section 4.5). The wordlevel version of Q U E ST2 was used for word-level feature extraction. Word-level classiﬁers were trained with CRFSuite3 . The CRF error models were trained with CRF++4 . POS tagging was performed with TreeTagger (Schmid, 1994). Sentence-level QuEst uses 17 baseline features5 for all tasks. Word-level QuEst reimplements the set of 30 baseline features described in (Luong et al., 2014). The QE models were built and tested based on the data provided for the WMT14 English–Spanish QE shared task (Section 4.3). The statistics on error distributions were computed using the English–Spanish part of training data for WMT13 shared task on QE6 . The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus (Koehn, 2005). We trained the alignment model with FastAlign (Dyer et al., 2013) and extracted the lexical probabilities tables for words using scripts for phrase table building in Moses (Koehn et al., 2007). For all the"
W15-4907,C10-2075,0,0.020581,"1 ): ﬁrst the start symbol &lt; s > is generated, then the next words are taken based on the word probability given the already generated words. Other approaches to discriminative LMs use the n-best list of the MT system as training data (Li and Khudanpur, 2008). The translation variant which is closest to the oracle (e.g. has the highest BLEU score) is used as a positive example, while the variant with high system score and low BLEU score is used as a negative example. Such dataset allows the classiﬁer to reduce the differences between the model score and the actual quality score of a sentence. Li et al. (2010) simulate the generation of an n-best list using translation tables from SMT systems. By taking entries from the translation table with the same source side they create a set of alternative translations for a given target phrase. For each sentence, these are combined, generating a confusion set for this sentence. 2.2 Quality estimation for MT QE can be modelled as a classiﬁcation task where the goal is to distinguish good from bad translations, or to provide a quality score to each translation. Therefore, examples of bad sentences or 52 words produced by the MT system are needed. To the best o"
W15-4907,P07-1010,0,0.0364285,"ir results. 2 2.1 Previous work Discriminative language modelling One example of task that requires low quality examples is discriminative language modelling (DLM), i.e., the classiﬁcation of sentences as ”good” or ”bad”. It was ﬁrst introduced in a monolingual context within automatic speech recognition (Collins et al., 2005), and later applied to MT. While in speech recognition negative examples can be created from system outputs that differ from the reference (Bhanuprasad and Svenson, 2008), in MT there are multiple correct outputs, so negative examples need to be deﬁned more carefully. In Okanohara (2007) bad sentences used as negative training instances are drawn from the distribution P (wi |wi−N +1 , ..., wi−1 ): ﬁrst the start symbol &lt; s > is generated, then the next words are taken based on the word probability given the already generated words. Other approaches to discriminative LMs use the n-best list of the MT system as training data (Li and Khudanpur, 2008). The translation variant which is closest to the oracle (e.g. has the highest BLEU score) is used as a positive example, while the variant with high system score and low BLEU score is used as a negative example. Such dataset allows"
W15-4907,W05-0909,0,0.0490621,"erate negative training data for QE model learning. Previous work has followed the hypothesis that machine translations can be assumed to have low quality (Gamon et al., 2005). However, this is not the case nowadays: many translations can be considered ﬂawless. Particularly for word-level QE, it is unrealistic to presume that every single word in the MT output is incorrect. Another possibility is to use automatic quality evaluation metrics based on reference translations to provide a quality score for MT data. Metrics such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) can be used to compare the automatic and reference translations. However, these scores can be very unreliable, especially for word-level QE, as every word that differs in form or position would be annotated as bad. Previous efforts have been made for negative data generation, including random generation of sentences from word distributions and the use of translations in low-ranked positions in n-best lists produced by statistical MT (SMT) systems. These methods are however unsuitable for QE at the word level, as they provide no information about the quality of individual words in a sentence."
W15-4907,P02-1040,0,0.0948915,"t is therefore desirable to devise automatic procedures to generate negative training data for QE model learning. Previous work has followed the hypothesis that machine translations can be assumed to have low quality (Gamon et al., 2005). However, this is not the case nowadays: many translations can be considered ﬂawless. Particularly for word-level QE, it is unrealistic to presume that every single word in the MT output is incorrect. Another possibility is to use automatic quality evaluation metrics based on reference translations to provide a quality score for MT data. Metrics such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) can be used to compare the automatic and reference translations. However, these scores can be very unreliable, especially for word-level QE, as every word that differs in form or position would be annotated as bad. Previous efforts have been made for negative data generation, including random generation of sentences from word distributions and the use of translations in low-ranked positions in n-best lists produced by statistical MT (SMT) systems. These methods are however unsuitable for QE at the word level, as they provide no"
W15-4907,I08-2113,0,0.0353692,"Missing"
W15-4907,P06-1032,0,0.0437309,"assumed that these are non-native speakers, who possibly translate the text from their native language. The difference is that in this task the source text is a hidden variable, whereas in MT it is observed. The strategy of adding errors to correct sentences has also been used for this task. Human errors are more intuitive to simulate as language learners explicitly attempt to use natural language grammars. Therefore, rule-based systems can be used to model some grammar errors, particularly those affecting closed class words, e.g. determiner errors (Izumi et al., 2003) or countability errors (Brockett et al., 2006). More recent statistical methods use the distributions of errors in corpora and small seed sets of errors. They often also concentrate on a single error type, usually with closed class words such as articles and prepositions (Rozovskaya and Roth, 2010). Felice and Yuan (2014) go beyond closed class words to evaluate how errors of different types are inﬂuenced by various linguistic parameters: text domain, learner’s ﬁrst language, POS tags and semantic classes of erroneous words. The approach led to the generation of high-quality artiﬁcial data for human error correction. However, it could not"
W15-4907,D10-1094,0,0.0270627,"sentences has also been used for this task. Human errors are more intuitive to simulate as language learners explicitly attempt to use natural language grammars. Therefore, rule-based systems can be used to model some grammar errors, particularly those affecting closed class words, e.g. determiner errors (Izumi et al., 2003) or countability errors (Brockett et al., 2006). More recent statistical methods use the distributions of errors in corpora and small seed sets of errors. They often also concentrate on a single error type, usually with closed class words such as articles and prepositions (Rozovskaya and Roth, 2010). Felice and Yuan (2014) go beyond closed class words to evaluate how errors of different types are inﬂuenced by various linguistic parameters: text domain, learner’s ﬁrst language, POS tags and semantic classes of erroneous words. The approach led to the generation of high-quality artiﬁcial data for human error correction. However, it could not be used for MT error identiﬁcation, as MT errors are different from human errors and usually cannot be assigned to a single type. 3 Generation of artiﬁcial data The easiest choice for artiﬁcial data generation is to create a sentence by taking all or s"
W15-4907,P05-1063,0,0.020677,"timately improve upon baseline models by extending the training data with suitable artiﬁcially created examples. In Section 2 we further review existing strategies for artiﬁcial data generation. We explain our generation strategies in Section 3. In Section 4 we describe our experiment and their results. 2 2.1 Previous work Discriminative language modelling One example of task that requires low quality examples is discriminative language modelling (DLM), i.e., the classiﬁcation of sentences as ”good” or ”bad”. It was ﬁrst introduced in a monolingual context within automatic speech recognition (Collins et al., 2005), and later applied to MT. While in speech recognition negative examples can be created from system outputs that differ from the reference (Bhanuprasad and Svenson, 2008), in MT there are multiple correct outputs, so negative examples need to be deﬁned more carefully. In Okanohara (2007) bad sentences used as negative training instances are drawn from the distribution P (wi |wi−N +1 , ..., wi−1 ): ﬁrst the start symbol &lt; s > is generated, then the next words are taken based on the word probability given the already generated words. Other approaches to discriminative LMs use the n-best list of"
W15-4907,N13-1073,0,0.0203925,"Sentence-level QuEst uses 17 baseline features5 for all tasks. Word-level QuEst reimplements the set of 30 baseline features described in (Luong et al., 2014). The QE models were built and tested based on the data provided for the WMT14 English–Spanish QE shared task (Section 4.3). The statistics on error distributions were computed using the English–Spanish part of training data for WMT13 shared task on QE6 . The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus (Koehn, 2005). We trained the alignment model with FastAlign (Dyer et al., 2013) and extracted the lexical probabilities tables for words using scripts for phrase table building in Moses (Koehn et al., 2007). For all the methods, errors were injected into the News Commentary corpus7 . 1 http://scikit-learn.org/ http://github.com/ghpaetzold/quest 3 http://www.chokkan.org/software/crfsuite/ 4 https://code.google.com/p/crfpp/ 5 http://www.quest.dcs.shef.ac.uk/ quest files/features blackbox baseline 17 6 http://www.quest.dcs.shef.ac.uk/ wmt13 qe.html 7 http://statmt.org/wmt14/ training-parallel-nc-v9.tgz 2 4.2 Generated data Combining three methods of errors generation and tw"
W15-4907,E14-3013,0,0.0479911,"for this task. Human errors are more intuitive to simulate as language learners explicitly attempt to use natural language grammars. Therefore, rule-based systems can be used to model some grammar errors, particularly those affecting closed class words, e.g. determiner errors (Izumi et al., 2003) or countability errors (Brockett et al., 2006). More recent statistical methods use the distributions of errors in corpora and small seed sets of errors. They often also concentrate on a single error type, usually with closed class words such as articles and prepositions (Rozovskaya and Roth, 2010). Felice and Yuan (2014) go beyond closed class words to evaluate how errors of different types are inﬂuenced by various linguistic parameters: text domain, learner’s ﬁrst language, POS tags and semantic classes of erroneous words. The approach led to the generation of high-quality artiﬁcial data for human error correction. However, it could not be used for MT error identiﬁcation, as MT errors are different from human errors and usually cannot be assigned to a single type. 3 Generation of artiﬁcial data The easiest choice for artiﬁcial data generation is to create a sentence by taking all or some of its words from a"
W15-4907,2005.eamt-1.15,0,0.0306394,"ated by c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 51 Lucia Specia University of Shefﬁeld Shefﬁeld, United Kingdom l.specia@sheffield.ac.uk human translators, who post-edit automatic translations, mark errors in translations, or rate translations for quality. This process is slow and expensive. It is therefore desirable to devise automatic procedures to generate negative training data for QE model learning. Previous work has followed the hypothesis that machine translations can be assumed to have low quality (Gamon et al., 2005). However, this is not the case nowadays: many translations can be considered ﬂawless. Particularly for word-level QE, it is unrealistic to presume that every single word in the MT output is incorrect. Another possibility is to use automatic quality evaluation metrics based on reference translations to provide a quality score for MT data. Metrics such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) can be used to compare the automatic and reference translations. However, these scores can be very unreliable, especially for word-level QE, as every"
W15-4907,P07-2045,0,0.00366224,"described in (Luong et al., 2014). The QE models were built and tested based on the data provided for the WMT14 English–Spanish QE shared task (Section 4.3). The statistics on error distributions were computed using the English–Spanish part of training data for WMT13 shared task on QE6 . The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus (Koehn, 2005). We trained the alignment model with FastAlign (Dyer et al., 2013) and extracted the lexical probabilities tables for words using scripts for phrase table building in Moses (Koehn et al., 2007). For all the methods, errors were injected into the News Commentary corpus7 . 1 http://scikit-learn.org/ http://github.com/ghpaetzold/quest 3 http://www.chokkan.org/software/crfsuite/ 4 https://code.google.com/p/crfpp/ 5 http://www.quest.dcs.shef.ac.uk/ quest files/features blackbox baseline 17 6 http://www.quest.dcs.shef.ac.uk/ wmt13 qe.html 7 http://statmt.org/wmt14/ training-parallel-nc-v9.tgz 2 4.2 Generated data Combining three methods of errors generation and two methods of errors insertion into sentences resulted in a total of six artiﬁcial datasets. Here we perform some analysis on th"
W15-4907,2005.mtsummit-papers.11,0,0.0379627,"4 . POS tagging was performed with TreeTagger (Schmid, 1994). Sentence-level QuEst uses 17 baseline features5 for all tasks. Word-level QuEst reimplements the set of 30 baseline features described in (Luong et al., 2014). The QE models were built and tested based on the data provided for the WMT14 English–Spanish QE shared task (Section 4.3). The statistics on error distributions were computed using the English–Spanish part of training data for WMT13 shared task on QE6 . The statistics on the distributions of words, alignments and lexical probabilities were extracted from the Europarl corpus (Koehn, 2005). We trained the alignment model with FastAlign (Dyer et al., 2013) and extracted the lexical probabilities tables for words using scripts for phrase table building in Moses (Koehn et al., 2007). For all the methods, errors were injected into the News Commentary corpus7 . 1 http://scikit-learn.org/ http://github.com/ghpaetzold/quest 3 http://www.chokkan.org/software/crfsuite/ 4 https://code.google.com/p/crfpp/ 5 http://www.quest.dcs.shef.ac.uk/ quest files/features blackbox baseline 17 6 http://www.quest.dcs.shef.ac.uk/ wmt13 qe.html 7 http://statmt.org/wmt14/ training-parallel-nc-v9.tgz 2 4.2"
W15-4907,2006.amta-papers.25,0,0.024422,"evise automatic procedures to generate negative training data for QE model learning. Previous work has followed the hypothesis that machine translations can be assumed to have low quality (Gamon et al., 2005). However, this is not the case nowadays: many translations can be considered ﬂawless. Particularly for word-level QE, it is unrealistic to presume that every single word in the MT output is incorrect. Another possibility is to use automatic quality evaluation metrics based on reference translations to provide a quality score for MT data. Metrics such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) can be used to compare the automatic and reference translations. However, these scores can be very unreliable, especially for word-level QE, as every word that differs in form or position would be annotated as bad. Previous efforts have been made for negative data generation, including random generation of sentences from word distributions and the use of translations in low-ranked positions in n-best lists produced by statistical MT (SMT) systems. These methods are however unsuitable for QE at the word level, as they provide no information about the quali"
W15-4907,W09-0441,0,0.0211644,"e original sentence, and thus the more natural it looks (lower perplexity). Note that sentences where errors were inserted from a general distribution (unigramWI) have lower perplexity than those generated using using paraphrases. This can be because the unigramWI model tends to choose high-frequency words with lower perplexity, while the constructed paraphrases contain more noise and rare words. 4.3 Experimental setup We evaluated the performance of the artiﬁcially generated data in three tasks: the ternary classiﬁcation of sentences as “good”, “almost good” or “bad”, the prediction of HTER (Snover et al., 2009) score for a sentence, and the classiﬁcation of words in a sentence as “good” or “bad” (tasks 1.1, 1.2 and 2 of WMT14 QE shared task8 , respectively). 8 http://statmt.org/wmt14/ quality-estimation-task.html 55 The goal of the experiments was to check whether it is possible to improve upon the baseline results by adding artiﬁcially generated examples to the training sets. The baseline models for all tasks were trained on the data provided for the corresponding shared tasks for the English–Spanish language pair. All models were tested on the ofﬁcial test sets provided for the corresponding share"
W15-4907,P13-4014,1,0.899318,"Missing"
W15-4915,W08-0312,0,0.0220333,"frequently in references more heavily (Doddington, 2002), as shown in Eq. 2. Inf o(w1 . . . wn ) = log( n=1 C∈Candi �ngram∈C Pn = � C∈Candi Due to the sparsity of n-grams with large n and the geometric average of n-gram precisions, BLEU is not suitable for sentence-level evaluation. Several smoothing approaches have been proposed to alleviate this issue, such as the standard plus-one smoothing (Lin and Och, 2004) and combinations of smoothing techniques (Chen and Cherry, 2014). A great deal of methods have been proposed to improve the performance of BLEU. These include metrics such as m-bleu (Agarwal and Lavie, 2008) and Amber (Chen and Kuhn, 2011). However, these metrics still treat n-grams in different references equally, regardless of whether the n-gram appears only once or is found in all references. Count(ngram� ) 1, if |c |≥ |r| e(1−|r|/|c|) , if |c |< |r| wn is a weighting factor usually set as 1/N , where N is the longest possible n-gram considered by the matching method. N is usually set to 4 to avoid data sparseness issues resulting from longer n-grams. Pn is the n-gram precision at a given n and in essence represents the proportion of ngrams in the candidate translation that also appear in the"
W15-4915,W14-3302,1,0.815884,"and NIST metrics. Our metrics are tested in two into-English machine translation datasets. They lead to a signiﬁcant increase in Pearson’s correlation with human ﬂuency judgements at system-level evaluation. The new NIST metric also outperforms the standard NIST for documentlevel evaluation. 1 Introduction Quality evaluation plays a critical role in Machine Translation (MT). Since its conception, the BLEU metric (Papineni et al., 2002) has had a signiﬁcant impact on MT development. Although human evaluation has been used in recent evaluation campaigns such as WMT (Workshop on Statistical MT) (Bojar et al., 2014) and other forms of reference-less metrics have been proposed (Gamon et al., 2005; Specia et al., 2010), the merit of language and resource-independent n-gram based metrics such as BLEU is undeniable. Despite its c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. criticisms, BLEU is thus still considered the de facto or at least a baseline metric for MT quality evaluation. Due to the cost of human translation, often only one reference translation is available at evaluation time. However, generally there are numerous v"
W15-4915,W14-3346,0,0.0219153,"2 min( , 1) , Lref (1)      (2) where wn logPn , (1) Countclip (ngram) ngram� ∈C � NIST The NIST metric weights n-grams that occur less frequently in references more heavily (Doddington, 2002), as shown in Eq. 2. Inf o(w1 . . . wn ) = log( n=1 C∈Candi �ngram∈C Pn = � C∈Candi Due to the sparsity of n-grams with large n and the geometric average of n-gram precisions, BLEU is not suitable for sentence-level evaluation. Several smoothing approaches have been proposed to alleviate this issue, such as the standard plus-one smoothing (Lin and Och, 2004) and combinations of smoothing techniques (Chen and Cherry, 2014). A great deal of methods have been proposed to improve the performance of BLEU. These include metrics such as m-bleu (Agarwal and Lavie, 2008) and Amber (Chen and Kuhn, 2011). However, these metrics still treat n-grams in different references equally, regardless of whether the n-gram appears only once or is found in all references. Count(ngram� ) 1, if |c |≥ |r| e(1−|r|/|c|) , if |c |< |r| wn is a weighting factor usually set as 1/N , where N is the longest possible n-gram considered by the matching method. N is usually set to 4 to avoid data sparseness issues resulting from longer n-grams. P"
W15-4915,W11-2105,0,0.0199464,"y (Doddington, 2002), as shown in Eq. 2. Inf o(w1 . . . wn ) = log( n=1 C∈Candi �ngram∈C Pn = � C∈Candi Due to the sparsity of n-grams with large n and the geometric average of n-gram precisions, BLEU is not suitable for sentence-level evaluation. Several smoothing approaches have been proposed to alleviate this issue, such as the standard plus-one smoothing (Lin and Och, 2004) and combinations of smoothing techniques (Chen and Cherry, 2014). A great deal of methods have been proposed to improve the performance of BLEU. These include metrics such as m-bleu (Agarwal and Lavie, 2008) and Amber (Chen and Kuhn, 2011). However, these metrics still treat n-grams in different references equally, regardless of whether the n-gram appears only once or is found in all references. Count(ngram� ) 1, if |c |≥ |r| e(1−|r|/|c|) , if |c |< |r| wn is a weighting factor usually set as 1/N , where N is the longest possible n-gram considered by the matching method. N is usually set to 4 to avoid data sparseness issues resulting from longer n-grams. Pn is the n-gram precision at a given n and in essence represents the proportion of ngrams in the candidate translation that also appear in the reference translation. BP is a p"
W15-4915,N12-1017,0,0.0834954,"lations for a given sentence or document. Different references provide valid variations in linguistic aspects such as style, word choice and word order. Therefore, having multiple reference translations is key to improve the reliability of n-gram based evaluation metrics: the more references, the more chances for n-grams correctly translated to be captured. HyTER, an n-gram matching metric based on an exponential number of reference translations for a given target sentence, demonstrates the potential for better machine translation evaluation results from having as many references as possible (Dreyer and Marcu, 2012). Nevertheless, in the more realistic case where only a few references are available, if these are simply taken as bags of n-grams, increasing the number of references will not lead to the best possible results, as pointed out by Doddington (2002). In this paper we explore how to use multiple references by means other than simply viewing them as bags of n-gram like BLEU, NIST (Doddington, 2002) and other n-gram co-occurrence based metrics do. Our assumption is that each reference reﬂects the complete meaning of the source segment. The semantic entirety of the translation will be adversely affe"
W15-4915,2005.eamt-1.15,0,0.0283879,"atasets. They lead to a signiﬁcant increase in Pearson’s correlation with human ﬂuency judgements at system-level evaluation. The new NIST metric also outperforms the standard NIST for documentlevel evaluation. 1 Introduction Quality evaluation plays a critical role in Machine Translation (MT). Since its conception, the BLEU metric (Papineni et al., 2002) has had a signiﬁcant impact on MT development. Although human evaluation has been used in recent evaluation campaigns such as WMT (Workshop on Statistical MT) (Bojar et al., 2014) and other forms of reference-less metrics have been proposed (Gamon et al., 2005; Specia et al., 2010), the merit of language and resource-independent n-gram based metrics such as BLEU is undeniable. Despite its c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. criticisms, BLEU is thus still considered the de facto or at least a baseline metric for MT quality evaluation. Due to the cost of human translation, often only one reference translation is available at evaluation time. However, generally there are numerous valid translations for a given sentence or document. Different references provide"
W15-4915,C02-1117,0,0.0185667,"ent function words, the distribution of n-grams is taken into account to improve Eq. 3. Less overlap among references may indicate that the translation is difﬁcult, or that several different valid translations exist. In this scenario, recurring n-grams tend to be function words rather than content words. For instance, only function words repeat in the three references below, which may indicate that the source can be translated in many ways: a. At this time, the police have blocked the bombing scene. An alternative approach of neutralising function words in references is to use the Zipf’s law. Ha et al. (2002) verify Zipf’s law on n-grams by ranking all n-grams (n ≥ 1). So the n-grams recurring in references in Eq. 3 can be represented by the product between frequency f and the ranking order r of n-grams divided by ref no, as in Eq. 7. R� = log(1 + r × f /ref no) S BM = BP × exp( c. The police has already blockaded the scene of the explosion. To address the problem, a unit called n-gram divergence is deﬁned as in Eq. 4 to describe the degree of concentration of n-grams among references. The more divergent the distribution of ngrams in the references, the lower weight that is assigned to the most fr"
W15-4915,2003.mtsummit-papers.32,0,0.0933966,"erefore, reducing the importance of these common n-grams is not beneﬁcial to quality evaluation. 2.3 Improvements on n-gram based metrics Current improvements on the n-gram cooccurrence evaluation metrics can be divided into three categories. The ﬁrst category extends the scope of similarity detection by using a more 114 ﬂexible matching strategy, for example using WordNet to capture synonyms as in METEOR (Banerjee and Lavie, 2005). The second category uses different functions to calculate the degree of similarity, for example edit distance, error rate, semantic distance (Nießen et al., 2000; Leusch et al., 2003; Snover et al., 2006; Snover et al., 2009). And the last category weights or combines the outcome of similarity functions as features (Liu et al., 2010; Gim´enez and M`arquez, 2010). These methods focus on different forms of comparison between candidates and references. However, to our knowledge there are no other attempts to mine recurring information from multiple references if these are provided. Assuming all the possible translations form a “semantic” space, each reference only covers a subspace. The recurring ngrams among them should constitute the core part of this semantic space, which"
W15-4915,W10-1754,0,0.0183083,"nts on the n-gram cooccurrence evaluation metrics can be divided into three categories. The ﬁrst category extends the scope of similarity detection by using a more 114 ﬂexible matching strategy, for example using WordNet to capture synonyms as in METEOR (Banerjee and Lavie, 2005). The second category uses different functions to calculate the degree of similarity, for example edit distance, error rate, semantic distance (Nießen et al., 2000; Leusch et al., 2003; Snover et al., 2006; Snover et al., 2009). And the last category weights or combines the outcome of similarity functions as features (Liu et al., 2010; Gim´enez and M`arquez, 2010). These methods focus on different forms of comparison between candidates and references. However, to our knowledge there are no other attempts to mine recurring information from multiple references if these are provided. Assuming all the possible translations form a “semantic” space, each reference only covers a subspace. The recurring ngrams among them should constitute the core part of this semantic space, which is more likely to represent the meaning of the source text. It is this kind of information that we want to explore and apply with our n-gram weighting"
W15-4915,niessen-etal-2000-evaluation,0,0.0988754,"nvey core meaning. Therefore, reducing the importance of these common n-grams is not beneﬁcial to quality evaluation. 2.3 Improvements on n-gram based metrics Current improvements on the n-gram cooccurrence evaluation metrics can be divided into three categories. The ﬁrst category extends the scope of similarity detection by using a more 114 ﬂexible matching strategy, for example using WordNet to capture synonyms as in METEOR (Banerjee and Lavie, 2005). The second category uses different functions to calculate the degree of similarity, for example edit distance, error rate, semantic distance (Nießen et al., 2000; Leusch et al., 2003; Snover et al., 2006; Snover et al., 2009). And the last category weights or combines the outcome of similarity functions as features (Liu et al., 2010; Gim´enez and M`arquez, 2010). These methods focus on different forms of comparison between candidates and references. However, to our knowledge there are no other attempts to mine recurring information from multiple references if these are provided. Assuming all the possible translations form a “semantic” space, each reference only covers a subspace. The recurring ngrams among them should constitute the core part of this"
W15-4915,P02-1040,0,0.0983601,"information on the n-gram distribution and on divergences in multiple references, we propose a method of ngram weighting and implement it to generate new versions of the popular BLEU and NIST metrics. Our metrics are tested in two into-English machine translation datasets. They lead to a signiﬁcant increase in Pearson’s correlation with human ﬂuency judgements at system-level evaluation. The new NIST metric also outperforms the standard NIST for documentlevel evaluation. 1 Introduction Quality evaluation plays a critical role in Machine Translation (MT). Since its conception, the BLEU metric (Papineni et al., 2002) has had a signiﬁcant impact on MT development. Although human evaluation has been used in recent evaluation campaigns such as WMT (Workshop on Statistical MT) (Bojar et al., 2014) and other forms of reference-less metrics have been proposed (Gamon et al., 2005; Specia et al., 2010), the merit of language and resource-independent n-gram based metrics such as BLEU is undeniable. Despite its c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. criticisms, BLEU is thus still considered the de facto or at least a baseline m"
W15-4915,2006.amta-papers.25,0,0.0715866,"importance of these common n-grams is not beneﬁcial to quality evaluation. 2.3 Improvements on n-gram based metrics Current improvements on the n-gram cooccurrence evaluation metrics can be divided into three categories. The ﬁrst category extends the scope of similarity detection by using a more 114 ﬂexible matching strategy, for example using WordNet to capture synonyms as in METEOR (Banerjee and Lavie, 2005). The second category uses different functions to calculate the degree of similarity, for example edit distance, error rate, semantic distance (Nießen et al., 2000; Leusch et al., 2003; Snover et al., 2006; Snover et al., 2009). And the last category weights or combines the outcome of similarity functions as features (Liu et al., 2010; Gim´enez and M`arquez, 2010). These methods focus on different forms of comparison between candidates and references. However, to our knowledge there are no other attempts to mine recurring information from multiple references if these are provided. Assuming all the possible translations form a “semantic” space, each reference only covers a subspace. The recurring ngrams among them should constitute the core part of this semantic space, which is more likely to re"
W15-4915,W09-0441,0,0.0212416,"common n-grams is not beneﬁcial to quality evaluation. 2.3 Improvements on n-gram based metrics Current improvements on the n-gram cooccurrence evaluation metrics can be divided into three categories. The ﬁrst category extends the scope of similarity detection by using a more 114 ﬂexible matching strategy, for example using WordNet to capture synonyms as in METEOR (Banerjee and Lavie, 2005). The second category uses different functions to calculate the degree of similarity, for example edit distance, error rate, semantic distance (Nießen et al., 2000; Leusch et al., 2003; Snover et al., 2006; Snover et al., 2009). And the last category weights or combines the outcome of similarity functions as features (Liu et al., 2010; Gim´enez and M`arquez, 2010). These methods focus on different forms of comparison between candidates and references. However, to our knowledge there are no other attempts to mine recurring information from multiple references if these are provided. Assuming all the possible translations form a “semantic” space, each reference only covers a subspace. The recurring ngrams among them should constitute the core part of this semantic space, which is more likely to represent the meaning of"
W15-4915,takezawa-etal-2002-toward,0,0.08387,"tio for recurring n-grams. For instance, for bigrams appearing twice in 10 references, the outcome of Eq. 5 is 0.3424, while for bigrams appearing once, the outcome of Eq. 5 is 0.3222. However since there are only four references, the weighting ratio is larger, 0.3979/0.3522. In other words, the larger the number of references, the lower the impact of the reweighting method on the results. Increasing the number of references could help discriminate function words and content words as well. To check the recurrence of n-grams in larger numbers of references, we investigate the devset13 of BTEC (Takezawa et al., 2002), which contains 1512 source sentences, each with 16 English references. We show the average 1-4grams distribution over 2 to 16 translations in Figure 3. As expected, the proportion of n-grams covered by multiple references decreases as the number of references increases, showing that more translation variety is Conclusions and future work Recurring n-grams in references can help capture important words and sequences of words that are chosen by various translators. By combining recurrence distributions, divergence information and the length of n-grams, a modiﬁed weighting strategy for BLEU and"
W15-4915,C04-1072,0,0.0506358,"o(w1 . . . wn )/ � w1 ...wn in system � �� � Lsys × exp βlog2 min( , 1) , Lref (1)      (2) where wn logPn , (1) Countclip (ngram) ngram� ∈C � NIST The NIST metric weights n-grams that occur less frequently in references more heavily (Doddington, 2002), as shown in Eq. 2. Inf o(w1 . . . wn ) = log( n=1 C∈Candi �ngram∈C Pn = � C∈Candi Due to the sparsity of n-grams with large n and the geometric average of n-gram precisions, BLEU is not suitable for sentence-level evaluation. Several smoothing approaches have been proposed to alleviate this issue, such as the standard plus-one smoothing (Lin and Och, 2004) and combinations of smoothing techniques (Chen and Cherry, 2014). A great deal of methods have been proposed to improve the performance of BLEU. These include metrics such as m-bleu (Agarwal and Lavie, 2008) and Amber (Chen and Kuhn, 2011). However, these metrics still treat n-grams in different references equally, regardless of whether the n-gram appears only once or is found in all references. Count(ngram� ) 1, if |c |≥ |r| e(1−|r|/|c|) , if |c |< |r| wn is a weighting factor usually set as 1/N , where N is the longest possible n-gram considered by the matching method. N is usually set to 4"
W15-4915,W05-0909,0,\N,Missing
W15-4916,2012.eamt-1.33,1,0.851202,"Missing"
W15-4916,C04-1046,0,0.147834,"h levels. We highlight crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between"
W15-4916,W12-3156,0,0.103809,"sing automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an"
W15-4916,P14-1065,0,0.0528997,"Missing"
W15-4916,P10-1064,1,0.914135,"Missing"
W15-4916,P14-2047,0,0.0154408,"the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use automatic evaluation m"
W15-4916,W13-3303,0,0.0260337,"ssues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use a"
W15-4916,P02-1040,0,0.0919032,"c and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may beneﬁt from information in surrounding sentences, leading to a document that is ﬁt for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall"
W15-4916,P09-2004,0,0.0711715,"ndomly selected from the full corpus. For PE1 and PE2, only source (English) paragraphs with 3-8 sentences were selected (ﬁlter SNUMBER) to ensure that there is enough information beyond sentence-level to be evaluated and make the task feasible for the annotators. These paragraphs were further ﬁltered to select those with cohesive devices. Cohesive devices are linguistic units that play a role in establishing cohesion between clauses, sentences or paragraphs (Halliday and Hasan, 1976). Pronouns and discourse connectives are examples of such devices. A list of pronouns and the connectives from Pitler and Nenkova (2009) was considered for that. Finally, paragraphs were ranked according to the number of cohesive devices they contain and the top 200 paragraphs were selected (ﬁlter C-DEV). Table 3 shows the statistics of the initial corpus and the resulting selection after each ﬁlter. FULL CORPUS S-NUMBER C-DEV Number of Paragraphs 1, 215 394 200 Number of Cohesive devices 6, 488 3, 329 2, 338 Table 3: WMT13 English source corpus. For the PE1 experiment, the paragraphs in CDEV were randomised. Then, sets containing seven paragraphs each were created. For each set, the sentences of its paragraphs were also rando"
W15-4916,potet-etal-2012-collection,0,0.104329,"Missing"
W15-4916,2014.eamt-1.21,1,0.850194,"consider more information than sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may beneﬁt from information in surrounding sentences, leading to a document that is ﬁt for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT"
W15-4916,2006.amta-papers.25,0,0.549835,"es on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the ﬁnal version – HTER (Snover et al., 2006). c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. There are, however, scenarios where quality prediction beyond sentence level is needed, most notably in cases when automatic translations without post-editing are required. This is the case, for example, of quality prediction for an entire product review translation in order to decide whether or not it can be published as is, so that customers speaking other languages can understand it. The quality of a document is often seen as some form of aggregation of the quali"
W15-4916,P10-1063,0,0.677539,"than sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may beneﬁt from information in surrounding sentences, leading to a document that is ﬁt for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes)"
W15-4916,2009.eamt-1.5,1,0.889526,"ht crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the"
W15-4916,D14-1025,0,0.053746,"Missing"
W15-4916,C14-2028,0,\N,Missing
W15-4916,W13-2201,1,\N,Missing
W16-2301,W13-3520,0,0.0148252,"Missing"
W16-2301,2011.mtsummit-papers.35,0,0.440323,"Missing"
W16-2301,W15-3001,1,0.419439,"Missing"
W16-2301,W16-2305,0,0.0273073,"Missing"
W16-2301,L16-1662,0,0.00889007,"introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model."
W16-2301,W11-2101,1,0.851508,"Missing"
W16-2301,W16-2306,0,0.0263259,"Missing"
W16-2301,W16-2382,0,0.0451767,"Missing"
W16-2301,W16-2302,1,0.346609,"Missing"
W16-2301,W16-2307,1,0.756182,"Missing"
W16-2301,W16-2308,0,0.0373021,"Missing"
W16-2301,buck-etal-2014-n,0,0.0150538,"Missing"
W16-2301,W13-2201,1,0.287085,"Missing"
W16-2301,W14-3302,1,0.399147,"Missing"
W16-2301,W14-3340,1,0.419045,"Missing"
W16-2301,W07-0718,1,0.759293,"oth automatically and manually. The human evaluation (§3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries. 1 Introduction We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016. This conference builds on nine previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015). 131 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 131–198, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. New"
W16-2301,W15-3025,1,0.888663,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,P15-2026,1,0.895785,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,W08-0309,1,0.809107,"Missing"
W16-2301,W16-2309,0,0.0305938,"Missing"
W16-2301,W10-1703,1,0.312318,"Missing"
W16-2301,W16-2336,0,0.0356983,"Missing"
W16-2301,W12-3102,1,0.235434,"ir native language. Consequently, health professionals may use the translated information to make clinical decisions impacting patients care. It is vital that translation systems do not contribute to the dissemination of incorrect clinical information. Therefore, the evaluation of biomedical translation systems should include an assessment at the document level indicating whether a translation conveyed erroneous clinical information. 6 Quality Estimation The fifth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015), with “traditional” tasks at sentence and word levels, a new task for entire documents quality prediction, and a variant of the word-level task: phrase-level estimation. The goals of this year’s shared task were: • To advance work on sentence and wordlevel quality estimation by providing domainspecific, larger and professionally annotated datasets. • To analyse the effectiveness of different types of quality labels provided by humans for longer texts in document-level prediction. • To investigate quality estimation at a new level of granularity: phrases. Plan"
W16-2301,W16-2330,0,0.0328047,"Missing"
W16-2301,W16-2310,1,0.835279,"Missing"
W16-2301,W11-2103,1,0.65163,"Missing"
W16-2301,W16-2331,0,0.0203904,"cance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W15-3009,1,0.788321,"Missing"
W16-2301,P15-1174,1,0.28154,"endently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Table 20. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: • Scoring: Pearson’s r correlation score (primary metric, official score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical significance on Pearson r and Spearman rho was computed using the William’s test, following the approach suggested in (Graham, 2015). Results Table 19 summarises the results for Task 1, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems according to the ranking variant. We note that three systems have not submitted results ranking evaluation variant. 6.4 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a b"
W16-2301,W16-2311,0,0.0122367,"016) PROMT Automated Translation Solutions (Molchanov and Bykov, 2016) QT21 System Combination (Peter et al., 2016b) RWTH Aachen (Peter et al., 2016a) ¨ TUBITAK (Bektas¸ et al., 2016) University of Edinburgh (Sennrich et al., 2016) University of Edinburgh (Williams et al., 2016) UEDIN - SYNTAX UEDIN - LMU UH -* USFD - RESCORING UUT YSDA ONLINE -[ A , B , F, G ] University of Edinburgh / University of Munich (Huck et al., 2016) University of Helsinki (Tiedemann et al., 2016) University of Sheffield (Blain et al., 2016) Uppsala University (Tiedemann et al., 2016) Yandex School of Data Analysis (Dvorkovich et al., 2016) Four online statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 136 Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized"
W16-2301,W13-2305,1,0.509145,"Missing"
W16-2301,W15-3036,0,0.0603245,"Missing"
W16-2301,E14-1047,1,0.831614,"Missing"
W16-2301,W16-2378,0,0.168663,"ts for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W11-2123,0,0.0116622,"s were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii) the loglinear combination of monolingual and bilingual models in an ensemble-like manner, iii) the addition of task-specific features in the log-linear model to control the final output quality. Concerning the data, in addition"
W16-2301,W16-2384,0,0.0416841,"Missing"
W16-2301,W13-0805,0,0.0110515,"of words in the reference. Lower TER values indicate lower distance from the reference as a proxy for higher MT quality. 33 http://www.cs.umd.edu/˜snover/tercom/ 34 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 27 The source sentences (together with their reference translations which were not used for the task) were provided by TAUS (https://www.taus.net/) and originally come from a unique IT vendor. 28 It consists of a phrase-based machine translation system leveraging generic and in-domain parallel training data and using a pre-reordering technique (Herrmann et al., 2013). It takes also advantages of POS and word class-based language models. 29 German native speakers working at Text&Form https: //www.textform.com/. 176 Train (12,000) Dev (1,000) Test (2,000) SRC 201,505 17,827 31,477 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, pro"
W16-2301,W16-2315,1,0.824148,"Missing"
W16-2301,P07-2045,1,0.017773,"gets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual translation as another term of comparison. To get some insights about the progress with respect to the first pilot task, participating systems were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii)"
W16-2301,W16-2337,0,0.0376798,"Missing"
W16-2301,W16-2303,1,0.821661,"Missing"
W16-2301,L16-1582,1,0.790825,"Missing"
W16-2301,W16-2385,0,0.0304494,"Missing"
W16-2301,P16-2095,1,0.885979,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,W15-3037,0,0.00849978,"stems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model. It uses the baseline features provided by the shared task organisers (with slight changes) conjoined with individual labels and pairs of consecutive labels. It also uses various syntactic dependency-based features (dependency relations, heads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produ"
W16-2301,W14-3342,0,0.0237077,"Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Task 1, QuEst++13 (Specia et al., 2015) was used to extract 17 features from the SMT source/target language training corpus: • Number of tokens in source & target sentences. • Target token, its left and right contexts of one word."
W16-2301,W16-2318,0,0.0145349,"Missing"
W16-2301,N06-1014,0,0.0215966,"d on n previous translation and reordering decisions. This technique is able to model both local and long-range reorderings that are quite useful when dealing with the German language. To improve the capability of choosing the correct edit to process, eight new features are added to the loglinear model. These features capture the cost of deleting a phrase and different information on possible gaps in reordering operations. The monolingual alignments between the MT outputs and their post-edits are computed using different methods based on TER, METEOR (Snover et al., 2006) and Berkeley Aligner (Liang et al., 2006). Only the task data is used for these submissions. 7.3 179 TER/BLEU results ID AMU Primary AMU Contrastive FBK Contrastive FBK Primary USAAR Primary USAAR Constrastive CUNI Primary (Simard et al., 2007) Baseline DCU Contrastive JUSAAR Primary JUSAAR Contrastive DCU Primary Avg. TER 21.52 23.06 23.92 23.94 24.14 24.14 24.31 24.64 24.76 26.79 26.92 26.97 28.97 BLEU 67.65 66.09 64.75 64.75 64.10 64.00 63.32 63.47 62.11 58.60 59.44 59.18 55.19 tained this year by the top runs can only be reached by moving from the basic statistical MT backbone shared by all last year’s participants to new and mor"
W16-2301,P13-2109,0,0.0179517,"eads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produced with a neural MT system (Bahdanau et al., 2014) were used. UGENT-LT3 (Task 1, Task 2): The submissions for the word-level task use 41 features in combination with the baseline feature set to train binary classifiers. The 41 additional features attempt to capture accuracy errors (concerned with the meaning transfer from the source to targe"
W16-2301,W16-2387,0,0.0253511,"Missing"
W16-2301,W16-2317,0,0.0414065,"Missing"
W16-2301,N13-1090,0,0.00833788,"Missing"
W16-2301,2015.iwslt-papers.4,1,0.737239,"Missing"
W16-2301,W16-2319,0,0.0339318,"Missing"
W16-2301,W16-2386,1,0.814188,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,L16-1470,1,0.826657,"Missing"
W16-2301,P03-1021,0,0.0340784,"7 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data. 7.1.3 PE 16,388 3,506 5,047 SRC 5,628 1,922 2,479 Lemmas TGT PE 11,418 13,244 2,686 2,806 3,753 4,050 the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Table 34. For each submitted run, the statistical significance of performance differences with respect to the baselines and the re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). Baseline The official baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual"
W16-2301,W16-2338,0,0.0374669,"Missing"
W16-2301,J03-1002,0,0.0268639,"word alignments and bilingual distributed representations to introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level tas"
W16-2301,W16-2321,0,0.0372138,"Missing"
W16-2301,W16-2388,1,0.748184,"Missing"
W16-2301,W16-2333,0,0.0259256,"Missing"
W16-2301,W15-3026,0,0.0339859,"Missing"
W16-2301,W16-2379,1,0.819672,"Missing"
W16-2301,W16-2334,1,0.811656,"Missing"
W16-2301,P02-1040,0,0.105557,", eventually, make the APE task more feasible by automatic systems. Other changes concern the language combination and the evaluation mode. As regards the languages, we moved from English-Spanish to English-German, which is one of the language pairs covered by the QT21 Project26 that supported data collection and post-editing. Concerning the evaluation, we changed from TER scores computed both in case-sensitive and caseinsensitive mode to a single ranking based on case sensitive measurements. Besides these changes the new round of the APE task included some extensions in the evaluation. BLEU (Papineni et al., 2002) has been introduced as a secondary evaluation metric to measure the improvements over the rough MT output. In addition, to gain further insights on final output quality, a subset of the outputs of the submitted systems has also been manually evaluated. Based on these changes and extensions, the goals of this year’s shared task were to: i) improve and stabilize the evaluation framework in view of future rounds, ii) analyze the effect on task 26 feasibility of data coming from a narrow domain, iii) analyze the effect of post-edits collected from professional translators, iv) analyze how humans"
W16-2301,W16-2389,0,0.0210658,"Missing"
W16-2301,W16-2390,0,0.0283246,"Missing"
W16-2301,W16-2322,0,0.0139204,"ng and evaluating the manual translations has settled into the following pattern. We ask human annotators to rank the outputs of five systems. From these rankings, we produce pairwise translation comparisons, and then evaluate them with a version of the TrueSkill algorithm adapted to our task. We refer to this approach (described in Section 3.4) as the relative ranking approach (RR), so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer their absolute quality. These results are used to produce the official ranking for the WMT 2016 tasks. However, work in evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality. In this setting, annotators are asked to provide an assessment of the direct quality of the output of a system relative to a reference translation. In order to evaluate the potential of this approach for future WMT evaluations, we conducted a direct assessment evaluation in parallel. This evaluation, together with a comparison of the official results, is described in Section 3.5. 2.3 3.1 2.2 Training data As in past years we provide"
W16-2301,W16-2392,1,0.772695,"Missing"
W16-2301,L16-1649,1,0.704564,"): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence is the more difficult it is to translate. For this purpose, it uses information provided by syntactic parsing (information from parsing trees,"
W16-2301,W16-2391,1,0.796337,"Missing"
W16-2301,2014.eamt-1.21,1,0.643048,"e extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence"
W16-2301,W15-3040,1,0.909092,"on-word corpus,18 with a vocabulary size of 527K words. Document embeddings are extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1)"
W16-2301,2006.amta-papers.25,0,0.850121,"ombinations of several features. A regression model was training to predict BLEU as target metric instead HTER. The machine learning pipeline uses an SVR with RBF kernel to predict BLEU scores, followed by a linear SVR to predict HTER scores from BLEU scores. As external resources, the system uses a syntactic parser, pseudo-references and back-translation from web-scale MT system, and a web-scale language model. 6.3 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. HTER (Snover et al., 2006) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: • Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. • Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true H"
W16-2301,W15-4916,1,0.824025,"Missing"
W16-2301,W16-2346,1,0.0602876,"Missing"
W16-2301,W16-2325,1,0.816227,"Missing"
W16-2301,W16-2393,0,0.033041,"Missing"
W16-2301,W16-2326,0,0.0221449,"Missing"
W16-2301,W16-2327,1,0.737603,"Missing"
W16-2301,W16-2328,0,0.0403946,"Missing"
W16-2301,C00-2137,0,0.0384283,"Missing"
W16-2301,D07-1091,1,\N,Missing
W16-2301,W09-0401,1,\N,Missing
W16-2301,P11-1105,0,\N,Missing
W16-2301,N07-1064,0,\N,Missing
W16-2301,W04-3250,1,\N,Missing
W16-2301,P15-4020,1,\N,Missing
W16-2301,W16-2377,1,\N,Missing
W16-2301,aziz-etal-2012-pet,1,\N,Missing
W16-2301,2012.eamt-1.31,0,\N,Missing
W16-2301,C14-1182,0,\N,Missing
W16-2301,W16-2316,0,\N,Missing
W16-2301,W16-2332,1,\N,Missing
W16-2301,W16-2320,1,\N,Missing
W16-2301,W16-2335,0,\N,Missing
W16-2301,W16-2329,0,\N,Missing
W16-2301,W16-2383,0,\N,Missing
W16-2301,W16-2381,1,\N,Missing
W16-2301,W16-2312,0,\N,Missing
W16-2301,P16-1162,1,\N,Missing
W16-2301,W13-2243,1,\N,Missing
W16-2301,W08-0509,0,\N,Missing
W16-2301,2015.eamt-1.17,1,\N,Missing
W16-2301,W14-6111,0,\N,Missing
W16-2301,2012.tc-1.5,1,\N,Missing
W16-2301,W16-2347,1,\N,Missing
W16-2301,W16-2314,0,\N,Missing
W16-2307,D11-1125,0,0.0650043,"Missing"
W16-2307,P07-2045,0,0.0315029,"Missing"
W16-2307,P03-1021,0,0.177003,"Missing"
W16-2307,W16-2304,0,0.047186,"Missing"
W16-2307,P02-1040,0,0.0974724,"Missing"
W16-2307,W12-3102,1,0.637872,"Missing"
W16-2307,N12-1047,0,0.0540485,"Missing"
W16-2307,P15-4020,1,0.878181,"Missing"
W16-2307,W11-2107,0,0.0264472,"based on PRO which targets weaknesses of PRO in sampling translation candidates. The two algorithms performed similarly on the task, with PRO obtaining better results from using larger development sets. We can observe that n-best rescoring with additional features from QE can help identify better hypotheses within the pool of translation candidates. However, as we can see in last the row, we are still far from selecting the best possible hypothesis among those in the n-best list. This “oracle” selection corresponds to the upper bound performance using the current n-best list, based on Meteor (Denkowski and Lavie, 2011) scores measured for each translation candidate against its reference translation. This allows us to compare the actual rank of a translation hypothesis after the rescoring process with the rank it should theoretically have, if our rescoring method were perfect. We also noticed that most of the weights associated with the QE features are set to 0 after the training of the rescoring weights, and therefore most of these features do not get used. Acknowledgments Table 3 shows the performances of our phrasebased system tuned with either PRO or WRO, instead of MERT. We ran these two tuning algorith"
W16-2307,W08-0509,0,0.205256,"Missing"
W16-2307,W11-2123,0,0.0960864,"Missing"
W16-2320,W16-2304,1,0.833347,"factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. T"
W16-2320,W05-0909,0,0.583183,"ions from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alignments between the input hypotheses, which are obtained from METEOR (Banerjee and Lavie, 2005). The hypotheses are then reordered to match a selected skeleton hypothesis in terms of word ordering. We generate I different CNs, each having one of the input systems as the skeleton hypothesis, and the final lattice is the union of all I generated CNs. In Figure 1 an example of a confusion network with I = 4 input translations is depicted. Decoding of a confusion network finds the best path in the network. Each arc is assigned a score of a linear model combination of M different models, which includes word penalty, 3-gram language model trained on the input hypotheses, a binary primary syst"
W16-2320,P13-2071,1,0.873371,"odel trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. The first part was used as development set while t"
W16-2320,D15-1129,1,0.847985,"training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based loc"
W16-2320,N13-1073,0,0.034805,"preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ ques"
W16-2320,J04-2004,0,0.0194677,"en systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probabilit"
W16-2320,2011.mtsummit-papers.30,0,0.020392,"-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 201"
W16-2320,N12-1047,0,0.591808,"rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard p"
W16-2320,E14-2008,1,0.72015,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,P05-1033,0,0.151933,"ative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combin"
W16-2320,J07-2003,0,0.558191,"this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the bou"
W16-2320,W14-3310,1,0.909876,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D14-1179,0,0.0138582,"Missing"
W16-2320,2014.iwslt-evaluation.7,1,0.925781,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D08-1089,0,0.0211464,", built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. T"
W16-2320,N15-1105,0,0.046766,"Missing"
W16-2320,W08-0509,0,0.06624,"probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs. We validate the model every 10 000 minibatches via B LEU on a validation set, and perform early stopping on B LEU. Decoding is performed with beam search with a beam size of 12. A more detailed description of the system, and more experimental results, can be found in (Sennrich et al., 2016a). 3.10 3.11 USFD’s phrase-based system is built using the Moses toolkit, with MGIZA (Gao and Vogel, 2008) for word alignment and KenLM (Heafield et al., 2013) for language model training. We use all available parallel data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the featur"
W16-2320,W16-2315,1,0.820309,"vs et al., 2012) that features language-specific data filtering and cleaning modules. Tilde’s system was trained on all available parallel data. Two language models are trained using KenLM (Heafield, 2011): 1) a 5-gram model using the Europarl and SETimes2 corpora, and 2) a 3-gram model using the Common Crawl corpus. We also apply a custom tokenization tool that takes into account specifics of the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural Sy"
W16-2320,D07-1103,0,0.032902,"bbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT ("
W16-2320,W14-3360,0,0.0191919,"NMT system on newsdev2016/2, but lags behind on newstest2016. Removing the by itself weakest system shows a slight degradation on newsdev2016/2 and newstest2016, hinting that it still provides valuable information. Table 2 shows a comparison between all systems by scoring the translation output against each other in T ER and B LEU. We see that the neural networks outputs differ the most from all the other systems. Figure 1: System A: the large building; System B: the large home; System C: a big house; System D: a huge house; Reference: the big house. classes were generated using the method of Green et al. (2014). 4 System Combination System combination produces consensus translations from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alig"
W16-2320,P07-2045,1,0.010375,", 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based"
W16-2320,P13-2121,0,0.0645203,"Missing"
W16-2320,W11-2123,0,0.124165,"nslation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phras"
W16-2320,N04-1022,0,0.037676,"f the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpu"
W16-2320,2015.iwslt-papers.3,1,0.744353,"g. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn e"
W16-2320,J06-4004,0,0.106202,"Missing"
W16-2320,2009.iwslt-papers.4,0,0.0984189,"target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SETimes2) directly into the log-linear combination of the system and let MIRA (Cherry and Foster, 2012) optimize their weights along with all other features in tuning, rather than relying on a single linearly interpolated language model. We add another background language model estimated over a concatenation of all Ro"
W16-2320,P10-2041,0,0.0238386,"ontaining the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using"
W16-2320,P07-1019,0,0.236745,"grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language"
W16-2320,2012.amta-papers.19,1,0.778005,"common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set usin"
W16-2320,W11-2211,1,0.902733,"ty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system provided by the RWTH is an attention-based recurrent neural network similar to (Bahdanau et al., 2015). The implementation is based on Blocks (van Merri¨enboer et al., 2015) and Theano (Bergstra et al., 20"
W16-2320,W13-2264,1,0.852517,"stic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additio"
W16-2320,W13-2258,1,0.869431,"extraction, we impose less strict extraction constraints than the Moses defaults. We extract more hierarchical rules by allowing for a maximum of ten symbols on the source side, a maximum span of twenty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system prov"
W16-2320,E99-1010,0,0.040797,"ion 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and th"
W16-2320,P03-1021,0,0.501814,". To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SE"
W16-2320,D14-1003,1,0.932306,"with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule appli"
W16-2320,P06-1055,0,0.0121776,"anslation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but als"
W16-2320,2007.tmi-papers.21,0,0.0230136,"n 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add t"
W16-2320,P16-1161,1,0.729045,"omanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard phrasebased setup is the addition of a feature-rich discriminative translation model which is conditioned on both source- and target-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-"
W16-2320,tufis-etal-2008-racais,0,0.107217,"Missing"
W16-2320,P16-1009,1,0.78198,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,P12-3008,0,0.0597108,"Missing"
W16-2320,P16-1162,1,0.259658,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,W10-1738,1,0.885055,"rget-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical ph"
W16-2320,P15-4020,1,0.815128,"data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the feature set with the 17 baseline black-box features from sentencelevel Quality Estimation (QE) produced with Quest++4 (Specia et al., 2015). The 1000-best lists are then reranked and the top-best hypothesis extracted using the nbest rescorer available within the Moses toolkit. 3.12 UvA We use a phrase-based machine translation system (Moses) with a distortion limit of 6 and lexicalized reordering. Before translation, the English source side is preordered using the neural preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in"
W16-2320,W16-2327,1,0.84726,"Missing"
W16-2320,D13-1138,1,0.859072,"(Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integra"
W16-2320,2002.tmi-tutorials.2,0,0.0608664,"omanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all"
W16-2339,W15-3046,1,0.916508,"urpose and intended use of the MT, manual evaluation can be performed in a number of different ways. However, in any setting both adequacy and fluency shape human perception of the overall translation quality. By contrast, automatic reference-based metrics are largely focused on MT adequacy, as they do not evaluate the appropriateness of the translation in the context of the target language. Translation fluency is thus assessed only indirectly, through the comparison with the reference. However, the difference from a particular human translation does not imply that the MT output is disfluent (Fomicheva et al., 2015a). We propose to explicitly model translation fluency in reference-based MT evaluation. To this end, we develop a number of features representing translation fluency and integrate them with our reference-based metric UPF-Cobalt, which was originally presented at WMT15 (Fomicheva et al., 2015b). Along with the features based on the target Language Model (LM) probability of the MT output, which have been widely used in the related fields of speech recognition (Uhrik and Ward, 1997) and quality estimation (Specia et al., 2009), we design a more detailed representation of MT fluency that takes in"
W16-2339,P11-1022,0,0.0153542,"is task, referred to as confidence or quality estimation, is aimed at MT systems in use and therefore has no access to reference translations (Specia et al., 2010). Quality estimation can be performed at different levels of granularity. Sentence-level quality estimation (Specia et al., 2009; Blatz et al., 2004) is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of MT sentences annotated with quality labels. In the word-level variant of this task, each word in the MT output is to be judged as correct or incorrect (Luong et al., 2015; Bach et al., 2011), or labelled for a specific error type. Research in the field of quality estimation is focused on the design of features and the selection of appropriate learning schemes to predict translation quality, using source sentences, MT outputs, internal MT system information and source and target language corpora. In particular, features that measure the probability of the MT output with respect to a target LM, thus capturing translation fluency, have demonstrated highly competitive performance in a variety of settings (Shah et 3 UPF-Cobalt Review UPF-Cobalt1 is an alignment-based evaluation metric"
W16-2339,C04-1046,0,0.103666,"zm´an et al., 2014; Yu et al., 2015). However, none of the above approaches explicitly addresses the fluency of the MT output. Predicting MT quality with respect to the target language norms has been investigated in a different evaluation scenario, when human translations are not available as benchmark. This task, referred to as confidence or quality estimation, is aimed at MT systems in use and therefore has no access to reference translations (Specia et al., 2010). Quality estimation can be performed at different levels of granularity. Sentence-level quality estimation (Specia et al., 2009; Blatz et al., 2004) is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of MT sentences annotated with quality labels. In the word-level variant of this task, each word in the MT output is to be judged as correct or incorrect (Luong et al., 2015; Bach et al., 2011), or labelled for a specific error type. Research in the field of quality estimation is focused on the design of features and the selection of appropriate learning schemes to predict translation quality, using source sentences, MT outputs, internal MT system information and source and target l"
W16-2339,P14-1065,0,0.0258889,"Missing"
W16-2339,W05-0904,0,0.0601197,"luency information into reference-based evaluation. able from WMT15 Metrics Task and obtain very promising results, which rival the best-performing system submissions. We have also submitted the metric to the WMT16 Metrics Task. 2 Related Work The recent advances in the field of MT evaluation have been largely directed to improving the informativeness and accuracy of candidate-reference comparison. Meteor (Denkowski and Lavie, 2014) allows for stem, synonym and paraphrase matches, thus addressing the problem of acceptable linguistic variation at lexical level. Other metrics measure syntactic (Liu and Gildea, 2005), semantic (Lo et al., 2012) or even discourse similarity (Guzm´an et al., 2014) between candidate and reference translations. Further improvements have been recently achieved by combining these partial measurements using different strategies including machine learning techniques (Comelles et al., 2012; Gim´enez and M`arquez, 2010b; Guzm´an et al., 2014; Yu et al., 2015). However, none of the above approaches explicitly addresses the fluency of the MT output. Predicting MT quality with respect to the target language norms has been investigated in a different evaluation scenario, when human tra"
W16-2339,W12-3129,0,0.0342631,"-based evaluation. able from WMT15 Metrics Task and obtain very promising results, which rival the best-performing system submissions. We have also submitted the metric to the WMT16 Metrics Task. 2 Related Work The recent advances in the field of MT evaluation have been largely directed to improving the informativeness and accuracy of candidate-reference comparison. Meteor (Denkowski and Lavie, 2014) allows for stem, synonym and paraphrase matches, thus addressing the problem of acceptable linguistic variation at lexical level. Other metrics measure syntactic (Liu and Gildea, 2005), semantic (Lo et al., 2012) or even discourse similarity (Guzm´an et al., 2014) between candidate and reference translations. Further improvements have been recently achieved by combining these partial measurements using different strategies including machine learning techniques (Comelles et al., 2012; Gim´enez and M`arquez, 2010b; Guzm´an et al., 2014; Yu et al., 2015). However, none of the above approaches explicitly addresses the fluency of the MT output. Predicting MT quality with respect to the target language norms has been investigated in a different evaluation scenario, when human translations are not available"
W16-2339,E06-1032,0,0.139235,"Missing"
W16-2339,comelles-etal-2012-verta,0,0.0248089,"Missing"
W16-2339,W14-3336,0,0.0582917,"ly not indicative of an error and should be penalized less. Based on this observation, we introduce a separate set of features that compute the word-level measurements discussed above only for the words that are not aligned to the reference translation. This results in 49 additional features, grouped here for space reasons: In our work we focus on sentence-level metrics’ performance, which is assessed by converting metrics’ scores to ranks and comparing them to the human judgements with Kendall rank correlation coefficient (τ ). We use the WMT14 official Kendall’s Tau implementation (Mach´acˇ ek and Bojar, 2014). Following the standard practice at WMT and to make our work comparable to the official metrics submitted to the task, we exclude ties in human judgments both for training and for testing our system. Our model is a simple linear interpolation of the features presented in the previous sections. For tuning the weights, we use the learn-to-rank approach (Burges et al., 2005), which has been successfully applied in similar settings in previous work (Guzm´an et al., 2014; Stanojevic and Sima’an, 2015). We use a standard implementation of Logistic Regression algorithm from the Python toolkit scikit"
W16-2339,W14-3348,0,0.318909,"nificant improvement over the reference-based evaluation systems on the task of predicting human postediting effort. We follow this line of research by focusing specifically on integrating fluency information into reference-based evaluation. able from WMT15 Metrics Task and obtain very promising results, which rival the best-performing system submissions. We have also submitted the metric to the WMT16 Metrics Task. 2 Related Work The recent advances in the field of MT evaluation have been largely directed to improving the informativeness and accuracy of candidate-reference comparison. Meteor (Denkowski and Lavie, 2014) allows for stem, synonym and paraphrase matches, thus addressing the problem of acceptable linguistic variation at lexical level. Other metrics measure syntactic (Liu and Gildea, 2005), semantic (Lo et al., 2012) or even discourse similarity (Guzm´an et al., 2014) between candidate and reference translations. Further improvements have been recently achieved by combining these partial measurements using different strategies including machine learning techniques (Comelles et al., 2012; Gim´enez and M`arquez, 2010b; Guzm´an et al., 2014; Yu et al., 2015). However, none of the above approaches ex"
W16-2339,W15-3048,0,0.0157603,"tinguish between the quality of two alternative candidate translations. For example, it may well be the case that both MT outputs are very different from human reference, but one constitutes a valid alternative translation, while the other is totally unacceptable. Finally, Groups III and VI contain the results of the best-performing evaluation systems from the WMT15 Metrics Task, as well as the baseline BLEU metric (Papineni et al., 2002) and a strong competitor, Meteor (Denkowski and Lavie, 2014), which we reproduce here for the sake of comparison. DPMFComb (Yu et al., 2015) and RATATOUILLE (Marie and Apidianaki, 2015) use a learnt combination of the scores from different evaluation metrics, while BEER Treepel (Stanojevic and Sima’an, 2015) combines word matching, word order and syntax-level features. We note that the number and complexity of the metrics used in the above approaches is quite high. For instance, DPMFComb is based on 72 separate evaluation systems, including the resource-heavy linguistic Acknowledgments This work was partially funded by TUNER (TIN2015-65308-C5-5-R) and MINECO/FEDER, UE. Marina Fomicheva was supported by funding from the FI-DGR grant program of the Generalitat de Catalunya. Ir"
W16-2339,W12-3110,1,0.839561,"ge use. Conversely, due to the variability of linguistic expression, neither lexical nor syntactic differences from a particular human translation imply ill-formedness of the MT output. Sentence fluency can be described in terms of the frequencies of the words with respect to a target LM. Here, in addition to the LM-based features that have been shown to perform well for sentence-level quality estimation (Shah et al., 2013), we introduce more complex features derived from word-level n-gram statistics. Besides the word-based representation, we rely on Part-ofSpeech (PoS) tags. As suggested by (Felice and Specia, 2012), morphosyntactic information can be a good indicator of ill-formedness in MT outputs. First, we select 16 simple sentence-level features from previous work (Felice and Specia, 2012; Specia et al., 2010), summarized below.  7,     6,           5,    4, b(wi ) =      3,          2,    1, if wi−2 , wi−1 , wi exists in the model if wi−2 , wi−1 and wi−1 , wi both exist in the model if only wi−1 , wi exists in the model if only wi−2 , wi−1 and wi exist separately in the model if wi−1 and wi both exist in the model if only wi exists in the model if wi is an out-o"
W16-2339,P02-1040,0,0.106436,"pensive, and objective numerical measurements of translation quality. As a cost-effective alternative to manual evaluation, the main concern of automatic evaluation metrics is to accurately approximate human judgments. The vast majority of evaluation metrics are based on the idea that the closer the MT output is to a human reference translation, the higher its quality. The evaluation task, therefore, is typically approached by measuring some kind of similarity between the MT (also called candidate translation) and a reference translation. The most widely used evaluation metrics, such as BLEU (Papineni et al., 2002), follow a simple strategy of counting the number of matching words or word sequences in the candidate and reference 483 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 483–490, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics al., 2013). Both translation evaluation and quality estimation aim to evaluate MT quality. Surprisingly, there have been very few attempts at joining the insights from these two related tasks. A notable exception is the work by Specia and Gim´enez (2010), who explore the combination of a"
W16-2339,L16-1437,1,0.831323,"The most important feature of the metric is a syntactically informed context penalty aimed at penalizing the matches of similar words that play different roles in the candidate and reference sentences. The metric has achieved highly competitive results on the data from previous WMT tasks, showing that the context penalty allows to better discriminate between acceptable candidatereference differences and the differences incurred by MT errors (Fomicheva et al., 2015b). Below we briefly review the main components of the metric. For a detailed description of the metric the reader is referred to (Fomicheva and Bel, 2016). 3.1 Alignment The alignment module of UPF-Cobalt builds on an existing system – Monolingual Word Aligner (MWA), which has been shown to significantly outperform state-of-the-art results for monolingual alignment (Sultan et al., 2014). We increase the coverage of the aligner by comparing distributed word representations as an additional source of lexical similarity information, 1 The metric is freely available for download at https://github.com/amalinovskiy/ upf-cobalt. 484 which allows to detect cases of quasi-synonyms (Fomicheva and Bel, 2016). 3.2 pair. The sentence-level average can be ob"
W16-2339,2013.mtsummit-papers.21,1,0.747085,"cular word or expression may be similar in meaning to the one present in the reference (adequacy), but awkward or even erroneous if considered in the context of the norms of the target language use. Conversely, due to the variability of linguistic expression, neither lexical nor syntactic differences from a particular human translation imply ill-formedness of the MT output. Sentence fluency can be described in terms of the frequencies of the words with respect to a target LM. Here, in addition to the LM-based features that have been shown to perform well for sentence-level quality estimation (Shah et al., 2013), we introduce more complex features derived from word-level n-gram statistics. Besides the word-based representation, we rely on Part-ofSpeech (PoS) tags. As suggested by (Felice and Specia, 2012), morphosyntactic information can be a good indicator of ill-formedness in MT outputs. First, we select 16 simple sentence-level features from previous work (Felice and Specia, 2012; Specia et al., 2010), summarized below.  7,     6,           5,    4, b(wi ) =      3,          2,    1, if wi−2 , wi−1 , wi exists in the model if wi−2 , wi−1 and wi−1 , wi both exis"
W16-2339,2010.amta-papers.3,1,0.729752,"Missing"
W16-2339,2009.eamt-1.5,1,0.839016,"r human translation does not imply that the MT output is disfluent (Fomicheva et al., 2015a). We propose to explicitly model translation fluency in reference-based MT evaluation. To this end, we develop a number of features representing translation fluency and integrate them with our reference-based metric UPF-Cobalt, which was originally presented at WMT15 (Fomicheva et al., 2015b). Along with the features based on the target Language Model (LM) probability of the MT output, which have been widely used in the related fields of speech recognition (Uhrik and Ward, 1997) and quality estimation (Specia et al., 2009), we design a more detailed representation of MT fluency that takes into account the number of disfluent segments observed in the candidate translation. We test our approach with the data availThe vast majority of Machine Translation (MT) evaluation approaches are based on the idea that the closer the MT output is to a human reference translation, the higher its quality. While translation quality has two important aspects, adequacy and fluency, the existing referencebased metrics are largely focused on the former. In this work we combine our metric UPF-Cobalt, originally presented at the WMT15"
W16-2339,P15-4020,1,0.82252,"rpolation of the features presented in the previous sections. For tuning the weights, we use the learn-to-rank approach (Burges et al., 2005), which has been successfully applied in similar settings in previous work (Guzm´an et al., 2014; Stanojevic and Sima’an, 2015). We use a standard implementation of Logistic Regression algorithm from the Python toolkit scikit-learn5 . The model is trained on WMT14 dataset and tested on WMT15 dataset. For the extraction of word-level backoff behaviour values and sentence-level fluency features, we use Quest++6 , an open source tool for quality estimation (Specia et al., 2015). We employ the LM used to build the baseline system for WMT15 Quality Estimation Task (Bojar et al., 2015).7 This LM provided was trained on data from the WMT12 translation task (a combination of news and Europarl data) and thus matches the domain of the dataset we use in our experiments. PoS tagging was performed with TreeTagger (Schmid, 1999). • Summary statistics of the LM backoff behaviour (word and PoS-tag LM) • Summary statistics of the LM backoff behaviour for non-aligned words only (word and PoS tag LM) • Percentage and number of words with low backoff behaviour value (word and PoS ta"
W16-2339,W15-3050,0,0.216238,"Missing"
W16-2339,Q14-1018,0,0.0166703,"titive results on the data from previous WMT tasks, showing that the context penalty allows to better discriminate between acceptable candidatereference differences and the differences incurred by MT errors (Fomicheva et al., 2015b). Below we briefly review the main components of the metric. For a detailed description of the metric the reader is referred to (Fomicheva and Bel, 2016). 3.1 Alignment The alignment module of UPF-Cobalt builds on an existing system – Monolingual Word Aligner (MWA), which has been shown to significantly outperform state-of-the-art results for monolingual alignment (Sultan et al., 2014). We increase the coverage of the aligner by comparing distributed word representations as an additional source of lexical similarity information, 1 The metric is freely available for download at https://github.com/amalinovskiy/ upf-cobalt. 484 which allows to detect cases of quasi-synonyms (Fomicheva and Bel, 2016). 3.2 pair. The sentence-level average can be obtained in a straightforward way from the word-level values (we use it as a feature in the decomposed version of the metric below). Scoring UPF-Cobalt’s sentence-level score is a weighted combination of precision and recall over the sum"
W16-2339,W15-3053,0,0.32949,"e-reference comparison. Meteor (Denkowski and Lavie, 2014) allows for stem, synonym and paraphrase matches, thus addressing the problem of acceptable linguistic variation at lexical level. Other metrics measure syntactic (Liu and Gildea, 2005), semantic (Lo et al., 2012) or even discourse similarity (Guzm´an et al., 2014) between candidate and reference translations. Further improvements have been recently achieved by combining these partial measurements using different strategies including machine learning techniques (Comelles et al., 2012; Gim´enez and M`arquez, 2010b; Guzm´an et al., 2014; Yu et al., 2015). However, none of the above approaches explicitly addresses the fluency of the MT output. Predicting MT quality with respect to the target language norms has been investigated in a different evaluation scenario, when human translations are not available as benchmark. This task, referred to as confidence or quality estimation, is aimed at MT systems in use and therefore has no access to reference translations (Specia et al., 2010). Quality estimation can be performed at different levels of granularity. Sentence-level quality estimation (Specia et al., 2009; Blatz et al., 2004) is addressed as"
W16-2346,P14-2074,1,0.681895,"https://github.com/Z -TANG/re-scorer. 10 https://github.com/elliottd/Grounded Translation 11 548 https://github.com/jhclark/multeval 4.2 System submissions that preserved casing or had been tokenised were further processed for lowercasing and detokenisation.12 For all of these preprocessing steps, we used Moses scripts.13 Task 2: Crosslingual Image Description Table 4 presents the final results for the Crosslingual Image Description task. Meteor is the primary evaluation measure because it has been shown to have a much stronger correlation with human judgements than BLEU or TER for this task (Elliott and Keller, 2014). The data for this task was lowercased and had punctuation removed where necessary. The strongest performing constrained submission (LIUM 2 TextNMT C) does not use any visual features. Including multimodal features (i.e., LIUM 2 MultimodalNMT C) results in a 2.8 Meteor drop in performance for that model type. The baseline system 2 GroundedTranslation C outperformed all but these two systems. In general, there is a wide range of performances, and an intriguing discrepancy between Meteor and BLEU rankings. This discrepancy was much larger than the one observed in Task 1, where the overall ranki"
W16-2346,W16-3210,1,0.489205,"Missing"
W16-2346,W15-3001,1,0.437372,"Missing"
W16-2346,N10-1125,0,0.0165056,"he following main goals: • To push existing work on multimodal language processing towards multilingual multimodal language processing. Introduction • To investigate the effectiveness of information from images in machine translation. In recent years, significant research has been done to address problems that require joint modelling of language and vision. Examples of popular applications involving both Natural Language Processing (NLP) and Computer Vision (CV) include image description generation and video captioning (Bernardi et al., 2016), image retrieval based on textual and visual cues (Feng and Lapata, 2010), visual question answering (Yang et al., 2015), among many others (see (Ramisa et al., 2016) for more examples). With very few exceptions (Grubinger et al., 2006; Funaki and Nakayama, 2015; • To investigate the effectiveness of crosslingual textual information in image description generation. The challenge was organised in the framework of the well-established WMT series of shared tasks.1 Participants were called to submit systems focusing on either or both of these task variants. The tasks differ in the training data and in 1 http://www.statmt.org/wmt16/ 543 Proceedings of the First Conferen"
W16-2346,W16-2358,0,0.527348,"Missing"
W16-2346,D15-1070,0,0.0302355,"information from images in machine translation. In recent years, significant research has been done to address problems that require joint modelling of language and vision. Examples of popular applications involving both Natural Language Processing (NLP) and Computer Vision (CV) include image description generation and video captioning (Bernardi et al., 2016), image retrieval based on textual and visual cues (Feng and Lapata, 2010), visual question answering (Yang et al., 2015), among many others (see (Ramisa et al., 2016) for more examples). With very few exceptions (Grubinger et al., 2006; Funaki and Nakayama, 2015; • To investigate the effectiveness of crosslingual textual information in image description generation. The challenge was organised in the framework of the well-established WMT series of shared tasks.1 Participants were called to submit systems focusing on either or both of these task variants. The tasks differ in the training data and in 1 http://www.statmt.org/wmt16/ 543 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 543–553, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics translate Ein brauner Hund ..."
W16-2346,W16-2359,1,0.497561,"Missing"
W16-2346,P11-2031,0,0.0597769,"fter that, an SVM-based model decides which one is better according to the sentence’s score from a language model and the score from the model that generated the sentence. The only difference between the two submissions is that the unconstrained one used Task 1 dataset in the training of text translator. 4 Results Tables 3 and 4 present the official results for the Multimodal Machine Translation and Crosslingual Image Description tasks. We evaluated the submissions based on Meteor (Denkowski and Lavie, 2014) (primary), BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) using MultEval (Clark et al., 2011)11 with default parameters. 4.1 Baseline - GroundedTranslation (Tasks 1 & 2) This method follows (Elliott et al., 2015):10 A source language multimodal RNN model is initialised with a visual feature vector (i.e., a multimodal model for the source language). The final hidden state is then used to initialise a target Task 1 Table 4 shows the final results for the Multimodal Machine Translation task on the official test set, where systems are ranked by their Meteor scores. Meteor, BLEU and TER were computed based on the single reference (human translation) provided for the test set. For Meteor, w"
W16-2346,P16-1227,0,0.269311,"ken classes (Stewart et al., 2014). 4 http://github.com/BVLC/caffe/releases /tag/rc2 5 http://github.com/karpathy/neuraltalk /tree/master/matlab_features_reference 6 https://glosbe.com/en/de/ http://www.crowdflower.com http://illinois.edu/fb/sec/229675 545 ID CMU+NTU CUNI DCU DCU-UVA HUCL IBM-IITM-Montreal-NYU LIUM SHEF UPC UPCb Participating team Carnegie Melon University (Huang et al., 2016) Univerzita Karlova v Praze (Libovick´y et al., 2016) Dublin City University (Hokamp and Calixto, 2016) Dublin City University & Universiteit van Amsterdam (Calixto et al., 2016) Universit¨at Heidelberg (Hitschler et al., 2016) IBM Research India, IIT Madras, Universit´e de Montr´eal & New York University Laboratoire d’Informatique de l’Universit´e du Maine (Caglayan et al., 2016) University of Sheffield (Shah et al., 2016) Universitat Polit`ecnica de Catalunya (Rodr´ıguez Guasch and Costa-juss`a, 2016) Universitat Polit`ecnica de Catalunya Table 2: Participants in the WMT16 multimodal machine translation shared task. DCU (Task 1) Both submissions from DCU are neural MT systems with an attention mechanism on the source-side representation (Bahdanau et al., 2014). The first submission is text-only, and the second sub"
W16-2346,W14-3348,0,0.307313,", 2014). The other one is generated based on the image feature using method proposed in (Vinyals et al., 2015). After that, an SVM-based model decides which one is better according to the sentence’s score from a language model and the score from the model that generated the sentence. The only difference between the two submissions is that the unconstrained one used Task 1 dataset in the training of text translator. 4 Results Tables 3 and 4 present the official results for the Multimodal Machine Translation and Crosslingual Image Description tasks. We evaluated the submissions based on Meteor (Denkowski and Lavie, 2014) (primary), BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) using MultEval (Clark et al., 2011)11 with default parameters. 4.1 Baseline - GroundedTranslation (Tasks 1 & 2) This method follows (Elliott et al., 2015):10 A source language multimodal RNN model is initialised with a visual feature vector (i.e., a multimodal model for the source language). The final hidden state is then used to initialise a target Task 1 Table 4 shows the final results for the Multimodal Machine Translation task on the official test set, where systems are ranked by their Meteor scores. Meteor, BLEU and TE"
W16-2346,P16-1159,0,0.00608079,"Missing"
W16-2346,W16-2360,0,0.440474,"Missing"
W16-2346,2006.amta-papers.25,0,0.156429,"proposed in (Vinyals et al., 2015). After that, an SVM-based model decides which one is better according to the sentence’s score from a language model and the score from the model that generated the sentence. The only difference between the two submissions is that the unconstrained one used Task 1 dataset in the training of text translator. 4 Results Tables 3 and 4 present the official results for the Multimodal Machine Translation and Crosslingual Image Description tasks. We evaluated the submissions based on Meteor (Denkowski and Lavie, 2014) (primary), BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) using MultEval (Clark et al., 2011)11 with default parameters. 4.1 Baseline - GroundedTranslation (Tasks 1 & 2) This method follows (Elliott et al., 2015):10 A source language multimodal RNN model is initialised with a visual feature vector (i.e., a multimodal model for the source language). The final hidden state is then used to initialise a target Task 1 Table 4 shows the final results for the Multimodal Machine Translation task on the official test set, where systems are ranked by their Meteor scores. Meteor, BLEU and TER were computed based on the single reference (human translation) prov"
W16-2346,P07-2045,0,0.0201639,"five descriptions of the same image in the target language, created independently from the corresponding source description (image description variant). The data used for both tasks is an extended version of the Flickr30K dataset. Participants were also allowed to use external data and resources for unconstrained submissions. Participants were encouraged to make use of both the sentences and the images as part of their submissions but they were not required to do so. The baseline systems for the translation task were a text-only Moses phrase-based statistical machine translation (SMT) model (Koehn et al., 2007) and the GroundedTranslation multilingual image description model (Elliott et al., 2015) (in particular, the MLM→LM variant). The baseline system for the description generation task was also the GroundedTranslation model. In this paper we describe the data, image features and participants of the shared task (Sections 2 and 3), present its main findings (Section 4), and discuss interesting issues and directions for future research (Section 5). 2 courage participation we released two types of features extracted from the images. The use of such features was not mandatory, and participants could a"
W16-2346,2014.amta-researchers.3,0,0.0922404,"Missing"
W16-2346,W16-2361,0,0.218958,"Missing"
W16-2346,P02-1040,0,0.10153,"1) The approach integrates separate attention mechanisms over the source language and the CONV5,4 visual features in a single decoder. The source language was represented using a bidirectional RNN with Gated Recurrent Units (GRU); the images were represented as 196x512 matrix from the pre-trained VGG-19 convolutional network. A separate, time546 VGG16 deep convolutional model (Simonyan and Zisserman, 2015), supplied by the task organisers) for retrieval of matches. The retrieval model architecture was identical to that in Hitschler et al. (2016). Instead of TF-IDF, a modified version of BLEU (Papineni et al., 2002) was used in order to re-score hypotheses based on the target-language text of retrieved captions. Fixed settings were used for some parameters (d = 90, b = 0.01 and km = 20), while kr and λ were optimised on the validation set (parameters as defined in (Hitschler et al., 2016)). LIUM 1 MosesNMTRnnLMSent2Vec C and LIUM 1 MosesNMTRnnLMSent2VecVGGFC7 C are phrase-based systems based on Moses (14 standard features plus operation sequence models. They include re-scoring with several models and more particularly with a continuous space language model (CSLM) and a neural MT system (see TextNMT syste"
W16-2346,Q14-1006,0,0.409641,"a regionbased convolution neural network (RCNN) are designed to be appended in the head/tail of the textual feature or dissipated in parallel long short term memory (LSTM) threads to assist the LSTM reader in computing a representation. For rescoring, an additional bilingual dictionary is used to select the best sentence from candidates generated by five different models. The submission is thus unconstrained, with the German-English Dictionary from GLOSBE6 used as additional resource. Datasets and image features We created a new dataset for the shared task by extending the Flickr30K dataset (Young et al., 2014) into another language. The Multi30K dataset (Elliott et al., 2016) contains two types of multilingual data: a corpus of English sentences translated into German (used for Task 1), and a corpus of independently collected English and German sentences (used for Task 2). For the translation corpus, one sentence (of five) was chosen for professional translation such that the final dataset is a combination of short, medium, and long length sentences. The second corpus consists of crowdsourced descriptions gathered from Crowdflower,2 where each worker generated an independent description of the imag"
W16-2346,W16-2363,1,0.301031,"uilt using only the shared task data. This is a remarkable result, given the size of the dataset: 29,000 parallel segments. They all use additional features to re-rank the k-best output of a text-only phrase-based system, including visual features, although these seem to play a minor role and lead to only marginally better results. Submissions based on the output of a Moses translation model – like the main baseline (1 en-de-Moses C) – have very similar Meteor scores. In fact, SHEF 1 en-de-Moses-rerank C and CMU+NTU 1 MNMT+RERANK U are not considered significantly different from this baseline Shah et al. (2016) provide some analysis on the differences between SHEF 1 en-de-Mosesrerank C and 1 en-de-Moses C. They show that the output of these systems differ in 260 out of the 1,000 segments. However, despite differences in the actual translations, the Meteor scores for many of these cases may be the same/close. Disappointingly, truly multimodal systems, which in most cases use neural MT approaches (e.g. CUNI 1 MMS2S-1 C, DCU 1 min-riskmultimodal C) do not fare as well as the text-only SMT systems (or those followed by multimodalbased translation rescoring), except when additional resources are used for"
W16-2346,N12-1017,0,\N,Missing
W16-2346,P10-4002,0,\N,Missing
W16-2346,W16-2362,0,\N,Missing
W16-2363,P11-1020,0,0.0315612,"Missing"
W16-2363,N12-1047,0,0.0322309,"Missing"
W16-2363,P07-2045,0,0.00971556,"Missing"
W16-2363,Q14-1006,0,0.2479,"various applications related to language processing have been gaining wider attention from the research community in recent years. The main motivation is to investigate whether contextual information from various sources can be helpful in improving system performance. Multimodal approaches have been ex660 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 660–665, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to the 1,000 categories of ILSVRC, and many categories may not be relevant to the Flickr30K dataset (Young et al., 2014) that is used for this task, and vice-versa, that is, many of the objects in the Flickr30K dataset may not exist in the ILSVRC dataset. This implies that the classification error in our dataset is probably much higher. We use image features from a Convolutional Neural Network (CNN) along with standard Moses features to re-rank the n-best list. We also propose an alternative scheme for the German-toEnglish direction, where terms in the English image descriptions are matched against 1,000 WordNet synsets, and the probability of these synsets occurring in the image estimated using CNN predictions"
W16-2363,P13-1006,0,0.0669127,"Missing"
W16-2363,P00-1056,0,0.0851616,"Missing"
W16-2363,P03-1021,0,0.0233193,"orrect if the correct category is within the top 5 guesses), and we expect such errors to propagate downstream to the translation task. Moreover, the classifiers are tuned 3.2 Training Both our submissions are based on the Moses SMT toolkit (Koehn et al., 2007) to build phrasebased SMT models. They are constructed as follows: First, word alignments in both directions are calculated using GIZA++ (Och and Ney, 2000). The phrases and reordering tables are extracted using the default settings of the Moses toolkit. The parameters of Moses were tuned on the provided development set, using the MERT (Och, 2003) algorithm. 4-gram back-off language models were built using the target side of the parallel corpus. Training was performed using only the data provided by the task organisers, and so systems for both directions were built in the constrained setting. We extracted the 100 best translations with our SMT model and re-ranked them using the image features described in Section 2, along with the standard Moses features. We used an off-the-shelf tool 1 to re-rank the n-best translations. More 1 https://github.com/moses-smt/ mosesdecoder/tree/master/scripts/ nbest-rescore 661 specifically, we performed"
W16-2363,W16-3210,1,\N,Missing
W16-2381,2009.eamt-1.5,1,0.8201,"f actions, which in this case are tag predictions. This setting allows us to incorporate structural information in the classifier by using features based on previous tag predictions. For instance, let us assume that we are trying to predict the tag ti for word wi . A simple classifier can use features derived from wi and also any other words in the sentence. By framing this as a sequence, it can also use features extracted from the previously predicted tags t{1:i−1} . Introduction Quality estimation (QE) models aim at predicting the quality of machine translated (MT) text (Blatz et al., 2004; Specia et al., 2009). This prediction can be at several levels, including word-, sentenceand document-level. In this paper we focus on our submission to the word-level QE WMT 2016 shared task, where the goal is to assign quality labels to each word of the output of an MT system. Word-level QE is traditionally treated as a structured prediction problem, similar to part-of-speech (POS) tagging. The baseline model used in the shared task employs a Conditional Random Field (CRF) (Lafferty et al., 2001) with a set of baseline features. Our system uses a linear classification model trained with imitation learning (Daum"
W16-2381,C04-1046,0,0.0316158,"perform a sequence of actions, which in this case are tag predictions. This setting allows us to incorporate structural information in the classifier by using features based on previous tag predictions. For instance, let us assume that we are trying to predict the tag ti for word wi . A simple classifier can use features derived from wi and also any other words in the sentence. By framing this as a sequence, it can also use features extracted from the previously predicted tags t{1:i−1} . Introduction Quality estimation (QE) models aim at predicting the quality of machine translated (MT) text (Blatz et al., 2004; Specia et al., 2009). This prediction can be at several levels, including word-, sentenceand document-level. In this paper we focus on our submission to the word-level QE WMT 2016 shared task, where the goal is to assign quality labels to each word of the output of an MT system. Word-level QE is traditionally treated as a structured prediction problem, similar to part-of-speech (POS) tagging. The baseline model used in the shared task employs a Conditional Random Field (CRF) (Lafferty et al., 2001) with a set of baseline features. Our system uses a linear classification model trained with im"
W16-2386,2011.mtsummit-papers.17,1,0.72267,"te their performance on the fly. The problem has been modelled to estimate the quality of translations at the word, sentence and document levels (Bojar et al., 2015). Word-level QE can be particularly useful for post-editing of machinetranslated texts: if we know the erroneous words in a sentence, we can highlight them to attract posteditor’s attention, which should improve both productivity and final translation quality. However, the choice of words in an automatically translated sentence is motivated by the context, so MT errors are also context-dependent. Moreover, as it has been shown in (Blain et al., 2011), errors in multiple adjacent words can be caused by a single incorrect decision — e.g. an incorrect lexical choice can result in errors in all its syntactic de800 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 800–805, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics belled as “BAD” or “OK” based on labelling generated with the TERcom tool1 : if an edit operation (substitution or insertion) was applied to a word, it is labelled as “BAD”; contrarily, if the word was left unchanged, it is considered “OK”. For"
W16-2386,W15-3001,1,0.834059,"n terms of multiplication of F1 -scores (primary evaluation metric), but are considerably different in terms of the F1 scores for individual classes. 1 Introduction Quality Estimation (QE) of Machine Translation (MT) is the task of determining the quality of an automatically translated text without comparing it to a reference translation. This task has received more attention recently because of the widespread use of MT systems and the need to evaluate their performance on the fly. The problem has been modelled to estimate the quality of translations at the word, sentence and document levels (Bojar et al., 2015). Word-level QE can be particularly useful for post-editing of machinetranslated texts: if we know the erroneous words in a sentence, we can highlight them to attract posteditor’s attention, which should improve both productivity and final translation quality. However, the choice of words in an automatically translated sentence is motivated by the context, so MT errors are also context-dependent. Moreover, as it has been shown in (Blain et al., 2011), errors in multiple adjacent words can be caused by a single incorrect decision — e.g. an incorrect lexical choice can result in errors in all it"
W16-2386,W03-0413,0,0.0829447,"Missing"
W16-2386,2015.iwslt-papers.4,1,0.440494,"03), but there the multi-word nature of predictions was motivated by the architecture of the MT system used in the experiment: an interactive MT system which did not translate entire sentences, but rather predicted the next n word translations in a sentence. An approach was designed to estimate the confidence of the MT system about the prediction and was aimed at improving translation prediction quality. The phrase-level QE in its current formulation – estimation of the quality of phrases in a pretranslated sentence using external features of these phrases – was first addressed in the work of Logacheva and Specia (2015), where the authors segmented automatically translated sentences into phrases, labelled these phrases based on wordlevel labels and trained several phrase-level QE models using different feature sets and machine learning algorithms. The baseline phrase-level QE system used in this shared task was based on the results in (Logacheva and Specia, 2015). This year’s Conference on Statistical Machine Translation (WMT16) includes a shared task on phrase-level QE (QE Task 2p) for the first time. This task uses the same training and test data as the one used for the word-level QE task (QE Task 2): the"
W16-2386,W15-3039,1,0.887997,"Missing"
W16-2386,W13-2248,0,0.0282407,"can be smoothed by filtering out sentences with little or no errors (Logacheva et al., 2015). Admittedly, if a sentence has no “BAD” words it lacks information about one of the classes of the problem, and thus it is less informative. We thus applied the same strategy to phrase-level QE: we ranked the • part of speech of the source/target left/right context (string) — we check parts of speech of words that precede or follow the phrase in the sentence. Some of these features (e.g. highest n-gram order, backoff behaviour, contexts) are used because they have been shown useful for word-level QE (Luong et al., 2013), others are included because we believe they can be relevant for understanding the quality of phrases. We compare the performance of the baseline feature set with the feature set extended with context information. The QE models are trained using CRFSuite toolkit (Okazaki, 2007). We chose to train a Conditional Random Fields (CRF) model because it has shown high performance in wordlevel QE (Luong et al., 2013) as well as phraselevel QE (Logacheva and Specia, 2015) tasks. CRFSuite provides five optimisation algorithms: L-BFGS with L1/L2 regularization (lbfgs), SGD with L2-regularization (l2sgd)"
W16-2386,W16-2392,1,0.393881,"s in the current phrase. • Word-level prediction features: 4/5. number of words predicted as “OK”/“BAD” in the current phrase; 6/7. number of words predicted as “OK”/“BAD” in the sentence. Similarly to the context-based model described in Section 2, we trained our prediction-based model with the CRFSuite toolkit and the PassiveAggressive algorithm. The phrase segmentation features are extracted from the data itself and do not need any additional information. The sentence-level score is produced by the SHEFLIUM-NN system, a sentence-level QE system with neural network features as described in (Shah et al., 2016). The word-level prediction features are produced by the SHEF-MIME QE system (Beck et al., 2016), which uses imitation learning to predict translation quality at the word level. Prediction-based model Following the approach in (Specia et al., 2015), which makes use of word-level predictions at sentence level, we describe here the first attempt to using both word-level and sentence-level predictions for phrase-level QE (W&SLP4PT). Phrase-level labels by definition depend on the 4 Results We submitted two phrase-level QE systems: the first one uses the set of baseline features enhanced with cont"
W16-2386,P15-4020,1,0.914226,"belled as “OK” if it contains only words labelled as “OK”; if one or more words in a phrase are “BAD”’, the phrase is “BAD” itself. The predictions are done at the phrase level, but evaluated at the word level: for the evaluation phrase-level labels are unrolled back to their word-level versions (i.e. if a threeword phrase is labelled as “BAD”, it is equivalent to three “BAD” word-level labels). The baseline phrase-level features provided by the organisers of the task are black-box features that were originally used for sentence-level quality estimation and extracted using the QuEst toolkit2 (Specia et al., 2015). While this feature set considers many aspects of sentence quality (mostly the ones that do not depend on internal MT system information and do not require language-specific resources), it has an important limitation when applied to phrases. Namely, it does not take into account the context of the phrase, i.e. words and phrases in the sentence, either before or after the phrase of interest. In order to advance upon the baseline results, we enhanced the baseline feature set with contextual information for phrases. Another approach we experimented with is the use of predictions made by QE model"
W16-2388,W15-3001,1,0.818079,"her a regression or a classification problem. Sentence-level QE has been covered by shared tasks organised by WMT since 2012, with subsequent years covering also word and documentlevel tasks. Recent advances in Distributional Semantics have been showing promising results in the context of QE strategies for different prediction levels. An example of that are modern word embedding architectures, such as the CBOW and Skip-Gram models introduced by (Mikolov et al., 2013b), which have been used as features in some of the best ranking systems in the sentence and word-level QE shared tasks of WMT15 (Bojar et al., 2015). Word embeddings are not only versatile, but also cheap to produce, making for both reliable and cost-effective QE solutions. Neural Networks have also been successfully employed in QE. The FBK-UPV-UEdin (Bojar et al., 2014) and HDCL (Bojar et al., 2015) systems are good examples of that. They achieved 1st and 2nd places in the word-level QE tasks of WMT14 and WMT15, respectively, outperforming strategies that resort to much more resource-heavy features. Another successful example are neural Language Models for sentence-level QE (Shah et al., 2015). We were not able to find, however, any exam"
W16-2388,P13-1151,0,0.0165544,"NT/SVM1 RTM/RTM-SVR BASELINE SHEF/SimpleNets-SRC SHEF/SimpleNets-TGT r 0.525 0.460 0.451 0.447 0.430 0.412 0.377 0.376 0.370 0.363 0.358 0.351 0.320 0.283 MAE 12.30 13.58 12.88 13.52 12.97 19.57 13.60 13.46 13.43 20.01 13.59 13.53 13.92 14.35 RMSE 16.41 18.60 17.03 18.38 17.33 24.11 17.64 17.81 18.15 24.63 18.06 18.39 18.23 18.22 Table 1: Sentence-level QE scores of systems submitted to the WMT16 task models over a corpus of around 7 billion words comprised by SubIMDB (Paetzold and Specia, 2016b), UMBC webbase2 , News Crawl3 , SUBTLEX (Brysbaert and New, 2009), Wikipedia and Simple Wikipedia (Kauchak, 2013). For evaluation we use the task’s official metrics, which are Pearson correlation (r), Mean Average Error and Root Mean Squared Error. We compare our SimpleNets with the baseline provided by the task organisers, as well as all other systems submitted. The baseline uses SVM regression with an RBF kernel and grid search for parameter optimisation. 6 itself. Interestingly, the performance scores suggest that the model employed by SimpleNets is more proficient in learning how difficult it will be for the source sentence to be translated. The difference in performance between the SimpleNets varian"
W16-2388,W15-1521,0,0.042722,"Missing"
W16-2388,W16-4912,0,0.0250797,"Each of the M rows in matrix M×N represent a given word in the n-gram, and each of the N columns, its embedding values. If an n-gram is smaller than N , the matrix is padded with embedding values composed strictly of zeroes. The SimpleNets Approach SimpleNets aim to provide a resource-light and language agnostic approach for sentence-level QE. Our main goal in conceiving SimpleNets was to create a reliable enough solution that could be cheaply and easily adapted to other language pairs, moving away from the use of extensive feature engineering. The SimpleNets approach was first introduced by Paetzold and Specia (2016a) as a solution to the shared task on Quality Assessment for Text Simplification of QATS 20161 , in which participants were asked to create systems that predict discrete quality labels for a set of automatically produced text simplifications. Labels could take three values: “Good”, “Ok” and “Bad”. Text Simplification differs from Machine Translation in the sense 1 5. Learning: Training instances are then fed into a deep Long Short-Term Memory (LSTM) Recurrent Neural Network in minibatches so that a quality prediction model can be learned. Notice that this process yields a model that predicts"
W16-2388,2014.eamt-1.22,1,0.747524,"n more about a translation’s quality by focusing on the original sentence, rather than on the translation itself. 1 Introduction The task of Machine Translation Quality Estimation (QE) has gained noticeable popularity in the last few years. The goal of QE is to predict the quality of translations produced by a certain Machine Translation (MT) system in the absence of reference translations. Reliable solutions for QE can be useful in various tasks, such as improving post-editing efficiency (Specia, 2011), selecting high quality translations (Soricut and Echihabi, 2010), translation re-ranking (Shah and Specia, 2014), and visual assistance for manual translation revision (Bach et al., 2011). QE can be performed in various ways in order to suit different purposes. The most widely addressed form of this task is sentence-level QE. Most existing work addresses this task as a supervised learning problem, in which a set of training examples is used to learn a model that predicts the quality of unseen translations. As quality labels, previous work uses either real valued scores estimated by humans, which require for a given QE 812 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task"
W16-2388,D15-1125,1,0.835695,"ce and word-level QE shared tasks of WMT15 (Bojar et al., 2015). Word embeddings are not only versatile, but also cheap to produce, making for both reliable and cost-effective QE solutions. Neural Networks have also been successfully employed in QE. The FBK-UPV-UEdin (Bojar et al., 2014) and HDCL (Bojar et al., 2015) systems are good examples of that. They achieved 1st and 2nd places in the word-level QE tasks of WMT14 and WMT15, respectively, outperforming strategies that resort to much more resource-heavy features. Another successful example are neural Language Models for sentence-level QE (Shah et al., 2015). We were not able to find, however, any examples of sentence-level QE systems that combine word embedding models and Neural Networks. In this paper, we present our efforts in doing so. We introduce SimpleNets: the resource-light and language agnostic sentence-level QE systems submitted to WMT16 that exploit the principle of compositionality for QE. In the Sections that follow, we describe the sentence-level QE task of WMT16, introduce the approach used by the SimpleNets systems, and present the results obtained. We introduce SimpleNets: a resource-light solution to the sentence-level Quality"
W16-2388,2006.amta-papers.25,0,0.0777774,"nd meaning. For the Quality Assessment for Text Simplification task of QATS 2016, SimpleNets used the approach illustrated in Figure 1. For training, it performed the following five steps: SimpleNets are two systems submitted to the sentence-level QE task of WMT16. In this task, participants were challenged to predict real-valued quality scores in 0,100 of sentences translated from English into German. The translations were produced by an in-house phrase-based Statistical Machine Translation system, and were then postedited by professional translators. The real-valued quality scores are HTER (Snover et al., 2006) values that represent the post-editing effort spent on each given translation. The task organisers provided three datasets: 1. Decomposition: Given a simplification and maximum n-gram size M , it obtains the ngrams with size 1 ≤ n ≤ M of both original and simplified sentences. • Training: Contains 12,000 translation instances accompanied by their respective postedits and HTER values. 2. Union: It then creates a pool of n-grams by simply obtaining the union of n-grams from the original and simplified sentences. • Development: Contains 1,000 translation instances accompanied by their respective"
W16-2388,P10-1063,0,0.0316205,"eriments show that, surprisingly, our models can learn more about a translation’s quality by focusing on the original sentence, rather than on the translation itself. 1 Introduction The task of Machine Translation Quality Estimation (QE) has gained noticeable popularity in the last few years. The goal of QE is to predict the quality of translations produced by a certain Machine Translation (MT) system in the absence of reference translations. Reliable solutions for QE can be useful in various tasks, such as improving post-editing efficiency (Specia, 2011), selecting high quality translations (Soricut and Echihabi, 2010), translation re-ranking (Shah and Specia, 2014), and visual assistance for manual translation revision (Bach et al., 2011). QE can be performed in various ways in order to suit different purposes. The most widely addressed form of this task is sentence-level QE. Most existing work addresses this task as a supervised learning problem, in which a set of training examples is used to learn a model that predicts the quality of unseen translations. As quality labels, previous work uses either real valued scores estimated by humans, which require for a given QE 812 Proceedings of the First Conferenc"
W16-2388,P15-4020,1,0.865284,"Attribution: Exploiting an interpretation of the principle of compositionality, which states that the quality of a simplification can be determined by the quality of its n-grams, it assigns the quality label of the simplification instance itself to each and every n-gram in the pool. • Test: Contains 2,000 translation instances only, without their respective post-edits or HTER values. Each instance is composed by the original sentence in English along with its translation in German. HTER scores were capped to 100. The organisers also provided 17 baseline feature values extracted using QuEst++ (Specia et al., 2015) for each dataset. 3 4. Structuring: Using a trained word embeddings model, it transforms each n-gram into a training instance described by a matrix M×N , where M is the previously mentioned maximum n-gram size, and N the size of the word embeddings used. Each of the M rows in matrix M×N represent a given word in the n-gram, and each of the N columns, its embedding values. If an n-gram is smaller than N , the matrix is padded with embedding values composed strictly of zeroes. The SimpleNets Approach SimpleNets aim to provide a resource-light and language agnostic approach for sentence-level QE"
W16-2388,2011.eamt-1.12,1,0.885439,"mplification quality assessment in the past. Our experiments show that, surprisingly, our models can learn more about a translation’s quality by focusing on the original sentence, rather than on the translation itself. 1 Introduction The task of Machine Translation Quality Estimation (QE) has gained noticeable popularity in the last few years. The goal of QE is to predict the quality of translations produced by a certain Machine Translation (MT) system in the absence of reference translations. Reliable solutions for QE can be useful in various tasks, such as improving post-editing efficiency (Specia, 2011), selecting high quality translations (Soricut and Echihabi, 2010), translation re-ranking (Shah and Specia, 2014), and visual assistance for manual translation revision (Bach et al., 2011). QE can be performed in various ways in order to suit different purposes. The most widely addressed form of this task is sentence-level QE. Most existing work addresses this task as a supervised learning problem, in which a set of training examples is used to learn a model that predicts the quality of unseen translations. As quality labels, previous work uses either real valued scores estimated by humans, w"
W16-2391,W15-3035,0,0.0283832,"Missing"
W16-2391,N13-1090,0,0.0451646,"and Cohn, 2013). However, this resulted in a worse model when compared to the cross validation scheme. We speculate that the resulting models overfit the training data, due to its small size. The best combination, which we use in our submission, employs two Rational Quadratic (RatQuad) kernels (Rasmussen and Williams, 2006).9 After fixing this combination, the hyperparameters are optimised by maximising the model likelihood on the full training data. Embedding features (called hereafter EMBsource and EMB-target). The word embeddings used in our experiments are learned with the word2vec tool5 (Mikolov et al., 2013b). The tool produces word embeddings using the Distributed Skip-Gram or Continuous Bag-of-Words (CBOW) models. The models are trained using large amounts of monolingual data with a neural network architecture that aims at predicting the neighbours of a given word. Unlike standard neural network-based language models for predicting the next word given the context of preceding words, a CBOW model predicts the word in the middle given the representation of the surrounding words, while the Skip-Gram model learns word embedding representations that can be used to predict a word’s context in the sa"
W16-2391,P02-1040,0,0.096538,"d to train the Quality Estimation models. The use of word embeddings (combined with baseline features) and a Gaussian Process model with two kernels led to the winning submission in the shared task. 1 Introduction The task of Quality Estimation (QE) of Machine Translation (MT) consists in predicting the quality of unseen data using Machine Learning (ML) models trained on labelled data points. Such a scenario does not require reference translations and only uses information from source and target documents. Therefore, QE is different from traditional automatic evaluation metrics (such as BLEU (Papineni et al., 2002)). Sentence-level and word-level QE have been widely explored along the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). On the other hand, documentlevel QE has only recently started to be addressed, with the first shared task organised last year (Bojar et al., 2015). Document-level QE is the task of predicting the quality of an entire document and is useful for gisting applications (mainly in cases where the user does not speak the source language) and fully automated uses of MT where post-editing is not an option. Predicting the quality of docu"
W16-2391,P09-2004,0,0.0340078,"higher for more shared entities. We calculate the coherence score of the source documents and of the target documents and incorporate these as features. Systems Description Our submissions for the shared task explore different approaches in terms of features and modelling. We describe them in detail in what follows. 2.1 Discourse-aware system Pronouns, Connectives, EDUs and RST features (called hereafter PCER). Following (Scarton et al., 2015a), we use information from the Charniak parser (Charniak, 2000), the Discourse Parser from Joty et al. (2013), and the Discourse Connectives Tagger from Pitler and Nenkova (2009) as features for our discourse-aware model (these features could only be extracted for English, and thus for the source documents): • Number of pronouns; • Number of connectives (total number and number of connectives per class); Model We combine the described features with the official baseline ones provided by the shared task organisers and use them in an SVR with RBF kernel and hyperparameters optimised via grid search (the same as the official shared task baseline system). We use the SVR implementation available in the scikit-learn toolkit (Pedregosa et al., 2011).4 • Number of EDU breaks;"
W16-2391,W15-3001,1,0.910288,"unt, including document-wide issues. Moreover, defining quality labels for documents is a complex task on itself, as pointed by Scarton et al. (2015b). Little previous research has addressed this problem. Soricut and Echihabi (2010) explore pseudo-references and document-aware features for document-level ranking, using BLEU as quality label. Scarton and Specia (2014) apply pseudoreferences, document-aware and discourse-aware features for document-level quality prediction, using BLEU and TER as quality scores. Last year, a paragraph-level QE shared task was organised for the first time at WMT (Bojar et al., 2015), using METEOR as quality label. Scarton (2015) explore discourse information for paragraph-level prediction. They also perform exhaustive search and find out that using only three features from the official baseline set leads to results comparable to those of the full baseline system. Bic¸ici et al. (2015) apply referential translation machines for paragraphlevel QE and obtain the best overall results in the shared task. Finally, Scarton (2015), Scarton and Specia (2015) and Scarton et al. (2015b) analyse the task of document-level QE from the perspective of defining reliable labels. They als"
W16-2391,D13-1100,0,0.0261669,"Missing"
W16-2391,W12-3102,1,0.892477,"Missing"
W16-2391,A00-2018,0,0.0444391,"Missing"
W16-2391,2014.eamt-1.21,1,0.935026,"lation Quality Estimation Carolina Scarton, Daniel Beck, Kashif Shah, Karin Sim Smith and Lucia Specia Department of Computer Science University of Sheffield, UK {c.scarton,debeck1,kashif.shah,kmsimsmith1,l.specia} @sheffield.ac.uk Abstract taken into account, including document-wide issues. Moreover, defining quality labels for documents is a complex task on itself, as pointed by Scarton et al. (2015b). Little previous research has addressed this problem. Soricut and Echihabi (2010) explore pseudo-references and document-aware features for document-level ranking, using BLEU as quality label. Scarton and Specia (2014) apply pseudoreferences, document-aware and discourse-aware features for document-level quality prediction, using BLEU and TER as quality scores. Last year, a paragraph-level QE shared task was organised for the first time at WMT (Bojar et al., 2015), using METEOR as quality label. Scarton (2015) explore discourse information for paragraph-level prediction. They also perform exhaustive search and find out that using only three features from the official baseline set leads to results comparable to those of the full baseline system. Bic¸ici et al. (2015) apply referential translation machines fo"
W16-2391,P13-1010,0,0.0231522,"ion of all sentences; In addition to the official results of our submitted systems, we experiment with other feature combinations, such as scores from graphbased entity grid coherence models extracted from source documents and word embeddings generated for target documents. In Section 2 we describe the models used in our experiments and in Section 3 we present our results. 2 • Average LSA cosine distance of all sentences. Entity graph-based features (called hereafter GRAPH-source and GRAPH-target). We use an Entity Graph Model (Sim Smith et al., 2016), which is based on the bipartite graph of Guinaudeau and Strube (2013) and tracks the occurrence of entities throughout the document, including between non-adjacent sentences. Entities are taken as all nouns occurring in the document, as recommended by (Elsner, 2011). For our experiments, a POS tagger3 is used to identify nouns. A local coherence score is calculated directly, without any training, and represents the distribution of entities in the document. This is based on the theory that coherent texts contain salient entities. Both the sentences and entities are represented as nodes, with edges connecting the entities to the sentences they occur in. The final"
W16-2391,W15-3040,1,0.852183,"uality scores. Last year, a paragraph-level QE shared task was organised for the first time at WMT (Bojar et al., 2015), using METEOR as quality label. Scarton (2015) explore discourse information for paragraph-level prediction. They also perform exhaustive search and find out that using only three features from the official baseline set leads to results comparable to those of the full baseline system. Bic¸ici et al. (2015) apply referential translation machines for paragraphlevel QE and obtain the best overall results in the shared task. Finally, Scarton (2015), Scarton and Specia (2015) and Scarton et al. (2015b) analyse the task of document-level QE from the perspective of defining reliable labels. They also investigate the correlation of discourse phenomena and document-lvel translation quality. In this paper, we focus on feature engineering and the use of different ML techniques for document-level QE in the context of the WMT16 QE shared task (Task 3). We submitted two systems: In this paper we present the results of the University of Sheffield (SHEF) submissions for the WMT16 shared task on document-level Quality Estimation (Task 3). Our submission explore discourse and document-aware informatio"
W16-2391,P13-1048,0,0.0315195,"umber of shared entities into account, rating the projections higher for more shared entities. We calculate the coherence score of the source documents and of the target documents and incorporate these as features. Systems Description Our submissions for the shared task explore different approaches in terms of features and modelling. We describe them in detail in what follows. 2.1 Discourse-aware system Pronouns, Connectives, EDUs and RST features (called hereafter PCER). Following (Scarton et al., 2015a), we use information from the Charniak parser (Charniak, 2000), the Discourse Parser from Joty et al. (2013), and the Discourse Connectives Tagger from Pitler and Nenkova (2009) as features for our discourse-aware model (these features could only be extracted for English, and thus for the source documents): • Number of pronouns; • Number of connectives (total number and number of connectives per class); Model We combine the described features with the official baseline ones provided by the shared task organisers and use them in an SVR with RBF kernel and hyperparameters optimised via grid search (the same as the official shared task baseline system). We use the SVR implementation available in the sc"
W16-2391,W15-4916,1,0.900042,"Missing"
W16-2391,N15-2016,1,0.853155,"ing quality labels for documents is a complex task on itself, as pointed by Scarton et al. (2015b). Little previous research has addressed this problem. Soricut and Echihabi (2010) explore pseudo-references and document-aware features for document-level ranking, using BLEU as quality label. Scarton and Specia (2014) apply pseudoreferences, document-aware and discourse-aware features for document-level quality prediction, using BLEU and TER as quality scores. Last year, a paragraph-level QE shared task was organised for the first time at WMT (Bojar et al., 2015), using METEOR as quality label. Scarton (2015) explore discourse information for paragraph-level prediction. They also perform exhaustive search and find out that using only three features from the official baseline set leads to results comparable to those of the full baseline system. Bic¸ici et al. (2015) apply referential translation machines for paragraphlevel QE and obtain the best overall results in the shared task. Finally, Scarton (2015), Scarton and Specia (2015) and Scarton et al. (2015b) analyse the task of document-level QE from the perspective of defining reliable labels. They also investigate the correlation of discourse phen"
W16-2391,L16-1649,1,0.748434,"nce of adjacent sentences; • Average LSA Spearman rho correlation of all sentences; In addition to the official results of our submitted systems, we experiment with other feature combinations, such as scores from graphbased entity grid coherence models extracted from source documents and word embeddings generated for target documents. In Section 2 we describe the models used in our experiments and in Section 3 we present our results. 2 • Average LSA cosine distance of all sentences. Entity graph-based features (called hereafter GRAPH-source and GRAPH-target). We use an Entity Graph Model (Sim Smith et al., 2016), which is based on the bipartite graph of Guinaudeau and Strube (2013) and tracks the occurrence of entities throughout the document, including between non-adjacent sentences. Entities are taken as all nouns occurring in the document, as recommended by (Elsner, 2011). For our experiments, a POS tagger3 is used to identify nouns. A local coherence score is calculated directly, without any training, and represents the distribution of entities in the document. This is based on the theory that coherent texts contain salient entities. Both the sentences and entities are represented as nodes, with"
W16-2391,2006.amta-papers.25,0,0.032266,"language pair, extracted from the WMT08-13 translation shared task datasets. The machine translation for each source document was randomly picked from the set of all systems that participated in the translation task. The documents were evaluated by following the two-stage post-editing method described in (Scarton et al., 2015a). In the first stage, sentences are post-edited out of context, whilst in the second stage the post-edited sentences are placed in context and any remaining mistakes are corrected. The quality scores are, then, a variation of Human-targeted Translation Edit Rate (HTER) (Snover et al., 2006) that combines results from both post-editing stages. 3.1 Results Table 1 shows the results for our experiments with discourse-aware features and SVR for the scoring sub-task. We report results of our 10-fold crossvalidation method over the training and the results on the official test set. Results in the first column (10-fold) show that both discourse feature combination lead to improvements over the baseline. However, when testing on the test set, the models do not outperform the baseline. More investigation with additional data would be necessary to draw any conclusions on the reasons behin"
W16-2391,P10-1063,0,0.024847,"Missing"
W16-2391,P15-4020,1,0.825892,"t set), the feature sets show a similar behaviour: the model using EMB-target does not perform better than the baseline. On the other hand, EMB-source and baseline + EMB-source outperform the baseline, with the later scoring second in the official results of the shared task. It is worth mentioning that EMB-source alone is able to outperform the baseline in both sub-tasks. This is an interesting finding since word embeddings are relatively easy to acquire and only require large raw corpora as external resources. Baseline We use the 17 Q U E ST++ baseline features to train our baseline systems (Specia et al., 2015). We build a baseline system with SVR and another with GP, in order to compare our systems with comparable models.10 Models using discourse features and SVR The features sets we experimented with are: • baseline + PCER + LSA + GRAPH-target + GRAPH-source; • baseline + PCER + LSA + GRAPH-target.11 Our models using discourse information were trained with SVR as described in Section 2.1. Models using word embeddings and GP The features sets we experimented with are: • baseline + EMB-source + EMB-target; • baseline + EMB-source;12 • EMB-source; Our models using word embeddings were trained using G"
W16-2392,P10-2041,0,0.0146331,"on output embeddings. A Gated Recurrent Unit (GRU) (Chung et al., 2014) is used for the encoder and decoder. They have 1000 hidden units each, leading to an annotation vector ht ∈ R2000 . The attention mechanism, implemented as a simple fully-connected feed-forward neural network, accepts the hidden state ht of the decoder’s recurrent layer and one input annotation at a time, Table 1, reports detailed statistics on the monolingual data used to train the back-off LM and CSLM. The training dataset consists of WMT16 translation task monolingual corpora with the Moore-Lewis data selection method (Moore and Lewis, 2010) to select the CSLM training data with respect to the task’s development set. The 1 Train 84G 79G 2 http://www-lium.univ-lemans.fr/cslm/ 839 github.com/kyunghyuncho/dl4mt-material • NMTll : A log likelihood feature for each source and target sentence using NMT as described in Section 3. to produce the attention coefficients. A softmax activation is applied on those attention coefficients to obtain the attention weights used to generate the weighted annotation vector for time t. Both NMT systems are trained with WMT16 Quality Estimation English-German datasets (we used post-editions on the Germ"
W16-2392,H05-1026,0,0.0519421,"ed from segment pairs in isolation, ignoring contextual clues from other segments in the text. The focus of our contributions this year is to explore a new set of features which are language-independent, require minimal resources, and can be extracted in unsupervised ways with the use of neural networks. Word embeddings have shown their potential in modelling long distance dependencies in data, including syntactic and semantic information. For instance, neural network language models (Bengio et al., 2003) have been successfully explored in many problems including Automatic Speech Recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and Machine Translation (Schwenk, 2012). In this paper, we extend our previous work (Shah et al., 2015a; Shah et al., 2015b) to investigate the use of sentence embeddings extracted from a neural network language model along with cross entropy scores as features for QE. We also investigate the use of a neural machine translation model to extract the log likelihood of sentences as QE features. The features extracted from such resources are used in isolation or combined with hand-crafted features from QuEst to learn prediction models. This paper describes our systems for Task 1 o"
W16-2392,C12-2104,0,0.19648,"other segments in the text. The focus of our contributions this year is to explore a new set of features which are language-independent, require minimal resources, and can be extracted in unsupervised ways with the use of neural networks. Word embeddings have shown their potential in modelling long distance dependencies in data, including syntactic and semantic information. For instance, neural network language models (Bengio et al., 2003) have been successfully explored in many problems including Automatic Speech Recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and Machine Translation (Schwenk, 2012). In this paper, we extend our previous work (Shah et al., 2015a; Shah et al., 2015b) to investigate the use of sentence embeddings extracted from a neural network language model along with cross entropy scores as features for QE. We also investigate the use of a neural machine translation model to extract the log likelihood of sentences as QE features. The features extracted from such resources are used in isolation or combined with hand-crafted features from QuEst to learn prediction models. This paper describes our systems for Task 1 of the WMT16 Shared Task on Quality Estimation. Our submi"
W16-2392,2013.mtsummit-papers.21,1,0.535938,"ll -CSLMboth−emb • SHEF-SVM-QuEst-CSLMce -NMTll -CSLMboth−emb These systems contain all of our CSLM and NMT features either with or without QuEst: 719 and 644 features in total, respectively. We named them SVM-NN-both-emb and SVM-NNboth-emb-QuEst in the official submissions. The official results are shown in Table 4. Our systems show promising performance across all of the metrics used for evaluation in both scoring and ranking task variants. Our best system was ranked: Features We extracted the following features: • QuEst: 79 black-box features using the QuEst framework (Specia et al., 2013; Shah et al., 2013a) as described in Shah et al. (2013b). The full set of features can be found on http: //www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox. • Third place in the scoring task variant according to Pearson r (official scoring metric), and second place according MAE and RMSE. • CSLMce : A cross-entropy feature for each source and target sentence using CSLM as described in Section 2. • Second place in the ranking task variant according to Spearman ρ (official ranking metric) and first place according to DeltaAvg. 840 System. Baseline (SVM) SHEF-SVM-QuEst SHEF-SVM-QuEst-CSLMce -NMTll SHEF-SVM-"
W16-2392,W15-3041,1,0.67056,"his year is to explore a new set of features which are language-independent, require minimal resources, and can be extracted in unsupervised ways with the use of neural networks. Word embeddings have shown their potential in modelling long distance dependencies in data, including syntactic and semantic information. For instance, neural network language models (Bengio et al., 2003) have been successfully explored in many problems including Automatic Speech Recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and Machine Translation (Schwenk, 2012). In this paper, we extend our previous work (Shah et al., 2015a; Shah et al., 2015b) to investigate the use of sentence embeddings extracted from a neural network language model along with cross entropy scores as features for QE. We also investigate the use of a neural machine translation model to extract the log likelihood of sentences as QE features. The features extracted from such resources are used in isolation or combined with hand-crafted features from QuEst to learn prediction models. This paper describes our systems for Task 1 of the WMT16 Shared Task on Quality Estimation. Our submissions use (i) a continuous space language model (CSLM) to extr"
W16-2392,D15-1125,1,0.906714,"his year is to explore a new set of features which are language-independent, require minimal resources, and can be extracted in unsupervised ways with the use of neural networks. Word embeddings have shown their potential in modelling long distance dependencies in data, including syntactic and semantic information. For instance, neural network language models (Bengio et al., 2003) have been successfully explored in many problems including Automatic Speech Recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and Machine Translation (Schwenk, 2012). In this paper, we extend our previous work (Shah et al., 2015a; Shah et al., 2015b) to investigate the use of sentence embeddings extracted from a neural network language model along with cross entropy scores as features for QE. We also investigate the use of a neural machine translation model to extract the log likelihood of sentences as QE features. The features extracted from such resources are used in isolation or combined with hand-crafted features from QuEst to learn prediction models. This paper describes our systems for Task 1 of the WMT16 Shared Task on Quality Estimation. Our submissions use (i) a continuous space language model (CSLM) to extr"
W16-2392,P13-4014,1,0.903998,"Missing"
W16-3210,P05-1033,0,0.0128193,"relationship between the sentences in different languages. In the translated corpus, we know there is a strong correspondence between the sentences in both languages. In the descriptions corpus, we only know that the sentences, regardless of the language, are supposed to describe the same image. A dataset of images paired with sentences in multiple languages broadens the scope of multimodal NLP research. Image description with multilingual data can also be seen as machine translation in a multimodal context. This opens up new avenues for researchers in machine translation (Koehn et al., 2003; Chiang, 2005; Sutskever et al., 2014; Bahdanau et al., 2015, inter-alia) to work with multilingual multimodal data. Image– sentence ranking using monolingual multimodal datasets (Hodosh et al., 2013, inter-alia) is also a natural task for multilingual modelling. The only existing datasets of images paired with multilingual sentences were created by professionally translating English into the target language: IAPR-TC12 with 20,000 English-German described images (Grubinger et al., 2006), and the Pascal Sentences Dataset of 1,000 JapaneseEnglish described images (Funaki and Nakayama, 2015). Multi30K dataset"
W16-3210,D13-1128,1,0.045395,"Missing"
W16-3210,D15-1070,0,0.217749,"ranslation (Koehn et al., 2003; Chiang, 2005; Sutskever et al., 2014; Bahdanau et al., 2015, inter-alia) to work with multilingual multimodal data. Image– sentence ranking using monolingual multimodal datasets (Hodosh et al., 2013, inter-alia) is also a natural task for multilingual modelling. The only existing datasets of images paired with multilingual sentences were created by professionally translating English into the target language: IAPR-TC12 with 20,000 English-German described images (Grubinger et al., 2006), and the Pascal Sentences Dataset of 1,000 JapaneseEnglish described images (Funaki and Nakayama, 2015). Multi30K dataset is larger than both of these and contains both independent and translated sentences. We hope this dataset will be of broad interest across NLP and CV research and anticipate that these communities will put the data to use in a broader range of tasks than we can foresee. 70 Proceedings of the 5th Workshop on Vision and Language, pages 70–74, c Berlin, Germany, August 12 2016. 2016 Association for Computational Linguistics 1. Brick layers constructing a wall. 1. The two men on the scaffolding are helping to build a red brick wall. 2. Maurer bauen eine Wand. 2. Zwei Mauerer mau"
W16-3210,P16-1227,0,0.103727,"slation Machine translation is typically performed using only textual data, for example news data, the Europarl corpora, or corpora harvested from the Web (CommonCrawl, Wikipedia, etc.). The Multi30K dataset makes it possible to further develop ma73 chine translation in a setting where multimodal data, such as images or video, are observed alongside text. The potential advantages of using multimodal information for machine translation include the ability to better deal with ambiguous source text and to avoid (untranslated) out-of-vocabulary words in the target language (Calixto et al., 2012). Hitschler and Riezler (2016) have demonstrated the potential of multimodal features in a targetside translation reranking model. Their approach is initially trained over large text-only translation copora and then fine-tuned with a small amount of in-domain data, such as our dataset. We expect a variety of translation models can be adapted to take advantage of multimodal data as features in a translation model or as feature vectors in neural machine translation models. 4 Conclusions We introduced Multi30K: a large-scale multilingual multimodal dataset for interdisciplinary machine learning research. Our dataset is an ext"
W16-3210,N03-1017,0,0.00435756,"hese corpora is the relationship between the sentences in different languages. In the translated corpus, we know there is a strong correspondence between the sentences in both languages. In the descriptions corpus, we only know that the sentences, regardless of the language, are supposed to describe the same image. A dataset of images paired with sentences in multiple languages broadens the scope of multimodal NLP research. Image description with multilingual data can also be seen as machine translation in a multimodal context. This opens up new avenues for researchers in machine translation (Koehn et al., 2003; Chiang, 2005; Sutskever et al., 2014; Bahdanau et al., 2015, inter-alia) to work with multilingual multimodal data. Image– sentence ranking using monolingual multimodal datasets (Hodosh et al., 2013, inter-alia) is also a natural task for multilingual modelling. The only existing datasets of images paired with multilingual sentences were created by professionally translating English into the target language: IAPR-TC12 with 20,000 English-German described images (Grubinger et al., 2006), and the Pascal Sentences Dataset of 1,000 JapaneseEnglish described images (Funaki and Nakayama, 2015). Mu"
W16-3210,W10-0721,0,0.0981994,"Missing"
W16-3210,W16-2346,1,0.477489,"Missing"
W16-3210,Q14-1006,0,0.793454,"s crowdsourced independently of the original English descriptions. We describe the data and outline how it can be used for multilingual image description and multimodal machine translation, but we anticipate the data will be useful for a broader range of tasks. 1 Introduction Image description is one of the core challenges at the intersection of Natural Language Processing (NLP) and Computer Vision (CV) (Bernardi et al., 2016). This task has only received attention in a monolingual English setting, helped by the availability of English datasets, e.g. Flickr8K (Hodosh et al., 2013), Flickr30K (Young et al., 2014), and MS COCO (Chen et al., 2015). However, the possible applications of image description are useful for all languages, such as searching for images using natural language, or providing alternativedescription text for visually impaired Web users. We introduce a large-scale dataset of images paired with sentences in English and German as an initial step towards studying the value and the characteristics of multilingual-multimodal data1 . 1 The dataset is freely available under the Creative Commons Attribution NonCommercial ShareAlike 4.0 International license from http://www.statmt.org/wmt 16/"
W16-3210,W10-0707,0,\N,Missing
W16-3407,J93-2003,0,0.0438733,"e representation. Further experiments could use grammatical productions as an alternative. 3.4 Syntax-based model with IBM 1 The syntax model by Louis and Nenkova (2012) does not model latent alignments. This is possible under the assumption that all available alignment configurations have been directly observed in the training data. It is worth highlighting that in reality the training data is incomplete in the sense that it lacks alignment information. We introduce alignments between syntactic patterns in adjacent sentences as a latent variable. Our model does that based on the IBM model 1 (Brown et al., 1993), where the current sentence is generated by the preceding one, one pattern at a time, with a uniform prior over alignment configurations. The latent alignment variable allows us to model the fact that some patterns are more likely to trigger certain subsequent patterns. In IBM model 1, a latent alignment function a : j 7→ i maps patterns in v1n (current sentence) to patterns in um 0 (preceding sentence), where u0 is a special N ULL symbol which models insertion. The score Y of a document is given by Equation 4. P (D) = p(v1 . . . vn , a1 . . . an |u0 . . . um ) (4) n (um 1 ,v1 )∈D Here n is t"
W16-3407,N10-1099,0,0.0277711,"is however guided by elements of discourse that we believe can be modelled automatically to some extent. Most previous computational models for assessing coherence have focused on entity transitions, syntactic patterns and discourse relations. The most popular models are detailed in Section 3. In what follows we describe these models, and our work to apply these models to MT. Lin et al. (2011) evaluate the coherence of texts from discourse role transitions in a grid-based model, on the basis that there is a preferential, canonical, ordering of discourse relations that leads to coherent texts. Burstein et al. (2010) use the entity-grid for student essay evaluation, which is a scenario closer to ours. They used a range of additional features specifically targeting grammar and style. These proved 180 Sim Smith et al. useful for discriminating good from bad quality essays, but it is unclear how much of the problem with low quality essays was due to coherence issues. Their features are not publicly available for us to assess this. Somasundaran et al. (2014) consider how lexical chains affect discourse coherence. They use lexical chaining features such as length, density, and link strength to detect textual c"
W16-3407,W12-3156,0,0.0217953,"strate the difference between assessing the output from MT systems and assessing the coherence of shuffled texts in a highly consistent, structured corpus. The remainder of this paper, is organised as follows: in Section 2 we review related work on coherence and cohesion in the context of MT. Section 3 covers the background of the coherence models used in this paper. Experiments and results are discussed in Section 4. 2 Related Work There has been recent work in the area of lexical cohesion in MT (Wong and Kit, 2012); Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012), as a sub-category of coherence, looking at the linguistic elements which hold a text together. However, there seems to be little work in the wider area of coherence as a whole. Coherence is indeed a more complex discourse element to define in the first place. While it does include cohesion, it also describes how a text becomes semantically meaningful overall, and how easy it is for the reader to follow. Louwerse (2005) defines “cohesion as continuity in word and sentence structure, and coherence as continuity in meaning and context”. While lexical cohesion can be detected and addressed to so"
W16-3407,W11-1211,0,0.0724736,"Missing"
W16-3407,P11-2022,0,0.0686515,"parser.shtml 182 3.2 Sim Smith et al. Entity graph approach Guinaudeau and Strube (2013) adapted the entity-grid into a graph format using a bipartite graph which they claim avoids the data sparsity issues encountered by Barzilay and Lapata (2008) and achieves equal performance, without training. Additionally, their representation can track any cross-sentential references, as opposed to only those present in adjacent sentences. The graph tracks the presence of all entities and connections to the sentences they occur in, taking all nouns in the document as discourse entities, as recommended by Elsner and Charniak (2011). The coherence of a text in this model is measured by calculating the average outdegree of a projection, summing the shared edges. The general form of the coherence score assigned to a document in this approach is shown in Equation 2. This is a centrality measure based on the average outdegree across the N sentences represented in the document graph. The outdegree of a sentence si , denoted o(si ), is the total weight of edges leaving that sentence, a notion of how connected (or how central) it is. This weight is the sum of the contributions of all edges connecting si to any sj ∈ D. s(D) = N"
W16-3407,W15-2504,0,0.0187447,"They assess their model on an MT reranking task, progressively reranking consecutive sentences. In the MT domain, Xiong et al. (2013) attempt to improve lexical coherence with a topic-based model. They extract a coherence chain for the source sentence, and project it onto the target sentence to try and make lexical choices taken during decoding more coherent. They report very marginal improvement with respect to a baseline system in terms of automatic evaluation. This could indicate that current evaluation metrics are limited in their ability to account for improvements related to discourse. Gong et al. (2015) attempt to integrate their lexical chain and topic-based metrics into traditional BLEU and METEOR scores, showing greater correlation with human judgements on MT output. While the task of automatically evaluating text coherence has been addressed previously within applications such as multi-document text summarisation or in terms of optimal ordering within shuffled texts, our aim is to further investigate these components in an MT context without the use of a reference translation. We ultimately expect to be able to bias the translation process to ensure coherence in MT. 3 Coherence Models He"
W16-3407,P13-1010,0,0.466941,"generated by standard MT systems, on a sentence-by-sentence basis, several phenomena spanning sentence boundaries can lead to incoherent document translations, such as incorrect co-referencing, inadequate discourse markers, and lack of lexical cohesion, as established by previous corpus analyses (Sim Smith et al., 2015). We apply three existing coherence models to original, shuffled and machine translated texts in an attempt to evaluate their ability to discriminate between coherent and incoherent documents: an entity-grid model (Barzilay and Lapata, 2008), an entity graph similarity metric (Guinaudeau and Strube, 2013), and a model based on syntactic patterns (Louis and Nenkova, 2012). In addition, we propose a fully generative extension of the syntax-based coherence model. We illustrate the difference between assessing the output from MT systems and assessing the coherence of shuffled texts in a highly consistent, structured corpus. The remainder of this paper, is organised as follows: in Section 2 we review related work on coherence and cohesion in the context of MT. Section 3 covers the background of the coherence models used in this paper. Experiments and results are discussed in Section 4. 2 Related Wo"
W16-3407,J12-4004,0,0.0789939,"Missing"
W16-3407,D14-1218,0,0.0391281,"ays, but it is unclear how much of the problem with low quality essays was due to coherence issues. Their features are not publicly available for us to assess this. Somasundaran et al. (2014) consider how lexical chains affect discourse coherence. They use lexical chaining features such as length, density, and link strength to detect textual continuity, elaboration, lexical variety and organisation, all vital aspects of coherent texts. They claim that the interaction between lexical chains and discourse cues can also show whether cohesive devices are organised in a coherent fashion. Recently, Li and Hovy (2014) developed a coherence model based on distributed sentence representation. They used recurrent and recursive neural networks to perform ordering and readability tasks. They leverage semantic representations to establish coherent orderings, using original texts as positive examples and shuffled versions as negative ones for optimising the neural networks. Li et al. (2015) train a hierarchical Long-Short Term Memory (LSTM) to explore neural Natural Language Generation, and assess whether local semantic and syntactic coherence can be represented at a higher level, namely paragraphs. In their mode"
W16-3407,P15-1107,0,0.0190308,"l variety and organisation, all vital aspects of coherent texts. They claim that the interaction between lexical chains and discourse cues can also show whether cohesive devices are organised in a coherent fashion. Recently, Li and Hovy (2014) developed a coherence model based on distributed sentence representation. They used recurrent and recursive neural networks to perform ordering and readability tasks. They leverage semantic representations to establish coherent orderings, using original texts as positive examples and shuffled versions as negative ones for optimising the neural networks. Li et al. (2015) train a hierarchical Long-Short Term Memory (LSTM) to explore neural Natural Language Generation, and assess whether local semantic and syntactic coherence can be represented at a higher level, namely paragraphs. In their model, different LSTM layers represents word embeddings, sentences, and paragraphs. They are then able to regenerate the text to a degree that indicates neural networks are able to capture certain elements of coherence. Lin and Li (2015) use a hierarchical recurrent neural network language model (RNNLM) to combine a word-level model with a sentence-level model for document m"
W16-3407,D15-1106,0,0.105287,"Missing"
W16-3407,P11-1100,0,0.0612579,"xtual indicators necessary for coherence assessment are much more difficult to capture, even though judging coherence is an intuitive process for a human reader. Coherence is undeniably a complex cognitive process, which is however guided by elements of discourse that we believe can be modelled automatically to some extent. Most previous computational models for assessing coherence have focused on entity transitions, syntactic patterns and discourse relations. The most popular models are detailed in Section 3. In what follows we describe these models, and our work to apply these models to MT. Lin et al. (2011) evaluate the coherence of texts from discourse role transitions in a grid-based model, on the basis that there is a preferential, canonical, ordering of discourse relations that leads to coherent texts. Burstein et al. (2010) use the entity-grid for student essay evaluation, which is a scenario closer to ours. They used a range of additional features specifically targeting grammar and style. These proved 180 Sim Smith et al. useful for discriminating good from bad quality essays, but it is unclear how much of the problem with low quality essays was due to coherence issues. Their features are"
W16-3407,D12-1106,0,0.533596,"eral phenomena spanning sentence boundaries can lead to incoherent document translations, such as incorrect co-referencing, inadequate discourse markers, and lack of lexical cohesion, as established by previous corpus analyses (Sim Smith et al., 2015). We apply three existing coherence models to original, shuffled and machine translated texts in an attempt to evaluate their ability to discriminate between coherent and incoherent documents: an entity-grid model (Barzilay and Lapata, 2008), an entity graph similarity metric (Guinaudeau and Strube, 2013), and a model based on syntactic patterns (Louis and Nenkova, 2012). In addition, we propose a fully generative extension of the syntax-based coherence model. We illustrate the difference between assessing the output from MT systems and assessing the coherence of shuffled texts in a highly consistent, structured corpus. The remainder of this paper, is organised as follows: in Section 2 we review related work on coherence and cohesion in the context of MT. Section 3 covers the background of the coherence models used in this paper. Experiments and results are discussed in Section 4. 2 Related Work There has been recent work in the area of lexical cohesion in MT"
W16-3407,W15-2507,1,0.437252,"79 can affect the application of such models in various ways, making it harder to pinpoint coherence-related problems. Finally, judgements on the coherence of the translations may be dependent on the source text. Nevertheless, measuring coherence in MT is important: given the way translations are generated by standard MT systems, on a sentence-by-sentence basis, several phenomena spanning sentence boundaries can lead to incoherent document translations, such as incorrect co-referencing, inadequate discourse markers, and lack of lexical cohesion, as established by previous corpus analyses (Sim Smith et al., 2015). We apply three existing coherence models to original, shuffled and machine translated texts in an attempt to evaluate their ability to discriminate between coherent and incoherent documents: an entity-grid model (Barzilay and Lapata, 2008), an entity graph similarity metric (Guinaudeau and Strube, 2013), and a model based on syntactic patterns (Louis and Nenkova, 2012). In addition, we propose a fully generative extension of the syntax-based coherence model. We illustrate the difference between assessing the output from MT systems and assessing the coherence of shuffled texts in a highly con"
W16-3407,C14-1090,0,0.0191759,"e role transitions in a grid-based model, on the basis that there is a preferential, canonical, ordering of discourse relations that leads to coherent texts. Burstein et al. (2010) use the entity-grid for student essay evaluation, which is a scenario closer to ours. They used a range of additional features specifically targeting grammar and style. These proved 180 Sim Smith et al. useful for discriminating good from bad quality essays, but it is unclear how much of the problem with low quality essays was due to coherence issues. Their features are not publicly available for us to assess this. Somasundaran et al. (2014) consider how lexical chains affect discourse coherence. They use lexical chaining features such as length, density, and link strength to detect textual continuity, elaboration, lexical variety and organisation, all vital aspects of coherent texts. They claim that the interaction between lexical chains and discourse cues can also show whether cohesive devices are organised in a coherent fashion. Recently, Li and Hovy (2014) developed a coherence model based on distributed sentence representation. They used recurrent and recursive neural networks to perform ordering and readability tasks. They"
W16-3407,P06-2103,0,0.107905,"nment is hidden, we marginalise over all possible configurations, which is tractable due to an independence assumption (that items align independently of each other). Equation 5 shows this tractable marginalisation. n X m Y Y p(D) = p(vj |ui ) (5) n j=1 i=0 (um 1 ,v1 )∈D We resort to Expectation Maximisation (EM) to estimate the parameters in Equation 5 (Brown et al., 1993): due to the convexity of IBM model 1, EM is guaranteed to converge to a global optimum. Moreover, as we observe more data this model converges to better parameters. A similar solution was proposed in a different context by Soricut and Marcu, (2006) in their work on word co-occurrences. 184 Sim Smith et al. Table 1: Number of documents and sentences in the training (Gigaword) and test (WMT14) sets. Corpus Gigaword WMT14 WMT14 WMT14 Portion Documents Sentences 12/2010 41,564 774,965 de-en 164 3,003 fr-en 176 3,003 ru-en 175 3,003 To avoid assigning 0 probability to documents containing unseen patterns, we modify the training procedure to treat all the singletons as pertaining to an unknown category (U NK), thus reserving probability mass for future unseen items.3 In addition to this special U NK item, we also include N ULL alignments, whi"
W16-3407,W10-2602,0,0.0300047,"tax-based coherence model. We illustrate the difference between assessing the output from MT systems and assessing the coherence of shuffled texts in a highly consistent, structured corpus. The remainder of this paper, is organised as follows: in Section 2 we review related work on coherence and cohesion in the context of MT. Section 3 covers the background of the coherence models used in this paper. Experiments and results are discussed in Section 4. 2 Related Work There has been recent work in the area of lexical cohesion in MT (Wong and Kit, 2012); Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012), as a sub-category of coherence, looking at the linguistic elements which hold a text together. However, there seems to be little work in the wider area of coherence as a whole. Coherence is indeed a more complex discourse element to define in the first place. While it does include cohesion, it also describes how a text becomes semantically meaningful overall, and how easy it is for the reader to follow. Louwerse (2005) defines “cohesion as continuity in word and sentence structure, and coherence as continuity in meaning and context”. While lexical"
W16-3407,D12-1097,0,0.145699,"In addition, we propose a fully generative extension of the syntax-based coherence model. We illustrate the difference between assessing the output from MT systems and assessing the coherence of shuffled texts in a highly consistent, structured corpus. The remainder of this paper, is organised as follows: in Section 2 we review related work on coherence and cohesion in the context of MT. Section 3 covers the background of the coherence models used in this paper. Experiments and results are discussed in Section 4. 2 Related Work There has been recent work in the area of lexical cohesion in MT (Wong and Kit, 2012); Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012), as a sub-category of coherence, looking at the linguistic elements which hold a text together. However, there seems to be little work in the wider area of coherence as a whole. Coherence is indeed a more complex discourse element to define in the first place. While it does include cohesion, it also describes how a text becomes semantically meaningful overall, and how easy it is for the reader to follow. Louwerse (2005) defines “cohesion as continuity in word and sentence structure, and cohe"
W16-3407,D13-1163,0,0.359832,"se a fully generative extension of the syntax-based coherence model. We illustrate the difference between assessing the output from MT systems and assessing the coherence of shuffled texts in a highly consistent, structured corpus. The remainder of this paper, is organised as follows: in Section 2 we review related work on coherence and cohesion in the context of MT. Section 3 covers the background of the coherence models used in this paper. Experiments and results are discussed in Section 4. 2 Related Work There has been recent work in the area of lexical cohesion in MT (Wong and Kit, 2012); Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012), as a sub-category of coherence, looking at the linguistic elements which hold a text together. However, there seems to be little work in the wider area of coherence as a whole. Coherence is indeed a more complex discourse element to define in the first place. While it does include cohesion, it also describes how a text becomes semantically meaningful overall, and how easy it is for the reader to follow. Louwerse (2005) defines “cohesion as continuity in word and sentence structure, and coherence as continuity"
W16-3407,J08-1001,0,\N,Missing
W16-3407,D11-1034,0,\N,Missing
W16-3407,W14-3302,1,\N,Missing
W16-3413,S15-2017,1,0.884314,"Missing"
W16-3413,W13-2242,0,0.051842,"Missing"
W16-3413,C04-1046,0,0.0571293,"text for our research. Section 3 introduces our approach to integrating semantic information into QE. Section 4 details our experimental set-up, including the Semantic Textual Similarity in Quality Estimation 257 tools we use for our experiments. Section 5 explains our experiments, details our new STS features and summarises the results we observe when adding these features to QuEst. Finally, Section 6 presents our concluding remarks and plans for future work. 2 Previous Work Early work in QE built on the concept of confidence estimation used in speech recognition (Gandrabur and Foster, 2003, Blatz et al., 2004). These systems usually relied on system-dependent features, and focused on measuring how confident a given system is rather than how correct the translation is. Later experiments in QE used only system-independent features based on the source sentence and target translation (Specia et al., 2009b). They trained a Support Vector Machine (SVM) regression model based on 74 shallow features, and reported significant gains in accuracy over MT evaluation metrics. At first, these approaches to QE focused mainly on shallow features based on the source and target sentences. Such features include n-gram"
W16-3413,W08-0309,0,0.06305,"Missing"
W16-3413,W12-3103,0,0.0155089,"y (TM) users based on estimated PE effort. In contrast, Specia et al., 2010 use QE to rank translations from different systems and highlight inadequate segments for post-editing. Since 2012, QE has been the focus of a shared task at the annual Workshop for Statistical Machine Translation (WMT) (Callison-Burch et al., 2012). This task has provided a common ground for the comparison and evaluation of different QE systems and data at the word, sentence and document level (Bojar et al., 2015). There have been a few attempts to integrate semantic similarity into the MT evaluation (Lo and Wu, 2011, Castillo and Estrella, 2012). The results reported are generally positive, showing that semantic information is not only useful, but often necessary, in order to assess the quality of machine translation output. Specia et al. (2011) bring semantic information into the realm of QE in order to address the problem of meaning preservation. The authors focus on what they term “adequacy indicators” and human annotations for adequacy. The results they report show improvement with respect to a majority class baseline. Rubino et al. (2013) also address MT adequacy using topic models for QE. By including topic model features that"
W16-3413,W03-0413,0,0.0627228,"of the art in QE and the context for our research. Section 3 introduces our approach to integrating semantic information into QE. Section 4 details our experimental set-up, including the Semantic Textual Similarity in Quality Estimation 257 tools we use for our experiments. Section 5 explains our experiments, details our new STS features and summarises the results we observe when adding these features to QuEst. Finally, Section 6 presents our concluding remarks and plans for future work. 2 Previous Work Early work in QE built on the concept of confidence estimation used in speech recognition (Gandrabur and Foster, 2003, Blatz et al., 2004). These systems usually relied on system-dependent features, and focused on measuring how confident a given system is rather than how correct the translation is. Later experiments in QE used only system-independent features based on the source sentence and target translation (Specia et al., 2009b). They trained a Support Vector Machine (SVM) regression model based on 74 shallow features, and reported significant gains in accuracy over MT evaluation metrics. At first, these approaches to QE focused mainly on shallow features based on the source and target sentences. Such fe"
W16-3413,P10-1064,0,0.0411147,"Missing"
W16-3413,W14-4008,0,0.0188572,"(2013) introduce the use of referential translation machines (RTM) for QE. RTM is a computational model for judging monolingual and bilingual similarity that achieves state-of-the-art results. The authors report top performance in both sentence level and word-level tasks of WMT 258 B´echara et al. 2013. Camargo de Souza et al. (2014) propose a set of features that explore word alignment information in order to address semantic relations between sentences. Their results show that POS indicator features improve over the baseline at the shared task for QE at the workshop for machine translation. Kaljahi et al. (2014) employ syntactic and semantic information in quality estimation and are able to improve over the baseline when combining these features with the surface features of the baseline. Our work builds on previous work, focusing on the necessity of semantic information for MT adequacy. As far as we are aware, our work is the first to explore quality scores from semantically similar sentences as surrogate to the quality of the current sentence. 3 Our Approach In this paper, we propose integrating semantic similarity into the quality estimation task. As STS relies on monolingual data, we employ the us"
W16-3413,2005.mtsummit-papers.11,0,0.432223,"ate-of-the-art phrase based Statistical Machine Translation (SMT) system Moses (Koehn et al., 2007). We build 5-gram language models with Kneser-Ney smoothing trained with SRILM, (Stolcke, 2002), and run the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al. (2003). We use Minimum Error Rate Training (MERT) (Och, 2003) for tuning. In order to keep our experiments consistent, we use the same SMT system for all datasets. We focus on English into French translations and we use the Europarl corpus (Koehn, 2005) for training. We train on 500,000 unique English–French sentences and then tune our system (using MERT) on 1,000 different unique sentences also from the Europarl corpus. We also train a French–English system to retrieve the backtranslations used in some of our experiments. Semantic Textual Similarity in Quality Estimation 5 261 Experiments As mentioned earlier, all our datasets focus on MTQE for English→French MT output. In all our experiments we have a set of machine translated sentences A for which we need a QE and a set of sentences B, semantically similar to the set of sentences A and fo"
W16-3413,P07-2045,0,0.0110638,"Corpus Pattern Analysis. The system performs well and obtained a mean 0.7216 Pearson correlation in the shared task, ranking 33 out of 74 systems. We train the STS tool on the SICK dataset Marelli et al., 2014, a dataset specifically designed for semantic similarity and used in previous SemEval tasks, augmented with training data from previous SemEval tasks (SemEval2014 and SemEval2015). 4.3 Statistical Machine Translation System All of our experiments require MT output to run MTQE tasks. To that end, we use the state-of-the-art phrase based Statistical Machine Translation (SMT) system Moses (Koehn et al., 2007). We build 5-gram language models with Kneser-Ney smoothing trained with SRILM, (Stolcke, 2002), and run the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al. (2003). We use Minimum Error Rate Training (MERT) (Och, 2003) for tuning. In order to keep our experiments consistent, we use the same SMT system for all datasets. We focus on English into French translations and we use the Europarl corpus (Koehn, 2005) for training. We train on 500,000 unique English–French sentences and then tune our sy"
W16-3413,N03-1017,0,0.0195562,"and used in previous SemEval tasks, augmented with training data from previous SemEval tasks (SemEval2014 and SemEval2015). 4.3 Statistical Machine Translation System All of our experiments require MT output to run MTQE tasks. To that end, we use the state-of-the-art phrase based Statistical Machine Translation (SMT) system Moses (Koehn et al., 2007). We build 5-gram language models with Kneser-Ney smoothing trained with SRILM, (Stolcke, 2002), and run the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al. (2003). We use Minimum Error Rate Training (MERT) (Och, 2003) for tuning. In order to keep our experiments consistent, we use the same SMT system for all datasets. We focus on English into French translations and we use the Europarl corpus (Koehn, 2005) for training. We train on 500,000 unique English–French sentences and then tune our system (using MERT) on 1,000 different unique sentences also from the Europarl corpus. We also train a French–English system to retrieve the backtranslations used in some of our experiments. Semantic Textual Similarity in Quality Estimation 5 261 Experiments As mentio"
W16-3413,P04-1077,0,0.0735564,"n 4.2) in all but one of our experiments, where we already have human annotations about STS. This experiment, the oracle experiment represents scores we could achieve if our STS was perfect. Quality Score for Sentence B: We calculate the quality of the MT output of Sentence B. This is either a S-B LEU score based on a reference translation, or a manual score provided by a human evaluator. S-B LEU Score for Sentence A: We have no human evaluation or reference translation for Sentence A, but we can calculate a quality score using Sentence B as a reference. We use sentence-level B LEU (S-B LEU) (Lin and Och, 2004). S-B LEU is designed to work at the sentence level and will still positively score segments that do not have a high order n-gram match. 4 Experimental Setting In this section, we start with a brief introduction to the QuEst framework, followed by a description of the settings for the experiments described in this paper. 4.1 The QuEst Framework QuEst (Specia et al., 2013) is an open source framework for MTQE.4 In addition to a feature extraction framework, QuEst also provides the machine learning algorithms necessary to build the prediction models. QuEst gives access to a large variety of feat"
W16-3413,P11-1023,0,0.070091,"Translation Memory (TM) users based on estimated PE effort. In contrast, Specia et al., 2010 use QE to rank translations from different systems and highlight inadequate segments for post-editing. Since 2012, QE has been the focus of a shared task at the annual Workshop for Statistical Machine Translation (WMT) (Callison-Burch et al., 2012). This task has provided a common ground for the comparison and evaluation of different QE systems and data at the word, sentence and document level (Bojar et al., 2015). There have been a few attempts to integrate semantic similarity into the MT evaluation (Lo and Wu, 2011, Castillo and Estrella, 2012). The results reported are generally positive, showing that semantic information is not only useful, but often necessary, in order to assess the quality of machine translation output. Specia et al. (2011) bring semantic information into the realm of QE in order to address the problem of meaning preservation. The authors focus on what they term “adequacy indicators” and human annotations for adequacy. The results they report show improvement with respect to a majority class baseline. Rubino et al. (2013) also address MT adequacy using topic models for QE. By includ"
W16-3413,marelli-etal-2014-sick,0,0.212547,"orpus of the source language percentage of unigrams in the source sentence seen in a corpus number of punctuation marks in source sentence number of punctuation marks in target sentence sentences. The authors train their system on a variety of linguistically motivated features inspired by deep semantics with distributional Similarity Measures, Conceptual Similarity Measures, Semantic Similarity Measures and Corpus Pattern Analysis. The system performs well and obtained a mean 0.7216 Pearson correlation in the shared task, ranking 33 out of 74 systems. We train the STS tool on the SICK dataset Marelli et al., 2014, a dataset specifically designed for semantic similarity and used in previous SemEval tasks, augmented with training data from previous SemEval tasks (SemEval2014 and SemEval2015). 4.3 Statistical Machine Translation System All of our experiments require MT output to run MTQE tasks. To that end, we use the state-of-the-art phrase based Statistical Machine Translation (SMT) system Moses (Koehn et al., 2007). We build 5-gram language models with Kneser-Ney smoothing trained with SRILM, (Stolcke, 2002), and run the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with ref"
W16-3413,P03-1021,0,0.0310902,"a from previous SemEval tasks (SemEval2014 and SemEval2015). 4.3 Statistical Machine Translation System All of our experiments require MT output to run MTQE tasks. To that end, we use the state-of-the-art phrase based Statistical Machine Translation (SMT) system Moses (Koehn et al., 2007). We build 5-gram language models with Kneser-Ney smoothing trained with SRILM, (Stolcke, 2002), and run the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al. (2003). We use Minimum Error Rate Training (MERT) (Och, 2003) for tuning. In order to keep our experiments consistent, we use the same SMT system for all datasets. We focus on English into French translations and we use the Europarl corpus (Koehn, 2005) for training. We train on 500,000 unique English–French sentences and then tune our system (using MERT) on 1,000 different unique sentences also from the Europarl corpus. We also train a French–English system to retrieve the backtranslations used in some of our experiments. Semantic Textual Similarity in Quality Estimation 5 261 Experiments As mentioned earlier, all our datasets focus on MTQE for English"
W16-3413,J03-1002,0,0.0167393,"dataset Marelli et al., 2014, a dataset specifically designed for semantic similarity and used in previous SemEval tasks, augmented with training data from previous SemEval tasks (SemEval2014 and SemEval2015). 4.3 Statistical Machine Translation System All of our experiments require MT output to run MTQE tasks. To that end, we use the state-of-the-art phrase based Statistical Machine Translation (SMT) system Moses (Koehn et al., 2007). We build 5-gram language models with Kneser-Ney smoothing trained with SRILM, (Stolcke, 2002), and run the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al. (2003). We use Minimum Error Rate Training (MERT) (Och, 2003) for tuning. In order to keep our experiments consistent, we use the same SMT system for all datasets. We focus on English into French translations and we use the Europarl corpus (Koehn, 2005) for training. We train on 500,000 unique English–French sentences and then tune our system (using MERT) on 1,000 different unique sentences also from the Europarl corpus. We also train a French–English system to retrieve the backtranslations used in some of our expe"
W16-3413,P13-4014,1,0.875959,"Missing"
W16-3413,2009.eamt-1.5,1,0.82388,"ew STS features and summarises the results we observe when adding these features to QuEst. Finally, Section 6 presents our concluding remarks and plans for future work. 2 Previous Work Early work in QE built on the concept of confidence estimation used in speech recognition (Gandrabur and Foster, 2003, Blatz et al., 2004). These systems usually relied on system-dependent features, and focused on measuring how confident a given system is rather than how correct the translation is. Later experiments in QE used only system-independent features based on the source sentence and target translation (Specia et al., 2009b). They trained a Support Vector Machine (SVM) regression model based on 74 shallow features, and reported significant gains in accuracy over MT evaluation metrics. At first, these approaches to QE focused mainly on shallow features based on the source and target sentences. Such features include n-gram counts, the average length of tokens, punctuation statistics and sentence length among other features. Later systems incorporate linguistic features such as part of speech tags, syntactic information and word alignment information (Specia et al., 2010). In the context of QE, the term “quality”"
W16-3413,2009.mtsummit-papers.16,1,0.829666,"ew STS features and summarises the results we observe when adding these features to QuEst. Finally, Section 6 presents our concluding remarks and plans for future work. 2 Previous Work Early work in QE built on the concept of confidence estimation used in speech recognition (Gandrabur and Foster, 2003, Blatz et al., 2004). These systems usually relied on system-dependent features, and focused on measuring how confident a given system is rather than how correct the translation is. Later experiments in QE used only system-independent features based on the source sentence and target translation (Specia et al., 2009b). They trained a Support Vector Machine (SVM) regression model based on 74 shallow features, and reported significant gains in accuracy over MT evaluation metrics. At first, these approaches to QE focused mainly on shallow features based on the source and target sentences. Such features include n-gram counts, the average length of tokens, punctuation statistics and sentence length among other features. Later systems incorporate linguistic features such as part of speech tags, syntactic information and word alignment information (Specia et al., 2010). In the context of QE, the term “quality”"
W16-3413,steinberger-etal-2006-jrc,0,0.352142,"machine translations. Each sentence in the WMT dataset comes with a score between 1 and 5, provided by human annotators. However, this method proved to be too time-consuming, as it often required scoring thousands of sentences before finding two that were similar. The first obstacle we faced in testing our approach with these datasets was the collection of similar sentences against which to compare and evaluate. We automatically searched large parallel corpora for sentences that yielded high similarity scores. These corpora included the Europarl corpus (Koehn, 2005), the Acquis Communautaire (Steinberger et al., 2006) and previous WMT data (from 2012 and 2013). Furthermore, the STS system we use (see Section 4.2) returned many false-positives. Some sentences which appeared similar to the STS system were actually too different to be usable. This led to noisy data and unusable results. The scarcity of semantically similar sentences and the computational cost of finding these sentences, lead us to look into alternate datasets, preferably those with semantic similarity built into the corpus: the DGT-TM and the SICK dataset. All our experiments have the same set-up. In all cases, we used 500 randomly selected s"
W16-3417,W09-2307,0,0.0125273,"ohibited zone in Hong Kong . 19-15(FUN) 17-10(SEM) 7,8-5[DET],6(GIS) 9[COO]-(NTR) 12,13-13,14(SEM) 18-11(SEM) 14[DEP]-12(PDE) 2,5,6-2(FUN) 3,4-3(SEM) 10,11[TEN]-7[TEN],8(GIS) 15,16[MEA]-9(GIF) -4[COO](NTR) -1[MET](MTA) 1[MET]-(MTA) The separate parts are combined to create a parallel aligned sentence for each line of our corpus. The Chinese segments were segmented into their more common forms typically found in Chinese dictionaries. For instance, ‘这 里’, becomes ‘这里’ (‘this area’ - in the case of this sentence). This step was performed using the Stanford Chinese Segmenter (Chang, et al., 2008; Chang, et al., 2009; Tseng,et al., 2005). The word alignments were then adjusted to accommodate the changes. The final stage of the process involves removing the meta-data/additional annotations and zero indexing the word alignments (to match other common word alignment formats). Multiple alignments are split into separate alignment points and then reordered to improve readability. Example 4 is the final version of Example 3 and shows the typical format of the sentences in our corpus. Ex(4) 从 那时 开始 这里 就 成 了 香港 的 一 个 禁区 。 since then , this area has become 9-7 10-7 11-8 11-9 12-13 4 Explicitation methods In this s"
W16-3417,J07-2003,0,0.0978465,"formation. The corpus annotations are derived from either oracle or automated word alignments. Predicted annotations (Predicted_Inserts) in the source of the test set are produced through a fully automated process, using a classifier trained on oracle alignments. This section also provides a number of examples highlighting how some translations have changed either for better or worse. 5.1 Settings and methodology Our SMT systems are built using the corpus described in Section 3. CDEC (Dyer et al., 2010) is used for rule extraction and decoding following the hierarchical phrase-based approach (Chiang, 2007) for Chinese-English translation. We use BLEU (Papineni et al., 2002) as the metric to evaluate the systems. For consistency, default parameters are used during different builds with the only change being the source of the word alignments. We perform the same experiments twice, with two different splits of the corpus. For each experiment, the corpus is first randomly shuffled. The development and test sets (dev and tst in the table) are then created using the first 2000 sentences (1000 for each) in the shuffled corpus, while the training set is made up of the remaining 41693 sentences. For an"
W16-3417,D10-1062,0,0.0643461,"Missing"
W16-3417,P10-4002,0,0.0223371,"th raw parallel data against SMT systems where the source side of the corpus is annotated with place holder information. The corpus annotations are derived from either oracle or automated word alignments. Predicted annotations (Predicted_Inserts) in the source of the test set are produced through a fully automated process, using a classifier trained on oracle alignments. This section also provides a number of examples highlighting how some translations have changed either for better or worse. 5.1 Settings and methodology Our SMT systems are built using the corpus described in Section 3. CDEC (Dyer et al., 2010) is used for rule extraction and decoding following the hierarchical phrase-based approach (Chiang, 2007) for Chinese-English translation. We use BLEU (Papineni et al., 2002) as the metric to evaluate the systems. For consistency, default parameters are used during different builds with the only change being the source of the word alignments. We perform the same experiments twice, with two different splits of the corpus. For each experiment, the corpus is first randomly shuffled. The development and test sets (dev and tst in the table) are then created using the first 2000 sentences (1000 for"
W16-3417,N13-1073,0,0.0213019,"ocess the word alignments may also give different results. For our training split of the dataset, using the oracle word alignments, ‘and_CC’ was inserted 12350 times across 41693 sentences, whilst ‘or_CC’ was only inserted 554 times. Insertions were made for the POS groups in Table 1 in over 26000 of the sentences. Thus far, the focus has been on insertions being made using oracle word alignments. However, we also experimented with automated word alignments, where we created an equivalent corpus using the same insertion rules, but with inserts made based on alignments extracted by Fast-Align (Dyer et al., 2013). Counts for insertions made on the same corpus, but using Fast-Align alignments, vary considerably. For example, ‘and_CC’ has 5015 insertions (previously 12350) whilst ‘or_CC’ has 95 (previously 554). Table 2 shows the difference in frequency of insertions made for ‘and_CC’, ‘or_CC’, and ‘the_DT’ using the oracle alignments and automated alignments, respectively. 312 Steele and Specia Table 2. Highlighting the differences between insertions of words based on oracle and automated alignments. W ord and_CC or_CC the_DT Oracle alignments 12350 554 1160 Automated alignments 5015 95 33751 The word"
W16-3417,W13-3303,0,0.0534284,"Missing"
W16-3417,P02-1040,0,0.0959773,"acle or automated word alignments. Predicted annotations (Predicted_Inserts) in the source of the test set are produced through a fully automated process, using a classifier trained on oracle alignments. This section also provides a number of examples highlighting how some translations have changed either for better or worse. 5.1 Settings and methodology Our SMT systems are built using the corpus described in Section 3. CDEC (Dyer et al., 2010) is used for rule extraction and decoding following the hierarchical phrase-based approach (Chiang, 2007) for Chinese-English translation. We use BLEU (Papineni et al., 2002) as the metric to evaluate the systems. For consistency, default parameters are used during different builds with the only change being the source of the word alignments. We perform the same experiments twice, with two different splits of the corpus. For each experiment, the corpus is first randomly shuffled. The development and test sets (dev and tst in the table) are then created using the first 2000 sentences (1000 for each) in the shuffled corpus, while the training set is made up of the remaining 41693 sentences. For an oracle build, all sets (dev, tst, and training) include the human cre"
W16-3417,N15-2015,1,0.481576,"into a Chinese-English MT task. The results show 308 Steele and Specia that the recovered empty elements contribute to improvements in both word alignment tasks and the overall quality of their MT system output. More recently, Steele and Specia (2014) discuss divergences in the usage of DMs for Chinese and English. They illustrate how DMs are vital contextual links, and through a detailed corpus analysis highlight significant divergences in their usage. The findings show how contextual omissions (implicit data) cause problems for MT systems and often lead to incoherent automatic translations. Steele (2015), builds upon this work with a focus on word alignments for four specific elements: ‘if’, ‘then’,‘because’,‘but’. Automatic alignments are used to ascertain the occurrence of implicit markers, which is found to be quite significant. Experiments show that when artificial tokens are inserted into the data, as a proxy for these markers, and the MT systems are rebuilt, there is a significant improvement over the baseline. However, to achieve the improvement the insertions of the markers were carried out using reference data. Clearly there is some overlap between the terms ‘empty categories’ and ‘i"
W16-3417,N03-1033,0,0.199791,"Missing"
W17-4716,C04-1046,0,0.0634356,"words unknown to the model. To test our approach, we experiment in two scenarios that pose different challenges to NMT. The first one is a translation task in which source sentences contain XML-annotated domain-specific terms. The presence of few annotated terms poses fewer constraints to the decoder in generating the output sentence. The second scenario is an automatic post-editing (APE) task, in which the NMT model is trained to translate “monolingually” from draft machine-translated sentences into humanquality post-edits. The external guidance is provided by word-level quality judgements (Blatz et al., 2004) indicating the “good” words in the machine-translated sentence that should be kept in the final APE output. In this case, the large number of “good” words already present in the original MT output poses more constraints to the decoding process. In both scenarios, our guidance mechanism achieves significant performance gains over the original NMT decoder. 2 Related Work In PBSMT, the injection of external knowledge in the decoder is usually handled with the socalled XML markup, a technique used to guide the decoder by supplying the desired translation for some of the source phrases. The suppli"
W17-4716,D16-1162,0,0.0331893,"replacing entries that cover the specific source phrase, or adding the alternative phrase translations to it, so that they are in competition. This problem has only recently started to be explored in NMT and, in most of the cases, the proposed solutions integrate external knowledge at training stage. Time-consuming training routines, however, limit the suitability of this strategy for applications requiring real-time translations. In Gulcehre et al. (2015), monolingual data is used to train a neural language model that is integrated in the NMT decoder by concatenating their hidden states. In Arthur et al. (2016), the probability of the next target word in the NMT decoder is biased by using lexicon probabilities computed from a bilingual lexicon. When the external knowledge is The closest approach to ours is the one by (Hokamp and Liu, 2017). They explore all the possible constraints (or translation options) at each time step making sure not to generate a constraint that have already been generated in the previous timestep. Their approach generates all the constraints in the final output, thus implicitly it assumes that only one translation options is provided as constraint for a given source word/phr"
W17-4716,E17-1050,1,0.888712,"Missing"
W17-4716,P17-1141,0,0.0370356,"the proposed solutions integrate external knowledge at training stage. Time-consuming training routines, however, limit the suitability of this strategy for applications requiring real-time translations. In Gulcehre et al. (2015), monolingual data is used to train a neural language model that is integrated in the NMT decoder by concatenating their hidden states. In Arthur et al. (2016), the probability of the next target word in the NMT decoder is biased by using lexicon probabilities computed from a bilingual lexicon. When the external knowledge is The closest approach to ours is the one by (Hokamp and Liu, 2017). They explore all the possible constraints (or translation options) at each time step making sure not to generate a constraint that have already been generated in the previous timestep. Their approach generates all the constraints in the final output, thus implicitly it assumes that only one translation options is provided as constraint for a given source word/phrase. However, in a more realistic scenario (e.g. in presence of a termbase or when the target language is more inflected than the source language), a source word can have multiple translation options from which the decoder should dec"
W17-4716,P15-1001,0,0.166045,"lberg et al., 2016) proposed an approach which can be used at decoding time. A hierarchical PBSMT system is used to generate the translation lattices, which are then re-scored by the NMT decoder. During decoding, the NMT posterior probabilities are adjusted using the posterior scores computed by the hierarchical model. However, by representing the additional information as a translation lattice, this approach does not allow the use of external knowledge in the form of bilingual terms or quality judgements as we do in §5 and §6. A different technique is postprocessing the translated sentences. Jean et al. (2015) and Luong and Manning (2015) replace the unknown words either with the most likely aligned source word or with the translation determined by another word alignment model. source words with the corresponding translation options. The guidance mechanism supervises the process, generating the final output with the expected translations, in the right place, including cases of external words unknown to the model. To test our approach, we experiment in two scenarios that pose different challenges to NMT. The first one is a translation task in which source sentences contain XML-annotated domain-speci"
W17-4716,W15-3025,1,0.843161,"signed&quot; helps GDec to correct other parts of the final translation, like generating “es gibt keine” (En: “There is no”) which is otherwise missing in the baseline translation. 6 Task 2: Automatic Post-Editing In our second experiment, we apply guided decoding in an automatic post-editing task. The goal of automatic post-editing (APE) is to correct errors in an MT-ed text. The problem is typically approached as a “monolingual translation” task, in which models are trained on parallel corpora containing (MT_output, MT_post-edit) pairs, with MT post-edits coming from humans (Simard et al., 2007; Chatterjee et al., 2015b, 2017). In their attempt to translate the entire input sentence, APE systems usually tend to over-correct the source words, i.e. to use all applicable correction options. This can happen even when the input is correct, often resulting in text deterioration (Bojar et al., 2015). To cope with this problem, neuralbased APE decoders would benefit from external knowledge indicating words in the input which are correct and thus should not be modified during decoding. For that we propose to use wordlevel binary quality estimation labels (Blatz et al., 2004; de Souza et al., 2014) to annotate the “g"
W17-4716,W16-2378,0,0.0509962,"on from the QE annotations. Base-APE improves the Base-MT up to 3.14 BLEU points. Similar to §5.2, the evaluation of our guided decoder is performed incrementally. GDec_base forces the “good” words in the automatic translation to appear in the output according to the mechanism described in §4.1. This basic guidance mechanism yields only marginal improvements over the Base-MT and is far behind the Base-APE. This can be explained by the large number of constraints (i.e. “good” words to be Experimental setting NMT models. We use the pre-trained model built for the best English-German submission (Junczys-Dowmunt and Grundkiewicz, 2016) at the WMT’16 APE task. This available model was trained with Nematus over a data set of ∼4M back-translated pairs, and then adapted to the taskspecific data segmented using the BPE technique. 164 Src: <n translation=&quot;durchsuchen||Durchsuchen&quot;&gt; browse </n&gt; all products Base: stöbern alle Produkte GDec: durchsuchen Sie alle Produkte Ref: durchsuchen Sie alle Produkte Src: <n translation=&quot;produkt||Produkt&quot;&gt; Product </n&gt; 4 - <n translation=&quot;plastics||Plastics&quot;&gt; Plastics </n&gt; <n translation=&quot;labs||Labs&quot;&gt; Labs </n&gt; Base: Produkt 4 - Kunststofflabore GDec: Produkt 4 - Plastics Labs Ref: Produkt 4 -"
W17-4716,P15-2026,1,0.665898,"signed&quot; helps GDec to correct other parts of the final translation, like generating “es gibt keine” (En: “There is no”) which is otherwise missing in the baseline translation. 6 Task 2: Automatic Post-Editing In our second experiment, we apply guided decoding in an automatic post-editing task. The goal of automatic post-editing (APE) is to correct errors in an MT-ed text. The problem is typically approached as a “monolingual translation” task, in which models are trained on parallel corpora containing (MT_output, MT_post-edit) pairs, with MT post-edits coming from humans (Simard et al., 2007; Chatterjee et al., 2015b, 2017). In their attempt to translate the entire input sentence, APE systems usually tend to over-correct the source words, i.e. to use all applicable correction options. This can happen even when the input is correct, often resulting in text deterioration (Bojar et al., 2015). To cope with this problem, neuralbased APE decoders would benefit from external knowledge indicating words in the input which are correct and thus should not be modified during decoding. For that we propose to use wordlevel binary quality estimation labels (Blatz et al., 2004; de Souza et al., 2014) to annotate the “g"
W17-4716,W04-3250,0,0.035147,"ed model built for the best EnglishGerman submission (Sennrich et al., 2016a) at the News Translation task at WMT’16 (Bojar et al., 2016). At test stage, it is supplied with terminology lists containing term recommendations in BPE format. In all the experiments we use a default beam size of 12. 5.2 Results and discussion Our results on the MT task are reported in Table 1, which shows system performance on the concatenation of the test sets from the two target domains. Performance is measured with BLEU (Papineni et al., 2002), and statistical significance is computed with bootstrap resampling (Koehn, 2004). The result of the word-level baseline system is computed after post-processing its output following the approach of Jean et al. (2015), which was customized to our scenario. This method (see §2) is driven by the attention model to replace the UNK tokens in the output with their corresponding recommendation supplied as external knowledge. This post-processing strategy is not used for the BPE-level baseline because it implicitly addresses the problem of OOVs. We evaluate our guided decoder incrementally, by adding one at a time the mechanisms described in §4. In the discussion, we do not compa"
W17-4716,W14-4012,0,0.0668892,"Missing"
W17-4716,2005.mtsummit-papers.11,0,0.126246,"r knowledge supplied as translation recommendations for domainspecific terms. The suggested terms (i.e. the constraints posed to the decoder) are usually few, thus leaving a large degree of freedom to the NMT decoder while generating the output. 5.1 Experimental setting NMT models. We evaluate guided decoding in its ability to improve the performance of two different English to German NMT models, both obtained with the Nematus toolkit (Sennrich et al., 2016a). The first system operates at word level and it is trained by using part of the JRC-Acquis corpus (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2009), which results in at total of about 1.8M parallel sentence pairs. The size of the vocabulary, word embedding, and hidden units is respectively set to 40K, 600, and 600, and parameters are optimised with Adagrad (Duchi et al., 2011) using a learning rate of 0.01. The batch size is set to 100, and the model is trained for 300K updates (∼17 epochs). At test stage, the word-level system is supplied with terminology lists containing term recommendations at the level of granularity of full words. The second system is trained on sub-word units by using the Byt"
W17-4716,W14-3340,1,0.91221,"Missing"
W17-4716,2015.iwslt-evaluation.11,0,0.0709613,"posed an approach which can be used at decoding time. A hierarchical PBSMT system is used to generate the translation lattices, which are then re-scored by the NMT decoder. During decoding, the NMT posterior probabilities are adjusted using the posterior scores computed by the hierarchical model. However, by representing the additional information as a translation lattice, this approach does not allow the use of external knowledge in the form of bilingual terms or quality judgements as we do in §5 and §6. A different technique is postprocessing the translated sentences. Jean et al. (2015) and Luong and Manning (2015) replace the unknown words either with the most likely aligned source word or with the translation determined by another word alignment model. source words with the corresponding translation options. The guidance mechanism supervises the process, generating the final output with the expected translations, in the right place, including cases of external words unknown to the model. To test our approach, we experiment in two scenarios that pose different challenges to NMT. The first one is a translation task in which source sentences contain XML-annotated domain-specific terms. The presence of fe"
W17-4716,P02-1040,0,0.122482,"a successful way to reduce the OOV rate. The system used in our evaluation is the pre-trained model built for the best EnglishGerman submission (Sennrich et al., 2016a) at the News Translation task at WMT’16 (Bojar et al., 2016). At test stage, it is supplied with terminology lists containing term recommendations in BPE format. In all the experiments we use a default beam size of 12. 5.2 Results and discussion Our results on the MT task are reported in Table 1, which shows system performance on the concatenation of the test sets from the two target domains. Performance is measured with BLEU (Papineni et al., 2002), and statistical significance is computed with bootstrap resampling (Koehn, 2004). The result of the word-level baseline system is computed after post-processing its output following the approach of Jean et al. (2015), which was customized to our scenario. This method (see §2) is driven by the attention model to replace the UNK tokens in the output with their corresponding recommendation supplied as external knowledge. This post-processing strategy is not used for the BPE-level baseline because it implicitly addresses the problem of OOVs. We evaluate our guided decoder incrementally, by addin"
W17-4716,C16-1172,0,0.0268576,"Missing"
W17-4716,W08-0509,0,0.019382,"Missing"
W17-4716,W16-2209,0,0.0179184,"ations of specific words or phrases, are a typical example of external knowledge used to guide the process to meet such constraints. Meeting predefined constraints, however, does not represent the only case in which an external guidance can support decoding. In ensemble MT architectures, for example, the output of a translation system 157 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 157–168 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics in the form of linguistic information, such as POS tags or lemmas, Sennrich and Haddow (2016) propose to compute separate embedding vectors for each linguistic information and then concatenate them, without altering the decoder. Other solutions exploit the strengths of PBSMT systems to improve NMT by pre-translating the source sentence. In Niehues et al. (2016), the NMT model is fed with a concatenation of the source and its PBSMT translation. Some of these solutions lead to improvements in performance, but they all require time-intensive training of the NMT models to use an enriched input representation or to optimize the parameters of the model. (Stahlberg et al., 2016) proposed an"
W17-4716,P16-1162,0,0.317137,"l knowledge without retraining of paramount importance. To address this gap, we investigate problems arising from the fact that NMT operates on implicit word and sentence representations in a continuous space, which makes influencing the process with external knowledge more complex. In particular, we attempt to answer the following questions: i) How to enforce the presence of a given translation recommendation in the decoder’s output? ii) How to place these word(s) in the right position? iii) How to guide the translation of outof-vocabulary terms? Our solution extends an existing NMT decoder (Sennrich et al., 2016a) by introducing the possibility to guide the translation process with constraints provided as XML annotations of the Differently from the phrase-based paradigm, neural machine translation (NMT) operates on word and sentence representations in a continuous space. This makes the decoding process not only more difficult to interpret, but also harder to influence with external knowledge. For the latter problem, effective solutions like the XML-markup used by phrase-based models to inject fixed translation options as constraints at decoding time are not yet available. We propose a “guide” mechani"
W17-4716,N07-1064,0,0.0222422,"f the source word &quot;assigned&quot; helps GDec to correct other parts of the final translation, like generating “es gibt keine” (En: “There is no”) which is otherwise missing in the baseline translation. 6 Task 2: Automatic Post-Editing In our second experiment, we apply guided decoding in an automatic post-editing task. The goal of automatic post-editing (APE) is to correct errors in an MT-ed text. The problem is typically approached as a “monolingual translation” task, in which models are trained on parallel corpora containing (MT_output, MT_post-edit) pairs, with MT post-edits coming from humans (Simard et al., 2007; Chatterjee et al., 2015b, 2017). In their attempt to translate the entire input sentence, APE systems usually tend to over-correct the source words, i.e. to use all applicable correction options. This can happen even when the input is correct, often resulting in text deterioration (Bojar et al., 2015). To cope with this problem, neuralbased APE decoders would benefit from external knowledge indicating words in the input which are correct and thus should not be modified during decoding. For that we propose to use wordlevel binary quality estimation labels (Blatz et al., 2004; de Souza et al.,"
W17-4716,2006.amta-papers.25,0,0.147198,"Missing"
W17-4716,P16-2049,0,0.0166403,"r lemmas, Sennrich and Haddow (2016) propose to compute separate embedding vectors for each linguistic information and then concatenate them, without altering the decoder. Other solutions exploit the strengths of PBSMT systems to improve NMT by pre-translating the source sentence. In Niehues et al. (2016), the NMT model is fed with a concatenation of the source and its PBSMT translation. Some of these solutions lead to improvements in performance, but they all require time-intensive training of the NMT models to use an enriched input representation or to optimize the parameters of the model. (Stahlberg et al., 2016) proposed an approach which can be used at decoding time. A hierarchical PBSMT system is used to generate the translation lattices, which are then re-scored by the NMT decoder. During decoding, the NMT posterior probabilities are adjusted using the posterior scores computed by the hierarchical model. However, by representing the additional information as a translation lattice, this approach does not allow the use of external knowledge in the form of bilingual terms or quality judgements as we do in §5 and §6. A different technique is postprocessing the translated sentences. Jean et al. (2015)"
W17-4716,C00-2137,0,0.0534122,"al., 2014) to annotate the “good” words that should be kept. Due to the relatively high quality of the MT outputs (62.11 BLEU), source sentences will usually contain many terms annotated as “good”. This, compared to the MT task, poses more constraints on the decoder. 6.1 6.2 Results and discussion Our results on the APE task are reported in Table 3. Performance is measured with the two WMT’16 APE task metrics, namely TER and BLEU (Bojar et al., 2016). The statistical significance for BLEU is computed using paired bootstrap resampling, while for TER we use stratified approximate randomization (Yeh, 2000). Our first baseline (Base-MT), the same used at WMT, corresponds to the original MT output left untouched. Our second baseline (Base-APE) is a neural APE system that was trained on (MT_output, MT_post-edit) pairs but ignores the information from the QE annotations. Base-APE improves the Base-MT up to 3.14 BLEU points. Similar to §5.2, the evaluation of our guided decoder is performed incrementally. GDec_base forces the “good” words in the automatic translation to appear in the output according to the mechanism described in §4.1. This basic guidance mechanism yields only marginal improvements"
W17-4716,steinberger-etal-2006-jrc,0,\N,Missing
W17-4716,P07-2045,1,\N,Missing
W17-4716,W16-2323,0,\N,Missing
W17-4717,W17-4758,0,0.0835228,"ese submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score. The two submissions use the baseline features and the EnglishGerman submission also uses features from (Avramidis, 2017a). JXNU (T1): The JXNU submissions use features extracted from a neural network, including embedding features and cross-entropy features of the source sentences and their machine translations. The sentence embedding features are extracted through global average pooling from word embedding, which are trained using the WORD 2 VEC toolkit. The sentence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus."
W17-4717,W17-4772,0,0.0310859,"Missing"
W17-4717,W17-4759,0,0.0327025,"Missing"
W17-4717,L16-1356,1,0.770826,"or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method. They report results considering the word and its context versus the word in isolation, as well as variants with and without the gold labels at training time. Finally, for the phrase-level task, SHEF made use of predictions generated by BMAPS for task 2 and the phrase labelling approaches in (Blain et al., 2016). These approaches use the number of BAD word-level predictions in a phrase: an optimistic version labels the phrase as OK if at least half of the words in it are predicted to be OK, and a superpessimistic version labels the phrase as BAD if any word is in is predicted to be BAD. UHH (T1): The UHH-STK submission is based on sequence and tree kernels applied on the source and target input data for predicting the HTER score. The kernels use a backtranslation of the MT output into the source language as an additional input data representation. Further hand-crafted features were deﬁned in the form"
W17-4717,W17-4760,1,0.841464,"Missing"
W17-4717,W17-4755,1,0.0393895,"nd Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair"
W17-4717,W07-0718,1,0.696979,") • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), part"
W17-4717,W08-0309,1,0.537902,"Missing"
W17-4717,W10-1703,1,0.603181,"Missing"
W17-4717,W12-3102,1,0.508067,"Missing"
W17-4717,W17-4761,0,0.0349831,"Missing"
W17-4717,W17-4723,0,0.0362034,"Missing"
W17-4717,W17-4724,1,0.839009,"Missing"
W17-4717,W11-2103,1,0.744276,"Missing"
W17-4717,W17-4773,1,0.839713,"Missing"
W17-4717,W15-3025,1,0.905105,"Instead of predicting the HTER score, the systems attempted to predict the number of each of the four postediting operations (add, replace, shift, delete) at the sentence level. However, this did not lead to positive results. In future editions of the task, we plan to make this detailed post-editing information available again and suggest clear ways of using it. 5 Automatic Post-editing Task The WMT shared task on MT automatic postediting (APE), this year at its third round at WMT, aims to evaluate systems for the automatic correction of errors in a machine translated text. As pointed out by (Chatterjee et al., 2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. The third round of the APE task proposed to parti"
W17-4717,W17-4718,1,0.0662673,"builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair. Chinese allowed us to co-operate with an ongoing evaluation campaign on Asian languages org"
W17-4717,W17-4725,0,0.0391077,"Missing"
W17-4717,W08-0509,0,0.00859402,"l post-editors) and WMT17 DE-EN (German-English, pharmacological domain, professional post-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁci"
W17-4717,W17-4726,0,0.0387381,"Missing"
W17-4717,W13-2305,1,0.899702,"(RR) approach, so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer absolute quality. For example, RR can be used to discover which systems perform better than others, but RR does not provide any information about the absolute quality of system translations, i.e. it provides no information about how far a given system is from producing perfect output according to a human user. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and last year’s evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with RR and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established last year (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for English-to-Russian (th"
W17-4717,E14-1047,1,0.508986,"Missing"
W17-4717,N15-1124,1,0.658439,"Missing"
W17-4717,W13-0805,0,0.0140095,"ely contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets released in the three rounds 24 We used phrase-based MT systems trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 25 For both language directions, the source sentences and reference translations were provided by TAUS (https://www.taus.net/). 197 EN-DE Train (23,000) Dev (1,000) Test (2,000) DE-EN Train (25,000) Dev (1,000) Test (2,000) SRC Tokens TGT PE SRC Types TGT PE SRC Lemmas TGT PE 384448 17827 65120 403306 19355 69812 411246 19763 71483 18220 2931 8061 27382 3333 9765 31652 3506 10502 10946 1922 2626 21959 2686 3976 25550 2806 4282 437833 17578 35087 453096 18130 36082 456163 18313 36480 29745 4426 6987 19866 3583 5391 19172 3642 5"
W17-4717,W17-4775,0,0.0513454,"Missing"
W17-4717,W17-4730,1,0.829861,"Missing"
W17-4717,W17-4731,0,0.029579,"Missing"
W17-4717,W17-4727,0,0.046917,"Missing"
W17-4717,W16-2378,0,0.0869978,"a manuallyrevised version of the target, done by professional translators. Test data consists of (source, target) pairs having similar characteristics of those in the training set. Human post-edits of the test target instances were left apart to measure system performance. English-German data were drawn from the Information Technology (IT) domain. Training and test sets respectively contain 11,000 and 2,000 triplets. The data released for the 2016 round of the task (15,000 instances) and the artiﬁcially generated post-editing triplets (4 million instances) used by last year’s winning system (Junczys-Dowmunt and Grundkiewicz, 2016) were also provided as additional training material. German-English data were drawn from the Pharmacological domain. Training and development sets respectively contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets rele"
W17-4717,W11-2123,0,0.00943973,"-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT o"
W17-4717,I17-1013,0,0.0334345,"Missing"
W17-4717,W16-2384,0,0.0137431,"features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma a"
W17-4717,W17-4763,0,0.0300648,"ence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamb"
W17-4717,W04-3250,1,0.380171,"age28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to t"
W17-4717,P07-2045,1,0.0149362,"t with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to the task, which represented the common backbone of APE systems before the spread of neural solutions. The system is based on Moses (Koehn et al., 2007); translation and reordering models were estimated following the Moses protocol with default setup using 27 http://www.cs.umd.edu/˜snover/tercom/ https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 28 5.2 Participants Seven teams participated in the English-German task by submitting a total of ﬁfteen runs. Two of them also participated in the German-English task with ﬁve submitted runs. Participants are listed in Table 27, and a short description of their systems is provided in the following. Adam Mickiewicz University. AMU’s (ENDE) participation explores and"
W17-4717,C14-1017,0,0.0141812,"ord embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained from three matrices corresponding to the training data, the development set and a “truth” matrix between them, which is built from the word alignments and the gold labels to indicate which lexical items form a pair, and whether or not their lexical relation is OK or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method."
W17-4717,Q17-1015,0,0.0112193,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W16-2387,0,0.0126259,"the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisation algorithm. We note th"
W17-4717,W17-4764,0,0.0178514,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W06-3114,1,0.104699,"g (Bojar et al., 2017b) • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news trans"
W17-4717,P03-1021,0,0.144477,"ed better than monolingual models. The code for these models is freely available.15 DCU (T2): DCU’s submission is an ensemble of neural MT systems with different input factors, designed to jointly tackle both the automatic post-editing and word-level QE. 15 https://github.com/patelrajnath/rnn4nlp 186 Word-level features which have proven effective for QE, such as part-of-speech tags and dependency labels are included as input factors to NMT systems. NMT systems using different input representations are ensembled together in a log-linear model which is tuned for the F1 -mult metric using MERT (Och, 2003). The output of the ensemble is a pseudo-reference that is then TER aligned with the original MT to obtain OK/BAD tags for each word in the MT hypothesis. DFKI (T1): These submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negativ"
W17-4717,W15-3037,0,0.0138062,"n in the source side of the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisatio"
W17-4717,P16-1160,0,0.0330991,"t attention (looking at information anywhere in the source sequence during decoding) and hard monotonic attention (looking at one encoder state at a time from left to right, thus being more conservative and faithful to the original input), which are combined in different ways in the case of multi-source models. The artiﬁcial data provided by JunczysDowmunt and Grundkiewicz (2016) are used to boost performance by increasing the size of the corpus used for training. Univerzita Karlova v Praze. CUNI’s (EN-DE) system is based on the character-to-character neural network architecture described in (Lee et al., 2016). This architecture was compared with the standard neural network architecture proposed by Bahdanau et al. (2014) which uses byte-pair encoding (Sennrich et al., 2015) for generating translation tokens. During the experiments, two setups have been compared for each architecture: i) a single encoder with SRC and MT sentences concatenated, and ii) a two-encoder system, where each SRC and MT sentence is fed to a separate encoder. The submitted system uses the two-encoder architecture with a character-level encoder and decoder. The initial state of the decoder is a weighted combination of the ﬁnal"
W17-4717,W17-4733,0,0.0368107,"Missing"
W17-4717,W17-4765,1,0.786172,"Missing"
W17-4717,L16-1582,1,0.768758,"T 183 translations labelled with task-speciﬁc labels. Participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same fo"
W17-4717,C16-1241,0,0.0353857,"Missing"
W17-4717,W14-3342,0,0.0181229,"lity estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might inﬂuence the probability of a word being wrong. • Target token, its left and right contexts of 1 word. • Source word aligne"
W17-4717,P16-2046,0,0.0171489,"Missing"
W17-4717,W16-2379,0,0.0228966,"Missing"
W17-4717,P02-1040,0,0.119222,"Missing"
W17-4717,W16-2389,0,0.038207,"Missing"
W17-4717,W17-4736,0,0.0268357,"Missing"
W17-4717,W17-4737,0,0.0383776,"Missing"
W17-4717,W17-4738,0,0.0427074,"Missing"
W17-4717,W14-3301,1,0.655758,"Missing"
W17-4717,W16-2391,1,0.761669,"target sentences into sequences of character embeddings, and then passes them through a series of deep parallel stacked convolution/max pooling layers. The baseline features are provided through a multi-layer perceptron, 187 and then concatenated with the characterlevel information. Finally, the concatenation is passed onto another multi-layer perceptron and the very last layer outputs HTER values. The two submissions differ in the the use of standard (CNN+BASE-Single) and multi-task learning (CNN+BASE-Multi) for training. The QUEST-EMB submission follows the word embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained"
W17-4717,W16-2323,1,0.349233,"theses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target information in order to increase robustness and precision of the automatic corrections. The n-best hypotheses produced by 200 this ensemble are further re-ranked using features based on the edit distance between the original MT output and each APE hypothesis, as well as other statistical models (n-gram language model and operation sequence mod"
W17-4717,P16-1159,0,0.0156896,"different input factors, designed to jointly tackle both the APE task and the Word-Level QE task. Word-Level features which have proven effective for QE, such as word-alignments, partof-speech tags, and dependency labels, are included as input factors to neural machine translation systems, which are trained to output PostEdited MT hypotheses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target inf"
W17-4717,2006.amta-papers.25,0,0.817721,"trast to last year, we also provide datasets for two language pairs. The structure used for the data have been the same since WMT15. Each data instance consists of (i) a source sentence, (ii) its automatic translation into the target language, (iii) the manually post-edited version of the automatic translation, (iv) a free reference translation of the source sentence. Post-edits are used to extract labels for the 4.4 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the proportion of their words that need to be ﬁxed. HTER (Snover et al., 2006b) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version. Labels HTER labels were computed using the TERCOM tool16 with default settings (tokenised, case insensitive, exact matching only), with scores capped to 1. 188 16 http://www.cs.umd.edu/˜snover/tercom/ Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: with an edit operation: insertion, deletion, substitution or no edit (correct word). We mark each edited word as BAD, and the remainingn as OK. • Scoring: Pears"
W17-4717,W17-4756,0,0.0529328,"Missing"
W17-4717,P15-4020,1,0.738308,"Missing"
W17-4717,P13-4014,1,0.634274,"Missing"
W17-4717,W17-4720,1,0.830567,"Missing"
W17-4717,W17-4776,0,0.0596806,"Missing"
W17-4717,W17-4777,1,0.832756,"Missing"
W17-4717,W17-4742,0,0.0342495,"Missing"
W17-4717,W17-4744,0,0.0607458,"Missing"
W17-4717,C00-2137,0,0.0420443,"ch edited word as BAD, and the remainingn as OK. • Scoring: Pearson’s r correlation score (primary metric, ofﬁcial score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). Evaluation Analogously to the last year’s task, the primary evaluation metric is the multiplication of F1 -scores for the OK and BAD classes, denoted as F1 -mult. Unlike previously used F1 BAD score this metric is not biased towards “pessimistic” labellings. We also report F1 -scores for individual classes for completeness. We test the signiﬁcance of the results using randomisation tests (Yeh, 2000) with Bonferroni correction (Abdi, 2007). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical signiﬁcance on Pearson r was computed using the William’s test.17 Results Tables 13 and 14 summarise the results for Task 1 on German–English and English– German datasets, respectively, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems for the ranking variant. The top three systems are the same for both datasets, and the ranking of systems according to their performance is similar for"
W17-4717,W17-4745,1,0.825998,"Missing"
W17-4717,P16-1147,0,0.0102821,"Missing"
W17-4718,W17-4746,1,0.668714,"Missing"
W17-4718,D17-1105,0,0.401049,"Missing"
W17-4718,W17-4749,0,0.112115,"Missing"
W17-4718,W14-3348,0,0.128767,"yer from a ResNet-50 (He et al., 2016) convolutional neural network trained on the ImageNet dataset (Russakovsky et al., 2015). Those feature maps provide spatial information on which the model focuses through the attention mechanism. 4 Text-similarity Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.3 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and 220 3 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 4.1.1 Task 1: English → German Multi30K 2017 test data Table 4 shows the resu"
W17-4718,W17-4748,0,0.0838327,"Missing"
W17-4718,1983.tc-1.13,0,0.273609,"Missing"
W17-4718,P14-2074,1,0.832004,"Missing"
W17-4718,W16-3210,1,0.583063,"tion at test time. The training data, however, consists of images with independent descriptions in both source and target languages. Introduction The Shared Task on Multimodal Translation and Multilingual Image Description tackles the problem of generating descriptions of images for languages other than English. The vast majority of image description research has focused on Englishlanguage description due to the abundance of crowdsourced resources (Bernardi et al., 2016). However, there has been a significant amount of recent work on creating multilingual image description datasets in German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Turkish (Unal et al., 2016), Chinese (Li et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), and Dutch (van Miltenburg et al., 2017). Progress on this problem will be useful for native-language image search, multilingual ecommerce, and audio-described video for visually impaired viewers. The first empirical results for multimodal translation showed the potential for visual context to The translation task has been extended to include a new language, French. This extension means the Multi30K dataset (Elliott et al., 201"
W17-4718,N16-1022,0,0.0283339,"authors selected 1,000 images from the collection to form the dataset for the Multimodal Translation task based on a manual inspection of the English descriptions. Professional German translations were collected for those 1,000 English-described images. The remaining 1,071 images were used for the Multilingual Image Description task. We collected five ad2 http://www.crowdflower.com As a secondary evaluation dataset for the Multimodal Translation task, we collected and translated a set of image descriptions that potentially contain ambiguous verbs. We based our selection on the VerSe dataset (Gella et al., 2016), which annotates a subset of the COCO (Lin et al., 2014) and TUHOI (Le et al., 2014) images with OntoNotes senses for 90 verbs which are ambiguous, e.g. play. Their goals were to test the feasibility of annotating images with the word sense of a given verb (rather than verbs themselves) and to provide a gold-labelled dataset for evaluating automatic visual sense disambiguation methods. Altogether, the VerSe dataset contains 3,518 images, but we limited ourselves to its COCO section, since for our purposes we also need the image descriptions, which are not available in TUHOI. The COCO portion"
W17-4718,P16-1227,0,0.0406008,"training data, however, consists of images with independent descriptions in both source and target languages. Introduction The Shared Task on Multimodal Translation and Multilingual Image Description tackles the problem of generating descriptions of images for languages other than English. The vast majority of image description research has focused on Englishlanguage description due to the abundance of crowdsourced resources (Bernardi et al., 2016). However, there has been a significant amount of recent work on creating multilingual image description datasets in German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Turkish (Unal et al., 2016), Chinese (Li et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), and Dutch (van Miltenburg et al., 2017). Progress on this problem will be useful for native-language image search, multilingual ecommerce, and audio-described video for visually impaired viewers. The first empirical results for multimodal translation showed the potential for visual context to The translation task has been extended to include a new language, French. This extension means the Multi30K dataset (Elliott et al., 2016) is now triple aligned"
W17-4718,W17-4750,0,0.0641413,"Missing"
W17-4718,P16-4010,0,0.0212154,"Missing"
W17-4718,P07-2045,0,0.0170989,"etail in (Calixto et al., 2017b). They are model IMGW , in which image features are used as words in the source-language encoder; model IMGE , where image features are used to initialise the hidden states of the forward and backward encoder RNNs; and model IMGD , where the image features are used as additional signals to initialise the decoder hidden state. Each image has one corresponding feature vector, obtained from the activations of the NICT (Task 1) These are constrained submissions for both language pairs. First, a hierarchical phrase-based (HPB) translation system s built using Moses (Koehn et al., 2007) with standard features. Then, an attentional encoder-decoder network (Bahdanau et al., 2015) is trained and used as an additional feature to rerank the n-best output of the HPB system. A unimodal NMT model is also trained to integrate visual information. Instead of integrating visual features into the NMT model directly, image retrieval methods are employed to obtain target language descriptions of images that are similar to the image described by the source sentence, and this target description information is integrated into the NMT model. A multimodal 219 NMT model is also used to rerank th"
W17-4718,W14-5403,0,0.0178273,"l Translation task based on a manual inspection of the English descriptions. Professional German translations were collected for those 1,000 English-described images. The remaining 1,071 images were used for the Multilingual Image Description task. We collected five ad2 http://www.crowdflower.com As a secondary evaluation dataset for the Multimodal Translation task, we collected and translated a set of image descriptions that potentially contain ambiguous verbs. We based our selection on the VerSe dataset (Gella et al., 2016), which annotates a subset of the COCO (Lin et al., 2014) and TUHOI (Le et al., 2014) images with OntoNotes senses for 90 verbs which are ambiguous, e.g. play. Their goals were to test the feasibility of annotating images with the word sense of a given verb (rather than verbs themselves) and to provide a gold-labelled dataset for evaluating automatic visual sense disambiguation methods. Altogether, the VerSe dataset contains 3,518 images, but we limited ourselves to its COCO section, since for our purposes we also need the image descriptions, which are not available in TUHOI. The COCO portion covers 82 verbs; we further discarded verbs that are unambiguous in the dataset, i.e."
W17-4718,P17-2031,0,0.221406,"Missing"
W17-4718,D15-1166,0,0.00458431,"models - as measured by BLEU on the Multi30K development set - was used for both the constrained and unconstrained submissions. SHEF (Task 1) The SHEF systems utilize the predicted posterior probability distribution over the image object classes as image features. To do so, they make use of the pre-trained ResNet-152 (He et al., 2016), a deep CNN based image network that is trained over the 1,000 object categories on the Imagenet dataset (Deng et al., 2009) to obtain the posterior distribution. The model follows a standard encoder-decoder NMT approach using softdot attention as described in (Luong et al., 2015). It explores image information in three ways: a) to initialize the encoder; b) to initialize the decoder; c) to condition each source word with the image class posteriors. In all these three ways, non-linear affine transformations over the posteriors are used as image features. Baseline — Task 1 The baseline system for the multimodal translation task is a text-only neural machine translation system built with the Nematus toolkit (Sennrich et al., 2017). Most settings and hyperparameters were kept as default, with a few exceptions: batch size of 40 (instead of 80 due to memory constraints) and"
W17-4718,W17-4751,0,0.0653545,"Missing"
W17-4718,2006.amta-papers.25,0,0.119578,"ation on which the model focuses through the attention mechanism. 4 Text-similarity Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.3 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and 220 3 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 4.1.1 Task 1: English → German Multi30K 2017 test data Table 4 shows the results on the Multi30K 2017 test data with a German target language. It interesting to note that the metrics do not fully agree on the ranking of systems, although th"
W17-4718,W17-4752,1,0.893716,"Missing"
W17-4718,W16-2346,1,0.498931,"Missing"
W17-4718,P16-1168,0,0.0742711,"Missing"
W17-4718,P03-1021,0,0.033669,"re to rerank the n-best output of the HPB system. A unimodal NMT model is also trained to integrate visual information. Instead of integrating visual features into the NMT model directly, image retrieval methods are employed to obtain target language descriptions of images that are similar to the image described by the source sentence, and this target description information is integrated into the NMT model. A multimodal 219 NMT model is also used to rerank the HPB output. All feature weights (including the standard features, the NMT feature and the multimodal NMT feature) were tuned by MERT (Och, 2003). On the development set, the NMT feature improved the HPB system significantly. However, the multimodal NMT feature did not further improve the HPB system that had integrated the NMT feature. OREGONSTATE (Task 1) The OREGONSTATE system uses a very simple but effective model which feeds the image information to both encoder and decoder. On the encoder side, the image representation was used as an initialization information to generate the source words’ representations. This step strengthens the relatedness between image’s and source words’ representations. Additionally, the decoder uses alignm"
W17-4718,tiedemann-2012-parallel,0,0.032402,"and generates more accurate target side sentence. UvA-TiCC (Task 1) The submitted systems are Imagination models (Elliott and K´ad´ar, 2017), which are trained to perform two tasks in a multitask learning framework: a) produce the target sentence, and b) predict the visual feature vector of the corresponding image. The constrained models are trained over only the 29,000 training examples in the Multi30K dataset with a source-side vocabulary of 10,214 types and a target-side vocabulary of 16,022 types. The unconstrained models are trained over a concatenation of the Multi30K, News Commentary (Tiedemann, 2012) parallel texts, and MS COCO (Chen et al., 2015) dataset with a joint source-target vocabulary of 17,597 word pieces (Schuster and Nakajima, 2012). In both constrained and unconstrained submissions, the models were trained to predict the 2048D GoogleLeNetV3 feature vector (Szegedy et al., 2015) of an image associated with a source language sentence. The output of an ensemble of the three best randomly initialized models - as measured by BLEU on the Multi30K development set - was used for both the constrained and unconstrained submissions. SHEF (Task 1) The SHEF systems utilize the predicted po"
W17-4718,P02-1040,0,0.113412,"trained on the ImageNet dataset (Russakovsky et al., 2015). Those feature maps provide spatial information on which the model focuses through the attention mechanism. 4 Text-similarity Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.3 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and 220 3 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 4.1.1 Task 1: English → German Multi30K 2017 test data Table 4 shows the results on the Multi30K 2017 test data with a German target languag"
W17-4718,W17-3503,1,0.863632,"Missing"
W17-4718,P17-2066,0,0.0342394,"Multimodal Translation and Multilingual Image Description tackles the problem of generating descriptions of images for languages other than English. The vast majority of image description research has focused on Englishlanguage description due to the abundance of crowdsourced resources (Bernardi et al., 2016). However, there has been a significant amount of recent work on creating multilingual image description datasets in German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Turkish (Unal et al., 2016), Chinese (Li et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), and Dutch (van Miltenburg et al., 2017). Progress on this problem will be useful for native-language image search, multilingual ecommerce, and audio-described video for visually impaired viewers. The first empirical results for multimodal translation showed the potential for visual context to The translation task has been extended to include a new language, French. This extension means the Multi30K dataset (Elliott et al., 2016) is now triple aligned, with English descriptions translated into both German and French. The description generation task has substantially changed since last year. T"
W17-4718,Q14-1006,0,0.339227,". In last year’s Crosslingual Image Description task, the aim was to produce a single target language description, given five source language descriptions and the image. In this year’s Multilingual Image Description task, participants received only an unseen image at test time, without source language descriptions. 2.2 En: A group of people are eating noddles. De: Eine Gruppe von Leuten isst Nudeln. Fr: Un groupe de gens mangent des nouilles. Datasets The Multi30K dataset (Elliott et al., 2016) is the primary dataset for the shared task. It contains 31K images originally described in English (Young et al., 2014) with two types of multilingual data: a collection of professionally translated German sentences, and a collection of independently crowdsourced German descriptions. This year the Multi30K dataset has been extended with new evaluation data for the Translation and Image Description tasks, and an additional language for the Translation task. In addition, we released a new evaluation dataset featuring ambiguities that we expected would benefit from visual context. Table 1 presents an overview of the new evaluation datasets. Figure 1 shows an example of an image with an aligned English-German-Fren"
W17-4718,W16-2323,0,0.0161023,"urce word with the image class posteriors. In all these three ways, non-linear affine transformations over the posteriors are used as image features. Baseline — Task 1 The baseline system for the multimodal translation task is a text-only neural machine translation system built with the Nematus toolkit (Sennrich et al., 2017). Most settings and hyperparameters were kept as default, with a few exceptions: batch size of 40 (instead of 80 due to memory constraints) and ADAM as optimizer. In order to handle rare and OOV words, we used the Byte Pair Encoding Compression Algorithm to segment words (Sennrich et al., 2016b). The merge operations for word segmentation were learned using training data in both source and target languages. These were then applied to all training, validation and test sets in both source and target languages. In post-processing, the original words were restored by concatenating the subwords. Baseline — Task 2 The baseline for the multilingual image description task is an attention-based image description system trained over only the German image descriptions (Caglayan et al., 2017b). The visual representation are extracted from the so-called res4f relu layer from a ResNet-50 (He et"
W17-4718,W17-4753,0,0.0938137,"sh component of the model is ignored and only German captions are ID AFRL-OHIOSTATE Participating team Air Force Research Laboratory & Ohio State University (Duselis et al., 2017) CMU Carnegie Melon University (Jaffe, 2017) CUNI Univerzita Karlova v Praze (Helcl and Libovick´y, 2017) DCU-ADAPT Dublin City University (Calixto et al., 2017a) LIUMCVC Laboratoire d’Informatique de l’Universit´e du Maine & Universitat Autonoma de Barcelona Computer Vision Center (Caglayan et al., 2017a) NICT National Institute of Information and Communications Technology & Nara Institute of Science and Technology (Zhang et al., 2017) OREGONSTATE SHEF UvA-TiCC Oregon State University (Ma et al., 2017) University of Sheffield (Madhyastha et al., 2017) Universiteit van Amsterdam & Tilburg University (Elliott and K´ad´ar, 2017) Table 3: Participants in the WMT17 multimodal machine translation shared task. generated. FC7 layer of the VGG19 network, and consist of a 4096D real-valued vector that encode information about the entire image. CUNI (Tasks 1 and 2) For Task 1, the submissions employ the standard neural MT (NMT) scheme enriched with another attentive encoder for the input image. It uses a hierarchical attention combina"
W17-4718,P16-1162,0,0.00319904,"urce word with the image class posteriors. In all these three ways, non-linear affine transformations over the posteriors are used as image features. Baseline — Task 1 The baseline system for the multimodal translation task is a text-only neural machine translation system built with the Nematus toolkit (Sennrich et al., 2017). Most settings and hyperparameters were kept as default, with a few exceptions: batch size of 40 (instead of 80 due to memory constraints) and ADAM as optimizer. In order to handle rare and OOV words, we used the Byte Pair Encoding Compression Algorithm to segment words (Sennrich et al., 2016b). The merge operations for word segmentation were learned using training data in both source and target languages. These were then applied to all training, validation and test sets in both source and target languages. In post-processing, the original words were restored by concatenating the subwords. Baseline — Task 2 The baseline for the multilingual image description task is an attention-based image description system trained over only the German image descriptions (Caglayan et al., 2017b). The visual representation are extracted from the so-called res4f relu layer from a ResNet-50 (He et"
W17-4718,E17-3017,0,0.0333329,"Missing"
W17-4718,P11-2031,0,\N,Missing
W17-4718,W17-4747,0,\N,Missing
W17-4718,I17-1014,1,\N,Missing
W17-4718,N16-1021,0,\N,Missing
W17-4734,W05-0909,0,0.158301,"ions from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alignments between the input hypotheses, which are obtained from METEOR (Banerjee and Lavie, 2005). The hypotheses are then reordered to match a selected skeleton hypothesis in terms of word ordering. We generate I different CNs, each having one of the input systems as the skeleton hypothesis, and the final lattice is the union of all I generated CNs. In Figure 1 an example of a confusion network with I = 4 input translations is depicted. Decoding of a confusion network finds the best path in the network. Each arc is assigned a score of a linear model combination of M different models, which includes word penalty, 3-gram language model trained on the input hypotheses, a binary primary syst"
W17-4734,P09-1064,0,0.0328843,"translations. pothesis, and a binary voting feature for each system. The binary voting feature for a system is 1 if and only if the decoded word is from that system, and 0 otherwise. The different model weights for system combination are trained with MERT (Och, 2003) and optimized towards 8·B LEU −T ER. 4.2 Consensus-based System Selection 5 Experimental Evaluation As a secondary solution for system combination, we used USFD’s consensus-based n-nbest list selection approach (Blain et al., 2017) for system combination by combining each system’s output in the form of a n-best list. Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varying n-best list in NMT. Given a n-best list, each translation hypothesis is scored against the other MT candidates of the search space towards an automatic metric. In our experiment we considered three automatic metrics amongst the most widely used and which have been shown to be well correlated with human judgments (Bojar et al., 2016): B LEU, B EER"
W17-4734,D17-1209,1,0.891192,"Missing"
W17-4734,E14-2008,1,0.856971,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W16-2302,1,0.832947,". Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varying n-best list in NMT. Given a n-best list, each translation hypothesis is scored against the other MT candidates of the search space towards an automatic metric. In our experiment we considered three automatic metrics amongst the most widely used and which have been shown to be well correlated with human judgments (Bojar et al., 2016): B LEU, B EER (Stanojevic and Simaan, 2014) or C HR F (Popovic, 2015). The entire list of MT candidates is then entirely re-ranked according to the averaged score of each candidate. Different from most re-ranking approaches which make use of additional information usually treated as new model components and combined with the existing ones, we here focus only on the MT candidates. The difference between the consensus-based n-best list selection and an oracle translation is the absence Since only one development set was provided we split the given development set into two parts: newsdev2017/1 a"
W17-4734,W14-3310,1,0.870459,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W17-4703,1,0.884283,"Missing"
W17-4734,2014.iwslt-evaluation.7,1,0.873477,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W17-4737,1,0.831634,"Missing"
W17-4734,W11-2123,0,0.0435093,"o this end, k-best hypothesis from the dictionary were generated, as well as the n-best hypothesis 3.4 Tilde The Tilde system is a Moses phrase-based SMT system that was trained on the Tilde MT platform (Vasil¸jevs et al., 2012). The system was trained using all available parallel data - 1.74 million unique sentence pairs after filtering, and 3 million unique sentence pairs that were acquired by re-translating a random selection of indomain monolingual sentences with a neural machine translation system (Pinnis et al., 2017). The system has a 5-gram language model that was trained using KenLM (Heafield, 2011) on all available monolingual data (27.83 million unique sentences). 3.5 UEDIN The University of Edinburgh’s system is an attentional encoder-decoder (Bahdanau et al., 2015), trained using the Nematus toolkit (Sennrich et al., 2017c). As training data, we used all parallel and synthetic data, which was tokenized, truecased, and filtered as described in Section 2. After filtering, the data was segmented into subword units using byte-pair-encoding (BPE), for which we used 90,000 operations, jointly learned over both sides of the parallel corpora. We used word embeddings of size 512 and hidden la"
W17-4734,W15-3049,0,0.0536506,"Missing"
W17-4734,E17-2025,0,0.0291776,"l and synthetic data, which was tokenized, truecased, and filtered as described in Section 2. After filtering, the data was segmented into subword units using byte-pair-encoding (BPE), for which we used 90,000 operations, jointly learned over both sides of the parallel corpora. We used word embeddings of size 512 and hidden layers of size 1024, with the size of the source and target network vocabularies fixed to the size of the respective BPE vocabularies. In order to reduce the size of the models, the target-side embedding weights were tied with the transpose of 350 the output weight matrix (Press and Wolf, 2017). We used a deep transition architecture inspired by the one proposed by Zilly et al. (2016) for language modelling. In experiments conducted during feature development, we found that this gave consistent improvements across multiple language pairs. We also applied layer normalisation (Ba et al., 2016) to all recurrent and feed-forward layers, except for layers that are followed by a softmax. In preliminary experiments, we found that using layer normalisation led to faster convergence and resulted in slightly better performance. We trained the models with adam (Kingma and Ba, 2015), using a le"
W17-4734,E17-3017,0,0.0486901,"Missing"
W17-4734,P17-4012,0,0.0301124,"stem for the WMT 2017 shared task for machine translation of news 1 are seven individual 1 http://www.statmt.org/wmt17/ translation-task.html 348 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 348–357 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics tool. The number of sentences being removed is approximately 50000. in Neural Monkey. Instead, the translations were generated using greedy search. 3 3.2 Translation Systems The neural machine translation models from KIT are built with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for p"
W17-4734,D17-1159,0,0.0716203,"Missing"
W17-4734,D16-1096,0,0.0289091,"rom backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about the effect of each technique is described in Pham et al. (2017) Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 KIT CUNI The CUNI component of"
W17-4734,P03-1021,0,0.0379532,"re case-sensitive. of reference translation: each translation hypothesis is scored against all the other hypotheses used as references while in an oracle translation each translation hypothesis is scored against a single reference. This results in obtaining as best translation hypothesis the candidate that is most similar to the most likely translations. pothesis, and a binary voting feature for each system. The binary voting feature for a system is 1 if and only if the decoded word is from that system, and 0 otherwise. The different model weights for system combination are trained with MERT (Och, 2003) and optimized towards 8·B LEU −T ER. 4.2 Consensus-based System Selection 5 Experimental Evaluation As a secondary solution for system combination, we used USFD’s consensus-based n-nbest list selection approach (Blain et al., 2017) for system combination by combining each system’s output in the form of a n-best list. Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varyi"
W17-4734,P16-1162,0,0.11892,"he effect of each technique is described in Pham et al. (2017) Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 KIT CUNI The CUNI component of the system was built using Neural Monkey2 (Helcl and Libovick´y, 2017), a flexible sequence-to-sequence toolkit implementing primarily the Bahdanau et al. (2015) model but useful also in multi-modal translation and multi-task training. We used essentially the baseline setup of the system as released for the WMT17 NMT Training Task3 (Bojar et al., 2017) for an 8GB GPU card. This involves BPE (Sennrich et al., 2016) with 30k merges, maximum sentence length for both source and target limited to 50 (BPE) tokens, no dropout and embeddings (both source and target) of 600, vocabulary shared between encoder and decoder, attention and conditional GRU (Firat and Cho, 2016). We experimented with the RNN size of the encoder and decoder and increased them to 800 instead of 600, at the expense of reducing batch size to 10. The batch size of 30 with this enlarged model would still fit into our GPU card but this run was prematurely interrupted due to a hardware failure and we noticed that it converges slower in terms"
W17-4734,W14-3354,0,0.0640118,"Missing"
W17-4734,P16-5005,0,0.0206781,"with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about th"
W17-4734,P16-1008,0,0.0235141,"with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about th"
W17-4734,P12-3008,0,0.0243602,"Missing"
W17-4752,P07-2045,0,0.00461687,"the visual feature at each time step. 4 NMT model We implemented our NMT system (Section 3.2) in PyTorch. We use a single layer bidirectional LSTM based encoder-decoder model. We used ReLU as the projection non-linearity and used dropout with probability of 0.2. We used the Adadelta optimizer (Zeiler, 2012) with the default learning rate (0.01). The batch size was set to 20. We trained it for 50 epochs and selected the model that performs best on the validation set using BLEU as the metric. We normalised punctuations, lowercased and tokenised the input text using the script provided in Moses (Koehn et al., 2007). Our experiments were performed with the vocabulary size of 6,000 English words, 6,500 French words and 8,000 German words after removing words that appeared only once in the training set (these words were replaced with &lt; U N K &gt;, as described in Section 3.2). At decoding time, we post-processed the output translations by replacing &lt; U N K &gt; with an empty string. Experimental settings We use our own implementation of a multimodal NMT approach and explore a number of variants of this model in order to understand the effects of using the classification layer instead of a lower level CNN layer a"
W17-4752,D15-1166,0,0.0252974,"ures were extracted from the 152-layer version of ResNet (He et al., 2015), a Deep Convolutional Neural Network (CNN) pre-trained on 1,000 object categories (synsets) of the classification task of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2015). We extracted the final layer after applying the softmax function. This layer is a 1,000dimensional vector providing class posterior probability estimates at image level for the 1,000 object Neural Machine Translation We use a standard LSTM-based bidirectional encoder-decoder architecture with global attention (Luong et al., 2015). All our NMT models have the following architecture: the input and output vocabulary are limited to words that appear at least twice in the training data and the remaining words are replaced by the &lt; U N K &gt; token. The hidden layer dimensionality is set to 256 and the 1 471 http://image-net.org/challenges/LSVRC/2015/results contains 29,000 training and 1,014 development instances: an image, a description in source language, and a description for each target language. There are two test sets: word dimensionality is set to 128, for both the encoder and decoder, as this configuration was found t"
W17-4752,W16-2358,0,0.0396086,"Missing"
W17-4752,P16-1168,0,0.108514,"Missing"
W17-4752,W16-2359,0,0.441364,"j.k.wang, l.specia}@sheffield.ac.uk Abstract cally a vector extracted from a Convolutional Neural Network (CNN) layer). However, it has not been clear thus far whether such image features actually help in the translation task and more important, if so it is not clear which aspects of the image can play a role and how. Recent approaches to Multimodal NMT have used low level image features, including dense fully connected vectors and spatial convolutional representations from an image classification network (Elliott et al., 2015; Huang et al., 2016). They also incorporate attention mechanisms (Calixto et al., 2016). However, the effect of image features or the efficacy of the representational contribution is still an open research question. For our submission, we propose replacing image representations used in current Multimodal NMT systems with a class-based probabilistic distribution that is estimated directly using a stateof-the-art image classification network. The core hypothesis is that such representations offer higher level semantic information and could be more beneficial to Multimodal NMT systems. In Section 2 we discuss the motivations behind our proposed system. In Section 3 we describe our"
W17-4752,W16-3210,1,0.917355,"Missing"
W17-4752,W16-2363,1,0.793595,"plac¸ant cˆot´e et se met sa raquette avec les deux mains . un joueur de tennis se d´eplace sur le ct´e et tient sa raquette avec ses deux mains . Figure 2: Example output translations from English to German (DE) and French (FR), for the Flickr test set (top) and the MSCOCO test set (bottom). We show the results of InitDec using Softmax as the visual feature. other languages such as German and Japanese (Elliott et al., 2016; Hitschler et al., 2016; Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017). nected layer or a convolutional layer of a CNN as image representation, with the exception of Shah et al. (2016) who used the classification output of VGG-16 as features to a phrase-based SMT system. In all cases, image information were found to provide only marginal improvements. The first known attempt at using NMT for machine translation of image descriptions is by Elliott et al. (2015), who conditioned an NMT system with a CNN image embedding (the penultimate layer of VGG-16 (Simonyan and Zisserman, 2014)) at the beginning of either the encoder or the decoder. The WMT16 shared task on Multimodal Machine Translation (Specia et al., 2016) has further encouraged research in this area. At the time, phra"
W17-4752,W16-2346,1,0.89995,"Missing"
W17-4752,P16-1227,0,0.029656,"itDec) DE (Reference) FR (Baseline) FR (InitDec) FR (Reference) un joueur de tennis se d´eplac¸ant de cˆote et sa raquette avec les deux mains . un joueur de tennis se d´eplac¸ant cˆot´e et se met sa raquette avec les deux mains . un joueur de tennis se d´eplace sur le ct´e et tient sa raquette avec ses deux mains . Figure 2: Example output translations from English to German (DE) and French (FR), for the Flickr test set (top) and the MSCOCO test set (bottom). We show the results of InitDec using Softmax as the visual feature. other languages such as German and Japanese (Elliott et al., 2016; Hitschler et al., 2016; Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017). nected layer or a convolutional layer of a CNN as image representation, with the exception of Shah et al. (2016) who used the classification output of VGG-16 as features to a phrase-based SMT system. In all cases, image information were found to provide only marginal improvements. The first known attempt at using NMT for machine translation of image descriptions is by Elliott et al. (2015), who conditioned an NMT system with a CNN image embedding (the penultimate layer of VGG-16 (Simonyan and Zisserman, 2014)) at the beginning of either th"
W17-4752,W16-2360,0,0.538401,"nt of Computer Science University of Sheffield, UK {p.madhyastha, j.k.wang, l.specia}@sheffield.ac.uk Abstract cally a vector extracted from a Convolutional Neural Network (CNN) layer). However, it has not been clear thus far whether such image features actually help in the translation task and more important, if so it is not clear which aspects of the image can play a role and how. Recent approaches to Multimodal NMT have used low level image features, including dense fully connected vectors and spatial convolutional representations from an image classification network (Elliott et al., 2015; Huang et al., 2016). They also incorporate attention mechanisms (Calixto et al., 2016). However, the effect of image features or the efficacy of the representational contribution is still an open research question. For our submission, we propose replacing image representations used in current Multimodal NMT systems with a class-based probabilistic distribution that is estimated directly using a stateof-the-art image classification network. The core hypothesis is that such representations offer higher level semantic information and could be more beneficial to Multimodal NMT systems. In Section 2 we discuss the mo"
W17-4752,P17-2066,0,0.128959,"(Reference) un joueur de tennis se d´eplac¸ant de cˆote et sa raquette avec les deux mains . un joueur de tennis se d´eplac¸ant cˆot´e et se met sa raquette avec les deux mains . un joueur de tennis se d´eplace sur le ct´e et tient sa raquette avec ses deux mains . Figure 2: Example output translations from English to German (DE) and French (FR), for the Flickr test set (top) and the MSCOCO test set (bottom). We show the results of InitDec using Softmax as the visual feature. other languages such as German and Japanese (Elliott et al., 2016; Hitschler et al., 2016; Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017). nected layer or a convolutional layer of a CNN as image representation, with the exception of Shah et al. (2016) who used the classification output of VGG-16 as features to a phrase-based SMT system. In all cases, image information were found to provide only marginal improvements. The first known attempt at using NMT for machine translation of image descriptions is by Elliott et al. (2015), who conditioned an NMT system with a CNN image embedding (the penultimate layer of VGG-16 (Simonyan and Zisserman, 2014)) at the beginning of either the encoder or the decoder. The WMT16 shared task on Mu"
W17-4752,Q14-1006,0,0.25429,"titative results. We conjecture that further hyperparameter search (increasing LSTM layers, dimensionality of the embeddings and hidden layers, etc.) and increasing the vocabulary size or using BPE could potentially improve the performance of our system on the task. 6 Related work There has been interest in recent years in the task of generating image descriptions (also known as image captioning). Bernardi et al. (2016) provide a detailed discussion on various image description generation approaches that have been developed. Currently, the two largest image description datasets are Flickr30K (Young et al., 2014) and MS COCO (Lin et al., 2014). These datasets are constructed in English and are aimed at advancing research on the generation of image descriptions in English. Recent attempts have been made to incorporate multilinguality into both these largescale datasets, with the datasets being extended to 473 EN A duck on the bank of a river DE (Baseline) DE (InitDec) DE (Reference) eine ente an der kste eines flusses . eine ente am ufer eines flusses eine ente am ufer eines flusses FR (Baseline) FR (InitDec) FR (Reference) un canard sur l’ eau , dans une rivi`ere un canard sur la rive d’ une rivi`ere"
W17-4760,W16-2391,1,0.679845,"Table 1 using FastText1 (Bojanowski et al., 2016) with 300 dimensions and learning rate set to 0.025. The default training settings are otherwise used. The in-domain data is the same as that used to train the SMT system that produced the translations in the QE datasets, as made available by the task organizers. For the word and phrase-level tasks, we used our word embeddings to obtain a word vector representation of 300 dimensions for each word of both the training and development sets. For the sentence-level task, the word embeddings are averaged for each sentence, as previously applied in (Scarton et al., 2016). Table 1: Statistics of the in-domain data used to train our embeddings. using the source-target word alignment by minimizing the negative log-likelihood using a `2 regularized objective as: L(W ) = − X s,t log(P r(t|s; W )) + λkW k2 (2) 3.3 where λ is the constant that controls the capacity of W with gradient descent-based optimization. We explore this approach for both word and phrase-level QE. For training, we rely on both the word-alignments and the gold QE labels (i.e. the OK/BAD labels). The former gives us the sourcetarget pairs, and the latter whether this pair is valid or not. Our as"
W17-4760,W15-3041,1,0.852685,"relied upon or should be fixed by post-editors. More recently, QE at phrase-level has emerged as a way of using quality predictions at decoding time in phrase-based Statistical MT (SMT) systems to guide the decoder such as to keep phrases which are predicted as good, and conversely to discard those which are predicted as bad (Logacheva, 2017). QE models are built based on a list of features along with a Machine Learning algorithm for either regression or classification. These features are usually extracted from the source and target texts or from the MT system that generated the translations. Shah et al. (2015) introduced a new set of features extracted using an unsupervised approach with the use of neural network: continuous-space n o exp φ(t)&gt; W φ(s) Pr(t|s; W ) = X t0 ∈T n o exp φ(t0 )&gt; W φ(s) (1) where φ denotes the word embeddings of any given word in a vocabulary V. The source words s and target words t are respectively taken from subspaces S ⊆ V and T ⊆ V. In essence, the problem can be reduced to first obtaining the corresponding word embeddings of the vocabularies of both source and target sentences using a substantially large monolingual corpus for each of the two languages, followed by us"
W17-4760,W09-0441,0,0.0419666,"23,000 sentences for training, 1,000 for development and 2,000 for test), and German→English segments on the Pharmaceutical domain (with 25,000 sentences for training, 1,000 for development and 2,000 for test). The same data is used for all three tasks: word, phrase and sentence-level prediction. For the word-level task, each token of the MT is annotated with OK or BAD labels. For the phrase-level task, phrases are segmented as given by an SMT decoder and also annotated with OK or BAD labels. Finally, for the sentence-level task, the quality label is a Human-Targeted Error Rate (HTER) score (Snover et al., 2009). 3.2 Tool 3.4 Evaluation We used the official task metrics to evaluate our results. For the word and phrase-level tasks, the metrics are F1 -BAD and F1 -OK which correspond to the F1 scores on both BAD and OK labels, and F1 multi which is the product of the two formers. For the sentence-level task, the metrics for scoring are Pearson’s correlation (primary metric), Mean Average Error (MAE) and Root Mean Squared Error (RMSE), and for ranking, Spearman’s rank correlation (primary metric) and DeltaAvg. Word Embeddings Word embeddings were used in our submissions for the three tasks. We trained i"
W17-4760,P15-4020,1,0.850897,"QE task. In grey are the results of the official baseline of the task. word embeddings trained on general purpose data, our embeddings are trained over in-domain data, as previously described. Word embeddings were averaged at sentence level in order to have a single vector representing each sentence. We then concatenated source and target in-domain embeddings with the 17 sentence-level baseline features provided by the organisers. An SVM regressor was used to train our QE model with hyper-parameters optimized via grid-search. For that we used the learning module available at QuEst++ toolkit (Specia et al., 2015). Although the sentence-level experiment is different from the approach applied for word and phrase-level tasks, our aim was to test the usability of the in-domain word embeddings. Our results are compared with the official baseline. Discussion The results of our sentence-level predictions are given in Table 4. Although the approach is rather simplistic, it achieves considerably good results by outperforming the baseline system and several other systems that participated in the shared task. For German→English, our system performed seventh out of 13 in the scoring task. For English→German, it p"
W17-4760,L16-1356,1,0.378591,"th both pessimistic approaches for en→de. One can also observe comparable performance for en→de when the surrounding context is used: the difference in terms of F1 -* scores between the full and window context is marginal. For de→en this is different: the phrase labelling based on word predictions using the window context outperforms the phrase laPhrase-level QE labelling (Task 3) While we could have chosen to predict phraselevel QE labels similarly to our word-level predictions, we opted for generating phrase-level labels from word-level labels following the labelling approaches described in Blain et al. (2016): • Optimistic: if half or more of words have a label OK, the phrase has the label OK (majority labelling). 548 English→German (2016) BMAPS-full-opti BMAPS-window-opti BMAPS-unigram-opti † BMAPS-unigram-nolabel-opti † BMAPS-unigram-opti BMAPS-full-pess BMAPS-window-pess BMAPS-unigram-pess • BMAPS-unigram-nolabel-pess • BMAPS-unigram-pess BMAPS-full-superpess BMAPS-window-superpess BMAPS-unigram-super-pess • BMAPS-unigram-nolabel-suppess • BMAPS-unigram-super-pess BASELINE English→German (2017) BMAPS-full-opti BMAPS-window-opti BMAPS-unigram-opti † BMAPS-unigram-nolabel-opti † BMAPS-unigram-opt"
W17-4760,C14-1017,0,0.3385,"ls to predict if information encoded in the source sentence is preserved in the target sentence after translation. This paper describes the SHEF submissions for the three sub-tasks of the Quality Estimation shared task of WMT17, namely: (i) a word-level prediction system using bilexical embeddings, (ii) a phrase-level labelling approach based on the word-level predictions, (iii) a sentencelevel prediction system using word embeddings and handcrafted baseline features. Results are promising for the sentence-level approach, but still very preliminary for the other two levels. 1 2 Bilinear Model Madhyastha et al. (2014) propose to use wordlevel embeddings to predict the strength of different types of lexical relationships between a pair of words, such as head-modifier relations between noun-adjective pairs. They designed a supervised framework for learning bilexical operators over distributional representations, based on learning bilinear forms W . We adapted their method to predict the strength of relationship between source and target words. This problem is formulated as a log-bilinear model, parametrized with W as follows: Introduction Quality Estimation (QE) allows the evaluation of Machine Translation ("
W17-4765,W16-2388,1,0.490661,"d much: most of the top ranking systems employ well-known regression methods and extensive feature engineering. Some of the most notable examples are the RTM systems of WMT 2014 and 15, which managed to reach the top of the ranks by employing Referential Translation Machines trained with SVMs for regression (Bicici, 2016). The LORIA (Langlois, 2015) and YSDA (Kozlova et al., 2016) systems of WMT 2015 and 2016, respectively, achieved similar performance by also pairing SVMs with many resource-heavy features. Neural Networks for sentence-level QE were introduced in WMT 2016 with the SimpleNets (Paetzold and Specia, 2016) and POSTECH (Kim and Lee, 2016) systems. While the SimpleNets system uses sequence-to-label LSTMs to predict the quality of a translation’s n-grams and then We present a new model for text regression that seamlessly combine engineered features and character-level information through deep parallel convolution stacks, multi-layer perceptrons and multitask learning. We use these models to create the SHEF/CNN systems for the sentence-level Quality Estimation task of WMT 2017 and Emotion Intensity Analysis task of WASSA 2017. Our experiments reveal that combining character-level clues and engineer"
W17-4765,S16-1077,0,0.0469556,"Missing"
W17-4765,C12-1008,0,0.0669293,"Missing"
W17-4765,2006.amta-papers.25,0,0.0315253,"vramidis, 2012), or they can be used to help human translators decide which automatic translations are worth post-editing, and which should be re-translated from scratch (Turchi et al., 2015). Sentence-level QE is the most popular variant, mostly due the fact that most modern statistical and neural MT systems translate one sentence at a time. In this task, the input is the original-translated sentence pair and the output is some numeric label that represents quality. The most commonly used label is HTER, which measures the human post-editing effort required to fix the translation in question (Snover et al., 2006). As shown in (Bojar et al., 2016), the performance of QE approaches submitted to the WMT shared tasks have steadily improved in recent years. However, the nature of these approaches have not changed much: most of the top ranking systems employ well-known regression methods and extensive feature engineering. Some of the most notable examples are the RTM systems of WMT 2014 and 15, which managed to reach the top of the ranks by employing Referential Translation Machines trained with SVMs for regression (Bicici, 2016). The LORIA (Langlois, 2015) and YSDA (Kozlova et al., 2016) systems of WMT 201"
W17-4765,W16-2382,0,0.0213286,"human post-editing effort required to fix the translation in question (Snover et al., 2006). As shown in (Bojar et al., 2016), the performance of QE approaches submitted to the WMT shared tasks have steadily improved in recent years. However, the nature of these approaches have not changed much: most of the top ranking systems employ well-known regression methods and extensive feature engineering. Some of the most notable examples are the RTM systems of WMT 2014 and 15, which managed to reach the top of the ranks by employing Referential Translation Machines trained with SVMs for regression (Bicici, 2016). The LORIA (Langlois, 2015) and YSDA (Kozlova et al., 2016) systems of WMT 2015 and 2016, respectively, achieved similar performance by also pairing SVMs with many resource-heavy features. Neural Networks for sentence-level QE were introduced in WMT 2016 with the SimpleNets (Paetzold and Specia, 2016) and POSTECH (Kim and Lee, 2016) systems. While the SimpleNets system uses sequence-to-label LSTMs to predict the quality of a translation’s n-grams and then We present a new model for text regression that seamlessly combine engineered features and character-level information through deep paralle"
W17-4765,D13-1170,0,0.00650731,"neered features and character-level information trained over HTER. The model used for the EIA task of WASSA 2017 applies only one convolution stack over the tweet being analysed, given that the task is not characterized by a sentence pair. The window range, convolution depth, as well as feature and final MLP depths are identical to the model used for the WMT 2017 task. We train one model for each emotion targeted in the shared task: anger, fear, joy and sadness. Since the organizers did not provide a set of baseline features, we produced our own features using the Stanford Sentiment Treebank (Socher et al., 2013), which is composed of 239,232 text segments annotated with respect to their positivity probability i.e. how likely they are to convey a positive emotion. The positivity values range from 0.0 (absolutely negative) to 1.0 (absolutely positive). Using this data, we extract nine features from each tweet: • SHEF/CNN-C+F+M: Uses the same architecture of SHEF/CNN-C+F, but the model is trained through multi-task learning over the values listed in Section 4. • The tweets’ emotion intensity; and Table 2 illustrates the Pearson, Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) scores for the"
W17-4765,P15-2087,0,0.013615,"ed Character-Level Convolutions for Text Regression Gustavo Henrique Paetzold and Lucia Specia Department of Computer Science University of Sheffield, UK {g.h.paetzold,l.specia}@sheffield.ac.uk Abstract sentence or even document level. Quality estimates can be incorporated in Machine Translation (MT) decoding or used for re-ranking of top candidates, for example, allowing for a more intelligently guided translation process (Avramidis, 2012), or they can be used to help human translators decide which automatic translations are worth post-editing, and which should be re-translated from scratch (Turchi et al., 2015). Sentence-level QE is the most popular variant, mostly due the fact that most modern statistical and neural MT systems translate one sentence at a time. In this task, the input is the original-translated sentence pair and the output is some numeric label that represents quality. The most commonly used label is HTER, which measures the human post-editing effort required to fix the translation in question (Snover et al., 2006). As shown in (Bojar et al., 2016), the performance of QE approaches submitted to the WMT shared tasks have steadily improved in recent years. However, the nature of these"
W17-4765,W16-2384,0,0.246775,"employ well-known regression methods and extensive feature engineering. Some of the most notable examples are the RTM systems of WMT 2014 and 15, which managed to reach the top of the ranks by employing Referential Translation Machines trained with SVMs for regression (Bicici, 2016). The LORIA (Langlois, 2015) and YSDA (Kozlova et al., 2016) systems of WMT 2015 and 2016, respectively, achieved similar performance by also pairing SVMs with many resource-heavy features. Neural Networks for sentence-level QE were introduced in WMT 2016 with the SimpleNets (Paetzold and Specia, 2016) and POSTECH (Kim and Lee, 2016) systems. While the SimpleNets system uses sequence-to-label LSTMs to predict the quality of a translation’s n-grams and then We present a new model for text regression that seamlessly combine engineered features and character-level information through deep parallel convolution stacks, multi-layer perceptrons and multitask learning. We use these models to create the SHEF/CNN systems for the sentence-level Quality Estimation task of WMT 2017 and Emotion Intensity Analysis task of WASSA 2017. Our experiments reveal that combining character-level clues and engineered features offers noticeable pe"
W17-4765,S16-1080,0,0.0670815,"Missing"
W17-4765,S16-1004,0,0.0220145,"uality at sentence-level. Though very interesting and distinct strategies, neither of them managed to outperform the best scoring SVM-based approach of WMT 2016. In the task of Emotion Intensity Analysis (EIA), Neural Networks have not yet been successfully employed. Unlike typical Sentiment Analysis tasks, which are set up as either binary or multiclass classification problems that require one to determine the opinion or sentiment in a given text, EIA aims at quantifying a certain emotion in a text, such as fear, anger, joy, sadness, etc. In the Emotion Intensity shared task of SemEval 2016 (Kiritchenko et al., 2016), which is the first of its kind, none of the five systems submitted employ neural regressors. We were also unable to find any other contributions outside the SemEval 2016 task that explore neural approaches to EIA. Given the volume of opportunities available when it comes to neural solutions for text regression, we introduce a new neural approach for the task. We innovate by using deep convolutional networks and multi-task learning to combine character-level information from the texts at hand with engineered features. Using this approach, we create the SHEF/CNN systems for the sentence-level"
W17-4765,W16-2385,0,0.281349,"lation in question (Snover et al., 2006). As shown in (Bojar et al., 2016), the performance of QE approaches submitted to the WMT shared tasks have steadily improved in recent years. However, the nature of these approaches have not changed much: most of the top ranking systems employ well-known regression methods and extensive feature engineering. Some of the most notable examples are the RTM systems of WMT 2014 and 15, which managed to reach the top of the ranks by employing Referential Translation Machines trained with SVMs for regression (Bicici, 2016). The LORIA (Langlois, 2015) and YSDA (Kozlova et al., 2016) systems of WMT 2015 and 2016, respectively, achieved similar performance by also pairing SVMs with many resource-heavy features. Neural Networks for sentence-level QE were introduced in WMT 2016 with the SimpleNets (Paetzold and Specia, 2016) and POSTECH (Kim and Lee, 2016) systems. While the SimpleNets system uses sequence-to-label LSTMs to predict the quality of a translation’s n-grams and then We present a new model for text regression that seamlessly combine engineered features and character-level information through deep parallel convolution stacks, multi-layer perceptrons and multitask"
W17-4765,W15-3038,0,0.022573,"required to fix the translation in question (Snover et al., 2006). As shown in (Bojar et al., 2016), the performance of QE approaches submitted to the WMT shared tasks have steadily improved in recent years. However, the nature of these approaches have not changed much: most of the top ranking systems employ well-known regression methods and extensive feature engineering. Some of the most notable examples are the RTM systems of WMT 2014 and 15, which managed to reach the top of the ranks by employing Referential Translation Machines trained with SVMs for regression (Bicici, 2016). The LORIA (Langlois, 2015) and YSDA (Kozlova et al., 2016) systems of WMT 2015 and 2016, respectively, achieved similar performance by also pairing SVMs with many resource-heavy features. Neural Networks for sentence-level QE were introduced in WMT 2016 with the SimpleNets (Paetzold and Specia, 2016) and POSTECH (Kim and Lee, 2016) systems. While the SimpleNets system uses sequence-to-label LSTMs to predict the quality of a translation’s n-grams and then We present a new model for text regression that seamlessly combine engineered features and character-level information through deep parallel convolution stacks, multi-"
W17-4765,S17-1007,0,0.0315924,"ements over using only one of these sources of information in isolation. 1 Introduction Text regression consists in estimating a numeric label based on information available from the text. The label can represent any abstract property of said text: its appropriateness, sentiment, fluency, simplicity, quality, etc. Due to their wide applicability in both research and industry, some of these tasks have been gaining a lot of attention. These include Quality Estimation and Emotion Intensity Analysis, which are the subjects of shared tasks held at the WMT 2017 conference1 and WASSA 2017 workshop2 (Mohammad and Bravo-Marquez, 2017), respectively. In Quality Estimation (QE), one attempts to estimate the quality of a machine translated text based on the information that can be extracted from the original sentence and its translation. The task has many variants, given that the quality of a translation can be estimated at word, phrase, 1 2 http://www.statmt.org/wmt17 http://optima.jrc.it/wassa2017 575 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 575–581 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics 23,000/25,000 and 1,000/1,000 i"
W17-4765,W16-2301,1,\N,Missing
W17-5910,S16-1150,0,0.136082,"Missing"
W17-5910,S16-1161,0,0.157498,"Missing"
W17-5910,L16-1284,1,0.850305,"Missing"
W17-5910,S16-1164,0,0.117013,"Missing"
W17-5910,S16-1157,0,0.169796,"Missing"
W17-5910,P13-3015,0,0.19796,"Goutte et al. (2016) for language variety identification. Introduction Lexical complexity plays a crucial role in reading comprehension. Several NLP systems have been developed to simplify texts to second language learners (Petersen and Ostendorf, 2007) and to native speakers with low literacy levels (Specia, 2010) and reading disabilities (Rello et al., 2013). Identifying which words are likely to be considered complex by a given target population is an important task in many text simplification pipelines called complex word identification (CWI). CWI has been addressed as a stand-alone task (Shardlow, 2013) and as part of studies in lexical and text simplification (Paetzold, 2016). The recent SemEval 2016 Task 11 on Complex Word Identification – henceforth SemEval CWI – addressed this challenge by providing participants with a manually annotated dataset for this purpose (Paetzold and Specia, 2016a). In the SemEval CWI dataset, words in context were tagged as complex or non-complex, that is, difficult to be understood by non-native English speakers, or not. Participating teams used this dataset to train classi2 Methods and Experiments In this section we present the data, the methods, and an overv"
W17-5910,S16-1162,0,0.0588997,"Missing"
W17-5910,S16-1146,0,0.0839136,"Missing"
W17-5910,yimam-etal-2017-multilingual,0,0.0467335,"otators over a set of 200 sentences, while the test set is composed by the judgments made over 9,000 sentences by only one annotator. The 9,200 sentences were evenly distributed across the 400 annotators. In the training set, a word is considered to be complex if at least one of the 20 annotators judged them so, thus reproducing a scenario that captures one of the biggest challenges in lexical simplification: predicting the vocabulary limitations of individuals based on the overall limitations of a group. This dataset is one of the few datasets available for CWI, another example is the one by Yimam et al. (2017). We build ensemble classifiers taking the output of systems that participated in the SemEval CWI task as input. This approach is equivalent to training multiple classifiers and combining them using ensembles. Our first goal is to build highperformance classifiers using plurality voting. Our second goal is to estimate the theoretical upper bound performance given the output of the systems that participated in the SemEval CWI competition using the oracle classifier. Following Malmasi et al. (2015) and Goutte et al. (2016) we use two approaches: 2.2 3.1 Plurality Voting: This approach selects th"
W17-5910,W15-0620,1,0.877257,"nvestigate whether human annotation correlates to the systems’ performance by carefully analyzing the samples of multiple annotators. Although in the shared task complexity was modeled as a binary classification task, we pose that lexical complexity should actually be seen in a continuum spectrum. Intuitively, words that are labeled as complex more often should be easier to be predicted by CWI systems. This hypothesis is investigated in Section 3.3. To the best of our knowledge, no evaluation of this kind has been carried out for CWI. The most similar analyses to ours have been carried out by Malmasi et al. (2015) for native language identification and by Goutte et al. (2016) for language variety identification. Introduction Lexical complexity plays a crucial role in reading comprehension. Several NLP systems have been developed to simplify texts to second language learners (Petersen and Ostendorf, 2007) and to native speakers with low literacy levels (Specia, 2010) and reading disabilities (Rello et al., 2013). Identifying which words are likely to be considered complex by a given target population is an important task in many text simplification pipelines called complex word identification (CWI). CWI"
W17-5910,S16-1155,1,0.693934,"Missing"
W17-5910,S16-1152,0,0.123637,"Missing"
W17-5910,C16-1069,1,0.841348,"cal complexity plays a crucial role in reading comprehension. Several NLP systems have been developed to simplify texts to second language learners (Petersen and Ostendorf, 2007) and to native speakers with low literacy levels (Specia, 2010) and reading disabilities (Rello et al., 2013). Identifying which words are likely to be considered complex by a given target population is an important task in many text simplification pipelines called complex word identification (CWI). CWI has been addressed as a stand-alone task (Shardlow, 2013) and as part of studies in lexical and text simplification (Paetzold, 2016). The recent SemEval 2016 Task 11 on Complex Word Identification – henceforth SemEval CWI – addressed this challenge by providing participants with a manually annotated dataset for this purpose (Paetzold and Specia, 2016a). In the SemEval CWI dataset, words in context were tagged as complex or non-complex, that is, difficult to be understood by non-native English speakers, or not. Participating teams used this dataset to train classi2 Methods and Experiments In this section we present the data, the methods, and an overview of the experiments we propose in this paper. The goal of the experiment"
W18-0507,W14-1206,0,0.0720755,"Missing"
W18-0507,W18-0519,0,0.153197,"Missing"
W18-0507,S16-1156,0,0.0850516,"Missing"
W18-0507,S16-1151,0,0.0753176,"Missing"
W18-0507,W18-0539,1,0.893727,"Missing"
W18-0507,W18-0538,0,0.100445,"lower log-probability for complex words. The systems submitted performed the best out of all systems for the cross-lingual task (the French dataset) both for the binary and probabilistic classification tasks, showing a promising direction in the creation of CWI dataset for new languages. 1 74 https://code.google.com/archive/p/word2vec/ tant features. Their best system shows an average performance compared to the other systems in the shared task for the monolingual English binary classification track. NLP-CIC present systems for the English and Spanish multilingual binary classification tasks (Aroyehun et al., 2018). The feature sets include morphological features such as frequency counts of target word on large corpora such as Wikipedia and Simple Wikipedia, syntactic and lexical features, psycholinguistic features from the MRC psycholinguistic database and entity features using the OpenNLP and CoreNLP tools, and word embedding distance as a feature which is computed between the target word and the sentence. Tree learners such as Random Forest, Gradient Boosted, and Tree Ensembles are used to train different classifiers. Furthermore, a deep learning approach based on 2D convolutional (CNN) and word embe"
W18-0507,S16-1148,0,0.0982594,"Missing"
W18-0507,W18-0518,0,0.065579,"on the ensemble techniques where AdaBoost classifier with 5000 estimators achieves the highest results, followed by the bootstrap aggregation classifier of Random Forest. All the features are used for the N EWS and W IKI N EWS datasets, but for the W IKIPEDIA dataset, MCR psycholinguistic features are excluded. For the probabilistic classification task, the same feature setups are used and the Linear Regression algorithm is used to estimate values of targets. CoastalCPH describe systems developed for multilingual and cross-lingual domains for the binary and probabilistic classification tasks (Bingel and Bjerva, 2018). Unlike most systems, they have focused mainly on German, Spanish, and French datasets in order to investigate if multitask learning can be applied to the cross-lingual CWI task. They have devised two models, using languageagnostic approach with an ensemble that comprises of Random Forests (random forest classifiers for the binary classification task and random forest regressors for the probabilistic classification tasks, with 100 trees) and feed-forward neural networks. Camb describes different systems (Gooding and Kochmar, 2018) they have developed for the monolingual English datasets both"
W18-0507,W18-0520,0,0.193647,"l domains for the binary and probabilistic classification tasks (Bingel and Bjerva, 2018). Unlike most systems, they have focused mainly on German, Spanish, and French datasets in order to investigate if multitask learning can be applied to the cross-lingual CWI task. They have devised two models, using languageagnostic approach with an ensemble that comprises of Random Forests (random forest classifiers for the binary classification task and random forest regressors for the probabilistic classification tasks, with 100 trees) and feed-forward neural networks. Camb describes different systems (Gooding and Kochmar, 2018) they have developed for the monolingual English datasets both for the binary and probabilistic classification tasks. They have used features that are based on the insights of the CWI shared task 2016 (Paetzold and Specia, 2016a) such as lexical features (word length, number of syllables, WordNet features such as the number of synsets), word n-gram and POS tags, and dependency parse relations. In addition, they have used features such as the number of words grammatically related to the target word, psycholinguistic features from the MRC database, CEFR (Common European Framework of Reference fo"
W18-0507,S16-1160,0,0.102164,"Missing"
W18-0507,W18-0540,0,0.0627627,"Missing"
W18-0507,W18-0521,0,0.24579,"layers followed by a sigmoid layer, which predicted the probability of the target word being complex. Their experiments show that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For the probabilistic classification task, their LSTMs achieve the best results. TMU submitted multilingual and cross-lingual CWI systems for both of the binary and probabilistic classification tasks (Kajiwara and Komachi, 2018). The systems use two variants of frequency features from the learner corpus (Lang-8 corpus) from Mizumoto et al. (2011) and from the general domain corpus (Wikipedia and WikiNews). The list of features used in building the model include the number of characters in the target word, number of words in the target phrase, and frequency of the target word in learner corpus (Lang-8 corpus) and general domain corpus (Wikipedia and WikiNews). Random forest classifiers are used for the binary classification task while random forest regressors are used for the probabilistic classification task using th"
W18-0507,S16-1164,0,0.0425763,"Missing"
W18-0507,S16-1149,1,0.538636,"elines is identifying which words are considered complex by a given target population (Shardlow, 2013). This task is known as complex word identification (CWI) and it has been attracting attention from the research community in the past few years. In this paper we present the findings of the second Complex Word Identification (CWI) shared task organized as part of the thirteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA) co-located with NAACL-HLT’2018. The second CWI shared task follows a successful first edition featuring 21 teams organized at SemEval’2016 (Paetzold and Specia, 2016a). While the first CWI shared task targeted an English dataset, the second edition focused on multilingualism providing datasets containing four languages: English, German, French, and Spanish. • English monolingual CWI; • German monolingual CWI; • Spanish monolingual CWI; and • Multilingual CWI with a French test set. For the first three tracks, participants were provided with training and testing data for the same language. For French, participants were provided only with a French test set and no French training data. In the CWI 2016, the task was cast as binary classification. To be able t"
W18-0507,S16-1162,0,0.138871,"Missing"
W18-0507,S16-1158,0,0.0719992,"Missing"
W18-0507,S16-1163,0,0.176167,"Missing"
W18-0507,D14-1162,0,0.0887949,"earning methods, 2) using the average embedding of target words as an input to a neural network, and 3) modeling the context of the target words using an LSTM. For the feature engineering-based systems, features such as linguistic, psycholinguistic, and language model features were used to train different binary and probabilistic classifiers. Lexical features include word length, number of syllables, and number of senses, hypernyms, and hyponyms in WordNet. For N-gram features, probabilities of the n-gram containing the target words were For embedding-based systems, a pre-trained GloVe model (Pennington et al., 2014) was used to get the vector representations of target words. For MWE, the average of the vectors is used. In the first approach, the resulting vector is passed on to a neural network with two ReLu layers followed by a sigmoid layer, which predicted the probability of the target word being complex. Their experiments show that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For t"
W18-0507,S16-1154,1,0.847385,"Missing"
W18-0507,W18-0541,0,0.102296,"Missing"
W18-0507,S16-1153,1,0.879975,"Missing"
W18-0507,S16-1161,0,0.200613,"Missing"
W18-0507,S16-1147,0,0.032877,"Missing"
W18-0507,S16-1157,0,0.212408,"Missing"
W18-0507,I11-1017,0,0.0188214,"that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For the probabilistic classification task, their LSTMs achieve the best results. TMU submitted multilingual and cross-lingual CWI systems for both of the binary and probabilistic classification tasks (Kajiwara and Komachi, 2018). The systems use two variants of frequency features from the learner corpus (Lang-8 corpus) from Mizumoto et al. (2011) and from the general domain corpus (Wikipedia and WikiNews). The list of features used in building the model include the number of characters in the target word, number of words in the target phrase, and frequency of the target word in learner corpus (Lang-8 corpus) and general domain corpus (Wikipedia and WikiNews). Random forest classifiers are used for the binary classification task while random forest regressors are used for the probabilistic classification task using the scikit-learn library. Feature ablation shows that both the length, frequency, and probability features (based on corpu"
W18-0507,P13-3015,0,0.247331,"ative speakers. To train their systems, participants received a labeled training set where words in context were annotated regarding their complexity. One month later, an unlabeled test set was provided and participating teams were required to upload their predictions for evaluation. More information about the data collection is presented in Section 3. Given the multilingual dataset provided, the CWI challenge was divided into four tracks: Introduction The most common first step in lexical simplification pipelines is identifying which words are considered complex by a given target population (Shardlow, 2013). This task is known as complex word identification (CWI) and it has been attracting attention from the research community in the past few years. In this paper we present the findings of the second Complex Word Identification (CWI) shared task organized as part of the thirteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA) co-located with NAACL-HLT’2018. The second CWI shared task follows a successful first edition featuring 21 teams organized at SemEval’2016 (Paetzold and Specia, 2016a). While the first CWI shared task targeted an English dataset, the second e"
W18-0507,S16-1152,0,0.262443,"Missing"
W18-0507,S16-1159,0,0.0576998,"Missing"
W18-0507,L16-1035,1,0.906663,"Missing"
W18-0507,W18-0522,0,0.230666,"n dependency relation, frequency features based on the BNC, Wikipedia, and Dale and Chall list corpora, number of synsets and senses in WordNet, and so on. The experiment is conducted using the Weka machine learning framework using the Support vector machine (with linear and radial basis function kernels), Na¨ıve Bayes, Logistic Regression, Random Tree, and Random Forest classification algorithms. The final experiments employ Support Vector Machines and Random Forest classifiers. CFILT IITB Developed ensemble-based classification systems for the English monolingual binary classification task (Wani et al., 2018). Lexical features based on WordNet for the target word are extracted as follows: 1) Degree of Polysemy: number of senses of the target word in WordNet, 2) Hyponym and Hypernym Tree Depth: the position of the word in WordNet’s hierarchical tree, and 3) Holonym and Meronym Counts: based on the relationship of the target word to its components (meronyms) or to the things it is contained in (Holonym’s). Additional feature classes include size-based features such as word count, word length, vowel counts, and syllable counts. They also use vocabulary-based features such as Ogden Basic (from Ogden’s"
W18-0507,S16-1146,0,0.112937,"Missing"
W18-0507,I17-2068,1,0.54282,"least 10 native English speakers and 10 non-native English speakers so that it is possible to investigate if native and non-native speakers have different CWI needs. 5) Complex words are not pre-highlighted, as in previous contributions, so that annotators are not biased to the pre-selection of the complex phrases. 6) In addition to single words, we allowed the annotation of multi-word expressions (MWE), up to a size of 50 characters. Table 2 shows the total, native, and non-native number of annotators that participated in the annotation task. Datasets We have used the CWIG3G2 datasets from (Yimam et al., 2017b,a) for the complex word identification (CWI) shared task 2018. The datasets are collected for multiple languages (English, German, Spanish). The English datasets cover different text genres, namely News (professionally written news), WikiNews (news written by amateurs), and Wikipedia articles. Below, we will briefly describe the annotation process and the statistics of collected datasets. For detail explanation of the datasets, please refer to the works of Yimam et al. (2017b,a) Furthermore, to bolster the cross-lingual CWI experiment, we have collected a CWI dataset for French. The French d"
W18-0507,yimam-etal-2017-multilingual,1,0.591517,"least 10 native English speakers and 10 non-native English speakers so that it is possible to investigate if native and non-native speakers have different CWI needs. 5) Complex words are not pre-highlighted, as in previous contributions, so that annotators are not biased to the pre-selection of the complex phrases. 6) In addition to single words, we allowed the annotation of multi-word expressions (MWE), up to a size of 50 characters. Table 2 shows the total, native, and non-native number of annotators that participated in the annotation task. Datasets We have used the CWIG3G2 datasets from (Yimam et al., 2017b,a) for the complex word identification (CWI) shared task 2018. The datasets are collected for multiple languages (English, German, Spanish). The English datasets cover different text genres, namely News (professionally written news), WikiNews (news written by amateurs), and Wikipedia articles. Below, we will briefly describe the annotation process and the statistics of collected datasets. For detail explanation of the datasets, please refer to the works of Yimam et al. (2017b,a) Furthermore, to bolster the cross-lingual CWI experiment, we have collected a CWI dataset for French. The French d"
W18-0507,W17-5910,1,0.857961,"Missing"
W18-0507,S16-1155,1,0.729801,"Missing"
W18-0507,S16-1150,0,\N,Missing
W18-1804,2011.mtsummit-papers.35,0,0.210498,"Missing"
W18-1804,W14-3302,1,0.872855,"Missing"
W18-1804,W15-3001,1,0.883251,"Missing"
W18-1804,W14-3340,1,0.893289,"Missing"
W18-1804,P15-1022,1,0.889823,"Missing"
W18-1804,W12-3102,1,0.867471,"Missing"
W18-1804,W17-4773,1,0.851701,"Missing"
W18-1804,W17-4716,1,0.771004,"stimation-task.html Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 28 In terms of methods, early works rely on a statistical phrase-based approach (Simard et al., 2007), possibly integrating source information to reduce systems’ tendency to over-correct and enhance their precision (B´echara et al., 2011; Chatterjee et al., 2015). More recently, large performance gains comparable to those achieved in MT by the adoption of neural approaches have also been observed in APE (Junczys-Dowmunt and Grundkiewicz, 2016), especially with multi-source neural systems (Chatterjee et al., 2017a) enhancing decoding with source language information. In order to evaluate the effects of combining QE with different APE technologies, the experiments discussed in Section 4.3 are performed with both phrase-based (Chatterjee et al., 2016) and neural-based (Junczys-Dowmunt and Grundkiewicz, 2016) architectures. The latter is the winning system at the WMT 2016 APE shared task.3 For a comprehensive overview of the evolution of the APE task and the proposed approaches, we refer the reader to the aforementioned WMT literature. 2.3 Combination of QE and APE So far, the interaction and the possibl"
W18-1804,P15-2026,1,0.764604,"tput are associated with a human-corrected version of the translation. Under this general formulation, previous work concentrated on several aspects, with outcomes that indicate a steady evolution of the approaches. 2 http://www.statmt.org/wmt17/quality-estimation-task.html Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 28 In terms of methods, early works rely on a statistical phrase-based approach (Simard et al., 2007), possibly integrating source information to reduce systems’ tendency to over-correct and enhance their precision (B´echara et al., 2011; Chatterjee et al., 2015). More recently, large performance gains comparable to those achieved in MT by the adoption of neural approaches have also been observed in APE (Junczys-Dowmunt and Grundkiewicz, 2016), especially with multi-source neural systems (Chatterjee et al., 2017a) enhancing decoding with source language information. In order to evaluate the effects of combining QE with different APE technologies, the experiments discussed in Section 4.3 are performed with both phrase-based (Chatterjee et al., 2016) and neural-based (Junczys-Dowmunt and Grundkiewicz, 2016) architectures. The latter is the winning syste"
W18-1804,W17-4775,0,0.212885,"output of an APE system as an extra feature to boost the performance of a neural QE architecture. The intuition is that word-level quality labels can be automatically obtained through TER (Snover et al., 2006) alignments between the translated and the post-edited sentence (used as “pseudo human post-edit”). The resulting APE-based QE system achieves state-of-the-art performance at the word and sentence-level QE tasks on the WMT 2015 and WMT 2016 data sets. Another line of research that is closer to our work has focused on improving APE performance by leveraging word-level QE predictions. In (Hokamp, 2017), this is done by incorporating word-level features as factors in the input (i.e. a concatenation of different word embedding representations) to a neural APE system. By taking a different approach, Chatterjee et al. (2017b) explore a “guided-decoding” mechanism to guide a neural APE system with word-level binary QE judgments. The idea of constraining the decoding process (i.e. forcing the system to keep “good” tokens in the APE output) is the basis for the integration approach described in Section 3.2: “QE as APE guidance”. A third solution for the integration of QE and APE is explored in (Ch"
W18-1804,W16-2378,0,0.778111,"indicate a steady evolution of the approaches. 2 http://www.statmt.org/wmt17/quality-estimation-task.html Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 28 In terms of methods, early works rely on a statistical phrase-based approach (Simard et al., 2007), possibly integrating source information to reduce systems’ tendency to over-correct and enhance their precision (B´echara et al., 2011; Chatterjee et al., 2015). More recently, large performance gains comparable to those achieved in MT by the adoption of neural approaches have also been observed in APE (Junczys-Dowmunt and Grundkiewicz, 2016), especially with multi-source neural systems (Chatterjee et al., 2017a) enhancing decoding with source language information. In order to evaluate the effects of combining QE with different APE technologies, the experiments discussed in Section 4.3 are performed with both phrase-based (Chatterjee et al., 2016) and neural-based (Junczys-Dowmunt and Grundkiewicz, 2016) architectures. The latter is the winning system at the WMT 2016 APE shared task.3 For a comprehensive overview of the evolution of the APE task and the proposed approaches, we refer the reader to the aforementioned WMT literature."
W18-1804,W17-4763,0,0.0128066,"(Turchi et al., 2014; C. de Souza et al., 2015) targeting ﬂexibility and robustness to domain differences between training and test data. In word-level QE, the required predictions can be either binary “good”/“bad” labels for each MT output token, or more ﬁne-grained multi-class labels indicating the type of error occurring in a speciﬁc word. The problem, initially approached as a sequence labelling task (Luong et al., 2013), has then been successfully tackled with neural solutions that now represent the state-of-the-art (C. de Souza et al., 2014; Kreutzer et al., 2015; Martins et al., 2016; Kim et al., 2017). For the experiments discussed in this paper (see Section 4.2) sentence-level and word-level quality estimates are obtained with the winning systems at the WMT 2016 QE shared task, described respectively in (Kozlova et al., 2016) and (Martins et al., 2016). For a comprehensive overview of the evolution of the QE task and the proposed approaches, we refer the reader to the rich WMT literature (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017). 2.2 Automatic Post-editing Automatic post-editing is the task of correcting errors produced by a machine translation system, typi"
W18-1804,P07-2045,0,0.0124647,"Missing"
W18-1804,W16-2385,0,0.0172752,"each MT output token, or more ﬁne-grained multi-class labels indicating the type of error occurring in a speciﬁc word. The problem, initially approached as a sequence labelling task (Luong et al., 2013), has then been successfully tackled with neural solutions that now represent the state-of-the-art (C. de Souza et al., 2014; Kreutzer et al., 2015; Martins et al., 2016; Kim et al., 2017). For the experiments discussed in this paper (see Section 4.2) sentence-level and word-level quality estimates are obtained with the winning systems at the WMT 2016 QE shared task, described respectively in (Kozlova et al., 2016) and (Martins et al., 2016). For a comprehensive overview of the evolution of the QE task and the proposed approaches, we refer the reader to the rich WMT literature (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017). 2.2 Automatic Post-editing Automatic post-editing is the task of correcting errors produced by a machine translation system, typically by exploiting knowledge acquired from human post-edits. APE research dates back to the work of (Knight and Chander, 1994; Simard et al., 2007), in which the problem is approached as a “monolingual translation” task. Similar"
W18-1804,W15-3037,0,0.0221818,"tion of online and multitask learning methods (Turchi et al., 2014; C. de Souza et al., 2015) targeting ﬂexibility and robustness to domain differences between training and test data. In word-level QE, the required predictions can be either binary “good”/“bad” labels for each MT output token, or more ﬁne-grained multi-class labels indicating the type of error occurring in a speciﬁc word. The problem, initially approached as a sequence labelling task (Luong et al., 2013), has then been successfully tackled with neural solutions that now represent the state-of-the-art (C. de Souza et al., 2014; Kreutzer et al., 2015; Martins et al., 2016; Kim et al., 2017). For the experiments discussed in this paper (see Section 4.2) sentence-level and word-level quality estimates are obtained with the winning systems at the WMT 2016 QE shared task, described respectively in (Kozlova et al., 2016) and (Martins et al., 2016). For a comprehensive overview of the evolution of the QE task and the proposed approaches, we refer the reader to the rich WMT literature (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017). 2.2 Automatic Post-editing Automatic post-editing is the task of correcting errors produ"
W18-1804,2013.tc-1.10,1,0.7845,"Missing"
W18-1804,W13-2248,0,0.0309467,"king. Together with the batch learning solutions that characterize the majority of the proposed approaches, recent work also explored the application of online and multitask learning methods (Turchi et al., 2014; C. de Souza et al., 2015) targeting ﬂexibility and robustness to domain differences between training and test data. In word-level QE, the required predictions can be either binary “good”/“bad” labels for each MT output token, or more ﬁne-grained multi-class labels indicating the type of error occurring in a speciﬁc word. The problem, initially approached as a sequence labelling task (Luong et al., 2013), has then been successfully tackled with neural solutions that now represent the state-of-the-art (C. de Souza et al., 2014; Kreutzer et al., 2015; Martins et al., 2016; Kim et al., 2017). For the experiments discussed in this paper (see Section 4.2) sentence-level and word-level quality estimates are obtained with the winning systems at the WMT 2016 QE shared task, described respectively in (Kozlova et al., 2016) and (Martins et al., 2016). For a comprehensive overview of the evolution of the QE task and the proposed approaches, we refer the reader to the rich WMT literature (Callison-Burch"
W18-1804,W16-2387,0,0.113116,"itask learning methods (Turchi et al., 2014; C. de Souza et al., 2015) targeting ﬂexibility and robustness to domain differences between training and test data. In word-level QE, the required predictions can be either binary “good”/“bad” labels for each MT output token, or more ﬁne-grained multi-class labels indicating the type of error occurring in a speciﬁc word. The problem, initially approached as a sequence labelling task (Luong et al., 2013), has then been successfully tackled with neural solutions that now represent the state-of-the-art (C. de Souza et al., 2014; Kreutzer et al., 2015; Martins et al., 2016; Kim et al., 2017). For the experiments discussed in this paper (see Section 4.2) sentence-level and word-level quality estimates are obtained with the winning systems at the WMT 2016 QE shared task, described respectively in (Kozlova et al., 2016) and (Martins et al., 2016). For a comprehensive overview of the evolution of the QE task and the proposed approaches, we refer the reader to the rich WMT literature (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017). 2.2 Automatic Post-editing Automatic post-editing is the task of correcting errors produced by a machine trans"
W18-1804,Q17-1015,0,0.0149624,"m at the WMT 2016 APE shared task.3 For a comprehensive overview of the evolution of the APE task and the proposed approaches, we refer the reader to the aforementioned WMT literature. 2.3 Combination of QE and APE So far, the interaction and the possible joint contribution of QE and APE technology has been scarcely explored. This is particularly surprising if we consider that both the tasks can rely on the type of same training data consisting of (source, MT, post edited MT) parallel triplets, which in principle allow for knowledge transfer and model sharing. Building on this consideration, (Martins et al., 2017) exploited the synergies between the two related tasks by using the output of an APE system as an extra feature to boost the performance of a neural QE architecture. The intuition is that word-level quality labels can be automatically obtained through TER (Snover et al., 2006) alignments between the translated and the post-edited sentence (used as “pseudo human post-edit”). The resulting APE-based QE system achieves state-of-the-art performance at the word and sentence-level QE tasks on the WMT 2015 and WMT 2016 data sets. Another line of research that is closer to our work has focused on impr"
W18-1804,P02-1040,0,0.10271,"Missing"
W18-1804,P16-1162,0,0.0627687,"thors of the winning system at the WMT 2016 APE shared task (Junczys-Dowmunt and Grundkiewicz, 2016) using the large “round-trip translation” dataset, and then adapted to taskspeciﬁc data. The network parameters are: word embedding dimensionality of 600, hidden unit size of 1,024, maximum sentence length of 50, batch size of 80, and vocabulary size of 40K. The network parameters are optimized with Adadelta (Zeiler, 2012). For the guided decoder, the best value step of the look-ahead mechanism is deﬁned on the development set. The data is segmented using the Byte-Pair Encoding (BPE) technique (Sennrich et al., 2016). Each QE word-level annotation is projected to all the subword units. We only use a single MT → pe model instead of an ensemble of models. 5 It is important to note that there are several perfectly valid translations of the same input text, so the gold QE predictions that we use are a subset of possible oracle labels generated based on the available reference sentence. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 32 It is important to note that both the APE systems are strong. In fact, they signiﬁcantly improve over the MT output (respectively +2.64 an"
W18-1804,N07-1064,0,0.864484,"tion, adaptation and evaluation, but also to aspects that are external to the core translation approach. Among them, quality estimation (QE) and automatic post-editing (APE) deal respectively with the possibility of predicting the quality of MT output and correcting it via downstream postprocessing. QE (Specia et al., 2010) is motivated by the need for estimating output quality at Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 26 run-time, that is when reference-based evaluation is unfeasible (the typical scenario when MT is deployed in production). APE (Simard et al., 2007) is motivated by the need of improving MT systems’ output in black-box conditions, in which the translation models are not accessible for internal modiﬁcation, retraining or adaptation (a typical situation for companies that rely on third-party MT systems). Both QE and APE have been successfully explored as standalone tasks in previous work, in particular within the well-established framework of the Conference on Machine Translation (WMT1 ). In six editions of the WMT QE shared task (2012-2017), the MT quality prediction problem has been formulated in different ways (e.g. ranking, scoring) and"
W18-1804,2006.amta-papers.25,0,0.0977762,"QE and APE technology has been scarcely explored. This is particularly surprising if we consider that both the tasks can rely on the type of same training data consisting of (source, MT, post edited MT) parallel triplets, which in principle allow for knowledge transfer and model sharing. Building on this consideration, (Martins et al., 2017) exploited the synergies between the two related tasks by using the output of an APE system as an extra feature to boost the performance of a neural QE architecture. The intuition is that word-level quality labels can be automatically obtained through TER (Snover et al., 2006) alignments between the translated and the post-edited sentence (used as “pseudo human post-edit”). The resulting APE-based QE system achieves state-of-the-art performance at the word and sentence-level QE tasks on the WMT 2015 and WMT 2016 data sets. Another line of research that is closer to our work has focused on improving APE performance by leveraging word-level QE predictions. In (Hokamp, 2017), this is done by incorporating word-level features as factors in the input (i.e. a concatenation of different word embedding representations) to a neural APE system. By taking a different approach"
W18-1804,P14-1067,1,0.894496,"Missing"
W18-5455,P15-2017,0,0.0630583,"sentially attempt to match images from the training set that are most similar to a test image, and generate a caption from the most similar training instances, or generate a ‘novel’ description from a combination of training instances, for example by ‘averaging’ the descriptions. Previous work has alluded to this fact (Karpathy, 2016; Vinyals et al., 2017), but it has not been thoroughly investigated. This phenomenon could also be in part attributed to the fact that the datasets are repetitive and simplistic, with a virtually constant and predictable linguistic structure (Lebret et al., 2015; Devlin et al., 2015; Vinyals et al., 2017). We empirically evaluate end-to-end IC systems where we vary the input image representation but keep the RNN text generation model constant. Our experiment demonstrates that regardless of the image representation (a continuous image embedding or a sparse, low-dimensional vector), end-toend IC systems seem to utilize a visual-semantic subspace for IC. We also analyze various types of image representations and their transformed versions. ∗ This is an abridged version of a recently published BMVC paper (Madhyastha et al., 2018) 1 http://cocodataset.org/ #captions-challenge"
W18-5455,W18-5455,1,0.0513054,"Missing"
W18-6320,2009.mtsummit-posters.5,0,0.123004,"Missing"
W18-6320,W15-4918,1,0.909017,"Missing"
W18-6320,W16-2301,1,0.829153,"Missing"
W18-6320,2001.mtsummit-papers.20,0,0.488483,"Missing"
W18-6320,1999.mtsummit-1.42,0,0.32113,"Missing"
W18-6320,W11-2123,0,0.0202271,"Missing"
W18-6320,E06-1032,0,0.205885,"Missing"
W18-6320,N07-2020,0,0.0997368,"Missing"
W18-6320,L16-1048,0,0.0241495,"Missing"
W18-6320,2014.eamt-1.40,0,0.0438492,"Missing"
W18-6320,2000.tc-1.5,0,0.320841,"Missing"
W18-6320,W15-2402,0,0.0382638,"Missing"
W18-6320,stymne-etal-2012-eye,0,0.038532,"Missing"
W18-6320,P07-2045,1,0.0112762,"Missing"
W18-6320,1993.tmi-1.22,0,0.82749,"Missing"
W18-6320,W11-2401,0,0.033505,"Missing"
W18-6320,2012.freeopmt-1.3,0,0.127684,"Missing"
W18-6320,weiss-ahrenberg-2012-error,0,0.0760185,"Missing"
W18-6320,N16-1125,0,0.0253847,"Missing"
W18-6402,W18-6438,1,0.861948,"Missing"
W18-6402,P11-2031,0,0.0245715,"5e−5 and a batch size of 64. The input embedding dimensionality was set to 128 and the remainder of the hyperparameters were kept as default. Bite-pair encoding with 10,000 merge operations was used for all language pairs. For Task 1b, only the English-Czech portion of the training corpus is used. 4 Automatic Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.5 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 Task 1: English → German Table 6 shows the results on the Test 2018 dataset with"
W18-6402,W18-6439,0,0.0587398,"Missing"
W18-6402,W14-3348,0,0.193691,"ality was set to 128 and the remainder of the hyperparameters were kept as default. Bite-pair encoding with 10,000 merge operations was used for all language pairs. For Task 1b, only the English-Czech portion of the training corpus is used. 4 Automatic Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.5 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 Task 1: English → German Table 6 shows the results on the Test 2018 dataset with a German target language. The first observation is that the best-p"
W18-6402,N13-1073,0,0.0242655,"t is the set of correct lexical translations of aw in the target language that conform to the context i. A word is said to be ambiguous in the source language if it has multiple translations (as given in the Multi30K corpus) with different meanings. We prepared the evaluation dataset following the procedure described in Lala and Specia (2018), with some additional steps. First, the parallel text in the Multi30K training and the validation sets are decompounded with SECOS (Riedl and Biemann, 2016) (for German only) and lemmatised3 . Second, we perform automatic word alignment using fast align (Dyer et al., 2013) to identify the English words that are aligned to two or more different words in the target language. This step results in a dictionary of {key : val} pairs, where key is a potentially ambiguous English word, and val is the set of words in the target language that align to key. This dictionary is then filtered by humans, students of translation studies who are fluent in both the source and target languages, to remove incorrect/noisy alignments and unambiguous instances, resulting in a cleaned dictionary containing {aw : lt} pairs, where aw is an ambiguous English word, and lt is the set of le"
W18-6402,W17-4718,1,0.509969,"using the image itself and its English description. This task can be addressed as either a pure translation task from the source English descriptions (ignoring the corresponding image), or as a multimodal translation task where the translation process is guided by the image in addition to the source description. Initial results in this area showed the potential for visual context to improve translation quality (Elliott et al., 2015; Hitschler et al., 2016). This was followed by a wide range of work in the first two editions of this shared task at the WMT in 2016 and 2017 (Specia et al., 2016; Elliott et al., 2017). This year we challenged participants to target the task of multimodal translation, with two variants: Task 1 is identical to previous editions of the shared task, however, it now includes an additional Czech target language. Therefore, participants can submit translations to any of the following languages: German, French and Czech. This extension means the Multi30K dataset (Elliott et al., 2016) is now 5-way aligned, with images described in English, which are translated into German, French and Czech.1 Task 1b is similar to Task 1; the main difference is that multiple source languages can be"
W18-6402,W16-3210,1,0.812877,"ation quality (Elliott et al., 2015; Hitschler et al., 2016). This was followed by a wide range of work in the first two editions of this shared task at the WMT in 2016 and 2017 (Specia et al., 2016; Elliott et al., 2017). This year we challenged participants to target the task of multimodal translation, with two variants: Task 1 is identical to previous editions of the shared task, however, it now includes an additional Czech target language. Therefore, participants can submit translations to any of the following languages: German, French and Czech. This extension means the Multi30K dataset (Elliott et al., 2016) is now 5-way aligned, with images described in English, which are translated into German, French and Czech.1 Task 1b is similar to Task 1; the main difference is that multiple source languages can be used (simultaneously) and Czech is the only target language. We introduce two new evaluation sets that extend the existing Multi30K dataset: a set of 1071 English sentences and their corresponding images and translations for Task 1, and 1,000 translations for the 2017 test set into Czech for Task 1b. Another new feature of this year’s shared task is the introduction of a new evaluation metric: Le"
W18-6402,I17-1014,1,0.823606,"Missing"
W18-6402,W18-6440,0,0.0330665,"Missing"
W18-6402,W18-6441,0,0.117332,"Missing"
W18-6402,P16-1227,0,0.120596,"ges, all parallel. Introduction The Shared Task on Multimodal Machine Translation tackles the problem of generating a description of an image in a target language using the image itself and its English description. This task can be addressed as either a pure translation task from the source English descriptions (ignoring the corresponding image), or as a multimodal translation task where the translation process is guided by the image in addition to the source description. Initial results in this area showed the potential for visual context to improve translation quality (Elliott et al., 2015; Hitschler et al., 2016). This was followed by a wide range of work in the first two editions of this shared task at the WMT in 2016 and 2017 (Specia et al., 2016; Elliott et al., 2017). This year we challenged participants to target the task of multimodal translation, with two variants: Task 1 is identical to previous editions of the shared task, however, it now includes an additional Czech target language. Therefore, participants can submit translations to any of the following languages: German, French and Czech. This extension means the Multi30K dataset (Elliott et al., 2016) is now 5-way aligned, with images desc"
W18-6402,P18-4020,0,0.0218896,"Missing"
W18-6402,P07-2045,0,0.00543567,"le 4: Statistics of dataset used for the LTA evaluation after human filtering. and their submission identifiers. AFRL-OHIO-STATE (Task 1) The AFRL-OHIO-STATE team builds on their previous year Visual Machine Translation (VMT) submission by combining it with text-only translation models. Two types of models were submitted: AFRL-OHIO-STATE 1 2IMPROVE U is a system combination of the VMT system and an instantiation of a Marian NMT model (Junczys-Dowmunt et al., 2018), and AFRL-OHIOSTATE 1 4COMBO U is a systems combination of the VMT system along with instantiations of Marian, OpenNMT, and Moses (Koehn et al., 2007). CUNI (Task 1) The CUNI submissions use two architectures based on the self-attentive Transformer model (Vaswani et al., 2017). For German and Czech, a language model is used to extract pseudo-in307 ID AFRL-OHIOSTATE CUNI LIUMCVC MeMAD OSU-BAIDU SHEF UMONS Participating team Air Force Research Laboratory & Ohio State University (Gwinnup et al., 2018) Univerzita Karlova v Praze (Helcl et al., 2018) Laboratoire d’Informatique de l’Universit´e du Maine & Universitat Autonoma de Barcelona Computer Vision Center (Caglayan et al., 2018) Aalto University, Helsinki University & EURECOM (Gr¨onroos et"
W18-6402,W18-6442,1,0.871581,"Missing"
W18-6402,L18-1602,1,0.908032,"ment and 2018 test datasets. The figures correspond to tuples with an image and parallel sentences in four languages: English, German, French and Czech. Dataset for LTA representing an instance in the test set, aw is an ambiguous word in English found in that instance i, and clt is the set of correct lexical translations of aw in the target language that conform to the context i. A word is said to be ambiguous in the source language if it has multiple translations (as given in the Multi30K corpus) with different meanings. We prepared the evaluation dataset following the procedure described in Lala and Specia (2018), with some additional steps. First, the parallel text in the Multi30K training and the validation sets are decompounded with SECOS (Riedl and Biemann, 2016) (for German only) and lemmatised3 . Second, we perform automatic word alignment using fast align (Dyer et al., 2013) to identify the English words that are aligned to two or more different words in the target language. This step results in a dictionary of {key : val} pairs, where key is a potentially ambiguous English word, and val is the set of words in the target language that align to key. This dictionary is then filtered by humans, st"
W18-6402,L16-1147,0,0.0341753,"nput image size for convolutional feature extraction process and found that multimodal attention without L2 normalisation performs significantly worse than baseline NMT. MeMAD (Task 1) The MeMAD team adapts the Transformer neural machine translation architecture to a multimodal setting. They use global image features extracted from Detectron (Girshick et al., 2018), a pre-trained object detection and localisation neural network, and two additional training corpora: MS-COCO (Lin et al., 2014) (an English multimodal dataset, which they extend with synthetic multilingual data) and OpenSubtitles (Lison and Tiedemann, 2016) (a multilingual, text-only dataset). Their experiments show that the effect of the visual features in the system is small; the largest differences in quality amongst the systems tested is attributed to the quality of the underlying text-only neural MT system. OSU-BAIDU (Tasks 1 and 1b) For Task 1, the OREGONSTATE system ensembles models including some neural machine translation models which only consider text information and multimodal machine translation models which also consider image information. Both types of models use global attention mechanism to align source to target words. For the"
W18-6402,P02-1040,0,0.10187,"kept as default. Bite-pair encoding with 10,000 merge operations was used for all language pairs. For Task 1b, only the English-Czech portion of the training corpus is used. 4 Automatic Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.5 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 Task 1: English → German Table 6 shows the results on the Test 2018 dataset with a German target language. The first observation is that the best-performing system, MeMAD 1 FLICKR DE MeMAD-OpenNMTmmod U, is sub"
W18-6402,D17-1120,0,0.0202937,"ates are re-ranked using word sense disambiguation (WSD) approaches: (i) most frequency sense (MFS), (ii) lexical translation (LT) and, (iii) multimodal lexical translation (MLT). Models (i) and (ii) are baselines, whilst MLT is a novel multimodal cross-lingual WSD model. The main idea is to have the cross-lingual WSD model select the translation candidate which correctly disambiguates ambiguous words in the source sentence and the intuition is that the image could help in the disambiguation process. The re-ranking cross-lingual WSD models are based on neural sequence learning models for WSD (Raganato et al., 2017; Yuan et al., 2016) trained on the Multimodal Lexical Translation Dataset (Lala and Specia, 2018). More specifically, they train LSTMs as taggers to disambiguate/translate every word in the source sentence. For Task 1b, the SHEF team explores three approaches. The first approach takes the concatenation of the 10-best translation candidates of German-Czech, French-Czech and English-Czech neural MT systems and then re-ranks them using the same multimodal cross-lingual WSD model as in Task 1. The second approach explores consensus between the different 10-best lists. The best hypothesis is selec"
W18-6402,N16-1075,0,0.024076,"taset for LTA representing an instance in the test set, aw is an ambiguous word in English found in that instance i, and clt is the set of correct lexical translations of aw in the target language that conform to the context i. A word is said to be ambiguous in the source language if it has multiple translations (as given in the Multi30K corpus) with different meanings. We prepared the evaluation dataset following the procedure described in Lala and Specia (2018), with some additional steps. First, the parallel text in the Multi30K training and the validation sets are decompounded with SECOS (Riedl and Biemann, 2016) (for German only) and lemmatised3 . Second, we perform automatic word alignment using fast align (Dyer et al., 2013) to identify the English words that are aligned to two or more different words in the target language. This step results in a dictionary of {key : val} pairs, where key is a potentially ambiguous English word, and val is the set of words in the target language that align to key. This dictionary is then filtered by humans, students of translation studies who are fluent in both the source and target languages, to remove incorrect/noisy alignments and unambiguous instances, resulti"
W18-6402,2006.amta-papers.25,0,0.106552,"ing with 10,000 merge operations was used for all language pairs. For Task 1b, only the English-Czech portion of the training corpus is used. 4 Automatic Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.5 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 Task 1: English → German Table 6 shows the results on the Test 2018 dataset with a German target language. The first observation is that the best-performing system, MeMAD 1 FLICKR DE MeMAD-OpenNMTmmod U, is substantially better than other s"
W18-6402,W16-2346,1,0.724118,"Missing"
W18-6402,P14-5003,0,0.0511421,"Missing"
W18-6402,Q14-1006,0,0.277424,"rs, pages 304–323 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64029 the accuracy of a system at translating correctly a subset of ambiguous source language words. Participants could submit both constrained (shared task data only) and unconstrained (any data) systems for both tasks, with a limit of two systems per task variant and language pair per team. 2 Datasets The Multi30K dataset (Elliott et al., 2016) is the primary resource for the shared task. It contains 31K images originally described in English (Young et al., 2014) with two types of multilingual data: a collection of professionally translated German sentences, and a collection of independently crowdsourced German descriptions. Over the two last years, we have extended the Multi30K dataset with 2,071 new images and two additional languages for the translation task: French and Czech. Table 1 presents an overview of the new evaluation datasets. Figure 1 shows an example of an image with an aligned EnglishGerman-French-Czech description. This year we also released a new version of the evaluation datasets featuring a subset of sentences that contain ambiguou"
W18-6402,W18-6443,0,0.0405294,"Missing"
W18-6442,W16-3210,1,0.903985,"Missing"
W18-6442,W16-5307,0,0.0423393,"Missing"
W18-6442,W17-4746,0,0.167179,"Missing"
W18-6442,W16-2358,0,0.0764839,"Missing"
W18-6442,L18-1602,1,0.784592,"to investigate a multimodal, image-based, cross-lingual WSD that predicts the translation candidate which correctly disambiguates ambiguous words in the source sentence. Our baseline NMT system is based on the attentive encoder-decoder (Bahdanau et al., 2015) approach with a Conditional GRU (CGRU) (Cho et al., 2014) decoder and is built using NMTPY toolkit (Caglayan et al., 2017b). Our cross-lingual WSD models are based on neural sequence learning models for WSD (Raganato et al., 2017; Yuan et al., 2016; K˚ageb¨ack and Salomonsson, 2016) applied to the Multimodal Lexical Translation Dataset (Lala and Specia, 2018). For task 1b, we explore three approaches. The first approach concatenates the 10-best translation hypotheses from DE-CS, FR-CS and EN-CS MT systems and then re-ranks them using the imageaware multimodal cross-lingual WSD mentioned earlier (the same way as in Task 1) (Section 3.1.2). The second approach explores the consensus between the different 10-best lists. The best hypothesis is selected according to the number of times it appeared in the different 10-best lists. We followed the order of the n-best lists, meaning that the highest ranked hypothesis with the majority votes was selected. T"
W18-6442,P02-1040,0,0.101237,"e try two different types of classifiers Random Forest and Recurrent Neural Network. Re-ranking using MLT For the re-ranking approach, we first train three baseline EN-CS, DECS and FR-CS NMT models. Given a source sentence in the test set, we generate 10-best translation hypotheses using each of the three models. The three 10-best lists are concatenated to form a list of 30 translation hypotheses. We then use the trained EN-CS MLT model for cross-lingual WSD and perform re-ranking as mentioned in 3.1.2 and 3.1.3. 4 For both tasks, the initial evaluation was performed in terms of METEOR, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), with METEOR as the primary metric. Direct human assessments of translation adequacy will be used for the final evaluation by the task organizers. For task 1, our submitted systems consisted of: a) SHEF LT: re-ranking using LT model; b) SHEF MLT: re-ranking using MLT model; c) SHEF MFS: re-ranking using MFS model; and d) SHEF Baseline: our baseline text-only ensemble NMT model Table 3 shows the official evaluation results of our systems submitted to Task 1 and the baseline system provided by the organizers. For all language pairs, our systems outperform the offic"
W18-6442,W14-3348,0,0.548606,"clala1, p.madhyastha, c.scarton, l.specia}@sheffield.ac.uk Abstract that standard text-only sequence to sequence neural machine translation models (NMT) with attention are able to obtain very high performance. Building on this, for further inspection, we built our own standard NMT systems for EN-DE, ENFR and EN-CS language directions and noticed that the translation hypotheses besides the 1-best output are also of high quality. We made our systems produce 20 translation hypotheses for English descriptions in the validation set and selected the hypothesis with the highest sentencelevel METEOR (Denkowski and Lavie, 2014) score, called the Oracle, and compared this to the 1-best. In this experiment, we observed that the Oracle performs way better (11 to 13.5 METEOR points) than the 1-best output (See Table 1). This preliminary experiment motivated us to investigate re-ranking approaches. This paper describes the University of Sheffield’s submissions to the WMT18 Multimodal Machine Translation shared task. We participated in both tasks 1 and 1b. For task 1, we build on a standard sequence to sequence attention-based neural machine translation system (NMT) and investigate the utility of multimodal re-ranking app"
W18-6442,D17-1120,0,0.134251,"d Hoste, 2013) and a system that performs re-ranking using the Most Frequent Sense (MFS) baseline (Section 3.1.2). Our main goal is to investigate a multimodal, image-based, cross-lingual WSD that predicts the translation candidate which correctly disambiguates ambiguous words in the source sentence. Our baseline NMT system is based on the attentive encoder-decoder (Bahdanau et al., 2015) approach with a Conditional GRU (CGRU) (Cho et al., 2014) decoder and is built using NMTPY toolkit (Caglayan et al., 2017b). Our cross-lingual WSD models are based on neural sequence learning models for WSD (Raganato et al., 2017; Yuan et al., 2016; K˚ageb¨ack and Salomonsson, 2016) applied to the Multimodal Lexical Translation Dataset (Lala and Specia, 2018). For task 1b, we explore three approaches. The first approach concatenates the 10-best translation hypotheses from DE-CS, FR-CS and EN-CS MT systems and then re-ranks them using the imageaware multimodal cross-lingual WSD mentioned earlier (the same way as in Task 1) (Section 3.1.2). The second approach explores the consensus between the different 10-best lists. The best hypothesis is selected according to the number of times it appeared in the different 10-best"
W18-6442,W17-4718,1,0.738829,"task. Task 1 consists in translating source sentences in English that describe an image into German (DE) or French (FR) or Czech (CS), given the image. Task 1b consists in translating source sentences in English that describe an image into Czech, given the image and the French and German translations of the source sentence. This task poses the challenging problem of building models that use both language and image modalities. The dataset for the shared task (Specia et al., 2016) has sentences with simple language constructions and it has been observed by earlier systems (Specia et al., 2016; Elliott et al., 2017) Lang-Pair 1-best Best of 20best (Oracle) Scope/difference (Oracle - 1-best) EN-DE EN-FR EN-CS 48.36 64.91 33.87 61.85 76.87 44.71 +13.49 +11.96 +10.84 Table 1: Motivation for re-ranking. In this preliminary experiment, we observe that re-ranking of the 20-best translation hypotheses generated by a standard NMT model has the potential of improving translation by upto 10.84 to 13.49 METEOR points for the three language pairs. For a re-ranking strategy, we were inspired by how humans use images to translate image descriptions. We believe humans look at the image usually to disambiguate ambiguous"
W18-6442,2006.amta-papers.25,0,0.0636218,"ssifiers Random Forest and Recurrent Neural Network. Re-ranking using MLT For the re-ranking approach, we first train three baseline EN-CS, DECS and FR-CS NMT models. Given a source sentence in the test set, we generate 10-best translation hypotheses using each of the three models. The three 10-best lists are concatenated to form a list of 30 translation hypotheses. We then use the trained EN-CS MLT model for cross-lingual WSD and perform re-ranking as mentioned in 3.1.2 and 3.1.3. 4 For both tasks, the initial evaluation was performed in terms of METEOR, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), with METEOR as the primary metric. Direct human assessments of translation adequacy will be used for the final evaluation by the task organizers. For task 1, our submitted systems consisted of: a) SHEF LT: re-ranking using LT model; b) SHEF MLT: re-ranking using MLT model; c) SHEF MFS: re-ranking using MFS model; and d) SHEF Baseline: our baseline text-only ensemble NMT model Table 3 shows the official evaluation results of our systems submitted to Task 1 and the baseline system provided by the organizers. For all language pairs, our systems outperform the official baseline for all metrics."
W18-6442,W16-2346,1,0.902517,"Missing"
W18-6442,C16-1130,0,0.0172244,"stem that performs re-ranking using the Most Frequent Sense (MFS) baseline (Section 3.1.2). Our main goal is to investigate a multimodal, image-based, cross-lingual WSD that predicts the translation candidate which correctly disambiguates ambiguous words in the source sentence. Our baseline NMT system is based on the attentive encoder-decoder (Bahdanau et al., 2015) approach with a Conditional GRU (CGRU) (Cho et al., 2014) decoder and is built using NMTPY toolkit (Caglayan et al., 2017b). Our cross-lingual WSD models are based on neural sequence learning models for WSD (Raganato et al., 2017; Yuan et al., 2016; K˚ageb¨ack and Salomonsson, 2016) applied to the Multimodal Lexical Translation Dataset (Lala and Specia, 2018). For task 1b, we explore three approaches. The first approach concatenates the 10-best translation hypotheses from DE-CS, FR-CS and EN-CS MT systems and then re-ranks them using the imageaware multimodal cross-lingual WSD mentioned earlier (the same way as in Task 1) (Section 3.1.2). The second approach explores the consensus between the different 10-best lists. The best hypothesis is selected according to the number of times it appeared in the different 10-best lists. We followed"
W18-6451,W18-6458,0,0.0410522,"Missing"
W18-6451,W13-2242,0,0.025847,"suring the closeness of the test sentences to the training data, the difficulty of translating them, and to identify translation acts between any two data sets for building prediction models. Task-specific quality prediction RTM models are built using the WMT News translation task corpora, taking MT models as a black-box and predicting translation scores independently on the MT model. Multiple machine learning techniques are used and averaged based on their training set performance for label prediction. For sequence classification tasks (T2 and T3), Global Linear Models with dynamic learning (Bicici, 2013) are used. via simple arithmetic means on rescaled values, i.e., no machine learning is used. Since it is unsupervised, the method can only be meaningfully evaluated on the ranking task. sMQE uses the same two features as uMQE, but with supervision. A Support Vector Regressor based on these two features is trained on the available data and used to predict QE scores. QEbrain (T1, T2): QE brain uses a conditional target language model as a robust feature extractor with a novel bidirectional transformer which is pretrained on a large parallel corpus filtered to contain “in-domain like” sentences."
W18-6451,aziz-etal-2012-pet,1,0.863758,"Missing"
W18-6451,W18-6457,0,0.051013,"Missing"
W18-6451,W17-4759,0,0.063973,"Missing"
W18-6451,W15-3037,0,0.0394885,"per source word in the phrase as given by the IBM model 1 extracted using the SMT parallel corpus (with different translation probability thresholds: 0.01, 0.05, 0.1, 0.2, 0.5). – average number of translations per source word in the phrase as given by the IBM model 1 extracted using the SMT parallel corpus (with different translation probability thresholds: 0.01, 0.05, 0.1, 0.2, 0.5) weighted by the frequency of each word in the source side of the parallel SMT corpus. In addition to that, six new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • • • • The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool.3 The model was trained using passive-aggressive optimisation algorithm. We note that this baseline system was only used to predict OK/BAD classes for existing words in the MT out"
W18-6451,W12-3102,1,0.84705,"Missing"
W18-6451,W18-6460,0,0.0606221,"Missing"
W18-6451,N13-1073,0,0.0490344,"ing the TERCOM tool. Default settings were used and shifts were disabled. Target tokens originating from insertion or substitution errors were labeled as BAD. All other tokens were labeled as OK. Gap and source word labels To annotate deletion errors, gap ‘tokens’ between each word and at the beginning of each target sentence were introduced. These gaps tokens were labeled as BAD in the presence of one or more deletion errors and OK otherwise. To annotate the source words related to insertion or substitution errors in the machine translated sentence, the IBM Model 2 alignments from fastalign (Dyer et al., 2013) were used. Each token in the source sentence was aligned to the post-edited sentence. For each token in the 12 https://github.com/Unbabel/ word-level-qe-corpus-builder 697 Pearson r SMT DATASET 0.74 • QEBrain DoubleBi w/ BPE+word-tok (ensemble) QEBrain DoubleBi w/ BPE-tok 0.73 UNQE 0.70 TSKQE2 0.49 SHEF-PT 0.49 TSKQE1 0.48 0.43 UTartu/QuEst+Attention UTartu/QuEst+Att+CrEmb3 0.42 sMQE 0.40 RTM MIX7 0.39 RTM MIX6 0.39 SHEF-bRNN 0.37 BASELINE 0.37 uMQE – UAlacant** 0.39 NMT DATASET • UNQE 0.51 • QEBrain DoubleBi w/ BPE+word-tok (ensemble) 0.50 • QEBrain DoubleBi w/ word-tok 0.50 0.42 TSKQE1 TSKQ"
W18-6451,L16-1582,1,0.867516,"rch with 5-fold cross validation on the training set, resulting in γ=0.01,  = 0.0825, C = 20. This baseline system has been consistently used as the baseline system for all editions of the sentence-level task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017), and has proved strong enough for predicting various forms of post-editing effort across a range of language pairs and text domains for statistical MT systems. This year it is also benchmarked on neural MT outputs. Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE, mostly inspired by (Luong et al., 2014). This is the same baseline system used in WMT17: Baseline systems Sentence-level baseline system: For Task 1, Q U E ST ++1 (Specia et al., 2015) was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a"
W18-6451,W18-6461,0,0.0415047,"Missing"
W18-6451,W14-0301,0,0.0184819,"baseline system for all editions of the sentence-level task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017), and has proved strong enough for predicting various forms of post-editing effort across a range of language pairs and text domains for statistical MT systems. This year it is also benchmarked on neural MT outputs. Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE, mostly inspired by (Luong et al., 2014). This is the same baseline system used in WMT17: Baseline systems Sentence-level baseline system: For Task 1, Q U E ST ++1 (Specia et al., 2015) was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. • Target token, its left and right contexts of one wo"
W18-6451,W18-6462,0,0.0661511,"Missing"
W18-6451,W16-2387,1,0.855333,"phrase as given by the IBM model 1 extracted using the SMT parallel corpus (with different translation probability thresholds: 0.01, 0.05, 0.1, 0.2, 0.5). – average number of translations per source word in the phrase as given by the IBM model 1 extracted using the SMT parallel corpus (with different translation probability thresholds: 0.01, 0.05, 0.1, 0.2, 0.5) weighted by the frequency of each word in the source side of the parallel SMT corpus. In addition to that, six new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • • • • The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool.3 The model was trained using passive-aggressive optimisation algorithm. We note that this baseline system was only used to predict OK/BAD classes for existing words in the MT output. No baseline system"
W18-6451,C18-1266,1,0.718016,"Missing"
W18-6451,W18-6463,1,0.879266,"Missing"
W18-6451,W18-6464,0,0.0250119,"Missing"
W18-6451,W17-4763,0,0.0612296,"ter the feature extraction model is trained, the features are extracted and combined with human-crafted features from the QE baseline system and fed into a Bi-LSTM predictive model for QE. A greedy ensemble selection method is used to decrease the individual model errors and increase model diversity. The bi-LSTM QE model is trained on the official QE data plus artificially generated data and fine-tuned with only the official WMT18 QE data. SHEF (T1, T2, T3, T4): SHEF submitted two systems per task variant: SHEF-PT and SHEF-bRNN. SHEFPT is based on a re-implementation of the POSTECH system of (Kim et al., 2017), SHEF-bRNN uses a bidirectional recurrent neural network (bRNN) (Ive et al., 2018a). PT systems are pre-trained using in-domain corpora provided by the organisers. bRNN systems uses two encoders to learn representations of &lt;source, MT> sentence pairs. These representations are used directly to make word-level predictions. A weighted sum over word representations as defined by an attention mechanism is used to make sentence-level predictions. For phrase-level, RTM (T1, T2, T3, T4): 693 a standard attention-based neural MT architecture is used. Different parts of the source sentence are attende"
W18-6451,P15-4020,1,0.85056,"proved strong enough for predicting various forms of post-editing effort across a range of language pairs and text domains for statistical MT systems. This year it is also benchmarked on neural MT outputs. Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE, mostly inspired by (Luong et al., 2014). This is the same baseline system used in WMT17: Baseline systems Sentence-level baseline system: For Task 1, Q U E ST ++1 (Specia et al., 2015) was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. • Target token, its left and right contexts of one word. • Source word aligned to the target token, its left and right contexts of one word. The alignments were given by the SMT system that produced"
W18-6451,W18-6465,0,0.0844531,"Missing"
W18-6451,W18-6466,0,0.0668061,"Missing"
W18-6451,C00-2137,0,0.065581,"ed or substituted in the machine translated text, the corresponding aligned source tokens were labeled as BAD. In this way, deletion errors also result in BAD tokens in the source, related to the missing words. All other words were labeled as OK. Evaluation Analogously to last year’s task, the primary evaluation metric is the multiplication of F1 -scores for the OK and BAD classes, denoted as F1 -Mult. The same metric was applied to gap and source token labels. We also report F1 -scores for individual classes for completeness. We test the significance of the results using randomisation tests (Yeh, 2000) with Bonferroni correction (Abdi, 2007). Task 2: Predicting word-level quality This task evaluates the extent to which we can detect word-level errors in MT output. Often the overall quality of a translated segment is significantly harmed by specific errors in a small number of words. As in previous years, each token of the target sentence is labeled as OK/BAD based on an available post-edited sentence. In addition to this, this year we also took into consideration word omission errors and the detection of words in the source related to target side errors. These types of errors become particu"
W18-6463,E17-1100,0,0.04307,"Missing"
W18-6463,C18-1266,1,0.865593,"Missing"
W18-6463,W17-4763,0,0.149675,"uent but less adequate (Toral and S´anchez-Cartagena, 2017). For tasks 2 and 3, this year’s edition introduces a new task variant of predicting missing words in the translations. Thus two additional prediction types are required: (i) binary labels for gaps in the translation to indicate whether one or more tokens are missing from a certain position, and (ii) 2 Systems Description Our light-weight neural QE approach is based on simple encoders and requires no pre-training (bRNN). We compare its performance to the performance of our re-implementation of the state-ofthe-art neural QE approach of Kim et al. (2017a,b) (POSTECH), which uses a complex architecture and requires resource-intensive pre-training. 2.1 Architecture Following current best practices in neural sequence-to-sequence modelling (Sutskever et al., 2014; Bahdanau et al., 2015), our bRNN approach employs encoders using recurrent neural networks (RNNs). Encoders encode input into an internal representation used to make classification decisions. bRNN representations at a given level rely on representations from more fine-grained levels (i.e. sentences for document, and words for phrase and sentence). bRNN uses two bi-directional RNNs to l"
W18-6463,L16-1582,1,0.832991,"g gaps, a gap tag is placed after each token and in the beginning of the sentence. A gap tag will be BAD if one or more words were expected to appear in the gap, and OK otherwise. Task 2 has 18 variants, for each of them we again submitted two systems: SHEF-PT and SHEF-bRNN. The primary evaluation metric of task 2 is F1MULT: multiplication of F1-scores for the OK and BAD classes. F1-scores of OK and BAD classes are used as secondary metrics. The baseline system for the target word predictions is a Conditional Random Fields (CRF) model trained with word-level baseline features from the Marmot (Logacheva et al., 2016) toolkit. There are no baseline systems for the prediction of gaps or source word issues. Table 2 shows the official results. For prediction of target words, SHEF-PT is the best for ENDE – SMT, EN-LV – SMT and EN-LV – NMT. SHEF-bRNN is the best for EN-DE – NMT. This confirms our previous conclusion that bRNN better 3.3 Task 3: Phrase-level QE This task considers a subset of the EnglishGerman SMT data from task 1 (Section 3.1). Here, the MT output has been manually anno797 SRC PE SMT gold PT bRNN SRC PE NMT gold PT bRNN to make your content accessible to screen readers , avoid using these modes"
W18-6463,2006.amta-papers.25,0,0.0333458,"combinations). Our systems show competitive results and outperform the baseline in nearly all cases. 1 Introduction Quality Estimation (QE) predicts the quality of Machine Translation (MT) when automatic evaluation or human assessment is not possible (typically at system run-time). QE is mainly addressed as a supervised Machine Learning problem with QE models trained using labelled data. These labels differ for different tasks, for example, binary labels for fine-grained predictions (e.g. OK/BAD for words or phrases) and continuous measurements of quality for coarse-grained levels (e.g. HTER (Snover et al., 2006) for sentences). For this year’s shared task, post-edited (PE) and manually annotated data were provided. They cover four levels of predictions: sentence-level (task 1), word-level (task 2), phrase-level (task 3) and document-level (task 4), over five language pairs: English into German, Latvian, Czech and French, as well as German-English. For the first time, these data contain translations produced by neural MT (NMT) systems. Such translations are known to be more fluent but less adequate (Toral and S´anchez-Cartagena, 2017). For tasks 2 and 3, this year’s edition introduces a new task varia"
W18-6463,P15-4020,1,0.839196,"ction is HTER in all of them. For each variant in this task we submitted two systems: SHEF-PT and SHEF-bRNN. For the ranking evaluation, we rank sentences using the predicted HTER outputted by our systems. Following the shared task setup, Pearson’s r correlation coefficient is used as the primary evaluation metric for the scoring task (with Mean Absolute Error – MAE – as the secondary metric), whilst Spearman’s ρ rank correlation coefficient is used as metric for the ranking task. The task baseline systems are Support Vector Machine (SVM) models trained with 17 baseline features from QuEst++ (Specia et al., 2015). We show the official results in Table 1. Both our systems outperform the baseline for all the language pairs according to the main evaluation metric (r). SHEF-bRNN is better than SHEF-PT only for EN-DE – NMT and EN-LV – SMT. These may be cases where bRNN is able to better capture the fluency of high-quality MT by encoding it directly as sequences rather than assessing it word for word as POSTECH. On the official development set,9 EN-DE – NMT and EN-LV – SMT translations have the best overall quality (on average HTER=0.17 versus HTER=0.28 for the rest of the systems). Tasks Participation The"
W19-1808,C16-1112,0,0.060818,"Missing"
W19-1808,W18-6402,1,0.915598,"y recognition. Our approach draws inspiration from neural sequence-based approaches to word sense disambiguation (Raganato et al., 2017; Yuan et al., 2016; K˚ageb¨ack and Salomonsson, 2016) and approaches to ground machine translation (Caglayan et al., 2017). More specifically, we propose and empirically evaluate grounded translation disambiguation models based on recurrent sequential units for the task of MLT. Our primary contributions are: Introduction The multimodal machine translation (MMT) shared task has been conducted for the past three years (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018) with the main goal of investigating the effectiveness of information from images in machine translation (MT). However, as acknowledged in Barrault et al. (2018), it has been difficult to evaluate the impact of multimodality (images) on the sentence-level translation quality, since the changes incurred by having an additional modality can be quite subtle. The MMT shared task consists of translating English sentences that describe an image into a target language given the English sentence itself and the image that it describes. Recently proposed, the multimodal lexical translation (MLT) (Lala a"
W19-1808,W17-4746,0,0.0374894,"Missing"
W19-1808,W16-5307,0,0.0665569,"Missing"
W19-1808,L18-1602,1,0.924704,"2018) with the main goal of investigating the effectiveness of information from images in machine translation (MT). However, as acknowledged in Barrault et al. (2018), it has been difficult to evaluate the impact of multimodality (images) on the sentence-level translation quality, since the changes incurred by having an additional modality can be quite subtle. The MMT shared task consists of translating English sentences that describe an image into a target language given the English sentence itself and the image that it describes. Recently proposed, the multimodal lexical translation (MLT) (Lala and Specia, 2018) is a • An investigation of the MLT dataset to understand the scope and challenges of the task: 78 Proceedings of the Second Workshop on Shortcomings in Vision and Language, pages 78–85 c Minneapolis, USA, June 6, 2019. 2017 Association for Computational Linguistics Sentences Train Val Test Language Pair UA APS APHW TCPA SR 29,000 1,014 1,000 EnDe 745 1.68 15.0 4.1 1.8 1.5 EnFR 661 1.39 12.5 3.0 1.6 1.3 Labels EnDe 49,626 1,775 1,708 Labels EnFr 41,191 1,427 1,298 Table 2: Some key statistics of the original dataset for MLT. UA: Unique Ambiguous words. APS: Ambiguous words Per Sentence. APHW:"
W19-1808,W17-4718,1,0.827076,"tagging or named entity recognition. Our approach draws inspiration from neural sequence-based approaches to word sense disambiguation (Raganato et al., 2017; Yuan et al., 2016; K˚ageb¨ack and Salomonsson, 2016) and approaches to ground machine translation (Caglayan et al., 2017). More specifically, we propose and empirically evaluate grounded translation disambiguation models based on recurrent sequential units for the task of MLT. Our primary contributions are: Introduction The multimodal machine translation (MMT) shared task has been conducted for the past three years (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018) with the main goal of investigating the effectiveness of information from images in machine translation (MT). However, as acknowledged in Barrault et al. (2018), it has been difficult to evaluate the impact of multimodality (images) on the sentence-level translation quality, since the changes incurred by having an additional modality can be quite subtle. The MMT shared task consists of translating English sentences that describe an image into a target language given the English sentence itself and the image that it describes. Recently proposed, the multimodal lexical t"
W19-1808,P16-2067,0,0.0646603,"Missing"
W19-1808,W16-3210,1,0.879777,"Missing"
W19-1808,N16-1022,0,0.0135372,"tags: sentier forêt Figure 1: A labeled example from the dataset for multimodal lexical translation. Only ambiguous words in the sentence are labeled to their corresponding translation in the target language. similar task but focused at the word level and only at ambiguous words. In MLT, the objective is to correctly translate each ambiguous word in the English source sentence into a corresponding word in the target language given the word itself, the English sentence in which it occurs and the image being described by that sentence. This is similar to the task of Visual Sense Disambiguation (Gella et al., 2016) where the objective is to disambiguate the ambiguous verbs using text and image contexts. The authors of MLT proposed to define a word in the source language to be ambiguous if it has multiple translations in the target language with different meanings in the dataset. However, they did not suggest any models for that. In this paper, we propose to treat MLT as a sequence labeling task, as depicted by the example in Figure 1, similar to part-of-speech tagging or named entity recognition. Our approach draws inspiration from neural sequence-based approaches to word sense disambiguation (Raganato"
W19-1808,C16-1330,0,0.0189892,"ote, our definition of Skewness Ratio is similar to the inverse of ‘Average Time-anchored Relative Frequency of Usage’ metric defined in Ilievski et al. (2016) which is used to assess potential bias of meaning dominance with respect to its temporal popularity. The averaged Skewness Ratios for both language pairs, mentioned in Table 2, are much closer to 1 than to their corresponding TCPAs. This implies that the distributions over the translations are highly skewed and suggests that it will be extremely challenging to demonstrate improvements over the MFT because of bias to MFT as indicated in Postma et al. (2016). • An investigation into data settings for the task: we find that models trained to tag all words, irrespective of their ambiguity level, perform better than other settings. • A study on the effect of visual representations for grounded recurrent models: we find that simple unidirectional recurrent models gain more with conditioning of visual information than stronger bidirectional recurrent models. • An investigation on different visual representations for the task: we find that structured image information (in the form of objects) perform better than the popularly used ResNet pool5 image fe"
W19-1808,D17-1120,0,0.118757,"l., 2016) where the objective is to disambiguate the ambiguous verbs using text and image contexts. The authors of MLT proposed to define a word in the source language to be ambiguous if it has multiple translations in the target language with different meanings in the dataset. However, they did not suggest any models for that. In this paper, we propose to treat MLT as a sequence labeling task, as depicted by the example in Figure 1, similar to part-of-speech tagging or named entity recognition. Our approach draws inspiration from neural sequence-based approaches to word sense disambiguation (Raganato et al., 2017; Yuan et al., 2016; K˚ageb¨ack and Salomonsson, 2016) and approaches to ground machine translation (Caglayan et al., 2017). More specifically, we propose and empirically evaluate grounded translation disambiguation models based on recurrent sequential units for the task of MLT. Our primary contributions are: Introduction The multimodal machine translation (MMT) shared task has been conducted for the past three years (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018) with the main goal of investigating the effectiveness of information from images in machine translation (MT). Ho"
W19-1808,W16-2346,1,0.882492,"Missing"
W19-1808,C16-1130,0,0.132538,"ective is to disambiguate the ambiguous verbs using text and image contexts. The authors of MLT proposed to define a word in the source language to be ambiguous if it has multiple translations in the target language with different meanings in the dataset. However, they did not suggest any models for that. In this paper, we propose to treat MLT as a sequence labeling task, as depicted by the example in Figure 1, similar to part-of-speech tagging or named entity recognition. Our approach draws inspiration from neural sequence-based approaches to word sense disambiguation (Raganato et al., 2017; Yuan et al., 2016; K˚ageb¨ack and Salomonsson, 2016) and approaches to ground machine translation (Caglayan et al., 2017). More specifically, we propose and empirically evaluate grounded translation disambiguation models based on recurrent sequential units for the task of MLT. Our primary contributions are: Introduction The multimodal machine translation (MMT) shared task has been conducted for the past three years (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018) with the main goal of investigating the effectiveness of information from images in machine translation (MT). However, as acknowled"
W19-5026,J82-2005,0,0.664569,"Missing"
W19-5026,N18-1204,0,0.0162765,"prompts for story generation, etc. Long text generation. One of the main challenges of the ED architecture remains the generation of long coherent text. In this work, we consider paragraphs as long text. Other NLP tasks may target documents, or even group of documents (e.g., multi-document summarisation systems). Existing vanilla ED models mainly focus on local lexical decisions which limits their ability to model the global integrity of the text. This issue can be tackled by varying the generation conditions: e.g., guiding the generation with prompts (Fan et al., 2018), with named entities (Clark et al., 2018) or template-based generation (Wiseman et al., 2018). All these conditions serve as binding elements to relate generated sentences and ensure the cohesion of the resulting text. In this work, we follow the approach of Peng et al. (2018) and guide the generation of Electronic Health Record (EHR) notes with the help of key phrases (phrases composed of frequent content words often co-occurring with other content words). These key phrases are sense-bearing elements extracted at the paragraph level. Using them as guidance ensures semantic integrity and relevance of the generated text. We experiment"
W19-5026,W04-1013,0,0.0453896,"before, in our attempt to find an optimal way to generate synthetic EHRs we experiment with the Transformer architecture. We extract key phrases at the paragraph level, match them at the sentence level and further use them as inputs into our generation model (see Figure 1). Thus, each paragraph is generated sentence by sentence but taking the information ensuring its integrity into account. Figure 1: Our generation methodology to guide the generation with key phrases. The intrinsic evaluation of the generated data is performed with a set of metrics standard for text generation tasks: ROUGE-L (Lin, 2004) and BLEU (Papineni et al., 2002). ROUGE-L measures the n-gram recall, BLEU– the n-gram precision. We also assess the length of the generated text. At the extrinsic evaluation step, we use generated data as training data in a phenotype classifi241 cation task and a temporal relation extraction task. For each task, we experiment with neural models. We compare performance of three models: one trained with real data, one trained using upsampled real data (the real dataset repeated twice) and one built using real data augmented with generated data for real test sets (see Figure 2). Development set"
W19-5026,P02-1040,0,0.112607,"t to find an optimal way to generate synthetic EHRs we experiment with the Transformer architecture. We extract key phrases at the paragraph level, match them at the sentence level and further use them as inputs into our generation model (see Figure 1). Thus, each paragraph is generated sentence by sentence but taking the information ensuring its integrity into account. Figure 1: Our generation methodology to guide the generation with key phrases. The intrinsic evaluation of the generated data is performed with a set of metrics standard for text generation tasks: ROUGE-L (Lin, 2004) and BLEU (Papineni et al., 2002). ROUGE-L measures the n-gram recall, BLEU– the n-gram precision. We also assess the length of the generated text. At the extrinsic evaluation step, we use generated data as training data in a phenotype classifi241 cation task and a temporal relation extraction task. For each task, we experiment with neural models. We compare performance of three models: one trained with real data, one trained using upsampled real data (the real dataset repeated twice) and one built using real data augmented with generated data for real test sets (see Figure 2). Development sets are also real across setups. By"
W19-5026,D18-1045,0,0.0166534,"erformance boosts for datagreedy neural network algorithms. We also demonstrate the usefulness of the generated data for NLP setups where it fully replaces real training data. 1 Introduction 2 Data availability is a major obstacle in the development of more powerful Natural Language Processing (NLP) methods in the biomedical domain. In particular, current state-of-the-art (SOTA) neural techniques used for NLP rely on substantial amounts of training data. In the NLP community, this low-resource problem is typically addressed by generating complementary data artificially (Poncelas et al., 2018; Edunov et al., 2018). This approach is also gaining attention in biomedical NLP. Most of these studies present work on the generation of short text (typically under 20 tokens), given structural information to guide this generation (e.g., chief complaints using basic patient and diagnosis information (Lee, 2018)). Evaluation scenarios for the utility of the artificial text usually involve a single downstream NLP task (typically, text classification). Related Work Natural Language Generation. Natural language generation is an NLP area with a range of applications such as dialogue generation, question-answering, mac"
W19-5026,P18-1082,0,0.0146174,"ng NLP models. source text for MT, story prompts for story generation, etc. Long text generation. One of the main challenges of the ED architecture remains the generation of long coherent text. In this work, we consider paragraphs as long text. Other NLP tasks may target documents, or even group of documents (e.g., multi-document summarisation systems). Existing vanilla ED models mainly focus on local lexical decisions which limits their ability to model the global integrity of the text. This issue can be tackled by varying the generation conditions: e.g., guiding the generation with prompts (Fan et al., 2018), with named entities (Clark et al., 2018) or template-based generation (Wiseman et al., 2018). All these conditions serve as binding elements to relate generated sentences and ensure the cohesion of the resulting text. In this work, we follow the approach of Peng et al. (2018) and guide the generation of Electronic Health Record (EHR) notes with the help of key phrases (phrases composed of frequent content words often co-occurring with other content words). These key phrases are sense-bearing elements extracted at the paragraph level. Using them as guidance ensures semantic integrity and rele"
W19-5026,W18-1505,0,0.0174128,"ents, or even group of documents (e.g., multi-document summarisation systems). Existing vanilla ED models mainly focus on local lexical decisions which limits their ability to model the global integrity of the text. This issue can be tackled by varying the generation conditions: e.g., guiding the generation with prompts (Fan et al., 2018), with named entities (Clark et al., 2018) or template-based generation (Wiseman et al., 2018). All these conditions serve as binding elements to relate generated sentences and ensure the cohesion of the resulting text. In this work, we follow the approach of Peng et al. (2018) and guide the generation of Electronic Health Record (EHR) notes with the help of key phrases (phrases composed of frequent content words often co-occurring with other content words). These key phrases are sense-bearing elements extracted at the paragraph level. Using them as guidance ensures semantic integrity and relevance of the generated text. We experiment with the SOTA ED Transformer model. The model is based on multi-head attention mechanisms. Such mechanisms decide which parts of input and previously generated output are relevant for the next generation decision. Heads are designed to"
W19-5026,D14-1162,0,0.0820762,"3 Models for Phenotype Classification For the phenotype classification task, we train two standard NLP models: test-gen-pheno test-gen-temp 1. Convolutional Neural Network (CNNs) model inspired by (Kim, 2014). The CNN model is built with 3 convolutional layers with window sizes of 3, 4 and 8 respectively. The word embedding dimensionality is 300, both convolution layers have 100 filters. The size of the hidden units of the dense layer is 100. We also use a dropout layer with a probability of 0.5. The network is implemented using the Pytorch toolkit7 with the pre-trained GloVe word embeddings (Pennington et al., 2014). BLEU avg. sent. l (gen./real) 67.74 48.47 40.62 20.91 13.27 / 17.50 18.61 / 16.81 Table 4: Qualitative evaluation and average sentence lengths. concatenation of the last hidden states of both layers goes into the ouput layer. We train our network with the Adam (Kingma and Ba, 2014) optimization algorithm with a batch size of 64 and a learning rate of 0.001. We use again the pre-trained GloVe word embeddings. The classifier is implemented using Pytorch. As for a non-neural model, we use again the NB model as for the phenotype classification task. We cast the task as a binary classification ta"
W19-5026,P18-1240,0,0.256558,"tactic dependencies or rare words (Voita et al., 2019). Usage of artificial data in NLP In MT, artificial data has been successfully used in addition to real data for training ED models. There have also been attempts to build MT models in low-resource conditions only with artificial data (Poncelas et al., 2018). In this work, we investigate the usefulness of the generated data both in the complementary setting and in the full replacement setting. Medical text generation. The generation of medical data destined to help clinicians has been addressed e.g. through generation of imaging reports by Jing et al. (2018); Liu (2018). However, to our knowledge, there have been 3 Methodology As mentioned before, in our attempt to find an optimal way to generate synthetic EHRs we experiment with the Transformer architecture. We extract key phrases at the paragraph level, match them at the sentence level and further use them as inputs into our generation model (see Figure 1). Thus, each paragraph is generated sentence by sentence but taking the information ensuring its integrity into account. Figure 1: Our generation methodology to guide the generation with key phrases. The intrinsic evaluation of the generated d"
W19-5026,D14-1181,0,0.00236352,"ing atorvastatin in addition on a basis . 4 gen real he was started on ibuprofen and his wife back pain was improved . the patient was initially treated with ibuprofen which was stopped after his back pain improved . Table 3: Examples of real and generated text. The underlined text highlights “good” (examples 1 and 3) or “bad” (examples 2 and 4) modifications. All sentences have been paraphrased. 4.3 Models for Phenotype Classification For the phenotype classification task, we train two standard NLP models: test-gen-pheno test-gen-temp 1. Convolutional Neural Network (CNNs) model inspired by (Kim, 2014). The CNN model is built with 3 convolutional layers with window sizes of 3, 4 and 8 respectively. The word embedding dimensionality is 300, both convolution layers have 100 filters. The size of the hidden units of the dense layer is 100. We also use a dropout layer with a probability of 0.5. The network is implemented using the Pytorch toolkit7 with the pre-trained GloVe word embeddings (Pennington et al., 2014). BLEU avg. sent. l (gen./real) 67.74 48.47 40.62 20.91 13.27 / 17.50 18.61 / 16.81 Table 4: Qualitative evaluation and average sentence lengths. concatenation of the last hidden state"
W19-5026,P17-2035,0,0.0392395,"Missing"
W19-5026,P19-1580,0,0.01644,"are sense-bearing elements extracted at the paragraph level. Using them as guidance ensures semantic integrity and relevance of the generated text. We experiment with the SOTA ED Transformer model. The model is based on multi-head attention mechanisms. Such mechanisms decide which parts of input and previously generated output are relevant for the next generation decision. Heads are designed to attend to information from different representation subspaces. Recent studies show that their roles are potentially linguistically intepretable: e.g., attending to syntactic dependencies or rare words (Voita et al., 2019). Usage of artificial data in NLP In MT, artificial data has been successfully used in addition to real data for training ED models. There have also been attempts to build MT models in low-resource conditions only with artificial data (Poncelas et al., 2018). In this work, we investigate the usefulness of the generated data both in the complementary setting and in the full replacement setting. Medical text generation. The generation of medical data destined to help clinicians has been addressed e.g. through generation of imaging reports by Jing et al. (2018); Liu (2018). However, to our knowle"
W19-5026,D18-1356,0,0.0243216,"ration. One of the main challenges of the ED architecture remains the generation of long coherent text. In this work, we consider paragraphs as long text. Other NLP tasks may target documents, or even group of documents (e.g., multi-document summarisation systems). Existing vanilla ED models mainly focus on local lexical decisions which limits their ability to model the global integrity of the text. This issue can be tackled by varying the generation conditions: e.g., guiding the generation with prompts (Fan et al., 2018), with named entities (Clark et al., 2018) or template-based generation (Wiseman et al., 2018). All these conditions serve as binding elements to relate generated sentences and ensure the cohesion of the resulting text. In this work, we follow the approach of Peng et al. (2018) and guide the generation of Electronic Health Record (EHR) notes with the help of key phrases (phrases composed of frequent content words often co-occurring with other content words). These key phrases are sense-bearing elements extracted at the paragraph level. Using them as guidance ensures semantic integrity and relevance of the generated text. We experiment with the SOTA ED Transformer model. The model is ba"
W19-5026,Q14-1012,0,\N,Missing
W19-5026,P17-4012,0,\N,Missing
W19-5324,P02-1040,0,0.104475,"y. The preprocessing methods include Chinese word segmentation, tokenization, data filtering based on rules and Byte Pair Encoding (BPE). Our baseline model is based on RNNSearch (Bahdanau et al., 2015) operating on word level and we use Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) as encoder and decoder. For character level word embeddings, we use the CharacterEnhanced Word Embedding (CWE) proposed by Chen et al. (2015). For the sub-character level embeddings, we use the Joint Learning Word Embedding (JWE) proposed by Yu et al. (2017). We use various metrics, namely BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006) and CharacTER (Wang et al., 2016) for evaluation. When compared with our baseline model, the models with pre-trained sub-character level embeddings on monolingual corpus show better performance, achieving an increase of +0.53 BLEU score with the sub-character level embeddings. We ran additional experiments on the character and subcharacter level pre-trained embeddings and found that the use of these embeddings can benefit the model when the training corpus size is limited. This paper is structured as follows: Section 2 introduces"
W19-5324,W11-2107,0,0.032921,"lude Chinese word segmentation, tokenization, data filtering based on rules and Byte Pair Encoding (BPE). Our baseline model is based on RNNSearch (Bahdanau et al., 2015) operating on word level and we use Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) as encoder and decoder. For character level word embeddings, we use the CharacterEnhanced Word Embedding (CWE) proposed by Chen et al. (2015). For the sub-character level embeddings, we use the Joint Learning Word Embedding (JWE) proposed by Yu et al. (2017). We use various metrics, namely BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006) and CharacTER (Wang et al., 2016) for evaluation. When compared with our baseline model, the models with pre-trained sub-character level embeddings on monolingual corpus show better performance, achieving an increase of +0.53 BLEU score with the sub-character level embeddings. We ran additional experiments on the character and subcharacter level pre-trained embeddings and found that the use of these embeddings can benefit the model when the training corpus size is limited. This paper is structured as follows: Section 2 introduces the related work including the model"
W19-5324,P16-1009,0,0.176237,"ken at current decoding time step. The context vector is calculated as a weighted average of all encoder hidden state vector, where the attention score is the weight. With the introduction of attention, the model does not need to rely on a single context vector to represent the whole sentence and thus can better handle long sentences. In recent years model architectures based on convolutional neural networks (Gehring et al., 2017) and transformers (Vaswani et al., 2017) have shown competitive or better performance than RNN-based architectures. In addition, strategies such as back translation (Sennrich et al., 2016a), reranking (Neubig et al., 2015) and model ensembling have led to improvements in translation quality. In our experiments, we only experiment with RNN architectures and focus on the effect of using character and sub-character level embeddings and only use ensembling for comparison purposes. We use the CWE model proposed by Chen et al. (2015) and the JWE model proposed by Yu et al. (2017) for pre-trained embeddings training. Both models are based on the word2vec proposed by Mikolov et al. (2013). Based on Continuous-Bagof-Word (CBOW), the CWE model construct a new word representation by summ"
W19-5324,W18-6412,0,0.0174238,"ra. In addition, the Common Crawl Corpus from WMT is used as monolingual data to pre-train the embeddings. We use the newsdev2018 and newsdev2017 as validation set 250 and the newstest2019 as our test data. We tokenize English sentences with the Moses tokenizer (Koehn et al., 2007). On the Chinese side we use Jieba for Chinese word segmentation.1 The data preprocessing consists of filtering sentences to be added to the parallel training corpus by rules and by alignment score. Following the preprocessing criteria from submissions in previous years (Xu and Carpuat, 2018; Stahlberg et al., 2018; Haddow et al., 2018), we filter the training data based on the following criteria: 2015). Our models are built with OpenNMT-py (Klein et al., 2017). We follow the hyperparameter setting of Deep RNN from Xu and Carpuat (2018) and use a four-layer LSTM for both the encoder and decoder. The embeddings and hidden layer size are limited to 512. We use the Adam optimizer (Kingma and Ba, 2015) with initial learning rate of 0.0005. We apply label smoothing (Szegedy et al., 2016) and dropout (Srivastava et al., 2014) of 0.1 to avoid overfitting. We use the multi-layer perception (mlp) attention as in (Bahdanau et al., 201"
W19-5324,P16-1162,0,0.202269,"ken at current decoding time step. The context vector is calculated as a weighted average of all encoder hidden state vector, where the attention score is the weight. With the introduction of attention, the model does not need to rely on a single context vector to represent the whole sentence and thus can better handle long sentences. In recent years model architectures based on convolutional neural networks (Gehring et al., 2017) and transformers (Vaswani et al., 2017) have shown competitive or better performance than RNN-based architectures. In addition, strategies such as back translation (Sennrich et al., 2016a), reranking (Neubig et al., 2015) and model ensembling have led to improvements in translation quality. In our experiments, we only experiment with RNN architectures and focus on the effect of using character and sub-character level embeddings and only use ensembling for comparison purposes. We use the CWE model proposed by Chen et al. (2015) and the JWE model proposed by Yu et al. (2017) for pre-trained embeddings training. Both models are based on the word2vec proposed by Mikolov et al. (2013). Based on Continuous-Bagof-Word (CBOW), the CWE model construct a new word representation by summ"
W19-5324,2006.amta-papers.25,0,0.115885,"okenization, data filtering based on rules and Byte Pair Encoding (BPE). Our baseline model is based on RNNSearch (Bahdanau et al., 2015) operating on word level and we use Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) as encoder and decoder. For character level word embeddings, we use the CharacterEnhanced Word Embedding (CWE) proposed by Chen et al. (2015). For the sub-character level embeddings, we use the Joint Learning Word Embedding (JWE) proposed by Yu et al. (2017). We use various metrics, namely BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006) and CharacTER (Wang et al., 2016) for evaluation. When compared with our baseline model, the models with pre-trained sub-character level embeddings on monolingual corpus show better performance, achieving an increase of +0.53 BLEU score with the sub-character level embeddings. We ran additional experiments on the character and subcharacter level pre-trained embeddings and found that the use of these embeddings can benefit the model when the training corpus size is limited. This paper is structured as follows: Section 2 introduces the related work including the model architecture and pre-train"
W19-5324,P17-4012,0,0.0140003,"8 and newsdev2017 as validation set 250 and the newstest2019 as our test data. We tokenize English sentences with the Moses tokenizer (Koehn et al., 2007). On the Chinese side we use Jieba for Chinese word segmentation.1 The data preprocessing consists of filtering sentences to be added to the parallel training corpus by rules and by alignment score. Following the preprocessing criteria from submissions in previous years (Xu and Carpuat, 2018; Stahlberg et al., 2018; Haddow et al., 2018), we filter the training data based on the following criteria: 2015). Our models are built with OpenNMT-py (Klein et al., 2017). We follow the hyperparameter setting of Deep RNN from Xu and Carpuat (2018) and use a four-layer LSTM for both the encoder and decoder. The embeddings and hidden layer size are limited to 512. We use the Adam optimizer (Kingma and Ba, 2015) with initial learning rate of 0.0005. We apply label smoothing (Szegedy et al., 2016) and dropout (Srivastava et al., 2014) of 0.1 to avoid overfitting. We use the multi-layer perception (mlp) attention as in (Bahdanau et al., 2015). The batch size is 4096 tokens per batch and the models are selected based on best performance on the validation set. All ou"
W19-5324,P07-2045,0,0.0117603,"and ci+1 represent characters in context words. si−1 and si+1 represent sub-characters of context characters and si is the sub-character of target word wi . 3 Data and Preprocessing We use all the parallel data provided by WMT for the zh-en translation task, including the News Commentary v14, UN Parallel Corpus V1.0 and the CWMT corpora. In addition, the Common Crawl Corpus from WMT is used as monolingual data to pre-train the embeddings. We use the newsdev2018 and newsdev2017 as validation set 250 and the newstest2019 as our test data. We tokenize English sentences with the Moses tokenizer (Koehn et al., 2007). On the Chinese side we use Jieba for Chinese word segmentation.1 The data preprocessing consists of filtering sentences to be added to the parallel training corpus by rules and by alignment score. Following the preprocessing criteria from submissions in previous years (Xu and Carpuat, 2018; Stahlberg et al., 2018; Haddow et al., 2018), we filter the training data based on the following criteria: 2015). Our models are built with OpenNMT-py (Klein et al., 2017). We follow the hyperparameter setting of Deep RNN from Xu and Carpuat (2018) and use a four-layer LSTM for both the encoder and decode"
W19-5324,W18-6427,0,0.0348049,"Missing"
W19-5324,D15-1166,0,0.00863543,"beneficial for model performance marginally when the training data is limited. 1 Lucia Specia Department of Computing Imperial College London, UK l.specia@imperial.ac.uk Introduction Neural Machine Translation (NMT) systems are mostly based on an encoder-decoder architecture with attention. Given a sentence x in source language, the model predicts a corresponding output sentence y in target language, which maximizes the conditional probability p(y|x). The attentionbased Recurrent Neural Network (RNN) version of this architecture has been a very popular approach to NMT (Bahdanau et al., 2015; Luong et al., 2015). Despite the success of these models, they still suffer from problems such as outof-vocabulary (OOV) words, i.e. words that have not been seen at training. To alleviate the OOV problem, we follow the methods used in word representation and segment words into smaller units. In some morphorlogically rich languages such as Chinese, a word can be divided into characters and then the characters can be further divided into smaller components called glyphs. Both character and glyph might contain semantic information and therefore utilizing such information might help alleviate the OOV problem. Based"
W19-5324,W15-5003,0,0.0158922,"he context vector is calculated as a weighted average of all encoder hidden state vector, where the attention score is the weight. With the introduction of attention, the model does not need to rely on a single context vector to represent the whole sentence and thus can better handle long sentences. In recent years model architectures based on convolutional neural networks (Gehring et al., 2017) and transformers (Vaswani et al., 2017) have shown competitive or better performance than RNN-based architectures. In addition, strategies such as back translation (Sennrich et al., 2016a), reranking (Neubig et al., 2015) and model ensembling have led to improvements in translation quality. In our experiments, we only experiment with RNN architectures and focus on the effect of using character and sub-character level embeddings and only use ensembling for comparison purposes. We use the CWE model proposed by Chen et al. (2015) and the JWE model proposed by Yu et al. (2017) for pre-trained embeddings training. Both models are based on the word2vec proposed by Mikolov et al. (2013). Based on Continuous-Bagof-Word (CBOW), the CWE model construct a new word representation by summing the word embeddings with charac"
W19-5324,W16-2342,0,0.0153199,"rules and Byte Pair Encoding (BPE). Our baseline model is based on RNNSearch (Bahdanau et al., 2015) operating on word level and we use Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) as encoder and decoder. For character level word embeddings, we use the CharacterEnhanced Word Embedding (CWE) proposed by Chen et al. (2015). For the sub-character level embeddings, we use the Joint Learning Word Embedding (JWE) proposed by Yu et al. (2017). We use various metrics, namely BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006) and CharacTER (Wang et al., 2016) for evaluation. When compared with our baseline model, the models with pre-trained sub-character level embeddings on monolingual corpus show better performance, achieving an increase of +0.53 BLEU score with the sub-character level embeddings. We ran additional experiments on the character and subcharacter level pre-trained embeddings and found that the use of these embeddings can benefit the model when the training corpus size is limited. This paper is structured as follows: Section 2 introduces the related work including the model architecture and pre-trained embeddings used in our experime"
W19-5324,W18-6431,0,0.0625754,"14, UN Parallel Corpus V1.0 and the CWMT corpora. In addition, the Common Crawl Corpus from WMT is used as monolingual data to pre-train the embeddings. We use the newsdev2018 and newsdev2017 as validation set 250 and the newstest2019 as our test data. We tokenize English sentences with the Moses tokenizer (Koehn et al., 2007). On the Chinese side we use Jieba for Chinese word segmentation.1 The data preprocessing consists of filtering sentences to be added to the parallel training corpus by rules and by alignment score. Following the preprocessing criteria from submissions in previous years (Xu and Carpuat, 2018; Stahlberg et al., 2018; Haddow et al., 2018), we filter the training data based on the following criteria: 2015). Our models are built with OpenNMT-py (Klein et al., 2017). We follow the hyperparameter setting of Deep RNN from Xu and Carpuat (2018) and use a four-layer LSTM for both the encoder and decoder. The embeddings and hidden layer size are limited to 512. We use the Adam optimizer (Kingma and Ba, 2015) with initial learning rate of 0.0005. We apply label smoothing (Szegedy et al., 2016) and dropout (Srivastava et al., 2014) of 0.1 to avoid overfitting. We use the multi-layer percepti"
W19-5324,D17-1027,0,0.213037,"by character and sub-character information respectively. The preprocessing methods include Chinese word segmentation, tokenization, data filtering based on rules and Byte Pair Encoding (BPE). Our baseline model is based on RNNSearch (Bahdanau et al., 2015) operating on word level and we use Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) as encoder and decoder. For character level word embeddings, we use the CharacterEnhanced Word Embedding (CWE) proposed by Chen et al. (2015). For the sub-character level embeddings, we use the Joint Learning Word Embedding (JWE) proposed by Yu et al. (2017). We use various metrics, namely BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006) and CharacTER (Wang et al., 2016) for evaluation. When compared with our baseline model, the models with pre-trained sub-character level embeddings on monolingual corpus show better performance, achieving an increase of +0.53 BLEU score with the sub-character level embeddings. We ran additional experiments on the character and subcharacter level pre-trained embeddings and found that the use of these embeddings can benefit the model when the training corpus size is limite"
W19-5356,W05-0909,0,0.879854,"against other types of metrics. 1 Introduction Current metrics to automatically evaluate machine translations, such as the popular BLEU (Papineni et al., 2002), are heavily based on string matching. They claim to account for adequacy by checking for overlapping words between the machine translation output and reference translation, and fluency by rewarding matches in sequences of more than one word. This way of viewing adequacy is very limiting; comparing strings makes it harder to evaluate any deviation from the semantics of the original text in the reference or machine translation. Meteor (Banerjee and Lavie, 2005) relaxes this constraint by allowing matching of lemmas, synonyms or paraphrases. However, this requires linguistic resources to lemmatise the data or lexical databases to fetch synonyms/paraphrases, which do not exist for most languages. Character-based metrics like chrF (Popovic, 2015) and CharacTER (Wang et al., 2016) also relax the exact word match constraint by allowing the matching of characters. However, they ultimately still assume a surface-level similarity between reference and machine translation output. 494 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2"
W19-5356,W17-4755,0,0.0317819,"bserve that, in most cases, the optimal weight seems to be 0.2. 3 then spoke loudly Figure 1: An example of chunks. This type of word order penalty is necessary to deal with examples such as that of Figure 2. The sentence gets a perfect WMD score because all of its words align exactly to another one in the vector space, with no regard to its fluency. With a fragmentation penalty, this type of situation would see the score get worse because of its different sentence structure to the reference. the sun is 3.1 is Datasets We used the WMT17 segment-level into-English datasets for our experiments (Bojar et al., 2017). This has data from seven different source languages, with 560 different texts each. Every text carries a reference translation and a machine translation, with a human annotation labelling how closely the machine translation relates to the reference. shining brightly brightly shining Experimental settings We performed experiments to verify the performance of the proposed metric, comparing the metric’s results against human annotations to measure a level of correlation. We used the PyEMD wrapper (Mayner, 2019) for calculating the WMD, based on (Pele and Werman, 2008, 2009). We did not remove a"
W19-5356,P15-2025,0,0.0423023,"Missing"
W19-5356,W15-3049,0,0.0958995,"n output and reference translation, and fluency by rewarding matches in sequences of more than one word. This way of viewing adequacy is very limiting; comparing strings makes it harder to evaluate any deviation from the semantics of the original text in the reference or machine translation. Meteor (Banerjee and Lavie, 2005) relaxes this constraint by allowing matching of lemmas, synonyms or paraphrases. However, this requires linguistic resources to lemmatise the data or lexical databases to fetch synonyms/paraphrases, which do not exist for most languages. Character-based metrics like chrF (Popovic, 2015) and CharacTER (Wang et al., 2016) also relax the exact word match constraint by allowing the matching of characters. However, they ultimately still assume a surface-level similarity between reference and machine translation output. 494 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 494–500 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics Tjb , as the measure of word dissimilarity, using the Euclidean distance between the embeddings corresponding to words. More precisely, the cost associated is de"
W19-5356,N19-1186,0,0.445746,"Missing"
W19-5356,C16-1110,0,0.0216938,"ia Specia Department of Computing Imperial College London, UK {julian.chow16,pranava,l.specia}@imperial.ac.uk Abstract Chen and Guo (2015) presented a number of experiments where both translation and reference sentences are compared in the embedding space rather than at surface level. They however simply extract these two embedding representations and measure the (cosine) similarity between them, which may account for some overall semantic similarity, but ignores other aspects of translation quality. A version of Meteor has been proposed that also performs matches at the word embedding space (Servan et al., 2016). Two words are considered to match if their cosine distance in the embedding space is above a certain threshold. In other words, the embeddings are only used to provide this binary decision, rather than to measure overall semantic distance between two sentences. In a similar vein, bleu2vec and ngram2vec (Tttar and Fishel, 2017) are a direct modification of BLEU where fuzzy matches are added to strict matches. The fuzzy match score is implemented via token and n-gram embedding similarities. As we show in Section 4, these metrics do not perform well. MEANT 2.0 (Lo, 2017) also relies on matching"
W19-5356,W17-4771,0,0.109598,"edding representations and measure the (cosine) similarity between them, which may account for some overall semantic similarity, but ignores other aspects of translation quality. A version of Meteor has been proposed that also performs matches at the word embedding space (Servan et al., 2016). Two words are considered to match if their cosine distance in the embedding space is above a certain threshold. In other words, the embeddings are only used to provide this binary decision, rather than to measure overall semantic distance between two sentences. In a similar vein, bleu2vec and ngram2vec (Tttar and Fishel, 2017) are a direct modification of BLEU where fuzzy matches are added to strict matches. The fuzzy match score is implemented via token and n-gram embedding similarities. As we show in Section 4, these metrics do not perform well. MEANT 2.0 (Lo, 2017) also relies on matching of words in the embedding space, but this is only used to score the similarity between pairs of words that have already been aligned based on their semantic roles, rather than to find the alignments between words. We suggest a more general way of using distributional representations of words, where distance in the semantic spac"
W19-5356,W16-2342,0,0.236039,"tion, and fluency by rewarding matches in sequences of more than one word. This way of viewing adequacy is very limiting; comparing strings makes it harder to evaluate any deviation from the semantics of the original text in the reference or machine translation. Meteor (Banerjee and Lavie, 2005) relaxes this constraint by allowing matching of lemmas, synonyms or paraphrases. However, this requires linguistic resources to lemmatise the data or lexical databases to fetch synonyms/paraphrases, which do not exist for most languages. Character-based metrics like chrF (Popovic, 2015) and CharacTER (Wang et al., 2016) also relax the exact word match constraint by allowing the matching of characters. However, they ultimately still assume a surface-level similarity between reference and machine translation output. 494 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 494–500 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics Tjb , as the measure of word dissimilarity, using the Euclidean distance between the embeddings corresponding to words. More precisely, the cost associated is defined as: Adjustments to EMD have"
W19-5356,W17-4767,0,0.0902944,"dding space (Servan et al., 2016). Two words are considered to match if their cosine distance in the embedding space is above a certain threshold. In other words, the embeddings are only used to provide this binary decision, rather than to measure overall semantic distance between two sentences. In a similar vein, bleu2vec and ngram2vec (Tttar and Fishel, 2017) are a direct modification of BLEU where fuzzy matches are added to strict matches. The fuzzy match score is implemented via token and n-gram embedding similarities. As we show in Section 4, these metrics do not perform well. MEANT 2.0 (Lo, 2017) also relies on matching of words in the embedding space, but this is only used to score the similarity between pairs of words that have already been aligned based on their semantic roles, rather than to find the alignments between words. We suggest a more general way of using distributional representations of words, where distance in the semantic space is viewed as a global decision between the entire machine and reference translations. More specifically, we propose an adaptation of a powerful and flexible metric that operates on the semantic space: Word Mover’s Distance (WMD) (Kusner et al.,"
W19-5356,P19-1654,1,0.763176,"ssociated is defined as: Adjustments to EMD have been used previously to create evaluation metrics based on word embeddings and word positions (Echizen’ya et al., 2019). Likewise, using vector word embeddings as an indicator of similarity and the word embeddings of each text as a distribution, WMD gives the optimal method of transforming the words of one document to the words of another document. WMD does not take word order into account and rather focuses on semantic similarity of word meanings. WMD has been recently used for the evaluation of image captioning models (Kilickaya et al., 2017; Madhyastha et al., 2019). It proved promising for image captioning evaluation, where word order is less relevant. The same image can be described similarly using different word orders as it is constrained by the image itself. We note that in machine translation evaluation, word order is more important, since the order is constrained by that of the source sentence. In this paper, we propose WMDO – an extension to WMD that incorporates word order. We show that this metric outperforms the standard WMD and performs on par or better than most state-ofthe-art evaluation metrics. 2 c(i, j) = kxi − xj k22 , This allows docum"
W19-5356,P02-1040,0,0.104853,"ce between distributions in the semantic vector space. Matching in the semantic space has been investigated for translation evaluation, but the constraints of a translation’s word order have not been fully explored. Building on the Word Mover’s Distance metric and various word embeddings, we introduce a fragmentation penalty to account for fluency of a translation. This word order extension is shown to perform better than standard WMD, with promising results against other types of metrics. 1 Introduction Current metrics to automatically evaluate machine translations, such as the popular BLEU (Papineni et al., 2002), are heavily based on string matching. They claim to account for adequacy by checking for overlapping words between the machine translation output and reference translation, and fluency by rewarding matches in sequences of more than one word. This way of viewing adequacy is very limiting; comparing strings makes it harder to evaluate any deviation from the semantics of the original text in the reference or machine translation. Meteor (Banerjee and Lavie, 2005) relaxes this constraint by allowing matching of lemmas, synonyms or paraphrases. However, this requires linguistic resources to lemmat"
