2010.jeptalnrecital-demonstration.4,zaghouani-etal-2010-adapting,1,0.873192,"Missing"
2020.lrec-1.768,W15-3209,1,0.811673,"the existing work on irony corpora generation. Section 3 presents the data collection process as well as annotation guidelines and challenges. Section 4 elaborates the corpus analysis. We conclude this paper in 1 The dataset can be downloaded at https://www.hbku. edu.qa/en/DAICT Section 5 with some suggestions for future research. 2. Related Work Recent years have witnessed a surge in the availability of corpora for the Arabic language with focus on dialectal Arabic such as the corpora created by (Bouamor et al., 2018; Zaghouani and Charfi, 2018; Maamouri et al., 2010; Zaghouani et al., 2014; Bouamor et al., 2015). Moreover, we observed a growing interest in collecting and processing Arabic user-generated content from social media sources as in the projects discussed in (Rangel et al., 2019a; Rangel et al., 2019b; Atanasova et al., 2018; Barr´on-Cede˜no et al., 2018). Similarly, irony and sarcasm detection has recently drawn a significant attention in computational linguistics as a standard text classification problem (Joshi et al., 2017). Multiple annotated corpora and their discussions have been published for the English language (Davidov et al., 2010; Filatova, 2012; Rajadesingan et al., 2015). Buil"
2020.lrec-1.768,L18-1535,1,0.83627,"witter: DAICT1 . This paper is structured as follows. The following section gives an overview of the existing work on irony corpora generation. Section 3 presents the data collection process as well as annotation guidelines and challenges. Section 4 elaborates the corpus analysis. We conclude this paper in 1 The dataset can be downloaded at https://www.hbku. edu.qa/en/DAICT Section 5 with some suggestions for future research. 2. Related Work Recent years have witnessed a surge in the availability of corpora for the Arabic language with focus on dialectal Arabic such as the corpora created by (Bouamor et al., 2018; Zaghouani and Charfi, 2018; Maamouri et al., 2010; Zaghouani et al., 2014; Bouamor et al., 2015). Moreover, we observed a growing interest in collecting and processing Arabic user-generated content from social media sources as in the projects discussed in (Rangel et al., 2019a; Rangel et al., 2019b; Atanasova et al., 2018; Barr´on-Cede˜no et al., 2018). Similarly, irony and sarcasm detection has recently drawn a significant attention in computational linguistics as a standard text classification problem (Joshi et al., 2017). Multiple annotated corpora and their discussions have been publishe"
2020.lrec-1.768,filatova-2012-irony,0,0.674263,"0; Zaghouani et al., 2014; Bouamor et al., 2015). Moreover, we observed a growing interest in collecting and processing Arabic user-generated content from social media sources as in the projects discussed in (Rangel et al., 2019a; Rangel et al., 2019b; Atanasova et al., 2018; Barr´on-Cede˜no et al., 2018). Similarly, irony and sarcasm detection has recently drawn a significant attention in computational linguistics as a standard text classification problem (Joshi et al., 2017). Multiple annotated corpora and their discussions have been published for the English language (Davidov et al., 2010; Filatova, 2012; Rajadesingan et al., 2015). Building on the approaches proposed in these studies, corpora have been created for other languages, for example, Italian (TWIT` (Cignarella et al., 2017), ironITA (Cignarella et al., TIRO 2018)); French (Karoui et al., 2017a); and Chinese (Tang and Chen, 2014; Lin and Hsieh, 2016). In comparison, fewer studies considered in detail irony detection in Arabic. Although Arabic is the third most spoken language in the world and is widely used online in social media, the only corpus on irony-detection – SOUKHRIA corpus (Karoui et al., 2017b) – remains available for a l"
2020.lrec-1.768,P11-2102,0,0.121512,"Missing"
2020.lrec-1.768,E17-1025,0,0.144954,"9b; Atanasova et al., 2018; Barr´on-Cede˜no et al., 2018). Similarly, irony and sarcasm detection has recently drawn a significant attention in computational linguistics as a standard text classification problem (Joshi et al., 2017). Multiple annotated corpora and their discussions have been published for the English language (Davidov et al., 2010; Filatova, 2012; Rajadesingan et al., 2015). Building on the approaches proposed in these studies, corpora have been created for other languages, for example, Italian (TWIT` (Cignarella et al., 2017), ironITA (Cignarella et al., TIRO 2018)); French (Karoui et al., 2017a); and Chinese (Tang and Chen, 2014; Lin and Hsieh, 2016). In comparison, fewer studies considered in detail irony detection in Arabic. Although Arabic is the third most spoken language in the world and is widely used online in social media, the only corpus on irony-detection – SOUKHRIA corpus (Karoui et al., 2017b) – remains available for a limited group of researchers and has not been released to public yet. In order to build corpora of ironic text, previous studies have used a variety of sources: Twitter, Facebook, Amazon.com, blogs, newspaper sites, etc. Some studies have defined their so"
2020.lrec-1.768,O16-1027,0,0.0130554,". Similarly, irony and sarcasm detection has recently drawn a significant attention in computational linguistics as a standard text classification problem (Joshi et al., 2017). Multiple annotated corpora and their discussions have been published for the English language (Davidov et al., 2010; Filatova, 2012; Rajadesingan et al., 2015). Building on the approaches proposed in these studies, corpora have been created for other languages, for example, Italian (TWIT` (Cignarella et al., 2017), ironITA (Cignarella et al., TIRO 2018)); French (Karoui et al., 2017a); and Chinese (Tang and Chen, 2014; Lin and Hsieh, 2016). In comparison, fewer studies considered in detail irony detection in Arabic. Although Arabic is the third most spoken language in the world and is widely used online in social media, the only corpus on irony-detection – SOUKHRIA corpus (Karoui et al., 2017b) – remains available for a limited group of researchers and has not been released to public yet. In order to build corpora of ironic text, previous studies have used a variety of sources: Twitter, Facebook, Amazon.com, blogs, newspaper sites, etc. Some studies have defined their source by way of focusing on the sites, dedicated to ironic"
2020.lrec-1.768,maamouri-etal-2010-speech,1,0.614155,"ws. The following section gives an overview of the existing work on irony corpora generation. Section 3 presents the data collection process as well as annotation guidelines and challenges. Section 4 elaborates the corpus analysis. We conclude this paper in 1 The dataset can be downloaded at https://www.hbku. edu.qa/en/DAICT Section 5 with some suggestions for future research. 2. Related Work Recent years have witnessed a surge in the availability of corpora for the Arabic language with focus on dialectal Arabic such as the corpora created by (Bouamor et al., 2018; Zaghouani and Charfi, 2018; Maamouri et al., 2010; Zaghouani et al., 2014; Bouamor et al., 2015). Moreover, we observed a growing interest in collecting and processing Arabic user-generated content from social media sources as in the projects discussed in (Rangel et al., 2019a; Rangel et al., 2019b; Atanasova et al., 2018; Barr´on-Cede˜no et al., 2018). Similarly, irony and sarcasm detection has recently drawn a significant attention in computational linguistics as a standard text classification problem (Joshi et al., 2017). Multiple annotated corpora and their discussions have been published for the English language (Davidov et al., 2010; F"
2020.lrec-1.768,W11-1715,0,0.0375256,"oncerns; and on top of that, they have feathers on the top of their caps like kings do! #sarcasm #irony2 While hashtags offer a convenient indication of an author’s stance, researchers highlight an important limitation: some tweets containing #sarcasm are about sarcasm (e.g. “I love #sarcasm”), while the tweets themselves are not sarcastic (Davidov et al., 2010). For this reason, in addition to digital methods of identifying ironic content, researchers have been experimenting with manual annotation of the data. Most published work has relied on manual annotation by crowd workers or linguists. Reyes and Rosso (2011), Filatova (2012), and Walker et al. (2012) employ crowdsourcing platforms and decide whether a text is ironic or not by considering inter-annotator agreement. However, crowd workers are not linguistic specialists and receive no specialized training; as a result, the annotation could be inaccurate. To address this problem, Karoui et al. (2017a) employ professional linguists and researchers in computational linguistics to label a multilingual irony corpus. This approach, while advancing the linguistic accuracy of annotating, brings to the fore the need for a clear definition of an elusive conce"
2020.lrec-1.768,C14-1120,0,0.0196063,"Cede˜no et al., 2018). Similarly, irony and sarcasm detection has recently drawn a significant attention in computational linguistics as a standard text classification problem (Joshi et al., 2017). Multiple annotated corpora and their discussions have been published for the English language (Davidov et al., 2010; Filatova, 2012; Rajadesingan et al., 2015). Building on the approaches proposed in these studies, corpora have been created for other languages, for example, Italian (TWIT` (Cignarella et al., 2017), ironITA (Cignarella et al., TIRO 2018)); French (Karoui et al., 2017a); and Chinese (Tang and Chen, 2014; Lin and Hsieh, 2016). In comparison, fewer studies considered in detail irony detection in Arabic. Although Arabic is the third most spoken language in the world and is widely used online in social media, the only corpus on irony-detection – SOUKHRIA corpus (Karoui et al., 2017b) – remains available for a limited group of researchers and has not been released to public yet. In order to build corpora of ironic text, previous studies have used a variety of sources: Twitter, Facebook, Amazon.com, blogs, newspaper sites, etc. Some studies have defined their source by way of focusing on the sites"
2020.lrec-1.768,walker-etal-2012-corpus,0,0.0307634,"rs on the top of their caps like kings do! #sarcasm #irony2 While hashtags offer a convenient indication of an author’s stance, researchers highlight an important limitation: some tweets containing #sarcasm are about sarcasm (e.g. “I love #sarcasm”), while the tweets themselves are not sarcastic (Davidov et al., 2010). For this reason, in addition to digital methods of identifying ironic content, researchers have been experimenting with manual annotation of the data. Most published work has relied on manual annotation by crowd workers or linguists. Reyes and Rosso (2011), Filatova (2012), and Walker et al. (2012) employ crowdsourcing platforms and decide whether a text is ironic or not by considering inter-annotator agreement. However, crowd workers are not linguistic specialists and receive no specialized training; as a result, the annotation could be inaccurate. To address this problem, Karoui et al. (2017a) employ professional linguists and researchers in computational linguistics to label a multilingual irony corpus. This approach, while advancing the linguistic accuracy of annotating, brings to the fore the need for a clear definition of an elusive concept irony. The complexity of the concept of"
2020.lrec-1.768,L18-1111,1,0.842016,"paper is structured as follows. The following section gives an overview of the existing work on irony corpora generation. Section 3 presents the data collection process as well as annotation guidelines and challenges. Section 4 elaborates the corpus analysis. We conclude this paper in 1 The dataset can be downloaded at https://www.hbku. edu.qa/en/DAICT Section 5 with some suggestions for future research. 2. Related Work Recent years have witnessed a surge in the availability of corpora for the Arabic language with focus on dialectal Arabic such as the corpora created by (Bouamor et al., 2018; Zaghouani and Charfi, 2018; Maamouri et al., 2010; Zaghouani et al., 2014; Bouamor et al., 2015). Moreover, we observed a growing interest in collecting and processing Arabic user-generated content from social media sources as in the projects discussed in (Rangel et al., 2019a; Rangel et al., 2019b; Atanasova et al., 2018; Barr´on-Cede˜no et al., 2018). Similarly, irony and sarcasm detection has recently drawn a significant attention in computational linguistics as a standard text classification problem (Joshi et al., 2017). Multiple annotated corpora and their discussions have been published for the English language ("
2020.lrec-1.768,zaghouani-etal-2014-large,1,0.815334,"on gives an overview of the existing work on irony corpora generation. Section 3 presents the data collection process as well as annotation guidelines and challenges. Section 4 elaborates the corpus analysis. We conclude this paper in 1 The dataset can be downloaded at https://www.hbku. edu.qa/en/DAICT Section 5 with some suggestions for future research. 2. Related Work Recent years have witnessed a surge in the availability of corpora for the Arabic language with focus on dialectal Arabic such as the corpora created by (Bouamor et al., 2018; Zaghouani and Charfi, 2018; Maamouri et al., 2010; Zaghouani et al., 2014; Bouamor et al., 2015). Moreover, we observed a growing interest in collecting and processing Arabic user-generated content from social media sources as in the projects discussed in (Rangel et al., 2019a; Rangel et al., 2019b; Atanasova et al., 2018; Barr´on-Cede˜no et al., 2018). Similarly, irony and sarcasm detection has recently drawn a significant attention in computational linguistics as a standard text classification problem (Joshi et al., 2017). Multiple annotated corpora and their discussions have been published for the English language (Davidov et al., 2010; Filatova, 2012; Rajadesin"
2020.osact-1.4,L18-1535,1,0.843534,"d and transcribed several interviews to enrich our corpus. We used multiple primary and secondary sources to increase the corpus coverage and the credibility of the acquired data (Liaquat, 2016). Finally, to validate and annotate our data into semantic categories, we used a crowdsourcing approach based on volunteers who filled a survey questionnaire to validate our corpus. Recently, the dialectal Arabic has attracted a considerable amount of research given the availability of social media data such as the MADAR project Bouamor et al. (2018) and Habash et al. (2018) and the ARAP-Tweet project (Zaghouani et al. 2018). Khalifa et al. (2016) built a large scale Gulf Arabic lexicon covering various Gulf dialects while Laoudi et al. (2018) created a Morrocan Arabic lexicon of words and idioms. On the other hand, Carmen Berlinches (2019) focused on building a Syrian Arabic idioms corpus. 3.1 Corpus Collection and Annotation The created corpus consists of 1000 colloquial Qatari traditional expressions (single and multi-word expressions). The corpus was mainly collected from various sources such as five printed books, online articles, and eight online sources such as the Mojam2, the AlArab newspaper lexicon3, El"
2020.osact-1.4,maamouri-etal-2010-speech,1,0.744549,"ace during the Oil discovery era. Recently, Georgetown University in Qatar created the “Qatari phrasebook”; a smartphone application that includes 1,500 traditional Qatari words and phrases and explains the meaning of each in English. Recent years have witnessed a surge in the availability of corpora and resources for the Arabic Natural Language Processing, the Modern Standard Arabic (MSA) variety has received most of the attention as presented in the surveys of Rosso (2018) and Zaghouani (2014). There are many parallel and monolingual data collected and annotated such as the Arabic Treebank (Maamouri et al., 2010) and the Arabic Propbank as in Palmer et al. (2008), Diab et al. (2008) and Zaghouani et al (2012). Other corpora focused on building an error annotated corpus or an Arabic diacritized corpus such as in Bouamor et al. (2015) and Zaghouani et al. (2014). Moreover, we observed a growing interest in collecting and processing Arabic usergenerated content from social media sources as in the projects discussed in (Rangel et al., 2019a; Rangel et al., 2019b; Atanasova et al., 2018; Barron-Cedeno et al. (2018). The pilot experiment described in this paper focused only on the Qatari dialects given the"
2020.osact-1.4,pasha-etal-2014-madamira,0,0.0315166,"rstyle, Transportation, Education, Old currency, Animal, Family, Shop, Building, Oil, and Gas. The full listing of the corpus themes is listed in table 5 of the Appendix 1 following the references. The above metadata will be extremly useful to study the Qatari dialects as in most of the the corpus linguistics studies, linguists usually rely on the corpus metadata to answer important research 3.3 The Data Validation Survey Questionnaire To validate and verify the data collected in our corpus, a survey questionnaire was designed and conducted to verify 7 We used MADAMIRA Part of Speech tag set (Pasha et al. 2014) 26 survey questionnaire. In Table 4 of Appendix 1, we listed a sample of the Corpus Entries for the various corpus themes. a sample of the collected expressions and asses the understanding on the idioms and expressions collected by the general population. An online survey questionnaire was created using Google forms and distributed to several Qatari participants. The age range of the survey participants ranged from 18 until 40 years old, and both genders answered the survey. Some participants helped in recruiting more participants from their family and friends circle using the snowball sampli"
2020.osact-1.4,L18-1574,1,0.790463,"me printed books. Moreover, we conducted, recorded and transcribed several interviews to enrich our corpus. We used multiple primary and secondary sources to increase the corpus coverage and the credibility of the acquired data (Liaquat, 2016). Finally, to validate and annotate our data into semantic categories, we used a crowdsourcing approach based on volunteers who filled a survey questionnaire to validate our corpus. Recently, the dialectal Arabic has attracted a considerable amount of research given the availability of social media data such as the MADAR project Bouamor et al. (2018) and Habash et al. (2018) and the ARAP-Tweet project (Zaghouani et al. 2018). Khalifa et al. (2016) built a large scale Gulf Arabic lexicon covering various Gulf dialects while Laoudi et al. (2018) created a Morrocan Arabic lexicon of words and idioms. On the other hand, Carmen Berlinches (2019) focused on building a Syrian Arabic idioms corpus. 3.1 Corpus Collection and Annotation The created corpus consists of 1000 colloquial Qatari traditional expressions (single and multi-word expressions). The corpus was mainly collected from various sources such as five printed books, online articles, and eight online sources su"
2020.osact-1.4,W18-4910,0,0.0178083,"orpus coverage and the credibility of the acquired data (Liaquat, 2016). Finally, to validate and annotate our data into semantic categories, we used a crowdsourcing approach based on volunteers who filled a survey questionnaire to validate our corpus. Recently, the dialectal Arabic has attracted a considerable amount of research given the availability of social media data such as the MADAR project Bouamor et al. (2018) and Habash et al. (2018) and the ARAP-Tweet project (Zaghouani et al. 2018). Khalifa et al. (2016) built a large scale Gulf Arabic lexicon covering various Gulf dialects while Laoudi et al. (2018) created a Morrocan Arabic lexicon of words and idioms. On the other hand, Carmen Berlinches (2019) focused on building a Syrian Arabic idioms corpus. 3.1 Corpus Collection and Annotation The created corpus consists of 1000 colloquial Qatari traditional expressions (single and multi-word expressions). The corpus was mainly collected from various sources such as five printed books, online articles, and eight online sources such as the Mojam2, the AlArab newspaper lexicon3, ElBadi message board4, the AlHewar website5 and the Mufradat online lexicon6. Regarding the Gulf dialects, there are multip"
2020.osact-1.4,zaghouani-etal-2014-large,1,0.895093,"Missing"
2021.findings-emnlp.56,2021.nlp4if-1.9,1,0.705503,"tions. Some of the larger datasets include the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), the ClaimsKG dataset and system (Tchechmedjiev et al., 2019) of 28K claims from eight factchecking organizations, the MultiFC dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), and the 10K claims Truth of Various Shades dataset (Rashkin et al., 2017). There have been also datasets for other languages, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino"
2021.findings-emnlp.56,N19-1216,1,0.795082,"tweets in Arabic, Bulgarian, Dutch, and English, and we are making it freely available to the research community. We further reported a number of evaluation results for all languages using various transformer architectures. Moreover, we performed advanced experiments, including multilingual training, modeling the Twitter context, the use of propagandistic language, and whether the user is likely to be a bot, as well as multitask learning. In future work, we plan to explore multimodality and explainability (Yu et al., 2021). We further want to model the task as a multitask ordinal regression (Baly et al., 2019), as Q2–Q5 are defined on an ordinal scale. Moreover, we would like to put the data and the system in some practical use; in fact, we have already used them to analyze disinformation about COVID-19 in Bulgaria (Nakov et al., 2021a) and Qatar (Nakov et al., 2021b). Finally, the data will be used in a shared task at the CLEF2022 CheckThat! lab; part of it was used for the NLP4IF-2021 shared task (Shaar et al., 2021a). Acknowledgments We thank Akter Fatema, Al-Awthan Ahmed, AlDobashi Hussein, El Messelmani Jana, Fayoumi 6.3 Multitask Learning Sereen, Mohamed Esraa, Ragab Saleh, and Shurafa For th"
2021.findings-emnlp.56,N18-2004,1,0.90491,"Missing"
2021.findings-emnlp.56,2020.acl-main.747,0,0.0346481,"a URL, and the factuality of the website it points to.4 Models Large-scale pretrained Transformer models have achieved state-of-the-art performance for several NLP tasks. We experimented with several such models to evaluate their efficacy under various training scenarios such as, binary vs. multiclass classification, multilingual setup, etc. We used BERT (Devlin et al., 2019) and RoBERTa for English, AraBERT (Antoun et al., 2020) for Arabic, and BERTje (de Vries et al., 2019) for Dutch. We further used multilingual transformers such as (Liu et al., 2019), multilingual BERT (mBERT) and XLM-r (Conneau et al., 2020). Finally, we used static embeddings from FastText (Joulin et al., 2017). 616 4 From http://mediabiasfactcheck.com English Q. Cls. Arabic Maj. FT BT RT Bulgarian Maj. FT ArBT XLM-r Dutch Maj. FT mBT XLM-r Maj. FT BTje XLM-r Binary (Coarse-grained) Q1 Q2 Q3 Q4 Q5 Q6 Q7 2 2 2 2 2 2 2 Avg. 48.7 91.6 96.3 66.7 67.7 86.7 78.3 77.7 89.0 69.3 96.3 83.8 92.1 80.6 76.5 92.1 96.4 85.6 80.6 88.9 85.5 78.6 92.7 96.9 89.0 84.4 90.5 86.1 76.6 84.1 86.5 88.3 83.8 84.0 96.0 90.3 65.9 88.9 77.4 84.2 83.1 96.3 89.0 66.7 89.8 77.4 58.3 95.0 96.5 86.8 70.5 83.2 80.1 84.0 94.7 96.0 87.7 80.5 84.5 81.6 87.6 95.0 96"
2021.findings-emnlp.56,2020.semeval-1.186,1,0.850189,"., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER"
2021.findings-emnlp.56,2020.acl-demos.32,1,0.926725,"., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER"
2021.findings-emnlp.56,S19-2147,0,0.0285024,"s, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar"
2021.findings-emnlp.56,2021.wanlp-1.9,0,0.0367297,"llected tweets by specifying a target language (English, Arabic, Bulgarian, or Dutch), a set of COVID-19 related keywords, as shown in Figure 2, and different time frames: from January 2020 till March 2021. We collected original tweets (no retweets or replies), we removed duplicates using a similarity-based approach (Alam et al., 2021b), and we filtered out tweets with less than five words. Finally, we selected the most frequently liked and retweeted tweets for annotation. COVID-19 Research There are a number of COVID-19 Twitter datasets: some unlabeled (Chen et al., 2020; Banda et al., 2021; Haouari et al., 2021), some automatically labeled with location information (Abdul-Mageed et al., 2021; Qazi et al., 2020), some labeled using distant supervision (Cinelli et al., 2020; Zhou et al., 2020), and some manually annotated (Song et al., 2020; Vidgen et al., 2020; Shahi and Nandini, 2020; Pulido et al., 2020; Dharawat et al., 2020). There is also work on credibility (Cinelli et al., 2020; Pulido et al., 2020; Zhou et al., 2020), racial prejudices and fear (Medford et al., 2020; Vidgen et al., 2020), as well as situational information, e.g., caution and advice (Li et al., 2020), as well as on detecting me"
2021.findings-emnlp.56,2020.nlpcovid19-2.11,0,0.043054,"Abdul-Mageed et al., 2021; Qazi et al., 2020), some labeled using distant supervision (Cinelli et al., 2020; Zhou et al., 2020), and some manually annotated (Song et al., 2020; Vidgen et al., 2020; Shahi and Nandini, 2020; Pulido et al., 2020; Dharawat et al., 2020). There is also work on credibility (Cinelli et al., 2020; Pulido et al., 2020; Zhou et al., 2020), racial prejudices and fear (Medford et al., 2020; Vidgen et al., 2020), as well as situational information, e.g., caution and advice (Li et al., 2020), as well as on detecting mentions and stance with respect to known misconceptions (Hossain et al., 2020). The closest work to ours is that of Song et al. (2020), who collected false and misleading claims about COVID-19 from IFCN Poynter, and annotated them as (1) Public authority, (2) Community spread and impact, (3) Medical advice, selftreatments, and virus effects, (4) Prominent actors, (5) Conspiracies, (6) Virus transmission, (7) Virus Figure 2: The keywords used to collect the tweets. origins and properties, (8) Public reaction, and (9) Vaccines, medical treatments, and tests. These categories partially overlap with ours, but account 3.2 Annotation Task for less perspectives. Moreover, we c"
2021.findings-emnlp.56,N18-5006,1,0.802211,"tweets (they used claims from news, speeches, political debates, community question answering fora, or were just made up by human annotators; RumourEval is a notable exception), targeted factuality only (we cover a number of other issues), were limited to a single language (typically English; except for CLEF), and did not focus on COVID-19. Check-Worthiness Estimation Another relevant research line is on detecting check-worthy claims in political debates using manual annotations (Hassan et al., 2015) or by observing the selection of fact-checkers (Gencheva et al., 2017; Patwari et al., 2017; Jaradat et al., 2018; Vasileva et al., 2019). 3 3.1 Dataset Data Collection We collected tweets by specifying a target language (English, Arabic, Bulgarian, or Dutch), a set of COVID-19 related keywords, as shown in Figure 2, and different time frames: from January 2020 till March 2021. We collected original tweets (no retweets or replies), we removed duplicates using a similarity-based approach (Alam et al., 2021b), and we filtered out tweets with less than five words. Finally, we selected the most frequently liked and retweeted tweets for annotation. COVID-19 Research There are a number of COVID-19 Twitter data"
2021.findings-emnlp.56,E17-2068,0,0.0257945,"le pretrained Transformer models have achieved state-of-the-art performance for several NLP tasks. We experimented with several such models to evaluate their efficacy under various training scenarios such as, binary vs. multiclass classification, multilingual setup, etc. We used BERT (Devlin et al., 2019) and RoBERTa for English, AraBERT (Antoun et al., 2020) for Arabic, and BERTje (de Vries et al., 2019) for Dutch. We further used multilingual transformers such as (Liu et al., 2019), multilingual BERT (mBERT) and XLM-r (Conneau et al., 2020). Finally, we used static embeddings from FastText (Joulin et al., 2017). 616 4 From http://mediabiasfactcheck.com English Q. Cls. Arabic Maj. FT BT RT Bulgarian Maj. FT ArBT XLM-r Dutch Maj. FT mBT XLM-r Maj. FT BTje XLM-r Binary (Coarse-grained) Q1 Q2 Q3 Q4 Q5 Q6 Q7 2 2 2 2 2 2 2 Avg. 48.7 91.6 96.3 66.7 67.7 86.7 78.3 77.7 89.0 69.3 96.3 83.8 92.1 80.6 76.5 92.1 96.4 85.6 80.6 88.9 85.5 78.6 92.7 96.9 89.0 84.4 90.5 86.1 76.6 84.1 86.5 88.3 83.8 84.0 96.0 90.3 65.9 88.9 77.4 84.2 83.1 96.3 89.0 66.7 89.8 77.4 58.3 95.0 96.5 86.8 70.5 83.2 80.1 84.0 94.7 96.0 87.7 80.5 84.5 81.6 87.6 95.0 96.5 88.4 82.9 85.1 81.7 36.5 64.9 62.3 63.9 44.4 84.7 65.6 75.4 75.1 76.9"
2021.findings-emnlp.56,2020.emnlp-demos.2,0,0.0153692,"80.2 69.2 68.3 Finally, we should note the strong performance Avg. 73.3 73.1 60.7 59.8 71.4 71.5 55.3 54.9 of context-free models such as FastText. We believe that it is suitable for the noisy text of Table 6: Multilingual experiments using mBERT. tweets due to its ability to model not only words Shown are results for monolingual vs. multilingual models (weighted F1 ). Mul is trained on the combined but also character n-grams. In future work, we English, Arabic, Bulgarian, and Dutch data. plan to try transformers specifically trained on tweets and/or on COVID-19 related data such as BERTweet (Nguyen et al., 2020) and COVID5 Twitter-BERT (Müller et al., 2020). We also tried XLM-r, but it performed worse. 618 6.2 Twitter/Propagandistic/Botometer We conducted experiments with Twitter, propaganda, and botness features alongside the posteriors from the BERT classifier, which we combined using XGBoost (Chen and Guestrin, 2016). The results are shown in Table 7. We can see that many of the combinations yielded improvements, with botness being the most useful, followed by propaganda, and finally by the Twitter object features. Binary (Coarse-grained) Q. Cls BERT B+TF B+Prop B+Bot B+All Q1 Q2 Q3 Q4 Q5 Q6 Q7 2"
2021.findings-emnlp.56,D17-1317,0,0.0286494,"onversations with a Ministry of Public Health. Our contributions can be summarized as follows: 2 Related Work Fact-Checking Research on fact-checking claims is largely based on datasets mined from major fact-checking organizations. Some of the larger datasets include the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), the ClaimsKG dataset and system (Tchechmedjiev et al., 2019) of 28K claims from eight factchecking organizations, the MultiFC dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), and the 10K claims Truth of Various Shades dataset (Rashkin et al., 2017). There have been also datasets for other languages, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers"
2021.findings-emnlp.56,2021.nlp4if-1.12,1,0.887854,"We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER task on fact ex1 traction and verification, focusing on claims about https://github.com/firojalam/ COVID-19-disinformation Wikipedia content (Thorne et al., 2018, 2019). 612 Unlike our work, the above datasets did not focus on tweets (they used claims from news, speeches, political debates, community question answering fora, or were just made up by human annotators; RumourEval is a notable exception), targeted factuality only (we cover a number of other issues), were limited to a single language (t"
2021.nlp4if-1.12,2020.findings-emnlp.58,0,0.0684544,"Missing"
2021.nlp4if-1.12,2021.wanlp-1.8,0,0.0202452,"her use distant supervision, and very few are manually annotated. Cinelli et al. (2020) studied COVID-19 rumor amplification in five social media platforms; their data was labeled using distant supervision. Other datasets include a multi-lingual dataset of 123M tweets (Chen et al., 2020), another one of 383M tweets (Banda et al., 2020), a billion-scale dataset of 65 languages and 32M geo-tagged tweets (Abdul-Mageed et al., 2021), and the GeoCoV19 dataset, consisting of 524M multilingual tweets, including 491M with GPS coordinates (Qazi et al., 2020). There are also Arabic datasets, both with (Haouari et al., 2021; Mubarak and Hassan, 2021) and without manual annotations (Alqurashi et al., 2020). We are not aware of Bulgarian datasets. Zhou et al. (2020) created the ReCOVery dataset, which combines 2,000 news articles about COVID19, annotated for their factuality, with 140,820 tweets. Vidgen et al. (2020) studied COVID-19 prejudices using a manually labeled dataset of 20K tweets with the following labels: hostile, criticism, prejudice, and neutral. 2.2 Censorship Detection There has been a lot of research aiming at developing strategies to detect and to evade censorship. Most work has focused on exploi"
2021.nlp4if-1.12,2021.nlp4if-1.17,0,0.140699,"0.803 0.669 0.599 0.480 0.819 0.748 0.672 0.631 0.687 0.556 0.399 0.678 0.605 0.606 0.606 0.650 0.303 0.498 0.706 0.686 0.630 0.630 0.700 0.631 0.528 3 4 Table 6: Task 1, Bulgarian: Evaluation results. For Q1 to Q7 results are in terms of weighted F1 score. Team HunterSpeechLab (Panda and Levitan, 2021) participated in all three languages. They explored the cross-lingual generalization ability of multitask models trained from scratch (logistic regression, transformers) and pre-trained models (English BERT, mBERT) for deception detection. They were 2nd for Arabic and Bulgarian. Team iCompass (Henia and Haddad, 2021) had a late submission for Arabic, and would have ranked 2nd. They used contextualized text representations from ARBERT, MARBERT, AraBERT, Arabic ALBERT and BERT-base-arabic, which they fine-tuned on the training data for task 1. They found that BERT-base-arabic performed best. 87 1. 2. 3. 4. 7. Team NARNIA (Kumar et al., 2021) experimented with a number of Deep Learning models, including different word embeddings such as Glove and ELMo, among others. They found that the BERTweet model achieved the best overall F1score of 0.881, securing them the third place on the English subtask. TOKOFOU dun"
2021.nlp4if-1.12,2020.osact-1.2,0,0.0468958,"tasks, similar to the shared task’s topic (e.g., hate speech and sarcasm detection). They fine-tuned each of these models on the task 1 training data, projecting a label from the sequence classification token for each of the seven questions in parallel. After model selection on the basis of development set F1 performance, they combined the models in a majority-class ensemble. Table 3: Task 2: Topics featured in the dataset. 4 Task Organization The Arabic Winner: Team R00 had the best performing system for Arabic. They used an ensemble of the follwoing fine-tuned Arabic transformers: AraBERT (Antoun et al., 2020), AsafayaBERT (Safaya et al., 2020), ARBERT. In addition, they also experimented with MARBERT (AbdulMageed et al., 2020). In this section, we describe the overall task organization, phases, and evaluation measures. 4.1 Task Phases We ran the shared tasks in two phases: Development Phase In the first phase, only training and development data were made available, and no gold labels were provided for the latter. The participants competed against each other to achieve the best performance on the development set. Test Phase In the second phase, the test set (unlabeled input only) was released, and"
2021.nlp4if-1.12,2020.nlpcovid19-2.11,0,0.24614,"racies, (6) Virus transmission, (7) Virus origins and properties, (8) Public reaction, and (9) Vaccines, medical treatments, and tests, and (10) Cannot determine. Another related dataset study by (Pulido et al., 2020) analyzed 1,000 tweets and categorized them based on factuality into the following categories: (i) False information, (ii) Science-based evidence, (iii) Fact-checking tweets, (iv) Mixed information, (v) Facts, (vi) Other, and (vii) Not valid. Ding et al. (2020) have a position paper discussing the challenges in combating the COVID-19 infodemic in terms of data, tools, and ethics. Hossain et al. (2020) developed the COVIDLies dataset by matching a known misconceptions with tweets, and manually annotated the tweets with stance: whether the target tweet agrees, disagrees, or has no position with respect to a known misconception. Finally, (Shuja et al., 2020) provided a comprehensive survey categorizing the COVID-19 literature into four groups: diagonisis related, transmission and mobility, social media analysis, and knowledge-based approaches. The most relevant previous work is (Alam et al., 2021b, 2020), where tweets about COVID-19 in Arabic and English were annotated based on an annotation"
2021.nlp4if-1.12,2021.nlp4if-1.13,0,0.121497,"rformance on the development set. Test Phase In the second phase, the test set (unlabeled input only) was released, and the participants were given a few days to submit their predictions. The Bulgarian Winner: We did not receive a submission for the best performing team for Bulgarian. The second best team, HunterSpeechLab (Panda and Levitan, 2021), explored the crosslingual generalization ability of multitask models trained from scratch (logistic regression, transformer encoder) and pre-trained models (English BERT, and mBERT) for deception detection. 4.2 5.3 Evaluation Measures DamascusTeam (Hussein et al., 2021) used a two-step pipeline, where the first step involves a series of pre-processing procedures to transform Twitter jargon, including emojis and emoticons, into plain text. In the second step, a version of AraBERT is fine-tuned and used to classify the tweets. Their system was ranked 5th for Arabic. The official evaluation measure for task 1 was the average of the weighted F1 scores for each of the seven questions; for task 2, it was accuracy. 5 Evaluation Results for Task 1 Below, we describe the baselines, the evaluation results, and the best systems for each language. 5.1 Team dunder_miffli"
2021.nlp4if-1.12,2021.louhi-1.1,0,0.0248397,"ision, and very few are manually annotated. Cinelli et al. (2020) studied COVID-19 rumor amplification in five social media platforms; their data was labeled using distant supervision. Other datasets include a multi-lingual dataset of 123M tweets (Chen et al., 2020), another one of 383M tweets (Banda et al., 2020), a billion-scale dataset of 65 languages and 32M geo-tagged tweets (Abdul-Mageed et al., 2021), and the GeoCoV19 dataset, consisting of 524M multilingual tweets, including 491M with GPS coordinates (Qazi et al., 2020). There are also Arabic datasets, both with (Haouari et al., 2021; Mubarak and Hassan, 2021) and without manual annotations (Alqurashi et al., 2020). We are not aware of Bulgarian datasets. Zhou et al. (2020) created the ReCOVery dataset, which combines 2,000 news articles about COVID19, annotated for their factuality, with 140,820 tweets. Vidgen et al. (2020) studied COVID-19 prejudices using a manually labeled dataset of 20K tweets with the following labels: hostile, criticism, prejudice, and neutral. 2.2 Censorship Detection There has been a lot of research aiming at developing strategies to detect and to evade censorship. Most work has focused on exploiting technological limitati"
2021.nlp4if-1.12,2021.nlp4if-1.14,0,0.0895043,"Missing"
2021.nlp4if-1.12,W18-4202,1,0.829941,"uld be factchecked, for example “The sky is blue.”, albeit being a claim, is not interesting to the general public and thus should not be fact-checked. Finally, there has been research that uses linguistic and content clues to detect censorship. Knockel et al. (2015) and Zhu et al. (2013) proposed detection mechanisms to categorize censored content and to automatically learn keywords that get censored. Bamman et al. (2012) uncovered a set of politically sensitive keywords and found that the presence of some of them in a Weibo blogpost contributed to a higher chance of the post being censored. Ng et al. (2018b) also targeted a set of topics that had been suggested to be sensitive, but unlike Bamman et al. (2012), they covered areas not limited to politics. Ng et al. (2018b), Ng et al. (2019), and Ng et al. (2020) investigated how the textual content might be relevant to censorship decisions when both censored and uncensored blogposts include the same sensitive keyword(s). 4. Harmfulness: To what extent is the tweet harmful to the society/person(s)/company(s)/product(s)? The purpose of this question is to determine whether the content of the tweet aims to and can negatively affect the society as a"
2021.nlp4if-1.12,W19-2105,1,0.881735,"Missing"
2021.nlp4if-1.12,C18-1283,0,0.0193051,"glish, and we further add an additional language: Bulgarian. In this section, we discuss studies relevant to the COVID-19 infodemic and to censorship detection. 2.1 COVID-19 Infodemic Disinformation, misinformation, and “fake news” thrive in social media. Lazer et al. (2018) and Vosoughi et al. (2018) in Science provided a general discussion on the science of “fake news” and the process of proliferation of true and false news online. There have also been several interesting surveys, e.g., Shu et al. (2017) studied how information is disseminated and consumed in social media. Another survey by Thorne and Vlachos (2018) took a fact-checking perspective on “fake news” and related problems. Yet another survey (Li et al., 2016) covered truth discovery in general. Some very recent surveys focused on stance for misinformation and disinformation detection (Hardalov et al., 2021), on automatic fact-checking to assist human fact-checkers (Nakov et al., 2021a), on predicting the factuality and the bias of entire news outlets (Nakov et al., 2021c), on multimodal disinformation detection (Alam et al., 2021a), and on abusive language in social media (Nakov et al., 2021b). A number of Twitter datasets have been developed"
2021.nlp4if-1.12,2021.nlp4if-1.19,0,0.519343,"measures. 4.1 Task Phases We ran the shared tasks in two phases: Development Phase In the first phase, only training and development data were made available, and no gold labels were provided for the latter. The participants competed against each other to achieve the best performance on the development set. Test Phase In the second phase, the test set (unlabeled input only) was released, and the participants were given a few days to submit their predictions. The Bulgarian Winner: We did not receive a submission for the best performing team for Bulgarian. The second best team, HunterSpeechLab (Panda and Levitan, 2021), explored the crosslingual generalization ability of multitask models trained from scratch (logistic regression, transformer encoder) and pre-trained models (English BERT, and mBERT) for deception detection. 4.2 5.3 Evaluation Measures DamascusTeam (Hussein et al., 2021) used a two-step pipeline, where the first step involves a series of pre-processing procedures to transform Twitter jargon, including emojis and emoticons, into plain text. In the second step, a version of AraBERT is fine-tuned and used to classify the tweets. Their system was ranked 5th for Arabic. The official evaluation mea"
2021.nlp4if-1.12,2021.nlp4if-1.18,0,0.220605,"ples in the training, development and test sets for the three languages. Note that, we have more data for Arabic and Bulgarian than for English. 2 http://freeweibo.com http://weiboscope.jmsc.hku.hk 4 http://open.weibo.com/wiki/API文档/en 3 85 Topic Censored Uncensored cultural revolution human rights family planning censorship & propaganda democracy patriotism China Trump Meng Wanzhou kindergarten abuse 55 53 15 32 119 70 186 320 55 48 60 67 25 54 107 105 194 244 76 5 Total 953 937 Below, we give a brief summary of the best performing systems for each language. The English Winner: Team TOKOFOU (Tziafas et al., 2021) performed best for English. They gathered six BERT-based models pre-trained in relevant domains (e.g., Twitter and COVID-themed data) or fine-tuned on tasks, similar to the shared task’s topic (e.g., hate speech and sarcasm detection). They fine-tuned each of these models on the task 1 training data, projecting a label from the sequence classification token for each of the seven questions in parallel. After model selection on the basis of development set F1 performance, they combined the models in a majority-class ensemble. Table 3: Task 2: Topics featured in the dataset. 4 Task Organization"
2021.nlp4if-1.12,2021.nlp4if-1.20,0,0.0827001,"Missing"
2021.nlp4if-1.12,2021.nlp4if-1.15,0,0.0237019,"Ë Ë   ËË Ë  1 (Tziafas et al., 2021) 2 (Suhane and Kowshik, 2021) 3 (Kumar et al., 2021) 4 (Uyangodage et al., 2021) 7 (Panda and Levitan, 2021) Table 7: Task 1: Overview of the approaches used by the participating systems for English. =part of the official submission; Ë=considered in internal experiments; Trans. is for Transformers; Repres. is for Representations. References to system description papers are shown below the table. Trans. Models Misc BERT multilingual AraBERT Asafaya-BERT ARBERT ALBERT MARBERT Logistic Regression Ranks Team Ensemble Under/Over-Sampling Team R00 (Qarqaz et al., 2021) had the best performing system for the Arabic subtask. They used an ensemble of neural networks combining a linear layer on top of one out of the following four pre-trained Arabic language models: AraBERT, Asafaya-BERT, ARBERT. In addition, they also experimented with MARBERT. Team TOKOFOU (Tziafas et al., 2021) participated in English only and theirs was the winning system for that language. They gathered six BERT-based models pre-trained in relevant domains (e.g., Twitter and COVID-themed data) or fine-tuned on tasks, similar to the shared task’s topic (e.g., hate speech and sarcasm detecti"
2021.nlp4if-1.12,2020.alw-1.19,0,0.0284727,"one of 383M tweets (Banda et al., 2020), a billion-scale dataset of 65 languages and 32M geo-tagged tweets (Abdul-Mageed et al., 2021), and the GeoCoV19 dataset, consisting of 524M multilingual tweets, including 491M with GPS coordinates (Qazi et al., 2020). There are also Arabic datasets, both with (Haouari et al., 2021; Mubarak and Hassan, 2021) and without manual annotations (Alqurashi et al., 2020). We are not aware of Bulgarian datasets. Zhou et al. (2020) created the ReCOVery dataset, which combines 2,000 news articles about COVID19, annotated for their factuality, with 140,820 tweets. Vidgen et al. (2020) studied COVID-19 prejudices using a manually labeled dataset of 20K tweets with the following labels: hostile, criticism, prejudice, and neutral. 2.2 Censorship Detection There has been a lot of research aiming at developing strategies to detect and to evade censorship. Most work has focused on exploiting technological limitations with existing routing protocols (Leberknight et al., 2012; Katti et al., 2005; Levin et al., 2015; Weinberg et al., 2012; Bock et al., 2020). Research that pays more attention to the linguistic properties of online censorship in the context of censorship evasion inc"
2021.nlp4if-1.12,2020.semeval-1.271,0,0.0720189,"s topic (e.g., hate speech and sarcasm detection). They fine-tuned each of these models on the task 1 training data, projecting a label from the sequence classification token for each of the seven questions in parallel. After model selection on the basis of development set F1 performance, they combined the models in a majority-class ensemble. Table 3: Task 2: Topics featured in the dataset. 4 Task Organization The Arabic Winner: Team R00 had the best performing system for Arabic. They used an ensemble of the follwoing fine-tuned Arabic transformers: AraBERT (Antoun et al., 2020), AsafayaBERT (Safaya et al., 2020), ARBERT. In addition, they also experimented with MARBERT (AbdulMageed et al., 2020). In this section, we describe the overall task organization, phases, and evaluation measures. 4.1 Task Phases We ran the shared tasks in two phases: Development Phase In the first phase, only training and development data were made available, and no gold labels were provided for the latter. The participants competed against each other to achieve the best performance on the development set. Test Phase In the second phase, the test set (unlabeled input only) was released, and the participants were given a few d"
2021.nlp4if-1.12,2021.nlp4if-1.16,0,0.208605,"ed a two-step pipeline, where the first step involves a series of pre-processing procedures to transform Twitter jargon, including emojis and emoticons, into plain text. In the second step, a version of AraBERT is fine-tuned and used to classify the tweets. Their system was ranked 5th for Arabic. The official evaluation measure for task 1 was the average of the weighted F1 scores for each of the seven questions; for task 2, it was accuracy. 5 Evaluation Results for Task 1 Below, we describe the baselines, the evaluation results, and the best systems for each language. 5.1 Team dunder_mifflin (Suhane and Kowshik, 2021) built a multi-output model using task-wise multi-head attention for inter-task information aggregation. This was built on top of the representations obtained from RoBERTa. To tackle the small size of the dataset, they used back-translation for data augmentation. Their loss function was weighted for each output, in accordance with the distribution of the labels for that output. They were the runners-up in the English subtask with a mean F1-score of 0.891 on the test set, without the use of any task-specific embeddings or ensembles. Baselines The baselines for Task 1 are (i) majority class, (ii"
2021.wanlp-1.36,2020.lrec-1.768,1,0.702285,"l one (Wilson, 2006). There have been several related works on English sarcasm detection including datasets such as the works reported in (Abercrombie and Hovy, 2016; Barbieri et al., 2014a,b; Filatova, 2012; Ghosh et al., 2015; Joshi et al., 2016; Oprea and Magdy, 2020) and detection systems such as (Rajadesingan et al., 2015; Joshi et al., 2015; Amir et al., 2016). Currently, there are few attempts to work on Arabic sarcasm. Those include the work by soukhria2017, a shared task on irony detection Ghanem et al. (2019) along with the participants’ submissions and dialectal sarcasm datasets by Abbes et al. (2020); Abu Farha and Magdy (2020). In this shared task, we offer our sarcasm and sentiment detection in Arabic task that is co-organised with the WANLP 2021 workshop on Arabic NLP. The goal of the shared task is to provide resources and encourage researchers to work on Arabic sarcasm detection. The shared task has two subtasks, sarcasm detection (subtask 1) and sentiment analysis (subtask 2). We provided the participant with a new dataset (ArSarcasm-v2), which is publicly available1 . The dataset is annotated for sarcasm, sentiment and dialect. We received 27 submissions for subtask 1 and 22 submis"
2021.wanlp-1.36,P11-2103,0,0.453691,"ng and subjective language analysis has been prominent in the natural language processing (NLP) field during the last two decades. One of the main tasks in this area is sentiment analysis (SA). One of the early works on SA is (Pang et al., 2002), where the authors analysed the sentiment in movie reviews. Following that, and embarked with the popularity of social media, SA became one of the popular topics in NLP. Most of the work on SA targeted English, while other languages, including Arabic, lagged behind. In the last decade, researchers on Arabic NLP started targeting SA such as the work of Abdul-Mageed et al. (2011). Since then, there have been numerous works on Arabic SA such as the works of (Abdulla et al., 2013; Alayba et al., 2018; Abdul-Mageed, 2019; Al-Smadi et al., 2019; Abu Farha and Magdy, 2021). Work on Arabic SA has been hindered by many challenges such as the large variation in dialects (Habash, 2010; Darwish et al., 2014) and the complex morphology of the language (AbdulMageed et al., 2011). With the advancement of work on SA, researchers started tackling the challenges affecting this task such as sarcasm (Hussein, 2018). Sarcasm can be defined as a form of verbal irony that is intended to e"
2021.wanlp-1.36,P16-3016,0,0.0824024,"(AbdulMageed et al., 2011). With the advancement of work on SA, researchers started tackling the challenges affecting this task such as sarcasm (Hussein, 2018). Sarcasm can be defined as a form of verbal irony that is intended to express contempt or ridicule (Joshi et al., 2017). Sarcasm is considered one of the main challenges for SA systems since it implies expressing the opinion in an indirect way, where the intended meaning is different from the literal one (Wilson, 2006). There have been several related works on English sarcasm detection including datasets such as the works reported in (Abercrombie and Hovy, 2016; Barbieri et al., 2014a,b; Filatova, 2012; Ghosh et al., 2015; Joshi et al., 2016; Oprea and Magdy, 2020) and detection systems such as (Rajadesingan et al., 2015; Joshi et al., 2015; Amir et al., 2016). Currently, there are few attempts to work on Arabic sarcasm. Those include the work by soukhria2017, a shared task on irony detection Ghanem et al. (2019) along with the participants’ submissions and dialectal sarcasm datasets by Abbes et al. (2020); Abu Farha and Magdy (2020). In this shared task, we offer our sarcasm and sentiment detection in Arabic task that is co-organised with the WANLP"
2021.wanlp-1.36,W19-4621,1,0.793828,"such as in Abdul-Mageed et al. (2011); Abbasi et al. (2008), focused on modern standard Arabic (MSA). Since then, researchers started targeting dialectal Arabic (DA) such as the work of Mourad and Darwish (2013), where the authors introduced an expandable Arabic sentiment lexicon along with a corpus of tweets. Other datasets include the works of Kiritchenko et al. (2016); Rosenthal et al. (2017); Elmadany et al. (2018). Other works focused on proposing and comparing various approaches for Arabic SA (El-Beltagy et al., 2017; Al-Smadi et al., 2019; Abdulla et al., 2013; Alayba et al., 2018; Abu Farha and Magdy, 2019). A recent comprehensive study by Abu Farha and Magdy (2021) provides a thorough comparative analysis of the available approaches on SA. In their work, they compared a large variety of models on three benchmark datasets. Their analysis shows that deep learning models combined with word embeddings achieve much better performance compared to classical machine learning models, such as SVMs. However, their experiments show that the utilisation of transformer-based language model achieves better results the best deep learning model architecture that uses word-embeddings. They show that using a fine"
2021.wanlp-1.36,2020.osact-1.5,1,0.806083,"re have been several related works on English sarcasm detection including datasets such as the works reported in (Abercrombie and Hovy, 2016; Barbieri et al., 2014a,b; Filatova, 2012; Ghosh et al., 2015; Joshi et al., 2016; Oprea and Magdy, 2020) and detection systems such as (Rajadesingan et al., 2015; Joshi et al., 2015; Amir et al., 2016). Currently, there are few attempts to work on Arabic sarcasm. Those include the work by soukhria2017, a shared task on irony detection Ghanem et al. (2019) along with the participants’ submissions and dialectal sarcasm datasets by Abbes et al. (2020); Abu Farha and Magdy (2020). In this shared task, we offer our sarcasm and sentiment detection in Arabic task that is co-organised with the WANLP 2021 workshop on Arabic NLP. The goal of the shared task is to provide resources and encourage researchers to work on Arabic sarcasm detection. The shared task has two subtasks, sarcasm detection (subtask 1) and sentiment analysis (subtask 2). We provided the participant with a new dataset (ArSarcasm-v2), which is publicly available1 . The dataset is annotated for sarcasm, sentiment and dialect. We received 27 submissions for subtask 1 and 22 submissions for subtask 2. This pa"
2021.wanlp-1.36,2021.wanlp-1.3,1,0.958347,"e of the early works on SA is (Pang et al., 2002), where the authors analysed the sentiment in movie reviews. Following that, and embarked with the popularity of social media, SA became one of the popular topics in NLP. Most of the work on SA targeted English, while other languages, including Arabic, lagged behind. In the last decade, researchers on Arabic NLP started targeting SA such as the work of Abdul-Mageed et al. (2011). Since then, there have been numerous works on Arabic SA such as the works of (Abdulla et al., 2013; Alayba et al., 2018; Abdul-Mageed, 2019; Al-Smadi et al., 2019; Abu Farha and Magdy, 2021). Work on Arabic SA has been hindered by many challenges such as the large variation in dialects (Habash, 2010; Darwish et al., 2014) and the complex morphology of the language (AbdulMageed et al., 2011). With the advancement of work on SA, researchers started tackling the challenges affecting this task such as sarcasm (Hussein, 2018). Sarcasm can be defined as a form of verbal irony that is intended to express contempt or ridicule (Joshi et al., 2017). Sarcasm is considered one of the main challenges for SA systems since it implies expressing the opinion in an indirect way, where the intended"
2021.wanlp-1.36,2021.wanlp-1.38,0,0.0660652,"Missing"
2021.wanlp-1.36,2021.wanlp-1.40,0,0.0701807,"Missing"
2021.wanlp-1.36,S17-2133,0,0.0631799,"Missing"
2021.wanlp-1.36,2021.wanlp-1.43,0,0.0669184,"Missing"
2021.wanlp-1.36,2021.wanlp-1.39,0,0.50792,"subtask 2. This paper provides an overview of the shared task 1 ArSarcasm-v2 is available at: http://github.com/ iabufarha/ArSarcasm-v2 296 Proceedings of the Sixth Arabic Natural Language Processing Workshop, pages 296–305 Kyiv, Ukraine (Virtual), April 19, 2021. and the achieved results by the participants along with their approaches. Most of the approaches used by participants were based on fine-tuning pre-trained language models. A small number of participants utilised other deep learning and conventional machine learning algorithms. The top team in the sarcasm detection task was BhamNLP (Alharbi and Lee, 2021), who achieved an F1-score of 0.6225 over the sarcastic class. While the top team in the sentiment analysis task was CS-UM6P (El Mahdaouy et al., 2021), who achieved F1P N of 0.748. 2 Related Work Our shared task offers two subtasks on Arabic Sarcasm and sentiment detection. In the following, we discuss the literature in both tasks within the Arabic NLP community. 2.1 Arabic Sarcasm Classification Arabic sarcasm did not receive the same degree of attention as English. The work on Arabic sarcasm detection is limited to a few works. soukhria2017 were the first to work on Arabic sarcasm/irony det"
2021.wanlp-1.36,K16-1017,0,0.0446405,"that is intended to express contempt or ridicule (Joshi et al., 2017). Sarcasm is considered one of the main challenges for SA systems since it implies expressing the opinion in an indirect way, where the intended meaning is different from the literal one (Wilson, 2006). There have been several related works on English sarcasm detection including datasets such as the works reported in (Abercrombie and Hovy, 2016; Barbieri et al., 2014a,b; Filatova, 2012; Ghosh et al., 2015; Joshi et al., 2016; Oprea and Magdy, 2020) and detection systems such as (Rajadesingan et al., 2015; Joshi et al., 2015; Amir et al., 2016). Currently, there are few attempts to work on Arabic sarcasm. Those include the work by soukhria2017, a shared task on irony detection Ghanem et al. (2019) along with the participants’ submissions and dialectal sarcasm datasets by Abbes et al. (2020); Abu Farha and Magdy (2020). In this shared task, we offer our sarcasm and sentiment detection in Arabic task that is co-organised with the WANLP 2021 workshop on Arabic NLP. The goal of the shared task is to provide resources and encourage researchers to work on Arabic sarcasm detection. The shared task has two subtasks, sarcasm detection (subta"
2021.wanlp-1.36,2021.wanlp-1.44,0,0.0635105,"Missing"
2021.wanlp-1.36,2020.osact-1.2,0,0.150451,"rehensive study by Abu Farha and Magdy (2021) provides a thorough comparative analysis of the available approaches on SA. In their work, they compared a large variety of models on three benchmark datasets. Their analysis shows that deep learning models combined with word embeddings achieve much better performance compared to classical machine learning models, such as SVMs. However, their experiments show that the utilisation of transformer-based language model achieves better results the best deep learning model architecture that uses word-embeddings. They show that using a fine-tuned AraBERT(Antoun et al., 2020) outperforms all existing classical and deep learning models on all the three benchmark datasets they examined. 297 Set Training Testing Total Sarcasm Sarcastic Non-sarcastic 2,168 10,380 821 2,179 2,989 12,559 Positive 2,180 575 2,577 Sentiment Negative Neutral 4,621 5,747 1,677 748 6,298 6,495 Total 12,548 3,000 15,548 Table 1: Statistics of training and testing sets, showing the number of examples for both sarcasm detection and sentiment analysis tasks. Dialect MSA Egypt Gulf Levant Maghreb Total Sarcastic 1,523 1,085 214 152 15 2,989 Non-Sarcastic 9,362 1,896 752 519 30 12,559 Negative 3,9"
2021.wanlp-1.36,filatova-2012-irony,0,0.0796777,"ork on SA, researchers started tackling the challenges affecting this task such as sarcasm (Hussein, 2018). Sarcasm can be defined as a form of verbal irony that is intended to express contempt or ridicule (Joshi et al., 2017). Sarcasm is considered one of the main challenges for SA systems since it implies expressing the opinion in an indirect way, where the intended meaning is different from the literal one (Wilson, 2006). There have been several related works on English sarcasm detection including datasets such as the works reported in (Abercrombie and Hovy, 2016; Barbieri et al., 2014a,b; Filatova, 2012; Ghosh et al., 2015; Joshi et al., 2016; Oprea and Magdy, 2020) and detection systems such as (Rajadesingan et al., 2015; Joshi et al., 2015; Amir et al., 2016). Currently, there are few attempts to work on Arabic sarcasm. Those include the work by soukhria2017, a shared task on irony detection Ghanem et al. (2019) along with the participants’ submissions and dialectal sarcasm datasets by Abbes et al. (2020); Abu Farha and Magdy (2020). In this shared task, we offer our sarcasm and sentiment detection in Arabic task that is co-organised with the WANLP 2021 workshop on Arabic NLP. The goal of"
2021.wanlp-1.36,2021.wanlp-1.45,0,0.0556882,"Missing"
2021.wanlp-1.36,W14-2609,0,0.0539095,"With the advancement of work on SA, researchers started tackling the challenges affecting this task such as sarcasm (Hussein, 2018). Sarcasm can be defined as a form of verbal irony that is intended to express contempt or ridicule (Joshi et al., 2017). Sarcasm is considered one of the main challenges for SA systems since it implies expressing the opinion in an indirect way, where the intended meaning is different from the literal one (Wilson, 2006). There have been several related works on English sarcasm detection including datasets such as the works reported in (Abercrombie and Hovy, 2016; Barbieri et al., 2014a,b; Filatova, 2012; Ghosh et al., 2015; Joshi et al., 2016; Oprea and Magdy, 2020) and detection systems such as (Rajadesingan et al., 2015; Joshi et al., 2015; Amir et al., 2016). Currently, there are few attempts to work on Arabic sarcasm. Those include the work by soukhria2017, a shared task on irony detection Ghanem et al. (2019) along with the participants’ submissions and dialectal sarcasm datasets by Abbes et al. (2020); Abu Farha and Magdy (2020). In this shared task, we offer our sarcasm and sentiment detection in Arabic task that is co-organised with the WANLP 2021 workshop on Arabi"
2021.wanlp-1.36,D15-1116,0,0.0443472,"rchers started tackling the challenges affecting this task such as sarcasm (Hussein, 2018). Sarcasm can be defined as a form of verbal irony that is intended to express contempt or ridicule (Joshi et al., 2017). Sarcasm is considered one of the main challenges for SA systems since it implies expressing the opinion in an indirect way, where the intended meaning is different from the literal one (Wilson, 2006). There have been several related works on English sarcasm detection including datasets such as the works reported in (Abercrombie and Hovy, 2016; Barbieri et al., 2014a,b; Filatova, 2012; Ghosh et al., 2015; Joshi et al., 2016; Oprea and Magdy, 2020) and detection systems such as (Rajadesingan et al., 2015; Joshi et al., 2015; Amir et al., 2016). Currently, there are few attempts to work on Arabic sarcasm. Those include the work by soukhria2017, a shared task on irony detection Ghanem et al. (2019) along with the participants’ submissions and dialectal sarcasm datasets by Abbes et al. (2020); Abu Farha and Magdy (2020). In this shared task, we offer our sarcasm and sentiment detection in Arabic task that is co-organised with the WANLP 2021 workshop on Arabic NLP. The goal of the shared task is t"
2021.wanlp-1.36,2021.wanlp-1.41,0,0.0600533,"Missing"
2021.wanlp-1.36,W13-1608,0,0.0342458,"ronic tweets, namely DAICT. To prepare the corpus, the authors followed the same approach used by Ghanem et al. (2019) the corpus consists of 5,358 tweets distributed as follows: 4,809 sarcastic, 435 non-sarcastic and 114 labelled as ambiguous. 2.2 Arabic Sentiment Analysis Unlike Arabic sarcasm detection, Arabic sentiment analysis (SA) has been under the researchers’ radar for a while. Early work on Arabic SA such as in Abdul-Mageed et al. (2011); Abbasi et al. (2008), focused on modern standard Arabic (MSA). Since then, researchers started targeting dialectal Arabic (DA) such as the work of Mourad and Darwish (2013), where the authors introduced an expandable Arabic sentiment lexicon along with a corpus of tweets. Other datasets include the works of Kiritchenko et al. (2016); Rosenthal et al. (2017); Elmadany et al. (2018). Other works focused on proposing and comparing various approaches for Arabic SA (El-Beltagy et al., 2017; Al-Smadi et al., 2019; Abdulla et al., 2013; Alayba et al., 2018; Abu Farha and Magdy, 2019). A recent comprehensive study by Abu Farha and Magdy (2021) provides a thorough comparative analysis of the available approaches on SA. In their work, they compared a large variety of mode"
2021.wanlp-1.36,2021.wanlp-1.46,0,0.133308,"ArabicProcessors (Gaanoun and Benelallam, 2021) BhamNLP (Alharbi and Lee, 2021) CS-UM6P (El Mahdaouy et al., 2021) DeepBlueAI (Song et al., 2021) DM-JUST(dalya) (Faraj and Abdullah, 2021) Fatemah (Husain and Uzuner, 2021) iCompass (Naski et al., 2021) IDC (Israeli et al., 2021) ITAM Juha (Abuzayed and Al-Khalifa, 2021) Laila & Daliyah (Laila) (Bashmal and Alzeer, 2021) Naglaa Abdelhade (Naglaa) NAYEL (Nayel et al., 2021) Phonemer (Wadhawan, 2021) rematchka (Abdel-Salam, 2021) SalamBERT (Husain and Uzuner, 2021) Serpente (Ghoul and Lejeune, 2021) SpeechTrans (Lichouri et al., 2021) SPPU AASM (Hengle et al., 2021) ZTeam (Elagbry et al., 2021) Affiliation of the first author A.I.M Technologies Alibaba Group, China INSEA, Morocco University of Birmingham, King Abdulaziz University Mohammed VI Polytechnic University, Morocco DeepBlue Technology (Shanghai) Co., Ltd, China Jordan University of Science and Technology, Jordan Kuwait University, Kuwait iCompass, Tunisia The Data Science Institute, Interdisciplinary Center, Israel University Mohamed First, Oujda, Morocco iWAN research group, Saudi Arabia King Saud University, Saudi Arabia Assiut university, Egypt Benha University, Egypt Flipkart Private Limited"
2021.wanlp-1.36,2021.wanlp-1.47,0,0.0683523,"Missing"
2021.wanlp-1.36,2021.wanlp-1.48,0,0.133159,"For subtask 2 (sentiment analysis), CS-UM6P El Mahdaouy et al. (2021) team achieved first place with an F1P N of 0.748. The main evaluation metric for subtask 1 (sarcasm detection) is the F1-score of the sarcastic class only 299 3 We received system description papers from only 17 of the participating teams. Team AIMTechnologies ALI-B2B-AI ArabicProcessors (Gaanoun and Benelallam, 2021) BhamNLP (Alharbi and Lee, 2021) CS-UM6P (El Mahdaouy et al., 2021) DeepBlueAI (Song et al., 2021) DM-JUST(dalya) (Faraj and Abdullah, 2021) Fatemah (Husain and Uzuner, 2021) iCompass (Naski et al., 2021) IDC (Israeli et al., 2021) ITAM Juha (Abuzayed and Al-Khalifa, 2021) Laila & Daliyah (Laila) (Bashmal and Alzeer, 2021) Naglaa Abdelhade (Naglaa) NAYEL (Nayel et al., 2021) Phonemer (Wadhawan, 2021) rematchka (Abdel-Salam, 2021) SalamBERT (Husain and Uzuner, 2021) Serpente (Ghoul and Lejeune, 2021) SpeechTrans (Lichouri et al., 2021) SPPU AASM (Hengle et al., 2021) ZTeam (Elagbry et al., 2021) Affiliation of the first author A.I.M Technologies Alibaba Group, China INSEA, Morocco University of Birmingham, King Abdulaziz University Mohammed VI Polytechnic University, Morocco DeepBlue Technology (Shanghai) Co., Ltd, China"
2021.wanlp-1.36,P15-2124,0,0.12506,"orm of verbal irony that is intended to express contempt or ridicule (Joshi et al., 2017). Sarcasm is considered one of the main challenges for SA systems since it implies expressing the opinion in an indirect way, where the intended meaning is different from the literal one (Wilson, 2006). There have been several related works on English sarcasm detection including datasets such as the works reported in (Abercrombie and Hovy, 2016; Barbieri et al., 2014a,b; Filatova, 2012; Ghosh et al., 2015; Joshi et al., 2016; Oprea and Magdy, 2020) and detection systems such as (Rajadesingan et al., 2015; Joshi et al., 2015; Amir et al., 2016). Currently, there are few attempts to work on Arabic sarcasm. Those include the work by soukhria2017, a shared task on irony detection Ghanem et al. (2019) along with the participants’ submissions and dialectal sarcasm datasets by Abbes et al. (2020); Abu Farha and Magdy (2020). In this shared task, we offer our sarcasm and sentiment detection in Arabic task that is co-organised with the WANLP 2021 workshop on Arabic NLP. The goal of the shared task is to provide resources and encourage researchers to work on Arabic sarcasm detection. The shared task has two subtasks, sarc"
2021.wanlp-1.36,2021.wanlp-1.51,0,0.0602721,"Missing"
2021.wanlp-1.36,2020.acl-main.118,1,0.786667,"ffecting this task such as sarcasm (Hussein, 2018). Sarcasm can be defined as a form of verbal irony that is intended to express contempt or ridicule (Joshi et al., 2017). Sarcasm is considered one of the main challenges for SA systems since it implies expressing the opinion in an indirect way, where the intended meaning is different from the literal one (Wilson, 2006). There have been several related works on English sarcasm detection including datasets such as the works reported in (Abercrombie and Hovy, 2016; Barbieri et al., 2014a,b; Filatova, 2012; Ghosh et al., 2015; Joshi et al., 2016; Oprea and Magdy, 2020) and detection systems such as (Rajadesingan et al., 2015; Joshi et al., 2015; Amir et al., 2016). Currently, there are few attempts to work on Arabic sarcasm. Those include the work by soukhria2017, a shared task on irony detection Ghanem et al. (2019) along with the participants’ submissions and dialectal sarcasm datasets by Abbes et al. (2020); Abu Farha and Magdy (2020). In this shared task, we offer our sarcasm and sentiment detection in Arabic task that is co-organised with the WANLP 2021 workshop on Arabic NLP. The goal of the shared task is to provide resources and encourage researcher"
2021.wanlp-1.36,W02-1011,0,0.0375969,"m, sentiment and dialect. We received 27 and 22 submissions for subtasks 1 and 2 respectively. Most of the approaches relied on using and fine-tuning pre-trained language models such as AraBERT and MARBERT. The top achieved results for the sarcasm detection and sentiment analysis tasks were 0.6225 F1-score and 0.748 F1P N respectively. 1 Introduction Work on opinion mining and subjective language analysis has been prominent in the natural language processing (NLP) field during the last two decades. One of the main tasks in this area is sentiment analysis (SA). One of the early works on SA is (Pang et al., 2002), where the authors analysed the sentiment in movie reviews. Following that, and embarked with the popularity of social media, SA became one of the popular topics in NLP. Most of the work on SA targeted English, while other languages, including Arabic, lagged behind. In the last decade, researchers on Arabic NLP started targeting SA such as the work of Abdul-Mageed et al. (2011). Since then, there have been numerous works on Arabic SA such as the works of (Abdulla et al., 2013; Alayba et al., 2018; Abdul-Mageed, 2019; Al-Smadi et al., 2019; Abu Farha and Magdy, 2021). Work on Arabic SA has bee"
2021.wanlp-1.36,S17-2088,0,0.118607,"Missing"
2021.wanlp-1.36,K16-1015,0,0.0586179,"ing the challenges affecting this task such as sarcasm (Hussein, 2018). Sarcasm can be defined as a form of verbal irony that is intended to express contempt or ridicule (Joshi et al., 2017). Sarcasm is considered one of the main challenges for SA systems since it implies expressing the opinion in an indirect way, where the intended meaning is different from the literal one (Wilson, 2006). There have been several related works on English sarcasm detection including datasets such as the works reported in (Abercrombie and Hovy, 2016; Barbieri et al., 2014a,b; Filatova, 2012; Ghosh et al., 2015; Joshi et al., 2016; Oprea and Magdy, 2020) and detection systems such as (Rajadesingan et al., 2015; Joshi et al., 2015; Amir et al., 2016). Currently, there are few attempts to work on Arabic sarcasm. Those include the work by soukhria2017, a shared task on irony detection Ghanem et al. (2019) along with the participants’ submissions and dialectal sarcasm datasets by Abbes et al. (2020); Abu Farha and Magdy (2020). In this shared task, we offer our sarcasm and sentiment detection in Arabic task that is co-organised with the WANLP 2021 workshop on Arabic NLP. The goal of the shared task is to provide resources"
2021.wanlp-1.36,2021.wanlp-1.52,0,0.119374,"eaderboard. For subtask 1 (sarcasm detection), BhamNLP Alharbi and Lee (2021) achieved first place with an F1-sarcastic of 0.6225. For subtask 2 (sentiment analysis), CS-UM6P El Mahdaouy et al. (2021) team achieved first place with an F1P N of 0.748. The main evaluation metric for subtask 1 (sarcasm detection) is the F1-score of the sarcastic class only 299 3 We received system description papers from only 17 of the participating teams. Team AIMTechnologies ALI-B2B-AI ArabicProcessors (Gaanoun and Benelallam, 2021) BhamNLP (Alharbi and Lee, 2021) CS-UM6P (El Mahdaouy et al., 2021) DeepBlueAI (Song et al., 2021) DM-JUST(dalya) (Faraj and Abdullah, 2021) Fatemah (Husain and Uzuner, 2021) iCompass (Naski et al., 2021) IDC (Israeli et al., 2021) ITAM Juha (Abuzayed and Al-Khalifa, 2021) Laila & Daliyah (Laila) (Bashmal and Alzeer, 2021) Naglaa Abdelhade (Naglaa) NAYEL (Nayel et al., 2021) Phonemer (Wadhawan, 2021) rematchka (Abdel-Salam, 2021) SalamBERT (Husain and Uzuner, 2021) Serpente (Ghoul and Lejeune, 2021) SpeechTrans (Lichouri et al., 2021) SPPU AASM (Hengle et al., 2021) ZTeam (Elagbry et al., 2021) Affiliation of the first author A.I.M Technologies Alibaba Group, China INSEA, Morocco Universit"
2021.wanlp-1.36,2021.wanlp-1.53,0,0.060949,"Missing"
2021.wanlp-1.36,S16-1004,0,0.0357618,"uted as follows: 4,809 sarcastic, 435 non-sarcastic and 114 labelled as ambiguous. 2.2 Arabic Sentiment Analysis Unlike Arabic sarcasm detection, Arabic sentiment analysis (SA) has been under the researchers’ radar for a while. Early work on Arabic SA such as in Abdul-Mageed et al. (2011); Abbasi et al. (2008), focused on modern standard Arabic (MSA). Since then, researchers started targeting dialectal Arabic (DA) such as the work of Mourad and Darwish (2013), where the authors introduced an expandable Arabic sentiment lexicon along with a corpus of tweets. Other datasets include the works of Kiritchenko et al. (2016); Rosenthal et al. (2017); Elmadany et al. (2018). Other works focused on proposing and comparing various approaches for Arabic SA (El-Beltagy et al., 2017; Al-Smadi et al., 2019; Abdulla et al., 2013; Alayba et al., 2018; Abu Farha and Magdy, 2019). A recent comprehensive study by Abu Farha and Magdy (2021) provides a thorough comparative analysis of the available approaches on SA. In their work, they compared a large variety of models on three benchmark datasets. Their analysis shows that deep learning models combined with word embeddings achieve much better performance compared to classical"
2021.wanlp-1.36,2021.wanlp-1.49,0,0.0689573,"Missing"
chiao-etal-2006-evaluation,W05-0809,0,\N,Missing
chiao-etal-2006-evaluation,W03-0301,0,\N,Missing
I13-2001,font-llitjos-carbonell-2004-translation,0,0.361837,"Missing"
I13-2001,P11-4010,0,0.0809888,", we show how our framework employs automatic annotators to correct basic Arabic spelling mistakes to speed up the annotation process. Our framework consists of two major interfaces: (a) an Admin interface, which enables the lead annotator to create, assign, monitor, evaluate and export annotation tasks in large scale; and (b) 1 2 Related Work Traditionally, manual text correction is performed under the context of post-editing machine translation (MT) output. The goal of post-editing is to evaluate MT systems rather than building corpora of edits. Tools like PET (Aziz et al., 2012) and BLAST (Stymne, 2011) provide annotators a text-editorlike interface to identify, record, and correct errors. Text-editor-like interfaces are very flexible and allow all forms of corrections to be performed, but, they are not capable of accurately tracking token alignment, if at all. Frameworks such as EXMARaLDA (Schmidt, 2010) and GATE (Cunningham et al., 2011) facilitate multi-layer and multi-round annotations. An example of such approach is the work of Dickinson and Ledbetter (2012) who annotated errors in Hungarian students essays using multiple annotation layers from phonology to syntax in different stages. T"
I13-2001,dickinson-ledbetter-2012-annotating,0,\N,Missing
I13-2001,aziz-etal-2012-pet,0,\N,Missing
I13-2001,2012.eamt-1.31,0,\N,Missing
I13-2001,2012.tc-1.5,0,\N,Missing
L16-1295,avramidis-etal-2014-taraxu,0,0.0610864,"A portion of the corpus contains an analysis of the type of errors made by the MT system. Elming (2006) created a 265K-word English-Danish MT manually corrected corpus by a human professional translator. The full corpus covers the chemical patents domain. Simard et al. (2007) created a 500K-word corpus of manually edited FrenchEnglish and English-French MT from the Canadian Job Bank website. The corpus is a collection of blocks composed of the source language texts, the machine translation output of a rule-based MT system and the final post-edited version done by a human translator. Moreover, Avramidis et al. (2014) built a corpus of human-annotated machine translations which was evaluated by professional human translators for the following three language pairs: GermanEnglish, English-German and Spanish-German. Fishel et al. (2012) created a corpus of automatically produced translations with detailed manual translation error analysis of 576 sentences for four language pairs: EnglishCzech;French-German;German-English;English-Serbian. Popescu-belis et al. (2002) produced a small corpus of 50 texts translated by students and corrected by their professors and all translation errors are annotated with their c"
L16-1295,D14-1026,1,0.764023,"was evaluated by professional human translators for the following three language pairs: GermanEnglish, English-German and Spanish-German. Fishel et al. (2012) created a corpus of automatically produced translations with detailed manual translation error analysis of 576 sentences for four language pairs: EnglishCzech;French-German;German-English;English-Serbian. Popescu-belis et al. (2002) produced a small corpus of 50 texts translated by students and corrected by their professors and all translation errors are annotated with their corrections in this corpus. For Arabic, we cite the effort of Bouamor et al. (2014) who created a medium scale human judgment corpus of Arabic machine translation using the output of six MT systems and a total of 1892 sentences and 22K rankings. Our corpus is a part of the Qatar Arabic Language Bank (QALB) project, a large scale manually annotated annotation project (Zaghouani et al., 2014b; Zaghouani et al., 1869 2015). The project goal was to create an error corrected 2M-words corpus for online user comments on news websites, native speaker essays, non-native speaker essays and machine translation output. The 100K-word machine translation portion was selected from various"
L16-1295,W12-5611,0,0.0270103,"Missing"
L16-1295,2006.eamt-1.27,0,0.166044,"pair. Keywords: Post-Editing, Guidelines, Annotation 1. 2. Introduction In recent years, machine translation (MT) became widely used by translation companies to reduce their costs and improve their speed. Therefore, the demand for quick and accurate machine translations is growing. Machine translation (MT) systems often produce incorrect output with many grammatical and lexical choice errors. Correcting machine-produced translation errors, or MT Post-Editing (PE) can be done automatically or manually. Successful automatic post-editing approaches using manually corrected MT output were used by Elming (2006) and Simard et al. (2007). The availability of annotated resources is required for such approaches. When it comes to the Arabic language, to the best of our knowledge, there is no manually post-edited MT corpora available to build such systems. Therefore, there is a clear need to build such valuable resources for the Arabic language. In this paper, we present our guidelines and annotation procedure to create a human corrected MT corpus for the Modern Standard Arabic (MSA). The creation of any manually annotated corpus usually presents many challenges. In order to address these challenges, we c"
L16-1295,fishel-etal-2012-terra,0,0.104056,"ers the chemical patents domain. Simard et al. (2007) created a 500K-word corpus of manually edited FrenchEnglish and English-French MT from the Canadian Job Bank website. The corpus is a collection of blocks composed of the source language texts, the machine translation output of a rule-based MT system and the final post-edited version done by a human translator. Moreover, Avramidis et al. (2014) built a corpus of human-annotated machine translations which was evaluated by professional human translators for the following three language pairs: GermanEnglish, English-German and Spanish-German. Fishel et al. (2012) created a corpus of automatically produced translations with detailed manual translation error analysis of 576 sentences for four language pairs: EnglishCzech;French-German;German-English;English-Serbian. Popescu-belis et al. (2002) produced a small corpus of 50 texts translated by students and corrected by their professors and all translation errors are annotated with their corrections in this corpus. For Arabic, we cite the effort of Bouamor et al. (2014) who created a medium scale human judgment corpus of Arabic machine translation using the output of six MT systems and a total of 1892 sen"
L16-1295,W14-3605,1,0.85988,"one. 7. Conclusions We have presented in detail the methodology used to create a 100K-word English to Arabic MT manually post-edited corpus, including the development of the guidelines as well as the annotation procedure and the quality control procedure using frequent inter-annotator measures. The created guidelines will be made publicly available and we look forward to distribute the post-edited corpus in a planned shared task on automatic error correction and getting feedback from the community on its usefulness as it was in the previous shared tasks we organized for the L1 and L2 corpus (Mohit et al., 2014; Rozovskaya et al., 2015). We believe that this corpus will be valuable to advance research efforts in the machine translation area since manually annotated data is often needed by the MT community. We believe that our methodology for guideline development and annotation consistency checking can be applied in other projects and other languages as well. In the future, we plan to increase the size of the corpus and also to add other corpus domains. Acknowledgements We would like to thank the anonymous reviewers for their valuable comments and suggestions. We also thank all our dedicated annotat"
L16-1295,I13-2001,1,0.511808,"e was closely monitored during the initial period, before allowing the annotator to join the official post-editing production phase. Moreover, a dedicated online discussion group was frequently used by the annotation team to keep track of the MT post-editing questions and issues raised during the annotation process. This mechanism, proved to help the annotators and the lead annotator to have a better communication. The annotation itself is done using the QAWI annotation tool, an in house built web annotation framework designed originally for the manual correction of errors in L1 and L2 texts (Obeid et al., 2013). This framework includes two major components: The annotation management interface which is used to assist the lead annotator in the general work-flow process, it allows the annotation manager easily upload and organize files and projects, manage users, assign files in a batch or individually, export annotation tasks and monitor the current annotation progress by processing real time annotation progress statistics. Moreover inter-annotator agreement (IAA), evaluation metrics such as the Word Error Rate (WER) are integrated with the management interface to allow the scores to be computed and t"
L16-1295,pasha-etal-2014-madamira,1,0.825321,"Word Edit: to correct/modify a word. 2. Word Move: to move words to the right location in the sentence. 3. Add Word: insert missing words in the text. 4. Delete: delete unnecessary words. 5. Merge and Split: to merge or split words. All post-editing action history previously mentioned are recorded in a database and can be exported to an XML file. Figure 2 shows an example of how the annotation actions are stored in the XML annotation export file. Finally, and in order to increase the post-editing speed and prior to the first human pass, an automatic post-editing pass is done through MADAMIRA (Pasha et al., 2014), a tool that automatically corrects common spelling errors using a prediction model based on the words in-context. MADAMIRA uses a morphological analyzer to produce, for each input word, a list of analyses specifying every possible morphological interpretation of that word, covering all morphological features of the word. Most of the errors automatically corrected are related to Ya/AlifMaqsura, Ha/Ta-Marbuta and Hamzated Alif forms, which are common spelling errors in Arabic.6 6. 6.1. Evaluation Inter-Annotator Agreement We use Word error Rate (WER) as a proxy of the Inter annotator agreement"
L16-1295,W15-3204,1,0.858567,"We have presented in detail the methodology used to create a 100K-word English to Arabic MT manually post-edited corpus, including the development of the guidelines as well as the annotation procedure and the quality control procedure using frequent inter-annotator measures. The created guidelines will be made publicly available and we look forward to distribute the post-edited corpus in a planned shared task on automatic error correction and getting feedback from the community on its usefulness as it was in the previous shared tasks we organized for the L1 and L2 corpus (Mohit et al., 2014; Rozovskaya et al., 2015). We believe that this corpus will be valuable to advance research efforts in the machine translation area since manually annotated data is often needed by the MT community. We believe that our methodology for guideline development and annotation consistency checking can be applied in other projects and other languages as well. In the future, we plan to increase the size of the corpus and also to add other corpus domains. Acknowledgements We would like to thank the anonymous reviewers for their valuable comments and suggestions. We also thank all our dedicated annotators: Noor Alzeer, Hoda Fat"
L16-1295,N07-1064,0,0.122569,"st-Editing, Guidelines, Annotation 1. 2. Introduction In recent years, machine translation (MT) became widely used by translation companies to reduce their costs and improve their speed. Therefore, the demand for quick and accurate machine translations is growing. Machine translation (MT) systems often produce incorrect output with many grammatical and lexical choice errors. Correcting machine-produced translation errors, or MT Post-Editing (PE) can be done automatically or manually. Successful automatic post-editing approaches using manually corrected MT output were used by Elming (2006) and Simard et al. (2007). The availability of annotated resources is required for such approaches. When it comes to the Arabic language, to the best of our knowledge, there is no manually post-edited MT corpora available to build such systems. Therefore, there is a clear need to build such valuable resources for the Arabic language. In this paper, we present our guidelines and annotation procedure to create a human corrected MT corpus for the Modern Standard Arabic (MSA). The creation of any manually annotated corpus usually presents many challenges. In order to address these challenges, we created comprehensive and"
L16-1295,2006.amta-papers.25,0,0.111195,"Missing"
L16-1295,wisniewski-etal-2014-corpus,0,0.0305494,"ed to check the annotation quality. To the best of our knowledge, this is the first published machine translation manual post-editing annotation effort for Arabic of this scale. In the next sections, we review related work (Section 2), describe our corpus and the development of the guidelines (Sections 3-4), and present our annotation procedure (Section 5), than we present the annotation evaluation in Section 6, finally we conclude our work in Section 7. Related Work Large scale manually corrected MT corpora are not yet widely available due to the high cost related to building such resources. Wisniewski et al. (2014) created a corpus of machine translation errors extracted from several translation students taking part in a master program in specialized translations. The texts are translated from English to French. A portion of the corpus contains an analysis of the type of errors made by the MT system. Elming (2006) created a 265K-word English-Danish MT manually corrected corpus by a human professional translator. The full corpus covers the chemical patents domain. Simard et al. (2007) created a 500K-word corpus of manually edited FrenchEnglish and English-French MT from the Canadian Job Bank website. The"
L16-1295,zaghouani-etal-2014-large,1,0.889004,"glishCzech;French-German;German-English;English-Serbian. Popescu-belis et al. (2002) produced a small corpus of 50 texts translated by students and corrected by their professors and all translation errors are annotated with their corrections in this corpus. For Arabic, we cite the effort of Bouamor et al. (2014) who created a medium scale human judgment corpus of Arabic machine translation using the output of six MT systems and a total of 1892 sentences and 22K rankings. Our corpus is a part of the Qatar Arabic Language Bank (QALB) project, a large scale manually annotated annotation project (Zaghouani et al., 2014b; Zaghouani et al., 1869 2015). The project goal was to create an error corrected 2M-words corpus for online user comments on news websites, native speaker essays, non-native speaker essays and machine translation output. The 100K-word machine translation portion was selected from various Wikinews English articles translated to Arabic automatically using the Google Translate tool.1 3. Corpus Description We collected a 100K-word corpus of English news articles taken from the collaborative journalism Wikinews website.2 Since Wikinews is a free-content news source, we avoided any copyrights comp"
L16-1295,W15-1614,1,0.623101,"Missing"
L16-1577,D15-1274,0,0.145302,"entence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter transliteration encoding system to represent Arabic in Romanized script (Buckwalter, 2002) Undiacritized Diacritized Buckwalter Y«ð Y « ð /waEad/ Y«ð Y«ð Y«ð Y«ð  ð Y« ð Y« Y « ð Y « ð English he p"
L16-1577,W15-3209,1,0.859315,"th the diacritics for a variety of Arabic texts covering more than 10 genres. The target size of the annotated corpus is 2 million words. The present work was mainly motivated by the lack of equivalent multi-genres large scale annotated corpus. The creation of manually annotated corpus usually presents many challenges and issues. In order to address those challenges, we created comprehensive and simplified annotation guidelines that were used by a team consisting of five annotators and an annotation manager. The guidelines were defined after an initial pilot annotation experiment described in Bouamor et al. (2015). In order to ensure a high annotation agreement between the annotators, multiple training sessions were held and a regular inter-annotator agreement (IAA) measures were performed to check annotation quality. To the best of our knowledge, this is the first Arabic diacritized multi-genres corpus. The remainder of this paper is organized as follows. We review related work in section 2. Afterwards, we discuss the challenges posed by the complexity of the Arabic diacritization process in section 3. Then, we describe our corpus and the development of the guidelines in sections 4 and 5. 3637 We pres"
L16-1577,2007.mtsummit-papers.20,1,0.926265,"nced, except the last letter diacritization, and b) case and mood diacritization, which exists above or below the last letter in each word, indicating its grammatical function in the sentence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter translite"
L16-1577,palmer-etal-2008-pilot,1,0.881222,"utomatic diacritization system for Arabic using rule-based, statistical and hybrid methods. We refer to the recent literature review in Abandah et al. (2015) for a general overview of these methods and tools. The most relevant resource to our work is the Penn Arabic Treebank (PATB), a large corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another re"
L16-1577,dukes-habash-2010-morphological,0,0.0333421,"corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to fin"
L16-1577,maamouri-etal-2008-enhancing,0,0.0233017,"about the text, the author and the source. In order to use the CCA corpus, a normalization effort was done to produce a consistent XML mark-up format to be used by our annotation tool. 5. Development of the Guidelines We provided the annotators with detailed guidelines, describing our diacritization scheme and specifying when and where to add the diacritics. We describe the annotation procedure and explained how to deal with borderline cases. We also include several annotated examples to illustrate the specified rules. Our guidelines are mostly inspired from the LDC POS annotation guidelines (Maamouri et al., 2008). Since, the LDC guidelines are mainly designed for the POS annotation and not specifically for the diacritization per se, we created a simplified version and added some specific diacritization rules to make the annotation process consistent. Below we provide some examples of diacritization exceptions and specific rules. The Shadda: The shadda mark should be added in all cases specified in the guidelines except the following in the definite artilce, where it should not be added to the letter ÊË@ /Allymwn/ ’lemon’ È /l/ of the definite article (e.g. àñÒJ  ÊË@ /A˜llymwn/). Moreover, the shadda"
L16-1577,maamouri-etal-2010-speech,1,0.874215,"and present our future work in section 8. 2. Related Work Since, our paper is mainly about the creation and evaluation of a large annotated corpus, we will focus mostly on this aspect in the previous works. There have been numerous approaches to build an automatic diacritization system for Arabic using rule-based, statistical and hybrid methods. We refer to the recent literature review in Abandah et al. (2015) for a general overview of these methods and tools. The most relevant resource to our work is the Penn Arabic Treebank (PATB), a large corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacrit"
L16-1577,W14-3605,1,0.918057,"Missing"
L16-1577,pasha-etal-2014-madamira,1,0.898534,"Missing"
L16-1577,W15-3204,1,0.903933,"Missing"
L16-1577,D15-1152,0,0.0742449,"indicating its grammatical function in the sentence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter transliteration encoding system to represent Arabic in Romanized script (Buckwalter, 2002) Undiacritized Diacritized Buckwalter Y«ð Y « ð /waEad/ Y«ð Y"
L16-1577,2006.amta-papers.25,0,0.0982236,"Missing"
L16-1577,W10-1836,1,0.806692,"tion system for Arabic using rule-based, statistical and hybrid methods. We refer to the recent literature review in Abandah et al. (2015) for a general overview of these methods and tools. The most relevant resource to our work is the Penn Arabic Treebank (PATB), a large corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to"
L16-1577,W12-2511,1,0.785024,"genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and"
L16-1577,zaghouani-etal-2014-large,1,0.532947,"roposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and diacritics. Letters comprise long vowels such as A, y, w"
L16-1577,W15-1614,1,0.736149,"ing the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and diacritics. Letters comprise long vowels such as A, y, w as well as consonants."
L16-1577,L16-1295,1,0.768521,"diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and diacritics. Letters comprise long vowels such as A, y, w as well as consonants. Diacritics on the other h"
L16-1577,P06-1073,0,0.603177,"t letter in each word, indicating its grammatical function in the sentence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter transliteration encoding system to represent Arabic in Romanized script (Buckwalter, 2002) Undiacritized Diacritized Buckwalter"
L16-1578,avramidis-etal-2014-taraxu,0,0.0184159,"(Carl et al., 2011; Vieira, 2014; Stymne et al., 2012; Doherty and O’Brien, 2009; O’Brien, 2011). Furthermore, the approach is more objective than the approaches which rely on human scores for perceived post-editing effort (Specia, 2011; De Sousa et al., 2011; Koponen, 2012; Vieira, 2014). In its essence, it is similar to other error classification approaches, such as (Flanagan, 1994; Font-Llitj´os et al., 2005; 3644 Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Blain et al., 2011; Stymne, 2011; Koponen, 2012; Koponen et al., 2012; Fishel et al., 2012; Stymne et al., 2012; Vieira, 2014; Avramidis et al., 2014). It is enriched, however by error ranking, based on information specifying which errors require more cognitive effort to be corrected, and which less. In this way, the approach only requires counting the number of errors of each type in the MT output. And thus it allows the comparison of the post-editing cost of different output texts of the same MT engine, the same text as an output of different MT engines, or for different language pairs. Temnikova (2010) tested her approach on two emergency instructions texts, one original (called “Complex”) and one manually simplified (called “Simplified”"
L16-1578,R11-1014,0,0.0530077,"Missing"
L16-1578,2009.mtsummit-posters.5,0,0.1322,"Missing"
L16-1578,2010.eamt-1.12,0,0.0241724,"cognitive evaluation approaches which are based on keystroke logging (Carl et al., 2011; Krings and Koby, 2001; Koponen et al., 2012) or eye-tracking (Carl et al., 2011; Vieira, 2014; Stymne et al., 2012; Doherty and O’Brien, 2009; O’Brien, 2011). Furthermore, the approach is more objective than the approaches which rely on human scores for perceived post-editing effort (Specia, 2011; De Sousa et al., 2011; Koponen, 2012; Vieira, 2014). In its essence, it is similar to other error classification approaches, such as (Flanagan, 1994; Font-Llitj´os et al., 2005; 3644 Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Blain et al., 2011; Stymne, 2011; Koponen, 2012; Koponen et al., 2012; Fishel et al., 2012; Stymne et al., 2012; Vieira, 2014; Avramidis et al., 2014). It is enriched, however by error ranking, based on information specifying which errors require more cognitive effort to be corrected, and which less. In this way, the approach only requires counting the number of errors of each type in the MT output. And thus it allows the comparison of the post-editing cost of different output texts of the same MT engine, the same text as an output of different MT engines, or for different language pairs. Te"
L16-1578,fishel-etal-2012-terra,0,0.0135384,"s and Koby, 2001; Koponen et al., 2012) or eye-tracking (Carl et al., 2011; Vieira, 2014; Stymne et al., 2012; Doherty and O’Brien, 2009; O’Brien, 2011). Furthermore, the approach is more objective than the approaches which rely on human scores for perceived post-editing effort (Specia, 2011; De Sousa et al., 2011; Koponen, 2012; Vieira, 2014). In its essence, it is similar to other error classification approaches, such as (Flanagan, 1994; Font-Llitj´os et al., 2005; 3644 Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Blain et al., 2011; Stymne, 2011; Koponen, 2012; Koponen et al., 2012; Fishel et al., 2012; Stymne et al., 2012; Vieira, 2014; Avramidis et al., 2014). It is enriched, however by error ranking, based on information specifying which errors require more cognitive effort to be corrected, and which less. In this way, the approach only requires counting the number of errors of each type in the MT output. And thus it allows the comparison of the post-editing cost of different output texts of the same MT engine, the same text as an output of different MT engines, or for different language pairs. Temnikova (2010) tested her approach on two emergency instructions texts, one original (called"
L16-1578,I13-2001,1,0.895716,"Missing"
L16-1578,P02-1040,0,0.106211,"6), from which the error classes were subsequently regrouped and ranked in an increasing order, so as to reflect the cognitive load post-editors experience while correcting the MT output. Error re-grouping and ranking was done on the basis of relevant psycholinguistic error correction litera1 https://translate.google.com ture (Harley, 2013; Larigauderie et al., 1998; Baddeley and Hitch, 1974). The aim of proposing such an approach was to create a better metric for the effort a post-editor faces while correcting MT texts, instead of relying on a nontransparent MT evaluation score such as BLEU (Papineni et al., 2002). Figure 1. shows the previous error ranking. The easiest errors to correct were considered those which required only a small change inside the word (CInF), followed by errors requiring replacing or adding a word (Styl, InW, etc.), while the hardest errors were considered those which required understanding of the whole sentence (e.g. InP, MissP, WoW and WoPh). Figure 1: Temnikova (2010)’s Error Ranking. The approach does not rely on using specific software, in contrast to PE cognitive evaluation approaches which are based on keystroke logging (Carl et al., 2011; Krings and Koby, 2001; Koponen"
L16-1578,pasha-etal-2014-madamira,1,0.803417,"Missing"
L16-1578,W15-3204,1,0.794797,"Missing"
L16-1578,2011.eamt-1.12,0,0.0234323,"rs were considered those which required understanding of the whole sentence (e.g. InP, MissP, WoW and WoPh). Figure 1: Temnikova (2010)’s Error Ranking. The approach does not rely on using specific software, in contrast to PE cognitive evaluation approaches which are based on keystroke logging (Carl et al., 2011; Krings and Koby, 2001; Koponen et al., 2012) or eye-tracking (Carl et al., 2011; Vieira, 2014; Stymne et al., 2012; Doherty and O’Brien, 2009; O’Brien, 2011). Furthermore, the approach is more objective than the approaches which rely on human scores for perceived post-editing effort (Specia, 2011; De Sousa et al., 2011; Koponen, 2012; Vieira, 2014). In its essence, it is similar to other error classification approaches, such as (Flanagan, 1994; Font-Llitj´os et al., 2005; 3644 Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Blain et al., 2011; Stymne, 2011; Koponen, 2012; Koponen et al., 2012; Fishel et al., 2012; Stymne et al., 2012; Vieira, 2014; Avramidis et al., 2014). It is enriched, however by error ranking, based on information specifying which errors require more cognitive effort to be corrected, and which less. In this way, the approach only requires counting the number o"
L16-1578,stymne-etal-2012-eye,0,0.0214094,"to correct were considered those which required only a small change inside the word (CInF), followed by errors requiring replacing or adding a word (Styl, InW, etc.), while the hardest errors were considered those which required understanding of the whole sentence (e.g. InP, MissP, WoW and WoPh). Figure 1: Temnikova (2010)’s Error Ranking. The approach does not rely on using specific software, in contrast to PE cognitive evaluation approaches which are based on keystroke logging (Carl et al., 2011; Krings and Koby, 2001; Koponen et al., 2012) or eye-tracking (Carl et al., 2011; Vieira, 2014; Stymne et al., 2012; Doherty and O’Brien, 2009; O’Brien, 2011). Furthermore, the approach is more objective than the approaches which rely on human scores for perceived post-editing effort (Specia, 2011; De Sousa et al., 2011; Koponen, 2012; Vieira, 2014). In its essence, it is similar to other error classification approaches, such as (Flanagan, 1994; Font-Llitj´os et al., 2005; 3644 Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Blain et al., 2011; Stymne, 2011; Koponen, 2012; Koponen et al., 2012; Fishel et al., 2012; Stymne et al., 2012; Vieira, 2014; Avramidis et al., 2014). It is enriched, however by e"
L16-1578,P11-4010,0,0.0197882,"ased on keystroke logging (Carl et al., 2011; Krings and Koby, 2001; Koponen et al., 2012) or eye-tracking (Carl et al., 2011; Vieira, 2014; Stymne et al., 2012; Doherty and O’Brien, 2009; O’Brien, 2011). Furthermore, the approach is more objective than the approaches which rely on human scores for perceived post-editing effort (Specia, 2011; De Sousa et al., 2011; Koponen, 2012; Vieira, 2014). In its essence, it is similar to other error classification approaches, such as (Flanagan, 1994; Font-Llitj´os et al., 2005; 3644 Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Blain et al., 2011; Stymne, 2011; Koponen, 2012; Koponen et al., 2012; Fishel et al., 2012; Stymne et al., 2012; Vieira, 2014; Avramidis et al., 2014). It is enriched, however by error ranking, based on information specifying which errors require more cognitive effort to be corrected, and which less. In this way, the approach only requires counting the number of errors of each type in the MT output. And thus it allows the comparison of the post-editing cost of different output texts of the same MT engine, the same text as an output of different MT engines, or for different language pairs. Temnikova (2010) tested her approach"
L16-1578,temnikova-etal-2012-clcm,1,0.861661,"cifying which errors require more cognitive effort to be corrected, and which less. In this way, the approach only requires counting the number of errors of each type in the MT output. And thus it allows the comparison of the post-editing cost of different output texts of the same MT engine, the same text as an output of different MT engines, or for different language pairs. Temnikova (2010) tested her approach on two emergency instructions texts, one original (called “Complex”) and one manually simplified (called “Simplified”), according to Controlled Language (CL) text simplification rules (Temnikova et al., 2012). Both texts were translated using the web version of Google Translate into three languages: Russian, Spanish, and Bulgarian. The MT output was manually post-edited by 3-5 human translators per language and then the number of errors per category was manually counted by one annotator per language. Several researchers based their work on Temnikova (2010)’s cognitive evaluation approach. Among them, Koponen et al. (2012) have modified the error classification by adding one additional class: “Typographical, upper/lowercase or similar orthographical edits”, and splitting the “Incorrect Word” (InW)"
L16-1578,temnikova-2010-cognitive,1,0.356794,"f different difficulty to be corrected, fair compensation of post-editing should take into account the difficulty of the task, which should thus be measured in the most reliable way. The best solution for this would be to build an automatic classifier which (a) assigns each MT error into a specific correction class, (b) assigns an effort value which reflects the cognitive effort a post-editor needs to make in order to make such a correction, and (c) gives a post-editing effort score to a text. On our way of building such a classifier, we investigate whether an existing cognitive effort model (Temnikova, 2010) could provide a fairer compensation for the post-editor, by testing it on a new language which strongly differs from the previous languages on which this methodology was tested. The model made use of the Statistical Machine Translation (SMT) error classification schema proposed in Vilar et al. (2006), from which the error classes were subsequently regrouped and ranked in an increasing order, so as to reflect the cognitive load post-editors experience while correcting the MT output. Error re-grouping and ranking was done on the basis of relevant psycholinguistic error correction litera1 https:"
L16-1578,vilar-etal-2006-error,0,0.113944,"Missing"
L16-1578,zaghouani-etal-2014-large,1,0.906904,"Missing"
L16-1578,W15-1614,1,0.887222,"Missing"
L16-1578,L16-1295,1,0.826208,"Missing"
L16-1578,2012.amta-wptp.2,0,\N,Missing
L16-1578,W14-3605,1,\N,Missing
L16-1578,W12-3123,0,\N,Missing
L18-1111,al-sabbagh-girju-2010-mining,0,0.0722465,"Missing"
L18-1111,I13-1048,0,0.0132906,"Arabic that was trained on MSA treebanks. Similarly, Sawaf (2010) worked on processing Dialectal Arabic using the training data from the standard Arabic Penn Treebank while Salama (2014) created an automatically annotated large-scale multidialectal Arabic corpus collected from user comments on Youtube videos. Their corpus covers five regions the Arab world, namely: Egypt, Gulf, Iraqi, Maghrebi and Levantine. Some other works such as Sajjad et al. (2013), Salloum and Habash (2013) and Sawaf (2010) used a translation of the dialectal Arabic to Standard Arabic as a pivot to translate to English. Boujelbane et al. (2013) created a dictionary based on the relation between Tunisian Arabic and MSA. Some other researchers followed crowdsourcing based approaches to create interesting resources such as the work of Zbib et al. (2012). At the regional level, we noted limited efforts focused on dialect identification such as in (Habash et al., 2008; Elfardy & Diab, 2013; Zaidan & Callison-Burch, 2013). As the Dialectal Arabic (DA) is becoming the language of informal online communication in emails, chats, SMS and in social media, we witnessed several efforts on creating different resources to help to build related too"
L18-1111,E06-1047,0,0.013219,"u 694 For Dialectal Arabic (DA), some limited efforts were made to create resources for some major Arabic dialects such as Egyptian and Levantine (Habash et al., 2013; Diab & Habash, 2007; Pasha et al., 2014). Within the framework of the Qatar Arabic Language Bank (QALB) project, a largescale annotated corpus of users’ comments was produced, dialectal words were marked (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016a.) Al-Sabbagh and Girju (2010) presented a method to extract information from the Internet in order to build a Dialectal to Modern Standard Arabic lexicon. Chiang et al. (2006) created a parser for Dialectal Arabic that was trained on MSA treebanks. Similarly, Sawaf (2010) worked on processing Dialectal Arabic using the training data from the standard Arabic Penn Treebank while Salama (2014) created an automatically annotated large-scale multidialectal Arabic corpus collected from user comments on Youtube videos. Their corpus covers five regions the Arab world, namely: Egypt, Gulf, Iraqi, Maghrebi and Levantine. Some other works such as Sajjad et al. (2013), Salloum and Habash (2013) and Sawaf (2010) used a translation of the dialectal Arabic to Standard Arabic as a"
L18-1111,N07-5003,0,0.25508,"notated with syntactic and semantic information such as the different iterations of the Penn Arabic Probanks (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and treebanks (Maamouri et al., 2010). Many tools and methods were developed to deal with the morphology, disambiguation (Zaghouani et al., 2016c), the diacritization (Zaghouani et al., 2016b) and syntactic parsing (Habash, 2010). http://arap.qatar.cmu.edu 694 For Dialectal Arabic (DA), some limited efforts were made to create resources for some major Arabic dialects such as Egyptian and Levantine (Habash et al., 2013; Diab & Habash, 2007; Pasha et al., 2014). Within the framework of the Qatar Arabic Language Bank (QALB) project, a largescale annotated corpus of users’ comments was produced, dialectal words were marked (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016a.) Al-Sabbagh and Girju (2010) presented a method to extract information from the Internet in order to build a Dialectal to Modern Standard Arabic lexicon. Chiang et al. (2006) created a parser for Dialectal Arabic that was trained on MSA treebanks. Similarly, Sawaf (2010) worked on processing Dialectal Arabic using the training data from th"
L18-1111,P13-2081,0,0.275231,"b world, namely: Egypt, Gulf, Iraqi, Maghrebi and Levantine. Some other works such as Sajjad et al. (2013), Salloum and Habash (2013) and Sawaf (2010) used a translation of the dialectal Arabic to Standard Arabic as a pivot to translate to English. Boujelbane et al. (2013) created a dictionary based on the relation between Tunisian Arabic and MSA. Some other researchers followed crowdsourcing based approaches to create interesting resources such as the work of Zbib et al. (2012). At the regional level, we noted limited efforts focused on dialect identification such as in (Habash et al., 2008; Elfardy & Diab, 2013; Zaidan & Callison-Burch, 2013). As the Dialectal Arabic (DA) is becoming the language of informal online communication in emails, chats, SMS and in social media, we witnessed several efforts on creating different resources to help to build related tools and applications. However, most of these efforts were disconnected from each other and they have only focused on a limited number of dialects in the Arab world. countries using the geolocation information associated with Twitter data. More recently, Bouamor et al. (2018) and Habash et al. (2018) created MADAR, an Arabic dialect corpus and lex"
L18-1111,N13-1044,0,0.0436822,"d parallel corpora annotated with syntactic and semantic information such as the different iterations of the Penn Arabic Probanks (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and treebanks (Maamouri et al., 2010). Many tools and methods were developed to deal with the morphology, disambiguation (Zaghouani et al., 2016c), the diacritization (Zaghouani et al., 2016b) and syntactic parsing (Habash, 2010). http://arap.qatar.cmu.edu 694 For Dialectal Arabic (DA), some limited efforts were made to create resources for some major Arabic dialects such as Egyptian and Levantine (Habash et al., 2013; Diab & Habash, 2007; Pasha et al., 2014). Within the framework of the Qatar Arabic Language Bank (QALB) project, a largescale annotated corpus of users’ comments was produced, dialectal words were marked (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016a.) Al-Sabbagh and Girju (2010) presented a method to extract information from the Internet in order to build a Dialectal to Modern Standard Arabic lexicon. Chiang et al. (2006) created a parser for Dialectal Arabic that was trained on MSA treebanks. Similarly, Sawaf (2010) worked on processing Dialectal Arabic using the"
L18-1111,pasha-etal-2014-madamira,0,0.134301,"c and semantic information such as the different iterations of the Penn Arabic Probanks (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and treebanks (Maamouri et al., 2010). Many tools and methods were developed to deal with the morphology, disambiguation (Zaghouani et al., 2016c), the diacritization (Zaghouani et al., 2016b) and syntactic parsing (Habash, 2010). http://arap.qatar.cmu.edu 694 For Dialectal Arabic (DA), some limited efforts were made to create resources for some major Arabic dialects such as Egyptian and Levantine (Habash et al., 2013; Diab & Habash, 2007; Pasha et al., 2014). Within the framework of the Qatar Arabic Language Bank (QALB) project, a largescale annotated corpus of users’ comments was produced, dialectal words were marked (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016a.) Al-Sabbagh and Girju (2010) presented a method to extract information from the Internet in order to build a Dialectal to Modern Standard Arabic lexicon. Chiang et al. (2006) created a parser for Dialectal Arabic that was trained on MSA treebanks. Similarly, Sawaf (2010) worked on processing Dialectal Arabic using the training data from the standard Arabic Pen"
L18-1111,2010.amta-papers.5,0,0.0444474,"dialects such as Egyptian and Levantine (Habash et al., 2013; Diab & Habash, 2007; Pasha et al., 2014). Within the framework of the Qatar Arabic Language Bank (QALB) project, a largescale annotated corpus of users’ comments was produced, dialectal words were marked (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016a.) Al-Sabbagh and Girju (2010) presented a method to extract information from the Internet in order to build a Dialectal to Modern Standard Arabic lexicon. Chiang et al. (2006) created a parser for Dialectal Arabic that was trained on MSA treebanks. Similarly, Sawaf (2010) worked on processing Dialectal Arabic using the training data from the standard Arabic Penn Treebank while Salama (2014) created an automatically annotated large-scale multidialectal Arabic corpus collected from user comments on Youtube videos. Their corpus covers five regions the Arab world, namely: Egypt, Gulf, Iraqi, Maghrebi and Levantine. Some other works such as Sajjad et al. (2013), Salloum and Habash (2013) and Sawaf (2010) used a translation of the dialectal Arabic to Standard Arabic as a pivot to translate to English. Boujelbane et al. (2013) created a dictionary based on the relati"
L18-1111,N12-1006,0,0.0829751,"tated large-scale multidialectal Arabic corpus collected from user comments on Youtube videos. Their corpus covers five regions the Arab world, namely: Egypt, Gulf, Iraqi, Maghrebi and Levantine. Some other works such as Sajjad et al. (2013), Salloum and Habash (2013) and Sawaf (2010) used a translation of the dialectal Arabic to Standard Arabic as a pivot to translate to English. Boujelbane et al. (2013) created a dictionary based on the relation between Tunisian Arabic and MSA. Some other researchers followed crowdsourcing based approaches to create interesting resources such as the work of Zbib et al. (2012). At the regional level, we noted limited efforts focused on dialect identification such as in (Habash et al., 2008; Elfardy & Diab, 2013; Zaidan & Callison-Burch, 2013). As the Dialectal Arabic (DA) is becoming the language of informal online communication in emails, chats, SMS and in social media, we witnessed several efforts on creating different resources to help to build related tools and applications. However, most of these efforts were disconnected from each other and they have only focused on a limited number of dialects in the Arab world. countries using the geolocation information as"
L18-1111,palmer-etal-2008-pilot,1,0.875284,"Missing"
L18-1111,L18-1574,1,0.779129,"identification such as in (Habash et al., 2008; Elfardy & Diab, 2013; Zaidan & Callison-Burch, 2013). As the Dialectal Arabic (DA) is becoming the language of informal online communication in emails, chats, SMS and in social media, we witnessed several efforts on creating different resources to help to build related tools and applications. However, most of these efforts were disconnected from each other and they have only focused on a limited number of dialects in the Arab world. countries using the geolocation information associated with Twitter data. More recently, Bouamor et al. (2018) and Habash et al. (2018) created MADAR, an Arabic dialect corpus and lexicon covering dialects of various cities across the Arab world. In the context of our ARAP project, co-organizers of the Author Profiling Share task PAN 20172 presented a dialectal Arabic corpus from four different regions (North Africa, Egypt, Levantine and Gulf). The corpus data were annotated with respect to age, gender and dialect (Rangel et al, 2017). For both Mubarak and Darwish (2014) and Rangel et al. (2017), the coverage was limited to only four countries out of 22 Arabic countries. In the ARAP project, we extend this coverage to all Ara"
L18-1111,maamouri-etal-2010-speech,1,0.757598,"us and the respective data collection and validation processes (Section 4). After that, we present our annotation guidelines and workflow (Section 5). Finally, we present the evaluation of the annotation quality (Section 6). 2. Related Work In the context of corpus creation for the modern standard Arabic, there are several efforts (Habash, 2010). In fact, there are many monolingual and parallel corpora annotated with syntactic and semantic information such as the different iterations of the Penn Arabic Probanks (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and treebanks (Maamouri et al., 2010). Many tools and methods were developed to deal with the morphology, disambiguation (Zaghouani et al., 2016c), the diacritization (Zaghouani et al., 2016b) and syntactic parsing (Habash, 2010). http://arap.qatar.cmu.edu 694 For Dialectal Arabic (DA), some limited efforts were made to create resources for some major Arabic dialects such as Egyptian and Levantine (Habash et al., 2013; Diab & Habash, 2007; Pasha et al., 2014). Within the framework of the Qatar Arabic Language Bank (QALB) project, a largescale annotated corpus of users’ comments was produced, dialectal words were marked (Zaghouani"
L18-1111,W14-3601,0,0.515421,"a limited number of dialects in the Arab world. countries using the geolocation information associated with Twitter data. More recently, Bouamor et al. (2018) and Habash et al. (2018) created MADAR, an Arabic dialect corpus and lexicon covering dialects of various cities across the Arab world. In the context of our ARAP project, co-organizers of the Author Profiling Share task PAN 20172 presented a dialectal Arabic corpus from four different regions (North Africa, Egypt, Levantine and Gulf). The corpus data were annotated with respect to age, gender and dialect (Rangel et al, 2017). For both Mubarak and Darwish (2014) and Rangel et al. (2017), the coverage was limited to only four countries out of 22 Arabic countries. In the ARAP project, we extend this coverage to all Arabid major dialects by covering 11 distinct dialects. The provided corpus can be used for applications in various domains such as cyber-security, business (e.g., for marketing and customer segmentation) and in healthcare (e.g., for suicide prevention). 3. Dialectal Arabic The Arabic language used in social media and online is a mix of Modern Standard Arabic (MSA) and other regional dialectal varieties. For this reason, it is important to r"
L18-1111,salama-etal-2014-youdacc,0,0.40103,"Missing"
L18-1111,L16-1295,1,0.906391,"Missing"
L18-1111,L16-1577,1,0.796479,"tation guidelines and workflow (Section 5). Finally, we present the evaluation of the annotation quality (Section 6). 2. Related Work In the context of corpus creation for the modern standard Arabic, there are several efforts (Habash, 2010). In fact, there are many monolingual and parallel corpora annotated with syntactic and semantic information such as the different iterations of the Penn Arabic Probanks (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and treebanks (Maamouri et al., 2010). Many tools and methods were developed to deal with the morphology, disambiguation (Zaghouani et al., 2016c), the diacritization (Zaghouani et al., 2016b) and syntactic parsing (Habash, 2010). http://arap.qatar.cmu.edu 694 For Dialectal Arabic (DA), some limited efforts were made to create resources for some major Arabic dialects such as Egyptian and Levantine (Habash et al., 2013; Diab & Habash, 2007; Pasha et al., 2014). Within the framework of the Qatar Arabic Language Bank (QALB) project, a largescale annotated corpus of users’ comments was produced, dialectal words were marked (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016a.) Al-Sabbagh and Girju (2010) presented a me"
L18-1111,W15-1614,1,0.862845,"d methods were developed to deal with the morphology, disambiguation (Zaghouani et al., 2016c), the diacritization (Zaghouani et al., 2016b) and syntactic parsing (Habash, 2010). http://arap.qatar.cmu.edu 694 For Dialectal Arabic (DA), some limited efforts were made to create resources for some major Arabic dialects such as Egyptian and Levantine (Habash et al., 2013; Diab & Habash, 2007; Pasha et al., 2014). Within the framework of the Qatar Arabic Language Bank (QALB) project, a largescale annotated corpus of users’ comments was produced, dialectal words were marked (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016a.) Al-Sabbagh and Girju (2010) presented a method to extract information from the Internet in order to build a Dialectal to Modern Standard Arabic lexicon. Chiang et al. (2006) created a parser for Dialectal Arabic that was trained on MSA treebanks. Similarly, Sawaf (2010) worked on processing Dialectal Arabic using the training data from the standard Arabic Penn Treebank while Salama (2014) created an automatically annotated large-scale multidialectal Arabic corpus collected from user comments on Youtube videos. Their corpus covers five regions the Arab world, namely:"
L18-1111,zaghouani-etal-2014-large,1,0.956843,"f the Qatar Arabic Language Bank (QALB) project, a largescale annotated corpus of users’ comments was produced, dialectal words were marked (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016a.) Al-Sabbagh and Girju (2010) presented a method to extract information from the Internet in order to build a Dialectal to Modern Standard Arabic lexicon. Chiang et al. (2006) created a parser for Dialectal Arabic that was trained on MSA treebanks. Similarly, Sawaf (2010) worked on processing Dialectal Arabic using the training data from the standard Arabic Penn Treebank while Salama (2014) created an automatically annotated large-scale multidialectal Arabic corpus collected from user comments on Youtube videos. Their corpus covers five regions the Arab world, namely: Egypt, Gulf, Iraqi, Maghrebi and Levantine. Some other works such as Sajjad et al. (2013), Salloum and Habash (2013) and Sawaf (2010) used a translation of the dialectal Arabic to Standard Arabic as a pivot to translate to English. Boujelbane et al. (2013) created a dictionary based on the relation between Tunisian Arabic and MSA. Some other researchers followed crowdsourcing based approaches to create interesting"
L18-1111,W12-2511,1,0.90403,"(Section 3). Then, we present our corpus and the respective data collection and validation processes (Section 4). After that, we present our annotation guidelines and workflow (Section 5). Finally, we present the evaluation of the annotation quality (Section 6). 2. Related Work In the context of corpus creation for the modern standard Arabic, there are several efforts (Habash, 2010). In fact, there are many monolingual and parallel corpora annotated with syntactic and semantic information such as the different iterations of the Penn Arabic Probanks (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and treebanks (Maamouri et al., 2010). Many tools and methods were developed to deal with the morphology, disambiguation (Zaghouani et al., 2016c), the diacritization (Zaghouani et al., 2016b) and syntactic parsing (Habash, 2010). http://arap.qatar.cmu.edu 694 For Dialectal Arabic (DA), some limited efforts were made to create resources for some major Arabic dialects such as Egyptian and Levantine (Habash et al., 2013; Diab & Habash, 2007; Pasha et al., 2014). Within the framework of the Qatar Arabic Language Bank (QALB) project, a largescale annotated corpus of users’ comments was produced,"
L18-1111,W10-1836,1,0.838231,"alectal Arabic varieties (Section 3). Then, we present our corpus and the respective data collection and validation processes (Section 4). After that, we present our annotation guidelines and workflow (Section 5). Finally, we present the evaluation of the annotation quality (Section 6). 2. Related Work In the context of corpus creation for the modern standard Arabic, there are several efforts (Habash, 2010). In fact, there are many monolingual and parallel corpora annotated with syntactic and semantic information such as the different iterations of the Penn Arabic Probanks (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and treebanks (Maamouri et al., 2010). Many tools and methods were developed to deal with the morphology, disambiguation (Zaghouani et al., 2016c), the diacritization (Zaghouani et al., 2016b) and syntactic parsing (Habash, 2010). http://arap.qatar.cmu.edu 694 For Dialectal Arabic (DA), some limited efforts were made to create resources for some major Arabic dialects such as Egyptian and Levantine (Habash et al., 2013; Diab & Habash, 2007; Pasha et al., 2014). Within the framework of the Qatar Arabic Language Bank (QALB) project, a largescale annotated corpus of users"
L18-1415,W15-3206,0,0.310868,"tactic annotation and morphological tokenization for Arabic. In general, many of these existing tools are not designed to handle the peculiarities of dialectal Arabic. They neither provide facilities for managing thousands of documents nor permit the distribution of tasks to tens of annotators, including managing inter-annotator agreement (IAA) tasks. Our interface borrows ideas from three other existing annotation tools: DIWAN, QAWI, and MANDIAC. Here we describe each of these tools and how they have influenced the design of our system. DIWAN is an annotation tool for Arabic dialectal texts (Al-Shargi and Rambow, 2015). It provides annotators with a set of tools for reducing duplicate effort including the use of morphological analyzers to pre-compute analyses, and the ability to apply analyses to multiple occurrences simultaneously. However, it requires installation on a Windows machine and the user interface is not very friendly to newcomers. QAWI (the QALB Annotation Web Interface) was used for token-based text editing to create raw and and text corrected parallel data for automatic text correction tasks (Obeid et al., 2013; Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016). It suppo"
L18-1415,L16-1646,0,0.0160278,"al. (2012), Stymne (2011), Llitjós and Carbonell (2004), and Dickinson and Ledbetter (2012). For Arabic, there are several existing annotation tools, however, they are designed to handle specific NLP tasks and are not easy to adapt to our project. Examples include tools for semantic annotation such as the work of Saleh and Al-Khalifa (2009) and El-ghobashy et al. (2014), 2616 3 ¯⌦ P@Y” madAriy means ‘my orbit’ in Arabic. and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool. Recently, Al-Twairesh et al. (2016) introduced MADAD, a general-purpose online collaborative annotation tool for readability assessments project in Arabic. In the COLABA initiative (Diab et al., 2010), the authors built tools and resources to process Arabic social media data such as blogs, discussion forums, and chats. Javed et al. (2018) presented an online interface for joint syntactic annotation and morphological tokenization for Arabic. In general, many of these existing tools are not designed to handle the peculiarities of dialectal Arabic. They neither provide facilities for managing thousands of documents nor permit the"
L18-1415,aziz-etal-2012-pet,0,0.0188875,"Missing"
L18-1415,dickinson-ledbetter-2012-annotating,0,0.0264408,"of our annotation framework. In Section 4. and Section 5., we discuss the annotation and management interfaces, respectively. We finally describe a user study of working with MADARi in Section 6. 2. Related Work Several annotation tools and interfaces were proposed for many languages and to achieve various annotation tasks. Some are general purpose annotation tools, such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013). Task-specific annotation tools for post-editing and error correction include the work of Aziz et al. (2012), Stymne (2011), Llitjós and Carbonell (2004), and Dickinson and Ledbetter (2012). For Arabic, there are several existing annotation tools, however, they are designed to handle specific NLP tasks and are not easy to adapt to our project. Examples include tools for semantic annotation such as the work of Saleh and Al-Khalifa (2009) and El-ghobashy et al. (2014), 2616 3 ¯⌦ P@Y” madAriy means ‘my orbit’ in Arabic. and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool. Recently, Al-Twairesh et al. (2016) introduced MADAD, a general-purpose online collaborative annotation tool for readabi"
L18-1415,L18-1574,1,0.916519,"the word i.J⌦ mÃ '@AÎÒK. AK⌦  wyAbwhAAlxlyj1 involves two spelling errors2 (a word merge and character replacement) which can be corrected as i.J⌦ mÃ '@ AÎÒK. Ag.  wjAbwhA Alxlyj ‘and they brought it to the Gulf’. Furthermore, the first of the two corrected words includes two clitics that when segmented produce the form: AÎ+ @ÒK. Ag. +  w+ jAbwA +hA ‘and+ they-brought +it’. 1 Transliterations are in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007). 2 Since Arabic dialects do not have a standard orthography, spelling correction here means to conventionalize as per the CODA standard (Habash et al., 2018). Previous work on Arabic morphology annotation interfaces focused either on the problem of manual annotations for POS tagging (Maamouri et al., 2014), or diacritization (Obeid et al., 2016), or spelling correction (Obeid et al., 2013). In this paper we present a tool that allows doing all of these tasks together, eliminating the possibility of error propagation from one annotation level to another. Our tool is named MADARi3 after the project under which it was created: Multi-Arabic Dialect Annotations and Resources (Bouamor et al., 2018). The remainder of this paper is structured as follows:"
L18-1415,L18-1345,1,0.571093,"on such as the work of Saleh and Al-Khalifa (2009) and El-ghobashy et al. (2014), 2616 3 ¯⌦ P@Y” madAriy means ‘my orbit’ in Arabic. and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool. Recently, Al-Twairesh et al. (2016) introduced MADAD, a general-purpose online collaborative annotation tool for readability assessments project in Arabic. In the COLABA initiative (Diab et al., 2010), the authors built tools and resources to process Arabic social media data such as blogs, discussion forums, and chats. Javed et al. (2018) presented an online interface for joint syntactic annotation and morphological tokenization for Arabic. In general, many of these existing tools are not designed to handle the peculiarities of dialectal Arabic. They neither provide facilities for managing thousands of documents nor permit the distribution of tasks to tens of annotators, including managing inter-annotator agreement (IAA) tasks. Our interface borrows ideas from three other existing annotation tools: DIWAN, QAWI, and MANDIAC. Here we describe each of these tools and how they have influenced the design of our system. DIWAN is an"
L18-1415,L18-1607,1,0.939775,"al., 2018). MANDIAC utilized the token-based editor used in QAWI to perform text diacritization tasks (Obeid et al., 2016). More importantly, it introduced a flexible hybrid data storage system that allows for adding new features to the annotation front-end with little to no modifications to the back-end. MADARi utilizes this design to provide the same utility. 3. MADARi Design The MADARi interface is designed to be used by human annotators to create a morphologically annotated corpus of Arabic text. The text we work with comes from social media and is highly dialectal (Bouamor et al., 2018; Khalifa et al., 2018) and has numerous spelling errors. The annotators will carefully correct the spelling of the words and also annotate their morphology. The in-context morphology annotation includes tokenization, POS tagging, lemmatization and English glossing. 3.1. Desiderata In order to manage and process the annotation of the large scale dialectal Arabic corpus, we needed to create a tool to streamline the annotation process. The desiderata for developing the MADARi annotation tool include the following: • The tool must have very minimal requirements on the annotators. • The tool must allow off-site data man"
L18-1415,font-llitjos-carbonell-2004-translation,0,0.0249819,"iscuss the design and architecture of our annotation framework. In Section 4. and Section 5., we discuss the annotation and management interfaces, respectively. We finally describe a user study of working with MADARi in Section 6. 2. Related Work Several annotation tools and interfaces were proposed for many languages and to achieve various annotation tasks. Some are general purpose annotation tools, such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013). Task-specific annotation tools for post-editing and error correction include the work of Aziz et al. (2012), Stymne (2011), Llitjós and Carbonell (2004), and Dickinson and Ledbetter (2012). For Arabic, there are several existing annotation tools, however, they are designed to handle specific NLP tasks and are not easy to adapt to our project. Examples include tools for semantic annotation such as the work of Saleh and Al-Khalifa (2009) and El-ghobashy et al. (2014), 2616 3 ¯⌦ P@Y” madAriy means ‘my orbit’ in Arabic. and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool. Recently, Al-Twairesh et al. (2016) introduced MADAD, a general-purpose online colla"
L18-1415,maamouri-etal-2014-developing,1,0.844143,"mÃ '@ AÎÒK. Ag.  wjAbwhA Alxlyj ‘and they brought it to the Gulf’. Furthermore, the first of the two corrected words includes two clitics that when segmented produce the form: AÎ+ @ÒK. Ag. +  w+ jAbwA +hA ‘and+ they-brought +it’. 1 Transliterations are in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007). 2 Since Arabic dialects do not have a standard orthography, spelling correction here means to conventionalize as per the CODA standard (Habash et al., 2018). Previous work on Arabic morphology annotation interfaces focused either on the problem of manual annotations for POS tagging (Maamouri et al., 2014), or diacritization (Obeid et al., 2016), or spelling correction (Obeid et al., 2013). In this paper we present a tool that allows doing all of these tasks together, eliminating the possibility of error propagation from one annotation level to another. Our tool is named MADARi3 after the project under which it was created: Multi-Arabic Dialect Annotations and Resources (Bouamor et al., 2018). The remainder of this paper is structured as follows: we present work related to this effort in Section 2. In Section 3., we discuss the design and architecture of our annotation framework. In Section 4."
L18-1415,I13-2001,1,0.898118,"st of the two corrected words includes two clitics that when segmented produce the form: AÎ+ @ÒK. Ag. +  w+ jAbwA +hA ‘and+ they-brought +it’. 1 Transliterations are in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007). 2 Since Arabic dialects do not have a standard orthography, spelling correction here means to conventionalize as per the CODA standard (Habash et al., 2018). Previous work on Arabic morphology annotation interfaces focused either on the problem of manual annotations for POS tagging (Maamouri et al., 2014), or diacritization (Obeid et al., 2016), or spelling correction (Obeid et al., 2013). In this paper we present a tool that allows doing all of these tasks together, eliminating the possibility of error propagation from one annotation level to another. Our tool is named MADARi3 after the project under which it was created: Multi-Arabic Dialect Annotations and Resources (Bouamor et al., 2018). The remainder of this paper is structured as follows: we present work related to this effort in Section 2. In Section 3., we discuss the design and architecture of our annotation framework. In Section 4. and Section 5., we discuss the annotation and management interfaces, respectively. We"
L18-1415,pasha-etal-2014-madamira,1,0.866692,"al., 2016). In particular, we utilized the client-server architecture, as well as the flexible hybrid SQL/JSON storage system used by MANDIAC. This allows us to easily extend our annotation interface with minor changes, if any, to the back-end. Our system stores documents one sentence per row, unlike MANDIAC which stores one document per row. This modification allows the annotation interface to handle larger file sizes without affecting its performance by only overwriting the JSON of the modified sentences and not that of the entire document. Like, DIWAN and MANDIAC, we also utilize MADAMIRA (Pasha et al., 2014), a morphological analyzer and disambiguator for Arabic to pre-compute analyses. 4. Annotation Interface The annotation interface (illustrated in Figures 1 to 4) is where annotators perform the annotation tasks assigned to them. Here we describe the different components and utilities provided this interface. 4.1. Task Overview When starting an annotation session, annotators are first shown the “Task Overview” screen (Figure 1). Here annotators can see information on the size of the current task and their progress so far (Figure 1a). The sentence list can be filtered to contain sentences matchi"
L18-1415,E12-2021,0,0.168509,"Missing"
L18-1415,P11-4010,0,0.0125903,"ection 3., we discuss the design and architecture of our annotation framework. In Section 4. and Section 5., we discuss the annotation and management interfaces, respectively. We finally describe a user study of working with MADARi in Section 6. 2. Related Work Several annotation tools and interfaces were proposed for many languages and to achieve various annotation tasks. Some are general purpose annotation tools, such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013). Task-specific annotation tools for post-editing and error correction include the work of Aziz et al. (2012), Stymne (2011), Llitjós and Carbonell (2004), and Dickinson and Ledbetter (2012). For Arabic, there are several existing annotation tools, however, they are designed to handle specific NLP tasks and are not easy to adapt to our project. Examples include tools for semantic annotation such as the work of Saleh and Al-Khalifa (2009) and El-ghobashy et al. (2014), 2616 3 ¯⌦ P@Y” madAriy means ‘my orbit’ in Arabic. and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool. Recently, Al-Twairesh et al. (2016) introduced MADAD,"
L18-1415,P13-4001,0,0.0513893,"Missing"
L18-1415,zaghouani-etal-2014-large,1,0.850041,"WAN is an annotation tool for Arabic dialectal texts (Al-Shargi and Rambow, 2015). It provides annotators with a set of tools for reducing duplicate effort including the use of morphological analyzers to pre-compute analyses, and the ability to apply analyses to multiple occurrences simultaneously. However, it requires installation on a Windows machine and the user interface is not very friendly to newcomers. QAWI (the QALB Annotation Web Interface) was used for token-based text editing to create raw and and text corrected parallel data for automatic text correction tasks (Obeid et al., 2013; Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016). It supported the exact recording of all modifications performed by the annotator which previous tools did not. We utilize this token-based editing system for minor text corrections that transform text of a given dialect into the appropriate CODA orthography (Habash et al., 2018). MANDIAC utilized the token-based editor used in QAWI to perform text diacritization tasks (Obeid et al., 2016). More importantly, it introduced a flexible hybrid data storage system that allows for adding new features to the annotation front-end with little to no modi"
L18-1415,W15-1614,1,0.860721,"l for Arabic dialectal texts (Al-Shargi and Rambow, 2015). It provides annotators with a set of tools for reducing duplicate effort including the use of morphological analyzers to pre-compute analyses, and the ability to apply analyses to multiple occurrences simultaneously. However, it requires installation on a Windows machine and the user interface is not very friendly to newcomers. QAWI (the QALB Annotation Web Interface) was used for token-based text editing to create raw and and text corrected parallel data for automatic text correction tasks (Obeid et al., 2013; Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016). It supported the exact recording of all modifications performed by the annotator which previous tools did not. We utilize this token-based editing system for minor text corrections that transform text of a given dialect into the appropriate CODA orthography (Habash et al., 2018). MANDIAC utilized the token-based editor used in QAWI to perform text diacritization tasks (Obeid et al., 2016). More importantly, it introduced a flexible hybrid data storage system that allows for adding new features to the annotation front-end with little to no modifications to the back-en"
L18-1415,L16-1295,1,0.845639,"exts (Al-Shargi and Rambow, 2015). It provides annotators with a set of tools for reducing duplicate effort including the use of morphological analyzers to pre-compute analyses, and the ability to apply analyses to multiple occurrences simultaneously. However, it requires installation on a Windows machine and the user interface is not very friendly to newcomers. QAWI (the QALB Annotation Web Interface) was used for token-based text editing to create raw and and text corrected parallel data for automatic text correction tasks (Obeid et al., 2013; Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016). It supported the exact recording of all modifications performed by the annotator which previous tools did not. We utilize this token-based editing system for minor text corrections that transform text of a given dialect into the appropriate CODA orthography (Habash et al., 2018). MANDIAC utilized the token-based editor used in QAWI to perform text diacritization tasks (Obeid et al., 2016). More importantly, it introduced a flexible hybrid data storage system that allows for adding new features to the annotation front-end with little to no modifications to the back-end. MADARi utilizes this d"
L18-1535,W14-1604,1,0.907171,"t. • Think of more than one translation into his/her dialect and carefully specify the city. • Use external informants to get more information for cities in his/her area if it is not his original city. • Enter the CODA and CAPHI versions of each entry, using the guidelines provided. Very recently, automatic DA processing has attracted a considerable amount of research in NLP (Shoufan and Alameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/ma"
L18-1535,L16-1207,1,0.824624,"lectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted in other dialects, but a native would not prod"
L18-1535,bouamor-etal-2014-multidialectal,1,0.953924,"cities has 12,000 sentences that are five-way parallel translations, and that could be used to build several Dialectal Arabic NLP applications such as machine translation. An example of a 28-way parallel sentences extracted from C ORPUS -25 is given in Figure 1.5 Translators, identified from each of the 25 cities specifically, were asked to read a set of sentences provided in English or French, and translate them into their dialects. The translators are all native speakers of the dialects of the cities they hail from. We did not choose MSA as a starting point to avoid biasing the translation (Bouamor et al., 2014). 6 4 The English, French and MSA versions we use are those provided in the IWSLT evaluation campaign (Eck and Hori, 2005). 5 The MADAR Corpus is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 6 The translation was handled by Ramitechs (http://www. ramitechs.com/), a company that creates and annotates several types of corpora and lexicons using expert linguists. 3388 English French MSA Beirut Cairo Doha Rabat Tunis Aleppo Alexandria Algiers Amman Aswan Baghdad Basra Benghazi Damascus Fes Jeddah Jerusalem Khartoum Mosul Muscat This room is too small. Cette chambre est trop pe"
L18-1535,cotterell-callison-burch-2014-multi,0,0.122726,"ri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions tha"
L18-1535,W14-3629,0,0.0380058,"these differences. Phonology An example of phonological differences is in the pronunciation of dialectal words whose MSA cog nate has the letter Qaf (  q).2 It is often observed that in Tunisian Arabic, this consonant appears as /q/ (similar to MSA), while in Egyptian and Levantine Arabic it is /P/ (glottal stop) and in Gulf Arabic it is /g/ (Haeri, 1991; Habash, 2010). Orthography While MSA has a standard orthography, the dialects do not. Often people write words reflecting the phonology or the etymology of these words. DA is sometimes written in the so-called Arabizi Romanization script (Darwish, 2014). In the context of NLP, a set of conventional orthography guidelines (CODA) has been proposed for a number of dialects (Habash et al., 2012a; Jar2 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym   ˇ ¯ ˆ ð', yˆ ø , ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (AS"
L18-1535,diab-etal-2014-tharwa,1,0.957896,"itten language of informal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified framework with common annotation guidelines and decisions, and targeting applications of Dialect Identification (DID) and Machine Translation (MT). The first resource is a large parallel corpus of 25 Arabic city dialects, in addition to the pre-existing parallel set for English, French and Modern Standard Arabic (MSA). The second resource is a 25-way lexicon of 1,045"
L18-1535,habash-etal-2012-conventional,1,0.901479,"the letter Qaf (  q).2 It is often observed that in Tunisian Arabic, this consonant appears as /q/ (similar to MSA), while in Egyptian and Levantine Arabic it is /P/ (glottal stop) and in Gulf Arabic it is /g/ (Haeri, 1991; Habash, 2010). Orthography While MSA has a standard orthography, the dialects do not. Often people write words reflecting the phonology or the etymology of these words. DA is sometimes written in the so-called Arabizi Romanization script (Darwish, 2014). In the context of NLP, a set of conventional orthography guidelines (CODA) has been proposed for a number of dialects (Habash et al., 2012a; Jar2 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym   ˇ ¯ ˆ ð', yˆ ø , ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq"
L18-1535,W12-2301,1,0.897253,"the letter Qaf (  q).2 It is often observed that in Tunisian Arabic, this consonant appears as /q/ (similar to MSA), while in Egyptian and Levantine Arabic it is /P/ (glottal stop) and in Gulf Arabic it is /g/ (Haeri, 1991; Habash, 2010). Orthography While MSA has a standard orthography, the dialects do not. Often people write words reflecting the phonology or the etymology of these words. DA is sometimes written in the so-called Arabizi Romanization script (Darwish, 2014). In the context of NLP, a set of conventional orthography guidelines (CODA) has been proposed for a number of dialects (Habash et al., 2012a; Jar2 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym   ˇ ¯ ˆ ð', yˆ ø , ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq"
L18-1535,L18-1574,1,0.896503,"I) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK AK nktb (Tunisian). Syntax Comparative studies of several Arabic dialects suggest that the syntactic differences between the dialects are relat"
L18-1535,W14-3603,1,0.936447,"Missing"
L18-1535,W14-3627,1,0.896251,"and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted in other dialects, but"
L18-1535,L16-1679,1,0.93096,"Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK AK nktb (Tunisian). Syntax Comparative studies of several Arabic di"
L18-1535,Y15-1004,0,0.20582,"h of their coverage and the fine location granularity. The focus on cities, as opposed to regions in studying Arabic dialects, opens new avenues to many areas of research from dialectology to dialect identification and machine translation. Keywords: Arabic Dialects, Parallel Corpus, Lexicon 1. Introduction 2. Dialectal Arabic (DA) is emerging nowadays as the primary written language of informal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified fra"
L18-1535,pasha-etal-2014-madamira,1,0.875164,"les from the BTEC parallel corpus. Tuples are then clustered based on their semantic similarity, such that each cluster represents a concept. The automatic process is followed by manual validation and fixing of errors resulting from the automatic process. 4.2.1. Automatic Extraction of Concept Keys Data Preprocessing Since the concept triplet words are represented in terms of lemmas, we pre-process the parallel data to map it into the lemma space. For English, we use the Stanford POS tagger (Toutanova et al., 2003) and for French, we use Treetagger (Schmid, 1994). For Arabic, we use MADAMIRA (Pasha et al., 2014) to tokenize words into the D3 scheme, which separates all clitics from the basewords. Arabic tokenization is required as the clitics attached to basewords in Arabic, are typically represented as separate words in English and French. The most common examples are the proclitic definite article + È@ Al+ ‘the’, and the enclitic possessive pronouns, such as è+ +h ‘his’. The goal here is to harmonize the forms of the three languages to encourage better word alignment and concept extraction. Triplet Extraction Our trilingual concept extraction approach focuses on collecting frequently used triplets."
L18-1535,W15-3208,1,0.95203,"hreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK AK nktb (Tunisian). Synt"
L18-1535,P13-2001,0,0.0207968,"nformal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified framework with common annotation guidelines and decisions, and targeting applications of Dialect Identification (DID) and Machine Translation (MT). The first resource is a large parallel corpus of 25 Arabic city dialects, in addition to the pre-existing parallel set for English, French and Modern Standard Arabic (MSA). The second resource is a 25-way lexicon of 1,045 entries in each city’s"
L18-1535,salama-etal-2014-youdacc,1,0.898896,"developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted i"
L18-1535,W15-3205,0,0.305387,"ponsible for. • Delete all entries that are NOT relevant to the cities he/she is responsible for. • Apply the necessary changes for some entries that may need some minor fixes. • Add new words that are not on the AUTO list. • Think of more than one translation into his/her dialect and carefully specify the city. • Use external informants to get more information for cities in his/her area if it is not his original city. • Enter the CODA and CAPHI versions of each entry, using the guidelines provided. Very recently, automatic DA processing has attracted a considerable amount of research in NLP (Shoufan and Alameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell"
L18-1535,N03-1033,0,0.0111141,"t key identification relies on an automatic process that extracts (English, French, Arabic) related tuples from the BTEC parallel corpus. Tuples are then clustered based on their semantic similarity, such that each cluster represents a concept. The automatic process is followed by manual validation and fixing of errors resulting from the automatic process. 4.2.1. Automatic Extraction of Concept Keys Data Preprocessing Since the concept triplet words are represented in terms of lemmas, we pre-process the parallel data to map it into the lemma space. For English, we use the Stanford POS tagger (Toutanova et al., 2003) and for French, we use Treetagger (Schmid, 1994). For Arabic, we use MADAMIRA (Pasha et al., 2014) to tokenize words into the D3 scheme, which separates all clitics from the basewords. Arabic tokenization is required as the clitics attached to basewords in Arabic, are typically represented as separate words in English and French. The most common examples are the proclitic definite article + È@ Al+ ‘the’, and the enclitic possessive pronouns, such as è+ +h ‘his’. The goal here is to harmonize the forms of the three languages to encourage better word alignment and concept extraction. Triplet Ex"
L18-1535,L18-1111,1,0.771723,"ons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted in other dialects, but a native would not produce naturally. The same conce"
L18-1535,P11-2007,0,0.621852,"erging nowadays as the primary written language of informal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified framework with common annotation guidelines and decisions, and targeting applications of Dialect Identification (DID) and Machine Translation (MT). The first resource is a large parallel corpus of 25 Arabic city dialects, in addition to the pre-existing parallel set for English, French and Modern Standard Arabic (MSA). The second resource is a 25-wa"
L18-1535,N12-1006,0,0.0438526,"(Shoufan and Alameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the"
L18-1535,zribi-etal-2014-conventional,1,0.960805,"ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK"
L18-1574,C16-2044,1,0.881168,"Missing"
L18-1574,2014.iwslt-papers.1,0,0.119633,"Missing"
L18-1574,P11-2062,1,0.860318,"f MSA patterns will retain the spelling choice of the MSA pattern if the difference in pronunciation can be expressed using diacritics (for vowel change or absence), or if the pronunciation is a shortened form of the MSA pattern vowels. Alif Maqsura The MSA rules for spelling the AlifMaqsura ( ø ý), which are sometimes based on roots and sometimes on patterns, apply in CODA*. 6.4.2. The Ta-Marbuta  The Ta-Marbuta ( è ¯h) is a secondary letter of the Arabic alphabet used to represent a particular suffix morpheme that is often (but not exclusively) associated with the femininesingular feature (Alkuhlani and Habash, 2011). This morpheme has a number of allomorphs with differing pronunciations. Most notably, it appears as a vowel at the end of nominals, and changes to a ∼ /t/ when followed by possessive pronominal enclitics. The Ta-Marbuta should be writ ten as è ¯h in word-final positions, regardless of its pronunciation, and following general CODA rules in non-word-final positions. See Table 3 for example cases. 6.3.3. Clitic Spelling The general rule on phonological clitic spelling is that clitics that are mapped into single letters (with possible diacritics) will be spelled attached to the word, and will n"
L18-1574,W14-3612,1,0.893572,"1 which it can be confused by speakers of dialects which do not have the phoneme [p]. Thus, pQ is included in CAPHI as /p./ as it is useful in describing the dialectal differences between Iraqi and other dialects. The complete CAPHI inventory is listed in Figure 2. 6. CODA* General Rules and Specifications While the goals of the CODA* guidelines is to precisely define the CODA choices, it is unavoidable that different versions of the guidelines will need to be presented differently for specific annotators on specific tasks for specific dialects: e.g., conversion form Arabizi to Arabic script (Bies et al., 2014), or lexicon construction (Diab et al., 2014). In this paper, we summarize and highlight specific contributions of the effort; but the full set of CODA* guidelines is described on its online page (See Section 8.). We start with a description of the technical terminology we use; then we discuss the various rules and how to use them. The border between the general rules and the specification rules is broadly drawn along the lines that general rules do not refer to any specific lexical items (morphemes or words) and pertain to the meta-mechanics of CODA; while the specification rules are lexicall"
L18-1574,L18-1535,1,0.789785,"presented in previous work; (c) the introduction of the concept of a multidialectal Seed Lexicon that is used to allow users of CODA* to have access to previous decisions when identifying spellings for new words in new dialects; and finally, (d) a set of online pages that give users easy public access to all of these resources. The CODA* guidelines and their connected resources are being used by three large Arabic dialect processing projects in three universities: The Multi-Arabic Dialect Applications and Resources at Carnegie Mellon University Qatar and New York University Abu Dhabi (NYUAD) (Bouamor et al., 2018), The Gulf Arabic Annotated Corpus (NYUAD) (Khalifa et al., 2018), and The Columbia Arabic Dialect Annotation project (Columbia University and NYUAD). The CODA* effort is large and ongoing; the goal of this paper is to introduce the effort and some of its important contributions on how to conceptualize and address the question of orthographic decisions in dialectal Arabic computational processing. The rest of the paper is structured as follows. We present common challenges to Arabic processing in Section 2. This is followed by related work in Section 3. We introduce CODA* in Section 4., and di"
L18-1574,diab-etal-2014-tharwa,1,0.959854,"sals such as the Asaakir system (‘Asaakir, 1950) and Akl’s system (Arkadiusz, 2006), neither of which are broadly used today. Various DA dictionaries used Arabic, Latin or mixed script orthographies (Badawi and Hinds, 1986). In the context of NLP, the Linguistic Data Consortium (LDC) guidelines for transcribing Levantine Arabic (Maamouri et al., 2004) and the COLABA project at Columbia University (Diab et al., 2010) were precursors to the work of Habash et al. (2012). After the CODA-Egyptian guidelines were created and used for the creation of Egyptian Arabic resources (Maamouri et al., 2014; Diab et al., 2014; Pasha et al., 2014; Eskander et al., 2013; Al-Badrashiny et al., 2014), two additional sets of guidelines were created for CODATunisian (Zribi et al., 2014) and CODA-Palestinian (Jarrar et al., 2014). These were part of projects involving morphology annotation (Palestinian) or speech recognition (Tunisian). A variant on CODA was proposed for speech recognition by Ali et al. (2014) and was shown to reduce OOV and perplexity. Since then, four more dialects followed: CODA-Algerian (Saadane and Habash, 2015), CODA-Gulf (Khalifa et al., 2016), CODA-Moroccan and CODA-Yemeni (Al-Shargi et al., 2016"
L18-1574,N13-1066,1,0.885238,"Missing"
L18-1574,habash-etal-2012-conventional,1,0.57243,"vary from MSA and from each other in terms of phonology, morphology, lexicon and syntax (Watson, 2007), using MSA orthographic standards cannot fully address the needs of the dialects. As an example of the degree of variety in dialectal spelling, Figure 1. presents the 27 actually attested spellings of one Egyptian Arabic word online. The large number of possibilities results from independent decisions such as whether the proclitic /ma/ should be written attached or separated (+Ó m+1 or AÓ mA), or whether to write the stem in a way  w), ˆ or etymology (  q). that reflects its phonology (ð' Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA); and they proposed a set of guidelines and exception lists for Egyptian Arabic. Their conventions were used in the Linguistic Data Consortium for annotating Egyptian Arabic (Maamouri et al., 2014). Since then, a number of additional efforts followed suit for other dialects (Zribi et al., 2014; Saadane and Habash, 2015; Jarrar et al., 2016; Khalifa et al., 2016). While the original CODA guidelines aimed at being easy to adjust to new dialects and contained some 1 Arabic script transliteration is presented in the Hab"
L18-1574,W14-3603,1,0.933108,"Missing"
L18-1574,L16-1679,1,0.936174,"should be written attached or separated (+Ó m+1 or AÓ mA), or whether to write the stem in a way  w), ˆ or etymology (  q). that reflects its phonology (ð' Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA); and they proposed a set of guidelines and exception lists for Egyptian Arabic. Their conventions were used in the Linguistic Data Consortium for annotating Egyptian Arabic (Maamouri et al., 2014). Since then, a number of additional efforts followed suit for other dialects (Zribi et al., 2014; Saadane and Habash, 2015; Jarrar et al., 2016; Khalifa et al., 2016). While the original CODA guidelines aimed at being easy to adjust to new dialects and contained some 1 Arabic script transliteration is presented in the Habash-SoudiBuckwalter transliteration scheme (Habash et al., 2007):   P     ¨ ¨ ¬   È Ð à è ð ø @ H. H H h. h p X XP ˇ ς γ f q k l m n hw y Â b t θ j H x dðr z s š S D T D ˇ ¯  and the additional symbols: ’ Z, Â @, A @, A @, wˆ ð', yˆ Zø', ¯h è, ý ø. Phonological forms are presented in IPA or in the CAPHI scheme, which is discussed in Section 5. Frequency ≈ 26,000 ≈ 13,000 ≤ 10,000 ≤ 1,000 ≤ 100 ≤ 10 Figure 1: 27 encountered ways"
L18-1574,L18-1607,1,0.954641,"f a multidialectal Seed Lexicon that is used to allow users of CODA* to have access to previous decisions when identifying spellings for new words in new dialects; and finally, (d) a set of online pages that give users easy public access to all of these resources. The CODA* guidelines and their connected resources are being used by three large Arabic dialect processing projects in three universities: The Multi-Arabic Dialect Applications and Resources at Carnegie Mellon University Qatar and New York University Abu Dhabi (NYUAD) (Bouamor et al., 2018), The Gulf Arabic Annotated Corpus (NYUAD) (Khalifa et al., 2018), and The Columbia Arabic Dialect Annotation project (Columbia University and NYUAD). The CODA* effort is large and ongoing; the goal of this paper is to introduce the effort and some of its important contributions on how to conceptualize and address the question of orthographic decisions in dialectal Arabic computational processing. The rest of the paper is structured as follows. We present common challenges to Arabic processing in Section 2. This is followed by related work in Section 3. We introduce CODA* in Section 4., and discuss its components in Section 5. (CAPHI), Section 6. (General R"
L18-1574,maamouri-etal-2014-developing,1,0.931749,"actually attested spellings of one Egyptian Arabic word online. The large number of possibilities results from independent decisions such as whether the proclitic /ma/ should be written attached or separated (+Ó m+1 or AÓ mA), or whether to write the stem in a way  w), ˆ or etymology (  q). that reflects its phonology (ð' Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA); and they proposed a set of guidelines and exception lists for Egyptian Arabic. Their conventions were used in the Linguistic Data Consortium for annotating Egyptian Arabic (Maamouri et al., 2014). Since then, a number of additional efforts followed suit for other dialects (Zribi et al., 2014; Saadane and Habash, 2015; Jarrar et al., 2016; Khalifa et al., 2016). While the original CODA guidelines aimed at being easy to adjust to new dialects and contained some 1 Arabic script transliteration is presented in the Habash-SoudiBuckwalter transliteration scheme (Habash et al., 2007):   P     ¨ ¨ ¬   È Ð à è ð ø @ H. H H h. h p X XP ˇ ς γ f q k l m n hw y Â b t θ j H x dðr z s š S D T D ˇ ¯  and the additional symbols: ’ Z, Â @, A @, A @, wˆ ð', yˆ Zø', ¯h è, ý ø. Phonological fo"
L18-1574,Y15-1004,0,0.179652,"Missing"
L18-1574,pasha-etal-2014-madamira,1,0.908097,"Missing"
L18-1574,W15-3208,1,0.95202,"t decisions such as whether the proclitic /ma/ should be written attached or separated (+Ó m+1 or AÓ mA), or whether to write the stem in a way  w), ˆ or etymology (  q). that reflects its phonology (ð' Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA); and they proposed a set of guidelines and exception lists for Egyptian Arabic. Their conventions were used in the Linguistic Data Consortium for annotating Egyptian Arabic (Maamouri et al., 2014). Since then, a number of additional efforts followed suit for other dialects (Zribi et al., 2014; Saadane and Habash, 2015; Jarrar et al., 2016; Khalifa et al., 2016). While the original CODA guidelines aimed at being easy to adjust to new dialects and contained some 1 Arabic script transliteration is presented in the Habash-SoudiBuckwalter transliteration scheme (Habash et al., 2007):   P     ¨ ¨ ¬   È Ð à è ð ø @ H. H H h. h p X XP ˇ ς γ f q k l m n hw y Â b t θ j H x dðr z s š S D T D ˇ ¯  and the additional symbols: ’ Z, Â @, A @, A @, wˆ ð', yˆ Zø', ¯h è, ý ø. Phonological forms are presented in IPA or in the CAPHI scheme, which is discussed in Section 5. Frequency ≈ 26,000 ≈ 13,000 ≤ 10,000 ≤ 1,"
L18-1574,zaghouani-etal-2014-large,1,0.923929,"Missing"
L18-1574,N18-1087,1,0.880283,"Missing"
L18-1574,zribi-etal-2014-conventional,1,0.961603,"ults from independent decisions such as whether the proclitic /ma/ should be written attached or separated (+Ó m+1 or AÓ mA), or whether to write the stem in a way  w), ˆ or etymology (  q). that reflects its phonology (ð' Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA); and they proposed a set of guidelines and exception lists for Egyptian Arabic. Their conventions were used in the Linguistic Data Consortium for annotating Egyptian Arabic (Maamouri et al., 2014). Since then, a number of additional efforts followed suit for other dialects (Zribi et al., 2014; Saadane and Habash, 2015; Jarrar et al., 2016; Khalifa et al., 2016). While the original CODA guidelines aimed at being easy to adjust to new dialects and contained some 1 Arabic script transliteration is presented in the Habash-SoudiBuckwalter transliteration scheme (Habash et al., 2007):   P     ¨ ¨ ¬   È Ð à è ð ø @ H. H H h. h p X XP ˇ ς γ f q k l m n hw y Â b t θ j H x dðr z s š S D T D ˇ ¯  and the additional symbols: ’ Z, Â @, A @, A @, wˆ ð', yˆ Zø', ¯h è, ý ø. Phonological forms are presented in IPA or in the CAPHI scheme, which is discussed in Section 5. Frequency ≈ 26,"
maamouri-etal-2010-speech,kulick-etal-2010-consistent,1,\N,Missing
maamouri-etal-2010-speech,maamouri-etal-2008-enhancing,1,\N,Missing
maamouri-etal-2010-speech,W04-1602,1,\N,Missing
palmer-etal-2008-pilot,W04-3212,1,\N,Missing
palmer-etal-2008-pilot,W05-0630,0,\N,Missing
palmer-etal-2008-pilot,W03-1006,0,\N,Missing
palmer-etal-2008-pilot,N07-2014,0,\N,Missing
palmer-etal-2008-pilot,P98-1013,0,\N,Missing
palmer-etal-2008-pilot,C98-1013,0,\N,Missing
palmer-etal-2008-pilot,P04-1043,0,\N,Missing
palmer-etal-2008-pilot,S07-1026,1,\N,Missing
palmer-etal-2008-pilot,W05-0620,0,\N,Missing
palmer-etal-2008-pilot,J02-3001,0,\N,Missing
palmer-etal-2008-pilot,P02-1031,1,\N,Missing
palmer-etal-2008-pilot,N04-1030,0,\N,Missing
palmer-etal-2008-pilot,N04-1032,0,\N,Missing
pouliquen-etal-2006-geocoding,E99-1001,0,\N,Missing
R19-1023,L16-1207,0,0.0320787,"Missing"
R19-1023,L18-1579,1,0.852283,"of this paper is organized as follows. Section 2 presents related work and Section 3 describes the methodology used in creating and annotating our corpus. Section 4 reports on the verification of the annotation as well as the evaluation. Section 5 discusses some challenges that we encountered when developing this corpus. Finally, Section 6 concludes the paper and outlines possible directions for future work. 198 Proceedings of Recent Advances in Natural Language Processing, pages 198–204, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_023 2 Related Work and Iraqi (Alsarsour et al., 2018). The annotation of this corpus was done through crowd sourcing. Bouamor et al. (2014) presented a multi-dialectal parallel corpus with 2,000 sentences translated to MSA, Tunisian, Jordanian, Palestinian and Syrian Arabic. Later on, MADAR was developed as a Multi-dialectal large scale corpus that provides parallel translation for 25 Arabic city dialects (Bouamor et al., 2018). All these efforts on creating Dialectal Arabic corpora either targeted some specific dialects only or did not provide the necessary annotation for author profiling such as annotation about age and gender. Most research o"
R19-1023,W14-3601,0,0.0227631,"ne-Grained Annotated Multi-Dialectal Arabic Corpus Anis Charfi Wajdi Zaghouani Information Systems College of Humanities and Social Sciences Carnegie Mellon University in Qatar Hamad Bin Khalifa University Qatar acharfi@andrew.cmu.edu wzaghouani@hbku.edu.qa Syed Hassan Mehdi Information Systems Carnegie Mellon University in Qatar smehdi@andrew.cmu.edu Esraa Mohamed Information Systems Carnegie Mellon University in Qatar emohamad@andrew.cmu.edu Abstract media a rise in the use of dialectal Arabic for informal online interactions such as those found in blogs, forums, chats, tweets, posts, etc. (Mubarak and Darwish, 2014; Bouamor et al., 2018). We present ARAP-Tweet 2.0, a corpus of 5 million dialectal Arabic tweets and 50 million words of about 3000 Twitter users from 17 Arab countries. Compared to the first version, the new corpus has significant improvements in terms of the data volume and the annotation quality. It is fully balanced with respect to dialect, gender, and three age groups: under 25 years, between 25 and 34, and 35 years and above. This paper describes the process of creating the corpus starting from gathering the dialectal phrases to find the users, to annotating their accounts and retrievin"
R19-1023,bouamor-etal-2014-multidialectal,0,0.0172873,"scribes the methodology used in creating and annotating our corpus. Section 4 reports on the verification of the annotation as well as the evaluation. Section 5 discusses some challenges that we encountered when developing this corpus. Finally, Section 6 concludes the paper and outlines possible directions for future work. 198 Proceedings of Recent Advances in Natural Language Processing, pages 198–204, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_023 2 Related Work and Iraqi (Alsarsour et al., 2018). The annotation of this corpus was done through crowd sourcing. Bouamor et al. (2014) presented a multi-dialectal parallel corpus with 2,000 sentences translated to MSA, Tunisian, Jordanian, Palestinian and Syrian Arabic. Later on, MADAR was developed as a Multi-dialectal large scale corpus that provides parallel translation for 25 Arabic city dialects (Bouamor et al., 2018). All these efforts on creating Dialectal Arabic corpora either targeted some specific dialects only or did not provide the necessary annotation for author profiling such as annotation about age and gender. Most research on Arabic NLP has focused on Modern Standard Arabic (MSA) (Habash, 2010). There are man"
R19-1023,palmer-etal-2008-pilot,1,0.631837,"was developed as a Multi-dialectal large scale corpus that provides parallel translation for 25 Arabic city dialects (Bouamor et al., 2018). All these efforts on creating Dialectal Arabic corpora either targeted some specific dialects only or did not provide the necessary annotation for author profiling such as annotation about age and gender. Most research on Arabic NLP has focused on Modern Standard Arabic (MSA) (Habash, 2010). There are many parallel and monolingual annotated data collections with syntactic and semantic information such as the different iterations of Penn Arabic Probanks (Palmer et al., 2008; Zaghouani et al., 2010, 2012) and treebanks (Maamouri et al., 2010). Based on such resources, various tools were developed for syntactic parsing and morphological analysis (Habash, 2010). Even though there are relatively many resources for MSA, Dialectal Arabic (DA) lags behind in terms of available resources. There have been some limited efforts toward creating resources for the most popular dialects such as the Egyptian and Levantine dialects which were presented in (Habash et al., 2013; Diab and Habash, 2007; Pasha et al., 2014). For example, Habash et al. created resources for morphologi"
R19-1023,pasha-etal-2014-madamira,0,0.0668832,"Missing"
R19-1023,N07-5003,0,0.0566808,"and semantic information such as the different iterations of Penn Arabic Probanks (Palmer et al., 2008; Zaghouani et al., 2010, 2012) and treebanks (Maamouri et al., 2010). Based on such resources, various tools were developed for syntactic parsing and morphological analysis (Habash, 2010). Even though there are relatively many resources for MSA, Dialectal Arabic (DA) lags behind in terms of available resources. There have been some limited efforts toward creating resources for the most popular dialects such as the Egyptian and Levantine dialects which were presented in (Habash et al., 2013; Diab and Habash, 2007; Pasha et al., 2014). For example, Habash et al. created resources for morphological analysis of Egyptian dialect (Habash et al., 2013). For their work on machine translation, Zbib et al. (2012) created Levantine-English and Egyptian-English parallel corpora using crowd sourcing. Khalifa et al. (2018) created a morphologically annotated data corpus of Emirati Dialect. Khalifa et al. (2016) created a corpus of 100M words covering various Arabic dialects. Other related projects worth to be mentioned are: the Egyptian Arabic Treebank (Maamouri et al., 2014); the Levantine Arabic Treebank (Maamou"
R19-1023,voss-etal-2014-finding,0,0.0217869,"created a corpus of 100M words covering various Arabic dialects. Other related projects worth to be mentioned are: the Egyptian Arabic Treebank (Maamouri et al., 2014); the Levantine Arabic Treebank (Maamouri et al., 2006), The Curras Palestinian Arabic annotated corpus with more than 70,000 words of various genres (Jarrar et al., 2014). Furthermore, AlShargi et al. (2016) created a Yemeni (Sanaa Dialect) dataset and also a Moroccan Arabic corpus, while Al-Twairesh et al. (2018) built SUAR, a Najdi Arabic corpus annotated with the morphological analyzer MADAMIRA (Pasha et al., 2014). Finally, Voss et al. (2014) presented a Moroccan Arabic corpus annotated for code-switching (French, Berber and Morrocan Arabic). Moreover, there have been some efforts towards creating Dialectal Arabic corpora by either translating existing corpora to dialects or by crowd sourcing annotation for data collected through various sources such as microblogs (e.g., Twitter). Along these lines, DART was developed as a Twitter based data set of dialectal Arabic covering five Arabic dialects: Egyptian, Levantine, Gulf, 3 Methodology In the following, we report on the methodology and process followed for the creation, annotation"
R19-1023,N13-1044,0,0.0278942,"ctions with syntactic and semantic information such as the different iterations of Penn Arabic Probanks (Palmer et al., 2008; Zaghouani et al., 2010, 2012) and treebanks (Maamouri et al., 2010). Based on such resources, various tools were developed for syntactic parsing and morphological analysis (Habash, 2010). Even though there are relatively many resources for MSA, Dialectal Arabic (DA) lags behind in terms of available resources. There have been some limited efforts toward creating resources for the most popular dialects such as the Egyptian and Levantine dialects which were presented in (Habash et al., 2013; Diab and Habash, 2007; Pasha et al., 2014). For example, Habash et al. created resources for morphological analysis of Egyptian dialect (Habash et al., 2013). For their work on machine translation, Zbib et al. (2012) created Levantine-English and Egyptian-English parallel corpora using crowd sourcing. Khalifa et al. (2018) created a morphologically annotated data corpus of Emirati Dialect. Khalifa et al. (2016) created a corpus of 100M words covering various Arabic dialects. Other related projects worth to be mentioned are: the Egyptian Arabic Treebank (Maamouri et al., 2014); the Levantine"
R19-1023,L18-1111,1,0.836736,"notation of gender, dialect, and age respectively. We also discuss some challenges encountered when developing this corpus. 1 The content written by the users in social media sites can reveal some characteristics and attributes about them, which is the main focus of author profiling research. However, the lack of Arabic language resources limits that research on author profiling for the Arabic language in particular dialectal Arabic. We present in this paper ARAP-Tweet 2.0, which is a large-scale manually-annotated multidialectal Arabic corpus. A first version of this corpus was presented in (Zaghouani and Charfi, 2018a,b) and it was extended significantly in terms of data volume, number of users, and annotation quality. ARAP-Tweet 2.0 covers dialects from 17 Arab countries and 15 regions. The number of users per region is 198, including a total of about 3000 users and approximately 5 million tweets. All users’ accounts were manually annotated with respect to the dialect, gender, and age. Thereby, we distinguished three age groups: under 25 years, between 25 years and 34 years, and 35 years and above. Moreover, significant effort was put in checking and improving the annotation quality. Introduction As the"
R19-1023,W10-1836,1,0.787057,"ulti-dialectal large scale corpus that provides parallel translation for 25 Arabic city dialects (Bouamor et al., 2018). All these efforts on creating Dialectal Arabic corpora either targeted some specific dialects only or did not provide the necessary annotation for author profiling such as annotation about age and gender. Most research on Arabic NLP has focused on Modern Standard Arabic (MSA) (Habash, 2010). There are many parallel and monolingual annotated data collections with syntactic and semantic information such as the different iterations of Penn Arabic Probanks (Palmer et al., 2008; Zaghouani et al., 2010, 2012) and treebanks (Maamouri et al., 2010). Based on such resources, various tools were developed for syntactic parsing and morphological analysis (Habash, 2010). Even though there are relatively many resources for MSA, Dialectal Arabic (DA) lags behind in terms of available resources. There have been some limited efforts toward creating resources for the most popular dialects such as the Egyptian and Levantine dialects which were presented in (Habash et al., 2013; Diab and Habash, 2007; Pasha et al., 2014). For example, Habash et al. created resources for morphological analysis of Egyptian"
R19-1023,L16-1679,0,0.0179056,"of available resources. There have been some limited efforts toward creating resources for the most popular dialects such as the Egyptian and Levantine dialects which were presented in (Habash et al., 2013; Diab and Habash, 2007; Pasha et al., 2014). For example, Habash et al. created resources for morphological analysis of Egyptian dialect (Habash et al., 2013). For their work on machine translation, Zbib et al. (2012) created Levantine-English and Egyptian-English parallel corpora using crowd sourcing. Khalifa et al. (2018) created a morphologically annotated data corpus of Emirati Dialect. Khalifa et al. (2016) created a corpus of 100M words covering various Arabic dialects. Other related projects worth to be mentioned are: the Egyptian Arabic Treebank (Maamouri et al., 2014); the Levantine Arabic Treebank (Maamouri et al., 2006), The Curras Palestinian Arabic annotated corpus with more than 70,000 words of various genres (Jarrar et al., 2014). Furthermore, AlShargi et al. (2016) created a Yemeni (Sanaa Dialect) dataset and also a Moroccan Arabic corpus, while Al-Twairesh et al. (2018) built SUAR, a Najdi Arabic corpus annotated with the morphological analyzer MADAMIRA (Pasha et al., 2014). Finally,"
R19-1023,W12-2511,1,0.891308,"Missing"
R19-1023,L18-1607,0,0.0329097,"h there are relatively many resources for MSA, Dialectal Arabic (DA) lags behind in terms of available resources. There have been some limited efforts toward creating resources for the most popular dialects such as the Egyptian and Levantine dialects which were presented in (Habash et al., 2013; Diab and Habash, 2007; Pasha et al., 2014). For example, Habash et al. created resources for morphological analysis of Egyptian dialect (Habash et al., 2013). For their work on machine translation, Zbib et al. (2012) created Levantine-English and Egyptian-English parallel corpora using crowd sourcing. Khalifa et al. (2018) created a morphologically annotated data corpus of Emirati Dialect. Khalifa et al. (2016) created a corpus of 100M words covering various Arabic dialects. Other related projects worth to be mentioned are: the Egyptian Arabic Treebank (Maamouri et al., 2014); the Levantine Arabic Treebank (Maamouri et al., 2006), The Curras Palestinian Arabic annotated corpus with more than 70,000 words of various genres (Jarrar et al., 2014). Furthermore, AlShargi et al. (2016) created a Yemeni (Sanaa Dialect) dataset and also a Moroccan Arabic corpus, while Al-Twairesh et al. (2018) built SUAR, a Najdi Arabi"
R19-1023,N12-1006,0,0.0331601,"various tools were developed for syntactic parsing and morphological analysis (Habash, 2010). Even though there are relatively many resources for MSA, Dialectal Arabic (DA) lags behind in terms of available resources. There have been some limited efforts toward creating resources for the most popular dialects such as the Egyptian and Levantine dialects which were presented in (Habash et al., 2013; Diab and Habash, 2007; Pasha et al., 2014). For example, Habash et al. created resources for morphological analysis of Egyptian dialect (Habash et al., 2013). For their work on machine translation, Zbib et al. (2012) created Levantine-English and Egyptian-English parallel corpora using crowd sourcing. Khalifa et al. (2018) created a morphologically annotated data corpus of Emirati Dialect. Khalifa et al. (2016) created a corpus of 100M words covering various Arabic dialects. Other related projects worth to be mentioned are: the Egyptian Arabic Treebank (Maamouri et al., 2014); the Levantine Arabic Treebank (Maamouri et al., 2006), The Curras Palestinian Arabic annotated corpus with more than 70,000 words of various genres (Jarrar et al., 2014). Furthermore, AlShargi et al. (2016) created a Yemeni (Sanaa D"
R19-1023,maamouri-etal-2006-developing,0,0.0592176,", 2007; Pasha et al., 2014). For example, Habash et al. created resources for morphological analysis of Egyptian dialect (Habash et al., 2013). For their work on machine translation, Zbib et al. (2012) created Levantine-English and Egyptian-English parallel corpora using crowd sourcing. Khalifa et al. (2018) created a morphologically annotated data corpus of Emirati Dialect. Khalifa et al. (2016) created a corpus of 100M words covering various Arabic dialects. Other related projects worth to be mentioned are: the Egyptian Arabic Treebank (Maamouri et al., 2014); the Levantine Arabic Treebank (Maamouri et al., 2006), The Curras Palestinian Arabic annotated corpus with more than 70,000 words of various genres (Jarrar et al., 2014). Furthermore, AlShargi et al. (2016) created a Yemeni (Sanaa Dialect) dataset and also a Moroccan Arabic corpus, while Al-Twairesh et al. (2018) built SUAR, a Najdi Arabic corpus annotated with the morphological analyzer MADAMIRA (Pasha et al., 2014). Finally, Voss et al. (2014) presented a Moroccan Arabic corpus annotated for code-switching (French, Berber and Morrocan Arabic). Moreover, there have been some efforts towards creating Dialectal Arabic corpora by either translatin"
R19-1023,maamouri-etal-2014-developing,0,0.0154278,"were presented in (Habash et al., 2013; Diab and Habash, 2007; Pasha et al., 2014). For example, Habash et al. created resources for morphological analysis of Egyptian dialect (Habash et al., 2013). For their work on machine translation, Zbib et al. (2012) created Levantine-English and Egyptian-English parallel corpora using crowd sourcing. Khalifa et al. (2018) created a morphologically annotated data corpus of Emirati Dialect. Khalifa et al. (2016) created a corpus of 100M words covering various Arabic dialects. Other related projects worth to be mentioned are: the Egyptian Arabic Treebank (Maamouri et al., 2014); the Levantine Arabic Treebank (Maamouri et al., 2006), The Curras Palestinian Arabic annotated corpus with more than 70,000 words of various genres (Jarrar et al., 2014). Furthermore, AlShargi et al. (2016) created a Yemeni (Sanaa Dialect) dataset and also a Moroccan Arabic corpus, while Al-Twairesh et al. (2018) built SUAR, a Najdi Arabic corpus annotated with the morphological analyzer MADAMIRA (Pasha et al., 2014). Finally, Voss et al. (2014) presented a Moroccan Arabic corpus annotated for code-switching (French, Berber and Morrocan Arabic). Moreover, there have been some efforts towards"
R19-1023,maamouri-etal-2010-speech,1,0.764392,"es parallel translation for 25 Arabic city dialects (Bouamor et al., 2018). All these efforts on creating Dialectal Arabic corpora either targeted some specific dialects only or did not provide the necessary annotation for author profiling such as annotation about age and gender. Most research on Arabic NLP has focused on Modern Standard Arabic (MSA) (Habash, 2010). There are many parallel and monolingual annotated data collections with syntactic and semantic information such as the different iterations of Penn Arabic Probanks (Palmer et al., 2008; Zaghouani et al., 2010, 2012) and treebanks (Maamouri et al., 2010). Based on such resources, various tools were developed for syntactic parsing and morphological analysis (Habash, 2010). Even though there are relatively many resources for MSA, Dialectal Arabic (DA) lags behind in terms of available resources. There have been some limited efforts toward creating resources for the most popular dialects such as the Egyptian and Levantine dialects which were presented in (Habash et al., 2013; Diab and Habash, 2007; Pasha et al., 2014). For example, Habash et al. created resources for morphological analysis of Egyptian dialect (Habash et al., 2013). For their wor"
W10-1836,W03-1006,0,0.060505,". 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument,"
W10-1836,choi-etal-2010-propbank-instance,1,0.897928,"integrated manner, how to identify one construction from the other, figuring out a language specific reliable diagnostic test, and whether we deal with these constructions as a whole unit or as separate parts; and how? (Hwang, et al., 2010) 4.2 Tools Frameset files are created in an XML format. During the Pilot Propbank project these files were created manually by editing the XML file related to a particular predicate. This proved to be time consuming and prone to many formatting errors. The Frame File creation for the revised APB is now performed with the recently developed Cornerstone tool (Choi et al., 2010a), which is a PropBank frameset editor that allows the creation and editing of Propbank framesets without requiring any prior knowledge of XML. Moreover, the annotation is now performed by Jubilee, a new annotation tool, which has improved the annotation process by displaying several types of relevant syntactic and semantic information at the same time. Having everything displayed helps the annotator quickly absorb and apply the necessary syntactic and semantic information pertinent to each predicate for consistent and efficient annotation (Choi et al., 20010b). Both tools are available as Op"
W10-1836,P08-1091,1,0.8821,"e Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be labeled as the agent/experiencer and ‘movies’ w"
W10-1836,J02-3001,0,0.0350371,"the data tagging and also simplify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the"
W10-1836,P02-1031,1,0.76542,"d multi-word expressions. New tools facilitate the data tagging and also simplify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the"
W10-1836,N07-2014,0,0.0683748,"Missing"
W10-1836,N06-2015,1,0.851769,"Missing"
W10-1836,W10-1810,1,0.834857,"ing of cohesion in the group. The APB has decided to thoroughly tackle light verb constructions and multi-word expressions as part of an effort to facilitate mapping between the different languages that are being PropBanked. In the process of setting this up a number of challenges have surfaced which include: how can we cross-linguistically approach these phenomena in a (semi) integrated manner, how to identify one construction from the other, figuring out a language specific reliable diagnostic test, and whether we deal with these constructions as a whole unit or as separate parts; and how? (Hwang, et al., 2010) 4.2 Tools Frameset files are created in an XML format. During the Pilot Propbank project these files were created manually by editing the XML file related to a particular predicate. This proved to be time consuming and prone to many formatting errors. The Frame File creation for the revised APB is now performed with the recently developed Cornerstone tool (Choi et al., 2010a), which is a PropBank frameset editor that allows the creation and editing of Propbank framesets without requiring any prior knowledge of XML. Moreover, the annotation is now performed by Jubilee, a new annotation tool, w"
W10-1836,P04-1043,0,0.0648245,"e in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be lab"
W10-1836,W05-0630,0,0.0204675,"tomated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be labeled as the agent/experi"
W10-1836,J05-1004,1,0.599345,"aper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be labeled as the agent/experiencer and ‘movies’ would be the theme/content. According to PropBank, ‘John’ is labeled Arg0 (or enjoyer) and ‘movies’ is labeled Arg1 (or thing enjoyed). Crucially, that independent of the l"
W10-1836,N04-1030,1,0.800833,"New tools facilitate the data tagging and also simplify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’"
W10-1836,W04-3212,1,0.799084,"implify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, a"
W10-1836,P80-1024,0,0.623039,"Missing"
W10-1836,P09-5003,0,\N,Missing
W10-1836,W05-0620,0,\N,Missing
W10-1836,palmer-etal-2008-pilot,1,\N,Missing
W12-2015,W04-1602,0,0.14525,"for each word token in a given passage. The entire passage, with the full set of possible SAMA solutions for each word token, was then presented to a native Arabic speaker experienced in the morphological analysis of MSA, and their task was to select the particular SAMA solution for each word based on their understanding of the context; where necessary, the annotator would manually edit the details of POS tags or glosses to fill gaps in SAMA’s coverage of the vocabulary. This is a standard approach used in the annotation of numerous Arabic text corpora, including the Arabic Treebank Project (Maamouri and Bies 2004). As described in section 5.2, the resulting annotation was fully reviewed by expert Arabic linguists using our reading facilitation tool, to identify and repair errors. A relational database was created to store the corpus and annotations. Separate tables were used to enumerate (a) the reading passages (keeping track of the book volume, chapter and page number of each passage), (b) the sequence of sentences in each passage, (c) the word token sequence for each sentence, (d) the inventory of distinct word types (i.e. orthographic word forms with their context-dependant analyses), and (e) the i"
W12-2015,W10-1002,0,0.027268,"we review some of the related work. In section 4 we discuss some of the specific challenges faced when learning the Arabic language. 3 Related work Many studies have shown that an on-line learning environment that supplements classroom instruction with additional study materials at an appropriate level for the learner may enhance language learning and development (Ware, 2004; Chiu et al., 2007; Yuan, 2003; Wang, 2005;). As a result, a number of recent projects have aimed to dynamically provide a supply of accessible authentic texts to language learners by drawing from online resources. WERTi (Meurers et al. 2010) is an intelli128 gent automatic workbook that uses texts from the Web to increase knowledge of English grammatical forms and functions. READ-X (Miltsakaki and Troutt, 2007) is a tool for finding texts at specified reading levels. SourceFinder (Sheehan et al.,2007) is an authoring tool for finding suitable texts for standardized test items on verbal reasoning and reading comprehension. Project REAP (ReaderSpecific Lexical Practice) (Brown and Eskenazi, 2004; Heilman et al., 2006) takes a different approach. Rather than teachers choosing texts, in REAP the system selects individualized practice"
W12-2015,C96-2140,0,0.160363,"Missing"
W12-2511,W11-0403,0,0.199417,"ization and information extraction. In order to build robust SRL systems there is a need for significant resources the most important of which are semantically annotated resources such as proposition banks. Several such resources exist now for different languages including FrameNet (Baker et al., 1998), VerbNet (Kipper et al. 2000) and PropBank (Palmer et al., 2005). These resources have marked a surge in efficient approaches to automatic SRL of the English language. Apart from English, there exist various PropBank projects in Chinese (Xue et al., 2009), Korean (Palmer et al. 2006) and Hindi (Ashwini et al., 2011). These resources exist on a large scale spearheading the SRL research in the associated languages (Carreras and Marquez, 2005), Surdeanu et al. (2008). However, resources created for Arabic are significantly more modest. The only Arabic Propank [APB] project (Zaghouani et al., 2010; Diab et al., 2008) based on the phrase structure syntactic Arabic Treebank (Maamouri et al. 2010) comprises a little over 4.5K verbs of newswire modern standard Arabic. Apart from the modesty in size, the Arabic language genre used in the APB does not represent the full scope of the Arabic language. The Arabic cul"
W12-2511,P98-1013,0,0.0204143,"he correlate of this characterization in natural language processing literature (Gildea and Jurafsky 2002). In SRL, the system automatically identifies predicates and their arguments and tags the identified arguments with meaningful semantic information. SRL has been successfully used in machine translation, summarization and information extraction. In order to build robust SRL systems there is a need for significant resources the most important of which are semantically annotated resources such as proposition banks. Several such resources exist now for different languages including FrameNet (Baker et al., 1998), VerbNet (Kipper et al. 2000) and PropBank (Palmer et al., 2005). These resources have marked a surge in efficient approaches to automatic SRL of the English language. Apart from English, there exist various PropBank projects in Chinese (Xue et al., 2009), Korean (Palmer et al. 2006) and Hindi (Ashwini et al., 2011). These resources exist on a large scale spearheading the SRL research in the associated languages (Carreras and Marquez, 2005), Surdeanu et al. (2008). However, resources created for Arabic are significantly more modest. The only Arabic Propank [APB] project (Zaghouani et al., 201"
W12-2511,choi-etal-2010-propbank-instance,0,0.0278624,"Missing"
W12-2511,palmer-etal-2008-pilot,1,0.935487,"erbNet (Kipper et al. 2000) and PropBank (Palmer et al., 2005). These resources have marked a surge in efficient approaches to automatic SRL of the English language. Apart from English, there exist various PropBank projects in Chinese (Xue et al., 2009), Korean (Palmer et al. 2006) and Hindi (Ashwini et al., 2011). These resources exist on a large scale spearheading the SRL research in the associated languages (Carreras and Marquez, 2005), Surdeanu et al. (2008). However, resources created for Arabic are significantly more modest. The only Arabic Propank [APB] project (Zaghouani et al., 2010; Diab et al., 2008) based on the phrase structure syntactic Arabic Treebank (Maamouri et al. 2010) comprises a little over 4.5K verbs of newswire modern standard Arabic. Apart from the modesty in size, the Arabic language genre used in the APB does not represent the full scope of the Arabic language. The Arabic culture has a long history of literary writing and a rich linguistic heritage in classical Arabic. In fact all historical religious non-religious texts are written in Classical Arabic. The ultimate source on classical Arabic language is the Quran. It is considered the Arabic language reference point for a"
W12-2511,W08-2121,0,0.0372406,"antically annotated resources such as proposition banks. Several such resources exist now for different languages including FrameNet (Baker et al., 1998), VerbNet (Kipper et al. 2000) and PropBank (Palmer et al., 2005). These resources have marked a surge in efficient approaches to automatic SRL of the English language. Apart from English, there exist various PropBank projects in Chinese (Xue et al., 2009), Korean (Palmer et al. 2006) and Hindi (Ashwini et al., 2011). These resources exist on a large scale spearheading the SRL research in the associated languages (Carreras and Marquez, 2005), Surdeanu et al. (2008). However, resources created for Arabic are significantly more modest. The only Arabic Propank [APB] project (Zaghouani et al., 2010; Diab et al., 2008) based on the phrase structure syntactic Arabic Treebank (Maamouri et al. 2010) comprises a little over 4.5K verbs of newswire modern standard Arabic. Apart from the modesty in size, the Arabic language genre used in the APB does not represent the full scope of the Arabic language. The Arabic culture has a long history of literary writing and a rich linguistic heritage in classical Arabic. In fact all historical religious non-religious texts ar"
W12-2511,W04-3212,0,0.0870691,"Missing"
W12-2511,W10-1836,1,0.844528,"Missing"
W12-2511,C98-1013,0,\N,Missing
W12-2511,W05-0620,0,\N,Missing
W12-2511,J02-3001,0,\N,Missing
W14-3605,C12-2011,0,0.247501,"Missing"
W14-3605,W14-3620,0,0.347563,"Missing"
W14-3605,W11-2838,0,0.0776592,"es. Our report includes an overview of the QALB corpus which was the source of the datasets used for training and evaluation, an overview of participating systems, results of the competition and an analysis of the results and systems. 1 Introduction The task of text correction has recently gained a lot of attention in the Natural Language Processing (NLP) community. Most of the effort in this area concentrated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers who focus on this problem and promote development and dissemination of key resources, such as benchmark datasets. Recently, there have been several efforts aimed at creating data resources related to the correction of Arabic text. Those include human annotated corpora (Zaghouani et al., 2014; Alfaifi and Atwell, 2012), spell-checking lexicon (Attia et al., 2012) and unannotated language learner corpora (Farwaneh and Tamimi, 2012). A natural ex"
W14-3605,W12-2006,0,0.0702953,"overview of the QALB corpus which was the source of the datasets used for training and evaluation, an overview of participating systems, results of the competition and an analysis of the results and systems. 1 Introduction The task of text correction has recently gained a lot of attention in the Natural Language Processing (NLP) community. Most of the effort in this area concentrated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers who focus on this problem and promote development and dissemination of key resources, such as benchmark datasets. Recently, there have been several efforts aimed at creating data resources related to the correction of Arabic text. Those include human annotated corpora (Zaghouani et al., 2014; Alfaifi and Atwell, 2012), spell-checking lexicon (Attia et al., 2012) and unannotated language learner corpora (Farwaneh and Tamimi, 2012). A natural extension to these res"
W14-3605,I13-2001,1,0.501252,"ompetitions is adopted: system outputs are compared against gold annotations using Precision, Recall and F1 . Systems are ranked based on the F1 scores obtained on the test set. The QALB Corpus One of the goals of the QALB project is to create a large manually corrected corpus of errors for a variety of Arabic texts, including user comments on news web sites, native and non-native speaker essays, and machine translation output. Within the framework of this project, comprehensive annotation guidelines and a specialized web-based annotation interface have been developed (Zaghouani et al., 2014; Obeid et al., 2013). The annotation process includes an initial automatic pre-processing step followed by an automatic correction of common spelling errors by the After the initial registration, the participants were provided with training and development sets and evaluation scripts. During the test period, the teams were given test data on which they needed to run their systems. Following the announcement of system results, the answer key to the test set was released. Participants authored description papers which will be presented in the Arabic NLP workshop. 40 Statistics Train. Dev. Test Number of docs. 19,41"
W14-3605,W12-5611,0,0.0562666,"Missing"
W14-3605,P05-1071,1,0.0923883,"Missing"
W14-3605,pasha-etal-2014-madamira,1,0.727075,"Missing"
W14-3605,W14-3615,0,0.0699428,"Missing"
W14-3605,W14-3618,1,0.793286,"Missing"
W14-3605,W14-3614,1,0.894222,"Missing"
W14-3605,W14-3621,0,0.0508845,"Missing"
W14-3605,zaghouani-etal-2014-large,1,0.605144,"ade by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers who focus on this problem and promote development and dissemination of key resources, such as benchmark datasets. Recently, there have been several efforts aimed at creating data resources related to the correction of Arabic text. Those include human annotated corpora (Zaghouani et al., 2014; Alfaifi and Atwell, 2012), spell-checking lexicon (Attia et al., 2012) and unannotated language learner corpora (Farwaneh and Tamimi, 2012). A natural extension to these resource production efforts is the creation of robust automatic systems for error correction. 2 Task Description The QALB shared task was created as a forum for competition and collaboration on automatic error correction in Modern Standard Arabic. The shared task makes use of the QALB corpus (Zaghouani et al., 2014), which is a manually-corrected collection of Arabic texts. The shared task participants were provided with tra"
W14-3605,W14-3617,0,0.0558201,"Missing"
W14-3605,W14-3616,0,0.123166,"Missing"
W14-3605,W14-3619,0,0.107829,"Missing"
W14-3605,W14-1701,0,\N,Missing
W14-3605,N12-1067,0,\N,Missing
W14-3605,W13-3602,1,\N,Missing
W14-3605,W15-3221,0,\N,Missing
W14-3605,W15-3220,0,\N,Missing
W14-3605,W15-3217,0,\N,Missing
W14-3605,W14-3622,1,\N,Missing
W14-3605,W15-3218,0,\N,Missing
W14-3605,W15-3214,0,\N,Missing
W14-3605,I08-2131,0,\N,Missing
W14-3605,W13-3601,0,\N,Missing
W14-3605,W11-2843,1,\N,Missing
W14-3618,D12-1052,0,0.0398906,"ata and selected the optimal subset to train their system. Alkanhal et al. (2012) presented a stochastic approach for spelling correction of Arabic text. They used a context-based system to automatically correct misspelled words. First of all, a list is generated with possible alternatives for each misspelled word using the Damerau-Levenshtein edit distance, then the right alternative for each misspelled word is selected stochastically using a lattice search, and an n-gram method. Shaalan et al. (2012) trained a Noisy Channel Model on word-based unigrams to detect and correct spelling errors. Dahlmeier and Ng (2012a) built specialized decoders for English grammatical error correction. More recently, (Pasha et al., 2014) created MADAMIRA, a system for morphological analysis and disambiguation of Arabic, this system can be used to improve the accuracy of spelling checking system especially with Hamza spelling correction. In contrast to the approaches described above, we use a machine translation (MT) based method to train an error correction system. To the best of our knowledge, this is the first error correction system for Arabic using an MT approach. 3 omission errors, which makes a good base for other"
W14-3618,P10-4002,0,0.0297961,"Missing"
W14-3618,P08-2015,0,0.0602574,"ts written by humans (e.g., non-native speakers), or machines (e.g., 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) presented a hybrid approach using morphological features and rules to fine 137 Proceedings of t"
W14-3618,I08-2131,0,0.42987,"curring errors in texts written by humans (e.g., non-native speakers), or machines (e.g., 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) presented a hybrid approach using morphological features and rules to fine 137 Pr"
W14-3618,P13-2121,0,0.0348173,"Missing"
W14-3618,P07-2045,0,0.00793191,"correct the spelling errors and 99K tokens were inserted (mostly punctuation marks). Furthermore, there is a total of 6,7K non necessary tokens deleted and 10.6K attached tokens split and 18.2 tokens merged. Finally, there are only 427 tokens moved in the sentence and 1563 multiple correction action. We experiment with different configurations and reach the sweet spot of performance when combining the different modules. Table 2: Clitics handled by the rule-based module. instance of that character (e.g. !!!!!!! would be replaced with !). Statistical Phrase-based Model We use the Moses toolkit (Koehn et al., 2007) to create a statistical phrase-based machine translation model built on the best pre-processed data, as described above. We treat this last step as a translation problem, where the source language is pre-processed incorrect Arabic text, and the reference is correct Arabic. Feature 14 extraction, rule-based correction, and character de-duplication are applied to both the train and dev sets. All but the last 1,000 sentences of the train data are used at the training set for the phrasebased model, the last 1,000 sentences of the train data are used as a tuning set, and the dev set is used for te"
W14-3618,C12-2011,0,0.527137,"Missing"
W14-3618,W14-3605,1,0.794478,"Missing"
W14-3618,J03-1002,0,0.0057042,"where the source language is pre-processed incorrect Arabic text, and the reference is correct Arabic. Feature 14 extraction, rule-based correction, and character de-duplication are applied to both the train and dev sets. All but the last 1,000 sentences of the train data are used at the training set for the phrasebased model, the last 1,000 sentences of the train data are used as a tuning set, and the dev set is used for testing and evaluation. We use fast align, the aligner included with the cdec decoder (Dyer et al., 2010) as the word aligner with grow-diag as the symmetrization heuristic (Och and Ney, 2003), and build a 5-gram language model from the correct Arabic training data with KenLM (Heafield et al., 2013). The system is evaluated with BLEU (Papineni et al., 2002) and then scored for precision, recall, and F1 measure against the dev set reference. We tested several different reordering window sizes since this is not a standard translation task, so we may want shorter distance reordering. Although 7 is the default size, we tested 7, 5, 4, 3, and 0, and found that a window of size 4 produces the best result according to BLEU score and F1 measure. 4 4.1 Results To evaluate the performance of"
W14-3618,J96-1003,1,0.552787,"evelop and evaluate spelling correction systems for Arabic trained either on naturally occurring errors in texts written by humans (e.g., non-native speakers), or machines (e.g., 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and"
W14-3618,P02-1040,0,0.10657,"de-duplication are applied to both the train and dev sets. All but the last 1,000 sentences of the train data are used at the training set for the phrasebased model, the last 1,000 sentences of the train data are used as a tuning set, and the dev set is used for testing and evaluation. We use fast align, the aligner included with the cdec decoder (Dyer et al., 2010) as the word aligner with grow-diag as the symmetrization heuristic (Och and Ney, 2003), and build a 5-gram language model from the correct Arabic training data with KenLM (Heafield et al., 2013). The system is evaluated with BLEU (Papineni et al., 2002) and then scored for precision, recall, and F1 measure against the dev set reference. We tested several different reordering window sizes since this is not a standard translation task, so we may want shorter distance reordering. Although 7 is the default size, we tested 7, 5, 4, 3, and 0, and found that a window of size 4 produces the best result according to BLEU score and F1 measure. 4 4.1 Results To evaluate the performance of our system on the development data, we compare its output to the reference (gold annotation). We then compute the usual measures of precision, recall and f-measure. R"
W14-3618,pasha-etal-2014-madamira,0,0.169771,"Missing"
W14-3618,N12-1067,0,\N,Missing
W14-3618,shaalan-etal-2012-arabic,0,\N,Missing
W14-3618,zaghouani-etal-2014-large,1,\N,Missing
W15-1614,abuhakema-etal-2008-annotating,0,0.311605,"access to detailed error statistics. This can provide learners with a very useful feedback and help them improve their proficiency level. 129 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 129–139, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics These errors may take place in words, phrases, language structures, and the ways words or expressions are used (Granger, 2003). For Arabic, there are few projects that aim at developing Arabic learner corpora and annotating them but most of them are not freely available for users or researchers (Abuhakema et al., 2008; Hassan and Daud, 2011). In this paper, we present our annotation method and our efforts for extending an L1 large scale Arabic language corpus and its manually edited corrections to include annotated non-native Arabic learner text (L2). This work is part of the Qatar Arabic Language Bank (QALB) project (Zaghouani et al., 2014b), a large-scale error annotation effort that aims to create a manually corrected corpus of errors for a variety of Arabic texts (the target size is 2 million words).1 Our overarching goal is to use our annotated corpus to develop components for automatic detection and"
W15-1614,W13-1703,0,0.0240313,"ar/CMU-CS-QTR-124.pdf 2 130 Section 2; then we describe the corpus and the annotation guidelines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. Abuhakema et al. (2008) annotated a small corpus of 9K"
W15-1614,dickinson-ledbetter-2012-annotating,0,0.136697,"ines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. Abuhakema et al. (2008) annotated a small corpus of 9K words of Arabic written materials produced by native speakers of English in the US who learned Ara"
W15-1614,N13-1066,1,0.853751,"ly accepted Arabic punctuation rules. 132 Dialectal Usage Errors: In comparison to Standard Arabic, where there are clear spelling standards and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing vario"
W15-1614,I08-1059,0,0.0165036,"such as POS, lemma, gender, number or person. The robust design of MADAMIRA allows it to consider different possible spellings of words, especially relating to Ya/Alif-Maqsura, Ha/Ta-Marbuta and Hamzated Alif forms, which are very common error sources. MADAMIRA selects the correct form in context, thus correcting for these errors which are often connected to lemma choice or morphology. 7 7.1 Evaluation Inter-Annotator Agreement Our annotation effort consists of a single annotation pass as commonly done in many annotation projects due to time and budget constraints (Rozovskaya and Roth, 2010; Gamon et al., 2008; Izumi et al., 2004; Nagata et al., 2006). In order to evaluate the quality of our correction annotations, we frequently measure the inter-annotator agreement (IAA) to ensure that the annotators are following the guidelines provided consistently. A high level of agreement between the annotators indicates that the annotation is reliable and the guidelines are useful in producing homogeneous and consistent data. We measure the IAA by averaging WER (Word Error Rate) over all pairs of annotations to compute the AWER (Average 135 Word Error Rate).7 For the purpose of this evaluation, the WER refer"
W15-1614,habash-etal-2012-conventional,1,0.127188,"text uses one of multiple widely acceptable transliterations, the annotators should not modify the word. Punctuation Errors: Punctuation errors should be corrected according to the commonly accepted Arabic punctuation rules. 132 Dialectal Usage Errors: In comparison to Standard Arabic, where there are clear spelling standards and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic"
W15-1614,N13-1044,1,0.82105,"tal Usage Errors: In comparison to Standard Arabic, where there are clear spelling standards and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing various spelling inconsistencies that frequently oc"
W15-1614,W10-1802,0,0.0226722,"non-native speakers of other languages such as English (Leacock et al., 2010; Rozovskaya and Roth, 2010). Lexical Correction: Finally, if it is impossible to fully correct the word using the previous four steps, there is a clear case of word choice errors and the annotator may have to replace the word used. This can be employed to especially correct inadequate lexical choices or unknown words. In the example given in 5 3. Correct derivation errors; but keep root intact. 4 The minimum edits approach in error correction have already been used in the Error-tagged Learner Corpus of Czech project (Hana et al., 2010) 133 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym ˇ ¯  ˆ ð', yˆ Zø', ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 6 A clitic is a linguistic unit that is pronounced and written like an affix but is grammatically independent. Inflection Error Correction Original ˇ knt qd bdÂnA fy AlςAm AlmADy rHl¯h Alaý mk¯h. Correction ˇ knt qd bdÂt fy AlςAm AlmADy rHl¯h Alaý mk¯h. English Original Correction English Original Correction English Original Correction English  úÍ@ éÊgP  ú"
W15-1614,W14-3603,1,0.49852,"and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing various spelling inconsistencies that frequently occur. So, as was done in the L1 annotation effort (Zaghouani et al., 2014b), we asked annotat"
W15-1614,W14-3605,1,0.412475,"us. The results obtained in the evaluation suggest that the annotators produced consistently similar results under the proposed guidelines. We believe that publishing this corpus will give researchers a common development and test set for developing related natural language processing applications. A subset of our L2 corpus will be used as part of the Second QALB Shared Task on Automatic Arabic Error Correction in conjunction with the ACL-2015 Workshop on Arabic NLP.9 This shared task follows the success of the First QALB Shared Task held in conjunction with EMNLP-2014 Workshop on Arabic NLP (Mohit et al., 2014). In the future, we will extend our annotation guidelines to address machine translation output correction (i.e., manual post-editing). We also plan to extend our systems for automatic correction of Arabic language errors (Jeblee et al., 2014; Rozovskaya et al., 2014) to handle L2 data, using the corpus discussed here for training and test purposes. 9 http://www.arabic-nlp.net/wanlp 137 Acknowledgements We thank anonymous reviewers for their valuable comments and suggestions. We also thank all our dedicated annotators: Noor Alzeer, Hoda Fathy, Hoda Ibrahim, Anissa Jrad, Samah Lakhal, Jihene Wa"
W15-1614,P06-1031,0,0.0280632,"erson. The robust design of MADAMIRA allows it to consider different possible spellings of words, especially relating to Ya/Alif-Maqsura, Ha/Ta-Marbuta and Hamzated Alif forms, which are very common error sources. MADAMIRA selects the correct form in context, thus correcting for these errors which are often connected to lemma choice or morphology. 7 7.1 Evaluation Inter-Annotator Agreement Our annotation effort consists of a single annotation pass as commonly done in many annotation projects due to time and budget constraints (Rozovskaya and Roth, 2010; Gamon et al., 2008; Izumi et al., 2004; Nagata et al., 2006). In order to evaluate the quality of our correction annotations, we frequently measure the inter-annotator agreement (IAA) to ensure that the annotators are following the guidelines provided consistently. A high level of agreement between the annotators indicates that the annotation is reliable and the guidelines are useful in producing homogeneous and consistent data. We measure the IAA by averaging WER (Word Error Rate) over all pairs of annotations to compute the AWER (Average 135 Word Error Rate).7 For the purpose of this evaluation, the WER refers to an annotation error and it is measure"
W15-1614,I13-2001,1,0.525343,"eyeglasses to read the book.’   @Q¯ @ ú Ë è @QÖÏ @ © A  P A¢ JË@ © A ú Ë H@ Table 1: Examples of the different parts of the correction priority order   ¯ h ‘mirror’ was replaced Table 1, the word è @QÖÏ @ AlmrA¯  P A¢ JË@ AlnDArAt ˇ by the word H@ ‘eyeglasses’. alignments starting from document tokenization to after human annotation. 5 6 The Annotation Tool In order to ensure the speed and efficiency of the annotation process, as well as better management, we provide the annotators with a web-based annotation framework, originally developed to manually correct errors in L1 texts (Obeid et al., 2013). The annotation interface allows annotators to perform different actions corresponding to the following types of corrections: (a) edit misspelled words; (b) move words that are not in the right location; (c) add missing words; (d) delete extraneous words; (e) merge words that have been split erroneously; and (f) split words that have been merged erroneously. In our final corpus output format, we record for each annotated file the list of actions taken by the annotator. These actions operate on one or two tokens depending on the action. We also supply token 134 The Annotation Pipeline The anno"
W15-1614,P02-1040,0,0.105018,"ear that ALC is less challenging than ALWC as shown in the IAA of the first round and second rounds. Overall, the high-level of agreement obtained in the second round shows that the annotators produced consistently similar results under the proposed guidelines; and their differences are all within acceptable variation. This of course makes the evaluation of automatic correction harder.8 7 The annotation manager is excluded from this evaluation. This problem might be solved by considering multiple references in the evaluation process similarly to what is done in machine translation evaluation (Papineni et al., 2002). Unfortu8 Original  ®Ö Ï @ úæîDKA ÐA« ú¯ éËA à@ ø ñK@ B@ . ZAKCJË@ ÉJ.¯ àA Anwy An sAnthy AlmqAl¯h fy ςAm AlAnsAn qbl AlθlAθA’. ‘I plan I will be-done the article in the year of humanity before Tuesday.’ Annotator 1 @ ÐA« á«  ®Ö Ï @ úæîE @ à @ ø ñK @ B éËA . ZAKCJË@ ÉJ.¯ àA ˇ Ânwy Ân Ânhy AlmqAl¯h ςn ςAm AlAnsAn qbl AlθlAθA’. ‘I plan to finish-off the article about the year of humanity before Tuesday.’ Annotator 2 @ ÕËA« á«  ®Ö Ï @ úæîE @ à @ ø ñK @ B éËA . ZAKCJË@ ÉJ.¯ àA ˇ Ânwy Ân Ânhy AlmqAl¯h ςn ςAlm AlAnsAn qbl AlθlAθA’. ‘I plan to finish-off the article about the"
W15-1614,pasha-etal-2014-madamira,1,0.792787,"Missing"
W15-1614,W10-1004,1,0.231979,"able at http://reports-archive.adm.cs.cmu.edu/ anon/qatar/CMU-CS-QTR-124.pdf 2 130 Section 2; then we describe the corpus and the annotation guidelines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. A"
W15-1614,W14-3622,1,0.888722,"Missing"
W15-1614,N13-1036,1,0.851271,"shed novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing various spelling inconsistencies that frequently occur. So, as was done in the L1 annotation effort (Zaghouani et al., 2014b), we asked annotators to flag the highly dialectal cases to be reviewed later by the annotation manager. The guidelines classify dialectal word issues into five categories inspired by Habash et al. (2008): dialectal lexical choice, p"
W15-1614,W08-1205,0,0.00798721,"etailed description of ALC is given at: http://www.arabiclearnercorpus.com/ 131 and structures, with some items overused and others significantly underused. They also contain varying degrees of grammatical, orthographic and lexical errors. Moreover, sentences written by Arabic L2 speaker have often a different structure and are not as fluent as sentences produced by a native speaker even when no clear mistakes can be found. Therefore, the correction task is complicated by the fact that the acceptability level of a given sentence differs widely within the native speaker annotators as stated by Tetreault and Chodorow (2008). These issues can be related to linguistic factors such as inter-language (L1 interference), the student’s teaching and learning methodology, and to the translation effect (conscious interference). Thus, correcting the Arabic L2 essays can be a very challenging task that requires a lot of interpretation efforts by the annotators. This will likely lead to lower inter-annotator agreement as there is often many possible ways to correct the L2 errors. In order to annotate the L2 corpus, we use our annotation guidelines designed for L1 (Zaghouani et al., 2014b) and add specific L2 annotation rules"
W15-1614,P11-1019,0,0.0242372,"ive.adm.cs.cmu.edu/ anon/qatar/CMU-CS-QTR-124.pdf 2 130 Section 2; then we describe the corpus and the annotation guidelines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. Abuhakema et al. (2008) annot"
W15-1614,zaghouani-etal-2014-large,1,0.588046,"Missing"
W15-1614,zribi-etal-2014-conventional,1,0.33632,"Missing"
W15-1614,W14-3618,1,\N,Missing
W15-3204,W15-3214,0,0.0408316,"Missing"
W15-3204,I08-2131,0,0.0157927,"trated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers and promote the development of relevant techniques and dissemination of key resources, such as benchmark data sets. In the area of Arabic text correction, there has been a significant body of work, as well (Shaalan et al., 2003; Hassan et al., 2008). However, due to the lack of a common benchmark data set, making progress on this task has been difficult. The QALB shared task on automatic text correction of Arabic, 1 http://nlp.qatar.cmu.edu/qalb/ 26 Proceedings of the Second Workshop on Arabic Natural Language Processing, pages 26–35, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics 2 Task Description The texts are manually annotated for errors by native Arabic speakers. The annotation begins with an initial automatic pre-processing step. Next, the files are processed with the morphological analysis and"
W15-3204,C12-2011,0,0.0931945,"Missing"
W15-3204,W14-3605,1,0.841304,"al Learning Systems, Columbia University 2 Carnegie Mellon University in Qatar 3 New York University Abu Dhabi 4 Ask.com alla@ccls.columbia.edu,hbouamor@qatar.cmu.edu,nizar.habash@nyu.edu wajdiz@qatar.cmu.edu,owo@qatar.cmu.edu,behrang@cmu.edu Abstract organized within the framework of the Qatar Arabic Language Bank (QALB) project,1 is the first effort aimed at constructing a benchmark data set, which will allow for development and evaluation of automatic correction systems for Arabic. In this paper, we present a summary of the second edition of the QALB competition. The first one – QALB-2014 (Mohit et al., 2014) – took place in conjunction with the Arabic NLP workshop at EMNLP-2014 and focused on errors found in online commentaries produced by native speakers of Arabic. QALB-2014 attracted a lot of attention and resulted in nine systems being submitted with a variety of approaches that included rule-based frameworks, machine-learning classifiers, and statistical machine translation methods. This year’s competition extends the first edition by adding another track that focuses on errors found in essays written by learners of Arabic. Eight teams participated in the competition this year, including seve"
W15-3204,W15-3216,0,0.0195413,"Missing"
W15-3204,W15-3220,0,0.0483413,"Missing"
W15-3204,W15-3217,1,0.853627,"Missing"
W15-3204,W15-3218,0,0.0309508,"Missing"
W15-3204,W15-3221,1,0.873886,"Missing"
W15-3204,W15-3215,0,0.125217,"Missing"
W15-3204,N12-1067,0,0.0704061,"ariety of techniques. For example, the CUFE system extracted rules from the morphological analyzer and learned their probabilities using the training data, while the UMMU system combined statistical machine6 Results In this section, we present the results of the competition. As was done in QALB-2014, we adopted the standard Precision (P), Recall (R), and F1 metric. This metric was also used in recent shared tasks on grammatical error correction in English: HOO competitions (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013). The results are computed using the M2 scorer (Dahlmeier and Ng, 2012) that was also used in the CoNLL shared tasks. Tables 8 and 9 present the official results of the evaluation on the test sets for the Aljazeera data and the L2 data, respectively. The results are sorted according to the F1 scores obtained by the 30 Rank 1 2 3 4 5 6 7 8 9 10 11 12 Team CUFE UMMU-1 GWU UMMU-2 QCRI QCMUQ TECH-2 TECH-1 TECH-3 ARIB-1 ARIB-2 SAHSOH MADAMIRA P 88.85 70.28 74.69 72.69 84.74 71.39 71.20 71.08 69.99 64.50 67.56 81.88 80.32 R 61.76 71.93 67.51 67.52 58.10 65.13 64.94 64.74 60.41 56.50 51.61 40.24 39.98 F1 72.87 71.10 70.92 70.01 68.94 68.12 67.93 67.76 64.85 60.23 58.52"
W15-3204,W11-2838,0,0.388272,"uage. The report includes an overview of the QALB corpus, which is the dataset used for training and evaluation, an overview of participating systems, results of the competition and an analysis of the results and systems. 1 Introduction The task of text correction has recently been attracting a lot of attention in the Natural Language Processing (NLP) community, but most of the effort in this area concentrated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers and promote the development of relevant techniques and dissemination of key resources, such as benchmark data sets. In the area of Arabic text correction, there has been a significant body of work, as well (Shaalan et al., 2003; Hassan et al., 2008). However, due to the lack of a common benchmark data set, making progress on this task has been difficult. The QALB shared task on automatic text correction of Arabic, 1 http://nlp.qatar.cmu.edu/qa"
W15-3204,I13-2001,1,0.600185,"ts are compared against gold annotations using Precision, Recall and F1 . Systems are ranked based on the F1 scores obtained on the test sets. 3 The QALB Corpus The QALB corpus was created as part of the QALB project. One of the goals of the QALB project is to develop a large manually corrected corpus for a variety of Arabic texts, including texts produced by native and non-native writers, as well as machine translation output. Within the framework of this project, comprehensive annotation guidelines and a specialized web-based annotation interface have been developed (Zaghouani et al., 2014; Obeid et al., 2013; Zaghouani et al., 2015a). 2 In the shared task, we specified two Add categories: add_before and add_after. Most of the add errors fall into the first category, and we combine these here into a single Add category. 3 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical ˇ order) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional ˇ ¯ symbols: ’ Z, Â @, A @ , A @, wˆ ð', yˆ Zø', ¯h è, ý ø. 4 Tables 1 and 2, and the appendix are reproduced from Mohit et al. (2014) to help explain the format of the files used in QALB-2014 and QALB-2015 sha"
W15-3204,W12-2006,0,0.289248,"n overview of the QALB corpus, which is the dataset used for training and evaluation, an overview of participating systems, results of the competition and an analysis of the results and systems. 1 Introduction The task of text correction has recently been attracting a lot of attention in the Natural Language Processing (NLP) community, but most of the effort in this area concentrated on English, especially on errors made by learners of English as a Second Language. Four competitions devoted to error correction for non-native English writers took place recently: HOO (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL (Ng et al., 2013; Ng et al., 2014). Shared tasks of this kind are extremely important, as they bring together researchers and promote the development of relevant techniques and dissemination of key resources, such as benchmark data sets. In the area of Arabic text correction, there has been a significant body of work, as well (Shaalan et al., 2003; Hassan et al., 2008). However, due to the lack of a common benchmark data set, making progress on this task has been difficult. The QALB shared task on automatic text correction of Arabic, 1 http://nlp.qatar.cmu.edu/qalb/ 26 Proceedings o"
W15-3204,W12-5611,0,0.0600366,"Missing"
W15-3204,pasha-etal-2014-madamira,1,0.852468,"Missing"
W15-3204,zaghouani-etal-2014-large,1,0.849061,"workshop at EMNLP-2014 (Mohit et al., 2014). QALB-2014 addressed errors in online user comments written to Aljazeera articles by native Arabic speakers. This year’s competition includes two tracks – native and non-native. In addition to the Aljazeera commentaries written by native speakers, it also includes texts produced by learners of Arabic as a foreign language (L2). Both the native and the non-native data is written in Modern Standard Arabic and is part of the QALB corpus (see Section 3), a manuallycorrected collection of Arabic texts. The Aljazeera section of the corpus is presented in Zaghouani et al. (2014). The L2 data is extracted from two learner corpora of Arabic – the Arabic Learners Written Corpus (ALWC) (Farwaneh and Tamimi, 2012) and the Arabic Learner Corpus (ALC) (Alfaifi and Atwell, 2012). For details about the L2 data, we refer the reader to Zaghouani et al. (2015a). The shared task participants were provided with training and development data to build their systems, but were also free to make use of additional resources, including corpora, linguistic resources, and software, as long as these were publicly available. For evaluation, a standard framework developed for similar error co"
W15-3204,W15-1614,1,0.759406,"nd the English translation. The errors in the original and the corrected forms are underlined and co-indexed. Table 2 presents a subset of the errors for the example shown in Table 1 along with the error types and annotation actions. The Appendix at the end of the paper lists all annotation actions for that example.4 Essays written by L2 speakers differ from the native texts both because of the genre and the types of mistakes. For this reason, the general QALB L1 annotation guidelines were extended by adding new rules describing the error correction procedure in texts produced by L2 speakers (Zaghouani et al., 2015a). Because the genres are different, the writing styles exhibit different distributions of words, phrases, and structures. Further, while native texts mostly contain orthographic and punctuation mistakes, non-native writings also reveal lexical choice errors, missing and extraneous words (e.g. articles, prepositions), and mistakes in word The QALB-2015 shared task extends QALB2014, the first shared task on Arabic text correction that was created as a forum for competition and collaboration on automatic error correction in Modern Standard Arabic and took place in conjunction with the Arabic NL"
W15-3204,W15-3219,1,0.765882,"Missing"
W15-3209,2007.mtsummit-papers.20,1,0.567467,"stances of a word type are observed in a corpus, and (2) ambiguity where a word has multiple readings or interpretations. Undiacritized surface forms of an Arabic word might have as many as 200 readings depending on the complexity of its morphology. The lack of diacritics usually leads to considerable lexical ambiguity, as shown in the example in Table 1, a reason for which diacritization, aka vowel/diacritic restoration, has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications. In general, building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available diacritized MSA corpora are generally limited to the newswire genres (as distributed by the LDC) or religion related texts such as the Quran or the Tashkeela corpus.2 In this paper we present a pilot study where we annotate a sample of non-diacritized text extracted from fiv"
W15-3209,P06-1073,0,0.583986,"lts show that readers benefited from the disambiguating diacritics. This study was a MIN scheme exploration focused on heterophonic-homographic target verbs that have different pronunciations in active and Related Work The task of diacritization is about adding diacritics to the canonical underspecified written form. This task has been discussed in several research works in various NLP areas addressing various applications. Automatic Arabic Diacritization Much work has been done on recovery of diacritics over the past two decades by developing automatic methods yielding acceptable accuracies. Zitouni et al. (2006) built a diacritization framework based on such as number, gender, aspect, voice, etc. Whereas a lemma is a conventionalized citation form. 82 ATB News ATB BN ATB WebLog Tashkeela Wikipedia Total Size in words 2,478 3,093 3,177 5,172 2,850 16,770 GOLD annotation Yes Yes Yes Yes No - Table 2: The size of the data for annotation per corpus genre passive. classical Arabic books). This corpus contains over 6 million words fully diacritized. For our study we include a subset of 5k words from this corpus. In this work we are interested in two components: annotating large amounts of varied genres typ"
W15-3209,P05-1071,0,0.0659168,"n between Form I and Form II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorized and un-factorized textual features to automatically diacritize raw Arabic text. Emam and Fischer (2011) introduced a hierarchical approach for diacritization based on a search method in a set of dictionaries of sentences, phrases and words, using a top down strategy. More"
W15-3209,N07-2014,0,0.648924,"acritic is the distinction between Form I and Form II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorized and un-factorized textual features to automatically diacritize raw Arabic text. Emam and Fischer (2011) introduced a hierarchical approach for diacritization based on a search method in a set of dictionaries of sentences, phrases and words, using a"
W15-3209,maamouri-etal-2008-enhancing,0,0.396831,"very text genre, two annotators were asked to annotate independently a sample of 100 words. We measured the IAA between two annotators by averaging WER (Word Error Rate) over all pairs of words. The higher the WER between two annotations, the lower their agreement. The results given in Table 5, show clearly that the Advanced mode is the best strategy to adopt for this diacritization task. It is the less confusing method on all text genres (with WER between 1.56 and 5.58). We note that Wiki annotations in Advanced mode garner the highest IAA with a very low WER. We extended the LDC guidelines (Maamouri et al., 2008) by adding some diacritization rules: The shadda mark should not be added to the definite article (e.g., àñÒJ ÊË@/’lemon’ and not àñÒJ ÊË@); The sukuun sign should not be indicated at the end /’from’); The letters folof silent words (e.g., áÓ lowed by a long Alif, should not be diacritized  as it is a deterministic diacritization ( Y« @ñ ®Ë@/’the rules’); Abbreviations are not diacritized ( Õ»/’km’, /’kg’). Ñª» We also added an appendix that sumWe measure the reliability of the annotations by comparing them against gold standard annotations. In order to build the gold Wiki annotations, we hi"
W15-3209,pasha-etal-2014-madamira,1,0.916558,"Missing"
W15-3209,P08-2030,1,0.710351,"II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorized and un-factorized textual features to automatically diacritize raw Arabic text. Emam and Fischer (2011) introduced a hierarchical approach for diacritization based on a search method in a set of dictionaries of sentences, phrases and words, using a top down strategy. More recently, Abandah et"
W15-3209,W04-1612,0,0.67965,"kAtib/’writer’ and I.KA¿/kAtab/’to correspond’ distinguishes between the meanings of the word (lexical disambiguation) rather than their inflections. Any of diacritics may be used to mark lexical variation. A common example with the shadda (gemination) diacritic is the distinction between Form I and Form II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorize"
W15-3219,C12-2011,0,0.277816,"Missing"
W15-3219,N12-1067,0,0.0611644,"tion has been explored widely by many researchers in the past years especially for the English language. Many approaches have been used to build systems (hybrid, rule base, supervised and unsupervised machine learning…). These systems used various NLP tools and resources including pre-existing lexicons, morphological analyzers and Part of Speech Taggers. We cite for the English language early works done by (Church and Gale, 1991; Kukich, 1992; Golding, 1995; Golding and Roth, 1996). Later on we find (Brill and Moore, 2000; Fossati and Di Eugenio, 2007) and more recently Han and Baldwin, 2011; Dahlmeier and Ng 2012; Wu et al., 2013). For Arabic, this problem has been investigated in a couple of papers as in Shaalan et al. (2003) who presented his work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) built a hybrid approach that used rules and some morphological features to correct non-words using contextual clues and Hassan et al. (2008) presented a language independent text correction method using Finite State Automata. More recently, Alkanhal et al. (2012) wrote a paper about a stochastic approach used for word spelling correction and Attia et al"
W15-3219,pasha-etal-2014-madamira,0,0.0940519,"Missing"
W15-3219,R11-1015,0,\N,Missing
W15-3219,P11-1088,0,\N,Missing
W15-3219,I13-2001,1,\N,Missing
W15-3219,W14-3605,1,\N,Missing
W15-3219,zaghouani-etal-2014-large,1,\N,Missing
W15-3219,W15-1614,1,\N,Missing
W15-3219,I08-2131,0,\N,Missing
W15-5116,zaghouani-etal-2014-large,1,0.882425,"Missing"
W16-4115,J11-4004,0,0.0319415,"ding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity There are many studies that evaluate the language complexity in addition to the quality of manual annotation and also allow the identification of many factors causing lower inter-annotator agreements. For example, Bayerl and Paul (2011) showed that there is a correlation between the inter-annotator agreement and the complexity of the annotation task; for instance, the larger the number of categories is, the lower the inter-annotator agreement is. Moreover, the categories prone to confusions are generally limited. This brings out two complexity issues related to the number of categories and to the existence of ambiguity between some the categories as explained in (Popescu-Belis, 2007). Furthermore, there are some annotation tasks for which the choice of a label is entirely left to the annotator, which can lead to even more co"
W16-4115,D15-1274,0,0.0235161,"ng ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation (OptDiac) project (Zaghouani et al., 2016a; Bouamor et al., 2015). The o"
W16-4115,W15-3209,1,0.748269,"5; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation (OptDiac) project (Zaghouani et al., 2016a; Bouamor et al., 2015). The overreaching goal of our project is to manually create a large-scale annotated corpus with the diacritics for a variety of Arabic texts. The creation of manually annotated corpora presents many challenges and issues related to the linguistic complexity of the Arabic language. In order to streamline the annotation process, we designed various annotation experimental conditions in order to answer the following questions: Can we automatically detect linguistic difficulties such as linguistic ambiguity? To what extent is there agreement between machines and human annotators when it comes to"
W16-4115,2007.mtsummit-papers.20,1,0.914733,"owels and diacritics rendering a mostly consonantal orthography (Schulz, 2004). Arabic diacritization is an orthographic way to describe Arabic word pronunciation, and avoid word reading ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents"
W16-4115,palmer-etal-2008-pilot,1,0.782763,"raining of the annotators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2."
W16-4115,2012.eamt-1.18,0,0.135091,"e analyses contain more than one possibility, the word is marked as ambiguous; otherwise, it is believed to be not ambiguous. Words that have no analysis generated using MADAMIRA are also considered ambiguous. For each sentence, we count the number of words that are marked as ambiguous using our approach, and then calculate the percentage of ambiguity. We sort the sentences according to their ambiguity percentages in descending order so that we give annotators ranked sentences for annotation. Because we are concerned with MSA dataset only, we further filter out dialectal sentences using AIDA (Elfardy and Diab, 2012), a tool that classifies words and sentences as MSA (formal Arabic) or DA (Dialectal Arabic). 5 Evaluation For the evaluation, we used a sample of 10K-Words from the CCA corpus representing 4 domains with approximately 2.5K-words per domain (children stories, economics,sports and politics). We have three experimental conditions for three evaluations carried over a period of six weeks. 1. The first condition (COND1): In the first experimental condition (COND1), four annotators were given raw undiacritized sentences and were asked to add the missing diacritics as per the guidelines. They either"
W16-4115,W07-1522,0,0.0406205,"l reasons that may cause disagreement in annotation decisions including human errors, lack of precision in the guidelines, and the lack of expertise and training of the annotators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally as"
W16-4115,W11-0405,0,0.0272132,"in annotation decisions including human errors, lack of precision in the guidelines, and the lack of expertise and training of the annotators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous ca"
W16-4115,maamouri-etal-2010-speech,1,0.827854,"ral ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity There are many studies that evaluate the language complexity in addition"
W16-4115,W12-2015,1,0.900794,"Missing"
W16-4115,W14-3605,1,0.871251,"Missing"
W16-4115,P06-1031,0,0.094128,"Missing"
W16-4115,I13-2001,1,0.828094,"or: the Shadda gemination mark, the Soukoun (absence of a vowel) and the Nunation marks at the end of a word. Moreover, in some cases, the letters followed by a long Alif  letter @, should not be diacritized as it is considered a deterministic diacritization as in AJJ Ó /miyvAqu/  ’Treaty’ and not AJJ Ó /miyvaAqu/.2 A summary of the most common Arabic diacritization rules is also added as a reference in the guidelines. 3.2 Annotation Tool We designed and implemented MANDIAC, a web-based annotation tool and a work-flow management interface (Obeid et al., 2016), the tool is based on QAWI (Obeid et al., 2013) a token-based editor, used to annotate and correct spelling errors in Arabic text for the Qatar Arabic Language Bank (QALB) project.3 The basic interface of the annotation tool is shown in Figure 1, apart from the surface controls, the interface allows annotators to select from an automatically generated diacritized words list and/or edit words manually as shown. The annotation interface allows users to undo/redo actions, and the history is kept over multiple sessions. The interface includes a timer to keep track of how long each sentence annotation has taken. We used the timer feature to mea"
W16-4115,pasha-etal-2014-madamira,1,0.873559,"Missing"
W16-4115,W10-1004,0,0.0710577,"Missing"
W16-4115,W15-3204,1,0.861846,"Missing"
W16-4115,D15-1152,0,0.0130569,"abic word pronunciation, and avoid word reading ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation (OptDiac) project (Zagh"
W16-4115,W10-1836,1,0.835215,"tators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity"
W16-4115,W12-2511,1,0.86205,"t rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity There are many studies th"
W16-4115,L16-1577,1,0.81745,"Missing"
W16-4115,L16-1295,1,0.886376,"Missing"
W16-4115,P06-1073,0,0.0303502,"hic way to describe Arabic word pronunciation, and avoid word reading ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation"
zaghouani-dukes-2014-crowdsourcing,D07-1116,0,\N,Missing
zaghouani-dukes-2014-crowdsourcing,D08-1027,0,\N,Missing
zaghouani-dukes-2014-crowdsourcing,P05-1071,0,\N,Missing
zaghouani-dukes-2014-crowdsourcing,maamouri-etal-2008-diacritic,0,\N,Missing
zaghouani-dukes-2014-crowdsourcing,W10-0701,0,\N,Missing
zaghouani-dukes-2014-crowdsourcing,D09-1030,0,\N,Missing
zaghouani-dukes-2014-crowdsourcing,W10-0733,0,\N,Missing
zaghouani-dukes-2014-crowdsourcing,N13-1049,0,\N,Missing
zaghouani-dukes-2014-crowdsourcing,dukes-habash-2010-morphological,1,\N,Missing
zaghouani-dukes-2014-crowdsourcing,dukes-etal-2010-syntactic,1,\N,Missing
zaghouani-dukes-2014-crowdsourcing,W10-0700,0,\N,Missing
zaghouani-etal-2010-adapting,W98-1002,0,\N,Missing
zaghouani-etal-2010-adapting,W98-1006,0,\N,Missing
zaghouani-etal-2014-large,W11-2602,1,\N,Missing
zaghouani-etal-2014-large,W10-1004,1,\N,Missing
zaghouani-etal-2014-large,P05-1071,1,\N,Missing
zaghouani-etal-2014-large,P11-1019,0,\N,Missing
zaghouani-etal-2014-large,P08-2015,1,\N,Missing
zaghouani-etal-2014-large,N13-1036,1,\N,Missing
zaghouani-etal-2014-large,W13-1703,0,\N,Missing
zaghouani-etal-2014-large,shaalan-etal-2012-arabic,0,\N,Missing
zaghouani-etal-2014-large,pasha-etal-2014-madamira,1,\N,Missing
zaghouani-etal-2014-large,dickinson-ledbetter-2012-annotating,0,\N,Missing
zaghouani-etal-2014-large,I13-2001,1,\N,Missing
zaghouani-etal-2014-large,abuhakema-etal-2008-annotating,0,\N,Missing
zaghouani-etal-2014-large,N13-1066,1,\N,Missing
