2020.acl-main.124,D19-1307,1,0.87014,"Missing"
2020.acl-main.124,N19-1423,0,0.0102998,"formation from the source documents is covered by the summaries. There exist a few unsupervised evaluation methods (Louis and Nenkova, 2013; Sun and Nenkova, 2019), but they have low correlation with human relevance ratings at summary level: given multiple summaries for the same source documents, these methods can hardly distinguish summaries with high relevance from those with low relevance (see §3). Contributions. First, to better measure the semantic overlap between source documents and machine-generated summaries, we propose to use state-of-the-art contextualized text encoders, e.g. BERT (Devlin et al., 2019) and its variant SentenceBERT (SBERT) (Reimers and Gurevych, 2019), which is optimized for measuring semantic similarity between sentences, to develop unsupervised evaluation methods. We measure the relevance of a summary in two steps: (i) identifying the salient information in the input documents, to build a pseudo reference summary, and (ii) measuring the semantic overlap between the pseudo reference and the 1347 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1347–1354 c July 5 - 10, 2020. 2020 Association for Computational Linguistics summary"
2020.acl-main.124,W04-1013,0,0.221477,"-the-art unsupervised summarizers. All source code is available at https://github.com/yg211/ acl20-ref-free-eval. 1 Doc 2 Pseudo reference summary Figure 1: Workflow of SUPERT. Evaluating the quality of machine-generated summaries is a highly laborious and hence expensive task. Most existing evaluation methods require certain forms of human involvement, thus are supervised: they either directly let humans rate the generated summaries (e.g. Pyramid (Nenkova and Passonneau, 2004)), elicit human-written reference summaries and measure their overlap with the generated summaries (e.g. using ROGUE (Lin, 2004a) or MoverScore (Zhao et al., 2019)), or collect some human annotations (e.g. preferences over pairs of summaries (Gao et al., 2019a)) to learn a summary evaluation function. Evaluation in multidocument summarization is particularly expensive: Lin (2004b) reports that it requires 3,000 hours of human effort to evaluate the summaries from the Document Understanding Conferences (DUC)1 . http://duc.nist.gov/ Summary relevance score Doc N Introduction 1 Salient Sentences Extractor Semantic Similarity Measurement To reduce the expenses for evaluating multidocument summaries, we investigate unsuper"
2020.acl-main.124,D18-1445,1,0.854219,"n annotators to rate summaries following some guidelines, e.g. Responsiveness, which measures the overall quality (relevance, fluency and readability) of summaries, and Pyramid (Nenkova and Passonneau, 2004), which measures summaries’ relevance. Recently, systems have been developed to ease the construction of Pyramid scores, e.g. (Hirao et al., 2018; Yang et al., 2016; Gao et al., 2019b; Shapira et al., 2019), but they still require human-annotated Summary Content Units (SCUs) to produce reliable scores. Besides SCUs, recent work has explored eliciting preferences over summaries (Zopf, 2018; Gao et al., 2018, 2019a) and annotations of important bi-grams (P.V.S and Meyer, 2017) to derive summary ratings. Some methods collect human ratings on a small number of summaries to train an evaluation function. Peyrard et al. (2017); Peyrard and Gurevych (2018) propose to learn an evaluation function from Pyramid and Responsiveness scores, by using classic supervised learning methods with hand-crafted features. ShafieiBavani et al. (2018b) use the same idea but design corpus based and lexical resource based word embeddings to build the features. B¨ohm et al. (2019) train a BERT-based evaluation function wit"
2020.acl-main.124,J13-2002,0,0.534871,"om the Document Understanding Conferences (DUC)1 . http://duc.nist.gov/ Summary relevance score Doc N Introduction 1 Salient Sentences Extractor Semantic Similarity Measurement To reduce the expenses for evaluating multidocument summaries, we investigate unsupervised evaluation methods, which require neither human annotations nor reference summaries. In particular, we focus on evaluating the relevance (Peyrard, 2019) of multi-document summaries, i.e. measuring how much salient information from the source documents is covered by the summaries. There exist a few unsupervised evaluation methods (Louis and Nenkova, 2013; Sun and Nenkova, 2019), but they have low correlation with human relevance ratings at summary level: given multiple summaries for the same source documents, these methods can hardly distinguish summaries with high relevance from those with low relevance (see §3). Contributions. First, to better measure the semantic overlap between source documents and machine-generated summaries, we propose to use state-of-the-art contextualized text encoders, e.g. BERT (Devlin et al., 2019) and its variant SentenceBERT (SBERT) (Reimers and Gurevych, 2019), which is optimized for measuring semantic similarit"
2020.acl-main.124,K19-1038,0,0.397213,"udo reference summary Figure 1: Workflow of SUPERT. Evaluating the quality of machine-generated summaries is a highly laborious and hence expensive task. Most existing evaluation methods require certain forms of human involvement, thus are supervised: they either directly let humans rate the generated summaries (e.g. Pyramid (Nenkova and Passonneau, 2004)), elicit human-written reference summaries and measure their overlap with the generated summaries (e.g. using ROGUE (Lin, 2004a) or MoverScore (Zhao et al., 2019)), or collect some human annotations (e.g. preferences over pairs of summaries (Gao et al., 2019a)) to learn a summary evaluation function. Evaluation in multidocument summarization is particularly expensive: Lin (2004b) reports that it requires 3,000 hours of human effort to evaluate the summaries from the Document Understanding Conferences (DUC)1 . http://duc.nist.gov/ Summary relevance score Doc N Introduction 1 Salient Sentences Extractor Semantic Similarity Measurement To reduce the expenses for evaluating multidocument summaries, we investigate unsupervised evaluation methods, which require neither human annotations nor reference summaries. In particular, we focus on evaluating the"
2020.acl-main.124,N04-1019,0,0.529859,"ermore, we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers. All source code is available at https://github.com/yg211/ acl20-ref-free-eval. 1 Doc 2 Pseudo reference summary Figure 1: Workflow of SUPERT. Evaluating the quality of machine-generated summaries is a highly laborious and hence expensive task. Most existing evaluation methods require certain forms of human involvement, thus are supervised: they either directly let humans rate the generated summaries (e.g. Pyramid (Nenkova and Passonneau, 2004)), elicit human-written reference summaries and measure their overlap with the generated summaries (e.g. using ROGUE (Lin, 2004a) or MoverScore (Zhao et al., 2019)), or collect some human annotations (e.g. preferences over pairs of summaries (Gao et al., 2019a)) to learn a summary evaluation function. Evaluation in multidocument summarization is particularly expensive: Lin (2004b) reports that it requires 3,000 hours of human effort to evaluate the summaries from the Document Understanding Conferences (DUC)1 . http://duc.nist.gov/ Summary relevance score Doc N Introduction 1 Salient Sentences"
2020.acl-main.124,W00-0405,0,0.371162,"hod correlates better with human ratings than ROUGE and BLEU. However, as their method is designed for evaluating single-document summaries, it correlates poorly with the Pyramid scores for multi-document summaries (see §3). Unsupervised Evaluation. Louis and Nenkova (2013) measure the relevance of a summary using multiple heuristics, for example by computing the Jensen-Shannon (JS) divergence between the word distributions in the summary and in the source documents. Ryang and Abekawa (2012); Rioux et al. (2014) develop evaluation heuristics inspired by the maximal marginal relevance metrics (Goldstein et al., 2000). But these methods have low correlation with human ratings at summary level (see §3). Scialom et al. (2019) propose to generate questions from source documents and evaluate the relevance of summaries by counting how many questions the summaries can answer. However, they do not detail how to generate questions from source documents; also, it remains unclear whether their method works for evaluating multi-document summaries. Sun and Nenkova (2019) propose a single-document summary evaluation method, which measures the cosine similarity of the ELMo embeddings (Peters et al., 2018) of the source"
2020.acl-main.124,D18-1450,0,0.0222955,"al., 2018a), word embeddings (Ng and Abrecht, 2015), or use contextualizedembedding-based methods (Zhang et al., 2019; Zhao et al., 2019) to measure the semantic similarity between references and summaries. Annotation-based Evaluation. Some methods directly ask human annotators to rate summaries following some guidelines, e.g. Responsiveness, which measures the overall quality (relevance, fluency and readability) of summaries, and Pyramid (Nenkova and Passonneau, 2004), which measures summaries’ relevance. Recently, systems have been developed to ease the construction of Pyramid scores, e.g. (Hirao et al., 2018; Yang et al., 2016; Gao et al., 2019b; Shapira et al., 2019), but they still require human-annotated Summary Content Units (SCUs) to produce reliable scores. Besides SCUs, recent work has explored eliciting preferences over summaries (Zopf, 2018; Gao et al., 2018, 2019a) and annotations of important bi-grams (P.V.S and Meyer, 2017) to derive summary ratings. Some methods collect human ratings on a small number of summaries to train an evaluation function. Peyrard et al. (2017); Peyrard and Gurevych (2018) propose to learn an evaluation function from Pyramid and Responsiveness scores, by using"
2020.acl-main.124,D19-1327,0,0.0354248,"Missing"
2020.acl-main.124,P02-1040,0,0.122442,"Missing"
2020.acl-main.124,N18-1202,0,0.0170867,"ce metrics (Goldstein et al., 2000). But these methods have low correlation with human ratings at summary level (see §3). Scialom et al. (2019) propose to generate questions from source documents and evaluate the relevance of summaries by counting how many questions the summaries can answer. However, they do not detail how to generate questions from source documents; also, it remains unclear whether their method works for evaluating multi-document summaries. Sun and Nenkova (2019) propose a single-document summary evaluation method, which measures the cosine similarity of the ELMo embeddings (Peters et al., 2018) of the source document and the summary. In §3, we show that their method performs poorly in evaluating multi-document summaries. SUPERT extends their method by using more advanced contextualized embeddings and more effective text alignment/matching methods (§4), and by introducing pseudo references (§5). 3 Datasets, Baselines and Upper Bounds Datasets. We use two multi-document summarization datasets from the Text Analysis Conference (TAC)2 shared tasks: TAC’08 and TAC’09. In line with Louis and Nenkova (2013), we only use the initial summaries (the A part) in these datasets. TAC’08 includes"
2020.acl-main.124,D19-1320,0,0.104589,"Missing"
2020.acl-main.124,P19-1101,0,0.0371373,"a summary evaluation function. Evaluation in multidocument summarization is particularly expensive: Lin (2004b) reports that it requires 3,000 hours of human effort to evaluate the summaries from the Document Understanding Conferences (DUC)1 . http://duc.nist.gov/ Summary relevance score Doc N Introduction 1 Salient Sentences Extractor Semantic Similarity Measurement To reduce the expenses for evaluating multidocument summaries, we investigate unsupervised evaluation methods, which require neither human annotations nor reference summaries. In particular, we focus on evaluating the relevance (Peyrard, 2019) of multi-document summaries, i.e. measuring how much salient information from the source documents is covered by the summaries. There exist a few unsupervised evaluation methods (Louis and Nenkova, 2013; Sun and Nenkova, 2019), but they have low correlation with human relevance ratings at summary level: given multiple summaries for the same source documents, these methods can hardly distinguish summaries with high relevance from those with low relevance (see §3). Contributions. First, to better measure the semantic overlap between source documents and machine-generated summaries, we propose t"
2020.acl-main.124,D18-1085,0,0.169507,"Second, we use SUPERT as reward functions to guide Reinforcement Learning (RL) based extractive summarizers. We show it outperforms the state-of-the-art unsupervised summarization methods (in multiple ROUGE metrics). 2 Related Work Reference-based Evaluation. Popular metrics like ROUGE (Lin, 2004a), BLEU (Papineni et al., 2002) and METEOR (Lavie and Denkowski, 2009) fall into this category. They require (preferably, multiple) human written references and measure the relevance of a summary by comparing its overlapping word sequences with references. More recent work extends ROUGE with WordNet (ShafieiBavani et al., 2018a), word embeddings (Ng and Abrecht, 2015), or use contextualizedembedding-based methods (Zhang et al., 2019; Zhao et al., 2019) to measure the semantic similarity between references and summaries. Annotation-based Evaluation. Some methods directly ask human annotators to rate summaries following some guidelines, e.g. Responsiveness, which measures the overall quality (relevance, fluency and readability) of summaries, and Pyramid (Nenkova and Passonneau, 2004), which measures summaries’ relevance. Recently, systems have been developed to ease the construction of Pyramid scores, e.g. (Hirao et"
2020.acl-main.124,W17-4510,0,0.142962,"h measures summaries’ relevance. Recently, systems have been developed to ease the construction of Pyramid scores, e.g. (Hirao et al., 2018; Yang et al., 2016; Gao et al., 2019b; Shapira et al., 2019), but they still require human-annotated Summary Content Units (SCUs) to produce reliable scores. Besides SCUs, recent work has explored eliciting preferences over summaries (Zopf, 2018; Gao et al., 2018, 2019a) and annotations of important bi-grams (P.V.S and Meyer, 2017) to derive summary ratings. Some methods collect human ratings on a small number of summaries to train an evaluation function. Peyrard et al. (2017); Peyrard and Gurevych (2018) propose to learn an evaluation function from Pyramid and Responsiveness scores, by using classic supervised learning methods with hand-crafted features. ShafieiBavani et al. (2018b) use the same idea but design corpus based and lexical resource based word embeddings to build the features. B¨ohm et al. (2019) train a BERT-based evaluation function with 2,500 human ratings for 500 machinegenerated summaries from the CNN/DailyMail dataset; their method correlates better with human ratings than ROUGE and BLEU. However, as their method is designed for evaluating single"
2020.acl-main.124,N18-2103,0,0.126418,"elevance. Recently, systems have been developed to ease the construction of Pyramid scores, e.g. (Hirao et al., 2018; Yang et al., 2016; Gao et al., 2019b; Shapira et al., 2019), but they still require human-annotated Summary Content Units (SCUs) to produce reliable scores. Besides SCUs, recent work has explored eliciting preferences over summaries (Zopf, 2018; Gao et al., 2018, 2019a) and annotations of important bi-grams (P.V.S and Meyer, 2017) to derive summary ratings. Some methods collect human ratings on a small number of summaries to train an evaluation function. Peyrard et al. (2017); Peyrard and Gurevych (2018) propose to learn an evaluation function from Pyramid and Responsiveness scores, by using classic supervised learning methods with hand-crafted features. ShafieiBavani et al. (2018b) use the same idea but design corpus based and lexical resource based word embeddings to build the features. B¨ohm et al. (2019) train a BERT-based evaluation function with 2,500 human ratings for 500 machinegenerated summaries from the CNN/DailyMail dataset; their method correlates better with human ratings than ROUGE and BLEU. However, as their method is designed for evaluating single-document summaries, it corre"
2020.acl-main.124,P17-1124,0,0.13388,"Missing"
2020.acl-main.124,D19-1410,0,0.0172833,"ries. There exist a few unsupervised evaluation methods (Louis and Nenkova, 2013; Sun and Nenkova, 2019), but they have low correlation with human relevance ratings at summary level: given multiple summaries for the same source documents, these methods can hardly distinguish summaries with high relevance from those with low relevance (see §3). Contributions. First, to better measure the semantic overlap between source documents and machine-generated summaries, we propose to use state-of-the-art contextualized text encoders, e.g. BERT (Devlin et al., 2019) and its variant SentenceBERT (SBERT) (Reimers and Gurevych, 2019), which is optimized for measuring semantic similarity between sentences, to develop unsupervised evaluation methods. We measure the relevance of a summary in two steps: (i) identifying the salient information in the input documents, to build a pseudo reference summary, and (ii) measuring the semantic overlap between the pseudo reference and the 1347 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1347–1354 c July 5 - 10, 2020. 2020 Association for Computational Linguistics summary to be evaluated. The resulting evaluation method is called SUPERT"
2020.acl-main.124,D14-1075,0,0.220224,"Missing"
2020.acl-main.124,D12-1024,0,0.0604084,"-based evaluation function with 2,500 human ratings for 500 machinegenerated summaries from the CNN/DailyMail dataset; their method correlates better with human ratings than ROUGE and BLEU. However, as their method is designed for evaluating single-document summaries, it correlates poorly with the Pyramid scores for multi-document summaries (see §3). Unsupervised Evaluation. Louis and Nenkova (2013) measure the relevance of a summary using multiple heuristics, for example by computing the Jensen-Shannon (JS) divergence between the word distributions in the summary and in the source documents. Ryang and Abekawa (2012); Rioux et al. (2014) develop evaluation heuristics inspired by the maximal marginal relevance metrics (Goldstein et al., 2000). But these methods have low correlation with human ratings at summary level (see §3). Scialom et al. (2019) propose to generate questions from source documents and evaluate the relevance of summaries by counting how many questions the summaries can answer. However, they do not detail how to generate questions from source documents; also, it remains unclear whether their method works for evaluating multi-document summaries. Sun and Nenkova (2019) propose a single-docum"
2020.acl-main.124,N19-1072,1,0.852947,"e contextualizedembedding-based methods (Zhang et al., 2019; Zhao et al., 2019) to measure the semantic similarity between references and summaries. Annotation-based Evaluation. Some methods directly ask human annotators to rate summaries following some guidelines, e.g. Responsiveness, which measures the overall quality (relevance, fluency and readability) of summaries, and Pyramid (Nenkova and Passonneau, 2004), which measures summaries’ relevance. Recently, systems have been developed to ease the construction of Pyramid scores, e.g. (Hirao et al., 2018; Yang et al., 2016; Gao et al., 2019b; Shapira et al., 2019), but they still require human-annotated Summary Content Units (SCUs) to produce reliable scores. Besides SCUs, recent work has explored eliciting preferences over summaries (Zopf, 2018; Gao et al., 2018, 2019a) and annotations of important bi-grams (P.V.S and Meyer, 2017) to derive summary ratings. Some methods collect human ratings on a small number of summaries to train an evaluation function. Peyrard et al. (2017); Peyrard and Gurevych (2018) propose to learn an evaluation function from Pyramid and Responsiveness scores, by using classic supervised learning methods with hand-crafted featur"
2020.acl-main.124,D19-1116,0,0.329663,"ding Conferences (DUC)1 . http://duc.nist.gov/ Summary relevance score Doc N Introduction 1 Salient Sentences Extractor Semantic Similarity Measurement To reduce the expenses for evaluating multidocument summaries, we investigate unsupervised evaluation methods, which require neither human annotations nor reference summaries. In particular, we focus on evaluating the relevance (Peyrard, 2019) of multi-document summaries, i.e. measuring how much salient information from the source documents is covered by the summaries. There exist a few unsupervised evaluation methods (Louis and Nenkova, 2013; Sun and Nenkova, 2019), but they have low correlation with human relevance ratings at summary level: given multiple summaries for the same source documents, these methods can hardly distinguish summaries with high relevance from those with low relevance (see §3). Contributions. First, to better measure the semantic overlap between source documents and machine-generated summaries, we propose to use state-of-the-art contextualized text encoders, e.g. BERT (Devlin et al., 2019) and its variant SentenceBERT (SBERT) (Reimers and Gurevych, 2019), which is optimized for measuring semantic similarity between sentences, to"
2020.acl-main.124,D15-1228,0,0.0613894,"Missing"
2020.acl-main.124,D19-1053,1,0.817691,"rizers. All source code is available at https://github.com/yg211/ acl20-ref-free-eval. 1 Doc 2 Pseudo reference summary Figure 1: Workflow of SUPERT. Evaluating the quality of machine-generated summaries is a highly laborious and hence expensive task. Most existing evaluation methods require certain forms of human involvement, thus are supervised: they either directly let humans rate the generated summaries (e.g. Pyramid (Nenkova and Passonneau, 2004)), elicit human-written reference summaries and measure their overlap with the generated summaries (e.g. using ROGUE (Lin, 2004a) or MoverScore (Zhao et al., 2019)), or collect some human annotations (e.g. preferences over pairs of summaries (Gao et al., 2019a)) to learn a summary evaluation function. Evaluation in multidocument summarization is particularly expensive: Lin (2004b) reports that it requires 3,000 hours of human effort to evaluate the summaries from the Document Understanding Conferences (DUC)1 . http://duc.nist.gov/ Summary relevance score Doc N Introduction 1 Salient Sentences Extractor Semantic Similarity Measurement To reduce the expenses for evaluating multidocument summaries, we investigate unsupervised evaluation methods, which requ"
2020.acl-main.124,P19-1628,0,0.0257646,"onsider two variants: the individual-graph version, which builds a graph for each source document and selects top-K sentences (SLR) or the centers (SC) from each graph; and the global-graph version, which builds a graph considering all sentences across all source documents for the same topic, and selects the top-M sentences (SLR) or all the centers (SC) in this large graph. According to our preliminary experiments on 20 randomly sampled topics, we set K = 10 and M = 90. Position-Aware Graphs. PacSum is a recently proposed graph-based method to select salient sentences from multiple documents (Zheng and Lapata, 2019). In PacSum, a sentence is more likely to be selected if it has higher average similarity with its succeeding sentences and lower average similarity with its preceding sentences. This strategy allows PacSum to prioritize the selection of earlyposition and “semantically central” sentences. We further extend PacSum by using SBERT to measure sentences similarity (the resulting method is denoted as SPS) and consider both the individualand global-graph versions of SPS. Furthermore, we propose a method called Top+Clique (TC), which selects the top-N sentences and the semantically central non-top-N s"
2020.acl-main.124,N18-1152,0,0.0187871,"tly ask human annotators to rate summaries following some guidelines, e.g. Responsiveness, which measures the overall quality (relevance, fluency and readability) of summaries, and Pyramid (Nenkova and Passonneau, 2004), which measures summaries’ relevance. Recently, systems have been developed to ease the construction of Pyramid scores, e.g. (Hirao et al., 2018; Yang et al., 2016; Gao et al., 2019b; Shapira et al., 2019), but they still require human-annotated Summary Content Units (SCUs) to produce reliable scores. Besides SCUs, recent work has explored eliciting preferences over summaries (Zopf, 2018; Gao et al., 2018, 2019a) and annotations of important bi-grams (P.V.S and Meyer, 2017) to derive summary ratings. Some methods collect human ratings on a small number of summaries to train an evaluation function. Peyrard et al. (2017); Peyrard and Gurevych (2018) propose to learn an evaluation function from Pyramid and Responsiveness scores, by using classic supervised learning methods with hand-crafted features. ShafieiBavani et al. (2018b) use the same idea but design corpus based and lexical resource based word embeddings to build the features. B¨ohm et al. (2019) train a BERT-based evalu"
2020.acl-main.124,D15-1222,0,\N,Missing
2020.acl-main.124,C18-1077,0,\N,Missing
2020.acl-main.151,Q19-1038,0,0.362553,"ality labels) (Specia et al., 2010) or language-specific preprocessing like semantic parsing (Lo et al., 2014; Lo, 2019), both hindering the wide applicability of the proposed metrics. Moreover, they have also typically exhibited performance levels well below those of standard reference-based metrics (Ma et al., 2019). In this work, we comparatively evaluate a number of reference-free MT evaluation metrics that build on the most recent developments in multilingual representation learning, namely cross-lingual contextualized embeddings (Devlin et al., 2019) and cross-lingual sentence encoders (Artetxe and Schwenk, 2019). We investigate two types of crosslingual reference-free metrics: (1) Soft token-level alignment metrics find the optimal soft alignment between source sentence and system translation using Word Mover’s Distance (WMD) (Kusner et al., 2015). Zhao et al. (2019) recently demonstrated that WMD operating on BERT representations (Devlin et al., 2019) substantially outperforms baseline MT evaluation metrics in the reference-based setting. In this work, we investigate whether WMD can yield comparable success in the reference-free (i.e., cross-lingual) setup; (2) Sentence-level similarity metrics meas"
2020.acl-main.151,W17-4755,0,0.423254,"erview of both reference-based MT evaluation metrics and recent research efforts towards reference-free MT evaluation, which leverage cross-lingual semantic representations and unsupervised MT techniques. Reference-based MT evaluation. Most of the commonly used evaluation metrics in MT compare system and reference translations. They are often based on surface forms such as n-gram overlaps like BLEU (Papineni et al., 2002), SentBLEU, NIST (Doddington, 2002), chrF++ (Popovi´c, 2017) or METEOR++(Guo and Hu, 2019). They have been extensively tested and compared in recent WMT metrics shared tasks (Bojar et al., 2017a; Ma et al., 2018a, 2019). These metrics, however, operate at the surface level, and by design fail to recognize semantic equivalence lacking lexical overlap. To overcome these limitations, some research efforts exploited static word embeddings (Mikolov et al., 2013b) and trained embedding-based supervised metrics on sufficiently large datasets with available human judgments of translation quality (Shimanaka 1657 et al., 2018). With the development of contextual word embeddings (Peters et al., 2018; Devlin et al., 2019), we have witnessed proposals of semantic metrics that account for word or"
2020.acl-main.151,S17-2001,0,0.0423846,"at are best-aligned mutual translations. YiSi2-SRL optionally combines an additional similarity score based on the alignment over the semantic structures (e.g., semantic roles and frames). Both metrics are reference-free, but YiSi-2-SRL is not resource-lean as it requires a semantic parser for both languages. Moreover, in contrast to our proposed metrics, they do not mitigate the misalignment of cross-lingual embedding spaces and do not integrate a target-side language model, which we identify to be crucial components. Recent progress in cross-lingual semantic similarity (Agirre et al., 2016; Cer et al., 2017) and unsupervised MT (Artetxe and Schwenk, 2019) has also led to novel reference-free metrics. For instance, Yankovskaya et al. (2019) propose to train a metric combining multilingual embeddings extracted from M-BERT and LASER (Artetxe and Schwenk, 2019) together with the log-probability scores from neural machine translation. Our work differs from that of Yankovskaya et al. (2019) in one crucial aspect: the cross-lingual reference-free metrics that we investigate and benchmark do not require any human supervision. Cross-lingual Representations. Cross-lingual text representations offer a prosp"
2020.acl-main.151,P19-1264,0,0.0652817,"sk has clearly defined and unambiguous labels and, in most cases, that an instance can be assigned few labels. These assumptions, however, do not hold for natural language generation (NLG) tasks like machine trans1 https://github.com/AIPHES/ ACL20-Reference-Free-MT-Evaluation The first remedy is to replace the hard symbolic comparison of natural language “labels” with a soft comparison of texts’ meaning, using semantic vector space representations. Recently, a number of MT evaluation methods appeared focusing on semantic comparison of reference and system translations (Shimanaka et al., 2018; Clark et al., 2019; Zhao et al., 2019). While these correlate better than n-gram overlap metrics with human assessments, they do not address inherent limitations stemming from the need for reference translations, namely: (1) references are expensive to obtain; (2) they assume a single correct solution and bias the evaluation, both automatic and human (Dreyer and Marcu, 2012; Fomicheva and Specia, 2016), and (3) limitation of MT evaluation to language pairs with available parallel data. Reliable reference-free evaluation metrics, directly measuring the (semantic) correspondence between the source language text a"
2020.acl-main.151,2020.acl-main.747,0,0.146174,"Missing"
2020.acl-main.151,N19-1423,0,0.445273,"her non-negligible supervision (i.e., human translation quality labels) (Specia et al., 2010) or language-specific preprocessing like semantic parsing (Lo et al., 2014; Lo, 2019), both hindering the wide applicability of the proposed metrics. Moreover, they have also typically exhibited performance levels well below those of standard reference-based metrics (Ma et al., 2019). In this work, we comparatively evaluate a number of reference-free MT evaluation metrics that build on the most recent developments in multilingual representation learning, namely cross-lingual contextualized embeddings (Devlin et al., 2019) and cross-lingual sentence encoders (Artetxe and Schwenk, 2019). We investigate two types of crosslingual reference-free metrics: (1) Soft token-level alignment metrics find the optimal soft alignment between source sentence and system translation using Word Mover’s Distance (WMD) (Kusner et al., 2015). Zhao et al. (2019) recently demonstrated that WMD operating on BERT representations (Devlin et al., 2019) substantially outperforms baseline MT evaluation metrics in the reference-based setting. In this work, we investigate whether WMD can yield comparable success in the reference-free (i.e.,"
2020.acl-main.151,N12-1017,0,0.0336737,"ge “labels” with a soft comparison of texts’ meaning, using semantic vector space representations. Recently, a number of MT evaluation methods appeared focusing on semantic comparison of reference and system translations (Shimanaka et al., 2018; Clark et al., 2019; Zhao et al., 2019). While these correlate better than n-gram overlap metrics with human assessments, they do not address inherent limitations stemming from the need for reference translations, namely: (1) references are expensive to obtain; (2) they assume a single correct solution and bias the evaluation, both automatic and human (Dreyer and Marcu, 2012; Fomicheva and Specia, 2016), and (3) limitation of MT evaluation to language pairs with available parallel data. Reliable reference-free evaluation metrics, directly measuring the (semantic) correspondence between the source language text and system translation, would remove the need for human references and allow for unlimited MT evaluations: any 1656 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1656–1671 c July 5 - 10, 2020. 2020 Association for Computational Linguistics monolingual corpus could be used for evaluating MT systems. However, t"
2020.acl-main.151,N13-1073,0,0.0603668,"e resulting shared vector space. We investigate two resource-lean choices for the re-mapping function f . Linear Cross-lingual Projection (CLP). Following related work (Schuster et al., 2019), we re-map contextualized embedding spaces using linear projection. Given ` and k, we stack all vectors of the source language words and target language words for pairs D, respectively, to form matrices X` and Xk ∈ Rn×d , with d as the embedding dimension and n as the number of word or sentence alignments. The word pairs we use to calibrate MBERT are extracted from EuroParl (Koehn, 2005) using FastAlign (Dyer et al., 2013), and the sentence pairs to calibrate LASER are sampled directly from EuroParl.4 Mikolov et al. (2013a) propose to learn a projection matrix W ∈ Rd×d by minimizing the Euclidean distance beetween the projected source language vectors and their corresponding target language vectors: min kW X` − Xk k2 . W Xing et al. (2015) achieve further improvement on the task of bilingual lexicon induction (BLI) by constraining W to an orthogonal matrix, i.e., such that W |W = I. This turns the optimization into the well-known Procrustes problem (Sch¨onemann, 1966) with the following closed-form solution: ˆ"
2020.acl-main.151,P16-2013,0,0.0239696,"comparison of texts’ meaning, using semantic vector space representations. Recently, a number of MT evaluation methods appeared focusing on semantic comparison of reference and system translations (Shimanaka et al., 2018; Clark et al., 2019; Zhao et al., 2019). While these correlate better than n-gram overlap metrics with human assessments, they do not address inherent limitations stemming from the need for reference translations, namely: (1) references are expensive to obtain; (2) they assume a single correct solution and bias the evaluation, both automatic and human (Dreyer and Marcu, 2012; Fomicheva and Specia, 2016), and (3) limitation of MT evaluation to language pairs with available parallel data. Reliable reference-free evaluation metrics, directly measuring the (semantic) correspondence between the source language text and system translation, would remove the need for human references and allow for unlimited MT evaluations: any 1656 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1656–1671 c July 5 - 10, 2020. 2020 Association for Computational Linguistics monolingual corpus could be used for evaluating MT systems. However, the proposals of referencefree"
2020.acl-main.151,P19-1070,1,0.907836,"Missing"
2020.acl-main.151,2020.acl-main.675,1,0.843387,"Missing"
2020.acl-main.151,W19-5357,0,0.0277596,"dedicated to the study of automatic evaluation metrics for machine translation. Here, we provide an overview of both reference-based MT evaluation metrics and recent research efforts towards reference-free MT evaluation, which leverage cross-lingual semantic representations and unsupervised MT techniques. Reference-based MT evaluation. Most of the commonly used evaluation metrics in MT compare system and reference translations. They are often based on surface forms such as n-gram overlaps like BLEU (Papineni et al., 2002), SentBLEU, NIST (Doddington, 2002), chrF++ (Popovi´c, 2017) or METEOR++(Guo and Hu, 2019). They have been extensively tested and compared in recent WMT metrics shared tasks (Bojar et al., 2017a; Ma et al., 2018a, 2019). These metrics, however, operate at the surface level, and by design fail to recognize semantic equivalence lacking lexical overlap. To overcome these limitations, some research efforts exploited static word embeddings (Mikolov et al., 2013b) and trained embedding-based supervised metrics on sufficiently large datasets with available human judgments of translation quality (Shimanaka 1657 et al., 2018). With the development of contextual word embeddings (Peters et al"
2020.acl-main.151,D18-1330,0,0.028184,"Missing"
2020.acl-main.151,C12-1089,0,0.0565539,"r instance, Yankovskaya et al. (2019) propose to train a metric combining multilingual embeddings extracted from M-BERT and LASER (Artetxe and Schwenk, 2019) together with the log-probability scores from neural machine translation. Our work differs from that of Yankovskaya et al. (2019) in one crucial aspect: the cross-lingual reference-free metrics that we investigate and benchmark do not require any human supervision. Cross-lingual Representations. Cross-lingual text representations offer a prospect of modeling meaning across languages and support crosslingual transfer for downstream tasks (Klementiev et al., 2012; R¨uckl´e et al., 2018; Glavaˇs et al., 2019; Josifoski et al., 2019; Conneau et al., 2020). Most recently, the (massively) multilingual encoders, such as multilingual M-BERT (Devlin et al., 2019), XLM-on-RoBERTa (Conneau et al., 2020), and (sentence-based) LASER, have profiled themselves as state-of-the-art solutions for (massively) multilingual semantic encoding of text. While LASER has been jointly trained on parallel data of 93 languages, M-BERT has been trained on the concatenation of monolingual data in more than 100 languages, without any cross-lingual mapping signal. There has been a"
2020.acl-main.151,2005.mtsummit-papers.11,0,0.0425747,"(wk ) are better aligned in the resulting shared vector space. We investigate two resource-lean choices for the re-mapping function f . Linear Cross-lingual Projection (CLP). Following related work (Schuster et al., 2019), we re-map contextualized embedding spaces using linear projection. Given ` and k, we stack all vectors of the source language words and target language words for pairs D, respectively, to form matrices X` and Xk ∈ Rn×d , with d as the embedding dimension and n as the number of word or sentence alignments. The word pairs we use to calibrate MBERT are extracted from EuroParl (Koehn, 2005) using FastAlign (Dyer et al., 2013), and the sentence pairs to calibrate LASER are sampled directly from EuroParl.4 Mikolov et al. (2013a) propose to learn a projection matrix W ∈ Rd×d by minimizing the Euclidean distance beetween the projected source language vectors and their corresponding target language vectors: min kW X` − Xk k2 . W Xing et al. (2015) achieve further improvement on the task of bilingual lexicon induction (BLI) by constraining W to an orthogonal matrix, i.e., such that W |W = I. This turns the optimization into the well-known Procrustes problem (Sch¨onemann, 1966) with th"
2020.acl-main.151,P07-2045,0,0.00620842,"cs), Latvian (lv), Finnish (fi), Russian (ru), and Turkish (tr), Gujarati (gu), Kazakh (kk), Lithuanian (lt) and Estonian (et). Each language pair in WMT17-19 has approximately 3,000 source sentences, each associated to one reference translation and to the automatic translations generated by participating systems. 4.2 Baselines We compare with a range of reference-free metrics: ibm1-morpheme and ibm1-pos4gram (Popovi´c, 2012), LASIM (Yankovskaya et al., 2019), LP (Yankovskaya et al., 2019), YiSi-2 and YiSi-2-srl (Lo, 2019), and reference-based baselines BLEU (Papineni et al., 2002), SentBLEU (Koehn et al., 2007) and ChrF++ (Popovi´c, 2017) for MT evaluation (see §2).6 The main results are reported on WMT17. We report the results obtained on WMT18 and WMT19 in the Appendix. 6 The code of these unsupervised metrics is not released, thus we compare to their official results on WMT19 only. 1660 Setting Metrics cs-en de-en fi-en lv-en ru-en tr-en zh-en Average m(y∗ , y) SENT BLEU CHR F++ 43.5 52.3 43.2 53.4 57.1 39.3 67.8 52.0 48.4 53.8 58.8 61.4 51.2 59.3 48.1 57.9 22.7 32.6 37.1 40.2 34.8 26.0 41.4 48.3 26.7 42.5 36.3 42.3 48.2 46.7 34.0 41.1 40.5 28.1 42.0 48.6 45.5 48.5 36.0 44.7 31.3 46.2 42.2 49.4 4"
2020.acl-main.151,W07-0734,0,0.151943,"i-mannheim.de, yang.gao@rhul.ac.uk {maxime.peyrard,robert.west}@epfl.ch Abstract lation (MT) (Bahdanau et al., 2015; Johnson et al., 2017) and text summarization (Rush et al., 2015; Tan et al., 2017), where we do not predict a single discrete label but generate natural language text. Thus, the set of labels for NLG is neither clearly defined nor finite. Yet, the standard evaluation protocols for NLG still predominantly follow the described default paradigm: (1) evaluation datasets come with human-created reference texts and (2) evaluation metrics, e.g., BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007) for MT and ROUGE (Lin and Hovy, 2003) for summarization, count the exact “label” (i.e., n-gram) matches between reference and system-generated text. In other words, established NLG evaluation compares semantically ambiguous labels from an unbounded set (i.e., natural language texts) via hard symbolic matching (i.e., string overlap). Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation ("
2020.acl-main.151,N03-1020,0,0.632893,"Missing"
2020.acl-main.151,W19-5358,0,0.464572,"remove the need for human references and allow for unlimited MT evaluations: any 1656 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1656–1671 c July 5 - 10, 2020. 2020 Association for Computational Linguistics monolingual corpus could be used for evaluating MT systems. However, the proposals of referencefree MT evaluation metrics have been few and far apart and have required either non-negligible supervision (i.e., human translation quality labels) (Specia et al., 2010) or language-specific preprocessing like semantic parsing (Lo et al., 2014; Lo, 2019), both hindering the wide applicability of the proposed metrics. Moreover, they have also typically exhibited performance levels well below those of standard reference-based metrics (Ma et al., 2019). In this work, we comparatively evaluate a number of reference-free MT evaluation metrics that build on the most recent developments in multilingual representation learning, namely cross-lingual contextualized embeddings (Devlin et al., 2019) and cross-lingual sentence encoders (Artetxe and Schwenk, 2019). We investigate two types of crosslingual reference-free metrics: (1) Soft token-level alignm"
2020.acl-main.151,P14-2124,0,0.184908,"ranslation, would remove the need for human references and allow for unlimited MT evaluations: any 1656 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1656–1671 c July 5 - 10, 2020. 2020 Association for Computational Linguistics monolingual corpus could be used for evaluating MT systems. However, the proposals of referencefree MT evaluation metrics have been few and far apart and have required either non-negligible supervision (i.e., human translation quality labels) (Specia et al., 2010) or language-specific preprocessing like semantic parsing (Lo et al., 2014; Lo, 2019), both hindering the wide applicability of the proposed metrics. Moreover, they have also typically exhibited performance levels well below those of standard reference-based metrics (Ma et al., 2019). In this work, we comparatively evaluate a number of reference-free MT evaluation metrics that build on the most recent developments in multilingual representation learning, namely cross-lingual contextualized embeddings (Devlin et al., 2019) and cross-lingual sentence encoders (Artetxe and Schwenk, 2019). We investigate two types of crosslingual reference-free metrics: (1) Soft token-l"
2020.acl-main.151,W18-6450,0,0.165948,"nce-based MT evaluation metrics and recent research efforts towards reference-free MT evaluation, which leverage cross-lingual semantic representations and unsupervised MT techniques. Reference-based MT evaluation. Most of the commonly used evaluation metrics in MT compare system and reference translations. They are often based on surface forms such as n-gram overlaps like BLEU (Papineni et al., 2002), SentBLEU, NIST (Doddington, 2002), chrF++ (Popovi´c, 2017) or METEOR++(Guo and Hu, 2019). They have been extensively tested and compared in recent WMT metrics shared tasks (Bojar et al., 2017a; Ma et al., 2018a, 2019). These metrics, however, operate at the surface level, and by design fail to recognize semantic equivalence lacking lexical overlap. To overcome these limitations, some research efforts exploited static word embeddings (Mikolov et al., 2013b) and trained embedding-based supervised metrics on sufficiently large datasets with available human judgments of translation quality (Shimanaka 1657 et al., 2018). With the development of contextual word embeddings (Peters et al., 2018; Devlin et al., 2019), we have witnessed proposals of semantic metrics that account for word order. For example,"
2020.acl-main.151,W19-5302,0,0.312607,"c July 5 - 10, 2020. 2020 Association for Computational Linguistics monolingual corpus could be used for evaluating MT systems. However, the proposals of referencefree MT evaluation metrics have been few and far apart and have required either non-negligible supervision (i.e., human translation quality labels) (Specia et al., 2010) or language-specific preprocessing like semantic parsing (Lo et al., 2014; Lo, 2019), both hindering the wide applicability of the proposed metrics. Moreover, they have also typically exhibited performance levels well below those of standard reference-based metrics (Ma et al., 2019). In this work, we comparatively evaluate a number of reference-free MT evaluation metrics that build on the most recent developments in multilingual representation learning, namely cross-lingual contextualized embeddings (Devlin et al., 2019) and cross-lingual sentence encoders (Artetxe and Schwenk, 2019). We investigate two types of crosslingual reference-free metrics: (1) Soft token-level alignment metrics find the optimal soft alignment between source sentence and system translation using Word Mover’s Distance (WMD) (Kusner et al., 2015). Zhao et al. (2019) recently demonstrated that WMD o"
2020.acl-main.151,N18-1202,0,0.0262973,"nd Hu, 2019). They have been extensively tested and compared in recent WMT metrics shared tasks (Bojar et al., 2017a; Ma et al., 2018a, 2019). These metrics, however, operate at the surface level, and by design fail to recognize semantic equivalence lacking lexical overlap. To overcome these limitations, some research efforts exploited static word embeddings (Mikolov et al., 2013b) and trained embedding-based supervised metrics on sufficiently large datasets with available human judgments of translation quality (Shimanaka 1657 et al., 2018). With the development of contextual word embeddings (Peters et al., 2018; Devlin et al., 2019), we have witnessed proposals of semantic metrics that account for word order. For example, Clark et al. (2019) introduce a semantic metric relying on sentence mover’s similarity and the contextualized ELMo embeddings (Peters et al., 2018). Similarly, Zhang et al. (2019) describe a reference-based semantic similarity metric based on contextualized BERT representations (Devlin et al., 2019). Zhao et al. (2019) generalize this line of work with their MoverScore metric, which computes the mover’s distance, i.e., the optimal soft alignment between tokens of the two sentences,"
2020.acl-main.151,N19-1162,0,0.031059,"ntence translations.3 To this end, we apply two simple, weakly-supervised linear projection methods for post-hoc improvement of the cross-lingual alignments in these multilingual representation spaces. Notation. Let D = {(w`1 , wk1 ), . . . , (w`n , wkn )} be a set of matched word or sentence pairs from two different languages ` and k. We define a remapping function f such that any f (E(w` )) and E(wk ) are better aligned in the resulting shared vector space. We investigate two resource-lean choices for the re-mapping function f . Linear Cross-lingual Projection (CLP). Following related work (Schuster et al., 2019), we re-map contextualized embedding spaces using linear projection. Given ` and k, we stack all vectors of the source language words and target language words for pairs D, respectively, to form matrices X` and Xk ∈ Rn×d , with d as the embedding dimension and n as the number of word or sentence alignments. The word pairs we use to calibrate MBERT are extracted from EuroParl (Koehn, 2005) using FastAlign (Dyer et al., 2013), and the sentence pairs to calibrate LASER are sampled directly from EuroParl.4 Mikolov et al. (2013a) propose to learn a projection matrix W ∈ Rd×d by minimizing the Eucli"
2020.acl-main.151,P19-1493,0,0.0396949,"019; Conneau et al., 2020). Most recently, the (massively) multilingual encoders, such as multilingual M-BERT (Devlin et al., 2019), XLM-on-RoBERTa (Conneau et al., 2020), and (sentence-based) LASER, have profiled themselves as state-of-the-art solutions for (massively) multilingual semantic encoding of text. While LASER has been jointly trained on parallel data of 93 languages, M-BERT has been trained on the concatenation of monolingual data in more than 100 languages, without any cross-lingual mapping signal. There has been a recent vivid discussion on the cross-lingual abilities of M-BERT (Pires et al., 2019; K et al., 2020; Cao et al., 2020). In particular, Cao et al. (2020) show that M-BERT often yields disparate vector space representations for mutual translations and propose a multilingual remapping based on parallel corpora, to remedy for this issue. In this work, we introduce re-mapping solutions that are resource-leaner and require easyto-obtain limited-size word translation dictionaries rather than large parallel corpora. 3 Reference-Free MT Evaluation Metrics In the following, we use x to denote a source sentence (i.e., a sequence of tokens in the source language), y to denote a system t"
2020.acl-main.151,W18-6456,0,0.0184043,"etup assumes that the task has clearly defined and unambiguous labels and, in most cases, that an instance can be assigned few labels. These assumptions, however, do not hold for natural language generation (NLG) tasks like machine trans1 https://github.com/AIPHES/ ACL20-Reference-Free-MT-Evaluation The first remedy is to replace the hard symbolic comparison of natural language “labels” with a soft comparison of texts’ meaning, using semantic vector space representations. Recently, a number of MT evaluation methods appeared focusing on semantic comparison of reference and system translations (Shimanaka et al., 2018; Clark et al., 2019; Zhao et al., 2019). While these correlate better than n-gram overlap metrics with human assessments, they do not address inherent limitations stemming from the need for reference translations, namely: (1) references are expensive to obtain; (2) they assume a single correct solution and bias the evaluation, both automatic and human (Dreyer and Marcu, 2012; Fomicheva and Specia, 2016), and (3) limitation of MT evaluation to language pairs with available parallel data. Reliable reference-free evaluation metrics, directly measuring the (semantic) correspondence between the so"
2020.acl-main.151,W12-3116,0,0.0409835,"Missing"
2020.acl-main.151,P18-1072,0,0.0464211,"Missing"
2020.acl-main.151,W17-4770,0,0.0274625,"Missing"
2020.acl-main.151,P13-4014,0,0.0327234,"Missing"
2020.acl-main.151,P17-1049,0,0.0342672,"temlevel human judgments that we use for evaluating the quality of our reference-free metrics. The segment-level judgments assign one direct assessment (DA) score to each pair of system and human translation, while system-level judgments associate each system with a single DA score averaged across all pairs in the dataset. We initially suspected the DA scores to be biased for our setup—which compares x with y—as they are based on comparing y? and y. Indeed, it is known that (especially) human professional translators “improve” y? , e.g., by making it more readable, relative to the original x (Rabinovich et al., 2017). We investigated the validity of DA scores by collecting human assessments in the cross-lingual settings (CLDA), where annotators directly compare source and translation pairs (x, y) from the WMT17 dataset. This small-scale manual analysis hints that DA scores are a valid proxy for CLDA. Therefore, we decided to treat them as reliable scores for our setup and evaluate our proposed metrics by comparing their correlation with DA scores. 6 Conclusion Existing semantically-motivated metrics for reference-free evaluation of MT systems have so far displayed rather poor correlation with human estima"
2020.acl-main.151,P02-1040,0,\N,Missing
2020.acl-main.151,W11-2109,0,\N,Missing
2020.acl-main.151,N15-1104,0,\N,Missing
2020.acl-main.151,S16-1081,0,\N,Missing
2020.acl-main.151,P17-1108,0,\N,Missing
2020.acl-main.151,P19-1269,0,\N,Missing
2020.acl-main.151,D19-1449,1,\N,Missing
2020.acl-main.151,D19-1053,1,\N,Missing
2020.acl-main.151,W19-5410,0,\N,Missing
2020.acl-main.151,2020.emnlp-main.215,0,\N,Missing
2020.codi-1.16,P15-1136,0,0.0485392,"Missing"
2020.codi-1.16,D16-1245,0,0.0385478,"Missing"
2020.codi-1.16,P16-1188,0,0.0188528,"egitimate to mitigate the obstacles, as they may falsely introduce mention overlaps in adversarial training and test sets, thus inflating the performance. 1 Table 1: Replacing ”the Gulf war” with ”the Gulf warfare” or ”the Gulf w¨arf¨are” addresses (1) exact match in the test example; (2) mention overlaps across examples. be the reason why coreference resolvers have little effect in downstream tasks. Introduction Resolution of coreferring expressions is a natural step for text understanding, but coreference resolvers appear to have a negligible effect in downstream NLP tasks (Yu and Ji, 2016; Durrett et al., 2016; Voita et al., 2018). For instance, Durrett et al. (2016) rewrite pronouns with their antecedents (e.g., he is replaced by Dominick Dunne), using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is in"
2020.codi-1.16,Q14-1037,0,0.0205489,"warfare” or ”the Gulf w¨arf¨are” addresses (1) exact match in the test example; (2) mention overlaps across examples. be the reason why coreference resolvers have little effect in downstream tasks. Introduction Resolution of coreferring expressions is a natural step for text understanding, but coreference resolvers appear to have a negligible effect in downstream NLP tasks (Yu and Ji, 2016; Durrett et al., 2016; Voita et al., 2018). For instance, Durrett et al. (2016) rewrite pronouns with their antecedents (e.g., he is replaced by Dominick Dunne), using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the training and test sets share a large size of mentions. This may As opposed to evaluating on standard benchmarks, recent work (Glockner et al., 2018; Pruthi et al., 2019; Eger et al., 2019; Eger a"
2020.codi-1.16,2020.aacl-main.79,1,0.704157,"2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the training and test sets share a large size of mentions. This may As opposed to evaluating on standard benchmarks, recent work (Glockner et al., 2018; Pruthi et al., 2019; Eger et al., 2019; Eger and Benz, 2020) investigates the generalization ability of NLP systems under adversarial attacks. For instance, Glockner et al. (2018) show that natural language inference systems fail blatantly when lexical changes, e.g., replacing a word by its synonym, occur in premises and hypotheses. Pruthi et al. (2019) observe that spelling errors distract text classification systems from correct prediction. Inspired by these works, we investigate published coreference resolvers in two realistic adversarial setups, which challenge (a) lexical inference ability to resolve coreferent mentions, where one mention is, e.g."
2020.codi-1.16,N19-1165,1,0.880844,"Missing"
2020.codi-1.16,P18-2103,0,0.0296253,"using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the training and test sets share a large size of mentions. This may As opposed to evaluating on standard benchmarks, recent work (Glockner et al., 2018; Pruthi et al., 2019; Eger et al., 2019; Eger and Benz, 2020) investigates the generalization ability of NLP systems under adversarial attacks. For instance, Glockner et al. (2018) show that natural language inference systems fail blatantly when lexical changes, e.g., replacing a word by its synonym, occur in premises and hypotheses. Pruthi et al. (2019) observe that spelling errors distract text classification systems from correct prediction. Inspired by these works, we investigate published coreference resolvers in two realistic adversarial setups, which challenge (a) lexical inference abil"
2020.codi-1.16,D19-1588,0,0.0245649,"Missing"
2020.codi-1.16,J13-4004,0,0.0764278,"Missing"
2020.codi-1.16,N18-2108,0,0.0289703,"Missing"
2020.codi-1.16,H05-1004,0,0.317241,"Missing"
2020.codi-1.16,P17-2003,1,0.924434,"a negligible effect in downstream NLP tasks (Yu and Ji, 2016; Durrett et al., 2016; Voita et al., 2018). For instance, Durrett et al. (2016) rewrite pronouns with their antecedents (e.g., he is replaced by Dominick Dunne), using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the training and test sets share a large size of mentions. This may As opposed to evaluating on standard benchmarks, recent work (Glockner et al., 2018; Pruthi et al., 2019; Eger et al., 2019; Eger and Benz, 2020) investigates the generalization ability of NLP systems under adversarial attacks. For instance, Glockner et al. (2018) show that natural language inference systems fail blatantly when lexical changes, e.g., replacing a word by its synonym, occur in premises and hypotheses. Pruthi et al. (2019) observe that spelling e"
2020.codi-1.16,W12-4501,0,0.275686,"tion of coreferring expressions is a natural step for text understanding, but coreference resolvers appear to have a negligible effect in downstream NLP tasks (Yu and Ji, 2016; Durrett et al., 2016; Voita et al., 2018). For instance, Durrett et al. (2016) rewrite pronouns with their antecedents (e.g., he is replaced by Dominick Dunne), using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the training and test sets share a large size of mentions. This may As opposed to evaluating on standard benchmarks, recent work (Glockner et al., 2018; Pruthi et al., 2019; Eger et al., 2019; Eger and Benz, 2020) investigates the generalization ability of NLP systems under adversarial attacks. For instance, Glockner et al. (2018) show that natural language inference systems fail blatantly when lexical changes"
2020.codi-1.16,W11-1901,0,0.0363748,"ect in downstream tasks. Introduction Resolution of coreferring expressions is a natural step for text understanding, but coreference resolvers appear to have a negligible effect in downstream NLP tasks (Yu and Ji, 2016; Durrett et al., 2016; Voita et al., 2018). For instance, Durrett et al. (2016) rewrite pronouns with their antecedents (e.g., he is replaced by Dominick Dunne), using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the training and test sets share a large size of mentions. This may As opposed to evaluating on standard benchmarks, recent work (Glockner et al., 2018; Pruthi et al., 2019; Eger et al., 2019; Eger and Benz, 2020) investigates the generalization ability of NLP systems under adversarial attacks. For instance, Glockner et al. (2018) show that natural language inference"
2020.codi-1.16,P19-1561,0,0.112757,"ty Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the training and test sets share a large size of mentions. This may As opposed to evaluating on standard benchmarks, recent work (Glockner et al., 2018; Pruthi et al., 2019; Eger et al., 2019; Eger and Benz, 2020) investigates the generalization ability of NLP systems under adversarial attacks. For instance, Glockner et al. (2018) show that natural language inference systems fail blatantly when lexical changes, e.g., replacing a word by its synonym, occur in premises and hypotheses. Pruthi et al. (2019) observe that spelling errors distract text classification systems from correct prediction. Inspired by these works, we investigate published coreference resolvers in two realistic adversarial setups, which challenge (a) lexical inference ability to resolve corefe"
2020.codi-1.16,S19-1021,0,0.0185025,"ts Modification Original → Modification S WAP D ELETE V ISUAL S YNONYM H YPONYM H YPERNYM people → peolpe rise → rse emergency → emergeˇncy next → upcoming people → workers pigeon → bird Table 2: Examples of text modification. antecedent the first African-American president of the US. The CoNLL dataset involves many such lexical overlaps in coreferent mentions. Furthermore, Moosavi and Strube (2017) find a large size of mentions are overlapping in the CoNLL training and test examples. Together, this shows that the CoNLL evaluation setup does require only little lexical inference requirements. Subramanian and Roth (2019) remove named entities overlapping in the training and test sets. In contrast, we choose a word overlap randomly from mentions and substitute it with its hyponym, hypernym and synonym, as found in WordNet (Miller, 1995). To prevent the meaning of a word substitution deviated from the original word, we make the substitution only when two words share one word sense (synset), obtained from adapted LESK algorithm (Banerjee and Pedersen, 2002). Orthographic Changes. Character-level (“lowlevel”) text changes, e.g., random swapping of characters (Pruthi et al., 2019), create surface form noise that o"
2020.codi-1.16,M95-1005,0,0.767328,"Missing"
2020.codi-1.16,P18-1117,0,0.027264,"the obstacles, as they may falsely introduce mention overlaps in adversarial training and test sets, thus inflating the performance. 1 Table 1: Replacing ”the Gulf war” with ”the Gulf warfare” or ”the Gulf w¨arf¨are” addresses (1) exact match in the test example; (2) mention overlaps across examples. be the reason why coreference resolvers have little effect in downstream tasks. Introduction Resolution of coreferring expressions is a natural step for text understanding, but coreference resolvers appear to have a negligible effect in downstream NLP tasks (Yu and Ji, 2016; Durrett et al., 2016; Voita et al., 2018). For instance, Durrett et al. (2016) rewrite pronouns with their antecedents (e.g., he is replaced by Dominick Dunne), using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the traini"
2020.codi-1.16,P16-1005,0,0.012354,"e sometimes not legitimate to mitigate the obstacles, as they may falsely introduce mention overlaps in adversarial training and test sets, thus inflating the performance. 1 Table 1: Replacing ”the Gulf war” with ”the Gulf warfare” or ”the Gulf w¨arf¨are” addresses (1) exact match in the test example; (2) mention overlaps across examples. be the reason why coreference resolvers have little effect in downstream tasks. Introduction Resolution of coreferring expressions is a natural step for text understanding, but coreference resolvers appear to have a negligible effect in downstream NLP tasks (Yu and Ji, 2016; Durrett et al., 2016; Voita et al., 2018). For instance, Durrett et al. (2016) rewrite pronouns with their antecedents (e.g., he is replaced by Dominick Dunne), using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the"
2020.coling-main.105,W19-4820,1,0.833802,"esentational similarity across languages in mBERT and disentangles it along genetic, geographic, and structural factors. In general, the urge to improve the interpretability of internal neural representations has become a major research field in recent years. Whereas dense representations of images can be projected back to pixels to facilitate visual inspection, interpreting the linguistic information captured in dense representation of languages is more complex (Alishahi et al., 2019; Conneau and Kiela, 2018). Diagnostic classifiers (Hupkes et al., 2018), representational stability analysis (Abnar et al., 2019) and indirect visualization techniques (Belinkov and Glass, 2019) are only a few examples for newly developed probing techniques. They are used to examine whether the representations capture part-of-speech information (Zhang and Bowman, 2018), syntactic agreement (Giulianelli et al., 2018), speech features (Chrupała et al., 2017), and cognitive cues (Wehbe et al., 2014). However, the majority of these interpretability studies focus solely on English. Krasnowska-Kiera´s and Wr´oblewska (2019) perform a contrastive analysis of the syntactic interpretability of English and Polish representations"
2020.coling-main.105,P17-1042,0,0.0135326,"in NLP, since they promise universal text processing in multiple human languages with labeled training data only in a single one. They go back at least to the work of Klementiev et al. (2012), and have seen an exploding number of contributions in recent years. Recent cross-lingual models provide representations for about 100 languages and vary in their training objectives. In offline learning, cross-lingual representations are obtained by projecting independently trained monolingual representations into a shared representational space using bilingual lexical resources (Faruqui and Dyer, 2014; Artetxe et al., 2017). In joint learning (Wang et al., 2020), the cross-lingual representations are learned directly, for example as a byproduct of large-scale machine translation (Artetxe and Schwenk, 2018). As parallel data is scarce for less frequent language pairs, the multilingual BERT model (mBERT) simply trains the BERT architecture (Devlin et al., 2019) on multilingual input from Wikipedia. The cross-lingual signal is thus only learned implicitly because mBERT uses the same representational space independent of the input language. This naive approach yields surprisingly high scores for cross-lingual downst"
2020.coling-main.105,Q19-1004,0,0.0137288,"tangles it along genetic, geographic, and structural factors. In general, the urge to improve the interpretability of internal neural representations has become a major research field in recent years. Whereas dense representations of images can be projected back to pixels to facilitate visual inspection, interpreting the linguistic information captured in dense representation of languages is more complex (Alishahi et al., 2019; Conneau and Kiela, 2018). Diagnostic classifiers (Hupkes et al., 2018), representational stability analysis (Abnar et al., 2019) and indirect visualization techniques (Belinkov and Glass, 2019) are only a few examples for newly developed probing techniques. They are used to examine whether the representations capture part-of-speech information (Zhang and Bowman, 2018), syntactic agreement (Giulianelli et al., 2018), speech features (Chrupała et al., 2017), and cognitive cues (Wehbe et al., 2014). However, the majority of these interpretability studies focus solely on English. Krasnowska-Kiera´s and Wr´oblewska (2019) perform a contrastive analysis of the syntactic interpretability of English and Polish representations and Eger et al. (2020) probe representations in three lower-resou"
2020.coling-main.105,N18-1083,0,0.0245374,"ions from the sentence-based LASER model (Artetxe and Schwenk, 2018) which is trained to learn language-neutral representations for machine translation. Libovick`y et al. (2019) analyze representations from mBERT and find that clustering over averaged representations for the 104 languages yields phylogenetically plausible language groups. They argue that mBERT is not language-neutral and that semantic phenomena are not modeled properly across languages. In our analysis, we further quantify the representational distance and disentangle it along phylogenetic, geographic, and structural factors. Bjerva and Augenstein (2018) train cross-lingual representations in an unsupervised way for different linguistic levels including phonology, morphology, and syntax. They use them to infer missing typological features for more than 800 languages. Malaviya et al. (2017) also infer missing features in typological databases from cross-lingual representations. Inferring such missing features can be considered a form of probing. Indeed, in contemporaneous work, Choenni and Shutova (2020) predict typological properties from representations of four different recent state-of-the-art cross-lingual encoders using probing classifier"
2020.coling-main.105,J19-2006,0,0.0742514,"tance. For instance, Dravidian and Indo-Aryan languages overlap completely. Eger et al. (2016) induce bilingual vector spaces for 21 Europarl languages and quantify representational distance between languages by averaging over the pairwise similarity between word representations. They find that the differences can be better explained by geographic than by phylogenetic factors. Conversely, Rabinovich et al. (2017) analyze English translations of sentences in 17 Europarl languages and find that syntactic traces of the native language of the translator can best be explained by language genetics. Bjerva et al. (2019) use the same dataset and train language representations on the linguistic structure of the sentences. They find that the representational distance between languages can be better explained by structural similarity (obtained from dependency trees) than by language genetics. Pretrained cross-lingual models are optimized for tasks such as bilingual lexicon induction and machine translation. Even if linguistic information is not explicitly provided during training, recent interpretability research indicates that phylogenetic properties are encoded in the resulting representations. Beinborn and Ch"
2020.coling-main.105,Q17-1010,0,0.0112839,"er, and works best for structurally similar languages. Cao et al. (2020) find that mBERT exhibits vector space misalignment across languages and zero-shot cross-lingual transfer is improved after their suggested re-mapping. K et al. (2020) show that lexical overlap plays no big role in cross-lingual transfer for mBERT, but the depth of the network does, with deeper models having better transfer. Zhao et al. (2020b) find that mBERT lacks fine-grained cross-lingual text understanding and can be fooled by adversarial inputs produced by the corrupt input produced by MT systems. FastText FastText (Bojanowski et al., 2017) builds static word representations on the basis of a word’s characters. This allows it to induce better representations for infrequent and unknown words. We use a joint multilingually aligned vector space spanning 44 languages using the RCLS method described in Joulin et al. (2018) and refer to it as mFastText.2 3.2 Representational Distance Assume we have M languages and N concepts (illustrated in Table 4 in the appendix). Assume further that each concept is expressed as a word in each language which is represented by a d-dimensional vector. If all the vectors reside in a cross-lingually sha"
2020.coling-main.105,P17-1057,0,0.0180149,"els to facilitate visual inspection, interpreting the linguistic information captured in dense representation of languages is more complex (Alishahi et al., 2019; Conneau and Kiela, 2018). Diagnostic classifiers (Hupkes et al., 2018), representational stability analysis (Abnar et al., 2019) and indirect visualization techniques (Belinkov and Glass, 2019) are only a few examples for newly developed probing techniques. They are used to examine whether the representations capture part-of-speech information (Zhang and Bowman, 2018), syntactic agreement (Giulianelli et al., 2018), speech features (Chrupała et al., 2017), and cognitive cues (Wehbe et al., 2014). However, the majority of these interpretability studies focus solely on English. Krasnowska-Kiera´s and Wr´oblewska (2019) perform a contrastive analysis of the syntactic interpretability of English and Polish representations and Eger et al. (2020) probe representations in three lower-resource languages. Cross-lingual interpretability research for multiple languages focuses on the ability to transfer representational knowledge across languages for zero-shot semantics (Pires et al., 2019) and for syntactic phenomena (Dhar and Bisazza, 2018). In this wo"
2020.coling-main.105,L18-1269,0,0.0147497,"cability as a testbed for cross-lingual or historical linguistic hypotheses. Our analysis quantifies the representational similarity across languages in mBERT and disentangles it along genetic, geographic, and structural factors. In general, the urge to improve the interpretability of internal neural representations has become a major research field in recent years. Whereas dense representations of images can be projected back to pixels to facilitate visual inspection, interpreting the linguistic information captured in dense representation of languages is more complex (Alishahi et al., 2019; Conneau and Kiela, 2018). Diagnostic classifiers (Hupkes et al., 2018), representational stability analysis (Abnar et al., 2019) and indirect visualization techniques (Belinkov and Glass, 2019) are only a few examples for newly developed probing techniques. They are used to examine whether the representations capture part-of-speech information (Zhang and Bowman, 2018), syntactic agreement (Giulianelli et al., 2018), speech features (Chrupała et al., 2017), and cognitive cues (Wehbe et al., 2014). However, the majority of these interpretability studies focus solely on English. Krasnowska-Kiera´s and Wr´oblewska (2019)"
2020.coling-main.105,D17-1070,0,0.121302,"proximated using hand-selected word lists and typological databases. Common ancestors for languages are typically inferred based on cases of shared word meaning and surface form overlap and it can be assumed that these core properties are also captured in large-scale cross-lingual representations to a certain extent. For example, Beinborn and Choenni (2019) find that phylogenetic relations between languages can be reconstructed from cross-lingual representations if the training objective optimizes monolingual semantic constraints for each language separately as in the multilingual MUSE model (Conneau et al., 2017). MUSE is restricted to only 29 frequent languages, however. While mBERT is a powerful cross-lingual model covering an order of magnitude more languages (104), a better understanding of the type of signal captured in its representations is needed to assess its applicability as a testbed for cross-lingual or historical linguistic hypotheses. Our analysis quantifies the representational similarity across languages in mBERT and disentangles it along genetic, geographic, and structural factors. In general, the urge to improve the interpretability of internal neural representations has become a maj"
2020.coling-main.105,P18-1198,0,0.0188465,"representations are sometimes close to the reference trees, they may confound multiple factors. A more-fined grained investigation of t-sne plots followed by matrix regression analyses suggests that representational distances correlate most with phylogenetic and geographical distances between languages. Further, the rankings from cross-lingual stability scores correlate significantly with meaning lists for items supposed to be resistant to cross-temporal lexical replacement. Our results contribute to the recent discourses on interpretability and introspection of black-box NLP representations (Conneau et al., 2018; Kudugunta et al., 2019; Jacovi and Goldberg, 2020). In our case, we asked how mBERT perceives of the similarity of two languages and related this to phylogenetic, geographic and structural factors. In future work, we aim to use our inferred similarities to predict transfer behavior in downstream tasks between specific language pairs. Finally, we strongly caution against using our conclusions as support for hypotheses relating semantics and language phylogeny (e.g., the Sapir-Whorf hypothesis). Our results for bilingual lexicon induction indicate that mBERT representations are only mildly sem"
2020.coling-main.105,N19-1423,0,0.178762,"ry in their training objectives. In offline learning, cross-lingual representations are obtained by projecting independently trained monolingual representations into a shared representational space using bilingual lexical resources (Faruqui and Dyer, 2014; Artetxe et al., 2017). In joint learning (Wang et al., 2020), the cross-lingual representations are learned directly, for example as a byproduct of large-scale machine translation (Artetxe and Schwenk, 2018). As parallel data is scarce for less frequent language pairs, the multilingual BERT model (mBERT) simply trains the BERT architecture (Devlin et al., 2019) on multilingual input from Wikipedia. The cross-lingual signal is thus only learned implicitly because mBERT uses the same representational space independent of the input language. This naive approach yields surprisingly high scores for cross-lingual downstream tasks, but the transfer does not work equally well for all languages. Pires et al. (2019) show that the performance differences between languages are gradual and that the representational similarity between languages seem to correlate with typological features. These relationships between languages remain opaque in cross-lingual repres"
2020.coling-main.105,W18-5453,0,0.0197232,"ch features (Chrupała et al., 2017), and cognitive cues (Wehbe et al., 2014). However, the majority of these interpretability studies focus solely on English. Krasnowska-Kiera´s and Wr´oblewska (2019) perform a contrastive analysis of the syntactic interpretability of English and Polish representations and Eger et al. (2020) probe representations in three lower-resource languages. Cross-lingual interpretability research for multiple languages focuses on the ability to transfer representational knowledge across languages for zero-shot semantics (Pires et al., 2019) and for syntactic phenomena (Dhar and Bisazza, 2018). In this work, we contribute to the nascent field of typological and comparative linguistic interpretability of language representations at scale (Kudugunta et al., 2019) and analyze representations for more than 100 languages. Our contributions: We probe the representations of one of the current most popular cross-lingual models (mBERT) and find that mBERT lacks information to perform well on cross-lingual semantic retrieval, but can indeed be used to accurately infer a phylogenetic language tree for 100 languages. Our results indicate that the quality of the induced tree depends on the infe"
2020.coling-main.105,P16-2009,1,0.838831,"j)). In our experiments, we use cosine distance, but d may in principle refer to any suitable distance measure, e.g., Euclidean distance or Spearman correlation.3,4 2 https://fasttext.cc/docs/en/aligned-vectors.html Note that we compute the average of the distances, while it is also possible to compute the distance of the average representations (Libovick`y et al., 2019). 4 ˆ k (i) for An alternative to this direct comparison of the word vectors is a ‘second-order’ encoding where the representation v a word is determined by the distances of its vector vk (i) to the vectors for the N concepts (Eger and Mehler, 2016; Beinborn 3 1217 When the corresponding words for each concept are not available in all languages, but only in one language (e.g., English), Beinborn and Choenni (2019) instead set vk (i) to be the nearest neighbor of the English word for concept k in language i. This has the advantage that one can infer language distances without translation data in target languages. A drawback of this approach is that the relation between nearest neighbors in a vector space may not be that of similarity but of relatedness, e.g., nose is related to mouth, but it is not a synonym (meaning-equivalent). In our"
2020.coling-main.105,C16-1331,1,0.919964,"etermining representational distance and for interpreting the typological signal. Kudugunta et al. (2019) obtain representations for 102 language pairs (English ↔ language X) using neural machine translation and then visualize the representations using dimensionality reduction. They explore the visualization qualitatively and find clusters which resemble language families. However, when zooming into the clusters, it becomes evident that a mixture of genetic and geographic factors contributes to the representational distance. For instance, Dravidian and Indo-Aryan languages overlap completely. Eger et al. (2016) induce bilingual vector spaces for 21 Europarl languages and quantify representational distance between languages by averaging over the pairwise similarity between word representations. They find that the differences can be better explained by geographic than by phylogenetic factors. Conversely, Rabinovich et al. (2017) analyze English translations of sentences in 17 Europarl languages and find that syntactic traces of the native language of the translator can best be explained by language genetics. Bjerva et al. (2019) use the same dataset and train language representations on the linguistic"
2020.coling-main.105,2020.conll-1.8,1,0.89446,"indirect visualization techniques (Belinkov and Glass, 2019) are only a few examples for newly developed probing techniques. They are used to examine whether the representations capture part-of-speech information (Zhang and Bowman, 2018), syntactic agreement (Giulianelli et al., 2018), speech features (Chrupała et al., 2017), and cognitive cues (Wehbe et al., 2014). However, the majority of these interpretability studies focus solely on English. Krasnowska-Kiera´s and Wr´oblewska (2019) perform a contrastive analysis of the syntactic interpretability of English and Polish representations and Eger et al. (2020) probe representations in three lower-resource languages. Cross-lingual interpretability research for multiple languages focuses on the ability to transfer representational knowledge across languages for zero-shot semantics (Pires et al., 2019) and for syntactic phenomena (Dhar and Bisazza, 2018). In this work, we contribute to the nascent field of typological and comparative linguistic interpretability of language representations at scale (Kudugunta et al., 2019) and analyze representations for more than 100 languages. Our contributions: We probe the representations of one of the current most"
2020.coling-main.105,E14-1049,0,0.0402972,"ecome extremely popular in NLP, since they promise universal text processing in multiple human languages with labeled training data only in a single one. They go back at least to the work of Klementiev et al. (2012), and have seen an exploding number of contributions in recent years. Recent cross-lingual models provide representations for about 100 languages and vary in their training objectives. In offline learning, cross-lingual representations are obtained by projecting independently trained monolingual representations into a shared representational space using bilingual lexical resources (Faruqui and Dyer, 2014; Artetxe et al., 2017). In joint learning (Wang et al., 2020), the cross-lingual representations are learned directly, for example as a byproduct of large-scale machine translation (Artetxe and Schwenk, 2018). As parallel data is scarce for less frequent language pairs, the multilingual BERT model (mBERT) simply trains the BERT architecture (Devlin et al., 2019) on multilingual input from Wikipedia. The cross-lingual signal is thus only learned implicitly because mBERT uses the same representational space independent of the input language. This naive approach yields surprisingly high scores f"
2020.coling-main.105,W18-5426,0,0.0157232,"tions of images can be projected back to pixels to facilitate visual inspection, interpreting the linguistic information captured in dense representation of languages is more complex (Alishahi et al., 2019; Conneau and Kiela, 2018). Diagnostic classifiers (Hupkes et al., 2018), representational stability analysis (Abnar et al., 2019) and indirect visualization techniques (Belinkov and Glass, 2019) are only a few examples for newly developed probing techniques. They are used to examine whether the representations capture part-of-speech information (Zhang and Bowman, 2018), syntactic agreement (Giulianelli et al., 2018), speech features (Chrupała et al., 2017), and cognitive cues (Wehbe et al., 2014). However, the majority of these interpretability studies focus solely on English. Krasnowska-Kiera´s and Wr´oblewska (2019) perform a contrastive analysis of the syntactic interpretability of English and Polish representations and Eger et al. (2020) probe representations in three lower-resource languages. Cross-lingual interpretability research for multiple languages focuses on the ability to transfer representational knowledge across languages for zero-shot semantics (Pires et al., 2019) and for syntactic pheno"
2020.coling-main.105,P19-1070,0,0.0278982,"Missing"
2020.coling-main.105,2020.acl-main.386,0,0.0140706,"erence trees, they may confound multiple factors. A more-fined grained investigation of t-sne plots followed by matrix regression analyses suggests that representational distances correlate most with phylogenetic and geographical distances between languages. Further, the rankings from cross-lingual stability scores correlate significantly with meaning lists for items supposed to be resistant to cross-temporal lexical replacement. Our results contribute to the recent discourses on interpretability and introspection of black-box NLP representations (Conneau et al., 2018; Kudugunta et al., 2019; Jacovi and Goldberg, 2020). In our case, we asked how mBERT perceives of the similarity of two languages and related this to phylogenetic, geographic and structural factors. In future work, we aim to use our inferred similarities to predict transfer behavior in downstream tasks between specific language pairs. Finally, we strongly caution against using our conclusions as support for hypotheses relating semantics and language phylogeny (e.g., the Sapir-Whorf hypothesis). Our results for bilingual lexicon induction indicate that mBERT representations are only mildly semantic cross-lingually which corroborates similar fin"
2020.coling-main.105,D18-1330,0,0.0210992,"Missing"
2020.coling-main.105,kamholz-etal-2014-panlex,0,0.0201738,"all languages, thus we do not need to resort to nearest neighbors of the English words. 3.3 Concept Lists All our experiments are based on multilingual word lists obtained from linguistic databases. NorthEuraLex5 features word lists for 1,016 concepts in 100 languages spoken in Northern Eurasia which have been transcribed by linguists (Dellert et al., 2020). The database is known for its high quality, but unfortunately covers only 54 of the 104 languages in mBERT. In order to analyze more languages, we additionally use PanLex6 which contains lists for 207 concepts in more than 500 languages (Kamholz et al., 2014). It covers 99 languages in mBERT, but the quality of the word lists is not uniform across languages. PanLex sometimes includes multiple word lists written in different scripts for the same language, e.g. for Greek. In such a case, we include all available word lists for the language in our analysis. 3.4 Evaluating language trees Historical differences between languages are commonly represented in phylogenetic trees which group languages by their evolution from common ancestors. We want to examine to which extent these phylogenetic differences can explain the observed representational distance"
2020.coling-main.105,C12-1089,0,0.0176402,"y structural factors and 3) present a novel measure for measuring diachronic meaning stability (based on cross-lingual representation variability) which correlates significantly with published ranked lists based on linguistic approaches. Our results contribute to the nascent field of typological interpretability of cross-lingual text representations. 1 Introduction Cross-lingual text representations have become extremely popular in NLP, since they promise universal text processing in multiple human languages with labeled training data only in a single one. They go back at least to the work of Klementiev et al. (2012), and have seen an exploding number of contributions in recent years. Recent cross-lingual models provide representations for about 100 languages and vary in their training objectives. In offline learning, cross-lingual representations are obtained by projecting independently trained monolingual representations into a shared representational space using bilingual lexical resources (Faruqui and Dyer, 2014; Artetxe et al., 2017). In joint learning (Wang et al., 2020), the cross-lingual representations are learned directly, for example as a byproduct of large-scale machine translation (Artetxe an"
2020.coling-main.105,P19-1573,0,0.0445546,"Missing"
2020.coling-main.105,D19-1167,0,0.363848,"iera´s and Wr´oblewska (2019) perform a contrastive analysis of the syntactic interpretability of English and Polish representations and Eger et al. (2020) probe representations in three lower-resource languages. Cross-lingual interpretability research for multiple languages focuses on the ability to transfer representational knowledge across languages for zero-shot semantics (Pires et al., 2019) and for syntactic phenomena (Dhar and Bisazza, 2018). In this work, we contribute to the nascent field of typological and comparative linguistic interpretability of language representations at scale (Kudugunta et al., 2019) and analyze representations for more than 100 languages. Our contributions: We probe the representations of one of the current most popular cross-lingual models (mBERT) and find that mBERT lacks information to perform well on cross-lingual semantic retrieval, but can indeed be used to accurately infer a phylogenetic language tree for 100 languages. Our results indicate that the quality of the induced tree depends on the inference algorithm and might also be the effect of several conflated signals. In order to better disentangle phylogenetic, geographic, and structural factors, we go beyond si"
2020.coling-main.105,P19-1493,0,0.372059,"Missing"
2020.coling-main.105,E17-2002,0,0.0247244,"ben heb lvs plt gle lmo-005 ita-008 cat vie 40 hun pes mal tel tam hin arb bul-001 hat kat yor hye-003 cym bre hye mkd mya pes-002 isl nld swe fra ltz ell bul srp als deu bak kir azj-003 azj-001 che-001 che ukr rus nno 20 tat tgk dan nob kaz chv tur azj uzn cmn-003 pol 0 10 20 30 Figure 1: The t-sne plot for the Swadesh list distances from layer 2. The family codes are from the ASJP database (Wichmann et al., 2020) and are explained in Table 5 in appendix. 1220 4.3 Other typological signals In order to better disentangle the typological signal, we examine additional categories established by Littell et al. (2017). We determine the explainable predictors for the representational distances between languages using matrix regression (Legendre et al., 1994). We regress dij = dist(i, j) computed based on Eq. (1) on the following language distances: • Phylogenetic distance (genij ) between two languages computed from Glottolog reference trees as the ratio between the number of non-shared branches divided by the number of branches from root to the tip. • Geographical distance (geoij ) between two points on Earth approximated through great circle distance (Department, 1997). • Structural distance (strucij ): C"
2020.coling-main.105,D17-1268,0,0.147446,"raged representations for the 104 languages yields phylogenetically plausible language groups. They argue that mBERT is not language-neutral and that semantic phenomena are not modeled properly across languages. In our analysis, we further quantify the representational distance and disentangle it along phylogenetic, geographic, and structural factors. Bjerva and Augenstein (2018) train cross-lingual representations in an unsupervised way for different linguistic levels including phonology, morphology, and syntax. They use them to infer missing typological features for more than 800 languages. Malaviya et al. (2017) also infer missing features in typological databases from cross-lingual representations. Inferring such missing features can be considered a form of probing. Indeed, in contemporaneous work, Choenni and Shutova (2020) predict typological properties from representations of four different recent state-of-the-art cross-lingual encoders using probing classifiers. We do not use probing classifiers in our work because the choice of classifier and the size of its training data may affect the probing outcomes (Eger et al., 2020). Table 1 summarizes selected interpretability approaches analyzing the t"
2020.coling-main.105,P17-1049,0,0.138406,"atively and find clusters which resemble language families. However, when zooming into the clusters, it becomes evident that a mixture of genetic and geographic factors contributes to the representational distance. For instance, Dravidian and Indo-Aryan languages overlap completely. Eger et al. (2016) induce bilingual vector spaces for 21 Europarl languages and quantify representational distance between languages by averaging over the pairwise similarity between word representations. They find that the differences can be better explained by geographic than by phylogenetic factors. Conversely, Rabinovich et al. (2017) analyze English translations of sentences in 17 Europarl languages and find that syntactic traces of the native language of the translator can best be explained by language genetics. Bjerva et al. (2019) use the same dataset and train language representations on the linguistic structure of the sentences. They find that the representational distance between languages can be better explained by structural similarity (obtained from dependency trees) than by language genetics. Pretrained cross-lingual models are optimized for tasks such as bilingual lexicon induction and machine translation. Even"
2020.coling-main.105,D14-1030,0,0.0175723,"eting the linguistic information captured in dense representation of languages is more complex (Alishahi et al., 2019; Conneau and Kiela, 2018). Diagnostic classifiers (Hupkes et al., 2018), representational stability analysis (Abnar et al., 2019) and indirect visualization techniques (Belinkov and Glass, 2019) are only a few examples for newly developed probing techniques. They are used to examine whether the representations capture part-of-speech information (Zhang and Bowman, 2018), syntactic agreement (Giulianelli et al., 2018), speech features (Chrupała et al., 2017), and cognitive cues (Wehbe et al., 2014). However, the majority of these interpretability studies focus solely on English. Krasnowska-Kiera´s and Wr´oblewska (2019) perform a contrastive analysis of the syntactic interpretability of English and Polish representations and Eger et al. (2020) probe representations in three lower-resource languages. Cross-lingual interpretability research for multiple languages focuses on the ability to transfer representational knowledge across languages for zero-shot semantics (Pires et al., 2019) and for syntactic phenomena (Dhar and Bisazza, 2018). In this work, we contribute to the nascent field of"
2020.coling-main.105,W18-5448,0,0.0171585,"ield in recent years. Whereas dense representations of images can be projected back to pixels to facilitate visual inspection, interpreting the linguistic information captured in dense representation of languages is more complex (Alishahi et al., 2019; Conneau and Kiela, 2018). Diagnostic classifiers (Hupkes et al., 2018), representational stability analysis (Abnar et al., 2019) and indirect visualization techniques (Belinkov and Glass, 2019) are only a few examples for newly developed probing techniques. They are used to examine whether the representations capture part-of-speech information (Zhang and Bowman, 2018), syntactic agreement (Giulianelli et al., 2018), speech features (Chrupała et al., 2017), and cognitive cues (Wehbe et al., 2014). However, the majority of these interpretability studies focus solely on English. Krasnowska-Kiera´s and Wr´oblewska (2019) perform a contrastive analysis of the syntactic interpretability of English and Polish representations and Eger et al. (2020) probe representations in three lower-resource languages. Cross-lingual interpretability research for multiple languages focuses on the ability to transfer representational knowledge across languages for zero-shot semant"
2020.coling-main.105,2020.acl-main.151,1,0.854352,"Missing"
2020.coling-main.152,L18-1269,0,0.0238924,"te performance on 10k sentences from News Crawl 2008. For speed reasons, we restrict sentence length to at most 15 tokens. Our RNN has 3 hidden layers with 1024 hidden units in each. In §3.3, we verify that the design choice of the decoder (e.g., MOS architecture vs. a simple RNN) plays a marginal role for the conclusions of V2S in our experiments. In the sequel, we test whether V2S can predict downstream task performance (§3.1) before introspecting the language generated by different encoders (§3.2). 3.1 Correlation with Downstream Tasks In Figure 2, we plot Spearman rank correlations (as in Conneau and Kiela (2018)) between our V2S statistics and the 14 downstream tasks for standardized comparison of sentence encoders from SentEval (Conneau and Kiela, 2018); full results are given in the appendix. The correlations indicate whether rankings of encoders induced by the probing tasks transfer to downstream tasks: i.e., to which degree it holds that A Probing B implies that A Downstream B, for encoders A and B. The downstream tasks and V2S have no training data overlap. Results We observe the following: (i) PERM is not strongly (but in general positively) correlated with downstream task performance. This m"
2020.coling-main.152,D17-1070,0,0.181259,"embeddings by conditionally generating from them. We perceive of this as a new unsupervised probing task and show that it correlates well with downstream task performance. We also illustrate how the language generated from different encoders differs. We apply our approach to generate sentence analogies from sentence embeddings. 1 Introduction Generalizing the concept of word embeddings to sentence level, sentence embeddings (a.k.a. sentence encoders) are ubiquitous in NLP as features in downstream classification tasks and in semantic similarity and retrieval applications (Kiros et al., 2015; Conneau et al., 2017). Probing sentence encoders for the linguistic information signals they contain has likewise become an important field of research, as this allows to introspect otherwise black-box representations (Adi et al., 2017; Conneau et al., 2018). The idea behind probing tasks is to query representations for certain kinds of linguistic information such as the dependency tree depth of an encoded sentence. There are a variety of problems surrounding current probing task specifications: (i) probing tasks need to be manually construed, which brings with it a certain degree of arbitrariness and incompletene"
2020.coling-main.152,P18-1198,0,0.211264,"oders differs. We apply our approach to generate sentence analogies from sentence embeddings. 1 Introduction Generalizing the concept of word embeddings to sentence level, sentence embeddings (a.k.a. sentence encoders) are ubiquitous in NLP as features in downstream classification tasks and in semantic similarity and retrieval applications (Kiros et al., 2015; Conneau et al., 2017). Probing sentence encoders for the linguistic information signals they contain has likewise become an important field of research, as this allows to introspect otherwise black-box representations (Adi et al., 2017; Conneau et al., 2018). The idea behind probing tasks is to query representations for certain kinds of linguistic information such as the dependency tree depth of an encoded sentence. There are a variety of problems surrounding current probing task specifications: (i) probing tasks need to be manually construed, which brings with it a certain degree of arbitrariness and incompleteness; (ii) most probing tasks require labeled datasets or trained classifiers such as dependency parsers for linguistic processing—however, these may be unavailable for many low-resource languages or available only to a limited degree; (ii"
2020.coling-main.152,K19-1085,0,0.0353907,"Missing"
2020.coling-main.152,W19-4308,1,0.884744,"Missing"
2020.coling-main.152,2020.conll-1.8,1,0.893087,"d sentence. There are a variety of problems surrounding current probing task specifications: (i) probing tasks need to be manually construed, which brings with it a certain degree of arbitrariness and incompleteness; (ii) most probing tasks require labeled datasets or trained classifiers such as dependency parsers for linguistic processing—however, these may be unavailable for many low-resource languages or available only to a limited degree; (iii) it is not entirely clear how probing tasks have to be designed, e.g., how much training data they require and which classifier to use for probing (Eger et al., 2020); (iv) Ravichander et al. (2020) also argue that standard probing tasks do not outline the information signals a classifier actually uses for making predictions. Our contribution is to design an alternative, more direct introspection of sentence embeddings, namely, through conditional natural language generation, which we call “vec2sent” (V2S). By retrieving and (manually) investigating the discrete output obtained from a dense vector representation, linguistic properties of the embedding may be ‘directly’ unveiled: e.g., we expect that a word-order insensitive model would have a comparatively"
2020.coling-main.152,L18-1473,0,0.0425841,"ic methods, which tune parameters on top of word embeddings. As non-parametric methods, we consider: (i) average word embeddings as a popular baseline, (ii) GEM (Yang et al., 2018), a weighted averaging model, (iii) hierarchical embeddings 1730 (Shen et al., 2018), an order-sensitive model where a max-pooling operation is applied to averages of word 3-gram embeddings in a sentence, (iv) the concatenation of average, hierarchical and max pooling (R¨uckl´e et al., 2018), and (v) sent2vec (Pagliardini et al., 2018), a compositional word n-gram model. For (i)-(iv) we use BPEmb subword embeddings (Heinzerling and Strube, 2018) as token representations. As parametric methods, we consider: InferSent (Conneau et al., 2017), which induces a sentence representation by learning a semantic entailment relationship between two sentences; QuickThought (Logeswaran and Lee, 2018) which reframes the popular SkipThought model (Kiros et al., 2015) in a classification context; LASER (Artetxe and Schwenk, 2018) derived from massively multilingual machine translation models; and sentence BERT (SBERT) (Reimers and Gurevych, 2019), which fine-tunes BERT representations on SNLI and then averages fine-tuned token embeddings to obtain a"
2020.coling-main.152,N19-1419,0,0.0291149,"robing tasks is to make the opaque embedding space Rd “linguistically” observable to humans. A plethora of different probing tasks have been suggested (Linzen et al., 2016; Shi et al., 2016; Adi et al., 2017), which Conneau et al. (2018) classify into surface, syntactic, and semantic probing tasks. More recently, multilingual extensions of probing tasks (Krasnowska-Kiera´s and Wr´oblewska, 2019; Sahin et al., 2019; Eger et al., 2020) have been considered, as well as word level probing for especially contextualized representations (Tenney et al., 2019; Liu et al., 2019). In the latter context, Hewitt and Manning (2019) discover an intriguing structural property of BERT, viz., to contain whole syntax trees in its representations, possibly, as we show, at the expense of lexical information (at least SBERT). We investigated V2S as an alternative, direct way of probing sentence encoders. We showed that V2S may be a good predictor of downstream task performance and, in particular, that one of two simple diagnostic tests had good predictive performance in all scenarios: whether an encoder can exactly retrieve the underlying sentence from an embedding (Id) and whether the fraction of exactly retrieved sentences am"
2020.coling-main.152,P19-1573,0,0.0350006,"Missing"
2020.coling-main.152,Q16-1037,0,0.0258847,"Table 3: Spearman rank correlation between “high-resource” MOS setting with 1.4m training sentences and (a) low-resource setting with same architecture (b) different architecture but same training size. for all diagnostic tests) and do not seem to vary much along the investigated two dimensions. This is a reassuring result, as it indicates that our results are not an artefact of any of these two choices. 4 Discussion & Conclusion The goal of probing tasks is to make the opaque embedding space Rd “linguistically” observable to humans. A plethora of different probing tasks have been suggested (Linzen et al., 2016; Shi et al., 2016; Adi et al., 2017), which Conneau et al. (2018) classify into surface, syntactic, and semantic probing tasks. More recently, multilingual extensions of probing tasks (Krasnowska-Kiera´s and Wr´oblewska, 2019; Sahin et al., 2019; Eger et al., 2020) have been considered, as well as word level probing for especially contextualized representations (Tenney et al., 2019; Liu et al., 2019). In the latter context, Hewitt and Manning (2019) discover an intriguing structural property of BERT, viz., to contain whole syntax trees in its representations, possibly, as we show, at the expe"
2020.coling-main.152,N19-1112,0,0.025234,"s. 4 Discussion & Conclusion The goal of probing tasks is to make the opaque embedding space Rd “linguistically” observable to humans. A plethora of different probing tasks have been suggested (Linzen et al., 2016; Shi et al., 2016; Adi et al., 2017), which Conneau et al. (2018) classify into surface, syntactic, and semantic probing tasks. More recently, multilingual extensions of probing tasks (Krasnowska-Kiera´s and Wr´oblewska, 2019; Sahin et al., 2019; Eger et al., 2020) have been considered, as well as word level probing for especially contextualized representations (Tenney et al., 2019; Liu et al., 2019). In the latter context, Hewitt and Manning (2019) discover an intriguing structural property of BERT, viz., to contain whole syntax trees in its representations, possibly, as we show, at the expense of lexical information (at least SBERT). We investigated V2S as an alternative, direct way of probing sentence encoders. We showed that V2S may be a good predictor of downstream task performance and, in particular, that one of two simple diagnostic tests had good predictive performance in all scenarios: whether an encoder can exactly retrieve the underlying sentence from an embedding (Id) and whet"
2020.coling-main.152,N18-1049,0,0.0197854,"ers, non-parametric methods which combine word embeddings in elementary ways, without training; and parametric methods, which tune parameters on top of word embeddings. As non-parametric methods, we consider: (i) average word embeddings as a popular baseline, (ii) GEM (Yang et al., 2018), a weighted averaging model, (iii) hierarchical embeddings 1730 (Shen et al., 2018), an order-sensitive model where a max-pooling operation is applied to averages of word 3-gram embeddings in a sentence, (iv) the concatenation of average, hierarchical and max pooling (R¨uckl´e et al., 2018), and (v) sent2vec (Pagliardini et al., 2018), a compositional word n-gram model. For (i)-(iv) we use BPEmb subword embeddings (Heinzerling and Strube, 2018) as token representations. As parametric methods, we consider: InferSent (Conneau et al., 2017), which induces a sentence representation by learning a semantic entailment relationship between two sentences; QuickThought (Logeswaran and Lee, 2018) which reframes the popular SkipThought model (Kiros et al., 2015) in a classification context; LASER (Artetxe and Schwenk, 2018) derived from massively multilingual machine translation models; and sentence BERT (SBERT) (Reimers and Gurevych,"
2020.conll-1.8,Q19-1038,0,0.0217242,"ular baseline, (ii) the concatenation of average, min and max pooling (pmeans) (R¨uckl´e et al., 2018); and Random LSTMs (Conneau et al., 2017; Wieting and Kiela, 2019), which feed word embeddings to randomly initialized LSTMs, then apply a pooling operation across time-steps. As parametric methods, we consider: InferSent (Conneau et al., 2017), which induces a sentence representation by learning a semantic entailment relationship between two sentences; QuickThought (Logeswaran and Lee, 2018) which reframes the popular SkipThought model (Kiros et al., 2015) in a classification context; LASER (Artetxe and Schwenk, 2019) derived from massively multilingual machine translation models, and BERT base (Devlin et al., 2019), where we average token embeddings of the last layer for a sentence representation. Dimensionalities of encoders are listed in the appendix. 3.2 Probing Tasks Following Conneau et al. (2018), we consider the following probing tasks: BigramShift (en, tr, ru, ka), TreeDepth (en), Length (en, tr, ru, ka), Subject Number (en, tr, ru), WordContent (en, tr, ru, ka), and TopConstituents (en). Approach In the absence of ground truth, our main interest is in a ‘stable’ structural setup for probing task"
2020.conll-1.8,J08-4004,0,0.0763376,"rpus using COMBO for dependency parsing (Rybak and Wr´oblewska, 2018). They find that en and pl probing results mostly agree, i.e., encoders store the same linguistic information across the two languages. 3 data size and classifier choice for probing tasks.3 For a selected set of points p0 , p1 , . . . in X , we evaluate all our encoders on pi , and determine the ‘outcomes’ Oi (e.g., ranking) of the encoders at pi . We consider a setup pi as stable if outcome Oi is shared by a majority of other settings pj . This can be considered a region of agreement, similarly to inter-annotator agreement (Artstein and Poesio, 2008). In other words, we identify ‘ideal’ test conditions by minimizing the influence of parameters pi on the outcome Oi . Below, we will approximate these intuitions using correlation. 3.1 We consider two types of sentence encoders, nonparametric methods which combine word embeddings in elementary ways, without training; and parametric methods, which tune parameters on top of word embeddings. As non-parametric methods, we consider: (i) average word embeddings as a popular baseline, (ii) the concatenation of average, min and max pooling (pmeans) (R¨uckl´e et al., 2018); and Random LSTMs (Conneau e"
2020.conll-1.8,C18-1133,0,0.0227386,"s only determines the opinion flavor of a statement. Since sentiment analysis is a very established NLP task, we did not machine translate en training data, but used original data for en, ru and tr and created a novel dataset for ka. For en, we use the US Airline Twitter Sentiment dataset, consisting of 14,148 tweets labeled in three sentiment classes8 . For tr, we took the Turkish Twitter Sentiment Dataset with 6,172 examples and three classes9 . For ru, we used the Russian Twitter Corpus (RuTweetCorp), which we reduced to 30,000 examples in two classes.10 For ka, we followed the approach by Choudhary et al. (2018) and crawled sentiment flavored tweets in a distant supervision manner. Emojis were used as distant signals to indicate sentiment on preselected tweets from the Twitter API. After post-processing, we were able to collect 11,513 Georgian tweets in three sentiment classes. The dataset will made available publicly, including more details on the creation process. TREC Question Type Detection Question type detection is an important part of QuestionAnswering systems. The Text Retrieval Conference (TREC) dataset consists of a set of questions labeled with their respective question types (six labels i"
2020.conll-1.8,L18-1269,0,0.489565,"probe for sentence-level linguistic knowledge encoded in sentence embeddings (Perone et al., 2018) in a multilingual setup which marginalizes out the effects of probing task design choices when comparing sentence representations. Sentence embeddings have become central for representing texts beyond the word level, e.g., in small data scenarios, where it is difficult to induce good higher-level text representations from word embeddings (Subramanian et al., 2018) or for clustering or text retrieval applications (Reimers and Gurevych, 2019). To standardize the comparison of sentence embeddings, Conneau and Kiela (2018) proposed the SentEval framework for evaluating the quality of sentence embeddings on a range of downstream and 10 probing tasks. Probing tasks are used to introspect embeddings for linguistic knowledge, by taking “probes” as dedicated syntactic or semantic micro tasks (K¨ohn, 2016). As opposed to an evaluation in downstream applications or benchmarks like GLUE (Wang et al., 2018), probing tasks target very specific linguistic knowledge which may otherwise be confounded in downstream applications. Since they are artificial tasks, they can also be better controlled for to avoid dataset biases a"
2020.conll-1.8,D17-1070,0,0.659799,"probing evaluation should thus be carried out on multiple languages in the future. High Mid Low (A,B,C) (A,C,B) (A,B,C) classifier MLP (A,B,C) (C,B,A) (B,A,C) NB RF (C,A,B) (A,B,C) (B,C,A) (C,B,A) (C,B,A) (A,B,C) Table 1: Schematic illustration of our concept of stability across two dimensions (classifier and training size). Here, three encoders, dubbed A,B,C, are ranked. The region of stability is given by those settings that support the majority ranking of encoders, which is ABC. Introduction Sentence embeddings (a.k.a. sentence encoders) have become ubiquitous in NLP (Kiros et al., 2015; Conneau et al., 2017), extending the concept of word embeddings to the sentence level. In the context of recent efforts to open the black box of deep learning models and representations (Linzen et al., 2019), it has also become fashionable to probe sentence embeddings for the linguistic information signals they contain (Perone et al., 2018), as this may not be clear from their performances in downstream tasks. Such probes are linguistic micro tasks—like detecting the length of a sentence or its dependency tree depth—that have to be solved by a classifier using given representations. The majority of approaches for"
2020.conll-1.8,P18-1198,0,0.477726,"h-quality dependency parsers, as required for standard probing tasks, exist only for a handful of languages. E.g., UDPipe (Straka, 2018) is available for only about 100 languages, and performance scores for some of these are considerably below those of English (Straka, 2018). 108 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 108–118 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 required for obtaining reliable probing task results. This question is also relevant for English: on the one hand, Conneau et al. (2018) claim that training data for a probing task should be plentiful, as otherwise (highly parametrized) classifiers on top of representations may be unable to extract the relevant information signals; on the other hand, Hewitt and Liang (2019) note that a sufficiently powerful classifier with enough training data can in principle learn any task, without this necessarily allowing to conclude that the representations adequately store the linguistic signal under scrutiny. Second, we ask how stable probing task results are across different classifiers (e.g., MLP vs. Naive Bayes). This question is clo"
2020.conll-1.8,N19-1423,0,0.0354004,"d Random LSTMs (Conneau et al., 2017; Wieting and Kiela, 2019), which feed word embeddings to randomly initialized LSTMs, then apply a pooling operation across time-steps. As parametric methods, we consider: InferSent (Conneau et al., 2017), which induces a sentence representation by learning a semantic entailment relationship between two sentences; QuickThought (Logeswaran and Lee, 2018) which reframes the popular SkipThought model (Kiros et al., 2015) in a classification context; LASER (Artetxe and Schwenk, 2019) derived from massively multilingual machine translation models, and BERT base (Devlin et al., 2019), where we average token embeddings of the last layer for a sentence representation. Dimensionalities of encoders are listed in the appendix. 3.2 Probing Tasks Following Conneau et al. (2018), we consider the following probing tasks: BigramShift (en, tr, ru, ka), TreeDepth (en), Length (en, tr, ru, ka), Subject Number (en, tr, ru), WordContent (en, tr, ru, ka), and TopConstituents (en). Approach In the absence of ground truth, our main interest is in a ‘stable’ structural setup for probing task design—with the end goal of applying this design to multilingual probing analyses (keeping their res"
2020.conll-1.8,W19-4308,1,0.755063,"Missing"
2020.conll-1.8,L18-1550,0,0.0282593,"ifier with a size of roughly 10k instances overall. Table 6 provides more details about the datasets. In line with SentEval (and partly supported by our results on dataset balance given in the appendix), we aim for as balanced label distributions as possible. Because of the small test sizes, we use inner 5-fold cross validation for all tasks except for SubjNumber, where we use pre-defined train/dev/test splits as in Conneau et al. (2018) to avoid leaking lexical information from train to test splits. We obtain average and pmeans embeddings through pooling over pre-trained FastText embeddings (Grave et al., 2018). The same embeddings Figure 3: Pearson correlations across languages for different encoders. (ii) Will encoder performances correlate across languages? For each encoder e, we correlate performances of e between en and the other languages on 5 (for ka) and 7 (for tr, ru) probing tasks (using 10k dataset size and LR for all involved languages, including en). In Figure 3, we observe that correlations between en and other languages are generally either zero or weakly positive. Only average embeddings have more than 1 positive correlation scores across the 3 language combinations with en. Among lo"
2020.conll-1.8,N18-1108,0,0.0227764,"to identify. We adopt Voice (en, tr, ru, ka) from Krasnowska-Kiera´s and Wr´oblewska (2019). For en, we additionally evaluate on TreeDepth and TopConstituents as hard syntactic tasks. We add two tasks not present in the canon of probing tasks given in SentEval: Subject-VerbAgreement (SV-Agree) (en, tr, ru, ka) and Subject-Verb-Distance (SV-Dist) (en, tr, ru). We probe representations for these properties because we suspect that agreement between subject and verb is a difficult task which requires inferring a relationship between pairs of words which may stand in a long-distance relationship (Gulordava et al., 2018). Moverover, we assume this task to be particularly hard in morphologically rich and word-order free languages, thus it could be a good predictor for performance in downstream tasks. To implement the probing tasks, for en, we use the probing tasks datasets defined in Conneau and Kiela (2018) and we apply spaCy4 to sentences extracted from Wikipedia for the newly added probing tasks Voice and SV-Agree. For tr, ru, and ka, we do not rely on dependency parsers because of quality issues and unavailability for ka. Instead, for trand ru, we use information from Universal Dependencies (UD) (Nivre et"
2020.conll-1.8,D19-1275,0,0.0216225,"below those of English (Straka, 2018). 108 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 108–118 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 required for obtaining reliable probing task results. This question is also relevant for English: on the one hand, Conneau et al. (2018) claim that training data for a probing task should be plentiful, as otherwise (highly parametrized) classifiers on top of representations may be unable to extract the relevant information signals; on the other hand, Hewitt and Liang (2019) note that a sufficiently powerful classifier with enough training data can in principle learn any task, without this necessarily allowing to conclude that the representations adequately store the linguistic signal under scrutiny. Second, we ask how stable probing task results are across different classifiers (e.g., MLP vs. Naive Bayes). This question is closely related to the question about size, since different classifiers have different sensitivities to data size; especially deep models are claimed to require more training data. We evaluate the sensitivity of probing task results to the two"
2020.conll-1.8,L18-1293,0,0.0286557,"Missing"
2020.conll-1.8,W16-2512,0,0.0531134,"Missing"
2020.conll-1.8,P19-1573,0,0.222202,"Missing"
2020.conll-1.8,W18-6011,0,0.0216002,"robing is typically either executed on type/token (word) (Tenney et al., 2019) or sentence level (Adi et al., 2017). For sentence level evaluation, SentEval thus far only includes en data. Each probing task in SentEval is balanced and has 100k train, 10k dev, and 10k test instances. The effects of these design choices are unclear, which is why our work addresses their influence systematically. In the multilingual setting, Sahin et al. (2019) propose 15 token and type level probing tasks. Their probing task data is sourced from UniMorph 2.0 (Kirov et al., 2018), Universal Dependency treebanks (McCarthy et al., 2018) and Wikipedia word frequency lists. To deal with lower-resourced languages, they only use 10K samples per probing task/language pair (7K/2K/1K for train/dev/test) and exclude task/language pairs for which this amount cannot be generated. Their final experi2 These questions are important because they indicate whether or not probing tasks (and their relation 109 Code and data are available from https://github. com/UKPLab/conll2020-multilingualsentence-probing. ments are carried out on five languages (Finnish, German, Spanish, ru, tr), for which enough training data is available. They find that"
2020.conll-1.8,L16-1262,0,0.0872553,"Missing"
2020.conll-1.8,W19-4318,0,0.06881,"e to probe sentence embeddings for the linguistic information signals they contain (Perone et al., 2018), as this may not be clear from their performances in downstream tasks. Such probes are linguistic micro tasks—like detecting the length of a sentence or its dependency tree depth—that have to be solved by a classifier using given representations. The majority of approaches for probing sentence embeddings target English, but recently some works have also addressed other languages such as Polish, Russian, or Spanish in a multiand cross-lingual setup (Krasnowska-Kiera´s and Wr´oblewska, 2019; Ravishankar et al., 2019). Motivations for considering a multi-lingual analysis include knowing whether findings from English transfer to other languages and determining a universal set of probing tasks that suits multiple languages, e.g., with richer morphology and freer word order. Our work is also inspired by probing sentence encoders in multiple (particularly low-resource) languages. We are especially interested in the formal structure of probing task design in this context. Namely, when designing probing tasks for lowresource languages, some questions arise naturally that are less critical in English. One of them"
2020.conll-1.8,D19-1410,1,0.836765,"have to be re-evaluated in languages other than en.2 2 Related work Our goal is to probe for sentence-level linguistic knowledge encoded in sentence embeddings (Perone et al., 2018) in a multilingual setup which marginalizes out the effects of probing task design choices when comparing sentence representations. Sentence embeddings have become central for representing texts beyond the word level, e.g., in small data scenarios, where it is difficult to induce good higher-level text representations from word embeddings (Subramanian et al., 2018) or for clustering or text retrieval applications (Reimers and Gurevych, 2019). To standardize the comparison of sentence embeddings, Conneau and Kiela (2018) proposed the SentEval framework for evaluating the quality of sentence embeddings on a range of downstream and 10 probing tasks. Probing tasks are used to introspect embeddings for linguistic knowledge, by taking “probes” as dedicated syntactic or semantic micro tasks (K¨ohn, 2016). As opposed to an evaluation in downstream applications or benchmarks like GLUE (Wang et al., 2018), probing tasks target very specific linguistic knowledge which may otherwise be confounded in downstream applications. Since they are ar"
2020.conll-1.8,K18-2004,0,0.0207424,"Missing"
2020.conll-1.8,D18-1402,1,0.765035,"ing and TREC.7 Statistics for all datasets are reported in Table 6. Argument Mining (AM) AM is an emergent NLP task requiring sophisticated reasoning capabilities. We reuse the sentence-level argument (stance) 5 https://clarino.uib.no/gnc http://translate.google.com 7 To estimate the quality of the machine translation, we measured its performance on parallel data. Details can be found in the appendix. While the machine translation is generally of acceptable quality, we cannot exclude the possibility that it may effect some of our downstream tasks results reported below. 6 detection dataset by Stab et al. (2018), which labels sentences extracted from web pages as pro-, con-, or non-arguments for eight different topics. A sentence only qualifies as pro or con argument when it both expresses a stance towards the topic and gives a reason for that stance. The classifier input is a concatenation of the sentence embedding and the topic encoding. In total, there are about 25,000 sentences. Sentiment Analysis As opposed to AM, sentiment analysis only determines the opinion flavor of a statement. Since sentiment analysis is a very established NLP task, we did not machine translate en training data, but used o"
2020.conll-1.8,W18-5446,0,0.060512,"Missing"
2020.lrec-1.205,P17-1067,0,0.0268003,"relies on lexical resources of emotionally charged words (Strapparava and Valitutti, 2004; Esuli and Sebastiani, 2006; Mohammad and Turney, 2013) and offers a straightforward and transparent way to detect emotions in text. In contrast to rule-based approaches, current models for emotion classification are often based on neural networks and commonly use word embeddings as features. Schuff et al. (2017) applied models from the classes of CNN, BiLSTM, and LSTM and compare them to linear classifiers (SVM and MaxEnt), where the BiLSTM shows best results with the most balanced precision and recall. Abdul-Mageed and Ungar (2017) claim the highest F1 with gated recurrent unit networks (Chung et al., 2015) for Plutchik’s emotion model. More recently, shared tasks on emotion analysis (Mohammad et al., 2018; Klinger et al., 2018) triggered a set of more advanced deep learning approaches, including BERT (Devlin et al., 2019) and other transfer learning methods (Dankers et al., 2019). 3. Data Collection For our annotation and modeling studies, we build on top of two poetry corpora (in English and German), which we refer to as PO-EMO. This collection represents important contributions to the literary canon over the last 400"
2020.lrec-1.205,C16-1074,0,0.317111,"d cost overhead associated with in-house annotation process (that usually involve training and close supervision of the annotators). We provide the final datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dicti"
2020.lrec-1.205,H05-1073,0,0.851899,"tion labels consists of Beauty/Joy, Sadness, Uneasiness, Vitality, Suspense, Awe/Sublime, Humor, Annoyance, and Nostalgia.1 1 The concepts Beauty and Awe/Sublime primarily define objectbased aesthetic virtues. Kant (2001) emphasized that such virtues are typically intuitively felt rather than rationally computed. Such 1652 In addition to selecting an adapted set of emotions, the annotation of poetry brings further challenges, one of which is the choice of the appropriate unit of annotation. Previous work considers words2 (Mohammad and Turney, 2013; Strapparava and Valitutti, 2004), sentences (Alm et al., 2005; Aman and Szpakowicz, 2007), utterances (Cevher et al., 2019), sentence triples (Kim and Klinger, 2018), or paragraphs (Liu et al., 2019) as the units of annotation. For poetry, reasonable units follow the logical document structure of poems, i.e., verse (line), stanza, and, owing to its relative shortness, the complete text. The more coarse-grained the unit, the more difficult the annotation is likely to be, but the more it may also enable the annotation of emotions in context. We find that annotating fine-grained units (lines) that are hierarchically ordered within a larger context (stanza,"
2020.lrec-1.205,C18-1164,0,0.117171,"al datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Furthermore, poetry also lends itself well to semantic (change) analysis (Haider, 2019; Haide"
2020.lrec-1.205,C18-1179,1,0.837616,". Emotion Annotation Emotion corpora have been created for different tasks and with different annotation strategies, with different units of analysis and different foci of emotion perspective (reader, writer, text). Examples include the ISEAR dataset (Scherer and Wallbott, 1994) (document-level); emotion annotation in children stories (Alm et al., 2005) and news headlines (Strapparava and Mihalcea, 2007) (sentence-level); and finegrained emotion annotation in literature by Kim and Klinger (2018) (phrase- and word-level). We refer the interested reader to an overview paper on existing corpora (Bostan and Klinger, 2018). We are only aware of a limited number of publications which look in more depth into the emotion perspective. Buechel and Hahn (2017a) report on an annotation study that focuses both on writer’s and reader’s emotions associated with English sentences. The results show that the reader perspective yields better inter-annotator agreement. Yang et al. (2009) also study the difference between writer and reader emotions, but not with a modeling perspective. The authors find that positive reader emotions tend to be linked to positive writer emotions in online blogs. 1653 3.1. ����� ������ ������ ���"
2020.lrec-1.205,E17-2092,0,0.574085,"emotion labels, the perspective of annotators plays a major role. Whether emotions are elicited in the reader, expressed in the text, or intended by the author largely changes the permissible labels. For example, feelings of Disgust or Love might be intended or expressed in the text, but the text might still fail to elicit corresponding feelings as these concepts presume a strong reaction in the reader. Our focus here was on the actual emotional experience of the readers rather than on hypothetical intentions of authors. We opted for this reader perspective based on previous research in NLP (Buechel and Hahn, 2017a; Buechel and Hahn, 2017b) and work in empirical aesthetics (Menninghaus et al., 2017), that specifically measured the reception of poetry. Our final set of emotion labels consists of Beauty/Joy, Sadness, Uneasiness, Vitality, Suspense, Awe/Sublime, Humor, Annoyance, and Nostalgia.1 1 The concepts Beauty and Awe/Sublime primarily define objectbased aesthetic virtues. Kant (2001) emphasized that such virtues are typically intuitively felt rather than rationally computed. Such 1652 In addition to selecting an adapted set of emotions, the annotation of poetry brings further challenges, one of wh"
2020.lrec-1.205,W17-0801,0,0.568203,"emotion labels, the perspective of annotators plays a major role. Whether emotions are elicited in the reader, expressed in the text, or intended by the author largely changes the permissible labels. For example, feelings of Disgust or Love might be intended or expressed in the text, but the text might still fail to elicit corresponding feelings as these concepts presume a strong reaction in the reader. Our focus here was on the actual emotional experience of the readers rather than on hypothetical intentions of authors. We opted for this reader perspective based on previous research in NLP (Buechel and Hahn, 2017a; Buechel and Hahn, 2017b) and work in empirical aesthetics (Menninghaus et al., 2017), that specifically measured the reception of poetry. Our final set of emotion labels consists of Beauty/Joy, Sadness, Uneasiness, Vitality, Suspense, Awe/Sublime, Humor, Annoyance, and Nostalgia.1 1 The concepts Beauty and Awe/Sublime primarily define objectbased aesthetic virtues. Kant (2001) emphasized that such virtues are typically intuitively felt rather than rationally computed. Such 1652 In addition to selecting an adapted set of emotions, the annotation of poetry brings further challenges, one of wh"
2020.lrec-1.205,D19-1227,0,0.0618039,"Missing"
2020.lrec-1.205,N19-1423,0,0.0881123,"often based on neural networks and commonly use word embeddings as features. Schuff et al. (2017) applied models from the classes of CNN, BiLSTM, and LSTM and compare them to linear classifiers (SVM and MaxEnt), where the BiLSTM shows best results with the most balanced precision and recall. Abdul-Mageed and Ungar (2017) claim the highest F1 with gated recurrent unit networks (Chung et al., 2015) for Plutchik’s emotion model. More recently, shared tasks on emotion analysis (Mohammad et al., 2018; Klinger et al., 2018) triggered a set of more advanced deep learning approaches, including BERT (Devlin et al., 2019) and other transfer learning methods (Dankers et al., 2019). 3. Data Collection For our annotation and modeling studies, we build on top of two poetry corpora (in English and German), which we refer to as PO-EMO. This collection represents important contributions to the literary canon over the last 400 years. We make this resource available in TEI P5 XML3 and an easy-to-use tab separated format. Table 1 shows a size overview of these data sets. Figure 1 shows the distribution of our data over time via density plots. Note that both corpora show a relative underrepresentation before the onset of"
2020.lrec-1.205,W16-0201,0,0.287208,"with in-house annotation process (that usually involve training and close supervision of the annotators). We provide the final datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al.,"
2020.lrec-1.205,esuli-sebastiani-2006-sentiwordnet,0,0.0387291,"thm, and rhyme in other studies (Haider and Kuhn, 2018; Haider et al., 2020). Figure 1: Temporal distribution of poetry corpora (Kernel Density Plots with bandwidth = 0.2). # tokens # lines # stanzas # poems # authors German English 20647 3651 731 158 51 3716 540 174 64 22 3.2. Table 1: Statistics on our poetry corpora PO-EMO. 2.3. Emotion Classification The task of emotion classification has been tackled before using rule-based and machine learning approaches. Rulebased emotion classification typically relies on lexical resources of emotionally charged words (Strapparava and Valitutti, 2004; Esuli and Sebastiani, 2006; Mohammad and Turney, 2013) and offers a straightforward and transparent way to detect emotions in text. In contrast to rule-based approaches, current models for emotion classification are often based on neural networks and commonly use word embeddings as features. Schuff et al. (2017) applied models from the classes of CNN, BiLSTM, and LSTM and compare them to linear classifiers (SVM and MaxEnt), where the BiLSTM shows best results with the most balanced precision and recall. Abdul-Mageed and Ungar (2017) claim the highest F1 with gated recurrent unit networks (Chung et al., 2015) for Plutch"
2020.lrec-1.205,W19-4702,0,0.297849,"yme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Furthermore, poetry also lends itself well to semantic (change) analysis (Haider, 2019; Haider and Eger, 2019), as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Corpus-based analysis of emotions in poetry has been considered, but there is no work on German, and little on English. Kao and Jurafsky (2015) analyze English poems with word associations from the Harvard Inquirer and LIWC, within the categories positive/negative outlook, positive/negative emotion and phys./psych. well-being. Hou and Frank (2"
2020.lrec-1.205,D10-1051,0,0.234926,"poetry, given time and cost overhead associated with in-house annotation process (that usually involve training and close supervision of the annotators). We provide the final datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper."
2020.lrec-1.205,W19-4727,1,0.934327,"2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Furthermore, poetry also lends itself well to semantic (change) analysis (Haider, 2019; Haider and Eger, 2019), as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Corpus-based analysis of emotions in poetry has been considered, but there is no work on German, and little on English. Kao and Jurafsky (2015) analyze English poems with word associations from the Harvard Inquirer and LIWC, within the categories positive/negative outlook, positive/negative emotion and phys./psych. well-being. Hou and Frank (2015) examine the binary sentiment polarity of Chinese poems with a weighted personalized PageRank algorithm. Bar"
2020.lrec-1.205,W18-4509,1,0.788717,"aining and close supervision of the annotators). We provide the final datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Furthermore, poetry also le"
2020.lrec-1.205,W15-3703,0,0.0256667,"and Alam, 2019). Furthermore, poetry also lends itself well to semantic (change) analysis (Haider, 2019; Haider and Eger, 2019), as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Corpus-based analysis of emotions in poetry has been considered, but there is no work on German, and little on English. Kao and Jurafsky (2015) analyze English poems with word associations from the Harvard Inquirer and LIWC, within the categories positive/negative outlook, positive/negative emotion and phys./psych. well-being. Hou and Frank (2015) examine the binary sentiment polarity of Chinese poems with a weighted personalized PageRank algorithm. Barros et al. (2013) followed a tagging approach with a thesaurus to annotate words that are similar to the words ‘Joy’, ‘Anger’, ‘Fear’ and ‘Sadness’ (moreover translating these from English to Spanish). With these word lists, they distinguish the categories ‘Love’, ‘Songs to Lisi’, ‘Satire’ and ‘Philosophical-Moral-Religious’ in Quevedo’s poetry. Similarly, Alsharif et al. (2013) classify unique Arabic ‘emotional text forms’ based on word unigrams. Mohanty et al. (2018) create a corpus of"
2020.lrec-1.205,W17-2201,0,0.340627,"nguage poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Furthermore, poetry also lends itself well to semantic (change) analysis (Haider, 2019; Haider and Eger, 2019), as linguistic inve"
2020.lrec-1.205,C18-1114,1,0.868707,"nnoyance, and Nostalgia.1 1 The concepts Beauty and Awe/Sublime primarily define objectbased aesthetic virtues. Kant (2001) emphasized that such virtues are typically intuitively felt rather than rationally computed. Such 1652 In addition to selecting an adapted set of emotions, the annotation of poetry brings further challenges, one of which is the choice of the appropriate unit of annotation. Previous work considers words2 (Mohammad and Turney, 2013; Strapparava and Valitutti, 2004), sentences (Alm et al., 2005; Aman and Szpakowicz, 2007), utterances (Cevher et al., 2019), sentence triples (Kim and Klinger, 2018), or paragraphs (Liu et al., 2019) as the units of annotation. For poetry, reasonable units follow the logical document structure of poems, i.e., verse (line), stanza, and, owing to its relative shortness, the complete text. The more coarse-grained the unit, the more difficult the annotation is likely to be, but the more it may also enable the annotation of emotions in context. We find that annotating fine-grained units (lines) that are hierarchically ordered within a larger context (stanza, poem) caters to the specific structure of poems, where emotions are regularly mixed and are more interp"
2020.lrec-1.205,C18-2002,0,0.0660867,"Missing"
2020.lrec-1.205,W18-6206,1,0.903998,"Missing"
2020.lrec-1.205,P19-1111,0,0.0262129,"nd Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Furthermore, poetry also lends itself well to semantic (change) analysis (Haider, 2019; Haider and Eger, 2019), as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Corpus-based analysis of emotions in poetry has been considered, but there is no work on German, and little on English. Kao and Jurafsky (2015) analyze English poems with word associations from the Harvard Inquirer and LIWC, within the categories positive/negative outlook, positive/negative emotion and phys./psych. well"
2020.lrec-1.205,D19-1656,0,0.0362616,"Missing"
2020.lrec-1.205,S18-1001,0,0.0419404,"rent way to detect emotions in text. In contrast to rule-based approaches, current models for emotion classification are often based on neural networks and commonly use word embeddings as features. Schuff et al. (2017) applied models from the classes of CNN, BiLSTM, and LSTM and compare them to linear classifiers (SVM and MaxEnt), where the BiLSTM shows best results with the most balanced precision and recall. Abdul-Mageed and Ungar (2017) claim the highest F1 with gated recurrent unit networks (Chung et al., 2015) for Plutchik’s emotion model. More recently, shared tasks on emotion analysis (Mohammad et al., 2018; Klinger et al., 2018) triggered a set of more advanced deep learning approaches, including BERT (Devlin et al., 2019) and other transfer learning methods (Dankers et al., 2019). 3. Data Collection For our annotation and modeling studies, we build on top of two poetry corpora (in English and German), which we refer to as PO-EMO. This collection represents important contributions to the literary canon over the last 400 years. We make this resource available in TEI P5 XML3 and an easy-to-use tab separated format. Table 1 shows a size overview of these data sets. Figure 1 shows the distribution"
2020.lrec-1.205,P11-2014,0,0.428395,"(that usually involve training and close supervision of the annotators). We provide the final datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Fur"
2020.lrec-1.205,W17-2204,0,0.157637,"We provide the final datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this reason, we refer to the whole set of category labels as emotions throughout this paper. 2 to create emotion dictionaries (Krishna et al., 2019; Gopidi and Alam, 2019). Furthermore, poetry also lends itself well to semantic (change) analys"
2020.lrec-1.205,W17-5203,1,0.79635,"stics on our poetry corpora PO-EMO. 2.3. Emotion Classification The task of emotion classification has been tackled before using rule-based and machine learning approaches. Rulebased emotion classification typically relies on lexical resources of emotionally charged words (Strapparava and Valitutti, 2004; Esuli and Sebastiani, 2006; Mohammad and Turney, 2013) and offers a straightforward and transparent way to detect emotions in text. In contrast to rule-based approaches, current models for emotion classification are often based on neural networks and commonly use word embeddings as features. Schuff et al. (2017) applied models from the classes of CNN, BiLSTM, and LSTM and compare them to linear classifiers (SVM and MaxEnt), where the BiLSTM shows best results with the most balanced precision and recall. Abdul-Mageed and Ungar (2017) claim the highest F1 with gated recurrent unit networks (Chung et al., 2015) for Plutchik’s emotion model. More recently, shared tasks on emotion analysis (Mohammad et al., 2018; Klinger et al., 2018) triggered a set of more advanced deep learning approaches, including BERT (Devlin et al., 2019) and other transfer learning methods (Dankers et al., 2019). 3. Data Collectio"
2020.lrec-1.205,N18-2006,1,0.844183,"trategy that would suit the crowdsourcing environment. The dataset presented in this paper can be of use for different application scenarios, including multi-label emotion classification, style-conditioned poetry generation, investigating the influence of rhythm/prosodic features on emotion, or analysis of authors, genres and diachronic variation (e.g., how emotions are represented differently in certain periods). Further, though our modeling experiments are still rudimentary, we propose that this data set can be used to investigate the intra-poem relations either through multi-task learning (Schulz et al., 2018) and/or with the help of hierarchical sequence classification approaches. Acknowledgements A special thanks goes to Gesine Fuhrmann, who created the guidelines and tirelessly documented the annotation progress. Also thanks to Annika Palm and Debby Trzeciak who annotated and gave lively feedback. For help with the conceptualization of labels we thank Ines Schindler. This research has been partially conducted within the CRETA center (http://www.creta. uni-stuttgart.de/) which is funded by the German Ministry for Education and Research (BMBF) and partially funded by the German Research Council (D"
2020.lrec-1.205,S07-1013,0,0.350778,"hese studies focus on basic emotions and binary sentiment polarity only, rather than addressing aesthetic emotions. Moreover, they annotate on the level of complete poems (instead of fine-grained verse and stanza-level). 2.2. Emotion Annotation Emotion corpora have been created for different tasks and with different annotation strategies, with different units of analysis and different foci of emotion perspective (reader, writer, text). Examples include the ISEAR dataset (Scherer and Wallbott, 1994) (document-level); emotion annotation in children stories (Alm et al., 2005) and news headlines (Strapparava and Mihalcea, 2007) (sentence-level); and finegrained emotion annotation in literature by Kim and Klinger (2018) (phrase- and word-level). We refer the interested reader to an overview paper on existing corpora (Bostan and Klinger, 2018). We are only aware of a limited number of publications which look in more depth into the emotion perspective. Buechel and Hahn (2017a) report on an annotation study that focuses both on writer’s and reader’s emotions associated with English sentences. The results show that the reader perspective yields better inter-annotator agreement. Yang et al. (2009) also study the differenc"
2020.lrec-1.205,strapparava-valitutti-2004-wordnet,0,0.826287,"the reception of poetry. Our final set of emotion labels consists of Beauty/Joy, Sadness, Uneasiness, Vitality, Suspense, Awe/Sublime, Humor, Annoyance, and Nostalgia.1 1 The concepts Beauty and Awe/Sublime primarily define objectbased aesthetic virtues. Kant (2001) emphasized that such virtues are typically intuitively felt rather than rationally computed. Such 1652 In addition to selecting an adapted set of emotions, the annotation of poetry brings further challenges, one of which is the choice of the appropriate unit of annotation. Previous work considers words2 (Mohammad and Turney, 2013; Strapparava and Valitutti, 2004), sentences (Alm et al., 2005; Aman and Szpakowicz, 2007), utterances (Cevher et al., 2019), sentence triples (Kim and Klinger, 2018), or paragraphs (Liu et al., 2019) as the units of annotation. For poetry, reasonable units follow the logical document structure of poems, i.e., verse (line), stanza, and, owing to its relative shortness, the complete text. The more coarse-grained the unit, the more difficult the annotation is likely to be, but the more it may also enable the annotation of emotions in context. We find that annotating fine-grained units (lines) that are hierarchically ordered wit"
2020.lrec-1.205,W13-1403,0,0.0324413,"ential of a crowdsourcing environment for the task of self-perceived emotion annotation in poetry, given time and cost overhead associated with in-house annotation process (that usually involve training and close supervision of the annotators). We provide the final datasets of German and English language poems annotated with reader emotions on verse level at https://github.com/tnhaider/ poetry-emotion. 2. 2.1. Related Work Poetry in Natural Language Processing Natural language understanding research on poetry has investigated stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015; Voigt and Jurafsky, 2013), with a focus on broadly accepted formal features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017; Baumann et al., 2018) and metaphor (Kesarwani et al., 2017; Reinig and Rehbein, 2019). Recent work has also explored the relationship of poetry and prose, mainly on a syntactic level feelings of Beauty and Sublime have therefore come to be subsumed under the rubrique of aesthetic emotions in recent psychological research (Menninghaus et al., 2019). For this re"
2020.semeval-1.22,P19-4007,0,0.0365015,"Missing"
2020.semeval-1.22,P16-2009,1,0.709099,"ubtask 2 for English. In addition to describing our system, we discuss our hyperparameter configurations and examine why our system lags behind for the other languages involved in the shared task (German, Swedish, Latin). Our code is available at https://github.com/DavidRother/semeval2020-task1 1 Introduction With the advent of increasingly sophisticated distributional models, semantic change analysis has gained considerable popularity in natural language processing. Earlier methods use static word embeddings to identify laws of semantic change (Hamilton et al., 2016a; Hamilton et al., 2016b; Eger and Mehler, 2016). Recently, contextualized embeddings (Devlin et al., 2018) have allowed to track more fine-grained sense changes of words over time (Hu et al., 2019). The SemEval-2020 “Unsupervised Lexical Semantic Change Detection task” (Schlechtweg et al., 2020) aims at two levels of granularity to detect lexical semantic change: 1) a binary classification task to identify whether or not a word lost/gained a sense between two time periods, and 2) a ranking task, where a list of words has to be ordered by the amount of change these have undergone between the same two time periods. Both subtasks have to be s"
2020.semeval-1.22,D16-1229,0,0.01478,"st on subtask 1 for English and ranks third in subtask 2 for English. In addition to describing our system, we discuss our hyperparameter configurations and examine why our system lags behind for the other languages involved in the shared task (German, Swedish, Latin). Our code is available at https://github.com/DavidRother/semeval2020-task1 1 Introduction With the advent of increasingly sophisticated distributional models, semantic change analysis has gained considerable popularity in natural language processing. Earlier methods use static word embeddings to identify laws of semantic change (Hamilton et al., 2016a; Hamilton et al., 2016b; Eger and Mehler, 2016). Recently, contextualized embeddings (Devlin et al., 2018) have allowed to track more fine-grained sense changes of words over time (Hu et al., 2019). The SemEval-2020 “Unsupervised Lexical Semantic Change Detection task” (Schlechtweg et al., 2020) aims at two levels of granularity to detect lexical semantic change: 1) a binary classification task to identify whether or not a word lost/gained a sense between two time periods, and 2) a ranking task, where a list of words has to be ordered by the amount of change these have undergone between the"
2020.semeval-1.22,P16-1141,0,0.0195833,"st on subtask 1 for English and ranks third in subtask 2 for English. In addition to describing our system, we discuss our hyperparameter configurations and examine why our system lags behind for the other languages involved in the shared task (German, Swedish, Latin). Our code is available at https://github.com/DavidRother/semeval2020-task1 1 Introduction With the advent of increasingly sophisticated distributional models, semantic change analysis has gained considerable popularity in natural language processing. Earlier methods use static word embeddings to identify laws of semantic change (Hamilton et al., 2016a; Hamilton et al., 2016b; Eger and Mehler, 2016). Recently, contextualized embeddings (Devlin et al., 2018) have allowed to track more fine-grained sense changes of words over time (Hu et al., 2019). The SemEval-2020 “Unsupervised Lexical Semantic Change Detection task” (Schlechtweg et al., 2020) aims at two levels of granularity to detect lexical semantic change: 1) a binary classification task to identify whether or not a word lost/gained a sense between two time periods, and 2) a ranking task, where a list of words has to be ordered by the amount of change these have undergone between the"
2020.semeval-1.22,P19-1379,0,0.0222408,"r languages involved in the shared task (German, Swedish, Latin). Our code is available at https://github.com/DavidRother/semeval2020-task1 1 Introduction With the advent of increasingly sophisticated distributional models, semantic change analysis has gained considerable popularity in natural language processing. Earlier methods use static word embeddings to identify laws of semantic change (Hamilton et al., 2016a; Hamilton et al., 2016b; Eger and Mehler, 2016). Recently, contextualized embeddings (Devlin et al., 2018) have allowed to track more fine-grained sense changes of words over time (Hu et al., 2019). The SemEval-2020 “Unsupervised Lexical Semantic Change Detection task” (Schlechtweg et al., 2020) aims at two levels of granularity to detect lexical semantic change: 1) a binary classification task to identify whether or not a word lost/gained a sense between two time periods, and 2) a ranking task, where a list of words has to be ordered by the amount of change these have undergone between the same two time periods. Both subtasks have to be solved for four languages: English, German, Latin and Swedish. To address the tasks, we deploy a language-agnostic system with three ingredients. i) We"
2020.semeval-1.22,N18-2027,0,0.136224,"00-1860 1946-1990 70M / 72M 48 1790-1830 1895-1903 71M / 111M 31 -200-0 0-2000 1.7M / 9.4M 40 Table 1: Corpus Overview Statistics We aim to detect word sense change for given lemmatized focus words in English, German, Swedish and Latin. For each involved language, the task organizers supply a corpus for two epochs; statistics are given in Table 1. Note that the corpus sizes vary considerably across the languages, with Latin being smallest in size. The manual annotation of the test data against which all systems were eventually evaluated was done by human experts according to the guidelines by Schlechtweg et al. (2018). Annotators were asked to label focus words with one of several discrete senses. The precise definition of the subtasks of the shared task is the following: • Subtask 1 is a binary classification task to identify whether or not a word lost/gained a sense between two time periods t1 and t2 , i.e., if it underwent meaning change or not. By the task organizers’ specification, a sense counts as lost/gained if it occurs at least n = 5 times in t1 (t2 ) and at most k = 2 times in t2 (t1 ) (Latin had n = 2 and k = 0, respectively). • Subtask 2 asks to rank a list of words by the amount of change the"
2020.semeval-1.22,2020.semeval-1.1,0,0.0806925,"Missing"
2021.acl-long.129,2020.acl-main.747,0,0.0283513,"Missing"
2021.acl-long.129,2020.acl-main.372,0,0.0126672,"th both short-term and long-term effects for public opinion and political trust (Gangl and Giustozzi, 1624 2018; Nicoli, 2017). Our paper addresses these problems by drawing on large volumes of longitudinal social media data that reflect potential fragmentation of political opinion (Sunstein, 2018) and its change over time. Our approach will thus uncover how contested European solidarity is and how it developed since the onset of COVID-19. Emotion and Sentiment Classification in NLP. In NLP, annotating and classifying text (in social media) for sentiment or emotions is a wellestablished task (Demszky et al., 2020; Ding et al., 2020; Haider et al., 2020; Hutto and Gilbert, 2014; Oberl¨ander and Klinger, 2018). Importantly, our approach focuses on expressions of (anti)solidarity: For example, texts containing a positive sentiment towards persons, groups or organizations which are at their core anti-European, nationalistic and excluding reflect anti-solidarity and are annotated as such. Our annotations therefore go beyond superficial assessment of sentiment. In fact, the correlation between sentiment labels—e.g., as obtained from Vader (Hutto and Gilbert, 2014)—and our annotations in §3 is only ∼0.2. Spe"
2021.acl-long.129,N19-1423,0,0.0668415,"Missing"
2021.acl-long.129,2020.emnlp-main.106,0,0.0229435,"long-term effects for public opinion and political trust (Gangl and Giustozzi, 1624 2018; Nicoli, 2017). Our paper addresses these problems by drawing on large volumes of longitudinal social media data that reflect potential fragmentation of political opinion (Sunstein, 2018) and its change over time. Our approach will thus uncover how contested European solidarity is and how it developed since the onset of COVID-19. Emotion and Sentiment Classification in NLP. In NLP, annotating and classifying text (in social media) for sentiment or emotions is a wellestablished task (Demszky et al., 2020; Ding et al., 2020; Haider et al., 2020; Hutto and Gilbert, 2014; Oberl¨ander and Klinger, 2018). Importantly, our approach focuses on expressions of (anti)solidarity: For example, texts containing a positive sentiment towards persons, groups or organizations which are at their core anti-European, nationalistic and excluding reflect anti-solidarity and are annotated as such. Our annotations therefore go beyond superficial assessment of sentiment. In fact, the correlation between sentiment labels—e.g., as obtained from Vader (Hutto and Gilbert, 2014)—and our annotations in §3 is only ∼0.2. Specifically, many twe"
2021.acl-long.129,2020.lrec-1.205,1,0.832283,"for public opinion and political trust (Gangl and Giustozzi, 1624 2018; Nicoli, 2017). Our paper addresses these problems by drawing on large volumes of longitudinal social media data that reflect potential fragmentation of political opinion (Sunstein, 2018) and its change over time. Our approach will thus uncover how contested European solidarity is and how it developed since the onset of COVID-19. Emotion and Sentiment Classification in NLP. In NLP, annotating and classifying text (in social media) for sentiment or emotions is a wellestablished task (Demszky et al., 2020; Ding et al., 2020; Haider et al., 2020; Hutto and Gilbert, 2014; Oberl¨ander and Klinger, 2018). Importantly, our approach focuses on expressions of (anti)solidarity: For example, texts containing a positive sentiment towards persons, groups or organizations which are at their core anti-European, nationalistic and excluding reflect anti-solidarity and are annotated as such. Our annotations therefore go beyond superficial assessment of sentiment. In fact, the correlation between sentiment labels—e.g., as obtained from Vader (Hutto and Gilbert, 2014)—and our annotations in §3 is only ∼0.2. Specifically, many tweets labeled as solida"
2021.acl-long.129,C18-1179,0,0.029506,"Missing"
2021.acl-long.129,N16-3020,0,0.0635916,"Missing"
2021.acl-long.129,2020.acl-main.151,1,0.697,"Missing"
2021.acl-long.179,J15-2005,0,0.0255962,"motivates aggregation mechanisms like BT. Aggregations. Pairwise comparison mechanisms date back to Thurstone (1927). Subsequently, the Bradley-Terry (BT) model has become a standard pairwise comparison model (Bradley and Terry, 1952). In NLP, BT-inspired mechanisms have sometimes been used to aggregate human assessments. For instance, Deriu et al. (2020) ranked chatbots regarding their ability to mimic conversational behavior of humans. Item response theory (IRT) has a similar formulation as BT, but also estimates the difficulty of each test instances using a latent-variable Bayesian model (Dras, 2015). 2307 Median vs. BT Mean vs. Median a) Per metric disagreement 0.3 0.2 0.1 0.0 -1 EU BL -2 EU BL -3 EU BL R EO T ME CI r DE sp E-2 E-2 E-1 GE-1 -re -W UG S3 E-W ROU GE RO UG O R U RO b) Per task disagreement Percentage disagreement Percentage disagreement Percentage disagreement Mean vs. BT 0.2 0.1 0.0 a ue log Di p Ca . tio m Su m. p rfp S ver Mo e cor r Sco RT BE e c) Per test set size disagreement 0.20 0.15 0.10 0.05 0 MT Ch 1000 2000 3000 4000 5000 Size of test set Figure 4: This figure measures the percentage of disagreement between each pair of aggregation mechanisms across different di"
2021.acl-long.179,P18-1128,0,0.127344,"s that system A is better. The latter case corresponds to most of the test instances being located in the upper-left triangle (A &gt; B). The half-space with more mass is shaded. Divine et al. (2018) showed that Wilcoxon’s signedrank test does not always properly account for the pairing of data, unlike the sign test. When performing statistical testing, it seems obvious that we should use the paired version of tests when the data is naturally paired (Rankel et al., 2011). Even works discussing statistical testing in NLP recommend Wilcoxon’s signed-rank test (Graham, 2015; Owczarzak et al., 2012; Dror et al., 2018). Yet, to obtain aggregated scores for systems, the community still mostly uses aggregation mechanisms that ignore the pairing, such as MEAN. MEDIAN is the outlier-resistant version of MEAN , and BT is the paired variant of MEDIAN. Whenever one recommends a paired test of medians, such as the sign test or Wilcoxon’s signed-rank test, to obtain p-values, one should use BT to compare system scores. 4 Simulations with synthetic data Next, we perform simulations to extend the analysis of the previous section to (i) N &gt; 2 systems, (ii) finitely many test samples, (iii) a practical implementation of"
2021.acl-long.179,2020.emnlp-main.5,0,0.0199541,"3, we know that the difference between MEAN and MEDIAN is due to the presence of statistical outliers, while the difference between ME DIAN and BT is due to the presence of different test instance types (Fig. 3). With real NLP datasets, in Fig. 4, we observe some discrepancy between MEAN and MEDIAN, indicating the presence of outliers. There is even more disagreement between MEDIAN and BT , indicating the presence of different types of test instances, as illustrated in Fig. 3. 6 Related work Several studies have made a critical assessment of the standard evaluation methodologies. For example, Freitag et al. (2020) demonstrate the advantages of carefully choosing which references to use for NLG evaluation. Mathur et al. (2020) show that outliers matter in practice. Recently, Graham et al. (2020) draws attention on test set size. Several works have emphasized the importance of careful statistical testing (Rankel et al., 2011; Owczarzak et al., 2012; Graham, 2015; Dror et al., 2018). They recommend paired statistical tests. Finally, Novikova et al. (2018) report that “relative rankings yield more discriminative results than absolute assessments”, which further motivates aggregation mechanisms like BT. Agg"
2021.acl-long.179,2020.emnlp-main.6,0,0.0359027,"st instance types (Fig. 3). With real NLP datasets, in Fig. 4, we observe some discrepancy between MEAN and MEDIAN, indicating the presence of outliers. There is even more disagreement between MEDIAN and BT , indicating the presence of different types of test instances, as illustrated in Fig. 3. 6 Related work Several studies have made a critical assessment of the standard evaluation methodologies. For example, Freitag et al. (2020) demonstrate the advantages of carefully choosing which references to use for NLG evaluation. Mathur et al. (2020) show that outliers matter in practice. Recently, Graham et al. (2020) draws attention on test set size. Several works have emphasized the importance of careful statistical testing (Rankel et al., 2011; Owczarzak et al., 2012; Graham, 2015; Dror et al., 2018). They recommend paired statistical tests. Finally, Novikova et al. (2018) report that “relative rankings yield more discriminative results than absolute assessments”, which further motivates aggregation mechanisms like BT. Aggregations. Pairwise comparison mechanisms date back to Thurstone (1927). Subsequently, the Bradley-Terry (BT) model has become a standard pairwise comparison model (Bradley and Terry,"
2021.acl-long.179,D16-1062,0,0.0214443,"0 MT Ch 1000 2000 3000 4000 5000 Size of test set Figure 4: This figure measures the percentage of disagreement between each pair of aggregation mechanisms across different dimensions with real evaluation setups. Fig. 4(a) shows the disagreement per evaluation metric averaged over tasks and uniformly subsampled test set sizes, Fig. 4(b) shows the disagreement per task averaged over evaluation metrics and uniformly subsampled test set sizes, and Fig. 4(c) shows the disagreement across test set sizes averaged over tasks and evaluation metrics. IRT has been applied to perform dataset filtering (Lalor et al., 2016, 2019), evaluate chatbots from human assessments (Sedoc and Ungar, 2020), and aggregate human assessments in machine translation (Dras, 2015). Elo (Elo, 1978) and TrueSkill (Herbrich et al., 2007) are famous extensions of the BT model commonly used to rate players in the context of gaming or sports events. Elo views player strengths as normally distributed random variables. TrueSkill is a Bayesian variant of Elo. Since 2015, the Workshop on Machine Translation (WMT) has been using TrueSkill to rank models based on human assessments following the methodology of Sakaguchi et al. (2014). We prov"
2021.acl-long.179,D19-1434,0,0.0216875,"Missing"
2021.acl-long.179,W07-0734,0,0.0610159,"ion, we use the shared tasks of WMT-17 (Bojar et al., 2017), WMT-18 (Ma et al., 2018), and WMT-19 (Ma et al., 2019). For image captioning, we use the MSCOCO (Lin et al., 2014) dataset, and for dialogue, we use the PersonaChat and TopicalChat (Mehri and Eskenazi, 2020) datasets. The evaluation scores are obtained with a total of 18 different evaluation metrics: BLEU-[1,2,3,4] (Papineni et al., 2002), ROUGE-[1,2,L] (Lin, 2004), ROUGE-WE-[1,2] (Ng and Abrecht, 2015), JS-[1,2] (Lin et al., 2006), S3-[pyr, resp] (Peyrard et al., 2017), CIDEr (Vedantam et al., 2015), Chrfpp (Popovic, 2017), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020). Some metrics are only available for some task; e.g., CIDEr, METEOR are only available for the image captioning task. We provide details about datasets, metrics, and their statistics in Appendix A.3. Overall, across datasets and metrics we have 296 evaluation setups, 73,471 pairs of systems, and 91,197 test instances. We also experiment with sub-sampling different sizes of test sets (see Appendix A.3) to simulate varying train/dev/test splits or cross-validation. Comparison of BT, MEAN, and MEDIAN In Table 1, we report the di"
2021.acl-long.179,W04-1013,0,0.30455,"ides whether system A is better than B based on the evaluation scores of the two systems. We use ΘT,M (A, B) = Θ(MA , MB ) to denote the comparison mechanism between A and B on the test set T with evaluation metric M. Here, Θ outputs its guess about which system is the best (or declares the comparison inconclusive if the difference is not statistically significant). For simplicity, we drop the dependency on T and M in the notation, simply writing Θ(A, B). For example in text summarization, xl is a source document from the test set, yl its corresponding reference summary, and M might be ROUGE (Lin, 2004). The decision mechanism Θ usually compares the individual systems’ mean evaluation scores, where the system with the highest mean score (here mean ROUGE score) is declared better. Consistent evaluation result. We say that the outcome of such an evaluation is consistent if it recovers the ordering of systems implied by the inherent strengths of systems: Θ(A, B) = A ⇐⇒ λA &gt; λB . Probabilistic model. As commonly done in the literature on statistical testing, we view the evaluation scores of a system A as n indexed random variables: (l) XA , l = 1, . . . , n, where n is the size of the test set."
2021.acl-long.179,N06-1059,0,0.0918497,"ks. For summarization, we use the TAC-08, TAC-09, TAC-11 and CNN/DM (Hermann et al., 2015) datasets. For machine translation, we use the shared tasks of WMT-17 (Bojar et al., 2017), WMT-18 (Ma et al., 2018), and WMT-19 (Ma et al., 2019). For image captioning, we use the MSCOCO (Lin et al., 2014) dataset, and for dialogue, we use the PersonaChat and TopicalChat (Mehri and Eskenazi, 2020) datasets. The evaluation scores are obtained with a total of 18 different evaluation metrics: BLEU-[1,2,3,4] (Papineni et al., 2002), ROUGE-[1,2,L] (Lin, 2004), ROUGE-WE-[1,2] (Ng and Abrecht, 2015), JS-[1,2] (Lin et al., 2006), S3-[pyr, resp] (Peyrard et al., 2017), CIDEr (Vedantam et al., 2015), Chrfpp (Popovic, 2017), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020). Some metrics are only available for some task; e.g., CIDEr, METEOR are only available for the image captioning task. We provide details about datasets, metrics, and their statistics in Appendix A.3. Overall, across datasets and metrics we have 296 evaluation setups, 73,471 pairs of systems, and 91,197 test instances. We also experiment with sub-sampling different sizes of test sets (see Appendix A.3"
2021.acl-long.179,W18-6450,0,0.0305076,"Missing"
2021.acl-long.179,W19-5302,0,0.0278746,"Missing"
2021.acl-long.179,2020.acl-main.448,0,0.0326269,"fference between ME DIAN and BT is due to the presence of different test instance types (Fig. 3). With real NLP datasets, in Fig. 4, we observe some discrepancy between MEAN and MEDIAN, indicating the presence of outliers. There is even more disagreement between MEDIAN and BT , indicating the presence of different types of test instances, as illustrated in Fig. 3. 6 Related work Several studies have made a critical assessment of the standard evaluation methodologies. For example, Freitag et al. (2020) demonstrate the advantages of carefully choosing which references to use for NLG evaluation. Mathur et al. (2020) show that outliers matter in practice. Recently, Graham et al. (2020) draws attention on test set size. Several works have emphasized the importance of careful statistical testing (Rankel et al., 2011; Owczarzak et al., 2012; Graham, 2015; Dror et al., 2018). They recommend paired statistical tests. Finally, Novikova et al. (2018) report that “relative rankings yield more discriminative results than absolute assessments”, which further motivates aggregation mechanisms like BT. Aggregations. Pairwise comparison mechanisms date back to Thurstone (1927). Subsequently, the Bradley-Terry (BT) mode"
2021.acl-long.179,2020.acl-main.64,0,0.0219897,"Missing"
2021.acl-long.179,D15-1222,0,0.0247526,"aluation scores from four NLG tasks. For summarization, we use the TAC-08, TAC-09, TAC-11 and CNN/DM (Hermann et al., 2015) datasets. For machine translation, we use the shared tasks of WMT-17 (Bojar et al., 2017), WMT-18 (Ma et al., 2018), and WMT-19 (Ma et al., 2019). For image captioning, we use the MSCOCO (Lin et al., 2014) dataset, and for dialogue, we use the PersonaChat and TopicalChat (Mehri and Eskenazi, 2020) datasets. The evaluation scores are obtained with a total of 18 different evaluation metrics: BLEU-[1,2,3,4] (Papineni et al., 2002), ROUGE-[1,2,L] (Lin, 2004), ROUGE-WE-[1,2] (Ng and Abrecht, 2015), JS-[1,2] (Lin et al., 2006), S3-[pyr, resp] (Peyrard et al., 2017), CIDEr (Vedantam et al., 2015), Chrfpp (Popovic, 2017), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020). Some metrics are only available for some task; e.g., CIDEr, METEOR are only available for the image captioning task. We provide details about datasets, metrics, and their statistics in Appendix A.3. Overall, across datasets and metrics we have 296 evaluation setups, 73,471 pairs of systems, and 91,197 test instances. We also experiment with sub-sampling different sizes o"
2021.acl-long.179,N18-2012,0,0.0245824,"nstances, as illustrated in Fig. 3. 6 Related work Several studies have made a critical assessment of the standard evaluation methodologies. For example, Freitag et al. (2020) demonstrate the advantages of carefully choosing which references to use for NLG evaluation. Mathur et al. (2020) show that outliers matter in practice. Recently, Graham et al. (2020) draws attention on test set size. Several works have emphasized the importance of careful statistical testing (Rankel et al., 2011; Owczarzak et al., 2012; Graham, 2015; Dror et al., 2018). They recommend paired statistical tests. Finally, Novikova et al. (2018) report that “relative rankings yield more discriminative results than absolute assessments”, which further motivates aggregation mechanisms like BT. Aggregations. Pairwise comparison mechanisms date back to Thurstone (1927). Subsequently, the Bradley-Terry (BT) model has become a standard pairwise comparison model (Bradley and Terry, 1952). In NLP, BT-inspired mechanisms have sometimes been used to aggregate human assessments. For instance, Deriu et al. (2020) ranked chatbots regarding their ability to mimic conversational behavior of humans. Item response theory (IRT) has a similar formulati"
2021.acl-long.179,W12-2601,0,0.209344,"riangle, then BT predicts that system A is better. The latter case corresponds to most of the test instances being located in the upper-left triangle (A &gt; B). The half-space with more mass is shaded. Divine et al. (2018) showed that Wilcoxon’s signedrank test does not always properly account for the pairing of data, unlike the sign test. When performing statistical testing, it seems obvious that we should use the paired version of tests when the data is naturally paired (Rankel et al., 2011). Even works discussing statistical testing in NLP recommend Wilcoxon’s signed-rank test (Graham, 2015; Owczarzak et al., 2012; Dror et al., 2018). Yet, to obtain aggregated scores for systems, the community still mostly uses aggregation mechanisms that ignore the pairing, such as MEAN. MEDIAN is the outlier-resistant version of MEAN , and BT is the paired variant of MEDIAN. Whenever one recommends a paired test of medians, such as the sign test or Wilcoxon’s signed-rank test, to obtain p-values, one should use BT to compare system scores. 4 Simulations with synthetic data Next, we perform simulations to extend the analysis of the previous section to (i) N &gt; 2 systems, (ii) finitely many test samples, (iii) a practic"
2021.acl-long.179,P02-1040,0,0.124433,"1 In this section, we perform large-scale experiments using real evaluation scores from four NLG tasks. For summarization, we use the TAC-08, TAC-09, TAC-11 and CNN/DM (Hermann et al., 2015) datasets. For machine translation, we use the shared tasks of WMT-17 (Bojar et al., 2017), WMT-18 (Ma et al., 2018), and WMT-19 (Ma et al., 2019). For image captioning, we use the MSCOCO (Lin et al., 2014) dataset, and for dialogue, we use the PersonaChat and TopicalChat (Mehri and Eskenazi, 2020) datasets. The evaluation scores are obtained with a total of 18 different evaluation metrics: BLEU-[1,2,3,4] (Papineni et al., 2002), ROUGE-[1,2,L] (Lin, 2004), ROUGE-WE-[1,2] (Ng and Abrecht, 2015), JS-[1,2] (Lin et al., 2006), S3-[pyr, resp] (Peyrard et al., 2017), CIDEr (Vedantam et al., 2015), Chrfpp (Popovic, 2017), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020). Some metrics are only available for some task; e.g., CIDEr, METEOR are only available for the image captioning task. We provide details about datasets, metrics, and their statistics in Appendix A.3. Overall, across datasets and metrics we have 296 evaluation setups, 73,471 pairs of systems, and 91,197 test"
2021.acl-long.179,W17-4510,1,0.853635,"AC-08, TAC-09, TAC-11 and CNN/DM (Hermann et al., 2015) datasets. For machine translation, we use the shared tasks of WMT-17 (Bojar et al., 2017), WMT-18 (Ma et al., 2018), and WMT-19 (Ma et al., 2019). For image captioning, we use the MSCOCO (Lin et al., 2014) dataset, and for dialogue, we use the PersonaChat and TopicalChat (Mehri and Eskenazi, 2020) datasets. The evaluation scores are obtained with a total of 18 different evaluation metrics: BLEU-[1,2,3,4] (Papineni et al., 2002), ROUGE-[1,2,L] (Lin, 2004), ROUGE-WE-[1,2] (Ng and Abrecht, 2015), JS-[1,2] (Lin et al., 2006), S3-[pyr, resp] (Peyrard et al., 2017), CIDEr (Vedantam et al., 2015), Chrfpp (Popovic, 2017), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020). Some metrics are only available for some task; e.g., CIDEr, METEOR are only available for the image captioning task. We provide details about datasets, metrics, and their statistics in Appendix A.3. Overall, across datasets and metrics we have 296 evaluation setups, 73,471 pairs of systems, and 91,197 test instances. We also experiment with sub-sampling different sizes of test sets (see Appendix A.3) to simulate varying train/dev/test sp"
2021.acl-long.179,W14-3301,0,0.0562296,"Missing"
2021.acl-long.179,2020.eval4nlp-1.3,0,0.155872,"native aggregation mechanism: the Bradley–Terry (BT) model 2301 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2301–2315 August 1–6, 2021. ©2021 Association for Computational Linguistics (Bradley and Terry, 1952). BT compares systems for each test instance and estimates the latent strength of systems based on how frequently one system scores higher than another. Such paired mechanisms have already been successfully used to aggregate human judgments (Novikova et al., 2018; Sedoc and Ungar, 2020); for example, WMT evaluation protocols regularly employ TrueSkill (Herbrich et al., 2007), a Bayesian variant of BT (Sakaguchi et al., 2014). Contributions. We contribute the first comprehensive analysis of the BT model (especially vis-à-vis mean and median) as an aggregation mechanism for comparing system scores in NLP. (i) We illustrate the importance of accounting for instance-level pairing and discuss the conditions under which the mean, median, and BT disagree about the ordering of systems. In Sec. 3, we draw parallels with the field of statistical testing, where paired statistical tests"
2021.acl-long.179,2020.acl-main.151,1,0.843346,"disagreement in real NLP evaluation setups. 7 Discussion We briefly discuss some possible questions raised by the use of BT-like metrics, with more details provided in Appendix E, F, G, and H. Extension to other evaluation setups. The experiments of Sec. 5 focus on reference-based NLG evaluation metrics. However, the arguments laid out throughout the paper apply beyond this setup. Any comparison of systems based on score aggregation is susceptible to suffer from outliers and complex pairing structures (e.g., Fig. 2). Future work should replicate our experimental setup for reference-free NLG (Zhao et al., 2020), classification, or regression tasks. Type imbalance. Imagine a test set with a majority of easy instances and few hard ones. A system A could perform slightly worse than B on easy instances but much better on hard ones and will be declared worse by BT. If one views this decision as problematic then one should probably acknowledge that the test set is not representative of what should be measured. If hard instances matter more there should be a majority of them in the test set. Hoping that MEAN will be swayed to output the intuitive ordering of systems from a minority of test instances is a p"
2021.acl-long.179,D19-1053,1,0.848396,"(Bojar et al., 2017), WMT-18 (Ma et al., 2018), and WMT-19 (Ma et al., 2019). For image captioning, we use the MSCOCO (Lin et al., 2014) dataset, and for dialogue, we use the PersonaChat and TopicalChat (Mehri and Eskenazi, 2020) datasets. The evaluation scores are obtained with a total of 18 different evaluation metrics: BLEU-[1,2,3,4] (Papineni et al., 2002), ROUGE-[1,2,L] (Lin, 2004), ROUGE-WE-[1,2] (Ng and Abrecht, 2015), JS-[1,2] (Lin et al., 2006), S3-[pyr, resp] (Peyrard et al., 2017), CIDEr (Vedantam et al., 2015), Chrfpp (Popovic, 2017), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020). Some metrics are only available for some task; e.g., CIDEr, METEOR are only available for the image captioning task. We provide details about datasets, metrics, and their statistics in Appendix A.3. Overall, across datasets and metrics we have 296 evaluation setups, 73,471 pairs of systems, and 91,197 test instances. We also experiment with sub-sampling different sizes of test sets (see Appendix A.3) to simulate varying train/dev/test splits or cross-validation. Comparison of BT, MEAN, and MEDIAN In Table 1, we report the disagreement between aggregation m"
2021.acl-long.179,W17-4770,0,0.0139054,"ts. For machine translation, we use the shared tasks of WMT-17 (Bojar et al., 2017), WMT-18 (Ma et al., 2018), and WMT-19 (Ma et al., 2019). For image captioning, we use the MSCOCO (Lin et al., 2014) dataset, and for dialogue, we use the PersonaChat and TopicalChat (Mehri and Eskenazi, 2020) datasets. The evaluation scores are obtained with a total of 18 different evaluation metrics: BLEU-[1,2,3,4] (Papineni et al., 2002), ROUGE-[1,2,L] (Lin, 2004), ROUGE-WE-[1,2] (Ng and Abrecht, 2015), JS-[1,2] (Lin et al., 2006), S3-[pyr, resp] (Peyrard et al., 2017), CIDEr (Vedantam et al., 2015), Chrfpp (Popovic, 2017), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020). Some metrics are only available for some task; e.g., CIDEr, METEOR are only available for the image captioning task. We provide details about datasets, metrics, and their statistics in Appendix A.3. Overall, across datasets and metrics we have 296 evaluation setups, 73,471 pairs of systems, and 91,197 test instances. We also experiment with sub-sampling different sizes of test sets (see Appendix A.3) to simulate varying train/dev/test splits or cross-validation. Comparison of BT, MEAN, and M"
2021.acl-long.179,D11-1043,0,0.0871879,"Missing"
2021.emnlp-main.701,Q19-1038,0,0.0246136,"between human reference and MT hypothesis using so-called ‘direct assessment’ (DA) scores which are framed in terms of one sentence • LaBSE (Feng et al., 2020) is a dual-encoder ‘adequately expressing the meaning’ of another. We framework. The encoders re-use pre-trained use these ratings as semantic scores in our setup. BERT and finetune it with Masked Language For the reference-based case, we use the hypotheModeling and Translation Language modeling ses and the references as sentence pairs. This data on monolingual and parallel data. is collected over multiple language pairs which • LASER (Artetxe and Schwenk, 2019) is a BiL- have English as target language (so both the huSTM encoder trained on parallel corpora. It pro- man reference and the hypothesis are in English) duces language-agnostic representations. The of WMT15-WMT17. For the reference-free sceencoder-decoder architecture is trained jointly on nario, we pair the source texts with MT hypotheses different languages. and use the corresponding reference-to-hypothesis • XMoverScore (Zhao et al., 2020) extends Mover- DA scores for similarity score. For German, we Score to operate in the cross-lingual setup, and take the data from WMT15 (Bojar et al.,"
2021.emnlp-main.701,Q17-1010,0,0.0772728,"Missing"
2021.emnlp-main.701,E06-1032,0,0.176285,"the modern evalumachine translation and summarization, BLEU and ation metrics sketched above rely on at least two ROUGE (Papineni et al., 2002a; Lin, 2004), have factors: BERT (or its variants) and different agmeasured lexical n-gram overlap between system gregation schemes, such as Earth Mover Distance prediction and a human reference. While simple (Kusner et al., 2015; Zhao et al., 2019) or greedy and easy to understand, early on, limitations of alignment (Zhang et al., 2020), on top of BERT. such lexical overlap metrics have been recognized Understanding BERT alone is thus not sufficient (Callison-Burch et al., 2006), e.g., in that they can for explaining BERT-based evaluation metrics. only measure surface level similarity, and they are especially inadequate when it comes to assessing Here, we present a simple global explanation current high-quality text generation systems (Rei technique of BERT-based evaluation metrics which et al., 2020; Mathur et al., 2020; Marie et al., 2021). disentangles them on prominent linguistic factors, Recently, a class of novel evaluation metrics viz., syntax, semantics, morphology and lexical based on BERT and its variants has been explored overlap. We find that all metrics"
2021.emnlp-main.701,W04-1013,0,0.0487798,"al. (2020) for an Evaluation metrics are a key ingredient in assess- overview), e.g., via probing (Tenney et al., 2019), analyzes by Hewitt and Liang (2019); Eger et al. ing the quality of text generation systems, be it machine translation, summarization, or conversa- (2020); Ravichander et al. (2021) indicate that probing results (based on supervision) are not always tional AI models. Traditional evaluation metrics in trustworthy. More importantly, the modern evalumachine translation and summarization, BLEU and ation metrics sketched above rely on at least two ROUGE (Papineni et al., 2002a; Lin, 2004), have factors: BERT (or its variants) and different agmeasured lexical n-gram overlap between system gregation schemes, such as Earth Mover Distance prediction and a human reference. While simple (Kusner et al., 2015; Zhao et al., 2019) or greedy and easy to understand, early on, limitations of alignment (Zhang et al., 2020), on top of BERT. such lexical overlap metrics have been recognized Understanding BERT alone is thus not sufficient (Callison-Burch et al., 2006), e.g., in that they can for explaining BERT-based evaluation metrics. only measure surface level similarity, and they are espec"
2021.emnlp-main.701,W19-5358,0,0.0216663,", the majority of which is based on BERT and similar high-quality text representations. They can be differentiated along two dimensions: (i) the input arguments they take, and (ii) whether they are supervised or unsupervised. Referencebased metrics compare human references to system predictions. Popular metrics are BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019), and BLEURT (Sellam et al., 2020). In contrast, reference-free metrics directly compare source texts to system predictions, thus they are more resource-lean. Popular examples are XMoverScore (Zhao et al., 2020), Yisi-2 (Lo, 2019), KoBE (Gekhman et al., 2020), and SentSim (Song et al., 2021). Rei et al. (2020) use all three information signals: source text, hypotheses and human references. There are also reference-free metrics outside the field of machine translation; for example, SUPERT (Gao et al., 2020) for summarization. Supervised metrics train on human sentence-level scores, e.g., Direct Assessment (DA) scores or postediting effort (HTER) for MT. These metrics in- 3 Our approach clude BLEURT and COMET (Rei et al., 2020). In MT, most metrics from the so-called ‘Quality Esti- In our scenario, we consider different"
2021.emnlp-main.701,2021.acl-long.566,0,0.0211147,"et al., 2019) or greedy and easy to understand, early on, limitations of alignment (Zhang et al., 2020), on top of BERT. such lexical overlap metrics have been recognized Understanding BERT alone is thus not sufficient (Callison-Burch et al., 2006), e.g., in that they can for explaining BERT-based evaluation metrics. only measure surface level similarity, and they are especially inadequate when it comes to assessing Here, we present a simple global explanation current high-quality text generation systems (Rei technique of BERT-based evaluation metrics which et al., 2020; Mathur et al., 2020; Marie et al., 2021). disentangles them on prominent linguistic factors, Recently, a class of novel evaluation metrics viz., syntax, semantics, morphology and lexical based on BERT and its variants has been explored overlap. We find that all metrics capture these that correlates much better with human assessments linguistic aspects to certain (but differing) degrees, 8912 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8912–8925 c November 7–11, 2021. 2021 Association for Computational Linguistics and they are particularly sensitive to lexical overlap, which makes the"
2021.emnlp-main.701,2020.acl-main.448,0,0.0263676,"er et al., 2015; Zhao et al., 2019) or greedy and easy to understand, early on, limitations of alignment (Zhang et al., 2020), on top of BERT. such lexical overlap metrics have been recognized Understanding BERT alone is thus not sufficient (Callison-Burch et al., 2006), e.g., in that they can for explaining BERT-based evaluation metrics. only measure surface level similarity, and they are especially inadequate when it comes to assessing Here, we present a simple global explanation current high-quality text generation systems (Rei technique of BERT-based evaluation metrics which et al., 2020; Mathur et al., 2020; Marie et al., 2021). disentangles them on prominent linguistic factors, Recently, a class of novel evaluation metrics viz., syntax, semantics, morphology and lexical based on BERT and its variants has been explored overlap. We find that all metrics capture these that correlates much better with human assessments linguistic aspects to certain (but differing) degrees, 8912 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8912–8925 c November 7–11, 2021. 2021 Association for Computational Linguistics and they are particularly sensitive to lexical ove"
2021.emnlp-main.701,2020.lrec-1.497,0,0.022932,"Missing"
2021.emnlp-main.701,P02-1040,0,0.125784,"ding BERT (see Rogers et al. (2020) for an Evaluation metrics are a key ingredient in assess- overview), e.g., via probing (Tenney et al., 2019), analyzes by Hewitt and Liang (2019); Eger et al. ing the quality of text generation systems, be it machine translation, summarization, or conversa- (2020); Ravichander et al. (2021) indicate that probing results (based on supervision) are not always tional AI models. Traditional evaluation metrics in trustworthy. More importantly, the modern evalumachine translation and summarization, BLEU and ation metrics sketched above rely on at least two ROUGE (Papineni et al., 2002a; Lin, 2004), have factors: BERT (or its variants) and different agmeasured lexical n-gram overlap between system gregation schemes, such as Earth Mover Distance prediction and a human reference. While simple (Kusner et al., 2015; Zhao et al., 2019) or greedy and easy to understand, early on, limitations of alignment (Zhang et al., 2020), on top of BERT. such lexical overlap metrics have been recognized Understanding BERT alone is thus not sufficient (Callison-Burch et al., 2006), e.g., in that they can for explaining BERT-based evaluation metrics. only measure surface level similarity, and t"
2021.emnlp-main.701,2020.wmt-1.122,0,0.499279,"Missing"
2021.emnlp-main.701,2021.eacl-main.295,0,0.195414,"ROUGE. This exposes limitations of these novelly proposed metrics, which we gap’ and introspect linguistic properties encoded in also highlight in an adversarial test scenario. BERT-based evaluation metrics. Although there is already considerable work on introspecting and un1 Introduction derstanding BERT (see Rogers et al. (2020) for an Evaluation metrics are a key ingredient in assess- overview), e.g., via probing (Tenney et al., 2019), analyzes by Hewitt and Liang (2019); Eger et al. ing the quality of text generation systems, be it machine translation, summarization, or conversa- (2020); Ravichander et al. (2021) indicate that probing results (based on supervision) are not always tional AI models. Traditional evaluation metrics in trustworthy. More importantly, the modern evalumachine translation and summarization, BLEU and ation metrics sketched above rely on at least two ROUGE (Papineni et al., 2002a; Lin, 2004), have factors: BERT (or its variants) and different agmeasured lexical n-gram overlap between system gregation schemes, such as Earth Mover Distance prediction and a human reference. While simple (Kusner et al., 2015; Zhao et al., 2019) or greedy and easy to understand, early on, limitations"
2021.emnlp-main.701,2020.emnlp-main.213,0,0.0276991,"presentations. They can be differentiated along two dimensions: (i) the input arguments they take, and (ii) whether they are supervised or unsupervised. Referencebased metrics compare human references to system predictions. Popular metrics are BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019), and BLEURT (Sellam et al., 2020). In contrast, reference-free metrics directly compare source texts to system predictions, thus they are more resource-lean. Popular examples are XMoverScore (Zhao et al., 2020), Yisi-2 (Lo, 2019), KoBE (Gekhman et al., 2020), and SentSim (Song et al., 2021). Rei et al. (2020) use all three information signals: source text, hypotheses and human references. There are also reference-free metrics outside the field of machine translation; for example, SUPERT (Gao et al., 2020) for summarization. Supervised metrics train on human sentence-level scores, e.g., Direct Assessment (DA) scores or postediting effort (HTER) for MT. These metrics in- 3 Our approach clude BLEURT and COMET (Rei et al., 2020). In MT, most metrics from the so-called ‘Quality Esti- In our scenario, we consider different metrics m mation’ (QE) tasks are also supervised reference- taking two arguments"
2021.emnlp-main.701,D19-1410,0,0.0648277,"Missing"
2021.emnlp-main.701,2020.emnlp-main.365,0,0.0490981,"Missing"
2021.emnlp-main.701,N16-3020,0,0.237656,"ghe et al., 2020) and BERGAMOT-LATTE (Fomicheva et al., 2020b). Unsupervised metrics require no such supervisory signal (e.g., MoverScore, BERTScore, XMoverScore, SentSim). Model introspection There has been a recent surge in interest in explaining deep learning models. The techniques for explainability differ in whether they provide justification or information for model outputs on individual instances (local explainability) or focus on a model as a whole and disclose its internal structure (global explainability) (Danilevsky et al., 2020). Popular examples for local explainability are LIME (Ribeiro et al., 2016) and SHAP (Lundberg and Lee, 2017) that find features from the input (such as particular words) relevant to model outputs. Concerning (global) interpretability of text representations, previous works (Adi et al., 2017; Conneau et al., 2018) introspect the properties encoded in vector representations through probing classifiers—trained on external data to perform a certain linguistic task, such as inducing the dependency tree depth from a text representation (of a sentence). Tenney et al. (2019) extend this idea by inspecting BERT representations layer-by-layer, and find that BERT captures more"
2021.emnlp-main.701,2020.tacl-1.54,0,0.0280963,"different metrics capture all asis sensitive to lexical overlap, it can be fooled by pects to some degree, but that they are all subusing the same words but in different order. stantially sensitive to lexical overlap, just like In this work, we fill the existing ‘explainability BLEU and ROUGE. This exposes limitations of these novelly proposed metrics, which we gap’ and introspect linguistic properties encoded in also highlight in an adversarial test scenario. BERT-based evaluation metrics. Although there is already considerable work on introspecting and un1 Introduction derstanding BERT (see Rogers et al. (2020) for an Evaluation metrics are a key ingredient in assess- overview), e.g., via probing (Tenney et al., 2019), analyzes by Hewitt and Liang (2019); Eger et al. ing the quality of text generation systems, be it machine translation, summarization, or conversa- (2020); Ravichander et al. (2021) indicate that probing results (based on supervision) are not always tional AI models. Traditional evaluation metrics in trustworthy. More importantly, the modern evalumachine translation and summarization, BLEU and ation metrics sketched above rely on at least two ROUGE (Papineni et al., 2002a; Lin, 2004),"
2021.emnlp-main.701,P19-1452,0,0.168903,"t that they are all subusing the same words but in different order. stantially sensitive to lexical overlap, just like In this work, we fill the existing ‘explainability BLEU and ROUGE. This exposes limitations of these novelly proposed metrics, which we gap’ and introspect linguistic properties encoded in also highlight in an adversarial test scenario. BERT-based evaluation metrics. Although there is already considerable work on introspecting and un1 Introduction derstanding BERT (see Rogers et al. (2020) for an Evaluation metrics are a key ingredient in assess- overview), e.g., via probing (Tenney et al., 2019), analyzes by Hewitt and Liang (2019); Eger et al. ing the quality of text generation systems, be it machine translation, summarization, or conversa- (2020); Ravichander et al. (2021) indicate that probing results (based on supervision) are not always tional AI models. Traditional evaluation metrics in trustworthy. More importantly, the modern evalumachine translation and summarization, BLEU and ation metrics sketched above rely on at least two ROUGE (Papineni et al., 2002a; Lin, 2004), have factors: BERT (or its variants) and different agmeasured lexical n-gram overlap between system gregatio"
2021.emnlp-main.701,N19-1131,0,0.025126,"hrase with high lexical overlap. A good metric m would have m(A, B) &gt; m(A, C) but the high lexical overlap between A and C makes this task difficult. C. Since Freitag et al. (2020) provided sentences in German, we translate all sentences into English using Google translate. We note that by inspection the translations are generally of high quality and satisfy our constraints of inducing paraphrases with low lexical overlap and non-paraphrases with high lexical overlap. Examples are shown in Table 7 and statistics in Table 6. PAWS We complement the analysis with the native English PAWS dataset (Zhang et al., 2019) which consists of paraphrase and non-paraphrase pairs that have high lexical overlap. For each sentence in the dataset, there are multiple paraphrases Freitag et al. Sentences A are taken as source sentences from WMT19. Freitag et al. (2020) and non-paraphrases. For a given sentence A, we provided alternative references for WMT19; they use the paraphrase with the smallest amount of lexinstructed human professional translators to para- ical overlap as sentence B and sentence C is the phrase the references as much as possible in terms non-paraphrase with the highest amount of lexical of lexical"
2021.emnlp-main.701,2020.acl-main.151,1,0.836018,"Missing"
2021.emnlp-main.701,D19-1053,1,0.940223,"translation, summarization, or conversa- (2020); Ravichander et al. (2021) indicate that probing results (based on supervision) are not always tional AI models. Traditional evaluation metrics in trustworthy. More importantly, the modern evalumachine translation and summarization, BLEU and ation metrics sketched above rely on at least two ROUGE (Papineni et al., 2002a; Lin, 2004), have factors: BERT (or its variants) and different agmeasured lexical n-gram overlap between system gregation schemes, such as Earth Mover Distance prediction and a human reference. While simple (Kusner et al., 2015; Zhao et al., 2019) or greedy and easy to understand, early on, limitations of alignment (Zhang et al., 2020), on top of BERT. such lexical overlap metrics have been recognized Understanding BERT alone is thus not sufficient (Callison-Burch et al., 2006), e.g., in that they can for explaining BERT-based evaluation metrics. only measure surface level similarity, and they are especially inadequate when it comes to assessing Here, we present a simple global explanation current high-quality text generation systems (Rei technique of BERT-based evaluation metrics which et al., 2020; Mathur et al., 2020; Marie et al.,"
2021.emnlp-main.701,2020.acl-main.704,0,0.0895323,"s specific metrics use, in general, and may expose their limitations. Evaluation metrics for Natural Language Generation In the last few years, several strong performing evaluation metrics have been proposed, the majority of which is based on BERT and similar high-quality text representations. They can be differentiated along two dimensions: (i) the input arguments they take, and (ii) whether they are supervised or unsupervised. Referencebased metrics compare human references to system predictions. Popular metrics are BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019), and BLEURT (Sellam et al., 2020). In contrast, reference-free metrics directly compare source texts to system predictions, thus they are more resource-lean. Popular examples are XMoverScore (Zhao et al., 2020), Yisi-2 (Lo, 2019), KoBE (Gekhman et al., 2020), and SentSim (Song et al., 2021). Rei et al. (2020) use all three information signals: source text, hypotheses and human references. There are also reference-free metrics outside the field of machine translation; for example, SUPERT (Gao et al., 2020) for summarization. Supervised metrics train on human sentence-level scores, e.g., Direct Assessment (DA) scores or postedi"
2021.eval4nlp-1.17,2020.emnlp-main.263,0,0.0148411,"ntence-level training data and strong sentencelevel models (see the TransQuest-LIME baseline in Section 4), and thus encourage the participants to either train their own inherently interpretable models or use post-hoc techniques on top of our existing sentence-level models. We accommodate the evaluation scheme to be suitable both for approaches that return continuous scores, and for supervised approaches that can return binary scores. Namely, we use evaluation metrics based on class probabilities that have been previously adapted for assessing the plausibility of rationale extraction methods (Atanasova et al., 2020). Since explainability methods typically proceed on instance-by-instance basis, and the scores produced for different instances are not necessarily comparable, we compute the evaluation metrics for each instance separately and average the results across all instances in the test set. Following Fomicheva et al. (2021), we define the following evaluation metrics to assess the performance of the submissions to the shared task at the word-level: 7 We did not ask the participants to normalize the scores, as we are only interested in the ranking of tokens according to their relevance for sentence-le"
2021.eval4nlp-1.17,C04-1046,0,0.186049,"er, comes at the cost of efficiency and interpretability. Interpretability is a major concern in modern Artificial Intelligence (AI) and NLP research (DoshiVelez and Kim, 2017; Danilevsky et al., 2020), as black-box models undermine users’ trust in new technologies (Mercado et al., 2016; Toreini et al., 2020). In the Eval4NLP 2021 shared task, we focus on evaluating machine translation (MT) as an example of this problem. Specifically, we look at the task of quality estimation (QE), where the aim is to predict the quality of MT output at inference time without access to reference translations (Blatz et al., 2004; Specia et al., 2018b).1 Translation quality can be assessed at different levels of granularity: sentencelevel, i.e. predicting the overall quality of translated sentences, and word-level, i.e. highlighting specific errors in the MT output. Those have traditionally been treated as two separate tasks, each one requiring dedicated training data. In this shared task, we propose to address wordlevel translation error identification as an explainability task.2 Explainability is a broad area aimed at explaining predictions of machine learning models. Rationale extraction methods achieve this by sel"
2021.eval4nlp-1.17,2020.emnlp-main.121,0,0.0119932,"s XBERTScore (i.e., BERTScore (Zhang et al., 2020) with cross-lingual embeddings) and XMoverScore and make them inherently interpretable by considering the token alignments produced by the models. The intuition is that words that are not well-aligned are most likely erroneous. Specifically, they explore XBERTScore and XMoverScore as sentence-level models and use the corresponding similarity (or distance) matrices to produce token-level scores. CLIP-UMD propose an ensemble of two approaches: (1) the LIME explanation technique applied to the TransQuest sentence-level model; (2) Divergent mBERT (Briakou and Carpuat, 2020), which is a BERT-based model that can detect crosslingual semantic divergences. Divergent mBERT is trained using synthetic data where semantic divergences are introduced automatically following a set of pre-defined perturbations. To produce a 172 Approaches Overall, the submitted approaches vary a lot in the way they addressed the task. The following trends can be identified: • Following the recent standard in QE and similar multilingual NLP tasks, all the approaches rely on multilingual Transformer-based language models. • Submissions to the unconstrained track use the SOTA approach to word-"
2021.eval4nlp-1.17,2020.acl-main.747,0,0.0396815,"Missing"
2021.eval4nlp-1.17,2020.aacl-main.46,0,0.0312329,"k on explainable NLP evaluation metrics. Datasets and results are available at https://github. com/eval4nlp/SharedTask2021. 1 Introduction Recent Natural Language Processing (NLP) systems based on pre-trained representations from Transformer language models, such as BERT (Devlin et al., 2019) and XLM-Roberta (Conneau et al., 2020), have achieved outstanding results in a variety of tasks. This boost in performance, however, comes at the cost of efficiency and interpretability. Interpretability is a major concern in modern Artificial Intelligence (AI) and NLP research (DoshiVelez and Kim, 2017; Danilevsky et al., 2020), as black-box models undermine users’ trust in new technologies (Mercado et al., 2016; Toreini et al., 2020). In the Eval4NLP 2021 shared task, we focus on evaluating machine translation (MT) as an example of this problem. Specifically, we look at the task of quality estimation (QE), where the aim is to predict the quality of MT output at inference time without access to reference translations (Blatz et al., 2004; Specia et al., 2018b).1 Translation quality can be assessed at different levels of granularity: sentencelevel, i.e. predicting the overall quality of translated sentences, and word-"
2021.eval4nlp-1.17,S16-1082,0,0.0229924,"source) sentence explaining the score. We detailed the data annotation, involving two novel non-English language pairs, our baselines (post-hoc explanation techniques on top of state-of-the-art QE models), as well as the participants’ approaches to the task. These include supervised approaches, training on synthetic data as well as genuine post-hoc and inherent explainability techniques. The scope for future research is huge: for example, we aim to include new language pairs, especially low-resource ones, address explainability for metrics in other NLP tasks, e.g. semantic textual similarity (Agirre et al., 2016) and summarization (Gao et al., 2020), and identify error categories of highlighted words, ideally in an unsupervised manner. Acknowledgments Marina Fomicheva was supported by funding from the Bergamot project (EU H2020 Grant No. 825303). Piyawat Lertvittayakumjorn was supported by a scholarship from Anandamahidol Foundation. We would like to thank Lisa Yankovskaya and Mark Fishel from the University of Tartu for helping organize and monitor the manual quality annotation. We also thank Anton Malinovskiy for adapting the Appraise interface for quality annotation with rationales. Finally, we gra"
2021.eval4nlp-1.17,2021.eval4nlp-1.23,0,0.0438678,"Missing"
2021.eval4nlp-1.17,2020.acl-main.124,1,0.866691,"Missing"
2021.eval4nlp-1.17,D19-1632,0,0.040513,"Missing"
2021.eval4nlp-1.17,2020.acl-main.409,0,0.0140788,"ntencelevel, i.e. predicting the overall quality of translated sentences, and word-level, i.e. highlighting specific errors in the MT output. Those have traditionally been treated as two separate tasks, each one requiring dedicated training data. In this shared task, we propose to address wordlevel translation error identification as an explainability task.2 Explainability is a broad area aimed at explaining predictions of machine learning models. Rationale extraction methods achieve this by selecting a portion of the input that justifies model output for a given data point (Lei et al., 2016; Jain et al., 2020). A natural way to explain sentencelevel quality assessment is to identify translation errors. Hence, we frame error identification as a task of providing explanations for the predictions of sentence-level QE models. We claim that this task represents a challenging new benchmark for testing explainability for NLP and provides a new way of addressing word-level QE. On the one hand, QE is different from other explainable NLP tasks with existing datasets (DeYoung et al., 2020) in various important aspects. First, it is a regression task, as opposed to binary or multiclass text classification expl"
2021.eval4nlp-1.17,2021.eval4nlp-1.22,0,0.0722899,"Missing"
2021.eval4nlp-1.17,2021.emnlp-main.701,1,0.75862,"Missing"
2021.eval4nlp-1.17,2020.emnlp-main.574,0,0.013912,"nded. used a set of explainability methods to extract the relevance of the input tokens from sentencelevel QE models built on top of XLM-Roberta and RemBERT. The explainability methods explored in this work include attention-based, gradient-based and perturbation based approaches, as well as rationalization by construction. The best performing method which was submitted to the competition relies on the attention mechanism of the pretrained Transformers in order to obtain the relevance scores for each token. In addition, scaling attention weights by the L2 norm of value vectors as suggested in Kobayashi et al. (2020) resulted in a further boost in performance. For the unconstrained track (”IST-Unbabel*” in Table 6), they add a word-level loss to the sentencelevel models and train jointly using the annotated data from the MLQE-PE dataset. HeyTUDa use the TransQuest QE models (Ranasinghe et al., 2020) for sentence-level prediction and a set of explainability techniques to estimate the relevance of each source and target word. Specifically, they explore three perturbation-based methods: LIME, SHAP, and occlusion (Zeiler and Fergus, 2014), as well as three gradient-based methods: DeepLift (Shrikumar et al., 2"
2021.eval4nlp-1.17,2020.wmt-1.118,0,0.31159,"tect crosslingual semantic divergences. Divergent mBERT is trained using synthetic data where semantic divergences are introduced automatically following a set of pre-defined perturbations. To produce a 172 Approaches Overall, the submitted approaches vary a lot in the way they addressed the task. The following trends can be identified: • Following the recent standard in QE and similar multilingual NLP tasks, all the approaches rely on multilingual Transformer-based language models. • Submissions to the unconstrained track use the SOTA approach to word-level supervision explored previously by Lee (2020). • The use of synthetic data produced by aligning MT outputs and reference translations from existing parallel corpora proves an efficient strategy to identify translation errors. Supervising the predictions based on Transformer attention weights with the labels derived from synthetic data was used by the winning submission to the shared task. • The approaches that rely on attention weights to predict human rationales (NICT-Kyoto and IST-Unbabel) achieve the best results for the constrained track. 11 This ranking is slightly different from the Codalab results, as one of the teams retracted fr"
2021.eval4nlp-1.17,D16-1011,0,0.0273503,"Missing"
2021.eval4nlp-1.17,2021.eval4nlp-1.16,0,0.0238686,"Missing"
2021.eval4nlp-1.17,2021.eval4nlp-1.15,0,0.0373775,"Missing"
2021.eval4nlp-1.17,N19-4009,0,0.0249542,"Missing"
2021.eval4nlp-1.17,P02-1040,0,0.113717,"andard higher scores mean better quality, we invert LIME explanations so that higher values correspond to errors. Participants For this first edition of the shared task, we had a total of 6 participating teams listed in Table 5.10 Below, we briefly describe the submitted approaches. NICT-Kyoto use synthetic data to fine-tune the XLM-Roberta language model for the QE task. To produce synthetic sentence-level scores, they translate publicly available parallel corpora using SOTA neural MT systems and compute three referencebased metrics: ChrF (Popovi´c, 2015), TER (Snover et al., 2006) and BLEU (Papineni et al., 2002). To simulate word-level annotation, they derive wordlevel labels from the alignment between the MT outputs and human reference translations. The QE model is then jointly trained to predict the scores from different metrics as well as word-level tags. A metric embedding component is proposed where each metric is represented with a set of learnable parameters. An attention mechanism between the metric embeddings and the input representations is employed to obtain word-level scores as explanations for the sentence-level predictions. XMover-SHAP uses the reference-free metric XMoverScore (Zhao et"
2021.eval4nlp-1.17,2006.amta-papers.25,0,0.157669,"ovided. of the source sentence, as well as to typological differences between languages, whereby there tends to be a one-to-many correspondence between the source and target words. Difference to existing QE datasets with wordlevel annotation The test set collected for this shared task is different from existing QE datasets with word-level annotation. A popular approach to building QE datasets is based on measuring post-editing effort (Bojar et al., 2017; Specia et al., 2018a; Fonseca et al., 2019; Specia et al., 2020). This can be done at sentence level, by computing the so called HTER score (Snover et al., 2006) that represents the minimum number of edits a human language expert is required to make in order to correct the MT output; or at word level, by aligning the MT output to its post-edited version and annotating the misaligned source and target words. An important limitation of this strategy is that the annotated words do not necessarily correspond to translation errors, as correcting a specific error may involve changing multiple related words in the sentence. This is exacerbated by the limitations of the heuristics used to automatically align the MT and its post-edited version. Indeed, as show"
2021.eval4nlp-1.17,2021.acl-long.179,1,0.703257,"el scores, with many high-quality translations, according to the annotators (see Section 2.1). Limitations of the evaluation settings Our current evaluation settings can be further improved in various ways. First, the submissions were ranked according to the global statistics, i.e. by comparing the mean AUC, AP and Rec-TopK scores of different submissions over a common set of test instances. However, such aggregation mechanisms ignore how many of its competitors a given submission outperforms and on how many test instances. In the future we plan to follow a more rigorous approach suggested by Peyrard et al. (2021) and use the Bradley–Terry (BT) model (Bradley and Terry, 1952), which leverages the instance-level pairing of metric scores. Second, the metrics used for evaluation are tailored for unsupervised explainability approaches that produce continuous scores, but they do not allow a direct comparison with the SOTA work on word-level QE, which is evaluated using F-score and Matthews correlation coefficient (Specia et al., 2020). One way to address this would be to require the participants to submit binary scores, but we discarded this option in this first edition of the shared task, as it would subst"
2021.eval4nlp-1.17,2021.eval4nlp-1.24,0,0.043359,"Missing"
2021.eval4nlp-1.17,W15-3049,0,0.0531757,"Missing"
2021.eval4nlp-1.17,2020.wmt-1.122,0,0.292685,"s well as for the sentence-level QE score. of its semantic overlapping with the source sentence, using re-mapped multilingual BERT embeddings and a target-side language model.9 To explain the contribution of each word in the rating, SHAP creates perturbations of the source/translation sentence by masking out some words and estimates the average marginal contribution of each word across all possible perturbations. The source code for all the baseline systems is available at https://github.com/eval4nlp/ SharedTask2021/tree/main/baselines. 5 Transquest-LIME uses TransQuest QE models described in Ranasinghe et al. (2020) to produce sentence-level scores. TransQuest follows the current standard practice of building task-specific NLP models by fine-tuning pre-trained multilingual language models, such as XLM-Roberta, on taskspecific data. For Ro-En and Et-En, the Ro-En and Et-En TransQuest models are used, whereas for the zero-shot language pairs we use the multilingual variant of TransQuest, which was trained on a concatenation of MLQE-PE data. The post-hoc LIME explanation method (Ribeiro et al., 2016) is then applied to generate relevance scores for the source and target words. LIME is a simplification-based"
2021.eval4nlp-1.17,N16-3020,0,0.451651,"ch sentence pair in the form of continuous scores, with the highest scores corresponding to the tokens considered as relevant by human annotators. The participants could submit to either constrained or unconstrained track. For the constrained track, the participants were expected to use no supervision at word level, while in the unconstrained track they were allowed to use any word-level data for training. Explanations can be obtained either by building inherently interpretable models (Yu et al., 2019) or by using post-hoc explanation methods which extract explanations from an existing model (Ribeiro et al., 2016; Lundberg and Lee, 2017; Sundararajan et al., 2017a; Schulz et al., 2020), for example by analysing the values of the gradient on each input feature. In this shared task, we provide both sentence-level training data and strong sentencelevel models (see the TransQuest-LIME baseline in Section 4), and thus encourage the participants to either train their own inherently interpretable models or use post-hoc techniques on top of our existing sentence-level models. We accommodate the evaluation scheme to be suitable both for approaches that return continuous scores, and for supervised approaches th"
2021.eval4nlp-1.17,2021.naacl-main.252,0,0.271935,"Missing"
2021.eval4nlp-1.17,D19-1002,0,0.0122658,"p but noisy annotations derived from post-editing (Fomicheva et al., 2020) and very informative but expensive explicit error annotation based on error taxonomies, such as the Multidimensional Quality Metrics (MQM) framework (Lommel et al., 2014b). For this shared task, we build a new test set with manually annotated explanations for sentence-level quality ratings. To the best of our knowledge, this is the first MT evaluation dataset annotated with human rationales. The main objective of the shared task is threefold. First, it aims to explore the plausibility of explainable evaluation metrics (Wiegreffe and Pinter, 2019), by proposing a test set with manually annotated rationales. It helps the community better understand how similar the generated explanations are to the human explanations. Second, the shared task encourages research on unsupervised or semisupervised methods for error identification, so as to reduce the cost on word-level MT error annotation. Last but not least, the shared task sheds light on how current NLP evaluation systems arrive at their predictions and to what extent this process is aligned with human reasoning. 2 Data For this shared task, we collected a new test set with (i) manual ass"
2021.eval4nlp-1.17,D19-1420,0,0.0141688,"t data used for evaluation is shown in Table 3. The participants were expected to provide explanations for each sentence pair in the form of continuous scores, with the highest scores corresponding to the tokens considered as relevant by human annotators. The participants could submit to either constrained or unconstrained track. For the constrained track, the participants were expected to use no supervision at word level, while in the unconstrained track they were allowed to use any word-level data for training. Explanations can be obtained either by building inherently interpretable models (Yu et al., 2019) or by using post-hoc explanation methods which extract explanations from an existing model (Ribeiro et al., 2016; Lundberg and Lee, 2017; Sundararajan et al., 2017a; Schulz et al., 2020), for example by analysing the values of the gradient on each input feature. In this shared task, we provide both sentence-level training data and strong sentencelevel models (see the TransQuest-LIME baseline in Section 4), and thus encourage the participants to either train their own inherently interpretable models or use post-hoc techniques on top of our existing sentence-level models. We accommodate the eva"
2021.eval4nlp-1.17,2020.acl-main.151,1,0.609579,"Missing"
2021.findings-acl.141,P18-2006,0,0.10476,", which can then be shielded against. In the NLP community, typically two different kinds of attack scenarios are considered. “High-level” attacks paraphrase (semantically or syntactically) the input sentence (Iyyer et al., 2018; Alzantot et al., 2018; Jin et al., 2020) so that the classification label does not change, but the model changes its decision. Often, this is framed as a search problem where the attacker has at least access to model predictions (Zang et al., 2020). “Low-level” attackers operate on the level of characters and may consist of adversarial typos (Belinkov and Bisk, 2018; Ebrahimi et al., 2018a; Pruthi et al., 2019; Jones et al., 2020) or replacement of characters with similarly looking ones (Eger et al., 2019; Li et al., 2020a). Such attacks may also be successful when the attacker operates in a blind mode, without having access to model predictions, and they are arguably more realistic, e.g., in social media. However, Pruthi 1616 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1616–1629 August 1–6, 2021. ©2021 Association for Computational Linguistics et al. (2019) showed that orthographic attacks can be addressed by placing a spelling correction"
2021.findings-acl.141,D15-1139,1,0.80559,"Missing"
2021.findings-acl.141,2020.aacl-main.79,1,0.90601,"Abstract Adversarial attacks expose important blind spots of deep learning systems. While wordand sentence-level attack scenarios mostly deal with finding semantic paraphrases of the input that fool NLP models, character-level attacks typically insert typos into the input stream. It is commonly thought that these are easier to defend via spelling correction modules. In this work, we show that both a standard spellchecker and the approach of Pruthi et al. (2019), which trains to defend against insertions, deletions and swaps, perform poorly on the character-level benchmark recently proposed in Eger and Benz (2020) which includes more challenging attacks such as visual and phonetic perturbations and missing word segmentations. In contrast, we show that an untrained iterative approach which combines context-independent character-level information with context-dependent information from BERT’s masked language modeling can perform on par with human crowd-workers from Amazon Mechanical Turk (AMT) supervised via 3-shot learning. 1 Substring Levenshtein distance Hypothesis before BERT improvements 1. a buy is paying trumpet. 2. a buy is paying a trumpet. 3. abuyla paying trumpet. 4. abuyla paying a trumpet. B"
2021.findings-acl.141,N19-1165,1,0.901803,"Missing"
2021.findings-acl.141,2020.acl-main.245,0,0.0563282,"NLP community, typically two different kinds of attack scenarios are considered. “High-level” attacks paraphrase (semantically or syntactically) the input sentence (Iyyer et al., 2018; Alzantot et al., 2018; Jin et al., 2020) so that the classification label does not change, but the model changes its decision. Often, this is framed as a search problem where the attacker has at least access to model predictions (Zang et al., 2020). “Low-level” attackers operate on the level of characters and may consist of adversarial typos (Belinkov and Bisk, 2018; Ebrahimi et al., 2018a; Pruthi et al., 2019; Jones et al., 2020) or replacement of characters with similarly looking ones (Eger et al., 2019; Li et al., 2020a). Such attacks may also be successful when the attacker operates in a blind mode, without having access to model predictions, and they are arguably more realistic, e.g., in social media. However, Pruthi 1616 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1616–1629 August 1–6, 2021. ©2021 Association for Computational Linguistics et al. (2019) showed that orthographic attacks can be addressed by placing a spelling correction module in front of a downstream classifier"
2021.findings-acl.141,2020.emnlp-main.500,0,0.0829922,"acks paraphrase (semantically or syntactically) the input sentence (Iyyer et al., 2018; Alzantot et al., 2018; Jin et al., 2020) so that the classification label does not change, but the model changes its decision. Often, this is framed as a search problem where the attacker has at least access to model predictions (Zang et al., 2020). “Low-level” attackers operate on the level of characters and may consist of adversarial typos (Belinkov and Bisk, 2018; Ebrahimi et al., 2018a; Pruthi et al., 2019; Jones et al., 2020) or replacement of characters with similarly looking ones (Eger et al., 2019; Li et al., 2020a). Such attacks may also be successful when the attacker operates in a blind mode, without having access to model predictions, and they are arguably more realistic, e.g., in social media. However, Pruthi 1616 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1616–1629 August 1–6, 2021. ©2021 Association for Computational Linguistics et al. (2019) showed that orthographic attacks can be addressed by placing a spelling correction module in front of a downstream classifier, which may be considered a natural solution to the problem.1 In this work, we apply their ap"
2021.findings-acl.141,P19-1561,0,0.121953,"ge Learning Group Technische Universit¨at Darmstadt {yannik.keller,jan.mackensen}@stud.tu-darmstadt.de eger@aiphes.tu-darmstadt.de Aɠuyĩˢpɬayįng aṱrǜmₚèt. Abstract Adversarial attacks expose important blind spots of deep learning systems. While wordand sentence-level attack scenarios mostly deal with finding semantic paraphrases of the input that fool NLP models, character-level attacks typically insert typos into the input stream. It is commonly thought that these are easier to defend via spelling correction modules. In this work, we show that both a standard spellchecker and the approach of Pruthi et al. (2019), which trains to defend against insertions, deletions and swaps, perform poorly on the character-level benchmark recently proposed in Eger and Benz (2020) which includes more challenging attacks such as visual and phonetic perturbations and missing word segmentations. In contrast, we show that an untrained iterative approach which combines context-independent character-level information with context-dependent information from BERT’s masked language modeling can perform on par with human crowd-workers from Amazon Mechanical Turk (AMT) supervised via 3-shot learning. 1 Substring Levenshtein dis"
2021.findings-acl.141,P18-1079,0,0.0293072,"´eroe and (iii) demonstrate that our iterative approach, which we call BERT-Defense, sometimes even outperforms human crowd-workers trained via 3-shot learning. 2 Related work Zeng et al. (2020) classify adversarial attack scenarios in terms of the accessibility of the victim model to the attacker:2 white-box attackers (Ebrahimi et al., 2018b) have full access to the victim model including its gradient to construct adversarial examples. In contrast, black-box attackers have only limited knowledge of the victim models: score(Alzantot et al., 2018; Jin et al., 2020) and decisionbased attackers (Ribeiro et al., 2018) require access 1 One could argue that such a pipeline solution is not entirely satisfactory from a more theoretical perspective, and that downstream classifiers should be innately robust to attacks in the same way as humans. 2 Another recent survey of adversarial attacks in NLP is provided by Roth et al. (2021). Attacker Sentence inner-shuffle full-shuffle disemvowel intrude keyboard-typo natural-typo truncate segmentation phonetic visual A man is drnviig a car. A amn is ginvdir a acr. A mn is drvng a cr. A ma#n i*s driving a caˆr. A mwn is dricing a caf. A wan his driving as car. A man is dr"
2021.findings-acl.141,2020.acl-main.540,0,0.0262234,"achines into misclassification but not humans (Goodfellow et al., 2015). One of their goals is to expose blind-spots of deep learning models, which can then be shielded against. In the NLP community, typically two different kinds of attack scenarios are considered. “High-level” attacks paraphrase (semantically or syntactically) the input sentence (Iyyer et al., 2018; Alzantot et al., 2018; Jin et al., 2020) so that the classification label does not change, but the model changes its decision. Often, this is framed as a search problem where the attacker has at least access to model predictions (Zang et al., 2020). “Low-level” attackers operate on the level of characters and may consist of adversarial typos (Belinkov and Bisk, 2018; Ebrahimi et al., 2018a; Pruthi et al., 2019; Jones et al., 2020) or replacement of characters with similarly looking ones (Eger et al., 2019; Li et al., 2020a). Such attacks may also be successful when the attacker operates in a blind mode, without having access to model predictions, and they are arguably more realistic, e.g., in social media. However, Pruthi 1616 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1616–1629 August 1–6, 2021. ©"
2021.findings-acl.141,2021.emnlp-main.576,0,0.0359802,"Missing"
2021.findings-acl.141,W18-5446,0,0.0770914,"Missing"
2021.inlg-1.32,2020.inlg-1.24,0,0.0607126,"Missing"
2021.inlg-1.32,L18-1025,0,0.043941,"Missing"
2021.inlg-1.32,P13-1166,0,0.064792,"Missing"
2021.inlg-1.32,W18-6532,0,0.0836595,"Missing"
2021.inlg-1.32,J18-4003,0,0.033039,"Missing"
2021.latechclfl-1.7,N18-2011,0,0.0143572,"s were highly handengineered and ‘top-down’, using specific grammar formalisms and linguistic resources to generate poetry; they typically built stylistic aspects such as rhyme and rhythm into their model architectures (Manurung et al., 2000; Gervás, 2001; Gonçalo Oliveira, 2013; Colton et al., 2012). Neural approaches to poetry generation learn from collections of real poetry data. Most of them use RNN language models (Zhang and Lapata, 2014; Lau et al., 2018). This framework has also been extended to related tasks such as generating poetry from images (Liu et al., 2018), translating poetry (Ghazvininejad et al., 2018), and generating song lyrics given an input melody (Watanabe et al., 2018). A drawback shared by most poetry generation systems, including even very recent ones (Van de Cruys, 2020; Agarwal and Kann, 2020), is that they require hard- and hand-coded rules to generate poetry with specific properties, such as rhyming and alliteration, or to filter output not having these properties. For example, to ensure rhyming, Lau et al. (2018) exploit the fact that their data (sonnets) has a particular structure from which they can infer that certain word pairs must rhyme, which they incorporate in the model"
2021.latechclfl-1.7,2020.emnlp-main.94,0,0.0242595,"itectures (Manurung et al., 2000; Gervás, 2001; Gonçalo Oliveira, 2013; Colton et al., 2012). Neural approaches to poetry generation learn from collections of real poetry data. Most of them use RNN language models (Zhang and Lapata, 2014; Lau et al., 2018). This framework has also been extended to related tasks such as generating poetry from images (Liu et al., 2018), translating poetry (Ghazvininejad et al., 2018), and generating song lyrics given an input melody (Watanabe et al., 2018). A drawback shared by most poetry generation systems, including even very recent ones (Van de Cruys, 2020; Agarwal and Kann, 2020), is that they require hard- and hand-coded rules to generate poetry with specific properties, such as rhyming and alliteration, or to filter output not having these properties. For example, to ensure rhyming, Lau et al. (2018) exploit the fact that their data (sonnets) has a particular structure from which they can infer that certain word pairs must rhyme, which they incorporate in the modelling. Hämäläinen and Alnajjar (2019) define rules for style features. Agarwal and Kann (2020) tell the model when a rhyming word is required and then modify the prediction process. This means that the mode"
2021.latechclfl-1.7,C16-1074,0,0.0214948,"which they can infer that certain word pairs must rhyme, which they incorporate in the modelling. Hämäläinen and Alnajjar (2019) define rules for style features. Agarwal and Kann (2020) tell the model when a rhyming word is required and then modify the prediction process. This means that the models themselves lack the ability to discover elementary properties of poetry themselves, but instead rely on the modeller’s intention and ingenuity to do so. Related work Poetry analysis. Computational analysis of poetry has been concerned largely with formal features such as metre (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016), rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), and enjambment (Ruiz et al., 2017; Baumann et al., 2018). More recently, higher-level phenomena, including semantic coherence (Herbelot, 2015), metaphor (Reinig and Rehbein, 2019; Kesarwani et al., 2017), and diachronic analysis of tropes (Haider and Eger, 2019) have come into focus. Haider et al. (2020) annotate poetry for fine-grained aesthetic emotion categories elicited in readers. Poetry generation. In the era of statistical NLP, text generation has evolved along the path of ?-gram language models (Kneser and"
2021.latechclfl-1.7,P17-4008,0,0.0166291,"2), a metric borrowed from machine translation, for the task of continuing an initial line of poetry. They also report perplexity, a widely used metric for language model evaluation. For manual evaluation, they ask human annotators to rate the generated poems for fluency, coherence, meaning, and poeticness. Hopkins and Kiela (2017) and Lau et al. (2018) use a sort of Turing test where they ask human participants to distinguish between human- and computer-generated poems. Lau et al. (2018) additionally have an expert rate generated poetry with respect to metre, rhyme, readability, and emotion. Ghazvininejad et al. (2017) ask crowdRhyming: How much rhyming is present in a base unit (e.g., a stanza or full poem) of poetry? We measure the degree of rhyming by using a 2https://github.com/UKPLab/ emnlp2017-bilstm-cnn-crf 59 ?1 SOS EOS ?2 c ?1 c ?2 3.1 Training and testing mode In training mode, we extract sentiment, rhyme, and alliteration levels of poems (or stanzas) and pair them with the poems themselves. The model then predicts the text of poems (next words) based on the previous text and the context vector c. The model is end to end and has no built-in understanding of rhyme, sentiment, nor alliteration, exce"
2021.latechclfl-1.7,C18-1164,0,0.012831,"s for style features. Agarwal and Kann (2020) tell the model when a rhyming word is required and then modify the prediction process. This means that the models themselves lack the ability to discover elementary properties of poetry themselves, but instead rely on the modeller’s intention and ingenuity to do so. Related work Poetry analysis. Computational analysis of poetry has been concerned largely with formal features such as metre (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016), rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), and enjambment (Ruiz et al., 2017; Baumann et al., 2018). More recently, higher-level phenomena, including semantic coherence (Herbelot, 2015), metaphor (Reinig and Rehbein, 2019; Kesarwani et al., 2017), and diachronic analysis of tropes (Haider and Eger, 2019) have come into focus. Haider et al. (2020) annotate poetry for fine-grained aesthetic emotion categories elicited in readers. Poetry generation. In the era of statistical NLP, text generation has evolved along the path of ?-gram language models (Kneser and Ney, 1995), whose inherent modelling limitations allow them to consider only a finite amount of history when generating next output symb"
2021.latechclfl-1.7,2021.mrl-1.1,0,0.0857988,"Missing"
2021.latechclfl-1.7,D10-1051,0,0.0250438,"cular structure from which they can infer that certain word pairs must rhyme, which they incorporate in the modelling. Hämäläinen and Alnajjar (2019) define rules for style features. Agarwal and Kann (2020) tell the model when a rhyming word is required and then modify the prediction process. This means that the models themselves lack the ability to discover elementary properties of poetry themselves, but instead rely on the modeller’s intention and ingenuity to do so. Related work Poetry analysis. Computational analysis of poetry has been concerned largely with formal features such as metre (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016), rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), and enjambment (Ruiz et al., 2017; Baumann et al., 2018). More recently, higher-level phenomena, including semantic coherence (Herbelot, 2015), metaphor (Reinig and Rehbein, 2019; Kesarwani et al., 2017), and diachronic analysis of tropes (Haider and Eger, 2019) have come into focus. Haider et al. (2020) annotate poetry for fine-grained aesthetic emotion categories elicited in readers. Poetry generation. In the era of statistical NLP, text generation has evolved along the path of ?-gram l"
2021.latechclfl-1.7,W19-4727,1,0.829361,"ntary properties of poetry themselves, but instead rely on the modeller’s intention and ingenuity to do so. Related work Poetry analysis. Computational analysis of poetry has been concerned largely with formal features such as metre (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016), rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), and enjambment (Ruiz et al., 2017; Baumann et al., 2018). More recently, higher-level phenomena, including semantic coherence (Herbelot, 2015), metaphor (Reinig and Rehbein, 2019; Kesarwani et al., 2017), and diachronic analysis of tropes (Haider and Eger, 2019) have come into focus. Haider et al. (2020) annotate poetry for fine-grained aesthetic emotion categories elicited in readers. Poetry generation. In the era of statistical NLP, text generation has evolved along the path of ?-gram language models (Kneser and Ney, 1995), whose inherent modelling limitations allow them to consider only a finite amount of history when generating next output symbols, to recurrent neural network (RNN) language models (Sutskever et al., 2011), which can take an infinite history into account but suffer from vanishing and exploding gradients, to Recently, Jhamtani et a"
2021.latechclfl-1.7,2020.lrec-1.205,1,0.827201,"stead rely on the modeller’s intention and ingenuity to do so. Related work Poetry analysis. Computational analysis of poetry has been concerned largely with formal features such as metre (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016), rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), and enjambment (Ruiz et al., 2017; Baumann et al., 2018). More recently, higher-level phenomena, including semantic coherence (Herbelot, 2015), metaphor (Reinig and Rehbein, 2019; Kesarwani et al., 2017), and diachronic analysis of tropes (Haider and Eger, 2019) have come into focus. Haider et al. (2020) annotate poetry for fine-grained aesthetic emotion categories elicited in readers. Poetry generation. In the era of statistical NLP, text generation has evolved along the path of ?-gram language models (Kneser and Ney, 1995), whose inherent modelling limitations allow them to consider only a finite amount of history when generating next output symbols, to recurrent neural network (RNN) language models (Sutskever et al., 2011), which can take an infinite history into account but suffer from vanishing and exploding gradients, to Recently, Jhamtani et al. (2019) introduced an intriguing approach"
2021.latechclfl-1.7,W18-4509,1,0.911351,"n the modelling. Hämäläinen and Alnajjar (2019) define rules for style features. Agarwal and Kann (2020) tell the model when a rhyming word is required and then modify the prediction process. This means that the models themselves lack the ability to discover elementary properties of poetry themselves, but instead rely on the modeller’s intention and ingenuity to do so. Related work Poetry analysis. Computational analysis of poetry has been concerned largely with formal features such as metre (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016), rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), and enjambment (Ruiz et al., 2017; Baumann et al., 2018). More recently, higher-level phenomena, including semantic coherence (Herbelot, 2015), metaphor (Reinig and Rehbein, 2019; Kesarwani et al., 2017), and diachronic analysis of tropes (Haider and Eger, 2019) have come into focus. Haider et al. (2020) annotate poetry for fine-grained aesthetic emotion categories elicited in readers. Poetry generation. In the era of statistical NLP, text generation has evolved along the path of ?-gram language models (Kneser and Ney, 1995), whose inherent modelling limitations allow them to consider only a"
2021.latechclfl-1.7,W19-8637,0,0.0273222,"generating song lyrics given an input melody (Watanabe et al., 2018). A drawback shared by most poetry generation systems, including even very recent ones (Van de Cruys, 2020; Agarwal and Kann, 2020), is that they require hard- and hand-coded rules to generate poetry with specific properties, such as rhyming and alliteration, or to filter output not having these properties. For example, to ensure rhyming, Lau et al. (2018) exploit the fact that their data (sonnets) has a particular structure from which they can infer that certain word pairs must rhyme, which they incorporate in the modelling. Hämäläinen and Alnajjar (2019) define rules for style features. Agarwal and Kann (2020) tell the model when a rhyming word is required and then modify the prediction process. This means that the models themselves lack the ability to discover elementary properties of poetry themselves, but instead rely on the modeller’s intention and ingenuity to do so. Related work Poetry analysis. Computational analysis of poetry has been concerned largely with formal features such as metre (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016), rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), and enjambment (Ruiz et"
2021.latechclfl-1.7,W16-0201,0,0.0222026,"certain word pairs must rhyme, which they incorporate in the modelling. Hämäläinen and Alnajjar (2019) define rules for style features. Agarwal and Kann (2020) tell the model when a rhyming word is required and then modify the prediction process. This means that the models themselves lack the ability to discover elementary properties of poetry themselves, but instead rely on the modeller’s intention and ingenuity to do so. Related work Poetry analysis. Computational analysis of poetry has been concerned largely with formal features such as metre (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016), rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), and enjambment (Ruiz et al., 2017; Baumann et al., 2018). More recently, higher-level phenomena, including semantic coherence (Herbelot, 2015), metaphor (Reinig and Rehbein, 2019; Kesarwani et al., 2017), and diachronic analysis of tropes (Haider and Eger, 2019) have come into focus. Haider et al. (2020) annotate poetry for fine-grained aesthetic emotion categories elicited in readers. Poetry generation. In the era of statistical NLP, text generation has evolved along the path of ?-gram language models (Kneser and Ney, 1995), whose inher"
2021.latechclfl-1.7,W17-4912,0,0.0286069,"es and the discriminator aims at distinguishing generated poetry from human-authored poetry. The authors show that this competition leads to models learning rhyming with higher probability. While this model comes close to our envisioned ideal of a poetry generation system learning from examples alone, it still has the drawback that the modellers need to make the model aware of what to pay attention to (i.e., character information of ending words). This is unsatisfactory given the large space of possible features to consider in generated poetry. Formally similar approaches to ours are those of Ficler and Goldberg (2017) and Manjavacas et al. (2019) (without emphasizing the ‘examples-only’ aspect, however). Ficler and Goldberg (2017) conditioned RNN language models on stylistic features such as length and whether the text appears to be professionally written, and on sentiment and theme. They showed this approach to be successful for the given conditions in the movie reviews domain, though they did not deal with the much more challenging case of poetry. Manjavacas et al. (2019) generate hip-hop lyrics conditional upon rhythm and rhyme. To enforce rhyming, they feed phonetic rhyme representations to the model,"
2021.latechclfl-1.7,P17-1016,0,0.0219937,"d Gurevych (2017).2 For the conditions ?, we consider the following: Poetry evaluation Evaluation of generated poetry is usually done either automatically or manually. Zhang and Lapata (2014) compare generated lines to manually produced ‘gold-standard’ lines using BLEU (Papineni et al., 2002), a metric borrowed from machine translation, for the task of continuing an initial line of poetry. They also report perplexity, a widely used metric for language model evaluation. For manual evaluation, they ask human annotators to rate the generated poems for fluency, coherence, meaning, and poeticness. Hopkins and Kiela (2017) and Lau et al. (2018) use a sort of Turing test where they ask human participants to distinguish between human- and computer-generated poems. Lau et al. (2018) additionally have an expert rate generated poetry with respect to metre, rhyme, readability, and emotion. Ghazvininejad et al. (2017) ask crowdRhyming: How much rhyming is present in a base unit (e.g., a stanza or full poem) of poetry? We measure the degree of rhyming by using a 2https://github.com/UKPLab/ emnlp2017-bilstm-cnn-crf 59 ?1 SOS EOS ?2 c ?1 c ?2 3.1 Training and testing mode In training mode, we extract sentiment, rhyme, an"
2021.latechclfl-1.7,D13-1181,0,0.0308828,"th century. A crucial factor when producing poetry (either by humans or automatically) is style. While definitions of style have varied considerably over time, space, and fields of study (Herrmann et al., 2015), there is widespread agreement that the study of literary form, not (just) content, is what allows us to determine the criteria for aesthetic appreciation and impact. That is, it is formal or stylistic features that often distinguish sophisticated, effectual, or simply ‘good’ poetry from poetry that is pedestrian, mundane, or just plain ‘bad’ (Jakobson, 1960; Shlovsky, 1965; Ganjigunte Ashok et al., 2013; Kao and Jurafsky, 2015; Menninghaus et al., 2017). In this work, we consider the problem of styleconditioned poetry generation, where the user may specify stylistic factors that the generated text has to fulfil. Our stylistic factors include rhyme, alliteration, sentiment, text length, and time period. Examples of some of our stylistic features (particularly rhyme and alliteration) can be found in Figure 1. (We include metre in the example, but do not consider it in the experiments.) In generating poetry conditioned upon stylistic factors, we explore a theme in the automatic genIntroduction"
2021.latechclfl-1.7,J86-2003,0,0.23127,"e sentiment), and the time period is the 19th century. A crucial factor when producing poetry (either by humans or automatically) is style. While definitions of style have varied considerably over time, space, and fields of study (Herrmann et al., 2015), there is widespread agreement that the study of literary form, not (just) content, is what allows us to determine the criteria for aesthetic appreciation and impact. That is, it is formal or stylistic features that often distinguish sophisticated, effectual, or simply ‘good’ poetry from poetry that is pedestrian, mundane, or just plain ‘bad’ (Jakobson, 1960; Shlovsky, 1965; Ganjigunte Ashok et al., 2013; Kao and Jurafsky, 2015; Menninghaus et al., 2017). In this work, we consider the problem of styleconditioned poetry generation, where the user may specify stylistic factors that the generated text has to fulfil. Our stylistic factors include rhyme, alliteration, sentiment, text length, and time period. Examples of some of our stylistic features (particularly rhyme and alliteration) can be found in Figure 1. (We include metre in the example, but do not consider it in the experiments.) In generating poetry conditioned upon stylistic factors, we ex"
2021.latechclfl-1.7,S18-2023,0,0.0253219,"Missing"
2021.latechclfl-1.7,D19-1621,0,0.210354,"çalo Oliveira et al., 2017), for any general solution to this task will have assuredly brought us a step closer to understanding human creativity.1 1Poetry generation systems may have manifold applications. For example, an AI tool that could assist songwriters in composing lyrics to a given tune, or even generate them entirely automatically, would have commercial potential. Besides such entertainment-oriented applications, producing tools for poetry generation will necessarily involve the development of poetry analysis tools – since generation and analysis are complementary aspects of poetry (Jhamtani et al., 2019) – which may prove useful in their own right. And the production and application of these analysis tools could result in insights into the structural and aesthetic features that distinguish humanpenned poetry from its artificially produced counterpart. 57 Proceedings of LaTeCH-CLfL 2021, pages 57–66 Punta Cana, Dominican Republic (Online), November 11, 2021. eration of poetry that we believe will dominate the field in the upcoming years: end-to-end generation of formal poetry from examples alone, without the need for human involvement in designing the model or filtering its output (e.g., via h"
2021.latechclfl-1.7,2015.lilt-12.3,0,0.030225,"l factor when producing poetry (either by humans or automatically) is style. While definitions of style have varied considerably over time, space, and fields of study (Herrmann et al., 2015), there is widespread agreement that the study of literary form, not (just) content, is what allows us to determine the criteria for aesthetic appreciation and impact. That is, it is formal or stylistic features that often distinguish sophisticated, effectual, or simply ‘good’ poetry from poetry that is pedestrian, mundane, or just plain ‘bad’ (Jakobson, 1960; Shlovsky, 1965; Ganjigunte Ashok et al., 2013; Kao and Jurafsky, 2015; Menninghaus et al., 2017). In this work, we consider the problem of styleconditioned poetry generation, where the user may specify stylistic factors that the generated text has to fulfil. Our stylistic factors include rhyme, alliteration, sentiment, text length, and time period. Examples of some of our stylistic features (particularly rhyme and alliteration) can be found in Figure 1. (We include metre in the example, but do not consider it in the experiments.) In generating poetry conditioned upon stylistic factors, we explore a theme in the automatic genIntroduction Poetry is a very old for"
2021.latechclfl-1.7,W17-2201,0,0.0369861,"hat the models themselves lack the ability to discover elementary properties of poetry themselves, but instead rely on the modeller’s intention and ingenuity to do so. Related work Poetry analysis. Computational analysis of poetry has been concerned largely with formal features such as metre (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016), rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), and enjambment (Ruiz et al., 2017; Baumann et al., 2018). More recently, higher-level phenomena, including semantic coherence (Herbelot, 2015), metaphor (Reinig and Rehbein, 2019; Kesarwani et al., 2017), and diachronic analysis of tropes (Haider and Eger, 2019) have come into focus. Haider et al. (2020) annotate poetry for fine-grained aesthetic emotion categories elicited in readers. Poetry generation. In the era of statistical NLP, text generation has evolved along the path of ?-gram language models (Kneser and Ney, 1995), whose inherent modelling limitations allow them to consider only a finite amount of history when generating next output symbols, to recurrent neural network (RNN) language models (Sutskever et al., 2011), which can take an infinite history into account but suffer from va"
2021.latechclfl-1.7,P11-2014,0,0.0357641,"which they incorporate in the modelling. Hämäläinen and Alnajjar (2019) define rules for style features. Agarwal and Kann (2020) tell the model when a rhyming word is required and then modify the prediction process. This means that the models themselves lack the ability to discover elementary properties of poetry themselves, but instead rely on the modeller’s intention and ingenuity to do so. Related work Poetry analysis. Computational analysis of poetry has been concerned largely with formal features such as metre (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016), rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), and enjambment (Ruiz et al., 2017; Baumann et al., 2018). More recently, higher-level phenomena, including semantic coherence (Herbelot, 2015), metaphor (Reinig and Rehbein, 2019; Kesarwani et al., 2017), and diachronic analysis of tropes (Haider and Eger, 2019) have come into focus. Haider et al. (2020) annotate poetry for fine-grained aesthetic emotion categories elicited in readers. Poetry generation. In the era of statistical NLP, text generation has evolved along the path of ?-gram language models (Kneser and Ney, 1995), whose inherent modelling limitations allow"
2021.latechclfl-1.7,D17-1035,0,0.0145751,"til it encounters an end-of-sentence token ‘EOS’. To take the context ? into account, we encode it as a feature vector c and concatenate it to every input representation. While there are other possible ways of including c, our way constantly ‘reminds’ the model of the style context, which we hope better enforces that style in the output. The model is illustrated in Figure 2. Our RNN language model also uses character-level information to build input representations, which may help it to generalize to rare words or sublexical phenomena, such as rhyming. Our implementation is taken from that of Reimers and Gurevych (2017).2 For the conditions ?, we consider the following: Poetry evaluation Evaluation of generated poetry is usually done either automatically or manually. Zhang and Lapata (2014) compare generated lines to manually produced ‘gold-standard’ lines using BLEU (Papineni et al., 2002), a metric borrowed from machine translation, for the task of continuing an initial line of poetry. They also report perplexity, a widely used metric for language model evaluation. For manual evaluation, they ask human annotators to rate the generated poems for fluency, coherence, meaning, and poeticness. Hopkins and Kiela"
2021.latechclfl-1.7,P18-1181,0,0.248306,"(Radford et al., 2019). Approaches to poetry generation can be categorized into pre- and post-deep learning era approaches. Pre-deep learning approaches were highly handengineered and ‘top-down’, using specific grammar formalisms and linguistic resources to generate poetry; they typically built stylistic aspects such as rhyme and rhythm into their model architectures (Manurung et al., 2000; Gervás, 2001; Gonçalo Oliveira, 2013; Colton et al., 2012). Neural approaches to poetry generation learn from collections of real poetry data. Most of them use RNN language models (Zhang and Lapata, 2014; Lau et al., 2018). This framework has also been extended to related tasks such as generating poetry from images (Liu et al., 2018), translating poetry (Ghazvininejad et al., 2018), and generating song lyrics given an input melody (Watanabe et al., 2018). A drawback shared by most poetry generation systems, including even very recent ones (Van de Cruys, 2020; Agarwal and Kann, 2020), is that they require hard- and hand-coded rules to generate poetry with specific properties, such as rhyming and alliteration, or to filter output not having these properties. For example, to ensure rhyming, Lau et al. (2018) explo"
2021.latechclfl-1.7,W17-2204,0,0.0227792,"(2019) define rules for style features. Agarwal and Kann (2020) tell the model when a rhyming word is required and then modify the prediction process. This means that the models themselves lack the ability to discover elementary properties of poetry themselves, but instead rely on the modeller’s intention and ingenuity to do so. Related work Poetry analysis. Computational analysis of poetry has been concerned largely with formal features such as metre (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016), rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), and enjambment (Ruiz et al., 2017; Baumann et al., 2018). More recently, higher-level phenomena, including semantic coherence (Herbelot, 2015), metaphor (Reinig and Rehbein, 2019; Kesarwani et al., 2017), and diachronic analysis of tropes (Haider and Eger, 2019) have come into focus. Haider et al. (2020) annotate poetry for fine-grained aesthetic emotion categories elicited in readers. Poetry generation. In the era of statistical NLP, text generation has evolved along the path of ?-gram language models (Kneser and Ney, 1995), whose inherent modelling limitations allow them to consider only a finite amount of history when gene"
2021.latechclfl-1.7,W19-8638,0,0.0164982,"t distinguishing generated poetry from human-authored poetry. The authors show that this competition leads to models learning rhyming with higher probability. While this model comes close to our envisioned ideal of a poetry generation system learning from examples alone, it still has the drawback that the modellers need to make the model aware of what to pay attention to (i.e., character information of ending words). This is unsatisfactory given the large space of possible features to consider in generated poetry. Formally similar approaches to ours are those of Ficler and Goldberg (2017) and Manjavacas et al. (2019) (without emphasizing the ‘examples-only’ aspect, however). Ficler and Goldberg (2017) conditioned RNN language models on stylistic features such as length and whether the text appears to be professionally written, and on sentiment and theme. They showed this approach to be successful for the given conditions in the movie reviews domain, though they did not deal with the much more challenging case of poetry. Manjavacas et al. (2019) generate hip-hop lyrics conditional upon rhythm and rhyme. To enforce rhyming, they feed phonetic rhyme representations to the model, and thus also encode relevant"
2021.latechclfl-1.7,2020.acl-main.223,0,0.0629298,"Missing"
2021.latechclfl-1.7,waltinger-2010-germanpolarityclues,0,0.0354629,"c (Mikolov et al., 2013)) with an additional character-level word representation of dimensionality 25. As optimizer, we use Adam (Kingma and Ba, 2014) with a learning rate of 0.001; we set the batch size to 25. In general, we obtained very similar results across our two English and German datasets. We report results only for one language per experimental condition. Sentiment: To determine the sentiment of a base unit of poetry, we use a lexicon lookup approach. For English, we use the opinion lexicon developed by Hu and Liu (2004) and for German, we use the German polarity clues word lists by Waltinger (2010). The English lexicon consists of 2,007 positive and 4,782 negative words and the German lexicon consists of 17,627 positive and 19,962 negative words. In each base unit of poetry, we count the number of words that appear in the positive and negative lists of the lexicon, respectively, and then calculate the difference between these two numbers. The final result is normalized by dividing by the number of total matches for this unit, resulting in the formula ? ? = ?−? ?+? , where ? is the number of positive words found and ? is the number of negative words found. For instance, if four words of"
2021.latechclfl-1.7,P19-1459,0,0.0364812,"Missing"
2021.latechclfl-1.7,P02-1040,0,0.109376,"t, which we hope better enforces that style in the output. The model is illustrated in Figure 2. Our RNN language model also uses character-level information to build input representations, which may help it to generalize to rare words or sublexical phenomena, such as rhyming. Our implementation is taken from that of Reimers and Gurevych (2017).2 For the conditions ?, we consider the following: Poetry evaluation Evaluation of generated poetry is usually done either automatically or manually. Zhang and Lapata (2014) compare generated lines to manually produced ‘gold-standard’ lines using BLEU (Papineni et al., 2002), a metric borrowed from machine translation, for the task of continuing an initial line of poetry. They also report perplexity, a widely used metric for language model evaluation. For manual evaluation, they ask human annotators to rate the generated poems for fluency, coherence, meaning, and poeticness. Hopkins and Kiela (2017) and Lau et al. (2018) use a sort of Turing test where they ask human participants to distinguish between human- and computer-generated poems. Lau et al. (2018) additionally have an expert rate generated poetry with respect to metre, rhyme, readability, and emotion. Gh"
2021.latechclfl-1.7,D14-1074,0,0.0992648,"ext generation abilities (Radford et al., 2019). Approaches to poetry generation can be categorized into pre- and post-deep learning era approaches. Pre-deep learning approaches were highly handengineered and ‘top-down’, using specific grammar formalisms and linguistic resources to generate poetry; they typically built stylistic aspects such as rhyme and rhythm into their model architectures (Manurung et al., 2000; Gervás, 2001; Gonçalo Oliveira, 2013; Colton et al., 2012). Neural approaches to poetry generation learn from collections of real poetry data. Most of them use RNN language models (Zhang and Lapata, 2014; Lau et al., 2018). This framework has also been extended to related tasks such as generating poetry from images (Liu et al., 2018), translating poetry (Ghazvininejad et al., 2018), and generating song lyrics given an input melody (Watanabe et al., 2018). A drawback shared by most poetry generation systems, including even very recent ones (Van de Cruys, 2020; Agarwal and Kann, 2020), is that they require hard- and hand-coded rules to generate poetry with specific properties, such as rhyming and alliteration, or to filter output not having these properties. For example, to ensure rhyming, Lau"
2021.starsem-1.22,P19-1310,0,0.0562086,"Missing"
2021.starsem-1.22,N19-1391,0,0.0198998,"ν : L(C ν , fΘ ) X = X kfΘ (i, s) − fΘ (j, t))k22 (s,t)∈C ν (i,j)∈a(s,t) (1) where Θ are the parameters of the encoder f . As in Cao et al. (2020), we use a regularization term to avoid for the resulting (re-aligned) embeddings to drift too far away from the initial encoder state f0 : 231 R(C ν , fΘ ) = X len(t) X kfΘ (i, t) − f0 (i, t)k22 t∈C ν i=1 (2) Like for the multilingual pre-training of m-BERT and XLM-R, we fine-tune the encoder f on the concatenation of k parallel corpora to handle resourcelean languages, which is in contrast to offline alignment with language-independent rotations (Aldarmaki and Diab, 2019; Schuster et al., 2019). Assume that English is a common pivot (source language) in all our k parallel corpora. Then the following objective function orients all non-English embeddings toward English: min Θ k X L(C ν , fΘ ) + R(C ν , fΘ ) (3) ν=1 In §5, we refer to the above described realignment step as J OINT-A LIGN. 3.2 Vector space normalization We add a batch normalization layer that constrains all embeddings of different languages into a distribution with zero mean and unit variance: f (i, s) − µβ f¯(i, s) = q σβ2 +  (4) where  is a constant value for numerical stability, µβ and σβ ar"
2021.starsem-1.22,P17-1042,0,0.0327956,"ing tasks; (ii) normalizing vector spaces is surprisingly effective, rivals much more resource-intensive methods such as remapping, and leads to more consistent gains; (iii) all three techniques—vector space normalization, re-mapping and input normalization—are orthogonal and their gains often stack. This is a very important finding as it allows for improvements on a much larger scale, especially for typologically 230 2 Related Work Cross-lingual Transfer Static cross-lingual representations have long been used for effective crosslingual transfer and can even be induced without parallel data (Artetxe et al., 2017; Lample et al., 2018). In the monolingual case, static cross-lingual embeddings have recently been succeeded by contextualized ones, which yield considerably better results. The capabilities and limitations of the contextualized multilingual BERT (m-BERT) representations is a topic of vivid discourse. Pires et al. (2019) show surprisingly good transfer performance for mBERT despite it being trained without parallel data, and that transfer is better for typologically similar languages. Wu et al. (2019) show that language representations are not correctly aligned in m-BERT, but can be linearly"
2021.starsem-1.22,N18-1083,1,0.80958,"y broader sample, and shows that vector space normalization is as effective as other recently proposed fixes for m-BERT’s limitations (especially re-mapping), but is much cheaper and orthogonal to other solutions (e.g., input normalization) in that gains are almost additive. Linguistic Typology in NLP. Structural properties of many of the world’s languages can be queried via databases such as WALS (Dryer and Haspelmath, 2013). O’Horan et al. (2016); Ponti et al. (2019) suggest to inject typological information into models to bridge the performance gap between high- and low-resource languages. Bjerva and Augenstein (2018); de Lhoneux et al. (2018); Bjerva and Augenstein (2021) show that crossOriginal - m-BERT 0 50 50 0.0 0.2 0.4 0.6 Original - XLM-R 0.8 1.0 0 0.0 0.2 0.4 0.6 Remapping - XLM-R 0.8 1.0 0.90 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.90 0 0.4 0.2 0.0 0.2 0.4 0.6 0.8 Remapping + Space Norm - XLM-R 1.0 50 50 50 0 Remapping + Space Norm - m-BERT Remapping - m-BERT 50 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.4 0.2 0.0 0.2 Matched pairs 0.4 0.6 0.8 1.0 Random pairs Figure 2: Histograms of cosine similarity scores of word pairs. lingual transfer can be more successful betwee"
2021.starsem-1.22,2021.eacl-main.38,1,0.736113,"ion is as effective as other recently proposed fixes for m-BERT’s limitations (especially re-mapping), but is much cheaper and orthogonal to other solutions (e.g., input normalization) in that gains are almost additive. Linguistic Typology in NLP. Structural properties of many of the world’s languages can be queried via databases such as WALS (Dryer and Haspelmath, 2013). O’Horan et al. (2016); Ponti et al. (2019) suggest to inject typological information into models to bridge the performance gap between high- and low-resource languages. Bjerva and Augenstein (2018); de Lhoneux et al. (2018); Bjerva and Augenstein (2021) show that crossOriginal - m-BERT 0 50 50 0.0 0.2 0.4 0.6 Original - XLM-R 0.8 1.0 0 0.0 0.2 0.4 0.6 Remapping - XLM-R 0.8 1.0 0.90 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.90 0 0.4 0.2 0.0 0.2 0.4 0.6 0.8 Remapping + Space Norm - XLM-R 1.0 50 50 50 0 Remapping + Space Norm - m-BERT Remapping - m-BERT 50 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.4 0.2 0.0 0.2 Matched pairs 0.4 0.6 0.8 1.0 Random pairs Figure 2: Histograms of cosine similarity scores of word pairs. lingual transfer can be more successful between languages which share, e.g., morphological properties."
2021.starsem-1.22,N19-1156,1,0.902609,"For instance, according to WALS, English firmly follows the subject-verb-object (SVO) structure, whereas there is no dominant order in German. We apply this reordering in order to decrease the linguistic gap between languages. For instance, when considering English and French, we reverse all noun-adjective pairings from French to match English. This alignment is done while considering a dependency tree. We re-align according to the typological features from WALS. Since such feature annotations are available for a large amount of languages, and can be obtained automatically with high accuracy (Bjerva et al., 2019a), we expect this method to scale to languages for which basic dependencies (such as noun-adjective attachment) can be obtained automatically. In §5, we refer to the above re-alignment step as T EXT. Input normalization In addition to joint alignment and vector space normalization, we investigate decreasing crosslinguistic differences between languages via the following surface form manipulation of input texts. 232 4 4.1 Experiments Transfer tasks Cross-lingual embeddings are usually evaluated via zero-shot cross-lingual transfer for supervised text classification tasks, or via unsupervised c"
2021.starsem-1.22,J19-2006,1,0.915658,"For instance, according to WALS, English firmly follows the subject-verb-object (SVO) structure, whereas there is no dominant order in German. We apply this reordering in order to decrease the linguistic gap between languages. For instance, when considering English and French, we reverse all noun-adjective pairings from French to match English. This alignment is done while considering a dependency tree. We re-align according to the typological features from WALS. Since such feature annotations are available for a large amount of languages, and can be obtained automatically with high accuracy (Bjerva et al., 2019a), we expect this method to scale to languages for which basic dependencies (such as noun-adjective attachment) can be obtained automatically. In §5, we refer to the above re-alignment step as T EXT. Input normalization In addition to joint alignment and vector space normalization, we investigate decreasing crosslinguistic differences between languages via the following surface form manipulation of input texts. 232 4 4.1 Experiments Transfer tasks Cross-lingual embeddings are usually evaluated via zero-shot cross-lingual transfer for supervised text classification tasks, or via unsupervised c"
2021.starsem-1.22,W17-4755,0,0.011817,"infer whether a premise sentence entails, contradicts, or is neutral towards a hypothesis sentence. Conneau et al. (2018) release a multilingual NLI corpus, where the English dev and test sets of the MultiNLI corpus (Williams et al., 2018) are translated to 15 languages by crowd-workers. RFEval. This task evaluates the translation quality, i.e. similarity of a target language translation and a source language sentence. Following Zhao et al. (2020), we collect source language sentences with their system and reference translations, as well as human judgments from the WMT17 metrics shared task (Bojar et al., 2017), which contains predictions of 166 translation systems across 12 language pairs in WMT17. Each language pair has approximately 3k source sentences, each associA Typologically Varied Language Sample We evaluate multilingual representations on two sets of languages: (1) a default language set with 4 languages from the official XNLI test sets and 2 languages from the WMT17 test sets; (2) a diagnostic language set which contains 19 languages with different levels of data resources from a typologically diverse sample4 covering five language families (each with at least three languages): Austronesi"
2021.starsem-1.22,P19-4007,0,0.0589207,"Missing"
2021.starsem-1.22,D18-1269,0,0.0246154,"result in high quality target language embeddings and gives a false impression of cross-lingual abilities (Libovick´y et al., 2020). Zhao et al. (2020) use the more difficult task of reference-free machine translation evaluation (RFEval) to expose limitations of cross-lingual encoders, i.e., a failure to properly represent finegrained language aspects, which may be exploited by natural adversarial inputs such as word-by-word translations. XNLI. The goal of natural language inference (NLI) is to infer whether a premise sentence entails, contradicts, or is neutral towards a hypothesis sentence. Conneau et al. (2018) release a multilingual NLI corpus, where the English dev and test sets of the MultiNLI corpus (Williams et al., 2018) are translated to 15 languages by crowd-workers. RFEval. This task evaluates the translation quality, i.e. similarity of a target language translation and a source language sentence. Following Zhao et al. (2020), we collect source language sentences with their system and reference translations, as well as human judgments from the WMT17 metrics shared task (Bojar et al., 2017), which contains predictions of 166 translation systems across 12 language pairs in WMT17. Each languag"
2021.starsem-1.22,N19-1423,0,0.67862,"2 4 6 0.65 0.70 0.75 0.80 0.85 Language Similarity RFEval (66.9) 3 Task Performance XNLI (91.7) 4 0.90 0.95 XNLI (72.8) 2 1 0 1 2 0.0 0.5 1.0 1.5 2.0 Wikipedia articles (in millions) 2.5 Figure 1: Zero-shot performance on XNLI and RFEval vs. language similarity to English (top), and data sizes in Wikipedia (bottom). Each point is a language; brackets give the Pearson correlation of points on the xand y-axis. Zero-shot performance is based on the last layer of m-BERT and is standardized (zero mean, unit standard deviation) for better comparison. Introduction Cross-lingual text representations (Devlin et al., 2019; Conneau et al., 2019) ideally allow for transfer between any language pair, and thus hold the promise to alleviate the data sparsity problem for low-resource languages. However, until now, crosslingual systems trained on English appear to transfer poorly to target languages dissimilar to English (Wu and Dredze, 2019; Pires et al., 2019) and for which only small monolingual corpora are available (Conneau et al., 2019; Hu et al., 2020; Lauscher et al., 2020), as illustrated in Fig. 1.2 As a remedy, recent work has suggested to train representations on larger multilingual corpora (Conneau et al"
2021.starsem-1.22,N13-1073,0,0.0394373,"Missing"
2021.starsem-1.22,D19-1006,0,0.0275513,"between languages which share, e.g., morphological properties. We draw inspiration from Wang and Eisner (2016), who use dependency statistics to generate a large collection of synthetic languages to augment training data for low-resource languages. This intuition of modifying languages based on syntactic features can also be used in order to decrease syntactic and morphological differences between languages. We go further than using syntactic features, and remove word contractions and reorder sentences based on typological information from WALS. 3 Language-Agnostic Representations Analyses by Ethayarajh (2019) indicate that random words are often assigned high cosine similarities in the upper layers of monolingual BERT. We examine this in a cross-lingual setting, by randomly selecting 500 German-English mutual word translations and random word pairs within parallel sentences from Europarl (Koehn, 2005). Fig. 2 (left) shows histograms based on the last layers of m-BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2019), respectively, which show that XLM-R wrongly assigns nearly perfect cosine similarity scores (+1) to both mutual word translations (matched word pairs) and random word pairs, wher"
2021.starsem-1.22,2005.mtsummit-papers.11,0,0.0416832,"ic features can also be used in order to decrease syntactic and morphological differences between languages. We go further than using syntactic features, and remove word contractions and reorder sentences based on typological information from WALS. 3 Language-Agnostic Representations Analyses by Ethayarajh (2019) indicate that random words are often assigned high cosine similarities in the upper layers of monolingual BERT. We examine this in a cross-lingual setting, by randomly selecting 500 German-English mutual word translations and random word pairs within parallel sentences from Europarl (Koehn, 2005). Fig. 2 (left) shows histograms based on the last layers of m-BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2019), respectively, which show that XLM-R wrongly assigns nearly perfect cosine similarity scores (+1) to both mutual word translations (matched word pairs) and random word pairs, whereas m-BERT sometimes assigns low scores to mutual translations. This reaffirms that both mBERT and XLM-R have difficulty in distinguishing matched from random word pairs. Surprisingly, vector space re-mapping does not seem to help for XLM-R, but better separates random from matched pairs for m-BER"
2021.starsem-1.22,D18-2012,0,0.0412338,"Missing"
2021.starsem-1.22,2020.emnlp-main.363,0,0.0533972,"r of m-BERT and is standardized (zero mean, unit standard deviation) for better comparison. Introduction Cross-lingual text representations (Devlin et al., 2019; Conneau et al., 2019) ideally allow for transfer between any language pair, and thus hold the promise to alleviate the data sparsity problem for low-resource languages. However, until now, crosslingual systems trained on English appear to transfer poorly to target languages dissimilar to English (Wu and Dredze, 2019; Pires et al., 2019) and for which only small monolingual corpora are available (Conneau et al., 2019; Hu et al., 2020; Lauscher et al., 2020), as illustrated in Fig. 1.2 As a remedy, recent work has suggested to train representations on larger multilingual corpora (Conneau et al., 2019) and, more importantly, to realign them post-hoc so as to address the deficits of state-of-the-art contextualized encoders which have not seen any parallel data during training (Schuster et al., 2019; Wu and Dredze, 2019; Cao et al., 2020). However, re-mapping (i) can be costly, (ii) requires parallel data on word or sentence level, which may not be available abundantly in low-resource set1 https://github.com/AIPHES/ Language-Agnostic-Contextualized-"
2021.starsem-1.22,D18-1543,1,0.866387,"Missing"
2021.starsem-1.22,P19-1493,0,0.167712,"Missing"
2021.starsem-1.22,2020.findings-emnlp.150,0,0.0338733,"Missing"
2021.starsem-1.22,2020.lrec-1.497,0,0.0468248,"Missing"
2021.starsem-1.22,C16-1123,0,0.0581884,"Missing"
2021.starsem-1.22,J19-3005,0,0.0199628,"Missing"
2021.starsem-1.22,N19-1162,0,0.072225,"now, crosslingual systems trained on English appear to transfer poorly to target languages dissimilar to English (Wu and Dredze, 2019; Pires et al., 2019) and for which only small monolingual corpora are available (Conneau et al., 2019; Hu et al., 2020; Lauscher et al., 2020), as illustrated in Fig. 1.2 As a remedy, recent work has suggested to train representations on larger multilingual corpora (Conneau et al., 2019) and, more importantly, to realign them post-hoc so as to address the deficits of state-of-the-art contextualized encoders which have not seen any parallel data during training (Schuster et al., 2019; Wu and Dredze, 2019; Cao et al., 2020). However, re-mapping (i) can be costly, (ii) requires parallel data on word or sentence level, which may not be available abundantly in low-resource set1 https://github.com/AIPHES/ Language-Agnostic-Contextualized-Encoders 2 We consider language similarity as the cosine similarity between the average representations of two languages over monolingual corpora from Wikipedia. 229 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 229–240 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguisti"
2021.starsem-1.22,L16-1680,0,0.0315374,"Missing"
2021.starsem-1.22,Q16-1035,0,0.0287008,"m-BERT 0 50 50 0.0 0.2 0.4 0.6 Original - XLM-R 0.8 1.0 0 0.0 0.2 0.4 0.6 Remapping - XLM-R 0.8 1.0 0.90 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.90 0 0.4 0.2 0.0 0.2 0.4 0.6 0.8 Remapping + Space Norm - XLM-R 1.0 50 50 50 0 Remapping + Space Norm - m-BERT Remapping - m-BERT 50 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.4 0.2 0.0 0.2 Matched pairs 0.4 0.6 0.8 1.0 Random pairs Figure 2: Histograms of cosine similarity scores of word pairs. lingual transfer can be more successful between languages which share, e.g., morphological properties. We draw inspiration from Wang and Eisner (2016), who use dependency statistics to generate a large collection of synthetic languages to augment training data for low-resource languages. This intuition of modifying languages based on syntactic features can also be used in order to decrease syntactic and morphological differences between languages. We go further than using syntactic features, and remove word contractions and reorder sentences based on typological information from WALS. 3 Language-Agnostic Representations Analyses by Ethayarajh (2019) indicate that random words are often assigned high cosine similarities in the upper layers o"
2021.starsem-1.22,N18-1101,0,0.0109127,"et al., 2020). Zhao et al. (2020) use the more difficult task of reference-free machine translation evaluation (RFEval) to expose limitations of cross-lingual encoders, i.e., a failure to properly represent finegrained language aspects, which may be exploited by natural adversarial inputs such as word-by-word translations. XNLI. The goal of natural language inference (NLI) is to infer whether a premise sentence entails, contradicts, or is neutral towards a hypothesis sentence. Conneau et al. (2018) release a multilingual NLI corpus, where the English dev and test sets of the MultiNLI corpus (Williams et al., 2018) are translated to 15 languages by crowd-workers. RFEval. This task evaluates the translation quality, i.e. similarity of a target language translation and a source language sentence. Following Zhao et al. (2020), we collect source language sentences with their system and reference translations, as well as human judgments from the WMT17 metrics shared task (Bojar et al., 2017), which contains predictions of 166 translation systems across 12 language pairs in WMT17. Each language pair has approximately 3k source sentences, each associA Typologically Varied Language Sample We evaluate multilingu"
2021.starsem-1.22,D19-1077,0,0.0168753,"ach point is a language; brackets give the Pearson correlation of points on the xand y-axis. Zero-shot performance is based on the last layer of m-BERT and is standardized (zero mean, unit standard deviation) for better comparison. Introduction Cross-lingual text representations (Devlin et al., 2019; Conneau et al., 2019) ideally allow for transfer between any language pair, and thus hold the promise to alleviate the data sparsity problem for low-resource languages. However, until now, crosslingual systems trained on English appear to transfer poorly to target languages dissimilar to English (Wu and Dredze, 2019; Pires et al., 2019) and for which only small monolingual corpora are available (Conneau et al., 2019; Hu et al., 2020; Lauscher et al., 2020), as illustrated in Fig. 1.2 As a remedy, recent work has suggested to train representations on larger multilingual corpora (Conneau et al., 2019) and, more importantly, to realign them post-hoc so as to address the deficits of state-of-the-art contextualized encoders which have not seen any parallel data during training (Schuster et al., 2019; Wu and Dredze, 2019; Cao et al., 2020). However, re-mapping (i) can be costly, (ii) requires parallel data on"
2021.starsem-1.22,2020.acl-main.151,1,0.879145,"Missing"
2021.starsem-1.22,D19-1053,1,0.832568,"h French Italian Indonesian Dutch Portuguese German English α α γ γ η γ γ η η β α δ δ δ α β δ β β low low low low middle low middle middle middle low middle high high high middle high high high high 29.3 26.5 24.8 24.0 23.8 22.2 21.7 20.1 19.8 19.6 19.2 18.5 18.2 18.0 17.7 16.3 16.2 15.6 0.0 0.08 0.06 0.08 0.06 0.20 0.13 0.15 0.47 0.46 0.09 0.33 1.56 2.16 1.57 0.51 1.99 1.02 2.37 5.98 low low low low low middle middle middle middle middle middle high high high high high high high high ated with one human reference translation and with the automatic translations of participating systems. As in Zhao et al. (2019, 2020), we use the Earth Mover Distance to compute the distances between source sentence and target language translations, based on the semantic similarities of their contextualized cross-lingual embeddings. We refer to this score as XMoverScore (Zhao et al., 2020) and report its Pearson correlation with human judgments in our experiments. 4.2 Table 1: Languages used, with their language families: Austronesian (α), Germanic (β), Indo-Aryan (γ), Romance (δ), and Uralic (η). The cosine distances between target languages and English are measured using m-BERT. This is, however, not likely to resu"
C12-1048,P00-1037,0,0.326695,"nd y = fi:niks given in Section 1. In the left lattice, we have arbitrarily (but suggestively) colored each step in either red or blue. Denote by Z the set of integers, by N the set of non-negative integers, and by R the set of real numbers. Consider the two-dimensional lattice Z2 . In Z2 , we call an ordered list of pairs (α0 , β0 ) = (0, 0), . . . , (αk , βk ) = (m, n) a path from (0, 0) to (m, n), and we call 1 The many-to-many alignment algorithm designed in (Jiampojamarn et al., 2007) is an extension of a one-to-one stochastic transducer devised in (Ristad and Yianilos, 1998). Moreover, (Brill and Moore, 2000) learn the weighted edit distance between string pairs where edit operations may encompass arbitrary subsequences of strings, a setting also closely related to our problem of monotone many-to-many alignments. 783 (ai , bi ) := (αi , βi ) − (αi−1 , βi−1 ), i = 1, . . . , k, steps. Moreover, we call a path λ in the lattice Z2 from (0, 0) to (m, n) monotone if all steps (a, b) are non-negative, i.e. a ≥ 0, b ≥ 0, and we call the monotone path λ S-restricted for a subset S of N2 if all steps lie within S, i.e. (a, b) ∈ S. Note that S-restricted monotone paths define S-restricted monotone alignment"
C12-1048,J90-2002,0,0.75685,"Missing"
C12-1048,D08-1113,0,0.172618,"raphones. 782 i i x ks The purpose of the present paper is to introduce a simple, flexible and general monotone many-to-many alignment algorithm (in Section 3) that competes with the approach suggested in (Jiampojamarn et al., 2007).1 Thereby, our algorithm is an intuitive and straightforward generalization of the classical Needleman-Wunsch algorithm for (biological or linguistic) sequence alignment. Moreover, we explore valuable extensions of the presented framework, likewise in Section 3, which may be useful e.g. to detect latent classes in alignments, similar to what has been done in e.g. (Dreyer et al., 2008). We also mention limitations of our procedure, in Section 4, and discuss the naive brute-force approach, exhaustive enumeration, as an alternative; furthermore, by specifying the search space for monotone many-to-many alignments, we indicate that exhaustive enumeration appears generally a feasible option in G2P and related fields. Next, in Section 6.1 we briefly mention how we perform training for string transductions in the monotone many-to-many alignment case. Then, a second contribution of this work is to suggest an alternative decoding procedure when transducing strings x into strings y,"
C12-1048,W09-2905,0,0.0132691,"i and yi end for ˆt+1 ← f ({xia , yai |i = 1, . . . , N }) ˆ t+1 , q ˆt+1 , L sim . The function f extracts (count) updates from the aligned data 8: t←t+1 9: end while 10: end procedure 6: 7: 2 But also note the dependence of the running time on the definition of sim, q and L. 785 As to the similarity measure sim employed in Algorithm 1, a popular choice is to specify it as the (logarithm of the) joint probability of the pair (u, v) ∈ Σ∗x × Σ∗y , but a multitude of alternatives is conceivable here such as the χ2 similarity, pointwise mutual information, etc. (see for instance the overview in (Hoang et al., 2009)). Also note that sim(u, v) is usually initially unknown but can be iteratively estimated via application of Algorithm 1 and count estimates in an EM-like fashion (cf. (Dempster et al., 1977)), see Algorithm 2.3 As concerns q and L, we can likewise estimate them iteratively from data, specifying their abstract forms via any well-defined (goodness) measures. The associated coefficients γ and χ can be optimized on a development set or set exogenously. 4 Exhaustive enumeration and alignments In the last section, we have specified a polynomial time algorithm for solving the monotonic S-restricted"
C12-1048,P08-1103,0,0.417377,"es for the other G2P data sets, and for the morphology data, the algorithm learns (less interestingly) to detect word endings and starts, under this specification. 7.2 Transductions We report results of experiments on transducing x strings to y strings for the G2P data and the morphology data sets. We exclude E-Celex because training the CRF with our parametrizations (e.g. all features in window size of four) did regularly not terminate, due to the large size of the data set (&gt; 60,000 string pairs). Likewise for computing resources reasons,7 we do not use ten-fold cross-validation but, as in (Jiampojamarn et al., 2008), train on the first 9 folds given by the Pascal challenge, testing on the last. Moreover, for the G2P data, we use an -scattering model with steps S = {(1, 0), (1, 1)} as a predictor model from which to infer the number of parts kˆ for decoding and then apply Algorithm 3.8 For alignments, we use in all cases Algorithms 1 and 2 with γ = 0 and χ = 0. As reference for the G2P data, we give word accuracy rates as announced by (Bisani and Ney, 7 E.g. a single run of the CRF on the G-Celex data takes longer than 24 hours on a standard PC. train the -scattering model on data where all multi-charac"
C12-1048,P10-1080,0,0.21038,"zero assumption may lead to alignments that, linguistically, seem nonsensical, such as p f h – o i: e n n i i k x s where the reader may verify that, no matter where the  is inserted, some associations will always appear unmotivated. Moreover, monotonicity appears in some cases violated as well, such as in the following, centre sent@r where it seems, linguistically, that the letter character r corresponds to phonemic r and graphemic word final e corresponds to @. Fortunately, better alignment models have been suggested to overcome these problems. For example, (Jiampojamarn et al., 2007) and (Jiampojamarn and Kondrak, 2010) suggest ‘manyto-many’ alignment models that address issue (i) above. Similar ideas were already present in (Baldwin and Tanaka, 1999), (Galescu and Allen, 2001) and (Taylor, 2005). (Bisani and Ney, 2008) likewise propose many-to-many alignment models; more precisely, their idea is to segment grapheme-phoneme pairs into non-overlapping parts (‘co-segmentation’), calling each segment a graphone, as in ph f oe i: n n which consists of five graphones. 782 i i x ks The purpose of the present paper is to introduce a simple, flexible and general monotone many-to-many alignment algorithm (in Section"
C12-1048,N07-1047,0,0.419571,"ained, the one-to-one or one-to-zero assumption may lead to alignments that, linguistically, seem nonsensical, such as p f h – o i: e n n i i k x s where the reader may verify that, no matter where the  is inserted, some associations will always appear unmotivated. Moreover, monotonicity appears in some cases violated as well, such as in the following, centre sent@r where it seems, linguistically, that the letter character r corresponds to phonemic r and graphemic word final e corresponds to @. Fortunately, better alignment models have been suggested to overcome these problems. For example, (Jiampojamarn et al., 2007) and (Jiampojamarn and Kondrak, 2010) suggest ‘manyto-many’ alignment models that address issue (i) above. Similar ideas were already present in (Baldwin and Tanaka, 1999), (Galescu and Allen, 2001) and (Taylor, 2005). (Bisani and Ney, 2008) likewise propose many-to-many alignment models; more precisely, their idea is to segment grapheme-phoneme pairs into non-overlapping parts (‘co-segmentation’), calling each segment a graphone, as in ph f oe i: n n which consists of five graphones. 782 i i x ks The purpose of the present paper is to introduce a simple, flexible and general monotone many-to-"
C12-1048,P07-2045,0,0.00351836,"MeR+A∗ : Results of Moses system on G2P data as reported by (Rama et al., 2009). CRF-3 Our approach with window size of 3 and 3-gram scoring model (see Algorithm 3). CRF-4: Our approach with window size of 4 and 3-gram scoring model. CRF-4∗ : Our approach with window size of 4 and 4-gram scoring model ˆ 1 and y ˆ 2 as the two most probable transductions of and 2-best lists (i.e. in Algorithm 3, obtain y s). In bold: Best results (no statistical tests). Underlined: best results using ‘pure’ alignments. 2008), (Jiampojamarn et al., 2007), and (Rama et al., 2009), who gives the Moses ‘baseline’ (Koehn et al., 2007). For the morphology data we use exactly the same training/test data splits as in (Dreyer et al., 2008). Moreover, because (Dreyer et al., 2008) report all results in terms of window sizes of 3, we do likewise for this data. For decoding we do not use a (complex) predictor model here but rely on simple statistics; e.g. we find that for the class 13SIA, k is always in {m − 2, m − 1, m}, where m is the length of x, so we apply Algorithm 3 three times and ˆ string. To avoid zeros in the decoding process (see discussion in select the best scoring y Section 6.2), we replace the (0, 2) steps used in"
C12-1048,P09-1016,0,0.541623,"be the best option because it may provide Algorithm 1 with too many ‘degrees of freedom’, allowing it to settle in unfavorable local optima, and thus may lead to suboptimal alignments (we find appropriate step restriction to have dramatic effects on alignment quality, which we investigate more thoroughly in subsequent research). A better, but potentially very costly, alternative is to exhaustively enumerate all possible subsets S of Ω, apply Algorithm 1 and/or Algorithm 2, and evaluate the quality of the resulting alignments with any choice of suitable measures such as alignment entropy (cf. (Pervouchine et al., 2009)), average log-likelihood, Akaike’s information criterion (Akaike, 1974) or the like. Another possibility would be to use a comprehensive Ω, but to penalize unlikely steps, which could be achieved by setting γ in Algorithm 1 to a ‘large’ real number and then, in subsequent runs, employ the remaining steps S ⊆ Ω; we outline this approach in Section 7. Sometimes, specific knowledge about a particular domain of application may be helpful, too. For example, in the field of G2P, we would expect most associations in alignments to be of the type M -to-1, i.e. one or several graphemes encode a single"
C12-1048,N09-3016,0,0.0180154,"92.0 94.0 Table 6: Data sets and word accuracy rates in percent. DSE-F: (Dreyer et al., 2008) using ‘pure’ alignments and features. DSE-FL: (Dreyer et al., 2008) using alignments, features and latent classes. Mos3, Mos15: Moses system with window sizes of 3 and 15, resp., as reported by (Dreyer et al., 2008). M-M+HMM: Many-to-many aligner with HMM and instance-based segmenter for decoding as reported by (Jiampojamarn et al., 2007). BN: (Bisani and Ney, 2008) using a machine translation motivated approach to many-to-many alignments. MeR+A∗ : Results of Moses system on G2P data as reported by (Rama et al., 2009). CRF-3 Our approach with window size of 3 and 3-gram scoring model (see Algorithm 3). CRF-4: Our approach with window size of 4 and 3-gram scoring model. CRF-4∗ : Our approach with window size of 4 and 4-gram scoring model ˆ 1 and y ˆ 2 as the two most probable transductions of and 2-best lists (i.e. in Algorithm 3, obtain y s). In bold: Best results (no statistical tests). Underlined: best results using ‘pure’ alignments. 2008), (Jiampojamarn et al., 2007), and (Rama et al., 2009), who gives the Moses ‘baseline’ (Koehn et al., 2007). For the morphology data we use exactly the same training/t"
C12-1048,N12-1087,0,0.065767,"Missing"
C16-1160,E14-1060,0,0.0742904,"prime example of a monotone Seq2Seq task, which — as a fundamental building block for speech recognition, speech synthesis, and related tasks — has been researched for decades. It differs from the previous two tasks in that input and output strings are defined over different alphabets. Lemmatization is the task of deriving the lemma from an inflected word form such as atmest→atmen. The problem is relatively simple for morphologically poor languages like English, but much harder for languages like Finnish. The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where an inflected form is generated from a lemma plus an inflection tag. 2 Our implementation of PCRF-Seq2Seq is available at: https://github.com/UKPLab/coling2016-pcrf-seq2seq 1704 3 Data Here we detail the data sets used in our experiments; examples are provided in Table 1. These datasets reflect the different Seq2Seq tasks we aim to investigate. The Text+Berg corpus (Bubenhofer et al., 2015) contains historic proceedings of the Schweizer Alpenclub (“Swiss Alpine Club”) from the years 1864–1899 in Swiss German and French. The data has been digi"
C16-1160,P00-1037,0,0.44826,"report on “par or better” performance of their inflection generation neural architecture. However, a closer inspection of their results suggests that their system is sometimes worse and sometimes better than traditional approaches. Here, we aim for a more balanced comparison on three exemplary monotone1 Seq2Seq tasks, namely spelling correction, G2P conversion, and lemmatization. Monotone Seq2Seq tasks such as morphological analysis/lemmatization, grapheme-to-phoneme conversion (G2P) (Yao and Zweig, 2015; Rao et al., 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction (Brill and Moore, 2000) have been fundamental problem classes in natural language processing (NLP) ever since the origins of the field. Their simplicity vis-`a-vis non-monotonic problems such as machine translation renders them as particularly tractable testbeds of technological progress. Unlike previous work, which has typically focussed on only one specific subproblem of monotone Seq2Seq tasks at a time, we consider model performances on three such tasks simultaneously. This leads to a more balanced view on the relative performance of different models. We compare three variants of encoder-decoder models — includin"
C16-1160,P05-1022,0,0.062135,"orkhorse for many sequence labeling tasks such as part-of-speech tagging and named entity recognition during the 2000s. Unfortunately, training and decoding time depend polynomially on the tag set size and exponentially on the order of the CRF. Here, order refers to the dependencies on the label side. This makes higher-order CRFs impractical for large training data sizes, which is the reason why virtually only first-order (linear chain) CRFs were used until recently. M¨uller et al. (2013) introduced pruned CRFs (PCRFs) that approximate the CRF objective function using coarse-to-fine decoding (Charniak and Johnson, 2005). PCRFs require much shorter runtime and are thus able to make use of higher orders. Higher orders, in turn, have been shown to be highly beneficial for coarse and fine-grained part-of-speech tagging, outperforming first-order models. For our tasks, we have adapted the implementation from M¨uller et al. (2013) — originally designed for sequence labeling — to general monotone Seq2Seq tasks. Sequence labeling assumes that an input sequence of length N is mapped to an output sequence of identical length N , while in Seq2Seq tasks, input string lengths may be shorter, longer, or equal to output st"
C16-1160,P14-2111,0,0.0282944,"ling correction, OCR errors often arise due to visual similarity of character sub-sequences such as rn → m or li → h. Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al., 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013). Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupała, 2014) were also investigated in previous works. G2P conversion is the problem of converting orthographic representations into sound representations. It is the prime example of a monotone Seq2Seq task, which — as a fundamental building block for speech recognition, speech synthesis, and related tasks — has been researched for decades. It differs from the previous two tasks in that input and output strings are defined over different alphabets. Lemmatization is the task of deriving the lemma from an inflected word form such as atmest→atmen. The problem is relatively simple for morphologically poor lan"
C16-1160,J81-4005,0,0.748202,"Missing"
C16-1160,W04-3238,0,0.0480566,"ng paper and scan quality, typeface, and OCR engine, OCR error rate can be extraordinarily high (Reynaert, 2014). OCR post-correction is of particular practical importance in the field of digital humanities. Here, paper quality, which is often bleached and tainted, and “unusual” typefaces typically cause major problems. Unlike in human spelling correction, OCR errors often arise due to visual similarity of character sub-sequences such as rn → m or li → h. Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al., 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013). Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupała, 2014) were also investigated in previous works. G2P conversion is the problem of converting orthographic representations into sound representations. It is the prime example of a monotone Seq2Seq task, which — as a fundamental building block for speech recognition, speech"
C16-1160,N13-1138,0,0.161001,"representations. It is the prime example of a monotone Seq2Seq task, which — as a fundamental building block for speech recognition, speech synthesis, and related tasks — has been researched for decades. It differs from the previous two tasks in that input and output strings are defined over different alphabets. Lemmatization is the task of deriving the lemma from an inflected word form such as atmest→atmen. The problem is relatively simple for morphologically poor languages like English, but much harder for languages like Finnish. The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where an inflected form is generated from a lemma plus an inflection tag. 2 Our implementation of PCRF-Seq2Seq is available at: https://github.com/UKPLab/coling2016-pcrf-seq2seq 1704 3 Data Here we detail the data sets used in our experiments; examples are provided in Table 1. These datasets reflect the different Seq2Seq tasks we aim to investigate. The Text+Berg corpus (Bubenhofer et al., 2015) contains historic proceedings of the Schweizer Alpenclub (“Swiss Alpine Club”) from the years 1864–1899 in Swiss German and French."
C16-1160,P14-2027,0,0.0261239,"al importance in the field of digital humanities. Here, paper quality, which is often bleached and tainted, and “unusual” typefaces typically cause major problems. Unlike in human spelling correction, OCR errors often arise due to visual similarity of character sub-sequences such as rn → m or li → h. Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al., 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013). Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupała, 2014) were also investigated in previous works. G2P conversion is the problem of converting orthographic representations into sound representations. It is the prime example of a monotone Seq2Seq task, which — as a fundamental building block for speech recognition, speech synthesis, and related tasks — has been researched for decades. It differs from the previous two tasks in that input and output strings are defined over"
C16-1160,N16-1077,0,0.134148,"correction (Schmaltz et al., 2016; Xie et al., 2016). We have noticed that, given the enormous interest currently surrounding neural architectures, recent research appears to somewhat over-enthusiastically praise the performance of encoder-decoder approaches for Seq2Seq tasks. For example, while the encoder-decoder G2P model by Rao et al. (2015) achieves an extremely low error rate on the CMUdict dataset (Kominek and Black, 2004), the neural architecture itself has a mediocre performance and only outperforms traditional models in combination with a weighted finite state transducer. Similarly, Faruqui et al. (2016) report on “par or better” performance of their inflection generation neural architecture. However, a closer inspection of their results suggests that their system is sometimes worse and sometimes better than traditional approaches. Here, we aim for a more balanced comparison on three exemplary monotone1 Seq2Seq tasks, namely spelling correction, G2P conversion, and lemmatization. Monotone Seq2Seq tasks such as morphological analysis/lemmatization, grapheme-to-phoneme conversion (G2P) (Yao and Zweig, 2015; Rao et al., 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction ("
C16-1160,P16-1154,0,0.0423358,"he decoder (top) generates the output sequence ~y . The attention-based mechanism (shown here) enables the decoder to “peek” into the input at every decoding step through multiple input representations at . Illustration from Bahdanau et al. (2014). • morph-trans: Faruqui et al. (2016) present a new encoder-decoder model designed for morphological inflection, proposing to feed the input sequence directly into the decoder. This approach is motivated by the observation that input and output are usually very similar in problems such as morphological inflection. Similar ideas have been proposed in Gu et al. (2016) in their so-called “CopyNet” encoder-decoder model (which they apply to text summarization) that allows for portions of the input sequence to be simply copied to the output sequence, without modifications. A priori, this observation seems to apply to our tasks too: at least in spelling correction, the output usually differs only marginally from the input. For the tested neural models, we follow the same overall approach as Faruqui et al. (2016): we perform decoding and evaluation of the test data using an ensemble of k = 5 independently trained models in order to deal with the non-convex natu"
C16-1160,P14-2028,0,0.0142773,"typeface, and OCR engine, OCR error rate can be extraordinarily high (Reynaert, 2014). OCR post-correction is of particular practical importance in the field of digital humanities. Here, paper quality, which is often bleached and tainted, and “unusual” typefaces typically cause major problems. Unlike in human spelling correction, OCR errors often arise due to visual similarity of character sub-sequences such as rn → m or li → h. Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al., 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013). Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupała, 2014) were also investigated in previous works. G2P conversion is the problem of converting orthographic representations into sound representations. It is the prime example of a monotone Seq2Seq task, which — as a fundamental building block for speech recognition, speech synthesis, and related"
C16-1160,N10-1103,0,0.360765,"ion 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 We call our tasks, described below, monotone because relationships between input and output sequence characters typically obey monotonicity. That is, unlike in machine translation, there are no ‘crossing edges’ in corresponding alignments. 1703 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1703–1714, Osaka, Japan, December 11-17 2016. well-established baselines for monotone Seq2Seq, namely Sequitur (Bisani and Ney, 2008), DirecTL+ (Jiampojamarn et al., 2010), and Phonetisaurus (Novak et al., 2012). We also offer our own contribution2 , which may be considered a variation of the principles underlying DirecTL+. For that purpose, we have adapted higher-order pruned conditional random fields (PCRFs) (M¨uller et al., 2013; Lafferty et al., 2001) to handle generic monotone Seq2Seq tasks. We find that traditional models appear to still be on par with or better than encoder-decoder models in most cases, depending on factors such as training data size and the complexity of the task at hand. We show that neural models unfold their strengths as soon as more"
C16-1160,D15-1166,0,0.0610405,"language processing (NLP) ever since the origins of the field. Their simplicity vis-`a-vis non-monotonic problems such as machine translation renders them as particularly tractable testbeds of technological progress. Unlike previous work, which has typically focussed on only one specific subproblem of monotone Seq2Seq tasks at a time, we consider model performances on three such tasks simultaneously. This leads to a more balanced view on the relative performance of different models. We compare three variants of encoder-decoder models — including attention-based models (Bahdanau et al., 2014; Luong et al., 2015) and the model proposed by Faruqui et al. (2016) — to three very This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 We call our tasks, described below, monotone because relationships between input and output sequence characters typically obey monotonicity. That is, unlike in machine translation, there are no ‘crossing edges’ in corresponding alignments. 1703 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1703–1714, Osaka, Japan,"
C16-1160,D13-1032,0,0.0649511,"Missing"
C16-1160,N15-1093,0,0.247831,"notone Seq2Seq task, which — as a fundamental building block for speech recognition, speech synthesis, and related tasks — has been researched for decades. It differs from the previous two tasks in that input and output strings are defined over different alphabets. Lemmatization is the task of deriving the lemma from an inflected word form such as atmest→atmen. The problem is relatively simple for morphologically poor languages like English, but much harder for languages like Finnish. The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where an inflected form is generated from a lemma plus an inflection tag. 2 Our implementation of PCRF-Seq2Seq is available at: https://github.com/UKPLab/coling2016-pcrf-seq2seq 1704 3 Data Here we detail the data sets used in our experiments; examples are provided in Table 1. These datasets reflect the different Seq2Seq tasks we aim to investigate. The Text+Berg corpus (Bubenhofer et al., 2015) contains historic proceedings of the Schweizer Alpenclub (“Swiss Alpine Club”) from the years 1864–1899 in Swiss German and French. The data has been digitized and OCR errors h"
C16-1160,W12-6208,0,0.321437,"http://creativecommons.org/licenses/by/4.0/ 1 We call our tasks, described below, monotone because relationships between input and output sequence characters typically obey monotonicity. That is, unlike in machine translation, there are no ‘crossing edges’ in corresponding alignments. 1703 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1703–1714, Osaka, Japan, December 11-17 2016. well-established baselines for monotone Seq2Seq, namely Sequitur (Bisani and Ney, 2008), DirecTL+ (Jiampojamarn et al., 2010), and Phonetisaurus (Novak et al., 2012). We also offer our own contribution2 , which may be considered a variation of the principles underlying DirecTL+. For that purpose, we have adapted higher-order pruned conditional random fields (PCRFs) (M¨uller et al., 2013; Lafferty et al., 2001) to handle generic monotone Seq2Seq tasks. We find that traditional models appear to still be on par with or better than encoder-decoder models in most cases, depending on factors such as training data size and the complexity of the task at hand. We show that neural models unfold their strengths as soon as more complex phenomena need to be learned. T"
C16-1160,D08-1047,0,0.0170633,"of particular practical importance in the field of digital humanities. Here, paper quality, which is often bleached and tainted, and “unusual” typefaces typically cause major problems. Unlike in human spelling correction, OCR errors often arise due to visual similarity of character sub-sequences such as rn → m or li → h. Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al., 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013). Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupała, 2014) were also investigated in previous works. G2P conversion is the problem of converting orthographic representations into sound representations. It is the prime example of a monotone Seq2Seq task, which — as a fundamental building block for speech recognition, speech synthesis, and related tasks — has been researched for decades. It differs from the previous two tasks in that input and output stri"
C16-1160,W16-0528,0,0.0260809,"s, including our own adaptation of pruned CRFs. 1 Introduction Encoder-decoder neural models (Sutskever et al., 2014) are a generic deep-learning approach to sequence-to-sequence translation (Seq2Seq) tasks. They encode an input sequence into a vector representation from which the decoder generates an output. These models have shown to achieve state-ofthe-art or at least highly competitive results for various NLP tasks including machine translation (Cho et al., 2014), conversation modeling (Vinyals and Le, 2015), question answering (Yin et al., 2016), and, more generally, language correction (Schmaltz et al., 2016; Xie et al., 2016). We have noticed that, given the enormous interest currently surrounding neural architectures, recent research appears to somewhat over-enthusiastically praise the performance of encoder-decoder approaches for Seq2Seq tasks. For example, while the encoder-decoder G2P model by Rao et al. (2015) achieves an extremely low error rate on the CMUdict dataset (Kominek and Black, 2004), the neural architecture itself has a mediocre performance and only outperforms traditional models in combination with a weighted finite state transducer. Similarly, Faruqui et al. (2016) report on “"
C16-1160,P07-1119,0,0.0392891,"e state transducer. Similarly, Faruqui et al. (2016) report on “par or better” performance of their inflection generation neural architecture. However, a closer inspection of their results suggests that their system is sometimes worse and sometimes better than traditional approaches. Here, we aim for a more balanced comparison on three exemplary monotone1 Seq2Seq tasks, namely spelling correction, G2P conversion, and lemmatization. Monotone Seq2Seq tasks such as morphological analysis/lemmatization, grapheme-to-phoneme conversion (G2P) (Yao and Zweig, 2015; Rao et al., 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction (Brill and Moore, 2000) have been fundamental problem classes in natural language processing (NLP) ever since the origins of the field. Their simplicity vis-`a-vis non-monotonic problems such as machine translation renders them as particularly tractable testbeds of technological progress. Unlike previous work, which has typically focussed on only one specific subproblem of monotone Seq2Seq tasks at a time, we consider model performances on three such tasks simultaneously. This leads to a more balanced view on the relative performance of different models. We compare th"
C16-1160,P02-1019,0,0.0682067,"on various factors including paper and scan quality, typeface, and OCR engine, OCR error rate can be extraordinarily high (Reynaert, 2014). OCR post-correction is of particular practical importance in the field of digital humanities. Here, paper quality, which is often bleached and tainted, and “unusual” typefaces typically cause major problems. Unlike in human spelling correction, OCR errors often arise due to visual similarity of character sub-sequences such as rn → m or li → h. Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al., 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013). Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupała, 2014) were also investigated in previous works. G2P conversion is the problem of converting orthographic representations into sound representations. It is the prime example of a monotone Seq2Seq task, which — as a fundamental building block for s"
C16-1160,W16-0103,0,0.0604195,"Missing"
C16-1160,P98-2246,0,\N,Missing
C16-1160,C98-2241,0,\N,Missing
C16-1331,W13-3520,0,0.0245253,"raints, this model jointly optimizes monolingual and cross-lingual objectives similarly as in Klementiev et al. (2012): X X L` (w, h; θ` ) + λΩ(θe , θf ) L= `∈{e,f } w,h∈D` is minimized, where w and h are target words and their contexts, respectively, and θe , θf are embedding parameters for two languages. The terms L` encode the monolingual constraints and the term Ω(θe , θf ) encodes the cross-lingual constraints, enforcing similar words across languages (obtained from sentence aligned data) to have similar embeddings. 3 Data For our experiments, we use the Wikipedia extracts available from Al-Rfou et al. (2013)2 as monolingual data and Europarl (Koehn, 2005) as bilingual database. We consider two settings, one in which we take all 21 (All21) languages available in Europarl and one in which we focus on the 10 (Big10) largest languages. These languages are bg, cs, da, de , el, en, es, et, fi, fr, hu, it, lt, lv, nl, pl, pt, ro, sk, sl, sv (Big10 languages highlighted). To induce a comparable setting, we extract in the All21 setup: 195,842 parallel sentences from Europarl and roughly 835K (randomly extracted) sentences from Wikipedia for each of the 21 languages. In the Big10 setup, we extract 1,098,89"
C16-1331,W16-1208,0,0.22335,"wo languages in a joint semantic space. Thus, it may be less sensitive to varying polysemous associations across different languages (cf. our vir example in Section 1), and hence less adequate for capturing cross-lingual polysemy.8 In terms of language similarity, we mention that our approach is formally similar to approaches as in 8 Thus, we would also expect CCA to perform better in monolingual intrinsic evaluations (as our experiments have partly confirmed) and BBA to perform better in multilingual intrinsic evaluations. We thank one reviewer for pointing this out. 3513 (Eger et al., 2015; Asgari and Mofrad, 2016) and others. Namely, we construct graphs, one for each language, and compare them to determine language distance. Compared to Eger et al. (2015), our approach differs in that they use translations in a second language ` to measure similarity between pivot language p words. This idea also underlies very well-known lexical semantic resources such as the paraphrase database (PPDB) (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013); see also Eger and Sejane (2010). In contrast, we directly use bilingual embeddings for this similarity measurement by jointly embedding p and `, which are a"
C16-1331,P14-2134,0,0.0318527,"6 Related work Besides the mono- and multilingual word vector representation research that forms the basis of our work and which has already been referred to, we mention the following three related approaches to language classification. Koehn (2005) compares down-stream task performance in SMT to language family relationship, finding positive correlation. Cooper (2008) measures semantic language distance via bilingual dictionaries, finding that French appears to be semantically closer to Basque than to German, supporting our arguments on contact as co-determining semantic language similarity. Bamman et al. (2014) and Kulkarni et al. (2015b) study semantic distance between dialects of English by comparing region specific word embeddings. Studying geographic variation of (different) languages is also closely related to studying temporal variation within one and the same language (Kulkarni et al., 2015a), with one crucial difference being the need to find a common representation in the former case. Word embeddings — in particular, monolingual ones — can also be used to address the latter scenario (Eger and Mehler, 2016; Hamilton et al., 2016). In terms of classifying languages, the work that is closest t"
C16-1331,P05-1074,0,0.0776918,"sic evaluations (as our experiments have partly confirmed) and BBA to perform better in multilingual intrinsic evaluations. We thank one reviewer for pointing this out. 3513 (Eger et al., 2015; Asgari and Mofrad, 2016) and others. Namely, we construct graphs, one for each language, and compare them to determine language distance. Compared to Eger et al. (2015), our approach differs in that they use translations in a second language ` to measure similarity between pivot language p words. This idea also underlies very well-known lexical semantic resources such as the paraphrase database (PPDB) (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013); see also Eger and Sejane (2010). In contrast, we directly use bilingual embeddings for this similarity measurement by jointly embedding p and `, which are arguably best suited for this task. Our approach also differs from Eger et al. (2015) in that we do not apply a random-surfer process to our semantic graphs. We finally note that the linguistic problem of (semantic) language classification, as we consider, involves some vagueness as there is de facto no gold standard that we can compare to. Reasonably, however, languages should be semantically similar to a degre"
C16-1331,P14-1023,0,0.0126228,"tions between down-stream task performance and second language similarity to the target language. Additionally, we show how bilingual word embeddings can be employed for the task of semantic language classification and that joint semantic spaces vary in meaningful ways across second languages. Our results support the hypothesis that semantic language similarity is influenced by both structural similarity as well as geography/contact. 1 Introduction Word embeddings derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 201"
C16-1331,P10-4002,0,0.011222,"similarity as measured by cosine distances in semantic spaces as in Figure 1 bottom right. Bilingual embedding models: We consider two approaches to constructing bilingual word embeddings. The first is the canonical correlation analysis (CCA) approach suggested in Faruqui and Dyer 3509 (2014). This takes independently constructed word vectors from two different languages and projects them onto a common vector space such that (one-best) translation pairs, as determined by automatic word alignments, are maximally linearly correlated. CCA relies on word level alignments and we use cdec for this (Dyer et al., 2010). The second approach we employ is called BilBOWA (BBA) (Gouws et al., 2015). Rather than separately training word vectors for two languages and subsequently enforcing cross-lingual constraints, this model jointly optimizes monolingual and cross-lingual objectives similarly as in Klementiev et al. (2012): X X L` (w, h; θ` ) + λΩ(θe , θf ) L= `∈{e,f } w,h∈D` is minimized, where w and h are target words and their contexts, respectively, and θe , θf are embedding parameters for two languages. The terms L` encode the monolingual constraints and the term Ω(θe , θf ) encodes the cross-lingual constr"
C16-1331,P16-2009,1,0.853758,"an, supporting our arguments on contact as co-determining semantic language similarity. Bamman et al. (2014) and Kulkarni et al. (2015b) study semantic distance between dialects of English by comparing region specific word embeddings. Studying geographic variation of (different) languages is also closely related to studying temporal variation within one and the same language (Kulkarni et al., 2015a), with one crucial difference being the need to find a common representation in the former case. Word embeddings — in particular, monolingual ones — can also be used to address the latter scenario (Eger and Mehler, 2016; Hamilton et al., 2016). In terms of classifying languages, the work that is closest to ours is that of Asgari and Mofrad (2016). A key difference between their approach and ours is that, in order to achieve a common representation between languages, they translate words. This has the disadvantage that translation pairs need to be known, which typically requires large amounts of parallel text. In contrast, bilingual word embeddings, which 3514 form the basis of our experiments, can be generated from as few as ten translation pairs, as demonstrated in Zhang et al. (2016). There is by now a lon"
C16-1331,S15-1014,1,0.843484,"ts for projecting two languages in a joint semantic space. Thus, it may be less sensitive to varying polysemous associations across different languages (cf. our vir example in Section 1), and hence less adequate for capturing cross-lingual polysemy.8 In terms of language similarity, we mention that our approach is formally similar to approaches as in 8 Thus, we would also expect CCA to perform better in monolingual intrinsic evaluations (as our experiments have partly confirmed) and BBA to perform better in multilingual intrinsic evaluations. We thank one reviewer for pointing this out. 3513 (Eger et al., 2015; Asgari and Mofrad, 2016) and others. Namely, we construct graphs, one for each language, and compare them to determine language distance. Compared to Eger et al. (2015), our approach differs in that they use translations in a second language ` to measure similarity between pivot language p words. This idea also underlies very well-known lexical semantic resources such as the paraphrase database (PPDB) (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013); see also Eger and Sejane (2010). In contrast, we directly use bilingual embeddings for this similarity measurement by jointly embe"
C16-1331,E14-1049,0,0.459107,"duction Word embeddings derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate semantic similarity of words from different languages; see Figure 1 for an illustration. Moreover, the resulting word vectors have been shown to produce on-par or better performance even in a monolingual setting, e.g., when using them for measuring se"
C16-1331,N15-1184,0,0.0214248,"s support the hypothesis that semantic language similarity is influenced by both structural similarity as well as geography/contact. 1 Introduction Word embeddings derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate semantic similarity of words from different languages; see Figure 1 for an illustration. Moreover, the resulti"
C16-1331,N13-1092,0,0.0734899,"Missing"
C16-1331,N15-1157,0,0.16569,"Missing"
C16-1331,P16-1141,0,0.0259237,"ments on contact as co-determining semantic language similarity. Bamman et al. (2014) and Kulkarni et al. (2015b) study semantic distance between dialects of English by comparing region specific word embeddings. Studying geographic variation of (different) languages is also closely related to studying temporal variation within one and the same language (Kulkarni et al., 2015a), with one crucial difference being the need to find a common representation in the former case. Word embeddings — in particular, monolingual ones — can also be used to address the latter scenario (Eger and Mehler, 2016; Hamilton et al., 2016). In terms of classifying languages, the work that is closest to ours is that of Asgari and Mofrad (2016). A key difference between their approach and ours is that, in order to achieve a common representation between languages, they translate words. This has the disadvantage that translation pairs need to be known, which typically requires large amounts of parallel text. In contrast, bilingual word embeddings, which 3514 form the basis of our experiments, can be generated from as few as ten translation pairs, as demonstrated in Zhang et al. (2016). There is by now a long-standing tradition tha"
C16-1331,P14-1006,0,0.0187252,"derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate semantic similarity of words from different languages; see Figure 1 for an illustration. Moreover, the resulting word vectors have been shown to produce on-par or better performance even in a monolingual setting, e.g., when using them for measuring semantic similarity in one of"
C16-1331,J15-4004,0,0.0218517,"ord alignments obtained on the respective Europarl data pair. For BBA, we use the monolingual Wikipedias of ` and `0 for the monolinugal constraints, and the Europarl sentence alignments of ` and `0 for the bilingual constraints. We only consider words that occur at least 100 times in the respective data sets. 4.1 Monolingual semantic task (Q1) We first evaluate the obtained BBA and CCA embedding vectors on monolingual p = English evaluation tasks, for varying second language `. The tasks we consider are WS353 (Finkelstein et al., 2002), MTurk287 (Radinsky et al., 2011), MTurk771,4 SimLex999 (Hill et al., 2015), and MEN (Bruni et al., 2014), which are standard semantic similarity datasets for English, documented in an array of previous research. In addition, we include the SimLex999-de and SimLex999-it (Leviant and Reichart, 2015) for p = German and p = Italian, respectively. In each task, the goal is to determine the semantic similarity between two language p words, such as dog and cat (when p = English). For the tasks, we indicate average Spearman correlation coefficients δ = δp,` between 2 https://sites.google.com/site/rmyeid/projects/polyglot. All other parameters set to default values. 4 http:/"
C16-1331,P12-1092,0,0.0132942,"ic language similarity is influenced by both structural similarity as well as geography/contact. 1 Introduction Word embeddings derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate semantic similarity of words from different languages; see Figure 1 for an illustration. Moreover, the resulting word vectors have been shown t"
C16-1331,C12-1089,0,0.238393,"eography/contact. 1 Introduction Word embeddings derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate semantic similarity of words from different languages; see Figure 1 for an illustration. Moreover, the resulting word vectors have been shown to produce on-par or better performance even in a monolingual setting, e.g., when usi"
C16-1331,2005.mtsummit-papers.11,0,0.124689,"This work is structured as follows. Section 2 introduces our approach of constructing graphs from bilingual word embeddings and its relation to the two questions outlined. Section 3 describes our data, 1 Our initial expectation was that bilingual word embeddings lead to better results in monolingual settings, at least for some second languages. However, this was not confirmed in any of our experiments. This may be related to our (small) data set sizes (see Section 3) or to other factors, but has no concern for the question (Q1) we are investigating. 3508 which is based on the Europarl corpus (Koehn, 2005). Section 4 details our experiments, which we discuss in Section 5. We relate to previous work in Section 6 and conclude in Section 7. 2 Model In this section, we formally outline our approach. Given N + 1 languages, choose one of them, p, as pivot language. Construct N weighted networks (p) (p) G` = (V (p) , E (p) , w` ) as follows: nodes V (p) are the words of language p, graphs are fully connected, (p) i.e., E (p) = V (p) × V (p) , and edge weights are w` (u, v) = sim(up,` , vp,` ). The similarity function sim is, e.g., cosine similarity, and up,` , vp,` ∈ Rd are bilingual word embeddings o"
C16-1331,N15-1028,0,0.0421879,"chitectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate semantic similarity of words from different languages; see Figure 1 for an illustration. Moreover, the resulting word vectors have been shown to produce on-par or better performance even in a monolingual setting, e.g., when using them for measuring semantic similarity in one of the two languages involved (Faruqui an"
C16-1331,D14-1113,0,0.0212587,"ty is influenced by both structural similarity as well as geography/contact. 1 Introduction Word embeddings derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate semantic similarity of words from different languages; see Figure 1 for an illustration. Moreover, the resulting word vectors have been shown to produce on-par or better"
C16-1331,D14-1162,0,0.0823932,"age classification and that joint semantic spaces vary in meaningful ways across second languages. Our results support the hypothesis that semantic language similarity is influenced by both structural similarity as well as geography/contact. 1 Introduction Word embeddings derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate sema"
C16-1331,P14-1131,0,0.0666447,"Missing"
C16-1331,N16-1160,0,0.0241553,"Missing"
C16-1331,N16-1156,0,0.0328774,"the latter scenario (Eger and Mehler, 2016; Hamilton et al., 2016). In terms of classifying languages, the work that is closest to ours is that of Asgari and Mofrad (2016). A key difference between their approach and ours is that, in order to achieve a common representation between languages, they translate words. This has the disadvantage that translation pairs need to be known, which typically requires large amounts of parallel text. In contrast, bilingual word embeddings, which 3514 form the basis of our experiments, can be generated from as few as ten translation pairs, as demonstrated in Zhang et al. (2016). There is by now a long-standing tradition that compares languages via analysis of complex networks that encode their words and the (semantic) relationships between them (Cancho and Sol´e, 2001; Gao et al., 2014). These studies often only look at very abstract statistics of networks such as average path lengths and clustering coefficients, rather than analyzing them on a level of content of their nodes and edges. In addition, they often substitute co-occurrence as a proxy for semantic similarity. However, as Asgari and Mofrad (2016) point out, co-occurrence is a naive estimate of similarity;"
C18-1071,Q16-1022,0,0.0291622,"result, acquiring (high-quality) datasets for new languages comes at a high cost—be it in terms of training and/or hiring expert annotators or querying large crowds in crowd-sourcing experiments. It is thus of utmost importance to train NLP systems in AM that are capable of going cross-language, so that annotation efforts do not have to be multiplied by the number of languages of interest. This is in line with current trends in NLP, which increasingly recognize the possibility and the necessity to work cross-lingually, be it in part-of-speech tagging (Zhang et al., 2016), dependency parsing (Agic et al., 2016), sentiment mining (Chen et al., 2016; Zhou et al., 2016), or other fields. In this work, we address the problem of cross-lingual (token-level) AM for the first time. We initially experiment with available resources in English, German, and Chinese. We show that the existing datasets for analyzing argumentation are not suitable for assessing cross-lingual component extraction due to their heterogeneity or lack of complexity. Given this scarcity of homogeneously annotated high-quality largescale datasets across different languages, our first contribution is to (1) provide a fully parallel (en-de"
C18-1071,W17-5115,0,0.0231203,", which is very cheap to obtain for dozens of high-resource languages. Our findings imply that current neural MT has reached a level where it can act as a substitute for costly (non-expert) HT even for problems that operate on the fine-grained token-level. 2 Related Work In what follows, we briefly summarize the works that most closely relate to our current research. Argumentation Mining AM seeks to automatically identify argument structures in text and has received a lot of attention in NLP lately. Existing approaches focus, for instance, on specific subtasks like argument unit segmentation (Ajjour et al., 2017), identifying different types of argument components (Mochales-Palau and Moens, 2009; Habernal and Gurevych, 2017), recognizing argumentative discourse relations (Nguyen and Litman, 2016) or extracting argument components and relations endto-end (Eger et al., 2017). However, most of these approaches are specifically designed for English and there are only few resources for other languages. For German, a few datasets annotated according to the claim-premise scheme are available (Eckle-Kohler et al., 2015; Liebeck et al., 2016). Furthermore, Peldszus and Stede (2015) annotated a small German dat"
C18-1071,W17-5108,0,0.0258307,"cally designed for English and there are only few resources for other languages. For German, a few datasets annotated according to the claim-premise scheme are available (Eckle-Kohler et al., 2015; Liebeck et al., 2016). Furthermore, Peldszus and Stede (2015) annotated a small German dataset with claims and premises and translated it to English subsequently. There are very few works studying AM in other languages, e.g. Basile et al. (2016) for Italian, Li et al. (2017) for Chinese and Sardianos et al. (2015) for Greek. There are also two recent papers addressing some form of cross-linguality: Aker and Zhang (2017) map argumentative sentences from English to Mandarin using machine translation in comparable Wikipedia articles. Sliwa et al. (2018) create corpora in Balkan languages and Arabic by labeling the English side of corresponding parallel corpora on the sentence level and then using the same label for the target sentences. In contrast to these works, we work on the more challenging token-level. Moreover, we actually train classifiers for language transfer rather than only creating annotated data in other languages based on parallel data. As mentioned, we focus on cross-lingual component extraction"
C18-1071,P17-2037,0,0.0140282,"ell as translations with humancreated and projected annotations. Major claims in bold, claims underlined, premises in italics. HT/MT =human/machine translation. language adaptation, because it is the most realistic cross-lingual scenario for AM, as it may be costly to even produce small amounts of training data in many different languages. Most cross-lingual sequence tagging approaches address POS tagging, and only few are devoted to NER (Mayhew et al., 2017; Tsai et al., 2016), aspect-based sentiment classification (Lambert, 2015), or even more challenging problems such as discourse parsing (Braud et al., 2017). While POS tagging and NER are in some sense very similar to AM, namely, insofar as both can be modeled as sequential tagging of tokens, there are also important differences. For example, in POS tagging and NER, the label for a current token usually strongly depends on the token itself plus some local context. This strong association between label, token and local context is largely absent in AM, causing some models that perform well on POS and NER to fail blatantly in AM.1 Cross-lingual Word Embeddings are the (modern) basis of the direct transfer method. As with monolingual embeddings, ther"
C18-1071,D17-1070,0,0.0612602,"Missing"
C18-1071,D17-1078,0,0.0598659,"Yang et al., 2017). Two standard approaches are projection (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and direct transfer (T¨ackstr¨om et al., 2012; Zhang et al., 2016). Projection uses parallel data to project annotations from one language to another. In contrast, in direct transfer, a system is trained in L1 using language independent or shared features and applied without modification to L2. While these approaches are typically unsupervised, i.e., they assume no annotations in L2, there are also supervised cross-lingual approaches based on multi-task learning (Cotterell and Heigold, 2017; Yang et al., 2017; Kim et al., 2017). These assume small training sets in L2, and a system trained on them is regularized by a larger amount of training data in L1. In our work, we only consider unsupervised 832 Name Docum. Tokens Sentences Major Cl. Cl. Prem. Genre Lang. MTX CRC PE 112 315 402 8,865 (en) 21,858 148,186 (en) 449 957 7,141 135 751 112 1,415 1,506 464 684 3,832 short texts reviews persuasive essays en, de zh [en] en [de, fr, es, zh] Table 1: Statistics for datasets used in this work. Languages in brackets added by the current work. Orig-EN HT-DE-HumanAnno HT-DE-ProjAnno MT-DE-"
C18-1071,P11-1061,0,0.025871,"ken with a BIO label plus its respective component type. The BIO label marks the start, continuation and end of specific argument components. Examples are given in Tables 2 and 3. Cross-lingual sequence tagging POS tagging and named-entity recognition (NER) are standard tasks in NLP. In recent years, there is increased interest not only in evaluating POS and NER models within multiple individual languages (Plank et al., 2016), but also cross-lingually (Zhang et al., 2016; Tsai et al., 2016; Mayhew et al., 2017; Yang et al., 2017). Two standard approaches are projection (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and direct transfer (T¨ackstr¨om et al., 2012; Zhang et al., 2016). Projection uses parallel data to project annotations from one language to another. In contrast, in direct transfer, a system is trained in L1 using language independent or shared features and applied without modification to L2. While these approaches are typically unsupervised, i.e., they assume no annotations in L2, there are also supervised cross-lingual approaches based on multi-task learning (Cotterell and Heigold, 2017; Yang et al., 2017; Kim et al., 2017). These assume small training sets in L"
C18-1071,D17-1218,1,0.898743,"Missing"
C18-1071,N13-1073,0,0.0299452,"., 2015) on >2 million aligned sentences from the Europarl corpus (Koehn, 2005). BIVCD concatenates bilingually aligned sentences (or documents), randomly shuffles each concatenation and trains a standard monolingual word embedding technique on the result; here, we use the word2vec skipgram model (Mikolov et al., 2013). BIVCD was shown to be competitive to more challenging approaches in Upadhyay et al. (2016). BISKIP is a variant of the standard skip-gram model which predicts monoand cross-lingual contexts. It requires word alignments between parallel sentences and we use fast-align for this (Dyer et al., 2013). For EN↔ZH we train the same models on the UN corpus (Ziemski et al., 2016), which comprises >11 million parallel sentences. We train embeddings of sizes 100 and 200. Projection To implement projection for the problem of token-level AM, we proceed as follows. We take our human-labeled L1 data and align it with its corresponding parallel L2 data using fast-align. Once we have word level alignment information, we consider for each argument component c(s) in L1 of type a (e.g., MajorClaim, Claim, Premise) with consecutive words s1 , . . . , sN : the word t1 with smallest index in the correspondi"
C18-1071,D15-1267,1,0.933711,"alysis, summarization, debate modeling, and law, among others (Peldszus and Stede, 2013a). Recent studies have successfully applied computational methods to analyze monological argumentation (Wachsmuth et al., 2017; Eger et al., 2017). Most of these studies view arguments as consisting of (at least) claims and premises—and so do we in this work. Thereby, our focus is on token-level argument component extraction, that is, the segmentation and typing of argument components. AM has thus far almost exclusively been performed monolingually, e.g. in English (Mochales-Palau and Moens, 2009), German (Eckle-Kohler et al., 2015), or Chinese (Li et al., 2017). Working only monolingually is problematic, however, because AM is a difficult task even for humans due to its dependence on background knowledge and parsing of complex pragmatic relations (Moens, 2017). As a result, acquiring (high-quality) datasets for new languages comes at a high cost—be it in terms of training and/or hiring expert annotators or querying large crowds in crowd-sourcing experiments. It is thus of utmost importance to train NLP systems in AM that are capable of going cross-language, so that annotation efforts do not have to be multiplied by the"
C18-1071,P17-1002,1,0.921063,"almost eliminates the loss from cross-lingual transfer. Moreover, we find that annotation projection works equally well when using either costly human or cheap machine translations. Our code and data are available at http://github.com/UKPLab/ coling2018-xling_argument_mining. 1 Introduction Argumentation mining (AM) is a fast-growing research field with applications in discourse analysis, summarization, debate modeling, and law, among others (Peldszus and Stede, 2013a). Recent studies have successfully applied computational methods to analyze monological argumentation (Wachsmuth et al., 2017; Eger et al., 2017). Most of these studies view arguments as consisting of (at least) claims and premises—and so do we in this work. Thereby, our focus is on token-level argument component extraction, that is, the segmentation and typing of argument components. AM has thus far almost exclusively been performed monolingually, e.g. in English (Mochales-Palau and Moens, 2009), German (Eckle-Kohler et al., 2015), or Chinese (Li et al., 2017). Working only monolingually is problematic, however, because AM is a difficult task even for humans due to its dependence on background knowledge and parsing of complex pragmati"
C18-1071,J17-1004,1,0.846653,"ral MT has reached a level where it can act as a substitute for costly (non-expert) HT even for problems that operate on the fine-grained token-level. 2 Related Work In what follows, we briefly summarize the works that most closely relate to our current research. Argumentation Mining AM seeks to automatically identify argument structures in text and has received a lot of attention in NLP lately. Existing approaches focus, for instance, on specific subtasks like argument unit segmentation (Ajjour et al., 2017), identifying different types of argument components (Mochales-Palau and Moens, 2009; Habernal and Gurevych, 2017), recognizing argumentative discourse relations (Nguyen and Litman, 2016) or extracting argument components and relations endto-end (Eger et al., 2017). However, most of these approaches are specifically designed for English and there are only few resources for other languages. For German, a few datasets annotated according to the claim-premise scheme are available (Eckle-Kohler et al., 2015; Liebeck et al., 2016). Furthermore, Peldszus and Stede (2015) annotated a small German dataset with claims and premises and translated it to English subsequently. There are very few works studying AM in o"
C18-1071,D17-1302,0,0.0862285,"projection (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and direct transfer (T¨ackstr¨om et al., 2012; Zhang et al., 2016). Projection uses parallel data to project annotations from one language to another. In contrast, in direct transfer, a system is trained in L1 using language independent or shared features and applied without modification to L2. While these approaches are typically unsupervised, i.e., they assume no annotations in L2, there are also supervised cross-lingual approaches based on multi-task learning (Cotterell and Heigold, 2017; Yang et al., 2017; Kim et al., 2017). These assume small training sets in L2, and a system trained on them is regularized by a larger amount of training data in L1. In our work, we only consider unsupervised 832 Name Docum. Tokens Sentences Major Cl. Cl. Prem. Genre Lang. MTX CRC PE 112 315 402 8,865 (en) 21,858 148,186 (en) 449 957 7,141 135 751 112 1,415 1,506 464 684 3,832 short texts reviews persuasive essays en, de zh [en] en [de, fr, es, zh] Table 1: Statistics for datasets used in this work. Languages in brackets added by the current work. Orig-EN HT-DE-HumanAnno HT-DE-ProjAnno MT-DE-ProjAnno MT-ES-ProjAnno MT-FR-ProjAnno"
C18-1071,2005.mtsummit-papers.11,0,0.0193789,"s induced via machine translation, then a second source of noise for projection is the ‘unreliable’ L2 input training data. Direct Transfer Here, we directly train a system on bilingual representations, which in our case come in the form of bilingual word embeddings. To retain some freedom over the choice and parameters of our word embeddings, we choose to train them ourselves instead of using pre-trained ones. For EN↔DE we induce bilingual word embeddings by training BIVCD (Vulic and Moens, 2015) and BISKIP models (Luong et al., 2015) on >2 million aligned sentences from the Europarl corpus (Koehn, 2005). BIVCD concatenates bilingually aligned sentences (or documents), randomly shuffles each concatenation and trains a standard monolingual word embedding technique on the result; here, we use the word2vec skipgram model (Mikolov et al., 2013). BIVCD was shown to be competitive to more challenging approaches in Upadhyay et al. (2016). BISKIP is a variant of the standard skip-gram model which predicts monoand cross-lingual contexts. It requires word alignments between parallel sentences and we use fast-align for this (Dyer et al., 2013). For EN↔ZH we train the same models on the UN corpus (Ziemsk"
C18-1071,P15-2128,0,0.0195247,"不能算计划中的运气] 。 Table 2: Human-annotated English sentence in the PE dataset as well as translations with humancreated and projected annotations. Major claims in bold, claims underlined, premises in italics. HT/MT =human/machine translation. language adaptation, because it is the most realistic cross-lingual scenario for AM, as it may be costly to even produce small amounts of training data in many different languages. Most cross-lingual sequence tagging approaches address POS tagging, and only few are devoted to NER (Mayhew et al., 2017; Tsai et al., 2016), aspect-based sentiment classification (Lambert, 2015), or even more challenging problems such as discourse parsing (Braud et al., 2017). While POS tagging and NER are in some sense very similar to AM, namely, insofar as both can be modeled as sequential tagging of tokens, there are also important differences. For example, in POS tagging and NER, the label for a current token usually strongly depends on the token itself plus some local context. This strong association between label, token and local context is largely absent in AM, causing some models that perform well on POS and NER to fail blatantly in AM.1 Cross-lingual Word Embeddings are the"
C18-1071,N16-1030,0,0.0187963,"Missing"
C18-1071,W16-2817,0,0.0139484,", for instance, on specific subtasks like argument unit segmentation (Ajjour et al., 2017), identifying different types of argument components (Mochales-Palau and Moens, 2009; Habernal and Gurevych, 2017), recognizing argumentative discourse relations (Nguyen and Litman, 2016) or extracting argument components and relations endto-end (Eger et al., 2017). However, most of these approaches are specifically designed for English and there are only few resources for other languages. For German, a few datasets annotated according to the claim-premise scheme are available (Eckle-Kohler et al., 2015; Liebeck et al., 2016). Furthermore, Peldszus and Stede (2015) annotated a small German dataset with claims and premises and translated it to English subsequently. There are very few works studying AM in other languages, e.g. Basile et al. (2016) for Italian, Li et al. (2017) for Chinese and Sardianos et al. (2015) for Greek. There are also two recent papers addressing some form of cross-linguality: Aker and Zhang (2017) map argumentative sentences from English to Mandarin using machine translation in comparable Wikipedia articles. Sliwa et al. (2018) create corpora in Balkan languages and Arabic by labeling the En"
C18-1071,W15-1521,0,0.0294941,"ons than direct transfer: it requires parallel data.3 When the parallel data is induced via machine translation, then a second source of noise for projection is the ‘unreliable’ L2 input training data. Direct Transfer Here, we directly train a system on bilingual representations, which in our case come in the form of bilingual word embeddings. To retain some freedom over the choice and parameters of our word embeddings, we choose to train them ourselves instead of using pre-trained ones. For EN↔DE we induce bilingual word embeddings by training BIVCD (Vulic and Moens, 2015) and BISKIP models (Luong et al., 2015) on >2 million aligned sentences from the Europarl corpus (Koehn, 2005). BIVCD concatenates bilingually aligned sentences (or documents), randomly shuffles each concatenation and trains a standard monolingual word embedding technique on the result; here, we use the word2vec skipgram model (Mikolov et al., 2013). BIVCD was shown to be competitive to more challenging approaches in Upadhyay et al. (2016). BISKIP is a variant of the standard skip-gram model which predicts monoand cross-lingual contexts. It requires word alignments between parallel sentences and we use fast-align for this (Dyer et"
C18-1071,P16-1101,0,0.124359,"Missing"
C18-1071,D17-1269,0,0.149769,"2), while having only annotated source language (L1) data. We operate on token-level by labeling each token with a BIO label plus its respective component type. The BIO label marks the start, continuation and end of specific argument components. Examples are given in Tables 2 and 3. Cross-lingual sequence tagging POS tagging and named-entity recognition (NER) are standard tasks in NLP. In recent years, there is increased interest not only in evaluating POS and NER models within multiple individual languages (Plank et al., 2016), but also cross-lingually (Zhang et al., 2016; Tsai et al., 2016; Mayhew et al., 2017; Yang et al., 2017). Two standard approaches are projection (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and direct transfer (T¨ackstr¨om et al., 2012; Zhang et al., 2016). Projection uses parallel data to project annotations from one language to another. In contrast, in direct transfer, a system is trained in L1 using language independent or shared features and applied without modification to L2. While these approaches are typically unsupervised, i.e., they assume no annotations in L2, there are also supervised cross-lingual approaches based on multi-task learning"
C18-1071,D11-1006,0,0.102983,"20-26, 2018. We then (2) machine translate the 402 student essays into German, Spanish, French, and Chinese. Both our human and machine translations contain argumentation annotations, in the form of either human annotations or automatically projected annotations. Our experiments indicate that both the translations and the projected annotations are of very high quality, cf. examples in Table 2. Besides contributing new datasets, (3) we perform the first evaluations of cross-lingual (token-level) AM, based on suitable adaptations of two popular cross-lingual techniques, namely, direct transfer (McDonald et al., 2011) and projection (Yarowsky et al., 2001). We find that projection works considerably better than direct transfer and almost closes the cross-lingual gap, i.e., cross-lingual performance is almost on par with in-language performance when we use parallel data and project annotations to the target language. This holds both for human (translated, HT) parallel data, which is costly to obtain, and machine translated (MT) parallel data, which is very cheap to obtain for dozens of high-resource languages. Our findings imply that current neural MT has reached a level where it can act as a substitute for"
C18-1071,D13-1032,0,0.0277835,"Missing"
C18-1071,P16-1107,0,0.0496735,"xpert) HT even for problems that operate on the fine-grained token-level. 2 Related Work In what follows, we briefly summarize the works that most closely relate to our current research. Argumentation Mining AM seeks to automatically identify argument structures in text and has received a lot of attention in NLP lately. Existing approaches focus, for instance, on specific subtasks like argument unit segmentation (Ajjour et al., 2017), identifying different types of argument components (Mochales-Palau and Moens, 2009; Habernal and Gurevych, 2017), recognizing argumentative discourse relations (Nguyen and Litman, 2016) or extracting argument components and relations endto-end (Eger et al., 2017). However, most of these approaches are specifically designed for English and there are only few resources for other languages. For German, a few datasets annotated according to the claim-premise scheme are available (Eckle-Kohler et al., 2015; Liebeck et al., 2016). Furthermore, Peldszus and Stede (2015) annotated a small German dataset with claims and premises and translated it to English subsequently. There are very few works studying AM in other languages, e.g. Basile et al. (2016) for Italian, Li et al. (2017) f"
C18-1071,W13-2324,0,0.136121,"ction and (ii) bilingual word embeddings based direct transfer strategies for cross-lingual AM, finding that the former performs considerably better and almost eliminates the loss from cross-lingual transfer. Moreover, we find that annotation projection works equally well when using either costly human or cheap machine translations. Our code and data are available at http://github.com/UKPLab/ coling2018-xling_argument_mining. 1 Introduction Argumentation mining (AM) is a fast-growing research field with applications in discourse analysis, summarization, debate modeling, and law, among others (Peldszus and Stede, 2013a). Recent studies have successfully applied computational methods to analyze monological argumentation (Wachsmuth et al., 2017; Eger et al., 2017). Most of these studies view arguments as consisting of (at least) claims and premises—and so do we in this work. Thereby, our focus is on token-level argument component extraction, that is, the segmentation and typing of argument components. AM has thus far almost exclusively been performed monolingually, e.g. in English (Mochales-Palau and Moens, 2009), German (Eckle-Kohler et al., 2015), or Chinese (Li et al., 2017). Working only monolingually is"
C18-1071,P16-2067,0,0.0153708,"n, that is, the segmentation and typing of argument components in a target language (L2), while having only annotated source language (L1) data. We operate on token-level by labeling each token with a BIO label plus its respective component type. The BIO label marks the start, continuation and end of specific argument components. Examples are given in Tables 2 and 3. Cross-lingual sequence tagging POS tagging and named-entity recognition (NER) are standard tasks in NLP. In recent years, there is increased interest not only in evaluating POS and NER models within multiple individual languages (Plank et al., 2016), but also cross-lingually (Zhang et al., 2016; Tsai et al., 2016; Mayhew et al., 2017; Yang et al., 2017). Two standard approaches are projection (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and direct transfer (T¨ackstr¨om et al., 2012; Zhang et al., 2016). Projection uses parallel data to project annotations from one language to another. In contrast, in direct transfer, a system is trained in L1 using language independent or shared features and applied without modification to L2. While these approaches are typically unsupervised, i.e., they assume no annotations i"
C18-1071,P17-1049,0,0.0252093,"hours to translate the whole PE corpus into German, and the resulting overall cost was roughly 3,000 USD. The motivations to ask translators to translate argument components contiguously were that (i) all monolingual AM datasets we know of have contiguous components, (ii) transfer would have been naturally hampered had components in the source language been contiguous but not in the target language, at least for methods such as direct transfer.2 2 We note that even professional translations typically differ from original, non-translated texts because they retain traces of the source language (Rabinovich et al., 2017). We thus speculate that our reported results are probably slightly upward biased compared to a situation where the test data consists of original German student essays. This latter situation would have been much more costly to produce, in any way: it would have required retrieval (and, if necessary, creation) of original student essays in German as well as induction of all subsequent annotation mark-up. 834 To obtain further parallel versions of the PE data, we also automatically translated them into German, French, Spanish, and Chinese using Google Translate. Of course, we cannot make any de"
C18-1071,N18-2006,1,0.855743,"hen this is possible, and then project labels to the translated text. This eliminates the (particular) “OOV” and “ordering” problems inherent to direct transfer. Prerequiste to this approach is high quality MT, which, with the advent of neural techniques, appears to be now available. We hope our new datasets fuel AM research in languages other than English. In this work, we did not consider cross-lingual argumentative relation identification, although relations are available in the newly created parallel PE and CRC datasets. Future work should explore cross-lingual multi-task learning for AM (Schulz et al., 2018) with the source language as main task and small amounts of labeled target language data, as well as adversarial training techniques (Yasunaga et al., 2018), which promise to be beneficial for the particular OOV problem that direct transfer is prone to (though not for the ordering problem). We also want to combine projection with direct transfer by training on the union of projected L2 data as well as the original L1 data using shared representations. Acknowledgements This work has been supported by the German Federal Ministry of Education and Research (BMBF) under the promotional reference 01"
C18-1071,J17-3005,1,0.940333,"for the first time. We initially experiment with available resources in English, German, and Chinese. We show that the existing datasets for analyzing argumentation are not suitable for assessing cross-lingual component extraction due to their heterogeneity or lack of complexity. Given this scarcity of homogeneously annotated high-quality largescale datasets across different languages, our first contribution is to (1) provide a fully parallel (en-de), human-translated version of one of the most popular current AM datasets, namely, the English dataset of persuasive student essays published by Stab and Gurevych (2017). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 831 Proceedings of the 27th International Conference on Computational Linguistics, pages 831–844 Santa Fe, New Mexico, USA, August 20-26, 2018. We then (2) machine translate the 402 student essays into German, Spanish, French, and Chinese. Both our human and machine translations contain argumentation annotations, in the form of either human annotations or automatically projected annotations. Our experiments indicate that both the translations and"
C18-1071,N12-1052,0,0.0561356,"Missing"
C18-1071,Q13-1001,0,0.0489631,"Missing"
C18-1071,K16-1022,0,0.0960127,"target language (L2), while having only annotated source language (L1) data. We operate on token-level by labeling each token with a BIO label plus its respective component type. The BIO label marks the start, continuation and end of specific argument components. Examples are given in Tables 2 and 3. Cross-lingual sequence tagging POS tagging and named-entity recognition (NER) are standard tasks in NLP. In recent years, there is increased interest not only in evaluating POS and NER models within multiple individual languages (Plank et al., 2016), but also cross-lingually (Zhang et al., 2016; Tsai et al., 2016; Mayhew et al., 2017; Yang et al., 2017). Two standard approaches are projection (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and direct transfer (T¨ackstr¨om et al., 2012; Zhang et al., 2016). Projection uses parallel data to project annotations from one language to another. In contrast, in direct transfer, a system is trained in L1 using language independent or shared features and applied without modification to L2. While these approaches are typically unsupervised, i.e., they assume no annotations in L2, there are also supervised cross-lingual approaches based on"
C18-1071,I05-3027,0,0.00813628,"mise scheme (Stab and Gurevych, 2017). We thus chose to include this dataset in our experiments, despite differences in the domain of the annotated texts. Li et al. (2017) used crowdsourcing to annotate Chinese hotel reviews from tripadvisor.com with four component types (major claim, claim, premise, premise supporting an implicit claim). We consider only those components with direct overlap with the components used by Stab and Gurevych (2017), thus considering components labeled as “premise supporting an implicit claim” as non-argumentative. We applied the CRF-based Chinese word segmenter by Tseng et al. (2005) to split Chinese character streams into tokens. Furthermore, we only use the “Easy Reviews Corpus” from Li et al. (2017). The remaining part of the corpus are isolated sentences from reviews with low overall inter-annotator agreement, which we ignored. An example from CRC can be found in Table 3. 3.3 A Large-Scale Parallel Dataset of Persuasive Essays (PE) Stab and Gurevych (2017) created a dataset of persuasive essays written by students on essaysforum.com. These are about controversial topics such as “competition or cooperation—which is better?”. To obtain a human-translated parallel versio"
C18-1071,P16-1157,0,0.0586659,"as sequential tagging of tokens, there are also important differences. For example, in POS tagging and NER, the label for a current token usually strongly depends on the token itself plus some local context. This strong association between label, token and local context is largely absent in AM, causing some models that perform well on POS and NER to fail blatantly in AM.1 Cross-lingual Word Embeddings are the (modern) basis of the direct transfer method. As with monolingual embeddings, there exists a veritable zoo of different approaches, but they often perform very similarly in applications (Upadhyay et al., 2016) and seemingly very different approaches are oftentimes also equivalent on a theoretical level (Ruder et al., 2017). 3 Data We chose three freely available datasets: a small parallel German-English dataset, and considerably larger English and Chinese datasets using (almost) the same inventory of argument types, which we therefore assumed to be adequate for cross-lingual experiments. We translated the two last named monolingual datasets in other languages, described below. Statistics for all datasets are given in Table 1. 1 E.g., we had tried out a word embedding based HMM model (Zhang et al.,"
C18-1071,P15-2118,0,0.0417021,"r step. Projection makes stronger assumptions than direct transfer: it requires parallel data.3 When the parallel data is induced via machine translation, then a second source of noise for projection is the ‘unreliable’ L2 input training data. Direct Transfer Here, we directly train a system on bilingual representations, which in our case come in the form of bilingual word embeddings. To retain some freedom over the choice and parameters of our word embeddings, we choose to train them ourselves instead of using pre-trained ones. For EN↔DE we induce bilingual word embeddings by training BIVCD (Vulic and Moens, 2015) and BISKIP models (Luong et al., 2015) on >2 million aligned sentences from the Europarl corpus (Koehn, 2005). BIVCD concatenates bilingually aligned sentences (or documents), randomly shuffles each concatenation and trains a standard monolingual word embedding technique on the result; here, we use the word2vec skipgram model (Mikolov et al., 2013). BIVCD was shown to be competitive to more challenging approaches in Upadhyay et al. (2016). BISKIP is a variant of the standard skip-gram model which predicts monoand cross-lingual contexts. It requires word alignments between parallel sentences a"
C18-1071,D17-1253,0,0.0205808,"considerably better and almost eliminates the loss from cross-lingual transfer. Moreover, we find that annotation projection works equally well when using either costly human or cheap machine translations. Our code and data are available at http://github.com/UKPLab/ coling2018-xling_argument_mining. 1 Introduction Argumentation mining (AM) is a fast-growing research field with applications in discourse analysis, summarization, debate modeling, and law, among others (Peldszus and Stede, 2013a). Recent studies have successfully applied computational methods to analyze monological argumentation (Wachsmuth et al., 2017; Eger et al., 2017). Most of these studies view arguments as consisting of (at least) claims and premises—and so do we in this work. Thereby, our focus is on token-level argument component extraction, that is, the segmentation and typing of argument components. AM has thus far almost exclusively been performed monolingually, e.g. in English (Mochales-Palau and Moens, 2009), German (Eckle-Kohler et al., 2015), or Chinese (Li et al., 2017). Working only monolingually is problematic, however, because AM is a difficult task even for humans due to its dependence on background knowledge and parsing"
C18-1071,H01-1035,0,0.738347,"ate the 402 student essays into German, Spanish, French, and Chinese. Both our human and machine translations contain argumentation annotations, in the form of either human annotations or automatically projected annotations. Our experiments indicate that both the translations and the projected annotations are of very high quality, cf. examples in Table 2. Besides contributing new datasets, (3) we perform the first evaluations of cross-lingual (token-level) AM, based on suitable adaptations of two popular cross-lingual techniques, namely, direct transfer (McDonald et al., 2011) and projection (Yarowsky et al., 2001). We find that projection works considerably better than direct transfer and almost closes the cross-lingual gap, i.e., cross-lingual performance is almost on par with in-language performance when we use parallel data and project annotations to the target language. This holds both for human (translated, HT) parallel data, which is costly to obtain, and machine translated (MT) parallel data, which is very cheap to obtain for dozens of high-resource languages. Our findings imply that current neural MT has reached a level where it can act as a substitute for costly (non-expert) HT even for proble"
C18-1071,N18-1089,0,0.0240798,"Missing"
C18-1071,N16-1156,0,0.477629,"x pragmatic relations (Moens, 2017). As a result, acquiring (high-quality) datasets for new languages comes at a high cost—be it in terms of training and/or hiring expert annotators or querying large crowds in crowd-sourcing experiments. It is thus of utmost importance to train NLP systems in AM that are capable of going cross-language, so that annotation efforts do not have to be multiplied by the number of languages of interest. This is in line with current trends in NLP, which increasingly recognize the possibility and the necessity to work cross-lingually, be it in part-of-speech tagging (Zhang et al., 2016), dependency parsing (Agic et al., 2016), sentiment mining (Chen et al., 2016; Zhou et al., 2016), or other fields. In this work, we address the problem of cross-lingual (token-level) AM for the first time. We initially experiment with available resources in English, German, and Chinese. We show that the existing datasets for analyzing argumentation are not suitable for assessing cross-lingual component extraction due to their heterogeneity or lack of complexity. Given this scarcity of homogeneously annotated high-quality largescale datasets across different languages, our first contribution i"
C18-1071,P16-1133,0,0.0251809,"ges comes at a high cost—be it in terms of training and/or hiring expert annotators or querying large crowds in crowd-sourcing experiments. It is thus of utmost importance to train NLP systems in AM that are capable of going cross-language, so that annotation efforts do not have to be multiplied by the number of languages of interest. This is in line with current trends in NLP, which increasingly recognize the possibility and the necessity to work cross-lingually, be it in part-of-speech tagging (Zhang et al., 2016), dependency parsing (Agic et al., 2016), sentiment mining (Chen et al., 2016; Zhou et al., 2016), or other fields. In this work, we address the problem of cross-lingual (token-level) AM for the first time. We initially experiment with available resources in English, German, and Chinese. We show that the existing datasets for analyzing argumentation are not suitable for assessing cross-lingual component extraction due to their heterogeneity or lack of complexity. Given this scarcity of homogeneously annotated high-quality largescale datasets across different languages, our first contribution is to (1) provide a fully parallel (en-de), human-translated version of one of the most popular cu"
C18-1071,L16-1561,0,0.0419904,"2005). BIVCD concatenates bilingually aligned sentences (or documents), randomly shuffles each concatenation and trains a standard monolingual word embedding technique on the result; here, we use the word2vec skipgram model (Mikolov et al., 2013). BIVCD was shown to be competitive to more challenging approaches in Upadhyay et al. (2016). BISKIP is a variant of the standard skip-gram model which predicts monoand cross-lingual contexts. It requires word alignments between parallel sentences and we use fast-align for this (Dyer et al., 2013). For EN↔ZH we train the same models on the UN corpus (Ziemski et al., 2016), which comprises >11 million parallel sentences. We train embeddings of sizes 100 and 200. Projection To implement projection for the problem of token-level AM, we proceed as follows. We take our human-labeled L1 data and align it with its corresponding parallel L2 data using fast-align. Once we have word level alignment information, we consider for each argument component c(s) in L1 of type a (e.g., MajorClaim, Claim, Premise) with consecutive words s1 , . . . , sN : the word t1 with smallest index in the corresponding L2 sentence that is aligned to some word in s1 , . . . , sN , and the ana"
C18-1132,E17-1005,0,0.144668,"antic tasks (NER, supersense tagging) on the higher levels show no improvement, they conclude that tasks need to be “sufficiently similar, e.g., all of syntactic nature” for multi-task learning to increase performance. Their conclusion is challenged by Hashimoto et al. (2017), who create a similar multi-task learning network in which lower layers predict syntactic tasks, while higher layers predict sentential relatedness and entailment. Their semantic tasks also improve when introducing shortcut connections, i.e., feeding the word representations into all layers of their network. In contrast, Alonso and Plank (2017) find generally mixed performance of MTL for semantic tasks. They also use syntactic tasks as low-level auxiliary tasks, but cannot improve performance over a single-task learning baseline for three out of five investigated semantic tasks (NER, supersense classification, frame identification). Schulz et al. (2018) apply MTL to another semantic task, argumentation mining. Instead of syntactic tasks as auxiliaries, they use diverse argumentation mining datasets from different domains. Similar to our tasks, annotation and labels vary across their different tasks due to inherent subjectivity and v"
C18-1132,W14-2302,0,0.181282,"eral language classification is the violation of selectional preferences (Wilks, 1978). It is used, e.g., in met* (Fass, 1991) to classify metaphors and metonyms. While the system distinguishes between both phenomena, it does so only after using the selectional preference information. In a related task, Horbach et al. (2016) employ this information for classifying idiomatic uses of infinitive verb compounds. Another feature used across different non-literal language detection tasks is topic information. While the work by Horbach et al. (2016) includes this feature for idiom detection, Beigman Klebanov et al. (2014) utilize it to classify metaphorically used tokens. Additionally, they make use of concreteness ratings, grounded in the Conceptual Metaphor Theory (Lakoff and Johnson, 1980). However, as argued in our introduction, concreteness is also useful for the detection of other kinds of non-literal language. For example, Zhang and Gelernter (2015) utilize such ratings to detect 1559 metonymy. Further, supersenses are employed to detect metaphors (Tsvetkov et al., 2014) or non-literal language in general (K¨oper and Schulte im Walde, 2017). One more feature that is often integrated is textual cohesion,"
C18-1132,C16-1013,0,0.10852,"are common ground in the first place or are many of them using arbitrary and mutually exclusive definitions? If the answer to this question is “yes”, then this would call for a re-thinking of a whole computational research area, and a return to safe grounds. To address our research questions, we apply multi-task learning (MTL) to four non-literal language datasets. Introduced by Caruana (1993), MTL has recently been used to improve a variety of NLP tasks with the help of other, auxiliary tasks. This includes chunking (Søgaard and Goldberg, 2016), using POS tagging, and spelling normalization (Bollmann and Søgaard, 2016), using orthographic data from other domains, but also semantic tasks like entailment (Hashimoto et al., 2017) or argument mining (Schulz et al., 2018). The underlying idea of MTL is that it is beneficial to learn several tasks jointly because of “spill-over” effects. Some researchers have claimed that a requirement for success in MTL is task relatedness. If this is true, then MTL is a formidable testbed for both of our research questions. We aim at four birds, namely: We consider detection of non-literal phenomena in four different datasets, regarding each as a separate task. These are (A) me"
C18-1132,W16-1104,1,0.884829,"test splits or usage of cross validation. The latter was too costly in terms of computation time for our tested architectures. Nonetheless, we include reference numbers for an approximate comparison. adj-noun-met is the only dataset for which we have the original test split. Here, the original feature based implementation of Tsvetkov et al. (2014) (F1 = 0.85) outperforms our approach (MTL: 0.63) by a large margin. We attribute this to heavy featureengineering on their part, using supersenses and concreteness information. In contrast, we perform on par with the state-of-the-art on tok-met (Do Dinh and Gurevych (2016): F1 = 0.56, our system: 1566 F1 = 0.56). They implement a simple MLP, incorporating also POS tags and concreteness features. Their test set is similarly large as ours. Horbach et al. (2016) report only accuracy (A = 0.86) for their cross validation experiments on inf-verb. Our system results are comparable to their approach, albeit in a different setup using a dedicated test set (A = 0.85 for MTL-de). However, as described in Section 2, Horbach et al. (2016) employ a multitude of semantic features, including selectional preferences and topic information. For part-verb, we do not reach the res"
C18-1132,J91-1003,0,0.140484,"andom forests and a variety of semantic features, including supersenses and concreteness values. The models are then used to classify metaphors in similarly annotated Spanish, Russian, and Farsi test sets. To the best of our knowledge, a combined detection of multiple non-literal phenomena has not been conducted before. This is surprising because common semantic features have already been used to classify different kinds of non-literal language. One such typical feature in non-literal language classification is the violation of selectional preferences (Wilks, 1978). It is used, e.g., in met* (Fass, 1991) to classify metaphors and metonyms. While the system distinguishes between both phenomena, it does so only after using the selectional preference information. In a related task, Horbach et al. (2016) employ this information for classifying idiomatic uses of infinitive verb compounds. Another feature used across different non-literal language detection tasks is topic information. While the work by Horbach et al. (2016) includes this feature for idiom detection, Beigman Klebanov et al. (2014) utilize it to classify metaphorically used tokens. Additionally, they make use of concreteness ratings,"
C18-1132,D17-1206,0,0.137069,"e answer to this question is “yes”, then this would call for a re-thinking of a whole computational research area, and a return to safe grounds. To address our research questions, we apply multi-task learning (MTL) to four non-literal language datasets. Introduced by Caruana (1993), MTL has recently been used to improve a variety of NLP tasks with the help of other, auxiliary tasks. This includes chunking (Søgaard and Goldberg, 2016), using POS tagging, and spelling normalization (Bollmann and Søgaard, 2016), using orthographic data from other domains, but also semantic tasks like entailment (Hashimoto et al., 2017) or argument mining (Schulz et al., 2018). The underlying idea of MTL is that it is beneficial to learn several tasks jointly because of “spill-over” effects. Some researchers have claimed that a requirement for success in MTL is task relatedness. If this is true, then MTL is a formidable testbed for both of our research questions. We aim at four birds, namely: We consider detection of non-literal phenomena in four different datasets, regarding each as a separate task. These are (A) metaphor detection in content tokens, (B) classification of metaphorical adjective-noun constructions, (C) detec"
C18-1132,L16-1135,0,0.421661,"arsi test sets. To the best of our knowledge, a combined detection of multiple non-literal phenomena has not been conducted before. This is surprising because common semantic features have already been used to classify different kinds of non-literal language. One such typical feature in non-literal language classification is the violation of selectional preferences (Wilks, 1978). It is used, e.g., in met* (Fass, 1991) to classify metaphors and metonyms. While the system distinguishes between both phenomena, it does so only after using the selectional preference information. In a related task, Horbach et al. (2016) employ this information for classifying idiomatic uses of infinitive verb compounds. Another feature used across different non-literal language detection tasks is topic information. While the work by Horbach et al. (2016) includes this feature for idiom detection, Beigman Klebanov et al. (2014) utilize it to classify metaphorically used tokens. Additionally, they make use of concreteness ratings, grounded in the Conceptual Metaphor Theory (Lakoff and Johnson, 1980). However, as argued in our introduction, concreteness is also useful for the detection of other kinds of non-literal language. Fo"
C18-1132,2005.mtsummit-papers.11,0,0.0702443,"Missing"
C18-1132,N16-1039,0,0.180947,"Missing"
C18-1132,E17-2086,0,0.169409,"Missing"
C18-1132,W15-1521,0,0.0483256,"Missing"
C18-1132,S16-2003,0,0.0378134,"d expression with lexicalized figurative sense). Much effort has been spent on the detection of metaphors, idioms, and general non-literal language use (Shutova, 2015). However, because of the named vague and subjective nature of these phenomena, a multitude of datasets using differing definitions has emerged in the process. Even when datasets address the same aspect of non-literality, they may use diverging or underspecified definitions; compare, e.g., the guidelines for annotating metaphors of Tsvetkov et al. (2014) (“[...] all words that, in your opinion, are used non-literally [...]”) and Mohammad et al. (2016) (“more complex; more distant from our senses; more abstract; more vague; ...”). The fuzziness of non-literality has two natural consequences: (1) training data is sparse, because different researchers may use diverging definitions, and hence may annotate different things rather than extend “the same story”; (2) high-quality training data is costly to obtain because there may be considerable disagreement among crowd-workers and even trained experts regarding the labels for different instances of (non-)literality. In this work, we address two research questions: This work is licensed under a Cr"
C18-1132,P17-1186,0,0.021685,"y Kahse (2017), a framework for multi-task learning sequence tagging generalizing the model of Søgaard and Goldberg (2016). An example of a two-task setup is shown in Figure 1. It uses English metaphor classification on a token level as the main task, and English metaphor detection in adjective-noun constructions as an auxiliary task (both described in Section 4). After an input layer that reads in word embeddings, there are multiple shared, bi-directional LSTM layers, thus implementing hard parameter sharing. The shared layers are followed by a number of fully connected task-specific layers (Peng et al., 2017), storing private information for each task. At their end, a softmax classifier predicts labels for each input token. In addition to using pre-trained word embeddings, the architecture incorporates character-level information to improve handling of out-of-vocabulary words. The framework can further be configured to terminate different tasks at different layers. We set this option to randomly use one of two scenarios: either all tasks use all BiLSTM layers, or all auxiliary tasks terminate one layer before the main task. 1560 Figure 1: Example setup for the sequence tagging MTL framework (Kahse"
C18-1132,W14-2303,0,0.0269138,"Missing"
C18-1132,N18-2006,1,0.879652,"s would call for a re-thinking of a whole computational research area, and a return to safe grounds. To address our research questions, we apply multi-task learning (MTL) to four non-literal language datasets. Introduced by Caruana (1993), MTL has recently been used to improve a variety of NLP tasks with the help of other, auxiliary tasks. This includes chunking (Søgaard and Goldberg, 2016), using POS tagging, and spelling normalization (Bollmann and Søgaard, 2016), using orthographic data from other domains, but also semantic tasks like entailment (Hashimoto et al., 2017) or argument mining (Schulz et al., 2018). The underlying idea of MTL is that it is beneficial to learn several tasks jointly because of “spill-over” effects. Some researchers have claimed that a requirement for success in MTL is task relatedness. If this is true, then MTL is a formidable testbed for both of our research questions. We aim at four birds, namely: We consider detection of non-literal phenomena in four different datasets, regarding each as a separate task. These are (A) metaphor detection in content tokens, (B) classification of metaphorical adjective-noun constructions, (C) detection of idiomatic use of infinitiveverb c"
C18-1132,schafer-bildhauer-2012-building,0,0.0395964,"Missing"
C18-1132,J15-4002,0,0.0199261,"ause of its non-lexicalized figurative meaning. Few would dispute the idiomaticity of “kicking the bucket”, as the non-literal meaning in this multi-word expression is largely conventionalized. However, in the sentence “One approach would be to draw the line by reference [...]” the expression “draw the line” could be classified as either metaphorical (because it still evokes the literal senses of its constituents) or idiomatic (as it is a fixed expression with lexicalized figurative sense). Much effort has been spent on the detection of metaphors, idioms, and general non-literal language use (Shutova, 2015). However, because of the named vague and subjective nature of these phenomena, a multitude of datasets using differing definitions has emerged in the process. Even when datasets address the same aspect of non-literality, they may use diverging or underspecified definitions; compare, e.g., the guidelines for annotating metaphors of Tsvetkov et al. (2014) (“[...] all words that, in your opinion, are used non-literally [...]”) and Mohammad et al. (2016) (“more complex; more distant from our senses; more abstract; more vague; ...”). The fuzziness of non-literality has two natural consequences: (1"
C18-1132,P16-2038,0,0.453474,"ction due to problem (2) above; • do existing datasets for non-literality share common ground in the first place or are many of them using arbitrary and mutually exclusive definitions? If the answer to this question is “yes”, then this would call for a re-thinking of a whole computational research area, and a return to safe grounds. To address our research questions, we apply multi-task learning (MTL) to four non-literal language datasets. Introduced by Caruana (1993), MTL has recently been used to improve a variety of NLP tasks with the help of other, auxiliary tasks. This includes chunking (Søgaard and Goldberg, 2016), using POS tagging, and spelling normalization (Bollmann and Søgaard, 2016), using orthographic data from other domains, but also semantic tasks like entailment (Hashimoto et al., 2017) or argument mining (Schulz et al., 2018). The underlying idea of MTL is that it is beneficial to learn several tasks jointly because of “spill-over” effects. Some researchers have claimed that a requirement for success in MTL is task relatedness. If this is true, then MTL is a formidable testbed for both of our research questions. We aim at four birds, namely: We consider detection of non-literal phenomena in"
C18-1132,E09-1086,0,0.0123563,"they make use of concreteness ratings, grounded in the Conceptual Metaphor Theory (Lakoff and Johnson, 1980). However, as argued in our introduction, concreteness is also useful for the detection of other kinds of non-literal language. For example, Zhang and Gelernter (2015) utilize such ratings to detect 1559 metonymy. Further, supersenses are employed to detect metaphors (Tsvetkov et al., 2014) or non-literal language in general (K¨oper and Schulte im Walde, 2017). One more feature that is often integrated is textual cohesion, e.g., in metaphor (Schulder and Hovy, 2014) and idiom detection (Sporleder and Li, 2009). The use of such common features suggests that different aspects of non-literality require similar information and that representation sharing may thus turn out beneficial. Introduced in the early nineties (Caruana, 1993), multi-task learning (MTL) has been more widely and successfully used in NLP recently (Ruder, 2017). MTL denotes a machine learning technique in which multiple tasks are trained in parallel in the same system, using a shared representation. The goal is to take advantage of commonalities between the different tasks. Bollmann and Søgaard (2016) use multi-task learning for hist"
C18-1132,P14-1024,0,0.248767,"ical (because it still evokes the literal senses of its constituents) or idiomatic (as it is a fixed expression with lexicalized figurative sense). Much effort has been spent on the detection of metaphors, idioms, and general non-literal language use (Shutova, 2015). However, because of the named vague and subjective nature of these phenomena, a multitude of datasets using differing definitions has emerged in the process. Even when datasets address the same aspect of non-literality, they may use diverging or underspecified definitions; compare, e.g., the guidelines for annotating metaphors of Tsvetkov et al. (2014) (“[...] all words that, in your opinion, are used non-literally [...]”) and Mohammad et al. (2016) (“more complex; more distant from our senses; more abstract; more vague; ...”). The fuzziness of non-literality has two natural consequences: (1) training data is sparse, because different researchers may use diverging definitions, and hence may annotate different things rather than extend “the same story”; (2) high-quality training data is costly to obtain because there may be considerable disagreement among crowd-workers and even trained experts regarding the labels for different instances of"
D15-1139,P14-2102,0,0.0764232,"stitutions is sought that transforms one string 3 We denote by x(a : b) the substring xa xa+1 · · · xb of the string x1 x2 · · · xt . into another. Substring-to-substring edit operations — or equivalently, (monotone) many-tomany alignments — have appeared in the NLP context, e.g., in Deligne et al. (1995), Brill and Moore (2000), Jiampojamarn et al. (2007), Bisani and Ney (2008), Jiampojamarn et al. (2010), or, significantly earlier, in Ukkonen (1985), V´eronis (1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., Ristad and Yianilos (1998), Cotterell et al. (2014), besides the works already mentioned. All of these approaches are special cases of our unigram model — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and sim1 .4 Eger (2015b), Yao and Kondrak (2015), and Eger (2015a) generalize to alignments of multiple strings, but likewise only consider unigram alignment models in their experiments. Probably the most closely related work to ours is Jiampojamarn and Kondrak (2010). There, older and specialized alignment techniques such as ALINE (Kondrak, 2000) (as well as partly heuristic/semi-automatic alignment methods) a"
D15-1139,D08-1113,0,0.0612388,"Missing"
D15-1139,C12-1048,1,0.787135,": i) and y(1 : j) that ends in the matchup of x(q : i) with y(w : j).3 The variable Mijqw satisfies a recurrence leading to a DP algorithm, shown in Algorithm 1. The actual alignment can be found by storing pointers to the maximizing steps taken. Running time of the algorithm is O(`2x `2y |S|). Note also that the sketched algorithm is supervised insofar as it assumes that the similarity values sim2 (·, ·) are known. Typically, such alignment algorithms can be converted into unsupervised algorithms in which similarity measures sim are learnt iteratively, e.g., in an EM-like fashion (cf., e.g., Eger (2012), Eger (2013)); however, in this paper, we only investigate the supervised base version as indicated. 3 Related work Monotone alignments have a long tradition in NLP. The classical Needleman-Wunsch algorithm (Needleman and Wunsch, 1970) computes the optimal alignment between two sequences when only single character matches, mismatches, and skips are allowed. It is a special case of the unigram model (1) for which S = {(1, 0), (0, 1), (1, 1)} and sim1 takes on values from {0, −1}, depending on whether compared subsequences match or not. As is well-known, this alignment specification is equivale"
D15-1139,P15-1088,1,0.784797,"-tomany alignments — have appeared in the NLP context, e.g., in Deligne et al. (1995), Brill and Moore (2000), Jiampojamarn et al. (2007), Bisani and Ney (2008), Jiampojamarn et al. (2010), or, significantly earlier, in Ukkonen (1985), V´eronis (1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., Ristad and Yianilos (1998), Cotterell et al. (2014), besides the works already mentioned. All of these approaches are special cases of our unigram model — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and sim1 .4 Eger (2015b), Yao and Kondrak (2015), and Eger (2015a) generalize to alignments of multiple strings, but likewise only consider unigram alignment models in their experiments. Probably the most closely related work to ours is Jiampojamarn and Kondrak (2010). There, older and specialized alignment techniques such as ALINE (Kondrak, 2000) (as well as partly heuristic/semi-automatic alignment methods) are compared with variants of the M2M alignment algorithm, which we also survey. This work does not consider supervised alignments or bigram alignments, as we do. Moreover, Jiampojamarn and Kondrak (2010) also"
D15-1139,P08-1112,0,0.0345801,"(1) (but does not allow for many-to-many match-ups). The contextual dependencies in this model are set up differently from the bigram dependencies in our paper. 1177 Algorithm 1 1: procedure BIGRAM - ALIGN(x = x1 . . . xn , y = y1 . . . ym ; S, sim2 ) 2: Mijqw ← −∞ for all (i, j, q, w) ∈ Z4 3: M0000 ← 0 4: for i = 0 . . . n do 5: for j = 0 . . . m do 6: for q = 0 . . . i + 1 do 7: for w = 0 . . . j + 1 do 8: if (i, j, q, w) 6= (0, 0, 0, 0) then 9: if (i − q + 1, j − w + 1) ∈ S then  10: Mijqw = max Mq−1,w−1,q−a,w−b +sim2 (a,b)∈S tionship between alignment quality and translation performance (Ganchev et al., 2008). In machine translation, the monotonicity assumption of string transduction does typically not hold, however, rendering alignment and translation techniques different and more heuristic in nature. 4 Data and systems 4.1 Data For English, we conduct experiments on the General American (GA) variant of the Combilex data set (Richmond et al., 2009). This contains about 128 000 grapheme-phoneme pairs as exemplified in Table 3. Importantly, Combilex provides goldstandard alignments, which we will make use of for the supervised alignment models as well as for measuring alignment quality. For German,"
D15-1139,P06-1085,0,0.28233,"rvised alignments or bigram alignments, as we do. Moreover, Jiampojamarn and Kondrak (2010) also evaluate the impact of alignment quality on overall G2P system accuracy by running a few experiments, finding that better alignment quality does not always translate into better G2P accuracy, but that there is a “strong correlation” between the two. We more thorougly investigate this question, using, arguably, more heterogeneous aligners, and many more experiments. We also quantitatively estimate how alignment quality influences G2P system accuracy on two different languages via linear regression. Goldwater et al. (2006) study the effect of context in (unsupervised) word/sequence segmentation, which may be considered the onedimensional specialization of sequence alignment, using a Bayesian method. They find that bigram models greatly outperform unigram models for their task. Of course, our study is also related to the field of machine translation and its studies on the rela4 In Cotterell et al. (2014), context influences alignments, so that the approach goes beyond the unigram model sketched in (1) (but does not allow for many-to-many match-ups). The contextual dependencies in this model are set up differentl"
D15-1139,P10-1080,0,0.0164523,"en (1985), V´eronis (1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., Ristad and Yianilos (1998), Cotterell et al. (2014), besides the works already mentioned. All of these approaches are special cases of our unigram model — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and sim1 .4 Eger (2015b), Yao and Kondrak (2015), and Eger (2015a) generalize to alignments of multiple strings, but likewise only consider unigram alignment models in their experiments. Probably the most closely related work to ours is Jiampojamarn and Kondrak (2010). There, older and specialized alignment techniques such as ALINE (Kondrak, 2000) (as well as partly heuristic/semi-automatic alignment methods) are compared with variants of the M2M alignment algorithm, which we also survey. This work does not consider supervised alignments or bigram alignments, as we do. Moreover, Jiampojamarn and Kondrak (2010) also evaluate the impact of alignment quality on overall G2P system accuracy by running a few experiments, finding that better alignment quality does not always translate into better G2P accuracy, but that there is a “strong correlation” between the"
D15-1139,N07-1047,0,0.521678,"(G2P) conversion is the problem of converting a string of letters into a string of phonetic symbols. Closely related to G2P are other string transduction problems in natural language processing (NLP) such as transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. While there are exceptions (e.g., (Rao et al., 2015)), most state-of-the-art modelings (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) view string transduction as a two-stage process in which string pairs (x, y) in the training data are first aligned, and then a subsequent (e.g., sequence labeling) module is learned on the aligned data. (iii) The unsupervised alignment models are unigram alignment models insofar as the overall score that the alignment model assigns an alignment is the same for all orderings of the matched-up subsequences (context independence). To illustrate point (iii), consider, in the field of lemmatization,"
D15-1139,P08-1103,0,0.147369,"string of letters into a string of phonetic symbols. Closely related to G2P are other string transduction problems in natural language processing (NLP) such as transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. While there are exceptions (e.g., (Rao et al., 2015)), most state-of-the-art modelings (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) view string transduction as a two-stage process in which string pairs (x, y) in the training data are first aligned, and then a subsequent (e.g., sequence labeling) module is learned on the aligned data. (iii) The unsupervised alignment models are unigram alignment models insofar as the overall score that the alignment model assigns an alignment is the same for all orderings of the matched-up subsequences (context independence). To illustrate point (iii), consider, in the field of lemmatization, the case of aligning an inflected word form with"
D15-1139,N10-1103,0,0.305736,"ring of phonetic symbols. Closely related to G2P are other string transduction problems in natural language processing (NLP) such as transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. While there are exceptions (e.g., (Rao et al., 2015)), most state-of-the-art modelings (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) view string transduction as a two-stage process in which string pairs (x, y) in the training data are first aligned, and then a subsequent (e.g., sequence labeling) module is learned on the aligned data. (iii) The unsupervised alignment models are unigram alignment models insofar as the overall score that the alignment model assigns an alignment is the same for all orderings of the matched-up subsequences (context independence). To illustrate point (iii), consider, in the field of lemmatization, the case of aligning an inflected word form with the extended infinitive in"
D15-1139,A00-2038,0,0.114215,"as been the topic of, e.g., Ristad and Yianilos (1998), Cotterell et al. (2014), besides the works already mentioned. All of these approaches are special cases of our unigram model — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and sim1 .4 Eger (2015b), Yao and Kondrak (2015), and Eger (2015a) generalize to alignments of multiple strings, but likewise only consider unigram alignment models in their experiments. Probably the most closely related work to ours is Jiampojamarn and Kondrak (2010). There, older and specialized alignment techniques such as ALINE (Kondrak, 2000) (as well as partly heuristic/semi-automatic alignment methods) are compared with variants of the M2M alignment algorithm, which we also survey. This work does not consider supervised alignments or bigram alignments, as we do. Moreover, Jiampojamarn and Kondrak (2010) also evaluate the impact of alignment quality on overall G2P system accuracy by running a few experiments, finding that better alignment quality does not always translate into better G2P accuracy, but that there is a “strong correlation” between the two. We more thorougly investigate this question, using, arguably, more heterogen"
D15-1139,J10-3002,0,0.0306738,"ram alignment models in G2P. We investigate whether there are phenomena in G2P that require bigram alignment models and, more generally, whether bigram alignment models produce better alignments — with respect to a human gold standard — than unigram alignment models within the G2P setting. We do so, secondly, in a supervised setting where the model learns from gold-standard alignments. While this may seem an odd scenario at first sight, modern alignment toolkits in the related field of machine translation typically include the possibility to learn both in a supervised and unsupervised manner (Liu et al., 2010; Liu and Sun, 2015). The rationale behind supervised learning models may be that they perform better than unsupervised models, and if alignment quality has a large impact upon subsequent string translation performance, then a supervised model may be a suitable alternative. Thirdly, we investigate how alignment quality affects overall G2P performance. This allows us to address whether it is worthwhile to work on better alignment models, which bigram and supervised alignment models promise to be. To our knowledge, all three outlined aspects of alignments — bigram models, supervised learning, an"
D15-1139,W12-6208,0,0.312486,"losely related to G2P are other string transduction problems in natural language processing (NLP) such as transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. While there are exceptions (e.g., (Rao et al., 2015)), most state-of-the-art modelings (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) view string transduction as a two-stage process in which string pairs (x, y) in the training data are first aligned, and then a subsequent (e.g., sequence labeling) module is learned on the aligned data. (iii) The unsupervised alignment models are unigram alignment models insofar as the overall score that the alignment model assigns an alignment is the same for all orderings of the matched-up subsequences (context independence). To illustrate point (iii), consider, in the field of lemmatization, the case of aligning an inflected word form with the extended infinitive in German, such as absag"
D15-1139,P09-1016,0,0.0779162,"Missing"
D15-1139,P07-1119,0,0.0294545,"nput and output sequences is preserved by the alignments. Furthermore, they are many-to-many in the sense that several x sequence characters may be matched up with several y sequence characters as illustrated in Table 1. (ii) The alignment is a latent variable and learnt in an unsupervised manner from pairs of strings in the training data. Introduction Grapheme-to-phoneme (G2P) conversion is the problem of converting a string of letters into a string of phonetic symbols. Closely related to G2P are other string transduction problems in natural language processing (NLP) such as transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. While there are exceptions (e.g., (Rao et al., 2015)), most state-of-the-art modelings (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) view string transduction as a two-stage process in which string pairs (x, y) in the training data are first aligned, and then a"
D15-1139,N15-1095,0,0.0125951,"ents — have appeared in the NLP context, e.g., in Deligne et al. (1995), Brill and Moore (2000), Jiampojamarn et al. (2007), Bisani and Ney (2008), Jiampojamarn et al. (2010), or, significantly earlier, in Ukkonen (1985), V´eronis (1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., Ristad and Yianilos (1998), Cotterell et al. (2014), besides the works already mentioned. All of these approaches are special cases of our unigram model — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and sim1 .4 Eger (2015b), Yao and Kondrak (2015), and Eger (2015a) generalize to alignments of multiple strings, but likewise only consider unigram alignment models in their experiments. Probably the most closely related work to ours is Jiampojamarn and Kondrak (2010). There, older and specialized alignment techniques such as ALINE (Kondrak, 2000) (as well as partly heuristic/semi-automatic alignment methods) are compared with variants of the M2M alignment algorithm, which we also survey. This work does not consider supervised alignments or bigram alignments, as we do. Moreover, Jiampojamarn and Kondrak (2010) also evaluate the impact of al"
D15-1139,P00-1037,0,\N,Missing
D17-1218,N16-1165,0,0.141831,"Missing"
D17-1218,W11-1701,0,0.0351308,"ce level. We include information about the part-of-speech and parse tree for each sentence. 4 For the same reason, we do not optimize any hyperparameters for individual learners, unless explicitly stated. 5 Using the liblinear library (Fan et al., 2008). Discourse Features encode information extracted with help of the Penn Discourse Treebank (PDTB) styled end-to-end discourse parser as presented by Lin et al. (2014). Embedding Features represent each sentence as a summation of its word embeddings (Guo et al., 2014). We further experimented with sentiment features (Habernal and Gurevych, 2015; Anand et al., 2011) and dictionary features (Misra et al., 2015; Rosenthal and McKeown, 2015) but these delivered very poor results and are not reported in this article. The full set of features and their parameters are described in the supplementary material to this article. We experiment with the full feature set, individual feature groups, and feature ablation (all features except for one group). 4.2 Deep Learning Approaches As alternatives to our feature-based systems, we consider three deep learning approaches. The first is the Convolutional Neural Net of Kim (Kim, 2014) which has shown to perform excellent"
D17-1218,W06-1615,0,0.166945,"Missing"
D17-1218,P07-1033,0,0.112158,"Missing"
D17-1218,P17-1002,1,0.781919,"13a). ‘Mining’ arguments usually involves several steps like separating argumentative from nonargumentative text units, parsing argument structures, and recognizing argument components such as claims—the main focus of this article. Claim identification itself is an important prerequisite for applications such as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eger et al., 2017), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017). Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (MochalesPalau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015) or student essays (Stab and Gurevych, 2017). The problem of generalizing systems or features and their robustness across heterogeneous datasets thus remains fairly unexplored. This situation motivated us to perform a detaile"
D17-1218,P11-1099,0,0.0686932,"fly describe six English datasets used in our empirical study; they all capture claims on the discourse level. Table 1 summarizes the dataset statistics relevant to claim identification. 3.1 Datasets The AraucariaDB corpus (Reed et al., 2008) includes various genres (VG) such as newspaper editorials, parliamentary records, or judicial summaries. The annotation scheme structures arguments as trees and distinguishes between claims and premises at the clause level. Although the reliability of the annotations is unknown, the corpus has been extensively used in argument mining (Moens et al., 2007; Feng and Hirst, 2011; Rooney et al., 2012). The corpus from Habernal and Gurevych (2017) includes user-generated web discourse (WD) such as blog posts, or user comments annotated with claims and premises as well as backings, rebuttals and refutations (αU 0.48) inspired by Toulmin’s model of argument (Toulmin, 2003). The persuasive essay (PE) corpus (Stab and Gurevych, 2017) includes 402 student essays. The scheme comprises major claims, claims and premises at the clause level (αU 0.77). The corpus has been extensively used in the argument mining community (Persing and Ng, 2015; Lippi and Torroni, 2015; Nguyen and"
D17-1218,N16-1138,0,0.0210111,"learner for this task, but rather 3 This is true only for the feature-based learners. The neural networks do not have access to information beyond individual sentences. 2058 want to compare the influence of different types of lexical, syntactical, and other kinds of information across datasets.4 Thus, we used a limited set of learners for our task: a) a standard L2-regularized logistic regression approach with manually defined feature sets5 , which is a simple yet robust and established technique for many text classification problems (Plank et al., 2014; He et al., 2015; Zhang et al., 2016a; Ferreira and Vlachos, 2016); and b) several deep learning approaches, using state-of-the-art neural network architectures. The in-domain experiments were carried out in a 10-fold cross-validation setup with fixed splits into training and test data. As for the crossdomain experiments, we train on the entire data of the source domain and test on the entire data of the target domain. In the domain adaptation terminology, this corresponds to an unsupervised setting. To address class-imbalance in our datasets (see Table 1), we downsample the negative class (nonclaim) both in-domain and cross-domain, so that positive and nega"
D17-1218,D14-1012,0,0.0325979,"atures are lowercased unigrams. Syntax Features account for grammatical information at the sentence level. We include information about the part-of-speech and parse tree for each sentence. 4 For the same reason, we do not optimize any hyperparameters for individual learners, unless explicitly stated. 5 Using the liblinear library (Fan et al., 2008). Discourse Features encode information extracted with help of the Penn Discourse Treebank (PDTB) styled end-to-end discourse parser as presented by Lin et al. (2014). Embedding Features represent each sentence as a summation of its word embeddings (Guo et al., 2014). We further experimented with sentiment features (Habernal and Gurevych, 2015; Anand et al., 2011) and dictionary features (Misra et al., 2015; Rosenthal and McKeown, 2015) but these delivered very poor results and are not reported in this article. The full set of features and their parameters are described in the supplementary material to this article. We experiment with the full feature set, individual feature groups, and feature ablation (all features except for one group). 4.2 Deep Learning Approaches As alternatives to our feature-based systems, we consider three deep learning approaches"
D17-1218,D15-1255,1,0.867498,"cal information at the sentence level. We include information about the part-of-speech and parse tree for each sentence. 4 For the same reason, we do not optimize any hyperparameters for individual learners, unless explicitly stated. 5 Using the liblinear library (Fan et al., 2008). Discourse Features encode information extracted with help of the Penn Discourse Treebank (PDTB) styled end-to-end discourse parser as presented by Lin et al. (2014). Embedding Features represent each sentence as a summation of its word embeddings (Guo et al., 2014). We further experimented with sentiment features (Habernal and Gurevych, 2015; Anand et al., 2011) and dictionary features (Misra et al., 2015; Rosenthal and McKeown, 2015) but these delivered very poor results and are not reported in this article. The full set of features and their parameters are described in the supplementary material to this article. We experiment with the full feature set, individual feature groups, and feature ablation (all features except for one group). 4.2 Deep Learning Approaches As alternatives to our feature-based systems, we consider three deep learning approaches. The first is the Convolutional Neural Net of Kim (Kim, 2014) which has shown"
D17-1218,J17-1004,1,0.842441,"its, parsing argument structures, and recognizing argument components such as claims—the main focus of this article. Claim identification itself is an important prerequisite for applications such as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eger et al., 2017), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017). Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (MochalesPalau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015) or student essays (Stab and Gurevych, 2017). The problem of generalizing systems or features and their robustness across heterogeneous datasets thus remains fairly unexplored. This situation motivated us to perform a detailed analysis of the concept of claims (as a key component of an argument) in existing argument mining datasets from different d"
D17-1218,D15-1076,0,0.037877,"Missing"
D17-1218,D14-1181,0,0.00585286,"Missing"
D17-1218,N16-1175,0,0.0437333,"Missing"
D17-1218,C14-1141,0,0.0631362,", politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eger et al., 2017), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017). Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (MochalesPalau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015) or student essays (Stab and Gurevych, 2017). The problem of generalizing systems or features and their robustness across heterogeneous datasets thus remains fairly unexplored. This situation motivated us to perform a detailed analysis of the concept of claims (as a key component of an argument) in existing argument mining datasets from different domains.1 We first review and qualitatively analyze six existing publicly available datasets for argument mining (§3), showing that the conceptualizations of claims in these datasets differ largely. In a next step, we analyze the"
D17-1218,P14-5010,0,0.00291693,"how they could be dealt with in practice. Put simply, the task we are trying to solve in the following is: given a sentence, classify whether or not it contains a claim. We opted to model the claim identification task on sentence level, as this is the only way to make all datasets compatible to each other. Different datasets model claim boundaries differently, e.g. MT includes discourse markers within the same sentence, whereas they are excluded in PE. All six datasets described in the previous section have been preprocessed by first segmenting documents into sentences using Stanford CoreNLP (Manning et al., 2014) and then annotating every sentence as claim, if one or more tokens within the sentence were labeled as claim (or major claim in PE). Analogously, each sentence is annotated as non-claim, if none of its tokens were labeled as claim (or major claim). Although our basic units of interest are sentences, we keep the content of the entire document to be able to retrieve information about the context of (non-)claims.3 We are not interested in optimizing the properties of a certain learner for this task, but rather 3 This is true only for the feature-based learners. The neural networks do not have ac"
D17-1218,N15-1046,0,0.0386276,"rt-of-speech and parse tree for each sentence. 4 For the same reason, we do not optimize any hyperparameters for individual learners, unless explicitly stated. 5 Using the liblinear library (Fan et al., 2008). Discourse Features encode information extracted with help of the Penn Discourse Treebank (PDTB) styled end-to-end discourse parser as presented by Lin et al. (2014). Embedding Features represent each sentence as a summation of its word embeddings (Guo et al., 2014). We further experimented with sentiment features (Habernal and Gurevych, 2015; Anand et al., 2011) and dictionary features (Misra et al., 2015; Rosenthal and McKeown, 2015) but these delivered very poor results and are not reported in this article. The full set of features and their parameters are described in the supplementary material to this article. We experiment with the full feature set, individual feature groups, and feature ablation (all features except for one group). 4.2 Deep Learning Approaches As alternatives to our feature-based systems, we consider three deep learning approaches. The first is the Convolutional Neural Net of Kim (Kim, 2014) which has shown to perform excellently on many diverse classification tasks such"
D17-1218,N15-1055,0,0.052016,"Missing"
D17-1218,D15-1050,0,0.0884378,"Missing"
D17-1218,W12-4301,0,0.0692407,"o support with reasons’ (Govier, 2010). Argument mining, a computational counterpart of manual argumentation analysis, is a recent growing sub-field of NLP (Peldszus and Stede, 2013a). ‘Mining’ arguments usually involves several steps like separating argumentative from nonargumentative text units, parsing argument structures, and recognizing argument components such as claims—the main focus of this article. Claim identification itself is an important prerequisite for applications such as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eger et al., 2017), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017). Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (MochalesPalau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015) or student essays (Stab and Gurevych, 2017). The"
D17-1218,W13-2324,0,0.247374,"lthough argumentation scholars provide us with a plethora of often clashing theories and models (van Eemeren et al., 2014). Despite the lack of a precise definition in the contemporary argumentation theory, Toulmin’s influential work on argumentation in the 1950’s introduced a claim as an ‘assertion that deserves our attention’ (Toulmin, 2003, p. 11); recent works describe a claim as ‘a statement that is in dispute and that we are trying to support with reasons’ (Govier, 2010). Argument mining, a computational counterpart of manual argumentation analysis, is a recent growing sub-field of NLP (Peldszus and Stede, 2013a). ‘Mining’ arguments usually involves several steps like separating argumentative from nonargumentative text units, parsing argument structures, and recognizing argument components such as claims—the main focus of this article. Claim identification itself is an important prerequisite for applications such as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eg"
D17-1218,D15-1110,0,0.0770528,"uch as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eger et al., 2017), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017). Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (MochalesPalau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015) or student essays (Stab and Gurevych, 2017). The problem of generalizing systems or features and their robustness across heterogeneous datasets thus remains fairly unexplored. This situation motivated us to perform a detailed analysis of the concept of claims (as a key component of an argument) in existing argument mining datasets from different domains.1 We first review and qualitatively analyze six existing publicly available datasets for argument mining (§3), showing that the conceptualizations of claims in these datasets differ"
D17-1218,P15-1053,0,0.0364649,"argument mining (Moens et al., 2007; Feng and Hirst, 2011; Rooney et al., 2012). The corpus from Habernal and Gurevych (2017) includes user-generated web discourse (WD) such as blog posts, or user comments annotated with claims and premises as well as backings, rebuttals and refutations (αU 0.48) inspired by Toulmin’s model of argument (Toulmin, 2003). The persuasive essay (PE) corpus (Stab and Gurevych, 2017) includes 402 student essays. The scheme comprises major claims, claims and premises at the clause level (αU 0.77). The corpus has been extensively used in the argument mining community (Persing and Ng, 2015; Lippi and Torroni, 2015; Nguyen and Litman, 2016). Biran and Rambow (2011a) annotated claims and premises in online comments (OC) from blog threads of LiveJournal (κ 0.69). In a subsequent work, Biran and Rambow (2011b) applied their annotation scheme to documents from Wikipedia talk pages (WTP) and annotated 118 threads. For our experiments, we consider each user comment in both corpora as a document, which yields 2, 805 documents in the OC corpus and 1, 985 documents in the WTP corpus. Peldszus and Stede (2016) created a corpus of German microtexts (MT) of controlled linguistic and rhetori"
D17-1218,P14-2083,0,0.0203363,"e not interested in optimizing the properties of a certain learner for this task, but rather 3 This is true only for the feature-based learners. The neural networks do not have access to information beyond individual sentences. 2058 want to compare the influence of different types of lexical, syntactical, and other kinds of information across datasets.4 Thus, we used a limited set of learners for our task: a) a standard L2-regularized logistic regression approach with manually defined feature sets5 , which is a simple yet robust and established technique for many text classification problems (Plank et al., 2014; He et al., 2015; Zhang et al., 2016a; Ferreira and Vlachos, 2016); and b) several deep learning approaches, using state-of-the-art neural network architectures. The in-domain experiments were carried out in a 10-fold cross-validation setup with fixed splits into training and test data. As for the crossdomain experiments, we train on the entire data of the source domain and test on the entire data of the target domain. In the domain adaptation terminology, this corresponds to an unsupervised setting. To address class-imbalance in our datasets (see Table 1), we downsample the negative class (n"
D17-1218,reed-etal-2008-language,0,0.165527,"Missing"
D17-1218,W15-4625,0,0.031781,"se tree for each sentence. 4 For the same reason, we do not optimize any hyperparameters for individual learners, unless explicitly stated. 5 Using the liblinear library (Fan et al., 2008). Discourse Features encode information extracted with help of the Penn Discourse Treebank (PDTB) styled end-to-end discourse parser as presented by Lin et al. (2014). Embedding Features represent each sentence as a summation of its word embeddings (Guo et al., 2014). We further experimented with sentiment features (Habernal and Gurevych, 2015; Anand et al., 2011) and dictionary features (Misra et al., 2015; Rosenthal and McKeown, 2015) but these delivered very poor results and are not reported in this article. The full set of features and their parameters are described in the supplementary material to this article. We experiment with the full feature set, individual feature groups, and feature ablation (all features except for one group). 4.2 Deep Learning Approaches As alternatives to our feature-based systems, we consider three deep learning approaches. The first is the Convolutional Neural Net of Kim (Kim, 2014) which has shown to perform excellently on many diverse classification tasks such as sentiment analysis and que"
D17-1218,I13-1023,0,0.0605698,"Missing"
D17-1218,P16-2038,0,0.0594238,"Missing"
D17-1218,D14-1006,1,0.847867,"P (Peldszus and Stede, 2013a). ‘Mining’ arguments usually involves several steps like separating argumentative from nonargumentative text units, parsing argument structures, and recognizing argument components such as claims—the main focus of this article. Claim identification itself is an important prerequisite for applications such as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eger et al., 2017), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017). Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (MochalesPalau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015) or student essays (Stab and Gurevych, 2017). The problem of generalizing systems or features and their robustness across heterogeneous datasets thus remains fairly unexplored. This situation motivated us"
D17-1218,W10-2605,0,0.0762813,"Missing"
D17-1218,W14-2508,0,0.0628573,"); recent works describe a claim as ‘a statement that is in dispute and that we are trying to support with reasons’ (Govier, 2010). Argument mining, a computational counterpart of manual argumentation analysis, is a recent growing sub-field of NLP (Peldszus and Stede, 2013a). ‘Mining’ arguments usually involves several steps like separating argumentative from nonargumentative text units, parsing argument structures, and recognizing argument components such as claims—the main focus of this article. Claim identification itself is an important prerequisite for applications such as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012). Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014; Eger et al., 2017), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017). Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (MochalesPalau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles ("
D17-1218,E17-1105,0,0.0404748,"Missing"
D17-1218,N15-1069,0,0.0543343,"Missing"
D17-1218,N16-3008,0,0.126993,"operties of a certain learner for this task, but rather 3 This is true only for the feature-based learners. The neural networks do not have access to information beyond individual sentences. 2058 want to compare the influence of different types of lexical, syntactical, and other kinds of information across datasets.4 Thus, we used a limited set of learners for our task: a) a standard L2-regularized logistic regression approach with manually defined feature sets5 , which is a simple yet robust and established technique for many text classification problems (Plank et al., 2014; He et al., 2015; Zhang et al., 2016a; Ferreira and Vlachos, 2016); and b) several deep learning approaches, using state-of-the-art neural network architectures. The in-domain experiments were carried out in a 10-fold cross-validation setup with fixed splits into training and test data. As for the crossdomain experiments, we train on the entire data of the source domain and test on the entire data of the target domain. In the domain adaptation terminology, this corresponds to an unsupervised setting. To address class-imbalance in our datasets (see Table 1), we downsample the negative class (nonclaim) both in-domain and cross-dom"
D17-1218,N16-1177,0,0.115384,"operties of a certain learner for this task, but rather 3 This is true only for the feature-based learners. The neural networks do not have access to information beyond individual sentences. 2058 want to compare the influence of different types of lexical, syntactical, and other kinds of information across datasets.4 Thus, we used a limited set of learners for our task: a) a standard L2-regularized logistic regression approach with manually defined feature sets5 , which is a simple yet robust and established technique for many text classification problems (Plank et al., 2014; He et al., 2015; Zhang et al., 2016a; Ferreira and Vlachos, 2016); and b) several deep learning approaches, using state-of-the-art neural network architectures. The in-domain experiments were carried out in a 10-fold cross-validation setup with fixed splits into training and test data. As for the crossdomain experiments, we train on the entire data of the source domain and test on the entire data of the target domain. In the domain adaptation terminology, this corresponds to an unsupervised setting. To address class-imbalance in our datasets (see Table 1), we downsample the negative class (nonclaim) both in-domain and cross-dom"
D17-1218,N16-1178,0,0.106465,"operties of a certain learner for this task, but rather 3 This is true only for the feature-based learners. The neural networks do not have access to information beyond individual sentences. 2058 want to compare the influence of different types of lexical, syntactical, and other kinds of information across datasets.4 Thus, we used a limited set of learners for our task: a) a standard L2-regularized logistic regression approach with manually defined feature sets5 , which is a simple yet robust and established technique for many text classification problems (Plank et al., 2014; He et al., 2015; Zhang et al., 2016a; Ferreira and Vlachos, 2016); and b) several deep learning approaches, using state-of-the-art neural network architectures. The in-domain experiments were carried out in a 10-fold cross-validation setup with fixed splits into training and test data. As for the crossdomain experiments, we train on the entire data of the source domain and test on the entire data of the target domain. In the domain adaptation terminology, this corresponds to an unsupervised setting. To address class-imbalance in our datasets (see Table 1), we downsample the negative class (nonclaim) both in-domain and cross-dom"
D17-1218,D09-1036,0,\N,Missing
D18-1472,P17-2037,0,0.0552652,"Missing"
D18-1472,D14-1082,0,0.0440121,"twork learning, e.g., in Parascandolo et al. (2016), where it was shown to enable faster learning on certain tasks than more established functions. penalized tanh (Xu et al., 2016) has been defined in analogy to the LReLU functions, which can be thought of as “penalizing” the identity function in the negative region. The reported good performance of penalized tanh on CIFAR-100 (Krizhevsky, 2009) lets the authors speculate that the slope of activation functions near the origin may be crucial for learning. linear is the identity function, f (x) = x. cube is the function f (x) = x3 , proposed in Chen and Manning (2014) for an MLP used in dependency parsing. elu (Clevert et al., 2015) has been proposed as (yet another) variant of relu that assumes negative values, making the mean activations more zero-centered. selu is a scaled variant of elu used in Klambauer et al. (2017) in the context of socalled self-normalizing neural nets. Properties of activation functions Many properties of activation functions have been speculated to be crucial for successful learning. Some of these are listed in Table 2, together with brief de4416 Property Description 0 derivative f zero-centered saturating monotonicity range cent"
D18-1472,D17-1070,0,0.0200354,"(6): The AM dataset with original split in train, dev, and test (Eger et al., 2017), and with InferSent input embeddings. (7): the same mini-experiment with Sent2Vec-unigram embeddings. Model We experiment with a multi-layer perceptron (MLP) applied to sentence-level classification tasks. That is, input to the MLP is a sentence or short text, represented as a fixed-size vector embedding. The output of the MLP is a label which classifies the sentence or short text. We use two sentence representation techniques, namely, Sent2Vec (Pagliardini et al., 2018), of dimensionality 600, and InferSent (Conneau et al., 2017), of dimensionality 4096. Our MLP has the form: xi = f (xi−1 · Wi + bi ) y = softmax(xN WN +1 + bN +1 ) where x0 is the input representation, x1 , . . . , xN are hidden layer representations, and y is the output, a probability distribution over the classes in the classification task. Vectors b and matrices W are the learnable parameters of our network. The activation function is given by f and ranges over the choices described in §2. Data We use four sentence classification tasks, namely: movie review classification (MR), subjectivitiy classification (SUBJ), question type classification (TREC)"
D18-1472,P17-1002,1,0.888027,"Missing"
D18-1472,P17-1052,0,0.0227763,"is < 0, we choose it to be m). sin function wins, followed by penalized tanh, maxout and swish. The difference between the best mean function, sin, and the worst, cube, is more than 30pp. This means that using cube is much riskier and requires more careful hyperparameter search compared to sin and the other top performers. 3.2 CNN & Document Classification Model Our second paradigm is document classification using a CNN. This approach has been popularized in NLP by the ground-breaking work of Kim (2014). Even though shallow CNNs do not reach state-of-the-art results on large datasets anymore (Johnson and Zhang, 2017), simple approaches like (shallow) CNNs are still very competitive for smaller datasets (Joulin et al., 2016). Our model operates on token-level and first embeds a sequence of tokens x1 , . . . , xn , represented as 1-hot vectors, into learnable embeddings x1 , . . . , xn . The model then applies 1Dconvolution on top of these embeddings. That is, a filter w of size h takes h successive embeddings xi:i+h−1 , performs a scalar product and obtains a feature ci : ci = f (w · xi:i+h−1 + b). Here, f is the activation function and b is a bias term. We take the number nk of different filters as a hype"
D18-1472,D14-1181,0,0.00399313,"µ and std s; µ = m is the default value from keras for the specific optimizer (if drawn learning rate is < 0, we choose it to be m). sin function wins, followed by penalized tanh, maxout and swish. The difference between the best mean function, sin, and the worst, cube, is more than 30pp. This means that using cube is much riskier and requires more careful hyperparameter search compared to sin and the other top performers. 3.2 CNN & Document Classification Model Our second paradigm is document classification using a CNN. This approach has been popularized in NLP by the ground-breaking work of Kim (2014). Even though shallow CNNs do not reach state-of-the-art results on large datasets anymore (Johnson and Zhang, 2017), simple approaches like (shallow) CNNs are still very competitive for smaller datasets (Joulin et al., 2016). Our model operates on token-level and first embeds a sequence of tokens x1 , . . . , xn , represented as 1-hot vectors, into learnable embeddings x1 , . . . , xn . The model then applies 1Dconvolution on top of these embeddings. That is, a filter w of size h takes h successive embeddings xi:i+h−1 , performs a scalar product and obtains a feature ci : ci = f (w · xi:i+h−1"
D18-1472,P14-2050,0,0.0591638,"Missing"
D18-1472,N18-1049,0,0.0324684,"Missing"
D18-1472,D17-1035,1,0.859144,"softmax(hi V + c) Here, wi are (pre-trained) word embeddings of words wi . Vectors b, c and matrices U, V, W are parameters to be learned during training. The above describes an RNN with only one hidden layer, hi , at each time step, but we consider the generalized form with N ≥ 1 hidden layers; we also choose a bidirectional RNN in which the hidden outputs of a forward RNN and a backward RNN are combined. RNNs are particularly deep We report macro-F1 for mini-experiments (1-4) and accuracy for (5-6). For our RNN implementations, we use the accompanying code of (the state-of-the-art model of) Reimers and Gurevych (2017), which is implemented in keras. The network uses a CRF layer as an output layer. We use a batch size of 32, train for 50 epochs and use a patience of 5 for early stopping. Results Figure 3 shows best and mean results, averaged over all 6 mini-experiments, for each activation function. We exclude prelu and the maxout functions because the keras implementation 4420 does not natively support these activation functions for RNNs. We also exclude the cube function because it performed very badly. it = σ([ht−1 ; xt ] · Wi ), 90 ot = σ([ht−1 ; xt ] · Wo ) 92 80 ct = ft ct−1 + it τ ([ht−1 ; xt ] · Wc"
D18-1472,N18-2006,1,0.878384,"Missing"
D18-1472,J17-3005,1,0.867373,"Missing"
D19-1053,J13-2002,0,0.353775,"Missing"
D19-1053,hovy-etal-2006-automated,0,0.0762672,"lenging. CIDEr (Vedantam et al., 2015) uses tf-idf weighted n-grams for similarity estimation; and SPICE (Anderson et al., 2016) incorporates Summarization A dominant metric for summarization evaluation is ROUGE (Lin, 2004), which measures the degree of lexical overlap between a system summary and a set of reference summaries. Its variants consider overlap of unigrams (-1), bigrams (-2), unigrams and skip bigrams with a maximum gap of 4 words (-SU4), longest common subsequences (-L) and its weighted version (-W-1.2), among others. Metrics such as Pyramid (Nenkova and Passonneau, 2004) and BE (Hovy et al., 2006; 564 scribe our method in detail. synonym matching over scene graphs. Novikova et al. (2017) examine a large number of word- and grammar-based metrics and demonstrate that they only weakly reflect human judgments of system outputs generated by data-driven, end-to-end natural language generation systems. 3 Our MoverScore Meric We have motivated the need for better metrics capable of evaluating disparate NLG tasks. We now describe our metric, namely MoverScore, built upon a combination of (i) contextualized representations of system and reference texts and (ii) a distance between these represen"
D19-1053,P19-1269,0,0.0446107,"(Peyrard et al., 2017; Shimanaka et al., 2018) to improve semantic similarity estimation, replacing lexical overlaps. In contemporaneous work, Zhang et al. (2019) describe a method comparing system and reference texts for semantic similarity leveraging the BERT representations (Devlin et al., 2018), which can be viewed as a special case of our metrics and will be discussed in more depth later. More recently, Clark et al. (2019) present a semantic metric relying on sentence mover’s similarity and the ELMo representations (Peters et al., 2018) and apply them to summarization and essay scoring. Mathur et al. (2019) introduce unsupervised and supervised metrics based on the BERT representations to improve MT evaluation, while Peyrard (2019a) provides a composite score combining redundancy, relevance and informativeness to improve summary evaluation. In this paper, we seek to accurately measure the (dis)similarity between system and reference texts drawing inspiration from contextualized representations and Word Mover’s Distance (WMD; Kusner et al., 2015). WMD finds the “traveling distance” of moving from the word frequency distribution of the system text to that of the reference, which is essential to ca"
D19-1053,W18-6319,0,0.0179801,"er metrics include SentBLEU, NIST, chrF, TER, WER, PER, CDER, and METEOR (Lavie and Agarwal, 2007) that are used and described in the WMT metrics shared task (Bojar et al., 2017; Ma et al., 2018). RUSE (Shimanaka et al., 2018) is a recent effort to improve MT evaluation by training sentence embeddings on large-scale data obtained in other tasks. Additionally, preprocessing reference texts is crucial in MT evaluation, e.g., normalization, tokenization, compound splitting, etc. If not handled properly, different preprocessing strategies can lead to inconsistent results using word-based metrics (Post, 2018). • Our metric outperforms or performs comparably to strong baselines on four text generation tasks including summarization, machine translation, image captioning, and data-to-text generation, suggesting this is a promising direction moving forward. 2 Related Work It is of fundamental importance to design evaluation metrics that can be applied to natural language generation tasks of similar nature, including summarization, machine translation, data-to-text generation, image captioning, and many others. All these tasks involve generating texts of sentence or paragraph length. The system texts a"
D19-1053,J18-3002,0,0.0128134,"e text as well as generating unseen words (See et al., 2017). This aspect is hardly covered by existing metrics. With greater flexibility comes increased demand for unbiased evaluation. Diversity-promoting objectives make it possible to generate diverse natural language descriptions (Li et al., 2016; Wiseman et al., 2018). But standard evaluation metrics including BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) compute the scores based primarily on n-gram co-occurrence statistics, which are originally proposed for diagnostic evaluation of systems but not capable of evaluating text quality (Reiter, 2018), as they are not designed to measure if, and to what extent, the system and reference texts with distinct surface forms have conveyed the same meaning. Recent effort on the applicability of these metrics reveals that while compelling text generation system ascend on standard metrics, the text quality of system output is still hard to be improved (B¨ohm et al., 2019). A robust evaluation metric has a profound impact on the development of text generation systems. A desirable metric compares system output against references based on their semantics rather than surface forms. In this paper we inv"
D19-1053,N04-1019,0,0.885746,"quality of image captions can be challenging. CIDEr (Vedantam et al., 2015) uses tf-idf weighted n-grams for similarity estimation; and SPICE (Anderson et al., 2016) incorporates Summarization A dominant metric for summarization evaluation is ROUGE (Lin, 2004), which measures the degree of lexical overlap between a system summary and a set of reference summaries. Its variants consider overlap of unigrams (-1), bigrams (-2), unigrams and skip bigrams with a maximum gap of 4 words (-SU4), longest common subsequences (-L) and its weighted version (-W-1.2), among others. Metrics such as Pyramid (Nenkova and Passonneau, 2004) and BE (Hovy et al., 2006; 564 scribe our method in detail. synonym matching over scene graphs. Novikova et al. (2017) examine a large number of word- and grammar-based metrics and demonstrate that they only weakly reflect human judgments of system outputs generated by data-driven, end-to-end natural language generation systems. 3 Our MoverScore Meric We have motivated the need for better metrics capable of evaluating disparate NLG tasks. We now describe our metric, namely MoverScore, built upon a combination of (i) contextualized representations of system and reference texts and (ii) a dista"
D19-1053,D15-1222,0,0.124306,"emantic textual similarity measures (Peters et al., 2018; Devlin et al., 2018); but also to accurately reflect to what extent the system text has deviated from the reference, i.e., union(A,B) - intersect(A,B), which is the intuition behind using a distance metric. Metrics based on Continuous Representations Moving beyond traditional metrics, we envision a new generation of automated evaluation metrics comparing system and reference texts based on semantics rather than surface forms to achieve better correlation with human judgments. A number of previous studies exploit static word embeddings (Ng and Abrecht, 2015; Lo, 2017) and trained classifers (Peyrard et al., 2017; Shimanaka et al., 2018) to improve semantic similarity estimation, replacing lexical overlaps. In contemporaneous work, Zhang et al. (2019) describe a method comparing system and reference texts for semantic similarity leveraging the BERT representations (Devlin et al., 2018), which can be viewed as a special case of our metrics and will be discussed in more depth later. More recently, Clark et al. (2019) present a semantic metric relying on sentence mover’s similarity and the ELMo representations (Peters et al., 2018) and apply them to"
D19-1053,P17-1099,0,0.237822,"Missing"
D19-1053,W18-6456,0,0.329942,"trics are commonly used in MT evaluation. Most of these metrics compare system and reference translations based on surface forms such as word/character n-gram overlaps and edit distance, but not the meanings they convey. BLEU (Papineni et al., 2002) is a precision metric measuring how well a system translation overlaps with human reference translations using n-gram co-occurrence statistics. Other metrics include SentBLEU, NIST, chrF, TER, WER, PER, CDER, and METEOR (Lavie and Agarwal, 2007) that are used and described in the WMT metrics shared task (Bojar et al., 2017; Ma et al., 2018). RUSE (Shimanaka et al., 2018) is a recent effort to improve MT evaluation by training sentence embeddings on large-scale data obtained in other tasks. Additionally, preprocessing reference texts is crucial in MT evaluation, e.g., normalization, tokenization, compound splitting, etc. If not handled properly, different preprocessing strategies can lead to inconsistent results using word-based metrics (Post, 2018). • Our metric outperforms or performs comparably to strong baselines on four text generation tasks including summarization, machine translation, image captioning, and data-to-text generation, suggesting this is a p"
D19-1053,P02-1040,0,0.106926,"Central Florida, US zhao@aiphes.tu-darmstadt.de, maxime.peyrard@epfl.ch feiliu@cs.ucf.edu, yang.gao@rhul.ac.uk meyer@ukp.informatik.tu-darmstadt.de eger@aiphes.tu-darmstadt.de Abstract the flexibility to copy content from source text as well as generating unseen words (See et al., 2017). This aspect is hardly covered by existing metrics. With greater flexibility comes increased demand for unbiased evaluation. Diversity-promoting objectives make it possible to generate diverse natural language descriptions (Li et al., 2016; Wiseman et al., 2018). But standard evaluation metrics including BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) compute the scores based primarily on n-gram co-occurrence statistics, which are originally proposed for diagnostic evaluation of systems but not capable of evaluating text quality (Reiter, 2018), as they are not designed to measure if, and to what extent, the system and reference texts with distinct surface forms have conveyed the same meaning. Recent effort on the applicability of these metrics reveals that while compelling text generation system ascend on standard metrics, the text quality of system output is still hard to be improved (B¨ohm et al., 2019). A robust ev"
D19-1053,N18-1202,0,0.554801,"latedness between abstractive summaries and their references, as a system abstract can convey the same meaning using different surface forms. Furthermore, large-scale summarization datasets such as CNN/Daily Mail (Hermann et al., 2015) and Newsroom (Grusky et al., 2018) use a single reference summary, making it harder to obtain unbiased results when only lexical overlap is considered during summary evaluation. multiple natural language generation tasks. Our new metric quantifies the semantic distance between system and reference texts by harnessing the power of contextualized representations (Peters et al., 2018; Devlin et al., 2018) and a powerful distance metric (Rubner et al., 2000) for better content matching. Our contributions can be summarized as follows: • We formulate the problem of evaluating generation systems as measuring the semantic distance between system and reference texts, assuming powerful continuous representations can encode any type of semantic and syntactic deviations. • We investigate the effectiveness of existing contextualized representations and Earth Mover’s Distance (Rubner et al., 2000) for comparing system predictions and reference texts, leading to our new automated eva"
L16-1239,D12-1133,0,0.0245951,"ost recent class of taggers is characterized by the availability to include word representations learned from unlabeled data, the possibility to apply feature-rich models to problems with large output spaces, and/or by making use of deep (rather than shallow) models such as neural networks that can in addition function without handcrafting features. In this work, we consider the following part-of-speech tagging systems, listed by the order of their year of publication: TreeTagger (Schmid, 1994), TnT (Brants, 2000), Stanford tagger (Toutanova et al., 2003), Lapos (Tsuruoka et al., 2011), Mate (Bohnet and Nivre, 2012). We also include the OpenNLPTagger, an official Apache project.4 For these systems, we refer to the original works for descriptions. Among the most recent generation of taggers, we consider the MarMoT (M¨uller et al., 2013) tagger, which implements a higher order CRF with approximations such that it can deal with large output spaces. In addition, MarMoT can be trained to fire on the predictions of lexical resources as well as on word embeddings, real vectorvalued representations of words. FLORS (Schnabel and Sch¨utze, 2014) tags a given word by constructing a feature vector representation of"
L16-1239,A00-1031,0,0.867929,"ion retrieval, knowledge extraction, or semantic analysis. In morphologically rich languages such as German and Latin, both problems are non-trivial due to the variability of lexical forms. This results both in large tagsets for POS tagging — which list such inflectional categories as case, gender, degree, etc., besides coarse-grained POS labels — and a large number of (potentially unseen) forms associated with each lemma. In this work, we survey tagging and lemmatization techniques for the two languages mentioned. Our survey includes both older, such as the TreeTagger (Schmid, 1994) and TnT (Brants, 2000), and more modern approaches to tagging and lemmatization. Although our expectation is clearly that technology steadily improves with time, it is apropri not obvious how large the gap between older and more modern approaches is, and also what the ordering of the most recent generation of systems is. We test our systems under the following requirements:1 • Ideally, we would want a learned system to perform well on the distribution (a specific text genre, historical language variant, etc.) on which it has been trained (in-domain (ID)) but also to perform decently on corpora of similar but differ"
L16-1239,D08-1113,0,0.0149464,"ncies between POS tagging and lemmatization, which should substantially improve its performance relative to approaches where the tasks are treated independently (M¨uller et al., 2015). • Finally, run times of systems may be of considerable interest for practitioners. Therefore, we include both training and testing time estimations of the different techniques. 2. Lemmatization We view lemmatization as the problem of transforming a word form into its canoncial form, or lemma. In a machine learning context,2 lemmatization has e.g. been considered as a character-level string transduction process (Dreyer et al., 2008; Nicolai et al., 2015; Eger, 2015), a prefix and suffix transformation problem (Jursic et al., 2010; Gesmundo and Samardzic, 2012) or as a pattern matching task (Durrett and DeNero, 2013; Ahlberg et al., 2014). While characterlevel string transducers may yield excellent results (Nicolai et al., 2015), particularly when trained and tested on lists of words randomly extracted from a lexicon (Eger, 2015), they tend to be slower to learn and typically consider the lemmatization problem in isolation, ignoring contextual word form cues.3 In this work, we experiment with two approaches to lemmatizat"
L16-1239,N13-1138,0,0.0192683,"). • Finally, run times of systems may be of considerable interest for practitioners. Therefore, we include both training and testing time estimations of the different techniques. 2. Lemmatization We view lemmatization as the problem of transforming a word form into its canoncial form, or lemma. In a machine learning context,2 lemmatization has e.g. been considered as a character-level string transduction process (Dreyer et al., 2008; Nicolai et al., 2015; Eger, 2015), a prefix and suffix transformation problem (Jursic et al., 2010; Gesmundo and Samardzic, 2012) or as a pattern matching task (Durrett and DeNero, 2013; Ahlberg et al., 2014). While characterlevel string transducers may yield excellent results (Nicolai et al., 2015), particularly when trained and tested on lists of words randomly extracted from a lexicon (Eger, 2015), they tend to be slower to learn and typically consider the lemmatization problem in isolation, ignoring contextual word form cues.3 In this work, we experiment with two approaches to lemmatization, both based on prefix and suffix transformations. LemmaGen (Jursic et al., 2010) learns ‘ripple down rules’ (Compton and Jansen, 1988), that is, tree-like decision 2 Alternatively, le"
L16-1239,W15-3716,1,0.894242,"Missing"
L16-1239,P12-2072,0,0.357689,"ere the tasks are treated independently (M¨uller et al., 2015). • Finally, run times of systems may be of considerable interest for practitioners. Therefore, we include both training and testing time estimations of the different techniques. 2. Lemmatization We view lemmatization as the problem of transforming a word form into its canoncial form, or lemma. In a machine learning context,2 lemmatization has e.g. been considered as a character-level string transduction process (Dreyer et al., 2008; Nicolai et al., 2015; Eger, 2015), a prefix and suffix transformation problem (Jursic et al., 2010; Gesmundo and Samardzic, 2012) or as a pattern matching task (Durrett and DeNero, 2013; Ahlberg et al., 2014). While characterlevel string transducers may yield excellent results (Nicolai et al., 2015), particularly when trained and tested on lists of words randomly extracted from a lexicon (Eger, 2015), they tend to be slower to learn and typically consider the lemmatization problem in isolation, ignoring contextual word form cues.3 In this work, we experiment with two approaches to lemmatization, both based on prefix and suffix transformations. LemmaGen (Jursic et al., 2010) learns ‘ripple down rules’ (Compton and Jansen"
L16-1239,D15-1025,0,0.0297568,"Missing"
L16-1239,L16-1677,1,0.87418,"Missing"
L16-1239,D13-1032,0,0.107869,"Missing"
L16-1239,D15-1272,0,0.313835,"Missing"
L16-1239,N15-1093,0,0.0185071,"ging and lemmatization, which should substantially improve its performance relative to approaches where the tasks are treated independently (M¨uller et al., 2015). • Finally, run times of systems may be of considerable interest for practitioners. Therefore, we include both training and testing time estimations of the different techniques. 2. Lemmatization We view lemmatization as the problem of transforming a word form into its canoncial form, or lemma. In a machine learning context,2 lemmatization has e.g. been considered as a character-level string transduction process (Dreyer et al., 2008; Nicolai et al., 2015; Eger, 2015), a prefix and suffix transformation problem (Jursic et al., 2010; Gesmundo and Samardzic, 2012) or as a pattern matching task (Durrett and DeNero, 2013; Ahlberg et al., 2014). While characterlevel string transducers may yield excellent results (Nicolai et al., 2015), particularly when trained and tested on lists of words randomly extracted from a lexicon (Eger, 2015), they tend to be slower to learn and typically consider the lemmatization problem in isolation, ignoring contextual word form cues.3 In this work, we experiment with two approaches to lemmatization, both based on pre"
L16-1239,W96-0213,0,0.596937,"nd the last character is replaced by en. This compact encoding allows to view lemmatization as a classification problem where the size of the output space is relatively small, on the orders of at most hundreds or thousands of labels. Moreover, lemmatization can then also be treated as a sequence labeling problem, where dependence between subsequent labels may be taken into account. 3. POS tagging POS tagging (or sequence labeling) has witnessed several milestones such as including dependencies between output labels (as in Markov models such as HMMs or CRFs), the broad use of lexical features (Ratnaparkhi, 1996; Toutanova et al., 2003), or the concept of the margin introduced in SVMs. The most recent class of taggers is characterized by the availability to include word representations learned from unlabeled data, the possibility to apply feature-rich models to problems with large output spaces, and/or by making use of deep (rather than shallow) models such as neural networks that can in addition function without handcrafting features. In this work, we consider the following part-of-speech tagging systems, listed by the order of their year of publication: TreeTagger (Schmid, 1994), TnT (Brants, 2000)"
L16-1239,Q14-1002,0,0.0446971,"Missing"
L16-1239,N03-1033,0,0.0203684,"er is replaced by en. This compact encoding allows to view lemmatization as a classification problem where the size of the output space is relatively small, on the orders of at most hundreds or thousands of labels. Moreover, lemmatization can then also be treated as a sequence labeling problem, where dependence between subsequent labels may be taken into account. 3. POS tagging POS tagging (or sequence labeling) has witnessed several milestones such as including dependencies between output labels (as in Markov models such as HMMs or CRFs), the broad use of lexical features (Ratnaparkhi, 1996; Toutanova et al., 2003), or the concept of the margin introduced in SVMs. The most recent class of taggers is characterized by the availability to include word representations learned from unlabeled data, the possibility to apply feature-rich models to problems with large output spaces, and/or by making use of deep (rather than shallow) models such as neural networks that can in addition function without handcrafting features. In this work, we consider the following part-of-speech tagging systems, listed by the order of their year of publication: TreeTagger (Schmid, 1994), TnT (Brants, 2000), Stanford tagger (Toutan"
L16-1239,W11-0328,0,0.0646816,"rgin introduced in SVMs. The most recent class of taggers is characterized by the availability to include word representations learned from unlabeled data, the possibility to apply feature-rich models to problems with large output spaces, and/or by making use of deep (rather than shallow) models such as neural networks that can in addition function without handcrafting features. In this work, we consider the following part-of-speech tagging systems, listed by the order of their year of publication: TreeTagger (Schmid, 1994), TnT (Brants, 2000), Stanford tagger (Toutanova et al., 2003), Lapos (Tsuruoka et al., 2011), Mate (Bohnet and Nivre, 2012). We also include the OpenNLPTagger, an official Apache project.4 For these systems, we refer to the original works for descriptions. Among the most recent generation of taggers, we consider the MarMoT (M¨uller et al., 2013) tagger, which implements a higher order CRF with approximations such that it can deal with large output spaces. In addition, MarMoT can be trained to fire on the predictions of lexical resources as well as on word embeddings, real vectorvalued representations of words. FLORS (Schnabel and Sch¨utze, 2014) tags a given word by constructing a fe"
L16-1239,D15-1155,0,0.0351593,"Missing"
N18-2006,N16-1165,0,0.150041,"Missing"
N18-2006,E17-1005,0,0.321473,"stics gree training a system to solve several conceptually different AM tasks jointly improves performance over learning in isolation, (2) compare performance gains across different dataset sizes, and (3) do so across various domains. Our findings show that MTL is helpful for AM—particularly in data sparsity settings—when treating other AM tasks as auxiliary.1 2 out of five semantic (i.e., higher level) tasks that they study. We are among the first to perform a structured investigation of MTL for higher-level pragmatic tasks, which are thought to be much more challenging than syntactic tasks (Alonso and Plank, 2017), and in particular, explore it for AM in cross-domain settings. 3 Related Work Experiments Data We experiment with six datasets for argument component identification, i.e. the token-level segmentation and typing of components. These datasets are all of different sizes, have different average text lengths, and different argument component types and label distributions, as summarized in Table 1. We only choose datasets containing both argumentative components and nonargumentative text. Claims are available in five of six datasets, and all datasets have premises (resp. “justification”), although"
N18-2006,P17-2054,0,0.0330385,"knowledge” is by means of multi-task learning (MTL), a paradigm that dates back to the 1990s (Caruana, 1993, 1996), but has only recently gained large attention (Collobert et al., 2011; Søgaard and Goldberg, 2016; Hashimoto et al., 2017). The idea behind MTL is to learn several tasks jointly, similarly to human learning, so that tasks serve as mutual sources of “inductive bias” for one another. MTL has been reported particularly beneficial when tasks exhibit “natural hierarchies” (Søgaard and Goldberg, 2016) or when the amount of training data for the main task is sparse (Benton et al., 2017; Augenstein and Søgaard, 2017), where the auxiliary tasks may act as regularizers to prevent overfitting (Ruder et al., 2017). The latter is precisely the scenario most relevant to us. In this paper, we (1) investigate to which deWe investigate whether and where multitask learning (MTL) can improve performance on NLP problems related to argumentation mining (AM), in particular argument component identification. Our results show that MTL performs particularly well (and better than single-task learning) when little training data is available for the main task, a common scenario in AM. Our findings challenge previous assumpti"
N18-2006,E17-2026,0,0.057282,"Missing"
N18-2006,C16-1013,0,0.0371812,"corpora. We use six such corpora, described in more detail in Section 3. On the modeling side, Stab and Gurevych (2017) and Persing and Ng (2016) rely on pipeline approaches for AM, combining parts of the pipeline using integer linear programming (ILP). Eger et al. (2017) propose neural end-to-end models for AM. While Daxenberger et al. (2017) show that there is little consensus on the conceptualization of a claim across AM corpora, Al-Khatib et al. (2016) use distant supervision to overcome domain gaps for identifying (non-)argumentative text. MTL has been applied in many different settings. Bollmann and Søgaard (2016) and Peng and Dredze (2017) use data from different domains as different tasks and thereby improve historical spelling normalization and Chinese word segmentation and NER, respectively. Plank et al. (2016) apply an MTL setup to POS tagging across 22 different languages, where the auxiliary task is to predict token frequency. Eger et al. (2017) explore sub-tasks (such as component identification) of a complex AM tagging problem (including relations between components) as auxiliaries and find that this improves performances. However, they stay within one single domain and dataset, and thus their"
N18-2006,P17-2037,0,0.0541036,"segmentation and NER, respectively. Plank et al. (2016) apply an MTL setup to POS tagging across 22 different languages, where the auxiliary task is to predict token frequency. Eger et al. (2017) explore sub-tasks (such as component identification) of a complex AM tagging problem (including relations between components) as auxiliaries and find that this improves performances. However, they stay within one single domain and dataset, and thus their approach does not address the question how new AM datasets with sparse data can profit from existing AM resources. Conceptually closest to our work, Braud et al. (2017) leverage data from different languages as well as different domains in order to improve discourse parsing. While MTL was shown effective for syntactic tasks under certain conditions (Søgaard and Goldberg, 2016), Alonso and Plank (2017) find that MTL does not improve performances in four Approach Due to the difference in annotations used in the different datasets, we consider each dataset as a separate AM task. We treat all of them as sequence tagging problems, where predicting BIO tags (argument segmentation) and argument component types (component classification) is framed as a joint task. T"
N18-2006,N16-1175,0,0.0695824,"Missing"
N18-2006,C16-1179,0,0.022509,"ience Technische Universit¨at Darmstadt www.ukp.tu-darmstadt.de Abstract reliably in texts (Habernal and Gurevych, 2017). To tackle AM in a new domain or develop new AM tasks, it may thus not be possible to create large datasets as required by most state-of-the-art machine learning approaches. On the other hand, the different conceptualizations of argumentation resulted in AM corpora with different argument component types, with very little conceptual overlap between some of these corpora (Daxenberger et al., 2017). This distinguishes AM from more established NLP tasks like discourse parsing (Braud et al., 2016) and makes it particularly challenging. Therefore, a natural question is how to handle new AM datasets in a new domain and with sparse data. Here, we investigate how existing AM datasets from different domains and with different conceptualizations of arguments can be leveraged to tackle these challenges. More precisely, we study whether conceptually diverse AM datasets from different domains can help deal with new AM datasets when data is limited. A promising direction to incorporate existing datasets as “auxiliary knowledge” is by means of multi-task learning (MTL), a paradigm that dates back"
N18-2006,D17-1142,0,0.0553516,"Missing"
N18-2006,D17-1218,1,0.908083,"er, Tobias Kahse, Iryna Gurevych Ubiquitous Knowledge Processing Lab (UKP-TUDA) Department of Computer Science Technische Universit¨at Darmstadt www.ukp.tu-darmstadt.de Abstract reliably in texts (Habernal and Gurevych, 2017). To tackle AM in a new domain or develop new AM tasks, it may thus not be possible to create large datasets as required by most state-of-the-art machine learning approaches. On the other hand, the different conceptualizations of argumentation resulted in AM corpora with different argument component types, with very little conceptual overlap between some of these corpora (Daxenberger et al., 2017). This distinguishes AM from more established NLP tasks like discourse parsing (Braud et al., 2016) and makes it particularly challenging. Therefore, a natural question is how to handle new AM datasets in a new domain and with sparse data. Here, we investigate how existing AM datasets from different domains and with different conceptualizations of arguments can be leveraged to tackle these challenges. More precisely, we study whether conceptually diverse AM datasets from different domains can help deal with new AM datasets when data is limited. A promising direction to incorporate existing dat"
N18-2006,P17-1002,1,0.926349,"nt and that MTL is difficult for semantic or higher-level tasks. 1 Introduction Computational argumentation mining (AM) deals with the automatic identification of argumentative structures within natural language. This can be beneficial in many applications such as summarizing arguments in texts to improve comprehensibility for end-users, or information retrieval and extraction (Persing and Ng, 2016). A common task is to segment a text into argumentative and nonargumentative components and identify the type of argumentative components. As an illustration, consider the (simplified) example from Eger et al. (2017): “Since [it killed many marine lives]Premise [tourism has threatened nature]Claim .” Here, the non-argumentative token “Since” is followed by two argumentative components: a premise that supports a claim. Argumentation is highly subjective and conceptualized in different ways (Peldszus and Stede, 2013; Al-Khatib et al., 2017). On the one hand, this implies that creating reliable ground-truth datasets for AM is costly, as it requires trained annotators. However, even trained annotators have problems identifying and classifying arguments 35 Proceedings of NAACL-HLT 2018, pages 35–41 c New Orlea"
N18-2006,D14-1162,0,0.0870575,"cannot be attributed to more available data. Figure 1 shows the general trends of our results. For each dataset, the figure plots the difference between normalized MTL and normalized STL macro-F1 scores (MTLnorm (k)−STLnorm (k)) for k = 1K, 6K, 12K, 21K training data points for the main task. For each specific dataset, the normalized macro-F1 score is defined as σnorm (k) = σ(k) STL(1K) , where σ(k) is the original macro-F1 score and STL(1K) denotes the STL score for 1K trainHyperparameter optimization For each sparsity scenario and dataset we train 50 STL/MTL systems using GloVe embeddings (Pennington et al., 2014) and 50 using the embeddings by Komninos and Manandhar (2016). For each run we randomly choose a layout with either one hidden layer of h ∈ {50, 100, 150} units or two layers of 100 units as well as variational dropout rates between 0.2 and 0.5 for the input layer and for the hidden units. 2 Or more, since whole documents are added to the training set until the sum of tokens is at least 21K. Similarly for smaller training and dev sets. 3 37 As implemented in scikit-learn (Pedregosa et al., 2011). Dataset 21K 12K 6K 1K 0.5 var – STL var – MTL var – BL 43.34 47.39 30.45 42.85 45.63 27.35 38.89 4"
N18-2006,N16-1164,0,0.209498,"and better than single-task learning) when little training data is available for the main task, a common scenario in AM. Our findings challenge previous assumptions that conceptualizations across AM datasets are divergent and that MTL is difficult for semantic or higher-level tasks. 1 Introduction Computational argumentation mining (AM) deals with the automatic identification of argumentative structures within natural language. This can be beneficial in many applications such as summarizing arguments in texts to improve comprehensibility for end-users, or information retrieval and extraction (Persing and Ng, 2016). A common task is to segment a text into argumentative and nonargumentative components and identify the type of argumentative components. As an illustration, consider the (simplified) example from Eger et al. (2017): “Since [it killed many marine lives]Premise [tourism has threatened nature]Claim .” Here, the non-argumentative token “Since” is followed by two argumentative components: a premise that supports a claim. Argumentation is highly subjective and conceptualized in different ways (Peldszus and Stede, 2013; Al-Khatib et al., 2017). On the one hand, this implies that creating reliable g"
N18-2006,J17-1004,1,0.835779,"Missing"
N18-2006,P16-2067,0,0.0290742,"using integer linear programming (ILP). Eger et al. (2017) propose neural end-to-end models for AM. While Daxenberger et al. (2017) show that there is little consensus on the conceptualization of a claim across AM corpora, Al-Khatib et al. (2016) use distant supervision to overcome domain gaps for identifying (non-)argumentative text. MTL has been applied in many different settings. Bollmann and Søgaard (2016) and Peng and Dredze (2017) use data from different domains as different tasks and thereby improve historical spelling normalization and Chinese word segmentation and NER, respectively. Plank et al. (2016) apply an MTL setup to POS tagging across 22 different languages, where the auxiliary task is to predict token frequency. Eger et al. (2017) explore sub-tasks (such as component identification) of a complex AM tagging problem (including relations between components) as auxiliaries and find that this improves performances. However, they stay within one single domain and dataset, and thus their approach does not address the question how new AM datasets with sparse data can profit from existing AM resources. Conceptually closest to our work, Braud et al. (2017) leverage data from different langua"
N18-2006,S18-1121,1,0.881689,"Missing"
N18-2006,D17-1206,0,0.0267578,"data. Here, we investigate how existing AM datasets from different domains and with different conceptualizations of arguments can be leveraged to tackle these challenges. More precisely, we study whether conceptually diverse AM datasets from different domains can help deal with new AM datasets when data is limited. A promising direction to incorporate existing datasets as “auxiliary knowledge” is by means of multi-task learning (MTL), a paradigm that dates back to the 1990s (Caruana, 1993, 1996), but has only recently gained large attention (Collobert et al., 2011; Søgaard and Goldberg, 2016; Hashimoto et al., 2017). The idea behind MTL is to learn several tasks jointly, similarly to human learning, so that tasks serve as mutual sources of “inductive bias” for one another. MTL has been reported particularly beneficial when tasks exhibit “natural hierarchies” (Søgaard and Goldberg, 2016) or when the amount of training data for the main task is sparse (Benton et al., 2017; Augenstein and Søgaard, 2017), where the auxiliary tasks may act as regularizers to prevent overfitting (Ruder et al., 2017). The latter is precisely the scenario most relevant to us. In this paper, we (1) investigate to which deWe inves"
N18-2006,reed-etal-2008-language,0,0.163321,"Missing"
N18-2006,D17-1035,1,0.828917,"he different datasets, we consider each dataset as a separate AM task. We treat all of them as sequence tagging problems, where predicting BIO tags (argument segmentation) and argument component types (component classification) is framed as a joint task. This is achieved through token-level BIO tagging with the label set {O} ∪ {B, I} × T , where T is a dataset specific set of argument component types, e.g. T = {claim, premise, . . .}. Thus, the overall number of tags in each dataset is twice the number of non-“O” component types plus one (2 · |T |+ 1). We use the state-of-the-art framework by Reimers and Gurevych (2017) for both single-task learning (STL) and MTL. It employs a bidirectional LSTM (BILSTM) model with a CRF layer over individual LSTM outputs to account for label dependencies. We use nadam as optimizer. For MTL, the recurrent layers of the deep BILSTM are shared by all tasks, with a separate CRF layer for each task. All tasks terminate at the same level. The main task determines the number of mini-batches used for training, i.e. in every iteration the main task is trained on all its mini-batches and all other 1 The code and data used for our experiments are available from https://github.com/UKPL"
N18-2006,P16-2038,0,0.269499,"new domain and with sparse data. Here, we investigate how existing AM datasets from different domains and with different conceptualizations of arguments can be leveraged to tackle these challenges. More precisely, we study whether conceptually diverse AM datasets from different domains can help deal with new AM datasets when data is limited. A promising direction to incorporate existing datasets as “auxiliary knowledge” is by means of multi-task learning (MTL), a paradigm that dates back to the 1990s (Caruana, 1993, 1996), but has only recently gained large attention (Collobert et al., 2011; Søgaard and Goldberg, 2016; Hashimoto et al., 2017). The idea behind MTL is to learn several tasks jointly, similarly to human learning, so that tasks serve as mutual sources of “inductive bias” for one another. MTL has been reported particularly beneficial when tasks exhibit “natural hierarchies” (Søgaard and Goldberg, 2016) or when the amount of training data for the main task is sparse (Benton et al., 2017; Augenstein and Søgaard, 2017), where the auxiliary tasks may act as regularizers to prevent overfitting (Ruder et al., 2017). The latter is precisely the scenario most relevant to us. In this paper, we (1) invest"
N18-2006,J17-3005,1,0.650405,"tive text. Claims are available in five of six datasets, and all datasets have premises (resp. “justification”), although it is unclear how large the conceptual overlap is across datasets. Further component types are idiosyncractic. hotel has the largest number of types, namely, six. Most datasets also come with further information, e.g. relations between argument components, which are not considered here. AM is a relatively new field in NLP. Hence, a lot of related work revolves around creating new corpora. We use six such corpora, described in more detail in Section 3. On the modeling side, Stab and Gurevych (2017) and Persing and Ng (2016) rely on pipeline approaches for AM, combining parts of the pipeline using integer linear programming (ILP). Eger et al. (2017) propose neural end-to-end models for AM. While Daxenberger et al. (2017) show that there is little consensus on the conceptualization of a claim across AM corpora, Al-Khatib et al. (2016) use distant supervision to overcome domain gaps for identifying (non-)argumentative text. MTL has been applied in many different settings. Bollmann and Søgaard (2016) and Peng and Dredze (2017) use data from different domains as different tasks and thereby i"
N18-2006,D17-1141,0,\N,Missing
N18-5005,W17-5115,0,0.023178,"hey provide no specialized support for them. Despite its obvious applications, argument search has attracted relatively little attention in the argument mining community. In this paper, we present ArgumenText, which we believe is the first system for topic-relevant argument search in heterogeneous texts. It takes a large collection of arbitrary Web texts, automatically identifies arguments relevant to a given topic, classifies them as “pro” or “con”, and 2 Related Work Most existing approaches consider argument mining at the discourse level and address tasks like argument unit identification (Ajjour et al., 2017), component classification (Mochales-Palau and Moens, 2009), or argument structure identification (Eger et al., 2017). These approaches focus on recognizing arguments within a single text but do not consider relevance to user-defined topics. Until now, there has been little work on identifying topic-relevant arguments. Wachsmuth et al. (2017) present a generic framework for argument search that relies on pre-structured arguments from debate portals. Levy et al. (2014) present a system designed specifically for detecting topic-relevant claims from Wikipedia, which was later extended to mine sup"
N18-5005,W14-5201,1,0.894337,"Missing"
N18-5005,D15-1050,0,0.102582,"hales-Palau and Moens, 2009), or argument structure identification (Eger et al., 2017). These approaches focus on recognizing arguments within a single text but do not consider relevance to user-defined topics. Until now, there has been little work on identifying topic-relevant arguments. Wachsmuth et al. (2017) present a generic framework for argument search that relies on pre-structured arguments from debate portals. Levy et al. (2014) present a system designed specifically for detecting topic-relevant claims from Wikipedia, which was later extended to mine supporting statements for claims (Rinott et al., 2015). The MARGOT system (Lippi and Torroni, 2015) is trained on Wikipedia data and extracts claims and evidence from user-provided texts. However, all these systems focus on specific text types and are not yet able to extract arguments 21 Proceedings of NAACL-HLT 2018: Demonstrations, pages 21–25 c New Orleans, Louisiana, June 2 - 4, 2018. 2018 Association for Computational Linguistics Online processing Offline processing Segmented documents Sentence segmentation Indexing Document Retrieval Topic-relevant documents Argument Recognition Pro and con arguments Web-Interface topic topic Documents Apac"
N18-5005,P17-1002,1,0.848666,"little attention in the argument mining community. In this paper, we present ArgumenText, which we believe is the first system for topic-relevant argument search in heterogeneous texts. It takes a large collection of arbitrary Web texts, automatically identifies arguments relevant to a given topic, classifies them as “pro” or “con”, and 2 Related Work Most existing approaches consider argument mining at the discourse level and address tasks like argument unit identification (Ajjour et al., 2017), component classification (Mochales-Palau and Moens, 2009), or argument structure identification (Eger et al., 2017). These approaches focus on recognizing arguments within a single text but do not consider relevance to user-defined topics. Until now, there has been little work on identifying topic-relevant arguments. Wachsmuth et al. (2017) present a generic framework for argument search that relies on pre-structured arguments from debate portals. Levy et al. (2014) present a system designed specifically for detecting topic-relevant claims from Wikipedia, which was later extended to mine supporting statements for claims (Rinott et al., 2015). The MARGOT system (Lippi and Torroni, 2015) is trained on Wikipe"
N18-5005,L16-1146,1,0.832586,"na, June 2 - 4, 2018. 2018 Association for Computational Linguistics Online processing Offline processing Segmented documents Sentence segmentation Indexing Document Retrieval Topic-relevant documents Argument Recognition Pro and con arguments Web-Interface topic topic Documents Apache UIMA Tensorflow / Keras Elasticsearch HTML / Javascript User Figure 1: System architecture. from a large collection of arbitrary texts. The approach most similar to ours, introduced by Hua and Wang (2017), extracts claim-relevant arguments from different text types, but is limited to sentential “pro” arguments. Habernal et al. (2016) for de-duplication, boilerplate removal using jusText (Pomikálek, 2011), and language detection.2 This left us with 400 million heterogeneous plain-text documents in English, with an overall size of 683 GiB. 3 3.2 System Description Each document is segmented into sentences with an Apache UIMA pipeline using components from DKPro Core (Eckart de Castilho and Gurevych, 2014). To facilitate processing of other languages in future work, we chose Apache OpenNLP which currently supports six languages. The modular nature of our setup allows us to easily integrate other sentence segmentation methods"
N18-5005,N13-1132,0,0.120427,"Missing"
N18-5005,P17-2032,0,0.0493008,"s and are not yet able to extract arguments 21 Proceedings of NAACL-HLT 2018: Demonstrations, pages 21–25 c New Orleans, Louisiana, June 2 - 4, 2018. 2018 Association for Computational Linguistics Online processing Offline processing Segmented documents Sentence segmentation Indexing Document Retrieval Topic-relevant documents Argument Recognition Pro and con arguments Web-Interface topic topic Documents Apache UIMA Tensorflow / Keras Elasticsearch HTML / Javascript User Figure 1: System architecture. from a large collection of arbitrary texts. The approach most similar to ours, introduced by Hua and Wang (2017), extracts claim-relevant arguments from different text types, but is limited to sentential “pro” arguments. Habernal et al. (2016) for de-duplication, boilerplate removal using jusText (Pomikálek, 2011), and language detection.2 This left us with 400 million heterogeneous plain-text documents in English, with an overall size of 683 GiB. 3 3.2 System Description Each document is segmented into sentences with an Apache UIMA pipeline using components from DKPro Core (Eckart de Castilho and Gurevych, 2014). To facilitate processing of other languages in future work, we chose Apache OpenNLP which"
N18-5005,C14-1141,0,0.185329,"Most existing approaches consider argument mining at the discourse level and address tasks like argument unit identification (Ajjour et al., 2017), component classification (Mochales-Palau and Moens, 2009), or argument structure identification (Eger et al., 2017). These approaches focus on recognizing arguments within a single text but do not consider relevance to user-defined topics. Until now, there has been little work on identifying topic-relevant arguments. Wachsmuth et al. (2017) present a generic framework for argument search that relies on pre-structured arguments from debate portals. Levy et al. (2014) present a system designed specifically for detecting topic-relevant claims from Wikipedia, which was later extended to mine supporting statements for claims (Rinott et al., 2015). The MARGOT system (Lippi and Torroni, 2015) is trained on Wikipedia data and extracts claims and evidence from user-provided texts. However, all these systems focus on specific text types and are not yet able to extract arguments 21 Proceedings of NAACL-HLT 2018: Demonstrations, pages 21–25 c New Orleans, Louisiana, June 2 - 4, 2018. 2018 Association for Computational Linguistics Online processing Offline processing"
N18-5005,W17-5106,0,0.0891137,"trary Web texts, automatically identifies arguments relevant to a given topic, classifies them as “pro” or “con”, and 2 Related Work Most existing approaches consider argument mining at the discourse level and address tasks like argument unit identification (Ajjour et al., 2017), component classification (Mochales-Palau and Moens, 2009), or argument structure identification (Eger et al., 2017). These approaches focus on recognizing arguments within a single text but do not consider relevance to user-defined topics. Until now, there has been little work on identifying topic-relevant arguments. Wachsmuth et al. (2017) present a generic framework for argument search that relies on pre-structured arguments from debate portals. Levy et al. (2014) present a system designed specifically for detecting topic-relevant claims from Wikipedia, which was later extended to mine supporting statements for claims (Rinott et al., 2015). The MARGOT system (Lippi and Torroni, 2015) is trained on Wikipedia data and extracts claims and evidence from user-provided texts. However, all these systems focus on specific text types and are not yet able to extract arguments 21 Proceedings of NAACL-HLT 2018: Demonstrations, pages 21–25"
N19-1165,N18-1202,0,0.137732,"Missing"
N19-1165,D17-1035,1,0.826645,"P I-NP . . . toxic, obscene, insult 5K/1K/1K 212K/44K/47K 212K/44K/47K 149K/10K/64K Table 4: NLP tasks considered in this work, along with (perturbed) examples and data split statistics. s∗ (p) 2009). We frame G2P as a sequence tagging task. To do so, we first hard-align input and output sequences using a 1-0,1-1,1-2 alignment scheme (Schnober et al., 2016) in which an input character is matched with zero, one, or two output characters. Once this preprocessing is done, input and output sequences have equal lengths and we can apply a standard BiLSTM on character-level to the aligned sequences (Reimers and Gurevych, 2017). 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 POS Chunk G2P TC 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 p POS & Chunking: We consider two word-level tasks. POS tagging associates each token with its corresponding word class (e.g., noun, adjective, verb). Chunking groups words into syntactic chunks such as noun and verb phrases (NP and VP), assigning a unique tag to each word, which encodes the position and type of the syntactic constituent, e.g., begin-noun-phrase (B-NP). We use the training, dev and test splits provided by the CoNLL-2000 shared task (Sang and Buchholz, 2000) and use the same BiLSTM"
N19-1165,W00-0726,0,\N,Missing
N19-1165,P14-5010,0,\N,Missing
N19-1165,D11-1006,0,\N,Missing
N19-1165,D13-1032,0,\N,Missing
N19-1165,N16-1175,0,\N,Missing
N19-1165,P18-1079,0,\N,Missing
N19-1165,P18-2006,0,\N,Missing
N19-1165,C18-1071,1,\N,Missing
N19-1165,P16-1101,0,\N,Missing
N19-1165,P18-1241,0,\N,Missing
N19-1165,P18-1096,0,\N,Missing
N19-1165,D18-1472,1,\N,Missing
P15-1088,P08-1065,0,0.0118099,"ment is iteratively constructed from successive pairwise alignments (Bangalore et al., 2001). Matusov et al. (2006) apply word reordering and subsequent pairwise monotone one-to-one alignments for MT system combination. 6 We made use of the CRF++ package available at https://code.google.com/p/crfpp/. 7 To be more precise on the training of the segmentation module, in an alignment as in Table 1, we consider the segmented x string — ph-oe-n-i-x — and then encode this segmentation in a binary string where 1’s indicate splits. Thus, segmentation becomes, again, a sequence labling task; see, e.g., Bartlett et al. (2008) or Eger (2013) for details. 913 Approx. alignments Secondly, we derive the optimal multiple many-to-many alignment of the strings in question by choosing an alignment that satisfies the condition that (1) each individual ˆ (1) , . . . , y ˆ (M ) , y is optimally segmented string x, y (e.g., ph-oe-n-i-x rather than pho-eni-x, f-i-n-I-ks rather than f-inIk-s) subject to the global constraint that (2) the number of segments must agree across the strings to align. This constitutes a separable alignment model as discussed in Section 2, and thus has much lower runtime complexity as the first model."
P15-1088,N09-3008,0,0.0220386,"ificantly earlier, in (Ukkonen, 1985), (V´eronis, 1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., (Ristad and Yianilos, 1998), (Cotterell et al., 2014), besides the works already mentioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e., multiple sequence alignment, has also been an issue both in NLP (e.g., Covington (1998), Bhargava and Kondrak (2009)) and bioinformatics (e.g., Durbin et al. (1998)). An interesting application of alignments of multiple sequences is to determine what has been called median string (Kohonen, 1985) or Steiner consensus string (Gusfield, 1997), defined as the string ¯s that minimizes the sum of distances, for a given distance function d(x, y), to a list of strings s1 , . . . , sN (Jiang et al., 2012); typically, d is the standard edit distance. As Gusfield (1997) shows, the Steiner consensus string may be retrieved from a multiple align(4) for some models fw1 , . . . , fwN and where Ψ : RN → R is non-decreasing"
P15-1088,P11-1041,0,0.0159109,"hat almost any sort of external knowledge source may be integrated to improve the performance of a baseline system. For example, supplemental information strings may appear in the form of transliterations of an input string in other languages; they may be predictions of other G2P systems, whether carefully manually crafted or learnt from data; they might even appear in the form of phonetic transcriptions of the input string in other dialects or languages. What distinguishes our solution to integrating supplemental information strings in string transduction settings from other research (e.g., (Bhargava and Kondrak, 2011; Bhargava and Kondrak, 2012)) is that rather than integrating systems on the global level of strings, we integrate them on the local level of smaller units, namely, substrings appropriated to the domain of application (e.g., in our context, phonemes/grapheme substructures). Both approaches may be considered complementary. Finally, another important contribution of our work is to outline an ‘approximation algorithm’ to inducing multiple many-to-many alignments of strings, which is otherwise an NP-hard problem for which (most likely) no efficient exact solutions exist, and to investigate its su"
P15-1088,W09-0408,0,0.0290541,"ssumption in this situation is that the sequences to be combined have equal length, which clearly cannot be expected to hold when, e.g., the outputs of several G2P, transliteration, etc., systems must be combined. In fact, the multiple many-to-many alignment models investigated in this work could act as a preprocessing step in this setup, since the alignment precisely serves the functionality of segmenting the strings into equal number of segments/substructures. Of course, combining outputs with varying number of elements is also an issue in machine translation (e.g., Macherey and Och (2007), Heafield et al. (2009)), but, there, the problem is harder due to the potential non-monotonicities in the ordering of elements, which typically necessitates (additional) heuristics. One approach for constructing multiple alignments is here progressive multiple alignment (Feng and Doolittle, 1987) in which a multiple (typically one-to-one) alignment is iteratively constructed from successive pairwise alignments (Bangalore et al., 2001). Matusov et al. (2006) apply word reordering and subsequent pairwise monotone one-to-one alignments for MT system combination. 6 We made use of the CRF++ package available at https://"
P15-1088,N12-1044,0,0.261512,"d the problem of classical string-to-string translation by assuming that, at training time, we have available (M + 2)-tuples ˆ (1) , . . . , y ˆ (M ) , y)}, where x is the of strings {(x, y ˆ (m) , for 1 ≤ m ≤ M , are supinput string, y plemental information strings, and y is the desired output string; at test time, we wish to preˆ (1) , . . . , y ˆ (M ) ). Generally, we dict y from (x, y (1) (M ˆ ,...,y ˆ ) as arbitrary strings may think of y over arbitrary alphabets Σ(m) , for 1 ≤ m ≤ M . ˆ (m) For example, x might be a letter-string and y might be a transliteration of x in language Lm (cf. Bhargava and Kondrak (2012)). Alternatively, and this is our model scenario in the current work, x ˆ (m) might be might be a letter input string and y the predicted string of phonemes, given x, produced by an (offline) system Tm . This situation is outlined in Table 3. In the table, we also illustrate a multiple (monotone) many-to-many alignˆ (1) , . . . , y ˆ (M ) , y). By this, we mean ment of (x, y an alignment where (1) subsequences of all M + 2 strings may be matched up with each other (manyIntroduction String-to-string translation (string transduction) is the problem of converting one string x over an alphabet Σ i"
P15-1088,N07-1047,0,0.567928,"osting baseline G2P systems using homogeneous as well as heterogeneous sources of supplemental information. 1 oe i n n i I x ks Table 1: Sample monotone many-to-many alignment between x = phoenix and y = finIks. characters may be matched up with several y sequence characters; Table 1 illustrates. Once the training data is aligned, since x and y sequences are then segmented into equal number of segments, string-to-string translation may be seen as a sequence labeling (tagging) problem in which x (sub-)sequence characters are observed variables and y (sub-)sequence characters are hidden states (Jiampojamarn et al., 2007; Jiampojamarn et al., 2010). In this work, we extend the problem of classical string-to-string translation by assuming that, at training time, we have available (M + 2)-tuples ˆ (1) , . . . , y ˆ (M ) , y)}, where x is the of strings {(x, y ˆ (m) , for 1 ≤ m ≤ M , are supinput string, y plemental information strings, and y is the desired output string; at test time, we wish to preˆ (1) , . . . , y ˆ (M ) ). Generally, we dict y from (x, y (1) (M ˆ ,...,y ˆ ) as arbitrary strings may think of y over arbitrary alphabets Σ(m) , for 1 ≤ m ≤ M . ˆ (m) For example, x might be a letter-string and y"
P15-1088,P00-1037,0,0.772933,"t of (x, y an alignment where (1) subsequences of all M + 2 strings may be matched up with each other (manyIntroduction String-to-string translation (string transduction) is the problem of converting one string x over an alphabet Σ into another string y over a possibly different alphabet Γ. The most prominent applications of string-to-string translation in natural language processing (NLP) are graphemeto-phoneme conversion, in which x is a letterstring and y is a string of phonemes, transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. Thereby, all state-of-the-art modelings we are aware of (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) proceed by first aligning the string pairs (x, y) in the training data. Also, these modelings acknowledge that alignments may typically be of a rather complex nature in which several x sequence 909 Proceedings of the 53rd Annual Meeting of the Associ"
P15-1088,P08-1103,0,0.264299,"ost prominent applications of string-to-string translation in natural language processing (NLP) are graphemeto-phoneme conversion, in which x is a letterstring and y is a string of phonemes, transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. Thereby, all state-of-the-art modelings we are aware of (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) proceed by first aligning the string pairs (x, y) in the training data. Also, these modelings acknowledge that alignments may typically be of a rather complex nature in which several x sequence 909 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 909–919, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics x = schizo ˆ (1) = skaIz@U y ˆ (2) = saIz@U y ˆ (3) = skIts@ y ˆ (4) = Sits@U y ˆ (5) = skI"
P15-1088,N10-1103,0,0.63241,"using homogeneous as well as heterogeneous sources of supplemental information. 1 oe i n n i I x ks Table 1: Sample monotone many-to-many alignment between x = phoenix and y = finIks. characters may be matched up with several y sequence characters; Table 1 illustrates. Once the training data is aligned, since x and y sequences are then segmented into equal number of segments, string-to-string translation may be seen as a sequence labeling (tagging) problem in which x (sub-)sequence characters are observed variables and y (sub-)sequence characters are hidden states (Jiampojamarn et al., 2007; Jiampojamarn et al., 2010). In this work, we extend the problem of classical string-to-string translation by assuming that, at training time, we have available (M + 2)-tuples ˆ (1) , . . . , y ˆ (M ) , y)}, where x is the of strings {(x, y ˆ (m) , for 1 ≤ m ≤ M , are supinput string, y plemental information strings, and y is the desired output string; at test time, we wish to preˆ (1) , . . . , y ˆ (M ) ). Generally, we dict y from (x, y (1) (M ˆ ,...,y ˆ ) as arbitrary strings may think of y over arbitrary alphabets Σ(m) , for 1 ≤ m ≤ M . ˆ (m) For example, x might be a letter-string and y might be a transliteration o"
P15-1088,P14-2102,0,0.317417,"venshtein, 1966) in which the minimal number of insertions, deletions and substitutions is sought that transforms one string into another. Substringto-substring edit operations — or equivalently, (monotone) many-to-many alignments — have appeared in the NLP context, e.g., in (Deligne et al., 1995), (Brill and Moore, 2000), (Jiampojamarn et al., 2007), (Bisani and Ney, 2008), (Jiampojamarn et al., 2010), or, significantly earlier, in (Ukkonen, 1985), (V´eronis, 1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., (Ristad and Yianilos, 1998), (Cotterell et al., 2014), besides the works already mentioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e., multiple sequence alignment, has also been an issue both in NLP (e.g., Covington (1998), Bhargava and Kondrak (2009)) and bioinformatics (e.g., Durbin et al. (1998)). An interesting application of alignments of multiple sequences is to determine what has been called median string (Kohonen, 1985)"
P15-1088,P98-1043,0,0.123109,"., 2010), or, significantly earlier, in (Ukkonen, 1985), (V´eronis, 1988). Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., (Ristad and Yianilos, 1998), (Cotterell et al., 2014), besides the works already mentioned. All of these approaches are special cases of our unigram model outlined in Section 2 — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)}) and/or restrict attention to only N = 2 strings.5 Alignments between multiple sequences, i.e., multiple sequence alignment, has also been an issue both in NLP (e.g., Covington (1998), Bhargava and Kondrak (2009)) and bioinformatics (e.g., Durbin et al. (1998)). An interesting application of alignments of multiple sequences is to determine what has been called median string (Kohonen, 1985) or Steiner consensus string (Gusfield, 1997), defined as the string ¯s that minimizes the sum of distances, for a given distance function d(x, y), to a list of strings s1 , . . . , sN (Jiang et al., 2012); typically, d is the standard edit distance. As Gusfield (1997) shows, the Steiner consensus string may be retrieved from a multiple align(4) for some models fw1 , . . . , fwN and where"
P15-1088,D08-1113,0,0.462731,"Missing"
P15-1088,D07-1105,0,0.0274991,"ces therein), a typical assumption in this situation is that the sequences to be combined have equal length, which clearly cannot be expected to hold when, e.g., the outputs of several G2P, transliteration, etc., systems must be combined. In fact, the multiple many-to-many alignment models investigated in this work could act as a preprocessing step in this setup, since the alignment precisely serves the functionality of segmenting the strings into equal number of segments/substructures. Of course, combining outputs with varying number of elements is also an issue in machine translation (e.g., Macherey and Och (2007), Heafield et al. (2009)), but, there, the problem is harder due to the potential non-monotonicities in the ordering of elements, which typically necessitates (additional) heuristics. One approach for constructing multiple alignments is here progressive multiple alignment (Feng and Doolittle, 1987) in which a multiple (typically one-to-one) alignment is iteratively constructed from successive pairwise alignments (Bangalore et al., 2001). Matusov et al. (2006) apply word reordering and subsequent pairwise monotone one-to-one alignments for MT system combination. 6 We made use of the CRF++ packa"
P15-1088,E06-1005,0,0.038374,"of segments/substructures. Of course, combining outputs with varying number of elements is also an issue in machine translation (e.g., Macherey and Och (2007), Heafield et al. (2009)), but, there, the problem is harder due to the potential non-monotonicities in the ordering of elements, which typically necessitates (additional) heuristics. One approach for constructing multiple alignments is here progressive multiple alignment (Feng and Doolittle, 1987) in which a multiple (typically one-to-one) alignment is iteratively constructed from successive pairwise alignments (Bangalore et al., 2001). Matusov et al. (2006) apply word reordering and subsequent pairwise monotone one-to-one alignments for MT system combination. 6 We made use of the CRF++ package available at https://code.google.com/p/crfpp/. 7 To be more precise on the training of the segmentation module, in an alignment as in Table 1, we consider the segmented x string — ph-oe-n-i-x — and then encode this segmentation in a binary string where 1’s indicate splits. Thus, segmentation becomes, again, a sequence labling task; see, e.g., Bartlett et al. (2008) or Eger (2013) for details. 913 Approx. alignments Secondly, we derive the optimal multiple"
P15-1088,W12-6208,0,0.0705144,"tion in natural language processing (NLP) are graphemeto-phoneme conversion, in which x is a letterstring and y is a string of phonemes, transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. Thereby, all state-of-the-art modelings we are aware of (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) proceed by first aligning the string pairs (x, y) in the training data. Also, these modelings acknowledge that alignments may typically be of a rather complex nature in which several x sequence 909 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 909–919, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics x = schizo ˆ (1) = skaIz@U y ˆ (2) = saIz@U y ˆ (3) = skIts@ y ˆ (4) = Sits@U y ˆ (5) = skIts@ y y = skIts@U to-many alignments), and where"
P15-1088,N12-1024,0,0.0202044,"the situation N = 2 and S = {(1, 0), (0, 1), (1, 1)}. 912 4 ment of s1 , . . . , sN by concatenating the columnwise majority characters in the alignment, ignoring skips. Since median string computation (and hence also the multiple many-to-many alignment problem, as we consider) is an NP-hard problem (Sim and Park, 2003), designing approximations is an active field of research. For example, Marti and Bunke (2001) ignore part of the search space by declaring matches-up of distant characters as unlikely, and Jiang et al. (2012) apply an approximation based on string embeddings in vector spaces. Paul and Eisner (2012) apply dual decomposition to compute Steiner consensus strings. Via the approach taken in this paper, median strings may be computed in case d is a (distance) function taking substring-to-substring edit operations into account, a seemingly straightforward, yet extremely useful generalization in several NLP applications, as indicated in the introduction. 4.1 Data and systems Data We conduct experiments on the General American (GA) variant of the Combilex data set (Richmond et al., 2009). This contains about 144,000 grapheme-phoneme pairs as exemplarily illustrated in Table 2. In our experiments"
P15-1088,P07-1119,0,0.257059,"ate a multiple (monotone) many-to-many alignˆ (1) , . . . , y ˆ (M ) , y). By this, we mean ment of (x, y an alignment where (1) subsequences of all M + 2 strings may be matched up with each other (manyIntroduction String-to-string translation (string transduction) is the problem of converting one string x over an alphabet Σ into another string y over a possibly different alphabet Γ. The most prominent applications of string-to-string translation in natural language processing (NLP) are graphemeto-phoneme conversion, in which x is a letterstring and y is a string of phonemes, transliteration (Sherif and Kondrak, 2007), lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000). The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x, y)} and then to evaluate model performance on test data. Thereby, all state-of-the-art modelings we are aware of (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) proceed by first aligning the string pairs (x, y) in the training data. Also, these modelings acknowledge that alignments may typically be of a rather complex"
P15-1088,C98-1043,0,\N,Missing
P16-2009,P14-1023,0,0.0544445,", several wellknown examples of meaning change in English have been documented. For example, the word gay’s meaning has shifted, during the 1970s, from an adjectival meaning of cheerful at the beginning of the 20th century to its present meaning of homosexual (Kulkarni et al., 2015a). Similarly, technological progress has led to semantic broadening of terms such as transmission, mouse, or apple. In our work, we capture semantics by means of word embeddings derived from context-predicting neural network architectures, which have become the state-of-the-art in distributional semantics modeling (Baroni et al., 2014). Our approach and results are partly independent of this representation, however, in that we take a structuralist approach: we derive new, ‘second-order embeddings’ by modeling the meaning of words by 52 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 52–58, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics t ∈ T . Here, T is a set of time indices. Denote an embedding of a word wi at time period t as wi (t) ∈ Rd . Since embeddings wi (s), wi (t) for two different time periods s, t are generally not comparable, a"
P16-2009,E12-1060,0,0.0110016,"just aim at capturing whether meaning change has occurred. In contrast, more fine-grained analyses typically senselabel word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015). Sense-labeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012). Finally, there are studies that test particular meaning change hypotheses such as whether similar words tend to diverge in meaning over time (according to the ‘law of differentiation’) (Xu and Kemp, 2015) and papers that intend to detect corresponding terms across time (words with similar meanings/roles in two time periods but potentially different lexical forms) (Zhang et al., 2015). 3.1 A linear model of semantic change We postulate, and subsequently test, the following model of meaning dynamics which describes meaning change over time for words wi : wi (t) = p X X n=1 wj ∈V ∩N (wi ) αnwj"
P16-2009,D14-1110,0,0.00994504,"period (Jatowt and Duh, 2014; Kulkarni et al., 2015a). Such coarse-grained models, by themselves, do not specify in which respects a word has changed (e.g., semantic broadening or narrowing), but just aim at capturing whether meaning change has occurred. In contrast, more fine-grained analyses typically senselabel word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015). Sense-labeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012). Finally, there are studies that test particular meaning change hypotheses such as whether similar words tend to diverge in meaning over time (according to the ‘law of differentiation’) (Xu and Kemp, 2015) and papers that intend to detect corresponding terms across time (words with similar meanings/roles in two time periods but potentially different lexical forms) (Zhang et al., 2015). 3.1 A linear mo"
P16-2009,S15-1014,1,0.664239,"We apply our two models to corpora across three different languages. We find that semantic change is linear in two senses. Firstly, today’s embedding vectors (= meaning) of words can be derived as linear combinations of embedding vectors of their neighbors in previous time periods. Secondly, self-similarity of words decays linearly in time. We consider both findings as new laws/hypotheses of semantic change. 1 Introduction Meaning is not uniform, neither across space, nor across time. Across space, different languages tend to exhibit different polysemous associations for corresponding terms (Eger et al., 2015; Kulkarni et al., 2015b). Across time, several wellknown examples of meaning change in English have been documented. For example, the word gay’s meaning has shifted, during the 1970s, from an adjectival meaning of cheerful at the beginning of the 20th century to its present meaning of homosexual (Kulkarni et al., 2015a). Similarly, technological progress has led to semantic broadening of terms such as transmission, mouse, or apple. In our work, we capture semantics by means of word embeddings derived from context-predicting neural network architectures, which have become the state-of-the-art"
P16-2009,P14-1096,0,0.0453411,"analysis. On the one hand, coarse-grained trend analyses compare the semantics of a word in one time period with the meaning of the word in the preceding time period (Jatowt and Duh, 2014; Kulkarni et al., 2015a). Such coarse-grained models, by themselves, do not specify in which respects a word has changed (e.g., semantic broadening or narrowing), but just aim at capturing whether meaning change has occurred. In contrast, more fine-grained analyses typically senselabel word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015). Sense-labeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012). Finally, there are studies that test particular meaning change hypotheses such as whether similar words tend to diverge in meaning over time (according to the ‘law of differentiation’) (Xu and Kemp, 2015) and papers that intend to detect corresp"
P16-2009,E14-1049,0,0.0216492,"s wj (t − n) and p ≥ 1 is the order of the model. The set N (wi ) ⊆ V denotes a set of ‘neighbors’ of word wi .2 This model says that the meaning of a word wi at some time t is determined by reference to the meanings of its ‘neighbors’ in previous time periods, and that the underlying functional relationship is linear. We remark that the model described by Eq. (2) is a time-series model, and, in particular, a vector-autoregressive (VAR) model with special 1 An alternative to our second-order embeddings is to project vectors from different time periods in a common space (Mikolov et al., 2013a; Faruqui and Dyer, 2014), which requires to find corresponding terms across time. Further, one could also consider a ‘core’ vocabulary of semantically stable words, e.g., in the spirit of Swadesh (1952), instead of using all vocabulary words as reference. 2 We also constrain the vectors wi (t), for all wi ∈ V , to contain non-zero entries only for words in N (wi ). 3 Graph models Let V = {w1 , . . . , w|V |} be the common vocabulary (intersection) of all words in all time periods 2 53 We lemmatize and POS tag the data and likewise only consider nouns, verbs and adjectives, making the same frequency constraints as in"
P16-2009,D14-1113,0,0.0323176,"Duh, 2014; Kulkarni et al., 2015a). Such coarse-grained models, by themselves, do not specify in which respects a word has changed (e.g., semantic broadening or narrowing), but just aim at capturing whether meaning change has occurred. In contrast, more fine-grained analyses typically senselabel word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015). Sense-labeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012). Finally, there are studies that test particular meaning change hypotheses such as whether similar words tend to diverge in meaning over time (according to the ‘law of differentiation’) (Xu and Kemp, 2015) and papers that intend to detect corresponding terms across time (words with similar meanings/roles in two time periods but potentially different lexical forms) (Zhang et al., 2015). 3.1 A linear model of semantic change We p"
P16-2009,P12-1092,0,0.0608619,"the preceding time period (Jatowt and Duh, 2014; Kulkarni et al., 2015a). Such coarse-grained models, by themselves, do not specify in which respects a word has changed (e.g., semantic broadening or narrowing), but just aim at capturing whether meaning change has occurred. In contrast, more fine-grained analyses typically senselabel word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015). Sense-labeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012). Finally, there are studies that test particular meaning change hypotheses such as whether similar words tend to diverge in meaning over time (according to the ‘law of differentiation’) (Xu and Kemp, 2015) and papers that intend to detect corresponding terms across time (words with similar meanings/roles in two time periods but potentially different lexical forms) (Zhang et al., 201"
P16-2009,P11-2053,0,0.0226989,"aches to meaning change analysis. On the one hand, coarse-grained trend analyses compare the semantics of a word in one time period with the meaning of the word in the preceding time period (Jatowt and Duh, 2014; Kulkarni et al., 2015a). Such coarse-grained models, by themselves, do not specify in which respects a word has changed (e.g., semantic broadening or narrowing), but just aim at capturing whether meaning change has occurred. In contrast, more fine-grained analyses typically senselabel word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015). Sense-labeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012). Finally, there are studies that test particular meaning change hypotheses such as whether similar words tend to diverge in meaning over time (according to the ‘law of differentiation’) (Xu and Kemp, 2015) and papers that inte"
P16-2009,P15-1063,0,0.0417373,"d trend analyses compare the semantics of a word in one time period with the meaning of the word in the preceding time period (Jatowt and Duh, 2014; Kulkarni et al., 2015a). Such coarse-grained models, by themselves, do not specify in which respects a word has changed (e.g., semantic broadening or narrowing), but just aim at capturing whether meaning change has occurred. In contrast, more fine-grained analyses typically senselabel word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015). Sense-labeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012). Finally, there are studies that test particular meaning change hypotheses such as whether similar words tend to diverge in meaning over time (according to the ‘law of differentiation’) (Xu and Kemp, 2015) and papers that intend to detect corresponding terms across time (words with simi"
P17-1002,E17-1005,0,0.16357,"n Eger†‡ , Johannes Daxenberger† , Iryna Gurevych†‡ † Ubiquitous Knowledge Processing Lab (UKP-TUDA) Department of Computer Science, Technische Universitt Darmstadt ‡ Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research and Educational Information http://www.ukp.tu-darmstadt.de Abstract While different research has addressed different subsets of the AM problem (see below), the ultimate goal is to solve all of them, starting from unannotated plain text. Two recent approaches to this end-to-end learning scenario are Persing and Ng (2016) and Stab and Gurevych (2017). Both solve the end-to-end task by first training independent models for each subtask and then defining an integer linear programming (ILP) model that encodes global constraints such as that each premise has a parent, etc. Besides their pipeline architecture the approaches also have in common that they heavily rely on hand-crafted features. Hand-crafted features pose a problem because AM is to some degree an “arbitrary” problem in that the notion of “argument” critically relies on the underlying argumentation theory (Reed et al., 2008; Biran and Rambow, 2011; Habernal and Gurevych, 2015; Stab"
P17-1002,D12-1133,0,0.044595,"Missing"
P17-1002,Q16-1023,0,0.0212672,"Missing"
P17-1002,W15-0501,1,0.0288887,"rprising that ‘discourse parsing’ (Muller et al., 2012) has been suggested for AM (Peldszus and Stede, 2015). What distinguishes our approach from these previous ones is that we operate on the token level, rather than on the level of components, because we address the end-toend framework and, thus, do not assume that nonargumentative units have already been sorted out and/or that the boundaries of argumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016). Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of co"
P17-1002,D14-1082,0,0.00547069,"a structured prediction problem in which the goal is learning a scoring function over dependency trees such that correct trees are scored above all others. Traditional dependency parsers used handcrafted feature functions that look at “core” elements such as “word on top of the stack”, “POS of word on top of the stack”, and conjunctions of core features such as “word is X and POS is Y” (see McDonald et al. (2005)). Most neural parsers have not entirely abandoned feature engineering. Instead, they rely, for example, on encoding the core features of parsers as low-dimensional embedding vectors (Chen and Manning, 2014) but ignore feature combinations. Kiperwasser and Goldberg (2016) design a neural parser that uses only four features: the BiLSTM vector representations of the top 3 items on the stack and the first item on the buffer. In contrast, Dyer et al. (2015)’s neural parser associates each stack with a “stack LSTM” that encodes their contents. Actions are chosen based on the stack LSTM representations of the stacks, and no more feature engineering is necessary. Moreover, their parser has thus access to any part of the input, its history and stack contents. AM as Dependency Parsing: To frame a problem"
P17-1002,P15-1033,0,0.00538485,"as “word on top of the stack”, “POS of word on top of the stack”, and conjunctions of core features such as “word is X and POS is Y” (see McDonald et al. (2005)). Most neural parsers have not entirely abandoned feature engineering. Instead, they rely, for example, on encoding the core features of parsers as low-dimensional embedding vectors (Chen and Manning, 2014) but ignore feature combinations. Kiperwasser and Goldberg (2016) design a neural parser that uses only four features: the BiLSTM vector representations of the top 3 items on the stack and the first item on the buffer. In contrast, Dyer et al. (2015)’s neural parser associates each stack with a “stack LSTM” that encodes their contents. Actions are chosen based on the stack LSTM representations of the stacks, and no more feature engineering is necessary. Moreover, their parser has thus access to any part of the input, its history and stack contents. AM as Dependency Parsing: To frame a problem as a dependency parsing problem, each instance of the problem must be encoded as a directed tree, where tokens have heads, which in turn are labeled. For end-to-end AM, we propose the framing illustrated in Figure 3. We highlight two design decisions"
P17-1002,C14-1141,0,0.195964,"the boundaries of argumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016). Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity. To our best knowledge, Stab and Gurevych (2017) created the only corpus of attested high quality which annotates the AM problem in its entire complexity: it contains token-level annotations of components, their types, as well as relations and their types. Second, we frame the problem as a sequence tagging problem. This is a natural choice especially for"
P17-1002,P16-1101,0,0.0110928,"Missing"
P17-1002,W13-2707,0,0.319706,"do not assume that nonargumentative units have already been sorted out and/or that the boundaries of argumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016). Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity. To our best knowledge, Stab and Gurevych (2017) created the only corpus of attested high quality which annotates the AM problem in its entire complexity: it contains token-level annotations of components, their types, as well as relations and their types. Second, we frame th"
P17-1002,H05-1066,0,0.107517,"Missing"
P17-1002,P16-1105,0,0.0654382,"e AM as a multi-task (tagging) problem (Caruana, 1997; Collobert and Weston, 2008). We experiment with subtasks of AM—e.g., component identification—as auxiliary tasks and investigate whether this improves performance on the AM problem. Adding such subtasks can be seen as analogous to de-coupling, e.g., component identification from the full AM problem. 3 Data We use the dataset of persuasive essays (PE) from Stab and Gurevych (2017), which contains student essays written in response to controversial topics such as “competition or cooperation—which is better?” Fourth, we evaluate the model of Miwa and Bansal (2016) that combines sequential (entity) and tree structure (relation) information and is in principle applicable to any problem where the aim is to extract entities and their relations. As such, this model makes fewer assumptions than our dependency parsing and tagging approaches. Essays Paragraphs Tokens Train Test 322 1786 118648 80 449 29538 Table 1: Corpus statistics The contributions of this paper are as follows. (1) We present the first neural end-to-end solutions to computational AM. (2) We show that several of them perform better than the state-of-theart joint ILP model. (3) We show that a"
P17-1002,N16-1164,0,0.54771,"or Computational Argumentation Mining Steffen Eger†‡ , Johannes Daxenberger† , Iryna Gurevych†‡ † Ubiquitous Knowledge Processing Lab (UKP-TUDA) Department of Computer Science, Technische Universitt Darmstadt ‡ Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research and Educational Information http://www.ukp.tu-darmstadt.de Abstract While different research has addressed different subsets of the AM problem (see below), the ultimate goal is to solve all of them, starting from unannotated plain text. Two recent approaches to this end-to-end learning scenario are Persing and Ng (2016) and Stab and Gurevych (2017). Both solve the end-to-end task by first training independent models for each subtask and then defining an integer linear programming (ILP) model that encodes global constraints such as that each premise has a parent, etc. Besides their pipeline architecture the approaches also have in common that they heavily rely on hand-crafted features. Hand-crafted features pose a problem because AM is to some degree an “arbitrary” problem in that the notion of “argument” critically relies on the underlying argumentation theory (Reed et al., 2008; Biran and Rambow, 2011; Habe"
P17-1002,C12-1115,0,0.0262446,"ages 11–22 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1002 proposed for systems that operate on the coarser component level and that (4) a standard neural sequence tagging model that encodes distance information between components performs robustly in different environments. Finally, (5) we show that a multi-task learning setup where natural subtasks of the full AM problem are added as auxiliary tasks improves performance.1 or closely resemble them (see §3). Hence, it is not surprising that ‘discourse parsing’ (Muller et al., 2012) has been suggested for AM (Peldszus and Stede, 2015). What distinguishes our approach from these previous ones is that we operate on the token level, rather than on the level of components, because we address the end-toend framework and, thus, do not assume that nonargumentative units have already been sorted out and/or that the boundaries of argumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educati"
P17-1002,reed-etal-2008-language,0,0.100994,"end learning scenario are Persing and Ng (2016) and Stab and Gurevych (2017). Both solve the end-to-end task by first training independent models for each subtask and then defining an integer linear programming (ILP) model that encodes global constraints such as that each premise has a parent, etc. Besides their pipeline architecture the approaches also have in common that they heavily rely on hand-crafted features. Hand-crafted features pose a problem because AM is to some degree an “arbitrary” problem in that the notion of “argument” critically relies on the underlying argumentation theory (Reed et al., 2008; Biran and Rambow, 2011; Habernal and Gurevych, 2015; Stab and Gurevych, 2017). Accordingly, datasets typically differ with respect to their annotation of (often rather complex) argument structure. Thus, feature sets would have to be manually adapted to and designed for each new sample of data, a challenging task. The same critique applies to the designing of ILP constraints. Moreover, from a machine learning perspective, pipeline approaches are problematic because they solve subtasks independently and thus lead to error propagation rather than exploiting interrelationships between variables."
P17-1002,D13-1032,0,0.0236992,"Missing"
P17-1002,D15-1050,0,0.102406,"rgumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016). Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity. To our best knowledge, Stab and Gurevych (2017) created the only corpus of attested high quality which annotates the AM problem in its entire complexity: it contains token-level annotations of components, their types, as well as relations and their types. Second, we frame the problem as a sequence tagging problem. This is a natural choice especially for component identificati"
P17-1002,D15-1110,0,0.224931,"4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1002 proposed for systems that operate on the coarser component level and that (4) a standard neural sequence tagging model that encodes distance information between components performs robustly in different environments. Finally, (5) we show that a multi-task learning setup where natural subtasks of the full AM problem are added as auxiliary tasks improves performance.1 or closely resemble them (see §3). Hence, it is not surprising that ‘discourse parsing’ (Muller et al., 2012) has been suggested for AM (Peldszus and Stede, 2015). What distinguishes our approach from these previous ones is that we operate on the token level, rather than on the level of components, because we address the end-toend framework and, thus, do not assume that nonargumentative units have already been sorted out and/or that the boundaries of argumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educational domain has been highlighted by recent work on wr"
P17-1002,P16-2038,0,0.0353023,"Missing"
P17-1002,C16-1148,0,0.0193574,"e token level, rather than on the level of components, because we address the end-toend framework and, thus, do not assume that nonargumentative units have already been sorted out and/or that the boundaries of argumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016). Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity. To our best knowledge, Stab and Gurevych (2017) created the only corpus of attested high quality which annotates the AM problem in its entire complexity: it contains token-l"
P17-1002,J17-3005,1,0.537815,"Missing"
P17-1002,P13-1161,0,0.0642591,"Missing"
P17-1002,N16-1168,0,0.0294094,"es our approach from these previous ones is that we operate on the token level, rather than on the level of components, because we address the end-toend framework and, thus, do not assume that nonargumentative units have already been sorted out and/or that the boundaries of argumentative units are given. 2 Related Work AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015). Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016). Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015). Relatively few works address the full AM problem of component and relation identification. Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity. To our best knowledge, Stab and Gurevych (2017) created the only corpus of attested high quality which an"
P17-1002,E17-1063,0,0.0126958,"Missing"
P17-1002,N03-1031,0,\N,Missing
P17-1002,J17-1004,1,\N,Missing
P17-1002,D15-1253,0,\N,Missing
P17-1002,D14-1162,0,\N,Missing
P17-1002,D15-1255,1,\N,Missing
P17-1002,P15-1053,0,\N,Missing
P17-1002,N15-1142,0,\N,Missing
P19-1150,P19-1098,0,0.0275661,"s based on the EM algorithm, which achieves better accuracy on the smallNORB dataset. Zhang et al. (2018a) applied capsule networks to relation extraction in a multi-instance multi-label learning framework. Xiao et al. (2018) explored capsule networks for multi-task learning. Xia et al. (2018) studied the zero-shot intent detection problem with capsule networks, which aims to detect emerging user intents in an unsupervised manner. Zhao et al. (2018) investigated capsule networks with dynamic routing for text classification, and transferred knowledge from the single-label to multi-label cases. Cho et al. (2019) studied capsule networks with determinantal point processes for extractive multi-document summarization. Our work is different from our predecessors in the following aspects: (i) we evaluate the performance of routing processes at instance level, and introduce an adaptive optimizer to enhance the reliability of routing processes; (ii) we present capsule compression and partial routing to achieve better scalability of capsule networks on datasets with a large output space. 5 Conclusion Making computers perform more like humans is a major issue in NLP and machine learning. This not only include"
P19-1150,C18-1279,1,0.836682,"er extraction, early methods manually designed patterns to get them (Pasca). A recent popular trend is neural answer extraction. Various neural network models are employed to represent questions (Severyn and Moschitti, 2015; Wang and Nyberg, 2015). Since the attention mechanism naturally explores relevancy, it has been widely used in QA models to relate the question to candidate answers (Tan et al., 2016; Santos et al., 2016; Sha et al., 2018). Moreover, some researchers leveraged external large-scale knowledge bases to assist answer selection (Savenkov and Agichtein, 2017; Shen et al., 2018; Deng et al., 2018). Knowledge-based QA conducts semantic parsing on questions and transforms parsing results into logical forms. Those forms are adopted to match answers from structured knowledge bases (Yao and Van Durme, 2014; Yih et al., 2015; Bordes et al., 2015; Yin et al., 2016; Hao et al., 2017). Recent developments focused on modeling the interaction between question and answer pairs: Tensor layers (Qiu and Huang, 2015; Wan et al., 2016) and holographic composition (Tay et al., 2017) have pushed the state-of-the-art. 4.3 Capsule Networks Capsule networks were initially proposed by Hinton (Hinton et al.,"
P19-1150,N19-1165,1,0.818101,"Missing"
P19-1150,P17-1021,0,0.0135312,"lly explores relevancy, it has been widely used in QA models to relate the question to candidate answers (Tan et al., 2016; Santos et al., 2016; Sha et al., 2018). Moreover, some researchers leveraged external large-scale knowledge bases to assist answer selection (Savenkov and Agichtein, 2017; Shen et al., 2018; Deng et al., 2018). Knowledge-based QA conducts semantic parsing on questions and transforms parsing results into logical forms. Those forms are adopted to match answers from structured knowledge bases (Yao and Van Durme, 2014; Yih et al., 2015; Bordes et al., 2015; Yin et al., 2016; Hao et al., 2017). Recent developments focused on modeling the interaction between question and answer pairs: Tensor layers (Qiu and Huang, 2015; Wan et al., 2016) and holographic composition (Tay et al., 2017) have pushed the state-of-the-art. 4.3 Capsule Networks Capsule networks were initially proposed by Hinton (Hinton et al., 2011) to improve representations learned by neural networks against vanilla CNNs. Subsequently, Sabour et al. (2017) replaced the scalar-output feature detectors of CNNs with vector-output capsules and max-pooling with routing-by-agreement. Hinton et al. (2018) then proposed a new it"
P19-1150,D14-1181,0,0.0092432,"Missing"
P19-1150,J84-3009,0,0.727558,"Missing"
P19-1150,P16-1044,0,0.0180496,"d employ information retrieval techniques to retrieve a list of relevant passages to questions. Next, reading comprehension techniques are adopted to extract answers within the span of retrieved text. For answer extraction, early methods manually designed patterns to get them (Pasca). A recent popular trend is neural answer extraction. Various neural network models are employed to represent questions (Severyn and Moschitti, 2015; Wang and Nyberg, 2015). Since the attention mechanism naturally explores relevancy, it has been widely used in QA models to relate the question to candidate answers (Tan et al., 2016; Santos et al., 2016; Sha et al., 2018). Moreover, some researchers leveraged external large-scale knowledge bases to assist answer selection (Savenkov and Agichtein, 2017; Shen et al., 2018; Deng et al., 2018). Knowledge-based QA conducts semantic parsing on questions and transforms parsing results into logical forms. Those forms are adopted to match answers from structured knowledge bases (Yao and Van Durme, 2014; Yih et al., 2015; Bordes et al., 2015; Yin et al., 2016; Hao et al., 2017). Recent developments focused on modeling the interaction between question and answer pairs: Tensor layer"
P19-1150,D14-1162,0,0.0811892,"Missing"
P19-1150,P17-2047,0,0.014679,"swers within the span of retrieved text. For answer extraction, early methods manually designed patterns to get them (Pasca). A recent popular trend is neural answer extraction. Various neural network models are employed to represent questions (Severyn and Moschitti, 2015; Wang and Nyberg, 2015). Since the attention mechanism naturally explores relevancy, it has been widely used in QA models to relate the question to candidate answers (Tan et al., 2016; Santos et al., 2016; Sha et al., 2018). Moreover, some researchers leveraged external large-scale knowledge bases to assist answer selection (Savenkov and Agichtein, 2017; Shen et al., 2018; Deng et al., 2018). Knowledge-based QA conducts semantic parsing on questions and transforms parsing results into logical forms. Those forms are adopted to match answers from structured knowledge bases (Yao and Van Durme, 2014; Yih et al., 2015; Bordes et al., 2015; Yin et al., 2016; Hao et al., 2017). Recent developments focused on modeling the interaction between question and answer pairs: Tensor layers (Qiu and Huang, 2015; Wan et al., 2016) and holographic composition (Tay et al., 2017) have pushed the state-of-the-art. 4.3 Capsule Networks Capsule networks were initia"
P19-1150,P15-2116,0,0.0701694,"Missing"
P19-1150,D07-1003,0,0.0113359,"answers. Therefore, we use this dataset to investigate the generalizability of our approach. Baselines We compare our approach to the following baselines: CNN + LR (Yu et al., 2014b) using unigrams and bigrams, CNN (Severyn and Moschitti, 2015) using additional bilinear similarity features, CNTN (Qiu and Huang, 2015) using neural tensor network, LSTM (Tay et al., 2017) using single and multi-layer, MV-LSTM (Wan et al., 2016), NTN-LSTM and HD-LSTM (Tay et al., 2017) using holographic dual LSTM and CapsuleZhao (Zhao et al., 2018) using capsule networks. For evaluation, we use standard measures (Wang et al., 2007) such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). Implementation Details The word embeddings used for question answering pairs are initialized as 300-dimensional GloVe vectors. In the convolutional layer, we use a convolution operation with three different window sizes (3,4,5). All 16dimensional capsules in the primary capsule layer are condensed into 256 capsules by the capsule compression operation. Experimental Results and Discussions In Table 6, the best performance on MAP metric is achieved by our approach, which verifies the effectiveness of our model. We also observe"
P19-1150,D18-1348,0,0.163051,"ation capability in low-resource settings. 5 iteration 3 iteration 1 iteration 0.04 0.03 0.02 0.01 0.00 0 200 400 600 Training Step 800 Negative Agreement Score 0.05 Training Loss Capsule networks, instead, have the potential for learning hierarchical relationships between consecutive layers by using routing processes without parameters, which are clusteringlike methods (Sabour et al., 2017) and additionally improve the generalization capability. We contrast such routing processes with pooling and fully connected layers in Figure 2. Despite some recent success in NLP tasks (Wang et al., 2018; Xia et al., 2018; Xiao et al., 2018; Zhang et al., 2018a; Zhao et al., 2018), a few important obstacles still hinder the development of capsule networks for mature NLP applications. For example, selecting the number of iterations is crucial for routing processes, because they iteratively route low-level capsules to high-level capsules in order to learn hierarchical relationships between layers. However, existing routing algorithms use the same number of iterations for all examples, which is not reliable to judge the convergence of routing. As shown in Figure 3, a routing process with five iterations on all ex"
P19-1150,D18-1486,0,0.251742,"n low-resource settings. 5 iteration 3 iteration 1 iteration 0.04 0.03 0.02 0.01 0.00 0 200 400 600 Training Step 800 Negative Agreement Score 0.05 Training Loss Capsule networks, instead, have the potential for learning hierarchical relationships between consecutive layers by using routing processes without parameters, which are clusteringlike methods (Sabour et al., 2017) and additionally improve the generalization capability. We contrast such routing processes with pooling and fully connected layers in Figure 2. Despite some recent success in NLP tasks (Wang et al., 2018; Xia et al., 2018; Xiao et al., 2018; Zhang et al., 2018a; Zhao et al., 2018), a few important obstacles still hinder the development of capsule networks for mature NLP applications. For example, selecting the number of iterations is crucial for routing processes, because they iteratively route low-level capsules to high-level capsules in order to learn hierarchical relationships between layers. However, existing routing algorithms use the same number of iterations for all examples, which is not reliable to judge the convergence of routing. As shown in Figure 3, a routing process with five iterations on all examples converges to"
P19-1150,P14-1090,0,0.0409877,"Missing"
P19-1150,P15-1128,0,0.0182653,"g and Nyberg, 2015). Since the attention mechanism naturally explores relevancy, it has been widely used in QA models to relate the question to candidate answers (Tan et al., 2016; Santos et al., 2016; Sha et al., 2018). Moreover, some researchers leveraged external large-scale knowledge bases to assist answer selection (Savenkov and Agichtein, 2017; Shen et al., 2018; Deng et al., 2018). Knowledge-based QA conducts semantic parsing on questions and transforms parsing results into logical forms. Those forms are adopted to match answers from structured knowledge bases (Yao and Van Durme, 2014; Yih et al., 2015; Bordes et al., 2015; Yin et al., 2016; Hao et al., 2017). Recent developments focused on modeling the interaction between question and answer pairs: Tensor layers (Qiu and Huang, 2015; Wan et al., 2016) and holographic composition (Tay et al., 2017) have pushed the state-of-the-art. 4.3 Capsule Networks Capsule networks were initially proposed by Hinton (Hinton et al., 2011) to improve representations learned by neural networks against vanilla CNNs. Subsequently, Sabour et al. (2017) replaced the scalar-output feature detectors of CNNs with vector-output capsules and max-pooling with routing"
P19-1150,C16-1164,0,0.0272006,"Missing"
P19-1150,D18-1120,0,0.0849955,"ings. 5 iteration 3 iteration 1 iteration 0.04 0.03 0.02 0.01 0.00 0 200 400 600 Training Step 800 Negative Agreement Score 0.05 Training Loss Capsule networks, instead, have the potential for learning hierarchical relationships between consecutive layers by using routing processes without parameters, which are clusteringlike methods (Sabour et al., 2017) and additionally improve the generalization capability. We contrast such routing processes with pooling and fully connected layers in Figure 2. Despite some recent success in NLP tasks (Wang et al., 2018; Xia et al., 2018; Xiao et al., 2018; Zhang et al., 2018a; Zhao et al., 2018), a few important obstacles still hinder the development of capsule networks for mature NLP applications. For example, selecting the number of iterations is crucial for routing processes, because they iteratively route low-level capsules to high-level capsules in order to learn hierarchical relationships between layers. However, existing routing algorithms use the same number of iterations for all examples, which is not reliable to judge the convergence of routing. As shown in Figure 3, a routing process with five iterations on all examples converges to a lower training lo"
P19-1150,D18-1350,1,0.877519,"teration 1 iteration 0.04 0.03 0.02 0.01 0.00 0 200 400 600 Training Step 800 Negative Agreement Score 0.05 Training Loss Capsule networks, instead, have the potential for learning hierarchical relationships between consecutive layers by using routing processes without parameters, which are clusteringlike methods (Sabour et al., 2017) and additionally improve the generalization capability. We contrast such routing processes with pooling and fully connected layers in Figure 2. Despite some recent success in NLP tasks (Wang et al., 2018; Xia et al., 2018; Xiao et al., 2018; Zhang et al., 2018a; Zhao et al., 2018), a few important obstacles still hinder the development of capsule networks for mature NLP applications. For example, selecting the number of iterations is crucial for routing processes, because they iteratively route low-level capsules to high-level capsules in order to learn hierarchical relationships between layers. However, existing routing algorithms use the same number of iterations for all examples, which is not reliable to judge the convergence of routing. As shown in Figure 3, a routing process with five iterations on all examples converges to a lower training loss at system level, b"
S12-1015,2005.mtsummit-papers.11,0,0.00682838,"ctionaries is of course that they are scarcely available (and much less freely available); moreover, for the above described semantic association networks, it may be of crucial importance to have comparable data sources; e.g. using a general-purpose dictionary in one case and a technical dictionary in the other, or using dictionaries of vastly different sizes may severely affect the quality of results.1 We more generally propose to use bilingual corpora for the problem of inducing semantic association networks, where we particularly have e.g. sentence-aligned corpora like the Europarl corpus (Koehn, 2005) in mind (see also the study of Rama and Borin (2011) on cognates, with Europarl as the data basis). Then, translation relations Ti may be induced from these corpora by applying a statistical machine translation approach such as the Moses toolkit (Koehn et al., 2007). The translation relations may thus be probabilistic instead of binary, which may either be resolved via thresholding or by modifying Equation (1) as in Ei = {(u, v) |u, v ∈ Vi , uTi x, xTi v di (u, v) = for some x ∈ W [Li ]}, where by W [L] we denote the words of language L and by aTi b we denote that a translates into b under Ti"
S12-1015,P07-2045,0,0.00370833,"Missing"
S12-1015,W11-4622,0,0.0114124,"ly available (and much less freely available); moreover, for the above described semantic association networks, it may be of crucial importance to have comparable data sources; e.g. using a general-purpose dictionary in one case and a technical dictionary in the other, or using dictionaries of vastly different sizes may severely affect the quality of results.1 We more generally propose to use bilingual corpora for the problem of inducing semantic association networks, where we particularly have e.g. sentence-aligned corpora like the Europarl corpus (Koehn, 2005) in mind (see also the study of Rama and Borin (2011) on cognates, with Europarl as the data basis). Then, translation relations Ti may be induced from these corpora by applying a statistical machine translation approach such as the Moses toolkit (Koehn et al., 2007). The translation relations may thus be probabilistic instead of binary, which may either be resolved via thresholding or by modifying Equation (1) as in Ei = {(u, v) |u, v ∈ Vi , uTi x, xTi v di (u, v) = for some x ∈ W [Li ]}, where by W [L] we denote the words of language L and by aTi b we denote that a translates into b under Ti ; moreover, we assume Ti to be symmetric such that t"
S15-1014,P05-1074,0,0.143843,"ages, but this would have required 220 − 1 > 1 million comparisons. 5 Conclusion We have encoded lexical semantic spaces of different languages by means of the same pivot language in order to make the languages comparable. To this end, we introduced association networks in which links between words in the reference language depend on translations from the respective source language, weighted by probability of translation. Our methodology is closely related to analogous approaches in the paraphrasing community which interlink paraphrases by means of their translations in other languages (e.g., Bannard and Callison-Burch (2005), Kok and Brockett (2010)), but our application scenario is different and we also describe a principled manner to generate weighted links between lexical units from multilingual data. Using random walks to represent similarities among words in the association networks, we finally derived similarity values for pairs of languages. This allowed us to perform several cluster analyses to group the 20 source languages. Interestingly, in our data sample, semantic language classification appears to be almost perfectly correlated with genealogical relationships between languages. To the best of our kno"
S15-1014,P11-1041,0,0.0182122,"genealogical relations. To our knowledge, this constitutes the first large-scale quantitative lexical semantic typology that is completely unsupervised, bottom-up, and datadriven. Our results may be important for the decision which multilingual resources to integrate in a semantic evaluation task. 1 Introduction There has been a recent surge of interest in integrating multilingual resources in natural language processing (NLP). For example, Snyder et al. (2008) show that jointly considering morphological segmentations across languages improves performance compared to the monolingual baseline. Bhargava and Kondrak (2011) and Bhargava and Kondrak (2012) demonstrate that string transduction can benefit from supplemental information provided in other languages. Analogously, in lexical semantics, Navigli and Ponzetto (2012) explore semantic relations from Wikipedia in different languages to induce a huge integrated lexical semantic network. In this paper, we also focus on multilingual resources in lexical semantics. But rather than integrating them, we investigate their (dis-)similarities. In order to address this question, we provide a translation-based model of lexical semantic spaces. Our approach is to genera"
S15-1014,N12-1044,0,0.0232595,"nowledge, this constitutes the first large-scale quantitative lexical semantic typology that is completely unsupervised, bottom-up, and datadriven. Our results may be important for the decision which multilingual resources to integrate in a semantic evaluation task. 1 Introduction There has been a recent surge of interest in integrating multilingual resources in natural language processing (NLP). For example, Snyder et al. (2008) show that jointly considering morphological segmentations across languages improves performance compared to the monolingual baseline. Bhargava and Kondrak (2011) and Bhargava and Kondrak (2012) demonstrate that string transduction can benefit from supplemental information provided in other languages. Analogously, in lexical semantics, Navigli and Ponzetto (2012) explore semantic relations from Wikipedia in different languages to induce a huge integrated lexical semantic network. In this paper, we also focus on multilingual resources in lexical semantics. But rather than integrating them, we investigate their (dis-)similarities. In order to address this question, we provide a translation-based model of lexical semantic spaces. Our approach is to generate association networks in which"
S15-1014,D14-1112,0,0.0129445,"ply their methodology to the multilingual setup, however, which a typology necessitates. Orthographic, phonetic and syntactic similarity of languages have received considerably more attention than semantic similarity, as we focus on. Classical approaches in determining orthographic/phonetic relatedness of languages are based on lexico-statistical comparisons of items in standardized word lists (Campbell, 2003; Rama and Borin, 2015), such as the Swadesh lists (Swadesh, 1955). Rama and Borin (2015) study the impact of different string similarity measures on orthographic language classification. Ciobanu and Dinu (2014) measure orthographic similarity between Romanian and related languages. They also indicate applications of (knowledge of) similarity values between languages, such as serving as a guide for machine translation (Scannell, 2006). Koehn (2005) produces a genealogical clustering of the languages in Europarl based on ease of translation, as measured in BLEU scores, between any two languages (which, putatively, yields a syntactic similarity indication). This results in an imperfect reproduction of the geMAN vir PERSON HUMAN MAN homo HUSBAND WARRIOR PERSON HUMAN heros mensch HUSBAND HERO GUY DEMIGOD"
S15-1014,S12-1015,1,0.904617,"Missing"
S15-1014,D09-1124,0,0.183374,"4 details our experiments on clustering semantic spaces across selected languages of the European Union. We conclude in Section 5. 2 Related work A field related to our research is semantic relatedness, in which the task is to determine the degree of semantic similarity between pairs of words, such as tiger and cat, sex and love, etc. Classically, semantic word networks such as WordNet (Fellbaum, 1998) or EuroWordNet (Vossen, 1998) have been used to address this problem (Jiang and Conrath, 1997), and, more recently, taxonomies and knowledge bases such as Wikipedia (Strube and Ponzetto, 2006). Hassan and Mihalcea (2009) define the task of cross-lingual semantic relatedness, in which the goal is to determine the semantic similarity between words from different languages, and Navigli and Ponzetto (2012) have combined WordNet with Wikipedia to construct a multi-layer semantic net128 work in which computation of cross-lingual semantic relatedness may be performed. Most recently, neural network-based distributed semantic representations focusing on cross-language similarities between words and larger textual units have become popular (Chandar A P et al. (2014), Hermann and Blunsom (2014), Mikolov et al. (2013))."
S15-1014,P14-1006,0,0.0406309,"rube and Ponzetto, 2006). Hassan and Mihalcea (2009) define the task of cross-lingual semantic relatedness, in which the goal is to determine the semantic similarity between words from different languages, and Navigli and Ponzetto (2012) have combined WordNet with Wikipedia to construct a multi-layer semantic net128 work in which computation of cross-lingual semantic relatedness may be performed. Most recently, neural network-based distributed semantic representations focusing on cross-language similarities between words and larger textual units have become popular (Chandar A P et al. (2014), Hermann and Blunsom (2014), Mikolov et al. (2013)). There have been (a) few different computational approaches to semantic language classification. Mehler et al. (2011) test whether languages are genealogically separable via topological properties of semantic (concept) graphs derived from Wikipedia. This approach is top-down in that it assumes that the genealogical tree is the desired output of the classification. Cooper (2008) computes semantic distances between languages based on the curvature of translation histograms in bilingual dictionaries. While this results in some interesting findings as indicated, the approa"
S15-1014,O97-1002,0,0.143652,"arget. The paper is structured as follows. Section 2 outlines related work. Section 3 presents our formal model and Section 4 details our experiments on clustering semantic spaces across selected languages of the European Union. We conclude in Section 5. 2 Related work A field related to our research is semantic relatedness, in which the task is to determine the degree of semantic similarity between pairs of words, such as tiger and cat, sex and love, etc. Classically, semantic word networks such as WordNet (Fellbaum, 1998) or EuroWordNet (Vossen, 1998) have been used to address this problem (Jiang and Conrath, 1997), and, more recently, taxonomies and knowledge bases such as Wikipedia (Strube and Ponzetto, 2006). Hassan and Mihalcea (2009) define the task of cross-lingual semantic relatedness, in which the goal is to determine the semantic similarity between words from different languages, and Navigli and Ponzetto (2012) have combined WordNet with Wikipedia to construct a multi-layer semantic net128 work in which computation of cross-lingual semantic relatedness may be performed. Most recently, neural network-based distributed semantic representations focusing on cross-language similarities between words"
S15-1014,2005.mtsummit-papers.11,0,0.0404005,"es in determining orthographic/phonetic relatedness of languages are based on lexico-statistical comparisons of items in standardized word lists (Campbell, 2003; Rama and Borin, 2015), such as the Swadesh lists (Swadesh, 1955). Rama and Borin (2015) study the impact of different string similarity measures on orthographic language classification. Ciobanu and Dinu (2014) measure orthographic similarity between Romanian and related languages. They also indicate applications of (knowledge of) similarity values between languages, such as serving as a guide for machine translation (Scannell, 2006). Koehn (2005) produces a genealogical clustering of the languages in Europarl based on ease of translation, as measured in BLEU scores, between any two languages (which, putatively, yields a syntactic similarity indication). This results in an imperfect reproduction of the geMAN vir PERSON HUMAN MAN homo HUSBAND WARRIOR PERSON HUMAN heros mensch HUSBAND HERO GUY DEMIGOD MALE (a) English-Latin mann (b) English-German Figure 1: Excerpts of bilingual dictionaries as bipartite graphs with links between words if and only if one is a translation of the other. Data from www.latin-dictionary.net and dict.leo.org."
S15-1014,N10-1017,0,0.0753569,"ork generation, we first define the vector representation of node v k in graph Gk = (Vk , Wk ) as the probability vector of ending up in any of the nodes of Gk when a random surfer starts from v k and surfs on the graph Gk according to the normalized weight matrix Wk = [Wk (α, β)](α,β)∈Vk ×Vk . Note that the higher Wk (α, β), the higher the likelihood that the surfer takes the transition from α to β. More precisely, we let the meaning [[v k ]] of node v k in graph Gk be the vector vk that results as the limit of the iterative process (see, e.g., Brin and Page (1998), Gaume and Mathieu (2008), Kok and Brockett (2010)), k k (k) + (1 − d)v0k , vN +1 = dvN A k , for N ≥ 0, is a 1 × |Rvoc |vector, where each vN (k) A is obtained from Wk by normalizing all rows such that A(k) is row-stochastic, and d is a damping factor that describes preference for the starting vector v0k , which is a vector of zeros except for index 3 More correctly, one could define Pk [α|z] = f1z , whenever α is a translation of z, and Pk [α|z] = 0, otherwise, where fz is the number of translations of word z. This would lead to an analogous interpretation as the given one. 4 This reasoning ignores cases of homonymy, which weaken the semant"
S15-1014,J03-1002,0,0.00404703,"ontrasting such networks may then allow for clustering languages due to shared lexical semantic associations. As mentioned above, we generalize the model outlined so far to the situation of probabilistic translation relationships derived from corpus data, rather than from bilingual dictionaries. Working on corpus data has both advantages and disadvantages compared to using human compiled and edited dictionaries. On the one hand, • the translation relations induced from corpus data are noisy since their estimation is partially inaccurate due to limitations of alignment toolkits such as GIZA++ (Och and Ney, 2003) as employed by us. Implications of this inaccuracy are outlined below. • By using unannotated corpora, we cannot straightforwardly distinguish between cases of polysemy and homonymy. The problem is that homonymy should (ideally) not contribute to generating lexical semantic association networks as considered here. However, homonymy is apparently a rather rare phenomenon, while polysemy, which we expect to underlie the structure of our networks, is abundant (cf. L¨obner (2002)). On the other hand, • classical dictionaries can be very heterogeneous in their scope and denomination of translation"
S15-1014,D08-1109,0,0.0329044,"the obtained (crosslingually comparable) lexical semantic spaces. We find that, in our sample of languages, lexical semantic spaces largely coincide with genealogical relations. To our knowledge, this constitutes the first large-scale quantitative lexical semantic typology that is completely unsupervised, bottom-up, and datadriven. Our results may be important for the decision which multilingual resources to integrate in a semantic evaluation task. 1 Introduction There has been a recent surge of interest in integrating multilingual resources in natural language processing (NLP). For example, Snyder et al. (2008) show that jointly considering morphological segmentations across languages improves performance compared to the monolingual baseline. Bhargava and Kondrak (2011) and Bhargava and Kondrak (2012) demonstrate that string transduction can benefit from supplemental information provided in other languages. Analogously, in lexical semantics, Navigli and Ponzetto (2012) explore semantic relations from Wikipedia in different languages to induce a huge integrated lexical semantic network. In this paper, we also focus on multilingual resources in lexical semantics. But rather than integrating them, we i"
S17-2163,S17-2091,0,0.0764103,"Missing"
S17-2163,N16-1175,0,0.0333031,"Missing"
S17-2163,P14-2050,0,0.0187058,"we frame our classification problem as a mapping fθ (θ represents model parameters) from concatenated word embeddings to one of the three classes MATERIAL, PROCESS, and TASK: fθ : R`·d × Rc·d × Rr·d → {M, P, T}. Next, we describe the embeddings that we used and subsequently the machine learning models fθ . Word Embeddings We experimented with three kinds of word embeddings. We use the popular Glove embeddings (Pennington et al., 2014) (6B) of dimensions 50, 100, and 300, which largely capture semantic information. Further we employ the more syntactically oriented 300-dimensional embeddings of Levy and Goldberg (2014), as well as the 300dimensional embeddings of Komninos and Manandhar (2016), which are trained to predict both dependency- and standard window-based context. Deep Learning models Table 1: Class distribution in the datasets. Our first model is a character-level convolutional neural network (char-CNN) illustrated in Figure 1. This model (A) considers each of the three contexts (left, center, right) independently, representing them by a 100-dimensional vector as follows. Each character is represented by a 1-hot vector, which is then mapped to a 32-dimensional Inter-annotator agreement for the dat"
S17-2163,D14-1162,0,0.0792416,"Missing"
S17-2163,L16-1294,0,0.0161386,"nd standard window-based context. Deep Learning models Table 1: Class distribution in the datasets. Our first model is a character-level convolutional neural network (char-CNN) illustrated in Figure 1. This model (A) considers each of the three contexts (left, center, right) independently, representing them by a 100-dimensional vector as follows. Each character is represented by a 1-hot vector, which is then mapped to a 32-dimensional Inter-annotator agreement for the dataset was published to be between 0.45 and 0.85 (Cohen’s κ) (Augenstein et al., 2017). Reviewing similar annotation efforts (QasemiZadeh and Schumann, 2016) already shows that despite the seemingly simple annotation task, usually annotators do not reach high agreement neither on span of annotations nor the class assigned to each span3 . 3 100-d Implemented Approaches In this section, we describe the individual systems that form the basis of our experiments (see §4). Our basic setup for all of our systems was as follows. For each keyphrase we extracted its left context, right context and the keyphrase itself (center). We represent each of the three contexts as the concatenation of their word tokens: to have fixed-size representations, we limit the"
W15-3716,P11-1089,0,0.0594503,"Missing"
W15-3716,W07-0905,0,0.0135909,"25 898 4 821 9 594 113 87 75 44 32 103 87 109 14 11 Table 2: Distribution of the lexicon entries over the different parts of speech. (x, y) pairs as indicated. To transduce/lemmatize a new input form, rules (and their exceptions) are ordered, and the first condition that is satisfied fires the corresponding rule. inform each other. Lee et al. (2011) show that tagging and dependency parsing may mutually inform each other in such a setup, too. Concerning lexical resources for Latin, to our knowledge, there are concurrently three freely available resources for Latin: Perseus (Smith et al., 2000; Bamman and Crane, 2007), Proiel (Haug and Jøhndal, 2008), and the Index Thomisticus (IT) (Busa, 1980; McGilivray et al., 2009). Perseus and Proiel cover the more classical Latin era, while IT focuses on the writings of Thomas Aquinas. All resources indicate lemma and various part-of-speech information for its tokens. IT in addition provides dependency information. Concerning size, Perseus is the smallest resource with roughly 3 500 sentences, and Proiel and IT each contain about 13 000–14 000 Latin sentences. 4 5 Part-of-speech taggers Here, we briefly sketch the taggers we survey in Section 6. All taggers outlined"
W15-3716,D12-1133,0,0.0241411,"erable popularity up until recently. Concurrently, two freely available TreeTagger taggers for Latin are available.8 TnT (Brants, 2000) implements a trigram Hidden Markov tagger with a module for handling unknown words. It has been shown to perform similarly well as maximum entropy models. Lapos (Tsuruoka et al., 2011) is a ‘history based’ tagging model (this model class subsumes maximum entropy Markov model) incorporating a lookahead mechanism into its decision-making process. It has been reported to be competitive with globally optimized models such as CRFs and structured perceptrons. Mate (Bohnet and Nivre, 2012) implements a transition based system for joint part-of-speech tagging and dependency parsing reported to exhibit high performance for richly Lemmatization On our corpus, we learn a character-level string transducer as a component model of our tagger. This lemmatizer is trained on pairs of strings (x, y) where x is a full form (e.g., amavisse ‘have loved’) and y its corresponding lemma (e.g., amo ‘love’). Learning a statistical lemmatizer has the advantage that it can cope with OOV words and may adapt to the distribution of the corpus. Our lemmatization module is LemmaGen (Jurˇsiˇc et al., 201"
W15-3716,N15-1055,0,0.0650386,"Missing"
W15-3716,P13-1068,0,0.037986,"Missing"
W15-3716,D13-1032,0,0.0939245,"Missing"
W15-3716,A00-1031,0,0.575144,"ion provides dependency information. Concerning size, Perseus is the smallest resource with roughly 3 500 sentences, and Proiel and IT each contain about 13 000–14 000 Latin sentences. 4 5 Part-of-speech taggers Here, we briefly sketch the taggers we survey in Section 6. All taggers outlined are languageindependent and general-purpose taggers. The TreeTagger (Schmid, 1994) implements a tagger based on decision trees. Despite its simple architecture, it seems to enjoy considerable popularity up until recently. Concurrently, two freely available TreeTagger taggers for Latin are available.8 TnT (Brants, 2000) implements a trigram Hidden Markov tagger with a module for handling unknown words. It has been shown to perform similarly well as maximum entropy models. Lapos (Tsuruoka et al., 2011) is a ‘history based’ tagging model (this model class subsumes maximum entropy Markov model) incorporating a lookahead mechanism into its decision-making process. It has been reported to be competitive with globally optimized models such as CRFs and structured perceptrons. Mate (Bohnet and Nivre, 2012) implements a transition based system for joint part-of-speech tagging and dependency parsing reported to exhibi"
W15-3716,W02-1001,0,0.250134,"e, and the TreeTagger for the category ‘PoS’ (similar curves for the other tagging subtasks). Apparently, the more recent tagger generation generalizes substantially better than the older approaches, exhibiting much higher accuracies especially at small training set sizes. inflected languages, where there may be considerable dependence between morphology and syntax, as well as for more configurational languages like English. The OpenNLPTagger is an official Apache project and provides three different tagging methods: maximum entropy, perceptron and perceptron sequence (cf. (Ratnaparkhi, 1996; Collins, 2002)) for maximum/perceptron based entropy tagging). We evaluated the maximum entropy and the perceptron approach.9 The Stanford tagger (Toutanova et al., 2003) implements a bidirectional log-linear model that makes broad use of lexical features. The implementation lets the user specifically activate and deactivate desired features. We use default parametrizations for all taggers10 and trained all taggers on a random sample of our data of about 14 000 sentences and test them on the remainder of about 1 500 sentences. 6 6.2 Lemma accuracy is indicated in Table 4. As we mentioned, we employ two lemm"
W15-3716,W96-0213,0,0.50231,"taggers Lapos, Mate, and the TreeTagger for the category ‘PoS’ (similar curves for the other tagging subtasks). Apparently, the more recent tagger generation generalizes substantially better than the older approaches, exhibiting much higher accuracies especially at small training set sizes. inflected languages, where there may be considerable dependence between morphology and syntax, as well as for more configurational languages like English. The OpenNLPTagger is an official Apache project and provides three different tagging methods: maximum entropy, perceptron and perceptron sequence (cf. (Ratnaparkhi, 1996; Collins, 2002)) for maximum/perceptron based entropy tagging). We evaluated the maximum entropy and the perceptron approach.9 The Stanford tagger (Toutanova et al., 2003) implements a bidirectional log-linear model that makes broad use of lexical features. The implementation lets the user specifically activate and deactivate desired features. We use default parametrizations for all taggers10 and trained all taggers on a random sample of our data of about 14 000 sentences and test them on the remainder of about 1 500 sentences. 6 6.2 Lemma accuracy is indicated in Table 4. As we mentioned, we"
W15-3716,P09-1055,0,0.0652057,"Missing"
W15-3716,N03-1033,0,0.124082,"izes substantially better than the older approaches, exhibiting much higher accuracies especially at small training set sizes. inflected languages, where there may be considerable dependence between morphology and syntax, as well as for more configurational languages like English. The OpenNLPTagger is an official Apache project and provides three different tagging methods: maximum entropy, perceptron and perceptron sequence (cf. (Ratnaparkhi, 1996; Collins, 2002)) for maximum/perceptron based entropy tagging). We evaluated the maximum entropy and the perceptron approach.9 The Stanford tagger (Toutanova et al., 2003) implements a bidirectional log-linear model that makes broad use of lexical features. The implementation lets the user specifically activate and deactivate desired features. We use default parametrizations for all taggers10 and trained all taggers on a random sample of our data of about 14 000 sentences and test them on the remainder of about 1 500 sentences. 6 6.2 Lemma accuracy is indicated in Table 4. As we mentioned, we employ two lemmatization strategies based on the taggers’ outputs: either the lemma is retrieved from the lexicon given the predicted part-of-speech and the morphological"
W15-3716,W11-0328,0,0.150195,"ntences. 4 5 Part-of-speech taggers Here, we briefly sketch the taggers we survey in Section 6. All taggers outlined are languageindependent and general-purpose taggers. The TreeTagger (Schmid, 1994) implements a tagger based on decision trees. Despite its simple architecture, it seems to enjoy considerable popularity up until recently. Concurrently, two freely available TreeTagger taggers for Latin are available.8 TnT (Brants, 2000) implements a trigram Hidden Markov tagger with a module for handling unknown words. It has been shown to perform similarly well as maximum entropy models. Lapos (Tsuruoka et al., 2011) is a ‘history based’ tagging model (this model class subsumes maximum entropy Markov model) incorporating a lookahead mechanism into its decision-making process. It has been reported to be competitive with globally optimized models such as CRFs and structured perceptrons. Mate (Bohnet and Nivre, 2012) implements a transition based system for joint part-of-speech tagging and dependency parsing reported to exhibit high performance for richly Lemmatization On our corpus, we learn a character-level string transducer as a component model of our tagger. This lemmatizer is trained on pairs of string"
W18-4508,W14-2302,0,0.0211778,"ployed in previous work unnecessary. This includes e.g., abstractness values to model source and target concepts (Tsvetkov This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 70 Proceedings of Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 70–80 Santa Fe, New Mexico, USA, August 25, 2018. et al., 2014; Turney and Assaf, 2011), selectional preference violations (Wilks, 1978; Shutova, 2013) or topic modeling (Heintz et al., 2013; Beigman Klebanov et al., 2014). In contrast, because they are the only external resource we utilize, we investigate the influence of an important hyper-parameter of our network—different pre-trained embeddings—on the token-level metaphor detection task and show the genre-specific effects of these embedding models. 2 Related work Classification and detection of non-literal language has largely focused on metaphor detection. Another prominent task is the detection of idiomatic language. Similar features have been employed in those tasks, even though the specific phenomena differ. However, since the datasets used for these ta"
W18-4508,W16-1104,1,0.799586,"tasets created and used in these works often follow no unified annotation guidelines (compare Steen et al. (2010) and Tsvetkov et al. (2014)), or even no disclosed guidelines at all, e.g., Heintz et al. (2013), or annotate metaphors at different levels of granularity (Steen et al., 2010; Gutierrez et al., 2016). This is also true for many works in more general non-literal language detection. Consequently, methods are seldom compared on related tasks. Neural networks have been successfully applied to various natural language processing tasks, but few have applied them to metaphor detection (Do Dinh and Gurevych, 2016; Rei et al., 2017) or detection of non-literal and figurative language in general. In this paper, we test whether the same simple generic neural network approach is effective for four different non-literal language detection tasks: token and construction level metaphor detection, idiom classification and classification of literal and non-literal German particle verbs. We train a neural model using LSTMs to encode the context of a metaphor candidate or non-literal compound. We show that our approach outperforms existing state-of-the-art models on two tasks, while producing competitive results"
W18-4508,C18-1132,1,0.767991,"del using LSTMs to encode the context of a metaphor candidate or non-literal compound. We show that our approach outperforms existing state-of-the-art models on two tasks, while producing competitive results on another task, independent of the mode of classification (e.g., token vs. construction classification). In demonstrating the applicability of the same, simple neural network architecture to different non-literal language tasks, we lay the foundation for a more integrative approach. A joint modeling of these tasks, through data concatenation and multi-task learning, is investigated in Do Dinh et al. (2018). Given enough training data, our model renders many of the handcrafted features employed in previous work unnecessary. This includes e.g., abstractness values to model source and target concepts (Tsvetkov This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 70 Proceedings of Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 70–80 Santa Fe, New Mexico, USA, August 25, 2018. et al., 2014; Turney and Assaf, 2011), selectional preference viola"
W18-4508,P16-1018,0,0.0195188,"nently metaphor detection (Veale et al., 2016), has been plentiful. For metaphor detection, most works name the Conceptual Metaphor Theory (Lakoff and Johnson, 1980) as their underlying framework, in which metaphors are modeled as cognitive mappings of concepts from a source to a target domain. However, the datasets created and used in these works often follow no unified annotation guidelines (compare Steen et al. (2010) and Tsvetkov et al. (2014)), or even no disclosed guidelines at all, e.g., Heintz et al. (2013), or annotate metaphors at different levels of granularity (Steen et al., 2010; Gutierrez et al., 2016). This is also true for many works in more general non-literal language detection. Consequently, methods are seldom compared on related tasks. Neural networks have been successfully applied to various natural language processing tasks, but few have applied them to metaphor detection (Do Dinh and Gurevych, 2016; Rei et al., 2017) or detection of non-literal and figurative language in general. In this paper, we test whether the same simple generic neural network approach is effective for four different non-literal language detection tasks: token and construction level metaphor detection, idiom c"
W18-4508,W13-0908,0,0.0888009,"na. 1 Introduction Computational research of non-literal phenomena, e.g., metonymy, idiom, and prominently metaphor detection (Veale et al., 2016), has been plentiful. For metaphor detection, most works name the Conceptual Metaphor Theory (Lakoff and Johnson, 1980) as their underlying framework, in which metaphors are modeled as cognitive mappings of concepts from a source to a target domain. However, the datasets created and used in these works often follow no unified annotation guidelines (compare Steen et al. (2010) and Tsvetkov et al. (2014)), or even no disclosed guidelines at all, e.g., Heintz et al. (2013), or annotate metaphors at different levels of granularity (Steen et al., 2010; Gutierrez et al., 2016). This is also true for many works in more general non-literal language detection. Consequently, methods are seldom compared on related tasks. Neural networks have been successfully applied to various natural language processing tasks, but few have applied them to metaphor detection (Do Dinh and Gurevych, 2016; Rei et al., 2017) or detection of non-literal and figurative language in general. In this paper, we test whether the same simple generic neural network approach is effective for four d"
W18-4508,L16-1135,0,0.382951,"015) investigate metonymy identification, i.e. identification of instances where entities replace other associated entities. For example in the sentence “Washington and Beijing enter new trade talks”, Washington and Beijing are used to refer to the US and Chinese governments. Zhang and Gelernter (2015) reuse many features commonly used for the metaphor detection task, such as imageability and abstractness ratings. They further test different word representations—word embeddings, LSA, and one-hot-encoding—to detect metonymy using an SVM. A different non-literal language task is investigated by Horbach et al. (2016), in which they classify literal and idiomatic use of different German infinitive-verb compounds based on their context. They employ Naive Bayes and various features—including local skip-n-grams, POS tags, automatically obtained subject and object information, selectional preferences, and manually annotated topic information. K¨oper and Schulte im Walde (2016) classify literally and non-literally used German particle verbs across 10 particles. Using a random forest classifier and various features (e.g., unigrams, affective ratings, distributed word representations), they achieve an accuracy of"
W18-4508,N16-1175,0,0.144717,"ng on the task. For example, for the infinitive-verb classification, the annotated instance can consist of two tokens, thus we can have two center embeddings. To illustrate, consider the example: “Kinder sollten nicht mehr sitzen bleiben m¨ussen, sondern gef¨ordert werden.” In this sentence, we use (Kinder,sollten,nicht,mehr) as left context, (sitzen,bleiben) as center, and (m¨ussen,sondern,gef¨ordert,werden) as right context (see Figure 1). For the tasks with German data we use the word embeddings of Reimers et al. (2014). For construction level metaphor detection we employ the embeddings of Komninos and Manandhar (2016) as preliminary cross validation experiments on the training set show that they work well. On the other hand, preliminary experiments on the development set for token level metaphor detection show an advantage of the Google News word2vec embeddings (Mikolov et al., 2013) for this task, which is why we use them to work on the VUAMC (a more in-depth analysis validates our decision, Section 6). We conduct our experiments using Keras1 and Theano2 . We make our code publicly available3 . 5 Results The main results are laid out in Table 2, results broken down into subcorpora are shown in Table 3 (to"
W18-4508,N16-1039,0,0.0401523,"Missing"
W18-4508,E17-2086,0,0.0460612,"Missing"
W18-4508,P14-2050,0,0.0274791,"cles that the embeddings are trained on. Independently of the concrete embeddings used, the network performs consistently best on the news subcorpus, followed by the academic texts. Looking more closely into the classifications on the fiction subcorpus, we observe a large performance difference between Glove and the remaining embeddings. This is mainly due to low recall (0.356, see also Table 6), especially compared to the word2vec embeddings (0.710). The results on the conversation subcorpus are similarly noteworthy, because here both embedding models that encode dependency information, from Levy and Goldberg (2014) and Komninos and Manandhar (2016), perform worse than the remaining models (also due to lower recall). This is in line with our findings from Section 5.1 where we note that our network struggles with omissions or ungrammatical sentences—as the structure of the conversation sentences is more likely to be irregular, including “correct” syntactic information can apparently be detrimental. P word2vec GloVe ConceptNet Levy Komninos .576 .544 .604 .652 .634 academic R F1 .706 .594 .654 .535 .628 .634 .568 .628 .588 .631 P conversation R F1 .567 .470 .595 .629 .652 .518 .584 .478 .439 .396 .541 .521"
W18-4508,D14-1162,0,0.0797149,"Missing"
W18-4508,D17-1162,0,0.251401,"n these works often follow no unified annotation guidelines (compare Steen et al. (2010) and Tsvetkov et al. (2014)), or even no disclosed guidelines at all, e.g., Heintz et al. (2013), or annotate metaphors at different levels of granularity (Steen et al., 2010; Gutierrez et al., 2016). This is also true for many works in more general non-literal language detection. Consequently, methods are seldom compared on related tasks. Neural networks have been successfully applied to various natural language processing tasks, but few have applied them to metaphor detection (Do Dinh and Gurevych, 2016; Rei et al., 2017) or detection of non-literal and figurative language in general. In this paper, we test whether the same simple generic neural network approach is effective for four different non-literal language detection tasks: token and construction level metaphor detection, idiom classification and classification of literal and non-literal German particle verbs. We train a neural model using LSTMs to encode the context of a metaphor candidate or non-literal compound. We show that our approach outperforms existing state-of-the-art models on two tasks, while producing competitive results on another task, in"
W18-4508,schafer-bildhauer-2012-building,0,0.0372383,"Missing"
W18-4508,S13-1040,0,0.0220763,"ing data, our model renders many of the handcrafted features employed in previous work unnecessary. This includes e.g., abstractness values to model source and target concepts (Tsvetkov This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 70 Proceedings of Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 70–80 Santa Fe, New Mexico, USA, August 25, 2018. et al., 2014; Turney and Assaf, 2011), selectional preference violations (Wilks, 1978; Shutova, 2013) or topic modeling (Heintz et al., 2013; Beigman Klebanov et al., 2014). In contrast, because they are the only external resource we utilize, we investigate the influence of an important hyper-parameter of our network—different pre-trained embeddings—on the token-level metaphor detection task and show the genre-specific effects of these embedding models. 2 Related work Classification and detection of non-literal language has largely focused on metaphor detection. Another prominent task is the detection of idiomatic language. Similar features have been employed in those tasks, even though the s"
W18-4508,P14-1024,0,0.426115,"different languages, (iii) over different non-literal language phenomena. 1 Introduction Computational research of non-literal phenomena, e.g., metonymy, idiom, and prominently metaphor detection (Veale et al., 2016), has been plentiful. For metaphor detection, most works name the Conceptual Metaphor Theory (Lakoff and Johnson, 1980) as their underlying framework, in which metaphors are modeled as cognitive mappings of concepts from a source to a target domain. However, the datasets created and used in these works often follow no unified annotation guidelines (compare Steen et al. (2010) and Tsvetkov et al. (2014)), or even no disclosed guidelines at all, e.g., Heintz et al. (2013), or annotate metaphors at different levels of granularity (Steen et al., 2010; Gutierrez et al., 2016). This is also true for many works in more general non-literal language detection. Consequently, methods are seldom compared on related tasks. Neural networks have been successfully applied to various natural language processing tasks, but few have applied them to metaphor detection (Do Dinh and Gurevych, 2016; Rei et al., 2017) or detection of non-literal and figurative language in general. In this paper, we test whether th"
W18-4508,D11-1063,0,0.0197682,"sk learning, is investigated in Do Dinh et al. (2018). Given enough training data, our model renders many of the handcrafted features employed in previous work unnecessary. This includes e.g., abstractness values to model source and target concepts (Tsvetkov This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 70 Proceedings of Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 70–80 Santa Fe, New Mexico, USA, August 25, 2018. et al., 2014; Turney and Assaf, 2011), selectional preference violations (Wilks, 1978; Shutova, 2013) or topic modeling (Heintz et al., 2013; Beigman Klebanov et al., 2014). In contrast, because they are the only external resource we utilize, we investigate the influence of an important hyper-parameter of our network—different pre-trained embeddings—on the token-level metaphor detection task and show the genre-specific effects of these embedding models. 2 Related work Classification and detection of non-literal language has largely focused on metaphor detection. Another prominent task is the detection of idiomatic language. Simil"
W18-5216,D17-1078,0,0.0242199,"k Our work connects to different strands of research. Multi-Task Learning MTL was shown to be particularly beneficial when tasks stand in a natural hierarchy and when they are syntactic in nature (Søgaard and Goldberg, 2016). Moreover, it has been claimed that further main benefits for MTL are observed when data for the main task is sparse, in which case the auxiliary tasks may act as regularizers that prevent overfitting (Ruder et al., 2017). The latter is the case for PD3-MTL with little available parallel data. MTL has also been made use of for supervised cross-lingual transfer techniques (Cotterell and Heigold, 2017; Yang et al., 2017; Kim et al., 2017; Dinh et al., 2018). These assume small training sets in L2, and a system trained on them is regularized by a larger amount of training data in L1. In contrast to these, we assume no gold labels in L2 (unsupervised transfer), which necessitates a projection step. Our approach could also be combined with these supervised ones, by adding this small gold data to the three different datasets that we use in PD3. Argumentation Mining ArgMin is a fastgrowing field in NLP with applications in decision making and the legal domain (Palau and Moens, 2009) and can be"
W18-5216,N13-1073,0,0.0615894,"Missing"
W18-5216,P17-1002,1,0.846487,"em trained on them is regularized by a larger amount of training data in L1. In contrast to these, we assume no gold labels in L2 (unsupervised transfer), which necessitates a projection step. Our approach could also be combined with these supervised ones, by adding this small gold data to the three different datasets that we use in PD3. Argumentation Mining ArgMin is a fastgrowing field in NLP with applications in decision making and the legal domain (Palau and Moens, 2009) and can be solved on sentence-level (Daxenberger et al., 2017; Niculae et al., 2017; Stab et al., 2018) or token-level (Eger et al., 2017; Schulz et al., 2018). Cross-lingual ArgMin has recently attracted interest (Aker and Zhang, 2017; Eger et al., 2018). The proposed approaches mostly used machine translation, which is unavailable for the vast majority of the world’s languages. Low-resource transfer Low-resource language transfer has recently become very popular, e.g., when relying on only very few translation pairs for bilingual embedding space induction (Artetxe et al., 2017; Zhang et al., 2016) or in unsupervised machine translation using no parallel sources at all (Artetxe et al., 2018; Lample et al., 2018). Lowresource t"
W18-5216,C18-1071,1,0.769487,"labels in L2 (unsupervised transfer), which necessitates a projection step. Our approach could also be combined with these supervised ones, by adding this small gold data to the three different datasets that we use in PD3. Argumentation Mining ArgMin is a fastgrowing field in NLP with applications in decision making and the legal domain (Palau and Moens, 2009) and can be solved on sentence-level (Daxenberger et al., 2017; Niculae et al., 2017; Stab et al., 2018) or token-level (Eger et al., 2017; Schulz et al., 2018). Cross-lingual ArgMin has recently attracted interest (Aker and Zhang, 2017; Eger et al., 2018). The proposed approaches mostly used machine translation, which is unavailable for the vast majority of the world’s languages. Low-resource transfer Low-resource language transfer has recently become very popular, e.g., when relying on only very few translation pairs for bilingual embedding space induction (Artetxe et al., 2017; Zhang et al., 2016) or in unsupervised machine translation using no parallel sources at all (Artetxe et al., 2018; Lample et al., 2018). Lowresource transfer (on a level of domains rather than languages) has also been considered in ArgMin (Schulz et al., 2018), assumi"
W18-5216,J17-1004,1,0.807192,"ur focus is particularly on argumentation mining (ArgMin), a rapidly growing research field in NLP. Cross131 Proceedings of the 5th Workshop on Argument Mining, pages 131–143 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics lingual transfer is majorly important for ArgMin because it is inherently costly to get high-quality annotations for ArgMin due to: (i) subjectivity of argumentation as well as divergent and competing ArgMin theories (Daxenberger et al., 2017; Schulz et al., 2018), leading to disagreement among crowdworkers as well as expert annotators (Habernal and Gurevych, 2017), (ii) dependence of argument annotations on background knowledge and parsing of complex pragmatic relations (Moens, 2017). Thus, in order not to reproduce the same annotation costs for new languages, cross-lingual ArgMin methods are required. These techniques should both perform well with little available parallel data, to address many languages, and with general (nonargumentative) parallel data, because this is much more likely to be available. Our experiments address both of these requirements.1 2 PD3 Let LS = {(xS , y S )} denote a set of L1 data points in which each xS is an instance and"
W18-5216,P16-1101,0,0.0408731,"We do not apply dropout or `2 regularization. We report average macro F1 scores over 20 runs with different random initializations. For PD3merge, we shuffle the merged data before training— ˆ data. ˆ S , and D i.e., mini-batches can contain LS , D T For PD3-MTL, we shuffle L1 and L2 data individually and during training we sample each mini-batch from either task according to its size. In the MTL setup, we share the CNN layer across tasks and use task-specific softmax regression layers. Sequence tagging network architecture: For token-level POS tagging, we implement a bidirectional LSTM as in Ma and Hovy (2016) and Lample et al. (2016) with a CRF output layer. This is a state-of-the-art system for sequence tagging tasks such as POS and NER. Our model uses pretrained word embeddings and optionally concatenates these with a learned character-level representation. For all experiments, we use the same network topology: we use two hidden layers with 100 hidden units each, applying dropout on the hidden units and on the word embeddings. We use Adam as optimizer. Our network uses a CRF output layer rather than a softmax classifier to account for dependencies between successive labels. In the MTL setup, we"
W18-5216,D11-1006,0,0.0516663,"Missing"
W18-5216,P17-1091,0,0.0142348,"2018). These assume small training sets in L2, and a system trained on them is regularized by a larger amount of training data in L1. In contrast to these, we assume no gold labels in L2 (unsupervised transfer), which necessitates a projection step. Our approach could also be combined with these supervised ones, by adding this small gold data to the three different datasets that we use in PD3. Argumentation Mining ArgMin is a fastgrowing field in NLP with applications in decision making and the legal domain (Palau and Moens, 2009) and can be solved on sentence-level (Daxenberger et al., 2017; Niculae et al., 2017; Stab et al., 2018) or token-level (Eger et al., 2017; Schulz et al., 2018). Cross-lingual ArgMin has recently attracted interest (Aker and Zhang, 2017; Eger et al., 2018). The proposed approaches mostly used machine translation, which is unavailable for the vast majority of the world’s languages. Low-resource transfer Low-resource language transfer has recently become very popular, e.g., when relying on only very few translation pairs for bilingual embedding space induction (Artetxe et al., 2017; Zhang et al., 2016) or in unsupervised machine translation using no parallel sources at all (Art"
W18-5216,P14-1006,0,0.0732009,"Missing"
W18-5216,D17-1302,0,0.0230469,"arch. Multi-Task Learning MTL was shown to be particularly beneficial when tasks stand in a natural hierarchy and when they are syntactic in nature (Søgaard and Goldberg, 2016). Moreover, it has been claimed that further main benefits for MTL are observed when data for the main task is sparse, in which case the auxiliary tasks may act as regularizers that prevent overfitting (Ruder et al., 2017). The latter is the case for PD3-MTL with little available parallel data. MTL has also been made use of for supervised cross-lingual transfer techniques (Cotterell and Heigold, 2017; Yang et al., 2017; Kim et al., 2017; Dinh et al., 2018). These assume small training sets in L2, and a system trained on them is regularized by a larger amount of training data in L1. In contrast to these, we assume no gold labels in L2 (unsupervised transfer), which necessitates a projection step. Our approach could also be combined with these supervised ones, by adding this small gold data to the three different datasets that we use in PD3. Argumentation Mining ArgMin is a fastgrowing field in NLP with applications in decision making and the legal domain (Palau and Moens, 2009) and can be solved on sentence-level (Daxenberger"
W18-5216,2005.mtsummit-papers.11,0,0.0324484,"tions. The difference between MTL and single-task learning (STL) is illustrated in Figure 1. STL is a network with only one task, as in PD3-merge, direct transfer and standard projection. We report average accuracy over five (or 10, in case of very little data) random weight matrix initializations. In the MTL setup, we choose a mini-batch randomly in each iteration (containing instances from only one of the tasks as in our sentence-level ArgMin experiments). Cross-lingual Embeddings: For token-level experiments, we initially train 100-d BIVCD embeddings (Vuli´c and Moens, 2015) from Europarl (Koehn, 2005) (for EN-DE) and the UN corpus (Ziemski et al., 2016) (for EN-FR), respectively. For sentence-level experiments, we use 300-d BIVCD embeddings. This means that we initially assume that high-quality bilingual word embeddings are readily available for the two languages involved. At first sight, this appears a realistic assumption since high-quality bilingual embeddings can already be obtained with very little available bilingual data (Zhang et al., 2016; Artetxe et al., 2017). In low-resource settings, however, even little monolingual data is typically available for L2 and we address this setup"
W18-5216,N16-1030,0,0.0083579,"t or `2 regularization. We report average macro F1 scores over 20 runs with different random initializations. For PD3merge, we shuffle the merged data before training— ˆ data. ˆ S , and D i.e., mini-batches can contain LS , D T For PD3-MTL, we shuffle L1 and L2 data individually and during training we sample each mini-batch from either task according to its size. In the MTL setup, we share the CNN layer across tasks and use task-specific softmax regression layers. Sequence tagging network architecture: For token-level POS tagging, we implement a bidirectional LSTM as in Ma and Hovy (2016) and Lample et al. (2016) with a CRF output layer. This is a state-of-the-art system for sequence tagging tasks such as POS and NER. Our model uses pretrained word embeddings and optionally concatenates these with a learned character-level representation. For all experiments, we use the same network topology: we use two hidden layers with 100 hidden units each, applying dropout on the hidden units and on the word embeddings. We use Adam as optimizer. Our network uses a CRF output layer rather than a softmax classifier to account for dependencies between successive labels. In the MTL setup, we use the same architecture"
W18-5216,N16-1164,0,0.0603917,"Missing"
W18-5216,N18-2006,1,0.911452,"available, the most realistic scenario for many L2 languages. While our approach is general, our focus is particularly on argumentation mining (ArgMin), a rapidly growing research field in NLP. Cross131 Proceedings of the 5th Workshop on Argument Mining, pages 131–143 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics lingual transfer is majorly important for ArgMin because it is inherently costly to get high-quality annotations for ArgMin due to: (i) subjectivity of argumentation as well as divergent and competing ArgMin theories (Daxenberger et al., 2017; Schulz et al., 2018), leading to disagreement among crowdworkers as well as expert annotators (Habernal and Gurevych, 2017), (ii) dependence of argument annotations on background knowledge and parsing of complex pragmatic relations (Moens, 2017). Thus, in order not to reproduce the same annotation costs for new languages, cross-lingual ArgMin methods are required. These techniques should both perform well with little available parallel data, to address many languages, and with general (nonargumentative) parallel data, because this is much more likely to be available. Our experiments address both of these requirem"
W18-5216,P16-2038,0,0.434073,"from projection. Finally, we combine our original dataset LS with ˆ and ˆ S and D the two pseudo-labeled dataset D T train our classifier C on it; after training, our goal in cross-lingual transfer is to apply the trained classifiers to L2 data. We denote this combination operation by ~. A simple approach is to let ~ be the “merging” (or, concatenation) of both datasets (PD3-merge). In ˆ are merged ˆ S and D this variant of PD3, LS , D T into one big dataset on which training takes place. A more sophisticated approach is to let ~ represent a multi-task learning (MTL) scenario (Caruana, 1993; Søgaard and Goldberg, 2016) in which L1 and L2 instances represent one task each (PD3ˆ , ˆ S and D MTL). Here, rather than merging LS , D T ˆ S ) as we treat source language datasets (LS and D ˆ ) as anone task and target language datasets (D T other task, each having a dedicated output layer. This leads to a different network architecture than in PD3-merge, in which we now have two separate output layers (i.e., one for each language); this distinction is also illustrated in Figure 1 below. Thus, for each input instance, we predict two outputs (e.g., two ArgMin labels), one in the source language and one in the target l"
W18-5216,N18-5005,1,0.881065,"Missing"
W18-5216,C14-1142,1,0.89661,"Missing"
W18-5216,J17-3005,1,0.895422,"Missing"
W18-5216,W12-1908,0,0.0234812,"Missing"
W18-5216,Q13-1001,0,0.0516857,"Missing"
W18-5216,N13-1126,0,0.0617233,"Missing"
W18-5216,P15-2118,0,0.056319,"Missing"
W18-5216,P95-1026,0,0.486951,", standard projection ignores the available data in L1 once the L2 dataset has been created and standard direct transfer does not use any L2 information. In this work, we investigate whether the inclusion of both L1 and L2 data outperforms transfer approaches that exploit only one type of such information, and if so, under what conditions. More precisely, we first train a system on shared features as in standard direct transfer on labeled L1 data. Then, we make use of two further datasets. One is based on the source side of parallel unlabeled data; it is derived similarly as in self-training (Yarowsky, 1995) by applying the trained system to unlabeled data, from which a pseudo-labeled dataset is derived. The other is based on its target side—using annotation projections—as in standard projection. Thus, we explore the effects of combining Projection and Direct transfer using three datasets (PD3). Our approach is detailed in §2. We report results for two L2 languages (French, German) on one sentence-level problem (argumentation mining) and one token-level problem (POS tagging). We find that our suggested approach PD3 substantially outperforms both direct transfer and projection when little parallel"
W18-5216,H01-1035,0,0.245235,"ould not only suit English, an arguably particularly simple exemplar of the world’s roughly 7,000 languages. A further motivation for cross-lingual approaches is the fact that many labeled datasets are to this date only available in English and labeled data is generally costly to obtain—be it via expert annotators or through crowd-sourcing. Therefore, methods which are capable of training on labeled data in a resource-rich language such as English and which can then be applied to typically resourcepoor other languages are highly desirable. Two standard cross-lingual approaches are projection (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Agic et al., 2016) and direct transfer (McDonald et al., 2011). Direct transfer trains, in the source language L1, on language-independent or shared features and then directly applies the trained system to the target language of interest L2. In contrast, projection trains and evaluates on L2 itself. To do so, it uses parallel data, applies a system trained on L1 to its source side and then projects the inferred labels to the parallel L2 side. This projection step may involve word alignment information. After projection, an annotated L2 dataset"
W18-5216,N16-1156,0,0.0198624,"Moens, 2009) and can be solved on sentence-level (Daxenberger et al., 2017; Niculae et al., 2017; Stab et al., 2018) or token-level (Eger et al., 2017; Schulz et al., 2018). Cross-lingual ArgMin has recently attracted interest (Aker and Zhang, 2017; Eger et al., 2018). The proposed approaches mostly used machine translation, which is unavailable for the vast majority of the world’s languages. Low-resource transfer Low-resource language transfer has recently become very popular, e.g., when relying on only very few translation pairs for bilingual embedding space induction (Artetxe et al., 2017; Zhang et al., 2016) or in unsupervised machine translation using no parallel sources at all (Artetxe et al., 2018; Lample et al., 2018). Lowresource transfer (on a level of domains rather than languages) has also been considered in ArgMin (Schulz et al., 2018), assuming little annotated data in a new target domain due to annotation costs of ArgMin as a subjective high-level task. 138 7 Concluding Remarks We combined direct transfer with annotation projection, addressing short-comings of both methods and combining their strengths. We saw consistent gains over either of the two methods in isolation, particularly i"
W18-5216,L16-1561,0,0.0222929,"-task learning (STL) is illustrated in Figure 1. STL is a network with only one task, as in PD3-merge, direct transfer and standard projection. We report average accuracy over five (or 10, in case of very little data) random weight matrix initializations. In the MTL setup, we choose a mini-batch randomly in each iteration (containing instances from only one of the tasks as in our sentence-level ArgMin experiments). Cross-lingual Embeddings: For token-level experiments, we initially train 100-d BIVCD embeddings (Vuli´c and Moens, 2015) from Europarl (Koehn, 2005) (for EN-DE) and the UN corpus (Ziemski et al., 2016) (for EN-FR), respectively. For sentence-level experiments, we use 300-d BIVCD embeddings. This means that we initially assume that high-quality bilingual word embeddings are readily available for the two languages involved. At first sight, this appears a realistic assumption since high-quality bilingual embeddings can already be obtained with very little available bilingual data (Zhang et al., 2016; Artetxe et al., 2017). In low-resource settings, however, even little monolingual data is typically available for L2 and we address this setup subsequently. 3 We choose only 800 sentences in order"
W19-4308,D18-2029,0,0.0448298,"Missing"
W19-4308,L18-1269,0,0.0526912,"Conneau et al., 2018). Our motivation is to assemble diverse observations from different published works regarding problematic aspects of the emerging field of sentence encoders. We do so in order to provide future research with an easy-to-access reference about issues that may not (yet) be widely known. We also want to provide the newcomer to sentence encoders a guide for avoiding pitfalls that even experienced researchers have fallen prey to. We also recommend best practices, from our viewpoint. 2 Setup We compare several freely available sentence encoders (listed in Table 1) with SentEval (Conneau and Kiela, 2018), using its default settings. SentEval trains a logistic regression classifier for specific downstream tasks with the sentence embeddings as the input. We compare 6 downstream tasks from the fields of sentiment analysis (MR, SST), product reviews (CR), subjectivity (SUBJ), opinion polarity (MPQA), and question-type classification (TREC). In these tasks, the goal is to label a single sentence with one of several classes. We also evaluate on the STSBenchmark (Cer et al., 2017), which evaluates semantic similarity of pairs of sentences. Sentence Encoder InferSent (Conneau et al., 2017) Sent2Vec ("
W19-4308,D17-1070,0,0.526995,"e in a variety of contexts: (i) clustering of sentences and short texts; (ii) retrieval tasks, e.g., retrieving answer passages for a question; and (iii) when task-specific training data is scarce—i.e., when the full potential of task-specific word-level representation approaches cannot be leveraged (Subramanian et al., 2018). The popularity of sentence encoders has led to a large variety of proposed techniques. These range from ‘complex’ unsupervised RNN models predicting context sentences (Kiros et al., 2015) to supervised RNN models predicting semantic relationships between sentence pairs (Conneau et al., 2017). Even more complex models learn sentence embeddings in a multi-task setup (Subramanian et al., 2018). In contrast, ‘simple’ encoders compute sentence embeddings as an elementary function of word embeddings. They compute a weighted average of word embeddings and then modify these representations via principal component analysis (SIF) (Arora et al., 2017); average n-gram embeddings (Sent2Vec) (Pagliardini et al., 2018); consider generalized pooling mechanisms (Shen et al., 2018; R¨uckl´e et al., 2018); or combine word embeddings via randomly initialized projection matrices (Wieting and Kiela, 2"
W19-4308,P18-1198,0,0.346707,"er average word embeddings. Therefore, we strongly encourage future research to compare embeddings of the same sizes to provide a fair evaluation (or at least similar sizes). We compile several pitfalls when evaluating and comparing sentence encoders. These relate to (i) the embedding sizes, (ii) normalization of embeddings before feeding them to classifiers, and (iii) unsupervised semantic similarity evaluation. We also discuss (iv) the choice of classifier used on top of sentence embeddings and (v) divergence in performance results which compare downstream tasks and so-called probing tasks (Conneau et al., 2018). Our motivation is to assemble diverse observations from different published works regarding problematic aspects of the emerging field of sentence encoders. We do so in order to provide future research with an easy-to-access reference about issues that may not (yet) be widely known. We also want to provide the newcomer to sentence encoders a guide for avoiding pitfalls that even experienced researchers have fallen prey to. We also recommend best practices, from our viewpoint. 2 Setup We compare several freely available sentence encoders (listed in Table 1) with SentEval (Conneau and Kiela, 20"
W19-4308,W16-2506,0,0.0170749,"at least logistic regression and MLP. 58 References et al. (2018), where WC had the lowest correlation. Thus, it remains unclear to which extent downstram tasks benefit from the different properties that are defined by many probing tasks. 4 Frank J. Anscombe. 1973. Graphs in Statistical Analysis. The American Statistician, 27(1):17–21. Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A Simple but Tough-to-Beat Baseline for Sentence Embeddings. In International Conference on Learning Representations (ICLR 2017). Conclusion Others have laid out problems with the evaluation of word embeddings (Faruqui et al., 2016) using word similarity tasks. They referred to the vagueness of the data underlying the tasks (as well as its annotations), the low correlations between extrinsic and intrinsic evaluations, and the lack of statistical tests. Our critique differs (in part) from this in that we also address extrinsic evaluation and the evaluation techniques themselves,3 and in that we believe that the comparison between sentence embeddings is not always fair, especially given the current evaluations using logistic regression. This implicitly favors larger embeddings, and may therefore result in misleading conclu"
W19-4308,D17-1035,1,0.858614,"ngs. 1 Introduction The field of natural language processing (NLP) is currently in upheaval. A reason for this is the success story of deep learning, which has led to ever better reported performances across many different NLP tasks, sometimes exceeding the scores achieved by humans. These fanfares of victory are echoed by isolated voices raising concern about the trustworthiness of some of the reported results. For instance, Melis et al. (2017) find that neural language models have been misleadingly evaluated and that, under fair conditions, standard LSTMs outperform more recent innovations. Reimers and Gurevych (2017) find that reporting single performance scores is insufficient for comparing nondeterministic approaches such as neural networks. Post (2018) holds that neural MT systems are unfairly compared in the literature using different variants of the BLEU score metric. In an even more general context, Lipton and Steinhardt (2018) detect several current “troubling trends” in machine learning scholarship, some of which refer to evaluation. 55 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 55–60 c Florence, Italy, August 2, 2019. 2019 Association for Computation"
W19-4308,D18-1512,0,0.0575982,"Missing"
W19-4308,P18-1041,0,0.0381135,"Missing"
W19-4308,N15-1028,0,0.0614376,"Missing"
W19-4308,D18-1425,0,0.0142195,"es when the ranking of systems is not stable across different classifiers. To our knowledge, this is an open issue. We are only aware of Subramanian et al. (2018), who evaluate a few setups both with logistic regression and using an MLP, and their results indicate that their own approach profits much more from the MLP than the InferSent embeddings they compare to (+3.4pp vs. +2.2pp). Thus, future research might focus on more suitable (difficult) datasets and sentence classification tasks for the evaluation of sentence embeddings, a lesson already learned in other fields (L¨aubli et al., 2018; Yu et al., 2018). Importantly, depending on the set of evaluated sentence encoders, such correlations can yield contradictory outcomes. For example, Conneau et al. (2018) evaluate more than 40 combinations of similar sentence encoder architectures and observe the strongest correlation with downstream task performances for WC (cf. their figure 2). This is in contrast to the correlations from the results of Perone Thus, it is not sufficient to only report results with logistic regression, and evaluations with betterperforming approaches would provide a more realistic comparison for actual use-case scenarios. We"
W19-4308,N18-1049,0,0.393004,"hese range from ‘complex’ unsupervised RNN models predicting context sentences (Kiros et al., 2015) to supervised RNN models predicting semantic relationships between sentence pairs (Conneau et al., 2017). Even more complex models learn sentence embeddings in a multi-task setup (Subramanian et al., 2018). In contrast, ‘simple’ encoders compute sentence embeddings as an elementary function of word embeddings. They compute a weighted average of word embeddings and then modify these representations via principal component analysis (SIF) (Arora et al., 2017); average n-gram embeddings (Sent2Vec) (Pagliardini et al., 2018); consider generalized pooling mechanisms (Shen et al., 2018; R¨uckl´e et al., 2018); or combine word embeddings via randomly initialized projection matrices (Wieting and Kiela, 2019). The embeddings of different encoders vary across various dimensions, the most obvious being their size. E.g., the literature has proposed embeddings ranging from 300d average word embeddings to 700d n-gram embeddings, to 4096d InferSent embeddings, to 24k dimensional random embeddings (Wieting and Kiela, 2019). Unsurprisingly, comparing embeddings of different sizes is unfair when size itself is crucially relate"
W19-4308,D14-1162,0,0.0942991,"downstream tasks with the sentence embeddings as the input. We compare 6 downstream tasks from the fields of sentiment analysis (MR, SST), product reviews (CR), subjectivity (SUBJ), opinion polarity (MPQA), and question-type classification (TREC). In these tasks, the goal is to label a single sentence with one of several classes. We also evaluate on the STSBenchmark (Cer et al., 2017), which evaluates semantic similarity of pairs of sentences. Sentence Encoder InferSent (Conneau et al., 2017) Sent2Vec (Pagliardini et al., 2018) PMeans (R¨uckl´e et al., 2018) USE (Cer et al., 2018) Avg. Glove (Pennington et al., 2014) Avg. Word2Vec (Mikolov et al., 2013) SIF-Glove (Arora et al., 2017) Problems Cosine similarity and Pearson correlation may give misleading results. The following evaluation scenario is common when testing for semantic similarity: given two inputs (words or sentences), embed each of them, (i) compute the cosine similarity of the pairs of vectors, and then (ii) calculate the Pearson (or Spearman) correlation with human judgments for the same pairs. Both steps are problematic: (i) it is unclear whether cosine similarity is better suited to measure semantic similarity than other similarity functi"
W19-4308,W18-6319,0,0.0149364,"as led to ever better reported performances across many different NLP tasks, sometimes exceeding the scores achieved by humans. These fanfares of victory are echoed by isolated voices raising concern about the trustworthiness of some of the reported results. For instance, Melis et al. (2017) find that neural language models have been misleadingly evaluated and that, under fair conditions, standard LSTMs outperform more recent innovations. Reimers and Gurevych (2017) find that reporting single performance scores is insufficient for comparing nondeterministic approaches such as neural networks. Post (2018) holds that neural MT systems are unfairly compared in the literature using different variants of the BLEU score metric. In an even more general context, Lipton and Steinhardt (2018) detect several current “troubling trends” in machine learning scholarship, some of which refer to evaluation. 55 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 55–60 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 3 in downstream tasks, as has been highlighted before (R¨uckl´e et al., 2018; Wieting and Kiela, 2019). Size matters. Currentl"
W19-4308,S17-2001,0,\N,Missing
W19-4727,W17-2207,0,0.041785,"ropes (‘love is magic’) over time and detecting change points of meaning, which we find to occur particularly within the German Romantic period. Additionally, through self-similarity, we reconstruct literary periods and find evidence that the law of linear semantic change also applies to poetry. 1 Introduction Following in the footsteps of traditional poetry analysis, Natural Language Understanding (NLU) research has largely explored stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015), (over time) (Voigt and Jurafsky, 2013), with a focus on sound devices (McCurdy et al., 2015; Hench, 2017) and broadly canonised form features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017) and noun+noun metaphor (Kesarwani et al., 2017). However, poetry also lends itself well to semantic change analysis, as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Poetic language is generally very dense, where concepts / ideas cannot be easily paraphrased. With a distributional se"
W19-4727,C16-1074,0,0.442529,"nd to occur particularly within the German Romantic period. Additionally, through self-similarity, we reconstruct literary periods and find evidence that the law of linear semantic change also applies to poetry. 1 Introduction Following in the footsteps of traditional poetry analysis, Natural Language Understanding (NLU) research has largely explored stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015), (over time) (Voigt and Jurafsky, 2013), with a focus on sound devices (McCurdy et al., 2015; Hench, 2017) and broadly canonised form features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017) and noun+noun metaphor (Kesarwani et al., 2017). However, poetry also lends itself well to semantic change analysis, as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Poetic language is generally very dense, where concepts / ideas cannot be easily paraphrased. With a distributional semantics model, Herbelot (2014) finds that the coherence of poetry significantly differs from Wiki"
W19-4727,P14-2134,0,0.144984,"word pairs such as ‘love (is) magic’ that gain association strength (cosine similarity) over time, finding that most are gaining traction in the Romantic period. Further, we track the self-similarity of words, both with a change point analysis and by evaluating ‘total selfsimilarity’ of words over time. The former helps us to reconstruct literary periods, while the latter provides us with further evidence for the law of linearity of semantic change (Eger and Mehler, 2016) using our new method. We do this with a model that learns diachronic word2vec embeddings jointly over all our time slots (Bamman et al., 2014), avoiding the need to compute the cosine similarity of two word vector representations on second order to align the embeddings. Our contributions are: we (1) provide a large corpus of German poetry which consists of about 75k poems, ranging from the 16th to early 20th century with more than 11 million tokens.1 We then track semantic change in this corpus with (2) two self-similarity experiments and finally (3) by investigating the rise of tropes (e.g. ‘love is magic’) over time. Due to its semantic succinctness and novelty of expression, poetry is a great test bed for semantic change analysis"
W19-4727,D17-1118,0,0.0588548,"Kemp (2015) work with simple distributional count vectors, while Hamilton et al. (2016) and Eger and Mehler (2016) use lowdimensional dense vector representations. Both works use different approaches to map independently induced word vectors (across time) in a common space: Hamilton et al. (2016) learn to align word vectors using a projection matrix while Eger and Mehler (2016) induce second-order embeddings by computing the similarity of words, in each time slot, to a reference vocabulary. Kutuzov et al. (2018) survey and compare models of semantic change based on diachronic word embeddings. Dubossarsky et al. (2017) caution against confounds in semantic change models. An interesting approach besides computing independent word embeddings in each time period has been outlined by Bamman et al. (2014) who jointly compute embeddings across different linguistic variables: each word w has an embedding w(t) = ew Wmain + ew Wt This dispenses the need to align independently trained embeddings for every time slot. Instead, a joint (MAIN) model is learned that is then reweighted for every time epoch. While this is convenient, it does not necessarily mean that embeddings of a certain low-frequency word in a given tim"
W19-4727,W17-2201,0,0.17254,"antic change also applies to poetry. 1 Introduction Following in the footsteps of traditional poetry analysis, Natural Language Understanding (NLU) research has largely explored stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015), (over time) (Voigt and Jurafsky, 2013), with a focus on sound devices (McCurdy et al., 2015; Hench, 2017) and broadly canonised form features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017) and noun+noun metaphor (Kesarwani et al., 2017). However, poetry also lends itself well to semantic change analysis, as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Poetic language is generally very dense, where concepts / ideas cannot be easily paraphrased. With a distributional semantics model, Herbelot (2014) finds that the coherence of poetry significantly differs from Wikipedia and 1 http://github.com/ thomasnikolaushaider 216 Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change, pages 216–222"
W19-4727,C18-1117,0,0.0166358,"ions of words vectors at time t − 1, which allows to forecast meaning change. Regarding methods, Xu and Kemp (2015) work with simple distributional count vectors, while Hamilton et al. (2016) and Eger and Mehler (2016) use lowdimensional dense vector representations. Both works use different approaches to map independently induced word vectors (across time) in a common space: Hamilton et al. (2016) learn to align word vectors using a projection matrix while Eger and Mehler (2016) induce second-order embeddings by computing the similarity of words, in each time slot, to a reference vocabulary. Kutuzov et al. (2018) survey and compare models of semantic change based on diachronic word embeddings. Dubossarsky et al. (2017) caution against confounds in semantic change models. An interesting approach besides computing independent word embeddings in each time period has been outlined by Bamman et al. (2014) who jointly compute embeddings across different linguistic variables: each word w has an embedding w(t) = ew Wmain + ew Wt This dispenses the need to align independently trained embeddings for every time slot. Instead, a joint (MAIN) model is learned that is then reweighted for every time epoch. While thi"
W19-4727,P16-2009,1,0.88045,"e periods and applying these for downstream learning how to date a poem. Following in this vein, we offer a method to explore poetic tropes, i.e. word pairs such as ‘love (is) magic’ that gain association strength (cosine similarity) over time, finding that most are gaining traction in the Romantic period. Further, we track the self-similarity of words, both with a change point analysis and by evaluating ‘total selfsimilarity’ of words over time. The former helps us to reconstruct literary periods, while the latter provides us with further evidence for the law of linearity of semantic change (Eger and Mehler, 2016) using our new method. We do this with a model that learns diachronic word2vec embeddings jointly over all our time slots (Bamman et al., 2014), avoiding the need to compute the cosine similarity of two word vector representations on second order to align the embeddings. Our contributions are: we (1) provide a large corpus of German poetry which consists of about 75k poems, ranging from the 16th to early 20th century with more than 11 million tokens.1 We then track semantic change in this corpus with (2) two self-similarity experiments and finally (3) by investigating the rise of tropes (e.g."
W19-4727,W16-0201,0,0.281256,"ithin the German Romantic period. Additionally, through self-similarity, we reconstruct literary periods and find evidence that the law of linear semantic change also applies to poetry. 1 Introduction Following in the footsteps of traditional poetry analysis, Natural Language Understanding (NLU) research has largely explored stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015), (over time) (Voigt and Jurafsky, 2013), with a focus on sound devices (McCurdy et al., 2015; Hench, 2017) and broadly canonised form features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017) and noun+noun metaphor (Kesarwani et al., 2017). However, poetry also lends itself well to semantic change analysis, as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Poetic language is generally very dense, where concepts / ideas cannot be easily paraphrased. With a distributional semantics model, Herbelot (2014) finds that the coherence of poetry significantly differs from Wikipedia and 1 http://githu"
W19-4727,D10-1051,0,0.356759,"meaning, which we find to occur particularly within the German Romantic period. Additionally, through self-similarity, we reconstruct literary periods and find evidence that the law of linear semantic change also applies to poetry. 1 Introduction Following in the footsteps of traditional poetry analysis, Natural Language Understanding (NLU) research has largely explored stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015), (over time) (Voigt and Jurafsky, 2013), with a focus on sound devices (McCurdy et al., 2015; Hench, 2017) and broadly canonised form features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017) and noun+noun metaphor (Kesarwani et al., 2017). However, poetry also lends itself well to semantic change analysis, as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Poetic language is generally very dense, where concepts / ideas cannot be easily paraphrased. With a distributional semantics model, Herbelot (2014) finds that the coherence of poetry signi"
W19-4727,P11-2014,0,0.541916,"Additionally, through self-similarity, we reconstruct literary periods and find evidence that the law of linear semantic change also applies to poetry. 1 Introduction Following in the footsteps of traditional poetry analysis, Natural Language Understanding (NLU) research has largely explored stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015), (over time) (Voigt and Jurafsky, 2013), with a focus on sound devices (McCurdy et al., 2015; Hench, 2017) and broadly canonised form features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017) and noun+noun metaphor (Kesarwani et al., 2017). However, poetry also lends itself well to semantic change analysis, as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Poetic language is generally very dense, where concepts / ideas cannot be easily paraphrased. With a distributional semantics model, Herbelot (2014) finds that the coherence of poetry significantly differs from Wikipedia and 1 http://github.com/ thomasnikolaushaider 216 Pr"
W19-4727,W18-4509,1,0.431364,"lf-similarity, we reconstruct literary periods and find evidence that the law of linear semantic change also applies to poetry. 1 Introduction Following in the footsteps of traditional poetry analysis, Natural Language Understanding (NLU) research has largely explored stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015), (over time) (Voigt and Jurafsky, 2013), with a focus on sound devices (McCurdy et al., 2015; Hench, 2017) and broadly canonised form features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017) and noun+noun metaphor (Kesarwani et al., 2017). However, poetry also lends itself well to semantic change analysis, as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Poetic language is generally very dense, where concepts / ideas cannot be easily paraphrased. With a distributional semantics model, Herbelot (2014) finds that the coherence of poetry significantly differs from Wikipedia and 1 http://github.com/ thomasnikolaushaider 216 Proceedings of the 1st Int"
W19-4727,W17-2204,0,0.151864,"nd find evidence that the law of linear semantic change also applies to poetry. 1 Introduction Following in the footsteps of traditional poetry analysis, Natural Language Understanding (NLU) research has largely explored stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015), (over time) (Voigt and Jurafsky, 2013), with a focus on sound devices (McCurdy et al., 2015; Hench, 2017) and broadly canonised form features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017) and noun+noun metaphor (Kesarwani et al., 2017). However, poetry also lends itself well to semantic change analysis, as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Poetic language is generally very dense, where concepts / ideas cannot be easily paraphrased. With a distributional semantics model, Herbelot (2014) finds that the coherence of poetry significantly differs from Wikipedia and 1 http://github.com/ thomasnikolaushaider 216 Proceedings of the 1st International Workshop on Computational Approac"
W19-4727,P16-1141,0,0.0213925,"l Language Change, pages 216–222 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 2 Related Work embeddings in that each linguistic variable C corresponds to a time epoch t: Semantic change has been explored in various works in recent years. One focus has been on studying laws of semantic change. Xu and Kemp (2015) explore two earlier proposed laws quantitatively: the law of differentiation (near-synonyms tend to differentiate over time) and the law of parallel change (related words have analogous meaning changes), finding that the latter applies more broadly. Hamilton et al. (2016) find that frequent words have a lower chance of undergoing semantic change and more polysemous words are more likely to change semantically. Eger and Mehler (2016) find that semantic change is linear in two senses: semantic self-similarity of words tends to decrease linearly in time and word vectors at time t can be written as linear combinations of words vectors at time t − 1, which allows to forecast meaning change. Regarding methods, Xu and Kemp (2015) work with simple distributional count vectors, while Hamilton et al. (2016) and Eger and Mehler (2016) use lowdimensional dense vector repr"
W19-4727,W13-1403,0,0.134644,"ury. We then track semantic change in this corpus by investigating the rise of tropes (‘love is magic’) over time and detecting change points of meaning, which we find to occur particularly within the German Romantic period. Additionally, through self-similarity, we reconstruct literary periods and find evidence that the law of linear semantic change also applies to poetry. 1 Introduction Following in the footsteps of traditional poetry analysis, Natural Language Understanding (NLU) research has largely explored stylistic variation (Kaplan and Blei, 2007; Kao and Jurafsky, 2015), (over time) (Voigt and Jurafsky, 2013), with a focus on sound devices (McCurdy et al., 2015; Hench, 2017) and broadly canonised form features such as meter (Greene et al., 2010; Agirrezabal et al., 2016; Estes and Hench, 2016) and rhyme (Reddy and Knight, 2011; Haider and Kuhn, 2018), as well as enjambement (Ruiz et al., 2017) and noun+noun metaphor (Kesarwani et al., 2017). However, poetry also lends itself well to semantic change analysis, as linguistic invention (Underwood and Sellers, 2012; Herbelot, 2014) and succinctness (Roberts, 2000) are at the core of poetic production. Poetic language is generally very dense, where conc"
