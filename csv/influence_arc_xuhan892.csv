2020.aacl-main.75,D18-1247,1,0.843969,"red from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 1"
2020.aacl-main.75,D18-1514,1,0.903034,"red from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 1"
2020.aacl-main.75,P17-1004,1,0.853193,"t al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting tha"
2020.aacl-main.75,P16-1200,1,0.830477,"ut these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompanied by the wrong labeling problem. The reason is that not all sentences mentioning the two entities express their relations in KGs exactly. For example, we may mistakenly label “Bill Gates retired from Microsoft” with the relati"
2020.aacl-main.75,D17-1189,0,0.0141779,"Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relation distributions suffer from the long-tail problem. mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016); Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks. Liu et al. (2017) incorporate a softlabel scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have also been adopted in DS. The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models, and there still remains some open problems worth exploring: (1) Existing DS methods focus on denoising auto-labeled instances and it is certainly meaningful to follow this research direction. Besides, current DS schem"
2020.aacl-main.75,P15-2047,0,0.0397231,"Missing"
2020.aacl-main.75,Q16-1017,0,0.0159895,"arios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Bara"
2020.aacl-main.75,D12-1048,0,0.0405797,"pple’s current CEO. Relation B Satya Nadella became the CEO of Microsoft in 2014. Figure 9: An example of clustering-based relation discovery, which identifying potential relation types by clustering unlabeled relational instances. relation types only by humans. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction"
2020.aacl-main.75,P16-1105,0,0.0181338,"much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zh"
2020.aacl-main.75,D12-1104,0,0.0209936,"section, we introduce the development of RE methods following the typical supervised setting, from early pattern-based methods, statistical approaches, to recent neural models. 2.1 Pattern Extraction Models The pioneering methods use sentence analysis tools to identify syntactic elements in text, then automatically construct pattern rules from these elements (Soderland et al., 1995; Kim and Moldovan, 1995; Huffman, 1995; Califf and Mooney, 1997). In order to extract patterns with better coverage and accuracy, later work involves larger corpora (Carlson et al., 2010), more formats of patterns (Nakashole et al., 2012; Jiang et al., 2017), and more efficient ways of extraction (Zheng et al., 2019). As automatically constructed patterns may have mistakes, most of the above methods require further examinations from human experts, which is the main limitation of pattern-based models. 2.2 Statistical Relation Extraction Models As compared to using pattern rules, statistical methods bring better coverage and require less human efforts. Thus statistical relation extraction (SRE) has been extensively studied. 746 2 Sometimes there is a special class in the relation set indicating that the sentence does not expres"
2020.aacl-main.75,D16-1261,0,0.0146614,"ocuments), the current RE models for this challenge are still crude and straightforward. Followings are some directions worth further investigation: (1) Extracting relations from complicated context is a challenging task requiring reading, memorizing and reasoning for discovering relational facts across multiple sentences. Most of current RE models are still very weak in these abilities. (2) Besides documents, more forms of context is also worth exploring, such as extracting relational facts across documents, or understanding relational information based on heterogeneous data. (3) Inspired by Narasimhan et al. (2016), which utilizes search engines for acquiring external information, automatically searching and analysing context for RE may help RE models identify relational facts with more coverage and become practical for daily scenarios. 3.4 Orienting More Open Domains Most RE systems work within pre-specified relation sets designed by human experts. However, our world undergoes open-ended growth of relations and it is not possible to handle all these emerging Jeﬀ Bezos, an American entrepreneur, graduated from Princeton in 1986. Jeﬀ Bezos graduated from Princeton Figure 8: An example of open information"
2020.aacl-main.75,N07-2032,0,0.130676,"Missing"
2020.aacl-main.75,W15-1506,0,0.0165476,"al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embed"
2020.aacl-main.75,P11-2048,0,0.0293116,"fact, there have been various works exploring feasible approaches that lead to better RE abilities on realworld scenarios. In this section, we summarize these exploratory efforts into four directions, and give our review and outlook about these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompan"
2020.aacl-main.75,C18-1326,0,0.0187317,"(2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normalizing extracted results will largely benefit the applications of Open IE. There are already some preliminary works in this area (Gal´arraga et al., 2014; 751 Vashishth et al., 2018) and more efforts are needed. (2) The not applicable (N/A) relation has been hardly addressed in relation discovery. In previous work, it is usually assumed that the s"
2020.aacl-main.75,N13-1095,0,0.0126813,"us works exploring feasible approaches that lead to better RE abilities on realworld scenarios. In this section, we summarize these exploratory efforts into four directions, and give our review and outlook about these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompanied by the wrong la"
2020.aacl-main.75,Q17-1008,0,0.0129385,"here are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field, some document-level RE datasets have been proposed. Quirk and Poon (2017); Peng et al. (2017) build datasets by DS. Li et al. (2016); Peng et al. (2017) propose datasets for specific domains. Yao et al. (2019) construct a general document-level RE dataset annotated by crowdsourcing workers, suitable for evaluating general-purpose document-level RE systems. Although there are some effo"
2020.aacl-main.75,P09-1113,0,0.736156,"ions (Section 3) targeting more complex RE scenarios. Those feasible approaches leading to better RE abilities still require further efforts, and here we summarize them into four directions: 745 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 745–758 c December 4 - 7, 2020. 2020 Association for Computational Linguistics (1) Utilizing More Data (Section 3.1). Supervised RE methods heavily rely on expensive human annotations, while distant supervision (Mintz et al., 2009) introduces more auto-labeled data to alleviate this issue. Yet distant methods bring noise examples and just utilize single sentences mentioning entity pairs, which significantly weaken extraction performance. Designing schemas to obtain highquality and high-coverage data to train robust RE models still remains a problem to be explored. (2) Performing More Efficient Learning (Section 3.2). Lots of long-tail relations only contain a handful of training examples. However, it is hard for conventional RE methods to well generalize relation patterns from limited examples like humans. Therefore, de"
2020.aacl-main.75,E17-1110,0,0.0240382,"s many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field,"
2020.aacl-main.75,D16-1252,0,0.0244806,"t in 2014. Figure 9: An example of clustering-based relation discovery, which identifying potential relation types by clustering unlabeled relational instances. relation types only by humans. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved res"
2020.aacl-main.75,D12-1042,0,0.13543,"accompanied by the wrong labeling problem. The reason is that not all sentences mentioning the two entities express their relations in KGs exactly. For example, we may mistakenly label “Bill Gates retired from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora"
2020.aacl-main.75,N13-1008,0,0.101883,"ther statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it still faces some challenges. Feature-based and kernelbased models require many efforts to design features or kernel functions. While graphical and embedding methods can predict relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend"
2020.aacl-main.75,C02-1151,0,0.535663,"s mentioned in this work are collected into the following paper list https://github. com/thunlp/NREPapers. † to researching relation extraction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world"
2020.aacl-main.75,W04-2401,0,0.331956,"Missing"
2020.aacl-main.75,P15-1061,0,0.0207109,"askar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE"
2020.aacl-main.75,R11-1004,0,0.0168865,"ext As shown in Figure 7, one document generally mentions many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory a"
2020.aacl-main.75,P10-1040,0,0.00513333,", recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word embeddings and position embeddings, there are also other works integrating syntactic information into NRE models. Xu et al. (2015a) and Xu et al. (2015b) adopt CNNs and RNNs over shortest dependency paths respectively. Liu et al. (2015) propose a recursive neural network based on augm"
2020.aacl-main.75,N16-1103,0,0.0654802,"tences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Dista"
2020.aacl-main.75,W16-1312,0,0.049412,"Missing"
2020.aacl-main.75,N18-1080,0,0.0617987,"ces, and proportions of N/A instances respectively. iPhone is designed by Apple Inc. iPhone is a iconic product of Apple. Dataset Tim Cook I looked up Apple Inc. on my iPhone. Figure 3: An example of distantly supervised relation extraction. With the fact (Apple Inc., product, iPhone), DS finds all sentences mentioning the two entities and annotates them with the relation product, which inevitably brings noise labels. 2016; Riedel et al., 2013). Recently, Transformers (Vaswani et al., 2017) and pre-trained language models (Devlin et al., 2019) have also been explored for NRE (Du et al., 2018; Verga et al., 2018; Wu and He, 2019; Baldini Soares et al., 2019) and have achieved new state-of-the-arts. By concisely reviewing the above techniques, we are able to track the development of RE from pattern and statistical methods to neural models. Comparing the performance of state-of-the-art RE models in years (Figure 2), we can see the vast increase since the emergence of NRE, which demonstrates the power of neural methods. 3 “More” Directions for RE Although the above-mentioned NRE models have achieved superior results on benchmarks, they are still far from solving the problem of RE. Most of these models u"
2020.aacl-main.75,N06-1039,0,0.0823614,"shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normal"
2020.aacl-main.75,D12-1110,0,0.0508116,"relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-b"
2020.aacl-main.75,N16-1065,0,0.147165,"-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which enc"
2020.aacl-main.75,P16-1123,1,0.90021,"Missing"
2020.aacl-main.75,D18-1246,0,0.0377077,"Missing"
2020.aacl-main.75,I08-2119,0,0.0392005,"thods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings"
2020.aacl-main.75,C18-1099,1,0.853339,"simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relatio"
2020.aacl-main.75,D11-1135,0,0.0169809,"n work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open I"
2020.aacl-main.75,P19-1074,1,0.902979,"dels may overfit simple textual cues between relations instead of really understanding the semantics of the context. More details about the experiments are in Appendix A. 3.3 Handling More Complicated Context As shown in Figure 7, one document generally mentions many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inf"
2020.aacl-main.75,D13-1136,0,0.0269434,"There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it still faces some challenges. Feature-based and kernelbased models require many efforts to design features or kernel functions. While graphical and embedding methods can predict relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this pa"
2020.aacl-main.75,P19-1277,0,0.0237425,"Missing"
2020.aacl-main.75,W06-1671,0,0.0514824,"Missing"
2020.aacl-main.75,D19-1021,1,0.854571,"rguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normalizing extracted results will largely bene"
2020.aacl-main.75,D17-1187,0,0.0159463,"der Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relation distributions suffer from the long-tail problem. mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016); Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks. Liu et al. (2017) incorporate a softlabel scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have also been adopted in DS. The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models, and there still remains some open problems worth exploring: (1) Existing DS methods focus on denoising auto-labeled instances and it is certainly meaningful to follow this research direction. Besides, current DS schemes are still similar to the original one in (Mintz et al., 2009), which just covers the case that the entity pairs are mentioned in the same sentences. To achieve better coverage and less noise, e"
2020.aacl-main.75,C16-1119,0,0.0178955,"tions for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word embeddings and position embeddings, there"
2020.aacl-main.75,D15-1062,0,0.0540036,"Missing"
2020.aacl-main.75,C16-1138,0,0.0498916,"Missing"
2020.aacl-main.75,D15-1206,1,0.888587,"Missing"
2020.aacl-main.75,C10-2160,0,0.0215937,"e features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it sti"
2020.aacl-main.75,D15-1203,0,0.0647772,"we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position e"
2020.aacl-main.75,C14-1220,0,0.226372,"rching relation extraction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world is much more complicated than this simple setting: (1) collecting high-quality human annotations is expensive and t"
2020.aacl-main.75,D17-1186,1,0.860499,". In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field, some document-level RE datasets have been proposed. Quirk and Poon (2017); Peng et al. (2017) build datasets by DS. Li et al. (2016); Peng et al. (2017) propose datasets for specific domains. Yao et al. (2019) c"
2020.aacl-main.75,N06-1037,0,0.077591,"ach is feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from te"
2020.aacl-main.75,P06-1104,0,0.0618459,"ach is feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from te"
2020.aacl-main.75,N19-1306,0,0.067956,"hem. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 30"
2020.aacl-main.75,Y15-1009,0,0.0785539,"raction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world is much more complicated than this simple setting: (1) collecting high-quality human annotations is expensive and time-consuming, (2) ma"
2020.aacl-main.75,D18-1244,0,0.013069,"since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embedd"
2020.aacl-main.75,D17-1004,0,0.217937,"abel “Bill Gates retired from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distri"
2020.aacl-main.75,P19-1139,1,0.91764,"hem. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 30"
2020.aacl-main.75,P05-1052,0,0.0462651,"Missing"
2020.aacl-main.75,P19-1137,1,0.923863,"setting, from early pattern-based methods, statistical approaches, to recent neural models. 2.1 Pattern Extraction Models The pioneering methods use sentence analysis tools to identify syntactic elements in text, then automatically construct pattern rules from these elements (Soderland et al., 1995; Kim and Moldovan, 1995; Huffman, 1995; Califf and Mooney, 1997). In order to extract patterns with better coverage and accuracy, later work involves larger corpora (Carlson et al., 2010), more formats of patterns (Nakashole et al., 2012; Jiang et al., 2017), and more efficient ways of extraction (Zheng et al., 2019). As automatically constructed patterns may have mistakes, most of the above methods require further examinations from human experts, which is the main limitation of pattern-based models. 2.2 Statistical Relation Extraction Models As compared to using pattern rules, statistical methods bring better coverage and require less human efforts. Thus statistical relation extraction (SRE) has been extensively studied. 746 2 Sometimes there is a special class in the relation set indicating that the sentence does not express any pre-specified relation (usually named as N/A). 2.3 Neural Relation Extracti"
2020.aacl-main.75,P05-1053,0,0.264276,"Missing"
2020.aacl-main.75,P16-2034,0,0.0170838,"6) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word em"
2020.aacl-main.75,P19-1128,1,0.923461,"ght great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al.,"
2020.aacl-main.75,N07-1015,0,\N,Missing
2020.aacl-main.75,D14-1067,0,\N,Missing
2020.aacl-main.75,C96-1079,0,\N,Missing
2020.aacl-main.75,P10-1160,0,\N,Missing
2020.aacl-main.75,P04-1054,0,\N,Missing
2020.aacl-main.75,P11-1055,0,\N,Missing
2020.aacl-main.75,H05-1091,0,\N,Missing
2020.aacl-main.75,D11-1142,0,\N,Missing
2020.aacl-main.75,P15-1034,0,\N,Missing
2020.aacl-main.75,P16-1072,0,\N,Missing
2020.aacl-main.75,P18-2014,0,\N,Missing
2020.aacl-main.75,D18-1245,0,\N,Missing
2020.aacl-main.75,P18-2065,0,\N,Missing
2020.aacl-main.75,N19-1184,0,\N,Missing
2020.aacl-main.75,D17-1191,0,\N,Missing
2020.aacl-main.75,N19-1423,0,\N,Missing
2020.aacl-main.75,D19-1395,0,\N,Missing
2020.aacl-main.75,D19-3029,1,\N,Missing
2020.aacl-main.75,D19-1649,1,\N,Missing
2020.acl-main.573,P15-1034,0,0.019032,"pidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012"
2020.acl-main.573,P19-1279,0,0.0403217,"old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern ext"
2020.acl-main.573,D11-1142,0,0.0492626,"fined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. F"
2020.acl-main.573,P07-1073,0,0.0684863,"ther experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two i"
2020.acl-main.573,P18-2065,0,0.0197777,"ver all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learnin"
2020.acl-main.573,N19-1423,0,0.0290448,"hus add special tokens into the tokenized tokens to indicate the beginning and ending positions of those entities. For simplicity, we denote such an example encoding operation as the following equation, x = f (x), (1) where x ∈ Rd is the semantic embedding of x, and d is the embedding dimension. Note that the encoder is not our focus in this paper, we select bidirectional long short-term memory (BiLSTM) (Bengio et al., 1994) as representative encoders to encode examples. In fact, other neural text encoders like convolutional neural networks (Zeng et al., 2014) and pre-trained language models (Devlin et al., 2019) can also be adopted as example encoders. 3.3 Learning for New Tasks When the k-th task is arising, the example encoder has not touched any examples of new relations before, and cannot extract the semantic features of them. Hence, we first fine-tune the example Tk )} to encoder on Tk = {(xT1 k , y1Tk ), . . . , (xTNk , yN grasp new relation patterns in Rk . The loss function of learning the k-th task is as follows, ˜ L(θ) = − Rk | N |X X i=1 j=1 log P δyTk =r × j i exp(g(f (xTi k ), rj )) ˜k| |R Tk l=1 exp(g(f (xi ), rl )) (2) , where rj is the embedding of the j-th relation ˜ k in the all kno"
2020.acl-main.573,P15-1026,0,0.0299963,"ation exercise to keep a stable understanding of old relations. The experimental results show that EMAR could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patt"
2020.acl-main.573,D15-1205,0,0.115988,"Missing"
2020.acl-main.573,D18-1247,1,0.811523,"old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation"
2020.acl-main.573,D18-1514,1,0.884,"old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation"
2020.acl-main.573,P11-1055,0,0.0562083,"eness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation le"
2020.acl-main.573,P16-1200,1,0.933526,"the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation types from large-scale textual corpora; (2"
2020.acl-main.573,W15-1506,0,0.039566,"MAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to de"
2020.acl-main.573,P06-1015,0,0.060577,"relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Li and Hoiem, 2017; Liu et al., 2018; Ritter et al., 2018) which consolidate the model parameters i"
2020.acl-main.573,P15-2047,0,0.0194688,"gnificantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets."
2020.acl-main.573,D15-1204,0,0.0130382,"t is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning,"
2020.acl-main.573,Q16-1017,0,0.0168298,"ntion to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research direct"
2020.acl-main.573,D12-1048,0,0.0436977,"text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual le"
2020.acl-main.573,P09-1113,0,0.293741,"ses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open"
2020.acl-main.573,P16-1105,0,0.0195004,"forms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before,"
2020.acl-main.573,N13-1008,0,0.0615208,"d relations and outperform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation type"
2020.acl-main.573,P15-1061,0,0.0223495,"catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations"
2020.acl-main.573,D11-1135,0,0.0368578,"ers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods"
2020.acl-main.573,N06-1039,0,0.105623,", in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatri"
2020.acl-main.573,D12-1110,0,0.0747449,"iments on several RE datasets, and the results show that EMAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attentio"
2020.acl-main.573,D15-1203,0,0.0411191,"that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have"
2020.acl-main.573,D16-1252,0,0.01481,"in models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first e"
2020.acl-main.573,C14-1220,0,0.768848,"rform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation types from large-scale"
2020.acl-main.573,D17-1004,0,0.0606583,"Missing"
2020.acl-main.573,N19-1086,0,0.330723,"set. Although continual relation learning is vital for learning emerging relations, there are rare explorations for this field. A straightforward solution is to store all historical data and re-train models every time new relations and examples come in. Nevertheless, it is computationally expensive since relations are in sustainable growth. Moreover, the huge example number of each relation makes frequently mixing new and old examples become infeasible in the real world. Therefore, storing all data is not practical in continual relation learning. In view of this, the recent preliminary work (Wang et al., 2019) indicates that the main challenge of continual relation learning is the catastrophic forgetting problem, i.e., it is hard to learn new relations and meanwhile avoid forgetting old relations, considering memorizing all the data is almost impossible. 6429 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6429–6440 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Open Relation Learning Continual Relation Learning David Bowie was born in 8th Jan. 1947. Learn Date of Birth Date of Birth Detect New Relations … Data for Date of Birth Hi"
2020.acl-main.573,P05-1053,0,0.180337,"n prototypes. We conduct sufficient experiments on several RE datasets, and the results show that EMAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations."
2020.acl-main.573,D19-1021,1,0.842706,"pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Li"
2020.acl-main.573,D15-1206,0,0.0411578,"ng problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defin"
2020.coling-main.140,N19-1423,0,0.15562,"alized parameters according to the performance on the support set, and selects informative support instances to 1596 contribute more to the adaptation gradients (with meta-parameters φa ), which can be viewed as accurate fine-tuning from concrete instances. Meta-Optimization. In meta-optimization phase, the meta-parameters Φ = {φe , φn , φa } are optimized based on the performance of the adapted model on the query set. The framework is shown in Algorithm 1. 3.1 Instance Encoder Given a sentence and the corresponding target entity pair (i.e., head entity and tail entity), we employ BERT model (Devlin et al., 2019) to encode the instance into contextualized representations, due to its effectiveness on a broad variety of NLP tasks. Specifically, sentences are first tokenized into word pieces (Wu et al., 2016). Inspired by Soares et al. (2019), to mark the positions of entities, we adopt four special tokens as entity markers, and insert them to the start and end of each entity. We select the representations of the start tokens of the head entity and tail entity on the top layer, and concatenate them to obtain the instance representation. The instance encoder can be formulated as follows: xj = g(xj , h, t;"
2020.coling-main.140,D19-1649,1,0.78596,"s to grasp new tasks with only a handful of training data. There are mainly two lines of approaches for few-shot learning: (1) Metric-Learning methods learn an embedding space that can well measure the similarities between instances. Koch et al. (2015; Vinyals et al. (2016) use vector distance functions to measure the similarities of examples, while Sung et al. (2018; Garcia and Estrach (2018) use neural networks to learn the metrics. Besides, Snell et al. (2017) propose to calculate prototypes of each few-shot class for classification. Specifically targeting few-shot relation classification, Gao et al. (2019a) introduce a hybrid attention mechanism to alleviate noise data problems. Ye and Ling (2019; Soares et al. (2019; Gao et al. (2019b; Sui et al. (2020) utilize local feature comparison to further improve few-shot performance. (2) Meta-Learning models, on the other hand, transfer the experience about how to “learn” a new class from the training set to the test domain. One way of meta-learning is to use recurrent networks to grasp the meta knowledge and predict the updated parameters in a black-box manner (Ravi and Larochelle, 2017; Munkhdalai and Yu, 2017; Mishra et al., 2018). Another directi"
2020.coling-main.140,D18-1514,1,0.742534,"is scaled to 1e-3 in L2 norm, and added to ci to obtain the perturbed representation. 4 Experiments In this section, we empirically evaluate MIML on few-shot relation classification. To evaluate the robustness of MIML, we conduct experiments in the presence of noisy instances. We also show the potential of MIML in zero-shot classification. Ablation study and visualization are conducted to better understand the inner mechanism of MIML. 4.1 Experiment Settings We first introduce the experiment settings, including datasets, evaluation protocol and baselines. Dataset. We evaluate MIML on FewRel (Han et al., 2018), a widely-used few-shot relation classification dataset. FewRel contains 70, 000 labeled sentences in 100 relations (i.e., each relation has 700 sentences). The relation annotations are first generated under distant supervision assumption (Mintz et al., 2009) by aligning Wikipedia and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and then labeled by human annotators. The training set contains 44, 800 sentences in 64 relations, the valid set has 11, 200 sentences in 16 relations, and the test set has the rest 14, 000 sentences in 20 relations. Evaluation Protocol. Following the same settings in"
2020.coling-main.140,P09-1113,0,0.132459,"of noisy instances. We also show the potential of MIML in zero-shot classification. Ablation study and visualization are conducted to better understand the inner mechanism of MIML. 4.1 Experiment Settings We first introduce the experiment settings, including datasets, evaluation protocol and baselines. Dataset. We evaluate MIML on FewRel (Han et al., 2018), a widely-used few-shot relation classification dataset. FewRel contains 70, 000 labeled sentences in 100 relations (i.e., each relation has 700 sentences). The relation annotations are first generated under distant supervision assumption (Mintz et al., 2009) by aligning Wikipedia and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and then labeled by human annotators. The training set contains 44, 800 sentences in 64 relations, the valid set has 11, 200 sentences in 16 relations, and the test set has the rest 14, 000 sentences in 20 relations. Evaluation Protocol. Following the same settings in Han et al. (2018), we consider four types of fewshot settings in evaluation, namely 5-way-1-shot, 5-way-5-shot, 10-way-1-shot and 10-way-5-shot. The N -way-K-shot setting indicates that each evaluation batch has N classes that do not appear in training set and"
2020.coling-main.140,D14-1162,0,0.0851287,"this way, MIML learns meta-parameters that can effectively customize initialization parameters for each class, and select informative support instances for fast adaptation, so as to produce good classification results on the query set. 3.5 Implementation Details All hyper-parameters are selected by grid-search on the development set. The class distribution p(C) is implemented by uniform distribution. We adopt Adam (Kingma and Ba, 2015) to optimize metaparameters. The meta learning rate β is 1 for meta-initializer and meta-querier, and 5e-5 for instance encoder. We employ 50 dimensional GloVe (Pennington et al., 2014) for word embeddings and BERTBASE (Devlin et al., 2019) implemented by Wolf et al. (2019) as the instance encoder. The hidden state dimensions ds and dw are 1, 536 and 50 respectively. The number of adaptation steps T is 150. In virtual adversarial training, we first randomly generate a perturbation vector δ1 for meta-information 1598 Encoder Model 5-way-1-shot 5-way-5-shot 10-way-1-shot 10-way-5-shot CNN Meta Network* GNN* SNAIL* Proto Network* MLMAN* 64.46 ± 0.54 66.23 ± 0.75 67.29 ± 0.26 74.52 ± 0.07 82.98 ± 0.20 80.57 ± 0.48 81.28 ± 0.62 79.40 ± 0.22 88.40 ± 0.06 92.66 ± 0.09 53.96 ± 0.56"
2020.coling-main.140,P19-1279,0,0.0830438,"g from concrete instances. Meta-Optimization. In meta-optimization phase, the meta-parameters Φ = {φe , φn , φa } are optimized based on the performance of the adapted model on the query set. The framework is shown in Algorithm 1. 3.1 Instance Encoder Given a sentence and the corresponding target entity pair (i.e., head entity and tail entity), we employ BERT model (Devlin et al., 2019) to encode the instance into contextualized representations, due to its effectiveness on a broad variety of NLP tasks. Specifically, sentences are first tokenized into word pieces (Wu et al., 2016). Inspired by Soares et al. (2019), to mark the positions of entities, we adopt four special tokens as entity markers, and insert them to the start and end of each entity. We select the representations of the start tokens of the head entity and tail entity on the top layer, and concatenate them to obtain the instance representation. The instance encoder can be formulated as follows: xj = g(xj , h, t; φe ), (1) where xj is the sentence, h and t are head and tail entities respectively. g(·) is the encoder, φe is the parameters of the encoder, and xj ∈ Rds is the instance representation. 3.2 Meta-Information Guided Fast Initializ"
2020.coling-main.140,P19-1277,0,0.0153968,"roaches for few-shot learning: (1) Metric-Learning methods learn an embedding space that can well measure the similarities between instances. Koch et al. (2015; Vinyals et al. (2016) use vector distance functions to measure the similarities of examples, while Sung et al. (2018; Garcia and Estrach (2018) use neural networks to learn the metrics. Besides, Snell et al. (2017) propose to calculate prototypes of each few-shot class for classification. Specifically targeting few-shot relation classification, Gao et al. (2019a) introduce a hybrid attention mechanism to alleviate noise data problems. Ye and Ling (2019; Soares et al. (2019; Gao et al. (2019b; Sui et al. (2020) utilize local feature comparison to further improve few-shot performance. (2) Meta-Learning models, on the other hand, transfer the experience about how to “learn” a new class from the training set to the test domain. One way of meta-learning is to use recurrent networks to grasp the meta knowledge and predict the updated parameters in a black-box manner (Ravi and Larochelle, 2017; Munkhdalai and Yu, 2017; Mishra et al., 2018). Another direction is to learn how to better initialize parameters for new classes (Finn et al., 2017; Finn e"
2020.coling-main.140,N19-1108,0,0.019921,"ion phase in 5-way and 10-way setting, and ask the model to classify query instances with class-aware initialization parameters. We compare MIML with strong zero-shot classification baselines. DeViSE (Frome et al., 2013) utilizes word embeddings of class names to classify 1600 Setting Random DeViSE SK4 MIML 5-way-0-shot 10-way-0-shot 20.00 10.00 55.90 ± 0.09 42.29 ± 0.08 79.68 ± 0.12 66.17 ± 0.11 79.54 ± 0.06 61.14 ± 0.10 Table 3: Experimental results of zero-shot classification on FewRel development set. instances from unseen classes, and we implement the DeViSE model with BERT encoder. SK4 (Zhang et al., 2019) incorporates rich semantic knowledge of classes, including word embeddings, class descriptions, class hierarchy, and commonsense knowledge graphs. We report the results in Table 3, from which we observe that: Compared to models tailored for zero-shot classification problem, MIML achieves reasonable performance. This is because that the class-aware fast initialization parameters in MIML are guided by meta-information, and thus can potentially be used to severe as classifiers without further adaptation using support instances. In summary, the results show that MIML can effectively integrate hig"
2020.emnlp-demos.29,D11-1029,0,0.035567,"Missing"
2020.emnlp-demos.29,W19-1402,0,0.0824936,"Missing"
2020.emnlp-demos.29,W19-1420,0,0.0164561,"the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propose a novel neural approach for automatic decipherment of Ugaritic and Linear B. Doostmohammadi and Nassajian (2019); Bernier-Colborne et al. (2019) explore to learn language models for Cuneiform Text. These previous efforts have inspired us to apply machine learning methods to the task of processing OBS. However, there are still three main challenges: 227 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 227–233 c November 16-20, 2020. 2020 Association for Computational Linguistics 1300BC 1046BC 771BC 475BC 222BC 220AD Figure 1: The historical evolution of the character “horse” from OBS to modern Chinese. Figure 2: An example of an OBS document used in divination. (1) Different from those ancie"
2020.emnlp-demos.29,W16-2103,0,0.0408051,"and to analyze and understand OBS is of great significance to historical research. Considering that it is often sophisticated and time-consuming to manually process ancient languages, some efforts have been devoted to utilizing machine learning techniques in this field. In order to detect and recognize ancient characters, Anderson and Levoy (2002); Rothacker et al. (2015); Mousavi and Lyashenko (2017); Rahma et al. (2017); Yamauchi et al. (2018) utilize computer vision techniques to visualize Cuneiform tablets and recognize Cuneiform characters, Franken and van Gemert (2013); Nederhof (2015); Iglesias-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propos"
2020.emnlp-demos.29,P19-1303,0,0.0109063,"s-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propose a novel neural approach for automatic decipherment of Ugaritic and Linear B. Doostmohammadi and Nassajian (2019); Bernier-Colborne et al. (2019) explore to learn language models for Cuneiform Text. These previous efforts have inspired us to apply machine learning methods to the task of processing OBS. However, there are still three main challenges: 227 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 227–233 c November 16-20, 2020. 2020 Association for Computational Linguistics 1300BC 1046BC 771BC 475BC 222BC 220AD Figure 1: The historical evolution of the character “h"
2020.emnlp-demos.29,D18-1063,0,0.0129114,"tablets and recognize Cuneiform characters, Franken and van Gemert (2013); Nederhof (2015); Iglesias-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propose a novel neural approach for automatic decipherment of Ugaritic and Linear B. Doostmohammadi and Nassajian (2019); Bernier-Colborne et al. (2019) explore to learn language models for Cuneiform Text. These previous efforts have inspired us to apply machine learning methods to the task of processing OBS. However, there are still three main challenges: 227 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 227–233 c November 16-20, 2020. 2020 Association for Computational Linguistic"
2020.emnlp-demos.29,D17-1266,0,0.0177155,". (2017); Yamauchi et al. (2018) utilize computer vision techniques to visualize Cuneiform tablets and recognize Cuneiform characters, Franken and van Gemert (2013); Nederhof (2015); Iglesias-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propose a novel neural approach for automatic decipherment of Ugaritic and Linear B. Doostmohammadi and Nassajian (2019); Bernier-Colborne et al. (2019) explore to learn language models for Cuneiform Text. These previous efforts have inspired us to apply machine learning methods to the task of processing OBS. However, there are still three main challenges: 227 Proceedings of the 2020 EMNLP (Systems Demonstrations),"
2020.emnlp-demos.29,P10-1107,0,0.0277237,"ime-consuming to manually process ancient languages, some efforts have been devoted to utilizing machine learning techniques in this field. In order to detect and recognize ancient characters, Anderson and Levoy (2002); Rothacker et al. (2015); Mousavi and Lyashenko (2017); Rahma et al. (2017); Yamauchi et al. (2018) utilize computer vision techniques to visualize Cuneiform tablets and recognize Cuneiform characters, Franken and van Gemert (2013); Nederhof (2015); Iglesias-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propose a novel neural approach for automatic decipherment of Ugaritic and Linear B. Doostmohammadi and Nassajian (2019); B"
2020.emnlp-demos.29,L18-1115,0,0.0281369,"ifice, agriculture, as well as births, illnesses, and deaths of royal members (Flad et al., 2008). Therefore, OBS documents constitute the earliest Chinese textual corpora, and to analyze and understand OBS is of great significance to historical research. Considering that it is often sophisticated and time-consuming to manually process ancient languages, some efforts have been devoted to utilizing machine learning techniques in this field. In order to detect and recognize ancient characters, Anderson and Levoy (2002); Rothacker et al. (2015); Mousavi and Lyashenko (2017); Rahma et al. (2017); Yamauchi et al. (2018) utilize computer vision techniques to visualize Cuneiform tablets and recognize Cuneiform characters, Franken and van Gemert (2013); Nederhof (2015); Iglesias-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) a"
2020.emnlp-main.129,buyko-etal-2010-genereg,0,0.0403765,"erent ways. The early MUC series datasets (Grishman and Sundheim, 1996) define event extraction as a slot-filling task. The TDT corpus (Allan, 2012) and some recent datasets (Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019) follow the open-domain paradigm, which does not require models to classify events into pre-defined event types for better coverage but limits the downstream application of the extracted events. Some datasets are developed for ED on specific domains, like the biomedical domain (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Buyko et al., 2010; N´edellec et al., 2013), literature (Sims et al., 2019), Twitter (Ritter et al., 2012; Guo et al., 2013) and breaking news (Pustejovsky et al., 2003). These datasets are also typically small-scale due to the inherent complexity of event annotation, but their different settings are complementary to our work. 7 Conclusion and Future work In this paper, we present a massive general domain event detection dataset (MAVEN), which significantly alleviates the data scarcity and low coverage problems of existing datasets. We conduct a thorough evaluation of the state-of-the-art ED models on MAVEN. Th"
2020.emnlp-main.129,P17-1038,0,0.0587272,"vember 16–20, 2020. 2020 Association for Computational Linguistics ern sophisticated models. Moreover, the covered event types in existing datasets are limited. The ACE 2005 English dataset only contains 8 event types and 33 specific subtypes. The Rich ERE ontology (Song et al., 2015) used by TAC KBP challenges (Ellis et al., 2015, 2016) covers 9 event types and 38 subtypes. The coverage of these datasets is low for general domain events, which results in the models trained on these datasets cannot be easily transferred and applied on general applications. Recent research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data"
2020.emnlp-main.129,P15-1017,0,0.690388,"uld recognize that the word “founded” is the trigger of a Found event. ED ∗ Elect: 183 问ure: 142 Transfer-Ownership: 127 Phone-Write: 123 Start-Position: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated inst"
2020.emnlp-main.129,D18-1158,0,0.305267,"osition: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod"
2020.emnlp-main.129,N18-1076,0,0.0271789,"mains a challenging task and requires further research efforts. We also discuss further directions for general domain ED with empirical analyses. The source code and dataset can be obtained from https:// github.com/THU-KEG/MAVEN-dataset. 1 End-Position: 212 Transfer-Money: 198 Attack: 1543 Figure 1: Data distribution of the most widely-used ACE 2005 English dataset. It contains 33 event types, 599 documents and 5, 349 instances in total. is the first stage to extract event knowledge from text (Ahn, 2006) and also fundamental to various NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018; Yang et al., 2019). Introduction Event detection (ED) is an important task of information extraction, which aims to identify event triggers (the words or phrases evoking events in text) and classify event types. For instance, in the sentence “Bill Gates founded Microsoft in 1975”, an ED model should recognize that the word “founded” is the trigger of a Found event. ED ∗ Elect: 183 问ure: 142 Transfer-Ownership: 127 Phone-Write: 123 Start-Position: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the r"
2020.emnlp-main.129,N19-1423,0,0.0330633,"eural network baseline, which adopts the widely-used bi-directional long shortterm memory network to learn textual representations, and then uses the hidden states at the positions of trigger candidates for classifying event types. (3) MOGANED (Yan et al., 2019) is an advanced graph neural network (GNN) model. It proposes a multi-order graph attention network to effectively model the multi-order syntactic relations in dependency trees and improve ED. (4) DMBERT (Wang et al., 2019b) is a vanilla BERTbased model. It takes advantage of the effective pretrained language representation model BERT (Devlin et al., 2019) and also adopts the dynamic multi-pooling mechanism to aggregate features for ED. We use the BERTBASE architecture in our experiments. (5) Different from the above tokenlevel classification models, BiLSTM+CRF and BERT+CRF are sequence labeling models. To verify the effectiveness of modeling multiple event correlations, the two models both adopt the conditional random field (CRF) (Lafferty et al., 2001) as their output layers, which can model structured output dependencies. And they use BiLSTM and BERTBASE as their feature extractors respectively. As we manually tune hyperparameters and some t"
2020.emnlp-main.129,D19-1033,1,0.851706,"Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod1652 Proceedings of"
2020.emnlp-main.129,D16-1264,0,0.124091,"Missing"
2020.emnlp-main.129,P19-1276,0,0.0175457,"traction models (Ji and Grishman, 2008; Li et al., 2013; Chen et al., 2015; Feng et al., 2016; Liu et al., 2017; Zhao et al., 2018; Yan et al., 2019) are developed on these datasets. Our MAVEN follows the effective framework and extends it to numerous general domain event types and data instances. There are also various datasets defining the ED task in different ways. The early MUC series datasets (Grishman and Sundheim, 1996) define event extraction as a slot-filling task. The TDT corpus (Allan, 2012) and some recent datasets (Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019) follow the open-domain paradigm, which does not require models to classify events into pre-defined event types for better coverage but limits the downstream application of the extracted events. Some datasets are developed for ED on specific domains, like the biomedical domain (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Buyko et al., 2010; N´edellec et al., 2013), literature (Sims et al., 2019), Twitter (Ritter et al., 2012; Guo et al., 2013) and breaking news (Pustejovsky et al., 2003). These datasets are also typically small-scale due to the inherent complexity of event a"
2020.emnlp-main.129,D18-1156,0,0.0338402,"Missing"
2020.emnlp-main.129,P19-1353,0,0.0176268,"of ED and event extraction models (Ji and Grishman, 2008; Li et al., 2013; Chen et al., 2015; Feng et al., 2016; Liu et al., 2017; Zhao et al., 2018; Yan et al., 2019) are developed on these datasets. Our MAVEN follows the effective framework and extends it to numerous general domain event types and data instances. There are also various datasets defining the ED task in different ways. The early MUC series datasets (Grishman and Sundheim, 1996) define event extraction as a slot-filling task. The TDT corpus (Allan, 2012) and some recent datasets (Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019) follow the open-domain paradigm, which does not require models to classify events into pre-defined event types for better coverage but limits the downstream application of the extracted events. Some datasets are developed for ED on specific domains, like the biomedical domain (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Buyko et al., 2010; N´edellec et al., 2013), literature (Sims et al., 2019), Twitter (Ritter et al., 2012; Guo et al., 2013) and breaking news (Pustejovsky et al., 2003). These datasets are also typically small-scale due to the inherent co"
2020.emnlp-main.129,P19-1429,0,0.374622,"Missing"
2020.emnlp-main.129,W15-0812,0,0.406914,"problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod1652 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1652–1671, c November 16–20, 2020. 2020 Association for Computational Linguistics ern sophisticated models. Moreover, the covered event types in existing datasets are limited. The ACE 2005 English dataset only contains 8 event types and 33 specific subtypes. The Rich ERE ontology (Song et al., 2015) used by TAC KBP challenges (Ellis et al., 2015, 2016) covers 9 event types and 38 subtypes. The coverage of these datasets is low for general domain events, which results in the models trained on these datasets cannot be easily transferred and applied on general applications. Recent research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, so"
2020.emnlp-main.129,L16-1699,0,0.0619031,"Missing"
2020.emnlp-main.129,P09-1113,0,0.125379,"2016) covers 9 event types and 38 subtypes. The coverage of these datasets is low for general domain events, which results in the models trained on these datasets cannot be easily transferred and applied on general applications. Recent research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data are inevitably noisy and homogeneous due to the limited number and low diversity of event facts and seed data instances. In this paper, we present MAVEN, a humanannotated massive general domain event detection dataset constructed from English Wikipedia and FrameNet (Baker et al., 1998), which can alleviate the data scarcity and low c"
2020.emnlp-main.129,P18-2066,0,0.180224,"-Write: 123 Start-Position: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stab"
2020.emnlp-main.129,N19-1105,1,0.947896,"research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data are inevitably noisy and homogeneous due to the limited number and low diversity of event facts and seed data instances. In this paper, we present MAVEN, a humanannotated massive general domain event detection dataset constructed from English Wikipedia and FrameNet (Baker et al., 1998), which can alleviate the data scarcity and low coverage problems: (1) Our MAVEN dataset contains 111, 611 different events, 118, 732 event mentions, which is twenty times larger than the most widely-used ACE 2005 dataset, and 4, 480 annotated documents in total. To the best of our"
2020.emnlp-main.129,D19-1584,1,0.922895,"research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data are inevitably noisy and homogeneous due to the limited number and low diversity of event facts and seed data instances. In this paper, we present MAVEN, a humanannotated massive general domain event detection dataset constructed from English Wikipedia and FrameNet (Baker et al., 1998), which can alleviate the data scarcity and low coverage problems: (1) Our MAVEN dataset contains 111, 611 different events, 118, 732 event mentions, which is twenty times larger than the most widely-used ACE 2005 dataset, and 4, 480 annotated documents in total. To the best of our"
2020.emnlp-main.129,D19-1582,0,0.510191,"-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod1652 Proceedings of the 2020 Conferenc"
2020.emnlp-main.129,C96-1079,0,\N,Missing
2020.emnlp-main.129,W06-0901,0,\N,Missing
2020.emnlp-main.129,P98-1013,0,\N,Missing
2020.emnlp-main.129,C98-1013,0,\N,Missing
2020.emnlp-main.129,P09-2093,0,\N,Missing
2020.emnlp-main.129,P06-4018,0,\N,Missing
2020.emnlp-main.129,P13-1024,0,\N,Missing
2020.emnlp-main.129,P08-1030,0,\N,Missing
2020.emnlp-main.129,P13-1008,0,\N,Missing
2020.emnlp-main.129,D14-1162,0,\N,Missing
2020.emnlp-main.129,P15-2060,0,\N,Missing
2020.emnlp-main.129,D15-1247,0,\N,Missing
2020.emnlp-main.129,doddington-etal-2004-automatic,0,\N,Missing
2020.emnlp-main.129,N16-1034,0,\N,Missing
2020.emnlp-main.129,P16-1025,0,\N,Missing
2020.emnlp-main.129,P17-1164,0,\N,Missing
2020.emnlp-main.129,N18-2058,0,\N,Missing
2020.emnlp-main.129,C18-1075,0,\N,Missing
2020.emnlp-main.129,D18-1259,0,\N,Missing
2020.emnlp-main.129,W13-2001,0,\N,Missing
2020.emnlp-main.129,P19-1521,0,\N,Missing
2020.emnlp-main.129,P16-2060,0,\N,Missing
2020.emnlp-main.129,P18-1201,0,\N,Missing
2020.emnlp-main.298,2020.acl-main.142,0,0.0155763,", 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not discuss this line of work here for their promotion comes from relational knowledge of external sources, while we focus on text itself in the paper. Analysis of RE Han et al. (2020) suggest to study how RE models learn from context and mentions. Alt et al. (2020) also point out that there may exist shallow cues in entity mentions. However, there have not been systematical analyses about the topic and to the best of our knowledge, we are the first one to thoroughly carry out these studies. 6 Conclusion In this paper, we thoroughly study how textual context and entity mentions affect RE models respectively. Experiments and case studies prove that (i) both context and entity mentions (mainly as type information) provide critical information for relation extraction, and (ii) existing RE datasets may leak superficial cues through entity mentions and models"
2020.emnlp-main.298,P19-1279,0,0.356781,"n Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mention"
2020.emnlp-main.298,D14-1067,0,0.0277137,"ion provided by textual context and entity mentions in a typical RE scenario. From mentions, we can acquire type information and link entities to KGs, and access further knowledge about them. The IDs in the figure are from Wikidata. Introduction Relation extraction (RE) aims at extracting relational facts between entities from text, e.g., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder whic"
2020.emnlp-main.298,H05-1091,0,0.188054,"harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of s"
2020.emnlp-main.298,W97-1002,0,0.699414,"of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mentions (names). From human intuition, textual context should be the main source of information for RE. Researchers have reached a consensus that there exist interpretable patterns in textual context that express relational facts. For example, in Figure 1, “... be founded ... by ...” is a pattern for the relation founded by. The early RE systems (Huffman, 1995; Califf and Mooney, 1997) formalize patterns into string templates and determine relations by 3661 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3661–3672, c November 16–20, 2020. 2020 Association for Computational Linguistics matching these templates. The later neural models (Socher et al., 2012; Liu et al., 2013) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity"
2020.emnlp-main.298,P04-1054,0,0.103347,"nd few-shot settings, it is harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relat"
2020.emnlp-main.298,N19-1423,0,0.0521177,"se to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not discuss this line of work here for their promotion comes from relational knowledge of external sources, while we focus on text itself in the paper. Analysis of RE Ha"
2020.emnlp-main.298,D19-1649,1,0.865961,"ernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not disc"
2020.emnlp-main.298,D19-3029,1,0.896375,"Missing"
2020.emnlp-main.298,D18-1514,1,0.881235,"u et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 20"
2020.emnlp-main.298,W09-2415,0,0.134815,"Missing"
2020.emnlp-main.298,P16-1200,1,0.903989,"sed methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in t"
2020.emnlp-main.298,P18-1136,0,0.0177318,"tity mentions in a typical RE scenario. From mentions, we can acquire type information and link entities to KGs, and access further knowledge about them. The IDs in the figure are from Wikidata. Introduction Relation extraction (RE) aims at extracting relational facts between entities from text, e.g., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actu"
2020.emnlp-main.298,N13-1095,0,0.0136486,"lopment of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text"
2020.emnlp-main.298,D15-1203,0,0.0908899,"through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject ent"
2020.emnlp-main.298,P09-1113,0,0.906212,"d to MTB (in the dotted box), our method samples data with better diversity, which can not only increase the coverage of entity types and diverse context but also reduce the possibility of memorizing entity names. we adopt the idea of contrastive learning (Hadsell et al., 2006), which aims to learn representations by pulling “neighbors” together and pushing “nonneighbors” apart. After this, “neighbor” instances will have similar representations. So it is important to define “neighbors” in contrastive learning and we utilize the information from KGs to to that. Inspired by distant supervision (Mintz et al., 2009), we assume that sentences with entity pairs sharing the same relation in KGs are “neighbors”. Formally, denote the KG we use as K, which is composed of relational facts. Denote two random sentences as XA and XB , which have entity mentions hA , tA and hB , tB respectively. We define XA and XB as “neighbors” if there is a relation r such that (hA , r, tA ) ∈ K and (hB , r, tB ) ∈ K. We take Wikidata as the KG since it can be easily linked to the Wikipedia corpus used for pretraining. When training, we first sample a relation r with respect to its proportion in the KG, and then sample a sentenc"
2020.emnlp-main.298,C14-1220,0,0.793202,"xt+Mention (C+M) This is the most widely-used RE setting, where the whole sentence 3662 Model C+M C+T OnlyC OnlyM OnlyT C+M CNN BERT MTB 0.547 0.683 0.691 0.591 0.686 0.696 0.441 0.570 0.581 0.434 0.466 0.433 0.295 0.277 0.304 Although her family was from Arkansas, she was born in Washington state, where ... Label: per:state of birth Prediction: per:state of residence Table 1: TACRED results (micro F1 ) with CNN, BERT and MTB on different settings. (with both context and highlighted entity mentions) is provided. To let the models know where the entity mentions are, we use position embeddings (Zeng et al., 2014) for the CNN model and special entity markers (Zhang et al., 2019; Baldini Soares et al., 2019) for the pre-trained BERT. Context+Type (C+T) We replace entity mentions with their types provided in TACRED. We use special tokens to represent them: for example, we use [person] and [date] to represent an entity with type person and date respectively. Different from Zhang et al. (2017), we do not repeat the special tokens for entity-length times to avoid leaking entity length information. Besides the above settings, we also adopt three synthetic settings to study how much information context or men"
2020.emnlp-main.298,W15-1506,0,0.0213763,"better pre-training technique is a reliable direction towards better RE. 2 Pilot Experiment and Analysis To study which type of information affects existing neural RE models to make decisions, we first introduce some preliminaries of RE models and settings and then conduct pilot experiments as well as empirical analyses in this section. 2.1 Models and Dataset There are various NRE models proposed in previous work (refer to Section 5), and we select the following three representative neural models for our pilot experiments and analyses: CNN We use the convolutional neural networks described in Nguyen and Grishman (2015) and augment the inputs with part-of-speech, named entity recognition and position embeddings following Zhang et al. (2017). BERT BERT is a pre-trained language model that has been widely used in NLP tasks. We use BERT for RE following Baldini Soares et al. (2019). In short, we highlight entity mentions in sentences by special markers and use the concatenations of entity representations for classification. Matching the blanks (MTB) MTB (Baldini Soares et al., 2019) is an RE-oriented pre-trained model based on BERT. It is pre-trained by classifying whether two sentences mention the same entity"
2020.emnlp-main.298,D19-1005,0,0.0995482,"3) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity mentions also provide much information for relation classification. As shown in Figure 1, we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE (Zhang et al., 2019; Peters et al., 2019). Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training (Petroni et al., 2019). In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that: (1) Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type information of entities. (2"
2020.emnlp-main.298,D19-1250,0,0.0644701,"Missing"
2020.emnlp-main.298,D17-1004,0,0.334618,"formation affects existing neural RE models to make decisions, we first introduce some preliminaries of RE models and settings and then conduct pilot experiments as well as empirical analyses in this section. 2.1 Models and Dataset There are various NRE models proposed in previous work (refer to Section 5), and we select the following three representative neural models for our pilot experiments and analyses: CNN We use the convolutional neural networks described in Nguyen and Grishman (2015) and augment the inputs with part-of-speech, named entity recognition and position embeddings following Zhang et al. (2017). BERT BERT is a pre-trained language model that has been widely used in NLP tasks. We use BERT for RE following Baldini Soares et al. (2019). In short, we highlight entity mentions in sentences by special markers and use the concatenations of entity representations for classification. Matching the blanks (MTB) MTB (Baldini Soares et al., 2019) is an RE-oriented pre-trained model based on BERT. It is pre-trained by classifying whether two sentences mention the same entity pair with entity mentions randomly masked. It is fine-tuned for RE in the same way as BERT. Since it is not publicly releas"
2020.emnlp-main.298,P19-1139,1,0.940049,"012; Liu et al., 2013) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity mentions also provide much information for relation classification. As shown in Figure 1, we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE (Zhang et al., 2019; Peters et al., 2019). Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training (Petroni et al., 2019). In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that: (1) Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type infor"
2020.emnlp-main.298,P05-1053,0,0.416681,"for OnlyC and OnlyM. In the low resource and few-shot settings, it is harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 201"
2020.emnlp-main.298,C02-1151,0,0.404876,"nal patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the r"
2020.emnlp-main.298,W04-2401,0,0.450234,"Missing"
2020.emnlp-main.298,D12-1110,0,0.488425,"., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classi"
2020.emnlp-main.300,D19-1498,0,0.0662053,"g tasks. Experimental results on the large-scale DocRE benchmark show that our model can capture useful information from noisy DS data and achieve promising results. The source code of this paper can be found in https://github.com/thunlp/DSDocRE. 1 Now James serves for the Lakers ... [8] James locate_in Lebron James member_of educated_at Vincent–St. Mary High School in his hometown. St. Vincent–St. Mary High School Lakers Figure 1: An example of DocRE. Given a document, DocRE models should capture the relational semantics across sentences to extract multiple relational facts. tence relations (Christopoulou et al., 2019). Fig. 1 gives a brief illustration of DocRE. Relation extraction (RE) aims to identify relational facts between entities from texts. Recently, neural relation extraction (NRE) models have been verified in sentence-level RE (Zeng et al., 2014). Distant supervision (DS) (Mintz et al., 2009) provides large-scale distantly-supervised data that multiplies instances and enables sufficient model training. Sentence-level RE focuses on extracting intrasentence relations between entities in a sentence. However, it is extremely restricted with generality and coverage in practice, since there are plenty"
2020.emnlp-main.300,N19-1423,0,0.174441,"entity pairs within three consecutive sentences. Different from these works, we bring in document-level DS to DocRE and conduct pre-training to denoise these DS data. 3 Methodology In this section, we present our proposed model in detail. Fig. 2 gives an illustration of our framework. We first apply the pre-denoising module to screen out some NA instances from all documents. Then we pre-train the document encoder with three pre-training tasks on the document-level distantly supervised dataset. Finally, we fine-tune the model on the human-annotated dataset. 3.1 Document Encoder We adopt BERT (Devlin et al., 2019) as the document encoder to encode documents into representations of entity mentions, entities and relational instances. Let D = {ωi }ni=1 denote the input docu|V | ment which consists of n tokens, and V = {ei }i=1 be the set of entities mentioned in the document, i where entity ei = {mji }lj=1 contains li mentions in the document. Following Soares et al. (2019), we use entity markers [Ei] and [/Ei] for each entity ei . The start marker [Ei] is inserted at the begin of all mentions of entity ei , and the end marker [/Ei] is inserted at the end. 3684 We use BERT to encode the token sequence wit"
2020.emnlp-main.300,P16-1200,1,0.799391,"f DS data and verify the effectiveness of our pre-trained model for DocRE. To the best of our knowledge, we are the first to denoise document-level DS with pre-trained models. We will release our codes in the future. 2 Related Work Sentence-level RE. Conventional NRE models focus on sentence-level supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretrained model for sentence-level RE. Document-level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact Alignment Relational Instance Mention/Entity Document Encoder Sample data for pre-training Bilinear Ment"
2020.emnlp-main.300,N13-1095,0,0.0321775,"nduct detailed analysis and ablation test, which further highlight the significance of DS data and verify the effectiveness of our pre-trained model for DocRE. To the best of our knowledge, we are the first to denoise document-level DS with pre-trained models. We will release our codes in the future. 2 Related Work Sentence-level RE. Conventional NRE models focus on sentence-level supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretrained model for sentence-level RE. Document-level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact Alignment Relational"
2020.emnlp-main.300,P09-1113,0,0.825376,"aselines. We also conduct detailed analysis and ablation test, which further highlight the significance of DS data and verify the effectiveness of our pre-trained model for DocRE. To the best of our knowledge, we are the first to denoise document-level DS with pre-trained models. We will release our codes in the future. 2 Related Work Sentence-level RE. Conventional NRE models focus on sentence-level supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretrained model for sentence-level RE. Document-level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact A"
2020.emnlp-main.300,Q17-1008,0,0.0213299,"eep Transformer (BERT) Pre-denoise doc B Entity Pooling Document Encoder doc A Score Relation … [CLS] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] [SEP] Figure 2: The framework of our proposed model. to extend the scope of knowledge acquisition to the document level, which has attracted great attention recently (Yao et al., 2019). Some works use linguistic features (Xu et al., 2016; Gu et al., 2017) and graph-based models (Christopoulou et al., 2019; Sahu et al., 2019) to extract inter-sentence relations on human-annotated data. Quirk and Poon (2017) and Peng et al. (2017) attempt to extract inter-sentence relations with distantly supervised data. However, they only use entity pairs within three consecutive sentences. Different from these works, we bring in document-level DS to DocRE and conduct pre-training to denoise these DS data. 3 Methodology In this section, we present our proposed model in detail. Fig. 2 gives an illustration of our framework. We first apply the pre-denoising module to screen out some NA instances from all documents. Then we pre-train the document encoder with three pre-training tasks on the document-level distantly supervised dataset. F"
2020.emnlp-main.300,P18-1046,0,0.14999,"RE. To the best of our knowledge, we are the first to denoise document-level DS with pre-trained models. We will release our codes in the future. 2 Related Work Sentence-level RE. Conventional NRE models focus on sentence-level supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretrained model for sentence-level RE. Document-level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact Alignment Relational Instance Mention/Entity Document Encoder Sample data for pre-training Bilinear Mention Deep Transformer (BERT) Pre-denoise doc B Entity Pooling Document En"
2020.emnlp-main.300,P19-1423,0,0.0180819,"gnment Relational Instance Mention/Entity Document Encoder Sample data for pre-training Bilinear Mention Deep Transformer (BERT) Pre-denoise doc B Entity Pooling Document Encoder doc A Score Relation … [CLS] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] [SEP] Figure 2: The framework of our proposed model. to extend the scope of knowledge acquisition to the document level, which has attracted great attention recently (Yao et al., 2019). Some works use linguistic features (Xu et al., 2016; Gu et al., 2017) and graph-based models (Christopoulou et al., 2019; Sahu et al., 2019) to extract inter-sentence relations on human-annotated data. Quirk and Poon (2017) and Peng et al. (2017) attempt to extract inter-sentence relations with distantly supervised data. However, they only use entity pairs within three consecutive sentences. Different from these works, we bring in document-level DS to DocRE and conduct pre-training to denoise these DS data. 3 Methodology In this section, we present our proposed model in detail. Fig. 2 gives an illustration of our framework. We first apply the pre-denoising module to screen out some NA instances from all documents. Then we pre-trai"
2020.emnlp-main.300,P19-1279,0,0.246383,"evel supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretrained model for sentence-level RE. Document-level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact Alignment Relational Instance Mention/Entity Document Encoder Sample data for pre-training Bilinear Mention Deep Transformer (BERT) Pre-denoise doc B Entity Pooling Document Encoder doc A Score Relation … [CLS] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] [SEP] Figure 2: The framework of our proposed model. to extend the scope of knowledge acquisition to the do"
2020.emnlp-main.300,D17-1188,0,0.0179509,"e-tuning. We keep 2Nent entity pairs after pre-denoising for each document during fine-tuning, where Nent is the number of entities mentioned in the document. And we keep 20 entity pairs for each document during pre-training. We train our model with GeForce RTX 2080 Ti. All the special tokens including entity markers and the special blank symbol are implemented with unused tokens in the BERTBASE vocabulary. 4.2 4.4 Baseline We compare our model with the following baselines. (1) CNN/LSTM/BiLSTM (Yao et al., 2019): these models capture relational semantics via various encoder. (2) ContextAware (Sorokin and Gurevych, 2017): it considers the relations’ interactions with attention to jointly learn all entity 4.3 Implementation Details Main Result The main results are shown in Tab. 1. Specifically, D refers to the pre-denoising module and P indicates pre-training tasks. From the results, we can observe that: (1) Our model outperforms all baselines by a significant margin. It is due to the effectiveness of the pre-denoising mechanism 3686 F1 Model Dev IgnF1 F1 58.65 57.00 58.43 56.68 w/o MM w/o RD w/o RA 58.39 57.19 58.48 56.76 55.61 56.73 57.60 56.71 58.13 55.81 54.94 56.30 w/o Inter w/o Intra 58.68 57.78 56.96 56"
2020.emnlp-main.300,P19-1074,1,0.914989,"level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact Alignment Relational Instance Mention/Entity Document Encoder Sample data for pre-training Bilinear Mention Deep Transformer (BERT) Pre-denoise doc B Entity Pooling Document Encoder doc A Score Relation … [CLS] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] [SEP] Figure 2: The framework of our proposed model. to extend the scope of knowledge acquisition to the document level, which has attracted great attention recently (Yao et al., 2019). Some works use linguistic features (Xu et al., 2016; Gu et al., 2017) and graph-based models (Christopoulou et al., 2019; Sahu et al., 2019) to extract inter-sentence relations on human-annotated data. Quirk and Poon (2017) and Peng et al. (2017) attempt to extract inter-sentence relations with distantly supervised data. However, they only use entity pairs within three consecutive sentences. Different from these works, we bring in document-level DS to DocRE and conduct pre-training to denoise these DS data. 3 Methodology In this section, we present our proposed model in detail. Fig. 2 gives"
2020.emnlp-main.300,C14-1220,0,0.0453237,"sions and denoising irrelevant information from the document. In experiments, we evaluate our model on an open DocRE benchmark and achieve significant improvement over competitive baselines. We also conduct detailed analysis and ablation test, which further highlight the significance of DS data and verify the effectiveness of our pre-trained model for DocRE. To the best of our knowledge, we are the first to denoise document-level DS with pre-trained models. We will release our codes in the future. 2 Related Work Sentence-level RE. Conventional NRE models focus on sentence-level supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretra"
2020.emnlp-main.459,D19-1522,0,0.0916921,"cepts related to singer in Wikidata, then use the entities corresponding to these concepts to build the entity list. After that, we expand the entity list appropriately, and finally use the triples containing entities in the entity list to form the final dataset. The statistics of our five datasets are listed in Table 2. 4.2 Experiment Setup Baseline Models In our experiments, we select some KGE models and multi-hop reasoning models for comparison. For embedding-based models, we compared with TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), ConvE (Dettmers et al., 2018) and TuckER (Balazevic et al., 2019). For multi-hop reasoning, we evaluate the following five models 1 , Neural Logical Programming (NeuralLP) (Yang et al., 2017), Neural Theorem Prover (NTP) (Rockt¨aschel and Riedel, 2017), MINERVA (Das et al., 2018), MultiHopKG (Lin et al., 2018) and CPL 2 (Fu et al., 2019) . Besides, our model has three variations, DacKGR (sample), DacKGR (top) and DacKGR (avg), which use sample, top-one and average strategy (introduced in Section 3.3) respectively. Evaluation Protocol For every triple (es , rq , eo ) in the test set, we convert it to a triple query (es , rq , ?), and then use embedding-based"
2020.emnlp-main.459,N18-1165,0,0.0178678,"y of their predictions. 5.2 Multi-Hop Reasoning Different from embedding-based models, multihop reasoning for KGs aims to predict the tail entity for every triple query (es , rq , ?) and meanwhile provide a reasoning path to support the prediction. Before multi-hop reasoning task is formalized, there are some models on relation path reasoning task, which aims to predict the relation between entities like (es , ?, eo ) using path information. DeepPath (Xiong et al., 2017) first adopts reinforcement learning (RL) framework for relation path reasoning, which inspires much later work (e.g., DIVA (Chen et al., 2018) and AttnPath (Wang et al., 2019)). MINERVA (Das et al., 2018) is the first model that uses REINFORCE algorithm to do the multihop reasoning task. To make the training process of RL models stable, Shen et al. propose M-Walk to solve the reward sparsity problem using off-policy learning. MultiHopKG (Lin et al., 2018) further improves MINERVA using action dropout and reward shaping. Lv et al. (2019) propose MetaKGR to address the new task that multi-hop reasoning on few-shot relations. In order to adapt RL models to a dynamically growing KG, Fu et al. (2019) propose CPL to do multi-hop reasoning"
2020.emnlp-main.459,D19-1269,0,0.0620329,"gh paths between them as reasoning evidence, which makes it difficult for the agent to carry out the reasoning process. As shown in the lower part of Figure 1, there is no evidential path between Mark Twain and English since the relation publish area is missing. From Table 1 we can learn that some sampled KG datasets are actually sparse. Besides, some domain-specific KGs (e.g., WD-singer) do not have abundant knowledge and also face the problem of sparsity. As the performance of most existing multi-hop reasoning methods drops significantly on sparse KGs, some preliminary efforts, such as CPL (Fu et al., 2019), explore to introduce additional text information to ease the sparsity of KGs. Although these explorations have achieved promising results, they are still limited to those specific KGs whose entities have additional text information. Thus, reasoning over sparse KGs is still an important but not fully resolved problem, and requires a more generalized approach to this problem. In this paper, we propose a multi-hop reasoning model named DacKGR, along with two dynamic strategies to solve the two problems mentioned above: Dynamic Anticipation makes use of the limited information in a sparse KG to"
2020.emnlp-main.459,D15-1038,0,0.0305378,"glish Figure 1: An illustration of multi-hop reasoning task over sparse KG. The missing relations (black dashed arrows) between entities can be inferred from existing triples (solid black arrows) through reasoning paths (bold arrows). However, some relations in the reasoning path are missing (red dashed arrows) in sparse KG, which makes multi-hop reasoning difficult. their further development and adaption for related downstream tasks. Knowledge graphs (KGs) represent the world knowledge in a structured way, and have been proven to be helpful for many downstream NLP tasks like query answering (Guu et al., 2015), dialogue generation (He et al., 2017) and machine reading comprehension (Yang et al., 2019). Despite their wide applications, many KGs still face serious incompleteness (Bordes et al., 2013), which limits Corresponding Author isa child Mark Twain Roughing It American child? spouse Introduction ∗ isa Olivia Langdon To alleviate this issue, some embedding-based models (Bordes et al., 2013; Dettmers et al., 2018) are proposed, most of which embed entities and relations into a vector space and make link predictions to complete KGs. These models focus on efficiently predicting knowledge but lack"
2020.emnlp-main.459,P17-1162,0,0.0279627,"-hop reasoning task over sparse KG. The missing relations (black dashed arrows) between entities can be inferred from existing triples (solid black arrows) through reasoning paths (bold arrows). However, some relations in the reasoning path are missing (red dashed arrows) in sparse KG, which makes multi-hop reasoning difficult. their further development and adaption for related downstream tasks. Knowledge graphs (KGs) represent the world knowledge in a structured way, and have been proven to be helpful for many downstream NLP tasks like query answering (Guu et al., 2015), dialogue generation (He et al., 2017) and machine reading comprehension (Yang et al., 2019). Despite their wide applications, many KGs still face serious incompleteness (Bordes et al., 2013), which limits Corresponding Author isa child Mark Twain Roughing It American child? spouse Introduction ∗ isa Olivia Langdon To alleviate this issue, some embedding-based models (Bordes et al., 2013; Dettmers et al., 2018) are proposed, most of which embed entities and relations into a vector space and make link predictions to complete KGs. These models focus on efficiently predicting knowledge but lack necessary interpretability. In order to"
2020.emnlp-main.459,D18-1362,0,0.336179,"al., 2019). Despite their wide applications, many KGs still face serious incompleteness (Bordes et al., 2013), which limits Corresponding Author isa child Mark Twain Roughing It American child? spouse Introduction ∗ isa Olivia Langdon To alleviate this issue, some embedding-based models (Bordes et al., 2013; Dettmers et al., 2018) are proposed, most of which embed entities and relations into a vector space and make link predictions to complete KGs. These models focus on efficiently predicting knowledge but lack necessary interpretability. In order to solve this problem, Das et al. (2018) and Lin et al. (2018) propose multihop reasoning models, which use the REINFORCE algorithm (Williams, 1992) to train an agent to search over KGs. These models can not only give the predicted result but also an interpretable path to indicate the reasoning process. As shown in the upper part of Figure 1, for a triple query (Olivia Langdon, child, ?), multi-hop reasoning models can predict the tail entity Susy Clemens through a reasoning path (bold arrows). Although existing multi-hop reasoning models have achieved good results, they still suffer two problems on sparse KGs: (1) Insufficient information. Compared with"
2020.emnlp-main.459,D19-1334,1,0.818399,"like (es , ?, eo ) using path information. DeepPath (Xiong et al., 2017) first adopts reinforcement learning (RL) framework for relation path reasoning, which inspires much later work (e.g., DIVA (Chen et al., 2018) and AttnPath (Wang et al., 2019)). MINERVA (Das et al., 2018) is the first model that uses REINFORCE algorithm to do the multihop reasoning task. To make the training process of RL models stable, Shen et al. propose M-Walk to solve the reward sparsity problem using off-policy learning. MultiHopKG (Lin et al., 2018) further improves MINERVA using action dropout and reward shaping. Lv et al. (2019) propose MetaKGR to address the new task that multi-hop reasoning on few-shot relations. In order to adapt RL models to a dynamically growing KG, Fu et al. (2019) propose CPL to do multi-hop reasoning and fact extraction jointly. In addition to the above RL-based reasoning models, there are some other neural symbolic models for multi-hop reasoning. NTP (Rockt¨aschel and Riedel, 2017) and NeuralLP (Yang et al., 2017) are two end-to-end reasoning models that can learn logic rules from KGs automatically. Compared with KGE models, multi-hop reasoning models sacrifice some accuracy for interpretabi"
2020.emnlp-main.459,N18-2053,0,0.0223203,"ories (Wang et al., 2017): (1) Translation-based models (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Sun et al., 2018) formalize the relation as a translation from a head entity to a tail entity, and often use distance-based score functions derived from these translation operations. (2) Tensor-factorization based models (Nickel et al., 2011; Yang et al., 2015; Balazevic et al., 2019) formulate KGE as a three-way tensor decomposition task and define the score function according to the decomposition operations. (3) Neural network models (Socher et al., 2013; Dettmers et al., 2018; Nguyen et al., 2018; Shang et al., 2019) usually design neural network modules to enhance the expressive abilities. Generally, given a triple query (es , rq , ?), KGE models select the entity eo , whose score function f (es , rq , eo ) has the highest score as the final prediction. Although KGE models are efficient, they lack interpretability of their predictions. 5.2 Multi-Hop Reasoning Different from embedding-based models, multihop reasoning for KGs aims to predict the tail entity for every triple query (es , rq , ?) and meanwhile provide a reasoning path to support the prediction. Before multi-hop reasoning"
2020.emnlp-main.459,D15-1174,0,0.0908108,"Missing"
2020.emnlp-main.459,D19-1264,0,0.0203564,"-Hop Reasoning Different from embedding-based models, multihop reasoning for KGs aims to predict the tail entity for every triple query (es , rq , ?) and meanwhile provide a reasoning path to support the prediction. Before multi-hop reasoning task is formalized, there are some models on relation path reasoning task, which aims to predict the relation between entities like (es , ?, eo ) using path information. DeepPath (Xiong et al., 2017) first adopts reinforcement learning (RL) framework for relation path reasoning, which inspires much later work (e.g., DIVA (Chen et al., 2018) and AttnPath (Wang et al., 2019)). MINERVA (Das et al., 2018) is the first model that uses REINFORCE algorithm to do the multihop reasoning task. To make the training process of RL models stable, Shen et al. propose M-Walk to solve the reward sparsity problem using off-policy learning. MultiHopKG (Lin et al., 2018) further improves MINERVA using action dropout and reward shaping. Lv et al. (2019) propose MetaKGR to address the new task that multi-hop reasoning on few-shot relations. In order to adapt RL models to a dynamically growing KG, Fu et al. (2019) propose CPL to do multi-hop reasoning and fact extraction jointly. In"
2020.emnlp-main.459,D17-1060,0,0.0187415,"ose score function f (es , rq , eo ) has the highest score as the final prediction. Although KGE models are efficient, they lack interpretability of their predictions. 5.2 Multi-Hop Reasoning Different from embedding-based models, multihop reasoning for KGs aims to predict the tail entity for every triple query (es , rq , ?) and meanwhile provide a reasoning path to support the prediction. Before multi-hop reasoning task is formalized, there are some models on relation path reasoning task, which aims to predict the relation between entities like (es , ?, eo ) using path information. DeepPath (Xiong et al., 2017) first adopts reinforcement learning (RL) framework for relation path reasoning, which inspires much later work (e.g., DIVA (Chen et al., 2018) and AttnPath (Wang et al., 2019)). MINERVA (Das et al., 2018) is the first model that uses REINFORCE algorithm to do the multihop reasoning task. To make the training process of RL models stable, Shen et al. propose M-Walk to solve the reward sparsity problem using off-policy learning. MultiHopKG (Lin et al., 2018) further improves MINERVA using action dropout and reward shaping. Lv et al. (2019) propose MetaKGR to address the new task that multi-hop r"
2020.emnlp-main.459,P19-1226,0,0.0265119,"tions (black dashed arrows) between entities can be inferred from existing triples (solid black arrows) through reasoning paths (bold arrows). However, some relations in the reasoning path are missing (red dashed arrows) in sparse KG, which makes multi-hop reasoning difficult. their further development and adaption for related downstream tasks. Knowledge graphs (KGs) represent the world knowledge in a structured way, and have been proven to be helpful for many downstream NLP tasks like query answering (Guu et al., 2015), dialogue generation (He et al., 2017) and machine reading comprehension (Yang et al., 2019). Despite their wide applications, many KGs still face serious incompleteness (Bordes et al., 2013), which limits Corresponding Author isa child Mark Twain Roughing It American child? spouse Introduction ∗ isa Olivia Langdon To alleviate this issue, some embedding-based models (Bordes et al., 2013; Dettmers et al., 2018) are proposed, most of which embed entities and relations into a vector space and make link predictions to complete KGs. These models focus on efficiently predicting knowledge but lack necessary interpretability. In order to solve this problem, Das et al. (2018) and Lin et al."
2020.nlpcovid19-2.31,N19-1090,0,0.0200894,"Missing"
2020.nlpcovid19-2.31,2020.nlpcovid19-acl.8,0,0.0422058,"Missing"
2021.acl-long.248,Q16-1026,0,0.0342972,"he outer circle represents the fine-grained entity types, some types are denoted by abbreviations. with an additional classifier achieve significant success on this task and gradually become the base paradigm. Such studies demonstrate that deep models could yield remarkable results accompanied by a large amount of annotated corpora. Named entity recognition (NER), as a fundamental task in information extraction, aims to locate and classify named entities from unstructured natural language. A considerable number of approaches equipped with deep neural networks have shown promising performance (Chiu and Nichols, 2016) on fully supervised NER. Notably, pre-trained language models (e.g., BERT (Devlin et al., 2019a)) equal contributions corresponding authors 1 The baselines are available at https://github. com/thunlp/Few-NERD † HoHoteraryr LibOthe on Introduction ∗ rt Airpsopitall Ele cti Dis 1 r Waaster Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised NER datasets and reorganize them into the few-shot setting"
2021.acl-long.248,P18-1009,0,0.0209705,"uld facilitate the understanding of textual knowledge for neural model (Huang et al., 2020). Due to the lack of specific benchmarks of few-shot NER, current methods collect existing NER datasets and use different few-shot settings. To provide a benchmark that could comprehensively assess the generalization of models under few examples, we annotate F EW-NERD. To make the dataset practical and close to reality, we adopt a fine-grained schema of 3199 entity annotation, which is inspired and modified from previous fine-grained entity recognition studies (Ling and Weld, 2012; Gillick et al., 2014; Choi et al., 2018; Ringland et al., 2019). 3 3.1 with dense entities. Thus, as shown in Algorithm 1 we adopt a N -way K∼2K-shot setting in our paper, the primary principle of which is to ensure that each class in S contain K∼2K examples, effectively alleviating the limitations of sampling. Problem Formulation Named Entity Recognition NER is normally formulated as a sequence labeling problem. Specifically, for an input sequence of tokens x = {x1 , x2 , ..., xt }, NER aims to assign each token xi a label yi ∈ Y to indicate either the token is a part of a named entity (such as Person, Organization, Location) or n"
2021.acl-long.248,W17-4418,0,0.0463172,"Missing"
2021.acl-long.248,N19-1423,0,0.188345,"with an additional classifier achieve significant success on this task and gradually become the base paradigm. Such studies demonstrate that deep models could yield remarkable results accompanied by a large amount of annotated corpora. Named entity recognition (NER), as a fundamental task in information extraction, aims to locate and classify named entities from unstructured natural language. A considerable number of approaches equipped with deep neural networks have shown promising performance (Chiu and Nichols, 2016) on fully supervised NER. Notably, pre-trained language models (e.g., BERT (Devlin et al., 2019a)) equal contributions corresponding authors 1 The baselines are available at https://github. com/thunlp/Few-NERD † HoHoteraryr LibOthe on Introduction ∗ rt Airpsopitall Ele cti Dis 1 r Waaster Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised NER datasets and reorganize them into the few-shot setting for empirical study. These strategies conventionally aim to recognize coarse-grained entity typ"
2021.acl-long.248,D19-1033,1,0.833167,"pe-level generalization and knowledge transfer of NER methods, respectively. We implement models based on the recent state-of-theart approaches and evaluate them on F EW-NERD (Section 7). And empirical results show that F EW-NERD is challenging on all these three settings. We also conduct sets of subsidiary experiments to analyze promising directions of few-shot NER. Hopefully, the research of few-shot NER could be further facilitated by F EW-NERD. 2 Related Work As a pivotal task of information extraction, NER is essential for a wide range of technologies (Cui et al., 2017; Li et al., 2019b; Ding et al., 2019; Shen et al., 2020). And a considerable number of NER datasets have been proposed over the years. For example, CoNLL’03 (Tjong Kim Sang, 2002) is regarded as one of the most popular datasets, which is curated from Reuters News and includes 4 coarsegrained entity types. Subsequently, a series of NER datasets from various domains are proposed (Balasuriya et al., 2009; Ritter et al., 2011; Weischedel et al., 2013; Stubbs and Uzuner, 2015; Derczynski et al., 2017). These datasets formulate a sequence labeling task and most of them contain 4-18 entity types. Among them, due to the high quality and"
2021.acl-long.248,D18-1514,1,0.532942,". In the testing procedure, all the classes are unseen in the training phase, and by using few labeled examples of support set Stest , few-shot learning systems need to make T predictions of the unlabeled query set Qtest (S Q = ∅). However, in the sequence labeling problem like NER, a sentence may contain multiple entities from different classes. And it is imperative to sample examples in sentence-level since contextual information is crucial for sequence labeling problems, especially for NER. Thus the sampling is more difficult than conventional classification tasks like relation extraction (Han et al., 2018). Some previous works (Yang and Katiyar, 2020; Li et al., 2020a) use greedy-based sampling strategies to iteratively judge if a sentence could be added into the support set, but the limitation becomes gradually strict during the sampling. For example, when it comes to a 5-way 5-shot setting, if the support set already had 4 classes with 5 examples and 1 class with 4 examples, the next sampled sentence must only contain the specific one entity to strictly meet the requirement of 5 way 5 shot. It is not suitable for F EW-NERD since it is annotated for i = 1 to N do Count[i] = 0 ; 7 8 9 10 4.1 Sc"
2021.acl-long.248,2020.acl-main.128,0,0.0919742,"Missing"
2021.acl-long.248,N16-1030,0,0.0178282,"udes 4 coarsegrained entity types. Subsequently, a series of NER datasets from various domains are proposed (Balasuriya et al., 2009; Ritter et al., 2011; Weischedel et al., 2013; Stubbs and Uzuner, 2015; Derczynski et al., 2017). These datasets formulate a sequence labeling task and most of them contain 4-18 entity types. Among them, due to the high quality and size, OntoNotes 5.0 (Weischedel et al., 2013) is considered as one of the most widely used NER datasets recently. As approaches equipped with deep neural networks have shown satisfactory performance on NER with sufficient supervision (Lample et al., 2016; Ma and Hovy, 2016), few-shot NER has received increasing attention (Hofer et al., 2018; Fritzler et al., 2019; Yang and Katiyar, 2020; Li et al., 2020a). Few-shot NER is a considerably challenging and practical problem that could facilitate the understanding of textual knowledge for neural model (Huang et al., 2020). Due to the lack of specific benchmarks of few-shot NER, current methods collect existing NER datasets and use different few-shot settings. To provide a benchmark that could comprehensively assess the generalization of models under few examples, we annotate F EW-NERD. To make the"
2021.acl-long.248,2020.acl-main.519,0,0.42669,"la r Otholiticianr Dir er ect or https://ningding97.github.io/fewnerd/ With the emerging of knowledge from various domains, named entities, especially ones that need professional knowledge to understand, are difficult to be manually annotated on a large scale. Under this circumstance, studying NER systems that could learn unseen entity types with few examples, i.e., few-shot NER, plays a critical role in this area. There is a growing body of literature that recognizes the importance of few-shot NER and contributes to the task (Hofer et al., 2018; Fritzler et al., 2019; Yang and Katiyar, 2020; Li et al., 2020a; Huang et al., 2020). Unfortunately, there is still no dataset specifically designed for 3198 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3198–3213 August 1–6, 2021. ©2021 Association for Computational Linguistics few-shot NER. Hence, these methods collect previously proposed supervised NER datasets and reorganize them into a few-shot setting. Common options of datasets include OntoNotes (Weischedel et al., 2013), CoNLL’03 (Tjong Kim Sang, 2002), WNUT’17 (Derczynski e"
2021.acl-long.248,P19-1430,1,0.820394,"generalization, type-level generalization and knowledge transfer of NER methods, respectively. We implement models based on the recent state-of-theart approaches and evaluate them on F EW-NERD (Section 7). And empirical results show that F EW-NERD is challenging on all these three settings. We also conduct sets of subsidiary experiments to analyze promising directions of few-shot NER. Hopefully, the research of few-shot NER could be further facilitated by F EW-NERD. 2 Related Work As a pivotal task of information extraction, NER is essential for a wide range of technologies (Cui et al., 2017; Li et al., 2019b; Ding et al., 2019; Shen et al., 2020). And a considerable number of NER datasets have been proposed over the years. For example, CoNLL’03 (Tjong Kim Sang, 2002) is regarded as one of the most popular datasets, which is curated from Reuters News and includes 4 coarsegrained entity types. Subsequently, a series of NER datasets from various domains are proposed (Balasuriya et al., 2009; Ritter et al., 2011; Weischedel et al., 2013; Stubbs and Uzuner, 2015; Derczynski et al., 2017). These datasets formulate a sequence labeling task and most of them contain 4-18 entity types. Among them, due to"
2021.acl-long.248,P16-1101,0,0.0303931,"entity types. Subsequently, a series of NER datasets from various domains are proposed (Balasuriya et al., 2009; Ritter et al., 2011; Weischedel et al., 2013; Stubbs and Uzuner, 2015; Derczynski et al., 2017). These datasets formulate a sequence labeling task and most of them contain 4-18 entity types. Among them, due to the high quality and size, OntoNotes 5.0 (Weischedel et al., 2013) is considered as one of the most widely used NER datasets recently. As approaches equipped with deep neural networks have shown satisfactory performance on NER with sufficient supervision (Lample et al., 2016; Ma and Hovy, 2016), few-shot NER has received increasing attention (Hofer et al., 2018; Fritzler et al., 2019; Yang and Katiyar, 2020; Li et al., 2020a). Few-shot NER is a considerably challenging and practical problem that could facilitate the understanding of textual knowledge for neural model (Huang et al., 2020). Due to the lack of specific benchmarks of few-shot NER, current methods collect existing NER datasets and use different few-shot settings. To provide a benchmark that could comprehensively assess the generalization of models under few examples, we annotate F EW-NERD. To make the dataset practical a"
2021.acl-long.248,P19-1510,0,0.0199172,"understanding of textual knowledge for neural model (Huang et al., 2020). Due to the lack of specific benchmarks of few-shot NER, current methods collect existing NER datasets and use different few-shot settings. To provide a benchmark that could comprehensively assess the generalization of models under few examples, we annotate F EW-NERD. To make the dataset practical and close to reality, we adopt a fine-grained schema of 3199 entity annotation, which is inspired and modified from previous fine-grained entity recognition studies (Ling and Weld, 2012; Gillick et al., 2014; Choi et al., 2018; Ringland et al., 2019). 3 3.1 with dense entities. Thus, as shown in Algorithm 1 we adopt a N -way K∼2K-shot setting in our paper, the primary principle of which is to ensure that each class in S contain K∼2K examples, effectively alleviating the limitations of sampling. Problem Formulation Named Entity Recognition NER is normally formulated as a sequence labeling problem. Specifically, for an input sequence of tokens x = {x1 , x2 , ..., xt }, NER aims to assign each token xi a label yi ∈ Y to indicate either the token is a part of a named entity (such as Person, Organization, Location) or not belong to any entitie"
2021.acl-long.248,D11-1141,0,0.0392524,"search of few-shot NER could be further facilitated by F EW-NERD. 2 Related Work As a pivotal task of information extraction, NER is essential for a wide range of technologies (Cui et al., 2017; Li et al., 2019b; Ding et al., 2019; Shen et al., 2020). And a considerable number of NER datasets have been proposed over the years. For example, CoNLL’03 (Tjong Kim Sang, 2002) is regarded as one of the most popular datasets, which is curated from Reuters News and includes 4 coarsegrained entity types. Subsequently, a series of NER datasets from various domains are proposed (Balasuriya et al., 2009; Ritter et al., 2011; Weischedel et al., 2013; Stubbs and Uzuner, 2015; Derczynski et al., 2017). These datasets formulate a sequence labeling task and most of them contain 4-18 entity types. Among them, due to the high quality and size, OntoNotes 5.0 (Weischedel et al., 2013) is considered as one of the most widely used NER datasets recently. As approaches equipped with deep neural networks have shown satisfactory performance on NER with sufficient supervision (Lample et al., 2016; Ma and Hovy, 2016), few-shot NER has received increasing attention (Hofer et al., 2018; Fritzler et al., 2019; Yang and Katiyar, 202"
2021.acl-long.248,W02-2024,0,0.528147,"and Katiyar, 2020; Li et al., 2020a; Huang et al., 2020). Unfortunately, there is still no dataset specifically designed for 3198 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3198–3213 August 1–6, 2021. ©2021 Association for Computational Linguistics few-shot NER. Hence, these methods collect previously proposed supervised NER datasets and reorganize them into a few-shot setting. Common options of datasets include OntoNotes (Weischedel et al., 2013), CoNLL’03 (Tjong Kim Sang, 2002), WNUT’17 (Derczynski et al., 2017), etc. These research efforts of few-shot learning for named entities mainly face two challenges: First, most datasets used for few-shot learning have only 418 coarse-grained entity types, making it hard to construct an adequate variety of “N-way” metatasks and learn correlation features. And in reality, we observe that most unseen entities are finegrained. Second, because of the lack of benchmark datasets, the settings of different works are inconsistent (Huang et al., 2020; Yang and Katiyar, 2020), leading to unclear comparisons. To sum up, these methods ma"
2021.acl-long.248,2020.emnlp-main.129,1,0.877673,"Missing"
2021.acl-long.248,2020.emnlp-main.516,0,0.358306,"Claar ne S S oldie P chola r Otholiticianr Dir er ect or https://ningding97.github.io/fewnerd/ With the emerging of knowledge from various domains, named entities, especially ones that need professional knowledge to understand, are difficult to be manually annotated on a large scale. Under this circumstance, studying NER systems that could learn unseen entity types with few examples, i.e., few-shot NER, plays a critical role in this area. There is a growing body of literature that recognizes the importance of few-shot NER and contributes to the task (Hofer et al., 2018; Fritzler et al., 2019; Yang and Katiyar, 2020; Li et al., 2020a; Huang et al., 2020). Unfortunately, there is still no dataset specifically designed for 3198 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3198–3213 August 1–6, 2021. ©2021 Association for Computational Linguistics few-shot NER. Hence, these methods collect previously proposed supervised NER datasets and reorganize them into a few-shot setting. Common options of datasets include OntoNotes (Weischedel et al., 2013), CoNLL’03 (Tjong Kim Sang, 2002), WNUT"
2021.acl-long.248,D18-1259,0,0.0541568,"Missing"
2021.acl-long.491,W13-2322,0,0.111426,"Missing"
2021.acl-long.491,D13-1185,0,0.0173718,"2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi6284 Event Semantic Pre-training Unsupervised Corpora Trigger-Argument Pair Discrimination attack CNN&apos;s Kelly Wallace reports on today&apos;s attack in Netanya. Text Encoder The army said two soldiers were also among the dead. Trigger Replacement … Netanya reports CNN&apos;s Kelly Wallace today&apos;s reports Argument Replacement AMR Parsing Event Structure Pre-training Parsed AMR Graphs ARG0 ARG1 ARG1 attack-01 time ARG1 today dead ARG1 ARG1 soldier quant today Netanya say-01 army time mod 2 also Subgraph Sa"
2021.acl-long.491,P11-1098,0,0.0297505,"ang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving remarkable performance in benchmarks such as ACE 2005 (Walker et al., 2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi6284 Event Semantic Pre-training Unsupervised Corpora Trigger-Argument Pair Discrimination attack CNN&apos;s Kelly Wallace reports on today&apos;s attack in Netanya. Text Encoder The army said two soldiers were also among the dead. Trigger Replacement … Netanya reports CNN&apos;s Kelly Wallace today&apos;s reports Argument Replacement AMR Parsing Event Structur"
2021.acl-long.491,P17-1038,0,0.0187182,"tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to find reasonable self-supervised signals (Chen et al., 2017; Wang et al., 2019a) for the diverse semantics and complex structures of events. Fortunately, previous work (Aguilar et al., 2014; Huang et al., 2016) has suggested that sentence semantic structures, such as abstract meaning representation (AMR) (Banarescu et al., 2013), contain broad and diverse semantic and structure information relating to events. As shown in Figure 1, the parsed AMR structure covers not only the annotated event (Attack) but also the event that is not defined in the ACE 2005 schema (Report). Considering the fact that the AMR structures of large-scale unsupervised data can"
2021.acl-long.491,P15-1017,0,0.34232,"1 CNN’s Kelly Wallace attack-01 Introduction ∗ Event Schema report-01 classify event types (Attack), as well as event argument extraction task to identify entities serving as event arguments (“today” and “Netanya”) and classify their argument roles (Time-within and Place) (Ahn, 2006). By explicitly capturing the event structure in the text, EE can benefit various downstream tasks such as information reˇ trieval (Glavaˇs and Snajder, 2014) and knowledge base population (Ji and Grishman, 2011). Existing EE methods mainly follow the supervised-learning paradigm to train advanced neural networks (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018) with humanannotated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods"
2021.acl-long.491,2020.emnlp-main.444,0,0.0363461,"et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al., 2020; You et al., 2020; Zhu et al., 2020). In the context of NLP, many established representation learning works can be viewed as contrastive learning methods, such as Word2Vec (Mikolov et al., 2013), BERT (Devlin et al., 2019; Kong et al., 2020) and ELECTRA (Clark et al., 2020). Similar to this work, contrastive learning is also widely-used to help specific tasks, including question answering (Yeh and Chen, 2019), discourse modeling (Iter et al., 2020), natural language inference (Cui et al., 2020) and relation extraction (Peng et al., 2020). 3 Methodology The overall CLEVE framework is illustrated in Figure 2. As shown in the illustration, our contrastive pre-training framework CLEVE consists of two components: event semantic pre-training and event structure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training. 3.1 Preprocessing CLEVE relies on AMR structures (Ban"
2021.acl-long.491,P09-2093,0,0.0217807,"y. Meanwhile, the pre-trained representations can also directly help extract events and discover new event schemata without any known event schema or annotated instances, leading to better generalizability. This is a challenging unsupervised setting named “liberal event extraction” (Huang et al., 2016). Experiments on the widely-used ACE 2005 and the large MAVEN datasets indicate that CLEVE can achieve significant improvements in both settings. 2 Related Work Event Extraction. Most of the existing EE works follow the supervised learning paradigm. Traditional EE methods (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013) rely on manually-crafted features to extract events. In recent years, the neural models become mainstream, which automatically learn effective features with neural networks, including convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015), recurrent neural networks (Nguyen et al., 2016), graph convolutional networks (Nguyen and Grishman, 2018; Lai et al., 2020). With the recent successes of BERT (Devlin et al., 2019), PLMs have also been used for EE (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving rem"
2021.acl-long.491,2020.acl-main.740,0,0.0611314,"Missing"
2021.acl-long.491,P16-1025,0,0.155473,"unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to find reasonable self-supervised signals (Chen et al., 2017; Wang et al., 2019a) for the diverse semantics and complex structures of events. Fortunately, previous work (Aguilar et al., 2014; Huang et al., 2016) has suggested that sentence semantic structures, such as abstract meaning representation (AMR) (Banarescu et al., 2013), contain broad and diverse semantic and structure information relating to events. As shown in Figure 1, the parsed AMR structure covers not only the annotated event (Attack) but also the event that is not defined in the ACE 2005 schema (Report). Considering the fact that the AMR structures of large-scale unsupervised data can be easily obtained with automatic parsers (Wang et al., 2015), we propose CLEVE, an event-oriented contrastive pre-training framework utilizing AMR str"
2021.acl-long.491,2020.emnlp-main.53,0,0.264824,"notated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-base"
2021.acl-long.491,P18-1201,0,0.019335,"employ a PLM as the text encoder and encourage the representations of the word pairs connected by the ARG, time, location edges in AMR structures to be closer in the semantic space than other unrelated words, since these pairs usually refer to the trigger-argument pairs of the same events (as shown in Figure 1) (Huang et al., 2016). This is done by contrastive learning with the connected word pairs as positive samples and unrelated words as negative samples. Moreover, considering event structures are also helpful in extracting events (Lai et al., 2020) and generalizing to new event schemata (Huang et al., 2018), we need to learn transferable event structure representations. Hence we further introduce a graph neural network (GNN) as the graph encoder to encode AMR structures as structure representations. The graph encoder is contrastively pre-trained on the parsed AMR structures of large unsupervised corpora with AMR subgraph discrimination as the objective. By fine-tuning the two pre-trained models on downstream EE datasets and jointly using the two representations, CLEVE can benefit the conventional supervised EE suffering from data scarcity. Meanwhile, the pre-trained representations can also dire"
2021.acl-long.491,2020.acl-main.439,0,0.0207388,"in various domains, such as computer vision (Wu et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al., 2020; You et al., 2020; Zhu et al., 2020). In the context of NLP, many established representation learning works can be viewed as contrastive learning methods, such as Word2Vec (Mikolov et al., 2013), BERT (Devlin et al., 2019; Kong et al., 2020) and ELECTRA (Clark et al., 2020). Similar to this work, contrastive learning is also widely-used to help specific tasks, including question answering (Yeh and Chen, 2019), discourse modeling (Iter et al., 2020), natural language inference (Cui et al., 2020) and relation extraction (Peng et al., 2020). 3 Methodology The overall CLEVE framework is illustrated in Figure 2. As shown in the illustration, our contrastive pre-training framework CLEVE consists of two components: event semantic pre-training and event structure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training. 3.1 Pr"
2021.acl-long.491,2020.findings-emnlp.326,0,0.0290477,"he golden trigger-argument pairs and event structures of ACE 2005 training set instead of the AMR structures of NYT. Similarly, the on ACE (AMR) model is pre-trained with the parsed AMR structures of ACE 2005 training set. We also compare CLEVE with various baselines, including: (1) feature-based method, the top-performing JointBeam (Li et al., 2013); (2) vanilla neural model DMCNN (Chen et al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations. On MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF. Evaluation Results The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa"
2021.acl-long.491,N16-1049,0,0.0182403,"16; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi6284 Event Semantic Pre-training Unsupervised Corpora Trigger-Argument Pair Discrimination attack CNN&apos;s Kelly Wallace reports on today&apos;s attack in Netanya. Text Encoder The army said two soldiers were also among the dead. Trigger Replacement … Netanya reports CNN&apos;s Kelly Wallace today&apos;s reports Argument Replacement AMR Parsing Event Structure Pre-training Parsed AMR Graphs ARG0 ARG1 ARG1 attack-01 time ARG1 today dead ARG1 ARG1 soldier quant today Netanya say-01 army time mod 2 also Subgraph Sampling say-01 quant 2 AMR Subgraph Discrimination rep"
2021.acl-long.491,2020.emnlp-main.128,0,0.0362011,"005 training set instead of the AMR structures of NYT. Similarly, the on ACE (AMR) model is pre-trained with the parsed AMR structures of ACE 2005 training set. We also compare CLEVE with various baselines, including: (1) feature-based method, the top-performing JointBeam (Li et al., 2013); (2) vanilla neural model DMCNN (Chen et al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations. On MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF. Evaluation Results The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa on both ACE 2005 and MAVEN. The p-values under the t-test a"
2021.acl-long.491,2021.ccl-1.108,0,0.0684993,"Missing"
2021.acl-long.491,2020.acl-main.522,1,0.754155,"(Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013) rely on manually-crafted features to extract events. In recent years, the neural models become mainstream, which automatically learn effective features with neural networks, including convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015), recurrent neural networks (Nguyen et al., 2016), graph convolutional networks (Nguyen and Grishman, 2018; Lai et al., 2020). With the recent successes of BERT (Devlin et al., 2019), PLMs have also been used for EE (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving remarkable performance in benchmarks such as ACE 2005 (Walker et al., 2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by"
2021.acl-long.491,P15-1019,0,0.0450974,"Missing"
2021.acl-long.491,N19-1105,1,0.917634,"only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to"
2021.acl-long.491,2020.emnlp-main.129,1,0.921459,"ced neural networks (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018) with humanannotated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the st"
2021.acl-long.491,D19-1584,1,0.881831,"only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to"
2021.acl-long.491,2020.emnlp-main.196,0,0.527338,"f two components: event semantic pre-training and event structure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training. 3.1 Preprocessing CLEVE relies on AMR structures (Banarescu et al., 2013) to build broad and diverse self-supervision signals for learning event knowledge from largescale unsupervised corpora. To do this, we use automatic AMR parsers (Wang et al., 2015; Xu et al., 2020) to parse the sentences in unsupervised corpora into AMR structures. Each AMR structure is a directed acyclic graph with concepts as nodes and semantic relations as edges. Moreover, each node typically only corresponds to at most one word, and a multi-word entity will be represented as a list of nodes connected with name and op (conjunction operator) edges. Considering pretraining entity representations will naturally benefits event argument extraction, we merge these lists into single nodes representing multi-word entities (like the “CNN’s Kelly Wallace” in Figure 1) during both event semanti"
2021.acl-long.491,D19-1582,0,0.0155406,"al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations. On MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF. Evaluation Results The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa on both ACE 2005 and MAVEN. The p-values under the t-test are 4×10−8 , 2×10−8 and 6 × 10−4 for ED on ACE 2005, EAE on ACE 2005, and ED on MAVEN, respectively. It also outperforms or achieves comparable results with 6288 ED Metric (B-Cubed) P R EAE F1 P R ED F1 LiberalEE 55.7 45.1 49.8 36.2 26.5 30.6 RoBERTa RoBERTa+VGAE 44.3 24.9 31.9 24.2 17.3 20.2 47.0 26.8 34.1 25.6 17.9 21.1 CLEVE w/o"
2021.findings-acl.112,W97-1002,0,0.455921,"oroughly study previous DS-RE methods using both held-out and human-labeled test sets, and find that human-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated"
2021.findings-acl.112,P17-1171,0,0.0181524,"ssumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a small proportion of the predictions. Moreover, different works may sample different sp"
2021.findings-acl.112,2020.bionlp-1.20,0,0.0132577,"rvations that have not been clearly demonstrated with the DS evaluation: Pre-trained Models First of all, BERT-based models have achieved supreme performance across all three metrics. To thoroughly examine BERT and its variants in the DS-RE scenario, we further plot their P-R curves with the bag-level manual test in Figure 4. It is surprising to see that all bag-level training strategies, especially the ATT strategy which brings significant improvements for PCNN-based models, do not help or even degenerate the performance with pre-trained ones. This observation is also consistent with that in Amin et al. (2020), though they only compare BERT+bag+AVG and BERT+bag+ATT. We hypothesize the reasons are that solely using pre-trained models already makes a strong baseline, since they exploit more parameters and they have gained pre-encoded knowledge from pretraining (Petroni et al., 2019), all of which make them easier to directly capture relational patterns from noisy data; and bag-level training, which essentially increases the batch size, may raise the optimization difficulty for these large models. Another unexpected observation is that, though the P-R curve of BERT is far above other models in the hel"
2021.findings-acl.112,P19-1279,0,0.0111756,"pment in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at"
2021.findings-acl.112,D18-1247,1,0.809527,"annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al"
2021.findings-acl.112,P11-1055,0,0.0641729,"cale auto-labeled data by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model"
2021.findings-acl.112,D19-1395,0,0.015522,"nn et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data for intermediate pre-training in order to boost supervised RE tasks. As mentioned in our introduction, the evalua1307 #facts Train #sents N/A #facts Validation #sents N/A #facts Test #sents N/A 53 25 18,409 17,137 52"
2021.findings-acl.112,D19-1250,0,0.0233314,"Missing"
2021.findings-acl.112,P18-1199,0,0.0588086,"eir relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data for intermediate pre-training in order to boost supervised RE tasks. As mentioned in ou"
2021.findings-acl.112,L18-1566,0,0.0357812,"Missing"
2021.findings-acl.112,E17-1110,0,0.0117008,"lp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a small proportion of th"
2021.findings-acl.112,N13-1008,0,0.0117331,"e DS relations or no relation at all, while we find that a large proportion of held-out data actually express some other relations; Li et al. (2020) propose active testing, an iterative method to correct the bias of DS evaluation. However, it still requires consistent human efforts during each evaluation phase. To the best of our knowledge, our work, building benchmarks with large-scale manuallylabeled test data, conducts the most comprehensive human evaluations of DS-RE methods so far. 3 DS-RE Datasets In this section, we introduce the way we build the manually-annotated test sets for NYT10 (Riedel et al., 2013) and Wiki20 (Han et al., 2020). We show the statistics of these datasets in Table 1. 3.1 NYT10 Dataset NYT10 is constructed by aligning facts from the FreeBase (Bollacker et al., 2008) with the New York Times (NYT) corpus (Sandhaus, 2008). The original NYT10 dataset contains 53 relations (including N/A). After thoroughly examining the dataset, we found that (1) there are many duplicate instances in the dataset, (2) there is no public validation set, and some previous works directly take the test set to tune the model, and (3) the relation ontology is not reasonable for an RE task. Therefore, w"
2021.findings-acl.112,C02-1151,0,0.0835297,"n-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use"
2021.findings-acl.112,2020.findings-emnlp.20,0,0.0351636,"manual test. tion of DS-RE has long been a problem, especially since many existing methods solely rely on autolabeled test data. Some preliminaries have noticed this problem: Jiang et al. (2018); Zhu et al. (2020) also annotate the test set of NYT10, yet Jiang et al. (2018) only sample 2, 040 sentences from it, and Zhu et al. (2020) discard all N/A data from DS, which are an important part of DS evaluation, and assume that the original held-out data have either the DS relations or no relation at all, while we find that a large proportion of held-out data actually express some other relations; Li et al. (2020) propose active testing, an iterative method to correct the bias of DS evaluation. However, it still requires consistent human efforts during each evaluation phase. To the best of our knowledge, our work, building benchmarks with large-scale manuallylabeled test data, conducts the most comprehensive human evaluations of DS-RE methods so far. 3 DS-RE Datasets In this section, we introduce the way we build the manually-annotated test sets for NYT10 (Riedel et al., 2013) and Wiki20 (Han et al., 2020). We show the statistics of these datasets in Table 1. 3.1 NYT10 Dataset NYT10 is constructed by a"
2021.findings-acl.112,P17-1004,1,0.862832,"Missing"
2021.findings-acl.112,D12-1042,0,0.0264152,"17). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora ("
2021.findings-acl.112,P16-1200,1,0.914386,"nal facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, wh"
2021.findings-acl.112,P09-1113,0,0.549708,"and observations can help advance future DS-RE research.1 1 Musk owns 28.9M Tesla shares. Tesla Inc. Elon Musk Figure 1: Typical errors made by DS evaluation. In the figure, DS labels the bag with only the relation CEO, while none of the sentences express the relation. Also, it misses a correct relation shareholder due to the incompleteness of the knowledge graphs. Introduction Relation extraction (RE) aims at extracting relational facts between entities from the text. One crucial challenge for building an effective RE system is how to obtain sufficient annotated data. To tackle this problem, Mintz et al. (2009) propose distant supervision (DS) to generate large-scale auto-labeled data by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely"
2021.findings-acl.112,2020.emnlp-main.298,1,0.820774,"ginal bag-level training, we carry out a pilot experiment to examine the effect of the sampled training. From Table 3, we can see that our sampling strategy does not significantly hurt the performance of the bag-level training. We also add another variant, BERT-M, in our evaluation. We observe from the top predictions of BERT models (Figure 3) that BERT tends to make false-positive errors for entity pairs that express a relation in the KG but do not have any sentence truly expressing the relation in the data, probably due to that model learns shallow cues solely from entities. Thus, following Peng et al. (2020), we mask entity mentions during training and inference to avoid learning biased heuristics from entities. 5 5.1 Experiment Implementation Details We use the OpenNRE toolkit (Han et al., 2019) for most of our experiments, including both sentencelevel and bag-level training. For CNN and PCNN, we follow the hyper-parameters of Han et al. (2019). For BERT, we use pre-trained checkpoint bert-base-uncased for initialization, take a batch size of 64, a bag size of 4 and a learning rate of 2 × 10−5 ,3 and train the model for 3 epochs. For RL-DSRE, RESIDE and BGWA, we directly use their original imple"
2021.findings-acl.112,Q17-1008,0,0.0120081,":// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a"
2021.findings-acl.112,D18-1157,0,0.024953,"Missing"
2021.findings-acl.112,N16-1103,0,0.0275135,"Missing"
2021.findings-acl.112,C18-1099,1,0.824993,"t, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data f"
2021.findings-acl.112,D17-1187,0,0.0175736,"thout human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020"
2021.findings-acl.112,D15-1203,0,0.1272,"by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and hu"
2021.findings-acl.112,C14-1220,0,0.0174976,"cially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel"
2021.findings-acl.112,N19-1306,0,0.0138632,"ades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informativ"
2021.findings-acl.112,D17-1004,0,0.0123124,", which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Sur"
2021.findings-acl.112,P19-1139,1,0.799285,"ades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informativ"
2021.findings-acl.112,P05-1053,0,0.0494374,"n-labeled test sets, and find that human-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without hum"
2021.findings-acl.112,P19-1128,1,0.843226,"generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Som"
2021.findings-acl.112,2020.coling-main.566,0,0.0927166,"Missing"
2021.naacl-main.452,P15-1034,0,0.023785,"Comprehensive experiments on two real-world datasets demonstrate the effectiveness of OHRE on both relation clustering and hierarchy expansion. 2 Related Works Open Relation Extraction. Recent years have witnessed an upsurge of interest in open relation extraction (OpenRE) that aims to identify new relations in unsupervised data. Existing OpenRE methods can be divided into tagging-based methods and clustering-based methods. Tagging-based methods seek to extract surface form of relational phrases from text in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008), or supervised paradigms (Angeli et al., 2015; Cui et al., 2018; Stanovsky et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Simon et al. (2019) introduce skewness loss to e"
2021.naacl-main.452,P98-1012,0,0.0117585,"(RWHAC) (Elsahar et al., 2017) is the state-of-the-art rich feature-based method. RW-HAC first extracts rich features, such as entity types, then reduces feature dimension via principal component analysis, and finally clusters the features with HAC. (4) Discrete-state variational autoencoder (VAE) (Elsahar et al., 2017) optimizes a relations classifier via reconstruction signals, with rich features including dependency paths and POS tags. Evaluation Metrics. Following Wu et al. (2019); Hu et al. (2020), we adopt instance-level evaluation metrics to evaluate relation clustering, including B3 (Bagga and Baldwin, 1998), V-measure (Rosenberg and Hirschberg, 2007) and Adjusted Rand Index (ARI) (Hubert and Arabie, 1985). We refer readers to the appendix for more detailed descriptions about the evaluation metrics. 4.2.2 Hierarchy Expansion Setting In this setting, models are required to first cluster novel relations, and then further add the extracted relations into the existing hierarchy in train set. Baselines. To the best of our knowledge, there are no existing OpenRE methods designed to directly expand an existing relation hierarchy. We design two strong baselines based on state-of-theart OpenRE architectur"
2021.naacl-main.452,P08-1004,0,0.484805,"relation hierarchies with a top-down algorithm. (3) Comprehensive experiments on two real-world datasets demonstrate the effectiveness of OHRE on both relation clustering and hierarchy expansion. 2 Related Works Open Relation Extraction. Recent years have witnessed an upsurge of interest in open relation extraction (OpenRE) that aims to identify new relations in unsupervised data. Existing OpenRE methods can be divided into tagging-based methods and clustering-based methods. Tagging-based methods seek to extract surface form of relational phrases from text in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008), or supervised paradigms (Angeli et al., 2015; Cui et al., 2018; Stanovsky et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Si"
2021.naacl-main.452,P18-2065,0,0.141205,"ering and hierarchy expansion. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/OHRE. Training instances significant person participant … Test instances winner Hierarchy Expansion relative spouse father child Representation Learning OHRE Novel relations Relation Clustering … … Figure 1: The workflow of OHRE framework. Trained with relation hierarchy and labeled instances, OHRE extracts novel relations from open-domain corpora and adds them into the existing hierarchy. task, which extracts relational phrases from sentences (Banko et al., 2007; Cui et al., 2018). In contrast, clustering-based methods aim to cluster relation instances into groups based on their semantic similarities, and regard each cluster as a relation (Yao et al., 2011; Wu et al., 2019). However, most OpenRE models cast different relation types in isolation, without considering their rich hierarchical dependencies. Hierarchical organization of relations has been shown to play a central role in the abstraction and generalization ability 1 Introduction of human (Tenenbaum et al., 2011). This hierarchical organization of relations also constitutes the Open relation extraction (OpenRE)"
2021.naacl-main.452,N19-1423,0,0.0216324,"Missing"
2021.naacl-main.452,D18-1247,1,0.819224,"downstream tasks. Hierarchical informa1 tion derived from concept ontologies can reveal E.g., the number of relations in Wikidata has grown to more than 8, 000 in the last 6 years. semantic similarity (Leacock and Chodorow, 1998; 5683 Ponzetto and Strube, 2007), and is widely applied in enhancing classification models (Rousu et al., 2005; Weinberger and Chapelle, 2009) and knowledge representation learning models (Hu et al., 2015; Xie et al., 2016). Similar to concept hierarchy, some recent works try to exploit semantic connections from relation hierarchy. In the field of relation extraction, Han et al. (2018a) propose a hierarchical attention scheme to alleviate the noise in distant supervision. Zhang et al. (2019) leverage implicit hierarchical knowledge from KBs and propose coarse-to-fine grained attention for long-tail relations. However, these methods are designed to identify pre-defined relations, and cannot be applied to OpenRE that aims to discover novel relations in open-domain corpora. 3 OHRE Framework We divide the open hierarchical relation extraction problem into two phases: (1) learning relation representations with hierarchical information and (2) clustering and linking novel relati"
2021.naacl-main.452,D18-1514,1,0.942413,"downstream tasks. Hierarchical informa1 tion derived from concept ontologies can reveal E.g., the number of relations in Wikidata has grown to more than 8, 000 in the last 6 years. semantic similarity (Leacock and Chodorow, 1998; 5683 Ponzetto and Strube, 2007), and is widely applied in enhancing classification models (Rousu et al., 2005; Weinberger and Chapelle, 2009) and knowledge representation learning models (Hu et al., 2015; Xie et al., 2016). Similar to concept hierarchy, some recent works try to exploit semantic connections from relation hierarchy. In the field of relation extraction, Han et al. (2018a) propose a hierarchical attention scheme to alleviate the noise in distant supervision. Zhang et al. (2019) leverage implicit hierarchical knowledge from KBs and propose coarse-to-fine grained attention for long-tail relations. However, these methods are designed to identify pre-defined relations, and cannot be applied to OpenRE that aims to discover novel relations in open-domain corpora. 3 OHRE Framework We divide the open hierarchical relation extraction problem into two phases: (1) learning relation representations with hierarchical information and (2) clustering and linking novel relati"
2021.naacl-main.452,2020.emnlp-main.299,0,0.713973,"et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Simon et al. (2019) introduce skewness loss to enable stable training of VAE. Hu et al. (2020) learn relation representations and clusters iteratively via self-training. Wu et al. (2019) improve conventional unsupervised clustering-based methods by combining supervised and unsupervised data via siamese networks, and achieve state-of-the-art performance. However, existing OpenRE methods cast different relation types in isolation without considering their rich hierarchical dependencies. Hierarchy Information Exploitation. Wellorganized taxonomy and hierarchies can facilitate many downstream tasks. Hierarchical informa1 tion derived from concept ontologies can reveal E.g., the number of r"
2021.naacl-main.452,Q16-1017,0,0.766042,"ods. Tagging-based methods seek to extract surface form of relational phrases from text in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008), or supervised paradigms (Angeli et al., 2015; Cui et al., 2018; Stanovsky et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Simon et al. (2019) introduce skewness loss to enable stable training of VAE. Hu et al. (2020) learn relation representations and clusters iteratively via self-training. Wu et al. (2019) improve conventional unsupervised clustering-based methods by combining supervised and unsupervised data via siamese networks, and achieve state-of-the-art performance. However, existing OpenRE methods cast different relation types in isolation without considering their rich hierarchical depend"
2021.naacl-main.452,D14-1162,0,0.0860062,"Missing"
2021.naacl-main.452,D19-1021,1,0.212409,"ances winner Hierarchy Expansion relative spouse father child Representation Learning OHRE Novel relations Relation Clustering … … Figure 1: The workflow of OHRE framework. Trained with relation hierarchy and labeled instances, OHRE extracts novel relations from open-domain corpora and adds them into the existing hierarchy. task, which extracts relational phrases from sentences (Banko et al., 2007; Cui et al., 2018). In contrast, clustering-based methods aim to cluster relation instances into groups based on their semantic similarities, and regard each cluster as a relation (Yao et al., 2011; Wu et al., 2019). However, most OpenRE models cast different relation types in isolation, without considering their rich hierarchical dependencies. Hierarchical organization of relations has been shown to play a central role in the abstraction and generalization ability 1 Introduction of human (Tenenbaum et al., 2011). This hierarchical organization of relations also constitutes the Open relation extraction (OpenRE) aims to extract novel relations types between entities from open- foundation of most modern KBs (Auer et al., 2007; Bollacker et al., 2008). Figure 1 illustrates an exdomain corpora, which plays a"
2021.naacl-main.452,D11-1135,0,0.228983,"cipant … Test instances winner Hierarchy Expansion relative spouse father child Representation Learning OHRE Novel relations Relation Clustering … … Figure 1: The workflow of OHRE framework. Trained with relation hierarchy and labeled instances, OHRE extracts novel relations from open-domain corpora and adds them into the existing hierarchy. task, which extracts relational phrases from sentences (Banko et al., 2007; Cui et al., 2018). In contrast, clustering-based methods aim to cluster relation instances into groups based on their semantic similarities, and regard each cluster as a relation (Yao et al., 2011; Wu et al., 2019). However, most OpenRE models cast different relation types in isolation, without considering their rich hierarchical dependencies. Hierarchical organization of relations has been shown to play a central role in the abstraction and generalization ability 1 Introduction of human (Tenenbaum et al., 2011). This hierarchical organization of relations also constitutes the Open relation extraction (OpenRE) aims to extract novel relations types between entities from open- foundation of most modern KBs (Auer et al., 2007; Bollacker et al., 2008). Figure 1 illustrates an exdomain corp"
2021.naacl-main.452,P12-1075,0,0.724872,"Missing"
2021.naacl-main.452,D07-1043,0,0.0263829,"e state-of-the-art rich feature-based method. RW-HAC first extracts rich features, such as entity types, then reduces feature dimension via principal component analysis, and finally clusters the features with HAC. (4) Discrete-state variational autoencoder (VAE) (Elsahar et al., 2017) optimizes a relations classifier via reconstruction signals, with rich features including dependency paths and POS tags. Evaluation Metrics. Following Wu et al. (2019); Hu et al. (2020), we adopt instance-level evaluation metrics to evaluate relation clustering, including B3 (Bagga and Baldwin, 1998), V-measure (Rosenberg and Hirschberg, 2007) and Adjusted Rand Index (ARI) (Hubert and Arabie, 1985). We refer readers to the appendix for more detailed descriptions about the evaluation metrics. 4.2.2 Hierarchy Expansion Setting In this setting, models are required to first cluster novel relations, and then further add the extracted relations into the existing hierarchy in train set. Baselines. To the best of our knowledge, there are no existing OpenRE methods designed to directly expand an existing relation hierarchy. We design two strong baselines based on state-of-theart OpenRE architectures. (1) RW-HAC for hierarchy expansion (RW-H"
2021.naacl-main.452,C14-1220,0,0.0267381,"erarchical than ri2 and rj2 in representation space, since ri1 and curriculum learning for robust model training. Pair- rj1 are close to each other in the relation hierarchy. wise virtual adversarial training is also introduced We design a hierarchical triplet objective with to improve the representation generalization ability. a dynamic margin which is determined by the disRelation Embedding Encoder. We adopt CNN to tance between relations in hierarchy. Specifically, encode sentences into relation representations. Fol- the dynamic margin is conducted over the instances lowing previous works (Zeng et al., 2014), given of the relations. As shown in Figure 2, given two a sentence s and target entity pair (eh , et ), each relations ri and rj sampled by hierarchical curricuword in the sentence is first transformed into in- lum training strategy (which will be introduced put representations by the concatenation of word later), we randomly sample two instances (namely embedding and position embedding indicating the anchor instance a and positive instance p) from ri , position of each entity. Then the input representa- and an instance (namely negative instance n) from tion is fed into a convolutional layer"
2021.naacl-main.452,N19-1306,0,0.0189905,"f relations in Wikidata has grown to more than 8, 000 in the last 6 years. semantic similarity (Leacock and Chodorow, 1998; 5683 Ponzetto and Strube, 2007), and is widely applied in enhancing classification models (Rousu et al., 2005; Weinberger and Chapelle, 2009) and knowledge representation learning models (Hu et al., 2015; Xie et al., 2016). Similar to concept hierarchy, some recent works try to exploit semantic connections from relation hierarchy. In the field of relation extraction, Han et al. (2018a) propose a hierarchical attention scheme to alleviate the noise in distant supervision. Zhang et al. (2019) leverage implicit hierarchical knowledge from KBs and propose coarse-to-fine grained attention for long-tail relations. However, these methods are designed to identify pre-defined relations, and cannot be applied to OpenRE that aims to discover novel relations in open-domain corpora. 3 OHRE Framework We divide the open hierarchical relation extraction problem into two phases: (1) learning relation representations with hierarchical information and (2) clustering and linking novel relations to existing hierarchies. Curriculum Learning *#! !! &quot;! #! *#$ !$ &quot;$ #$ layer 1 *&quot;! layer 2 *&quot;$ Dynamic Ma"
2021.naacl-main.452,P19-1133,0,0.0538988,"8), or supervised paradigms (Angeli et al., 2015; Cui et al., 2018; Stanovsky et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Simon et al. (2019) introduce skewness loss to enable stable training of VAE. Hu et al. (2020) learn relation representations and clusters iteratively via self-training. Wu et al. (2019) improve conventional unsupervised clustering-based methods by combining supervised and unsupervised data via siamese networks, and achieve state-of-the-art performance. However, existing OpenRE methods cast different relation types in isolation without considering their rich hierarchical dependencies. Hierarchy Information Exploitation. Wellorganized taxonomy and hierarchies can facilitate many downstream tasks. Hierarchical inf"
2021.naacl-main.452,N18-1081,0,0.0182471,"world datasets demonstrate the effectiveness of OHRE on both relation clustering and hierarchy expansion. 2 Related Works Open Relation Extraction. Recent years have witnessed an upsurge of interest in open relation extraction (OpenRE) that aims to identify new relations in unsupervised data. Existing OpenRE methods can be divided into tagging-based methods and clustering-based methods. Tagging-based methods seek to extract surface form of relational phrases from text in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008), or supervised paradigms (Angeli et al., 2015; Cui et al., 2018; Stanovsky et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Simon et al. (2019) introduce skewness loss to enable stable training of VAE. Hu et al. (20"
C18-1099,P17-1110,0,0.0236166,"t adverarial training strategies to transfer the features of one source domain to its corresponding target domain. Inspired by Ganin et al. (2016), adversarial training has also been explored in some typical NLP tasks for multi-feature fusion. Park and Im (2016) propose a multi-modal representation learning model based on adversarial training. Then, Liu et al. (2017a) employ adversarial training to construct a multi-task learning model for text classification by extending the original binary adversarial training to the multiclass version. And a similar adversarial framework is also adapted by Chen et al. (2017) to learn features from different datasets for chinese word segmentation. In this paper, we adopt adversarial training to boost feature fusion to grasp the consistency among different languages. 3 Methodology In this section, we introduce the overall framework of our proposed AMNRE in detail. As shown in Figure 1, for each entity pair, AMNRE encodes its corresponding sentences in different languages into several semantic spaces to grasp their individual language patterns. Meanwhile, a unified space is also set up to encode consistent features among languages. By explicitly encoding the consist"
C18-1099,W14-4012,0,0.133982,"Missing"
C18-1099,N15-1151,0,0.0761329,"roblems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this paper, we propose a novel multi-lingual NRE framework to explicitly encode language consistency and diversity into different semantic spaces, which can achieve more effective representations for RE. 2.2 Adversarial Training G"
C18-1099,P11-1055,0,0.162418,"versarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations"
C18-1099,P16-1200,1,0.944037,"lications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction (NRE) models merely focus on extracting relational facts from mono-lingual data, ignoring the rich information in multi-lingual data. Lin et al. (2017) propose a multi-lingual attention-based neural relation extraction (MNRE) model, which considers the consistency and complementarity in multi-lingual data. MNRE builds a sentence representation for each sentence in various languages and employs a mult"
C18-1099,P17-1004,1,0.867008,"the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction (NRE) models merely focus on extracting relational facts from mono-lingual data, ignoring the rich information in multi-lingual data. Lin et al. (2017) propose a multi-lingual attention-based neural relation extraction (MNRE) model, which considers the consistency and complementarity in multi-lingual data. MNRE builds a sentence representation for each sentence in various languages and employs a multi-lingual attention to capture the pattern consistency and complementarity among languages. Although MNRE achieves great success in multi-lingual RE, it still has some problems. MNRE learns a single representation for each sentence in various languages, which cannot well capture both the consistency and diversity of relation patterns in different"
C18-1099,P17-1001,0,0.0571961,"Missing"
C18-1099,D17-1189,0,0.248594,"d models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this"
C18-1099,P09-1113,0,0.957424,"take Chinese and English to show the effectiveness of AMNRE. The experimental results show that AMNRE outperforms all baseline models significantly by explicitly encoding the consistency and diversity among languages. And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same"
C18-1099,P15-1061,0,0.0339267,"dividual representations and consistent representations for each language. In experiments, we take Chinese and English to show the effectiveness of AMNRE. The experimental results show that AMNRE outperforms all baseline models significantly by explicitly encoding the consistency and diversity among languages. And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016)"
C18-1099,D12-1110,0,0.069634,"ifferences between individual representations and consistent representations for each language. In experiments, we take Chinese and English to show the effectiveness of AMNRE. The experimental results show that AMNRE outperforms all baseline models significantly by explicitly encoding the consistency and diversity among languages. And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervisi"
C18-1099,N16-1103,0,0.0745242,"eng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this paper, we propose a novel multi-lingual NRE framework to explicitly encode language consistency and diversity into different semantic spaces, which can achieve more effective representations for RE. 2.2 Adversarial Training Goodfellow et al. (2015)"
C18-1099,D17-1187,0,0.141618,"suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this paper, we propose"
C18-1099,C14-1220,0,0.647507,"nlp/AMNRE. 1 Introduction Relation extraction (RE) is a crucial task in NLP, which aims to extract semantic relations between entity pairs from the sentences containing them. For example, given an entity pair (Bill Gates, Microsoft) and a sentence “Bill Gates is the co-founder and CEO of Microsoft”, we want to figure out the relation Founder between the two entities. RE can potentially benefit many applications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction"
C18-1099,D15-1203,0,0.599989,"xample, given an entity pair (Bill Gates, Microsoft) and a sentence “Bill Gates is the co-founder and CEO of Microsoft”, we want to figure out the relation Founder between the two entities. RE can potentially benefit many applications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction (NRE) models merely focus on extracting relational facts from mono-lingual data, ignoring the rich information in multi-lingual data. Lin et al. (2017) propose a multi-lingual att"
C18-1099,D17-1186,1,0.868287,"distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to e"
C18-1099,D15-1031,0,0.0138338,"at our AMNRE model significantly outperforms the state-of-the-art models. The source code of this paper can be obtained from https://github.com/thunlp/AMNRE. 1 Introduction Relation extraction (RE) is a crucial task in NLP, which aims to extract semantic relations between entity pairs from the sentences containing them. For example, given an entity pair (Bill Gates, Microsoft) and a sentence “Bill Gates is the co-founder and CEO of Microsoft”, we want to figure out the relation Founder between the two entities. RE can potentially benefit many applications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attent"
D18-1121,D15-1103,0,0.0488134,"labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., 2016a; Yuan and Downey, 2018) is based on a large set of fine-grained types and is therefore more challenging. So far, neural models (Dong et al., 2015; Shimaoka et al., 2017; Xin et al., 2018) have achieved state-of-the-art results on FET. All current FET models rely on distant supervision (DS) (Mintz et al., 2009) to obtain training ∗ Schwarzenegger was elected to be the governor. Schwarzenegger acted in the film Terminator. 1 Since entities are classified into labels of types, type and label have the same meaning in this"
D18-1121,E17-2119,0,0.131243,"Missing"
D18-1121,P09-1113,0,0.0272593,"Missing"
D18-1121,D14-1200,0,0.0221804,"the compatibility between the context sentence and each distantly supervised label, in an unsupervised manner using meaning of the label. In previous works, the hierarchical structure of labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., 2016a; Yuan and Downey, 2018) is based on a large set of fine-grained types and is therefore more challenging. So far, neural models (Dong et al., 2015; Shimaoka et al., 2017; Xin et al., 2018) have achieved state-of-the-art results on FET. All current FET models rely on distant supervision (DS) (Mintz et al., 2009) to obtain training ∗ Schwarze"
D18-1121,N15-1054,0,0.0241505,"this problem, we propose Entity Typing with Language Model Enhancement (LME). It is able to measure the compatibility between the context sentence and each distantly supervised label, in an unsupervised manner using meaning of the label. In previous works, the hierarchical structure of labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., 2016a; Yuan and Downey, 2018) is based on a large set of fine-grained types and is therefore more challenging. So far, neural models (Dong et al., 2015; Shimaoka et al., 2017; Xin et al., 2018) have achieved state-of-the-art results on FET. All current"
D18-1121,D14-1162,0,0.0812471,"IKI O NTO N OTES Train Development Test 2,000,000 251,039 10,000 2,202 563 8,963 Table 2: Number of instances in each part of datasets. 3.2 Experiment Settings The baseline for comparison is the hybrid model NFGEC proposed by Shimaoka et al. (2017). It is described as the ET module of our model. Our own model is referred to as NFGEC+LME. We implement our model based on the source code of NFGEC.3 For a fair comparison, the ET module is unchanged, including all hyperparameters and methods of parameter random initialization. Word embeddings are initialized with pretrained embeddings provided by Pennington et al. (2014). There are a few additional hyperparameters in our model. The most important one is λ, the weight between two parts of the loss function. Other ones include the learning rate r for pretraining the language model and the hidden size h of LSTM used in the language model. We perform (5) (6) where L is the matrix of all label embeddings, and Jlm is loss function of the language model used in the training phase. In order to ensure that label embeddings are in the same semantic space with word embeddings, L is initialized with word embeddings of the labels’ names. In the training phase, parameters"
D18-1121,D16-1144,0,0.702433,"tion. On the other hand, entity typing aims to predict context-dependent types of the entity mention, and test datasets are all human-labeled. The difference between DS and human annotation leads to a huge gap in performances between training/development and test dataset.2 To address this problem, we propose Entity Typing with Language Model Enhancement (LME). It is able to measure the compatibility between the context sentence and each distantly supervised label, in an unsupervised manner using meaning of the label. In previous works, the hierarchical structure of labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Y"
D18-1121,Q13-1030,0,0.0465428,"Missing"
D18-1121,E17-1119,0,0.216419,"language model predicts high probability for a reasonable sentence. Before applying the LME module to enhance the ET module, the language model is pre-trained with sentences from the training set. The loss function for s in the pre-train phase is: Jpre = LM({l1 , l2 , ..., e, r1 , r2 , ...}), 3 3.1 PT i=1 yi Li , Jlm = LM({l1 , l2 , ..., h, r1 , r2 , ...}), (4) Dataset W IKI O NTO N OTES Train Development Test 2,000,000 251,039 10,000 2,202 563 8,963 Table 2: Number of instances in each part of datasets. 3.2 Experiment Settings The baseline for comparison is the hybrid model NFGEC proposed by Shimaoka et al. (2017). It is described as the ET module of our model. Our own model is referred to as NFGEC+LME. We implement our model based on the source code of NFGEC.3 For a fair comparison, the ET module is unchanged, including all hyperparameters and methods of parameter random initialization. Word embeddings are initialized with pretrained embeddings provided by Pennington et al. (2014). There are a few additional hyperparameters in our model. The most important one is λ, the weight between two parts of the loss function. Other ones include the learning rate r for pretraining the language model and the hidd"
D18-1121,P12-1076,0,0.0756863,"Missing"
D18-1121,N18-1002,0,0.290333,"aims to predict context-dependent types of the entity mention, and test datasets are all human-labeled. The difference between DS and human annotation leads to a huge gap in performances between training/development and test dataset.2 To address this problem, we propose Entity Typing with Language Model Enhancement (LME). It is able to measure the compatibility between the context sentence and each distantly supervised label, in an unsupervised manner using meaning of the label. In previous works, the hierarchical structure of labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al.,"
D18-1121,D15-1083,0,0.0905318,"Missing"
D18-1121,P15-2048,0,0.36254,"idered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., 2016a; Yuan and Downey, 2018) is based on a large set of fine-grained types and is therefore more challenging. So far, neural models (Dong et al., 2015; Shimaoka et al., 2017; Xin et al., 2018) have achieved state-of-the-art results on FET. All current FET models rely on distant supervision (DS) (Mintz et al., 2009) to obtain training ∗ Schwarzenegger was elected to be the governor. Schwarzenegger acted in the film Terminator. 1 Since entities are classified into labels of types, type and label have the same meaning in this paper. 2 In the W IKI d"
D18-1121,C12-2133,0,0.0329572,"of the label. In previous works, the hierarchical structure of labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., 2016a; Yuan and Downey, 2018) is based on a large set of fine-grained types and is therefore more challenging. So far, neural models (Dong et al., 2015; Shimaoka et al., 2017; Xin et al., 2018) have achieved state-of-the-art results on FET. All current FET models rely on distant supervision (DS) (Mintz et al., 2009) to obtain training ∗ Schwarzenegger was elected to be the governor. Schwarzenegger acted in the film Terminator. 1 Since entities are classified int"
D18-1247,P11-1055,0,0.879452,"Parent Relation indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG F"
D18-1247,D17-1191,0,0.558533,"; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mechanisms, such as reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017), have also been adapted for RE recently. However, most existing works model each relation in isolation to identify informative instances, neglecting rich correlations among relations, especially the hierarchical information of those relations. Hierarchical information is widely applied for model enhancement, especially for classification models (McCallum et al., 1998; Rousu et al., 2005; Weinberger and Chapelle, 2009; Zhao et al., 2011; Bi and Kwok,"
D18-1247,P16-1200,1,0.84698,"to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an example, in which relations are labeled as hierarchical structures. For example, the 2236 Proceedings of the 2018 Conference on Empirical Methods in Na"
D18-1247,D17-1189,0,0.645343,"wever, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an example, in which relations are labeled as hierarchical structures. For example, the 2236 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 22"
D18-1247,P09-1113,0,0.988836,"Parent Relation Parent Relation Tail Entity: Davos Ernst Haefliger died on Saturday in Davos. 0.8 0.6 0.4 Ernst Haefliger was born in Davos on July 6, 1919 0.1 0.2 0.3 Ernst Haefliger was born in Davos, Switzerland. 0.1 0.2 0.3 Attention Scores Figure 1: An example of hierarchical relation extraction. Relation extraction (RE) aims to predict relational facts from plain text. Conventional supervised RE models (Zelenko et al., 2003; Mooney and Bunescu, 2006) usually suffer from the lack of high-quality training data, because manual labeling of training data is time-consuming and humanintensive. Mintz et al. (2009) propose distant supervision to automatically label training instances by aligning existing knowledge graphs (KGs) and text: For an entity pair in KGs, those sentences containing both the entities will be labeled with † /people/deceased_person Parent Relation /people/deceased_person/place_of_death Introduction ∗ Parent Relation indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanie"
D18-1247,N13-1008,0,0.0863505,", especially for those long-tail relations. 2 Related Works Supervised models (Zelenko et al., 2003; Zhou et al., 2005; Mooney and Bunescu, 2006) for RE require adequate amounts of annotated data for their training. It is time-consuming to manually label large-scale training data. Hence, Mintz et al. (2009) propose distant supervision to automatically label data. Distant supervision inevitably accompanies with the wrong labeling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for dista"
D18-1247,D15-1206,0,0.0488813,"beling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advan"
D18-1247,D15-1203,0,0.837594,"orresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an exam"
D18-1247,C14-1220,0,0.921179,"s with the wrong labeling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al."
D18-1247,D17-1186,1,0.936619,"xplicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mechanisms, such as reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017), have also been adapted for RE recently. However, most existing works model each relation in isolation to identify informative instances, neglecting rich correlations among relations, especially the hierarchical information of those relations. Hierarchical information is widely applied for model enhancement, especially for classification mode"
D18-1247,P15-1061,0,0.0757283,"o alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategi"
D18-1247,D12-1042,0,0.778504,"es equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et a"
D18-1247,D17-1004,0,0.0499094,"tic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mechanisms, such as reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017), have also been adapted for RE recently. However, most existing works model each relation in isolation to identify informative instances, neglecting rich correlat"
D18-1247,N16-1103,0,0.0621293,"nt supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017"
D18-1247,W16-1312,0,0.0321395,"el et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mecha"
D18-1247,P05-1053,0,0.487507,"ction. Since there are more sufficient data for training the top-layer attention, the whole hierarchical attention scheme can enhance RE models for solving those long-tail relations. We conduct experiments on a large-scale benchmark dataset for RE in this paper. The experimental results show that the proposed coarse-tofine grained attention scheme based on relation hierarchies significantly outperforms other baseline methods, even as compared to the recent stateof-the-art attention-based models, especially for those long-tail relations. 2 Related Works Supervised models (Zelenko et al., 2003; Zhou et al., 2005; Mooney and Bunescu, 2006) for RE require adequate amounts of annotated data for their training. It is time-consuming to manually label large-scale training data. Hence, Mintz et al. (2009) propose distant supervision to automatically label data. Distant supervision inevitably accompanies with the wrong labeling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to"
D18-1247,D17-1187,0,0.700834,"tic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an example, in which relations are labeled as hierarchical structures. For example, the 2236 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2236–2245 c Brussels"
D18-1514,D17-1189,0,0.0448817,"Missing"
D18-1514,P17-1040,0,0.0127205,"ion. Recently, Yu et al. (2018) propose a multi-metric method for few-shot text classification. However, there lack systematic researches about adopting fewshot learning for NLP tasks. We propose FewRel: a new large-scale supervised Few-shot Relation Classification dataset. To address the wrong labeling problem in most distantly supervised RC datasets, we apply crowd-sourcing to manually remove the noise.i Besides constructing the dataset, we systematically implement the most recent state-of-theart few-shot learning methods and adapt them for i Many previous works, such as (Roth et al., 2013; Luo et al., 2017; Xin et al., 2018) have worked on automatically removing noise from distantly supervision. Instead, we use crowd-sourcing methods to achieve a high accuracy. RC. We conduct a detailed evaluation for all these models on our dataset. Though the state-of-theart few-shot learning methods have much lower results than humans on our challenging dataset, they significantly outperform the vanilla RC models, indicating that incorporating few-shot learning is promising and needs further research. In summary, our contribution is three-fold: (1) We formulate RC as a few-shot learning task, and propose a n"
D18-1514,C16-1017,0,0.0297203,"to learn fast-learning abilities from previous experience and rapidly generalize to new concepts. Many meta-learning models (Ravi and Larochelle, 2017; Santoro et al., 2016; Finn et al., 2017; Munkhdalai and Yu, 2017) achieve the state-of-the-art results on several few-shot benchmarks. Though meta-learning methods develop fast, most of these works evaluate on two popular datasets, Omniglot (Lake et al., 2015) and miniImageNet (Vinyals et al., 2016). Both the datasets concentrate on image classification. Many works in NLP mainly focus on the zero-shot/semisupervised scenario (Xie et al., 2016; Ma et al., 2016; Carlson et al., 2009), which incorporate extra information to classify objects never appearing in the training sets. However, the few-shot scenario needs models to classify objects with few instances without any extra information. Recently, Yu et al. (2018) propose a multi-metric method for few-shot text classification. However, there lack systematic researches about adopting fewshot learning for NLP tasks. We propose FewRel: a new large-scale supervised Few-shot Relation Classification dataset. To address the wrong labeling problem in most distantly supervised RC datasets, we apply crowd-so"
D18-1514,D15-1205,0,0.0717884,"Missing"
D18-1514,W09-2415,0,0.370152,"Missing"
D18-1514,P11-1055,0,0.360509,"Missing"
D18-1514,D17-1191,0,0.0382899,"Missing"
D18-1514,P16-1200,1,0.932224,"Missing"
D18-1514,strassel-etal-2008-linguistic,0,0.0427918,"irater kappa (Randolph, 2005), and keep the top 100 relations. 2.3 Dataset #cls. #insts. 9 24 42 57 100 6, 674 16, 771 21, 784 143, 391 70, 000 Table 3: Comparison of FewRel with existing RC datasets. Note that negative (no relation) instances in some datasets are ignored. idation, and testing respectively. Table 2 provides a comparison of our FewRel dataset to two other popular few-shot classification datasets, Omniglot and mini-ImageNet. Table 3 provides a comparison of FewRel to the previous RC datasets, including SemEval-2010 Task 8 dataset (Hendrickx et al., 2009), ACE 2003-2004 dataset (Strassel et al., 2008), TACRED dataset (Zhang et al., 2017), and NYT-10 dataset (Riedel et al., 2010). While some RC datasets contain instances with no relations (negative), we ignore such instances for comparison. 3 Experiments We conduct comprehensive evaluations of vanilla RC models with simple strategies such as finetune or kNN on our new dataset. We also evaluate the recent state-of-the-art few-shot learning methods. 3.1 Task Formulation In few-shot relation classification, we intend to obtain a function F : (R, S, x) 7→ y. Here R = {r1 , . . . , rm } defines the relations that the instances are classified int"
D18-1514,D12-1042,0,0.153708,"Missing"
D18-1514,D17-1187,0,0.0652808,"Missing"
D18-1514,D18-1121,1,0.796734,"et al. (2018) propose a multi-metric method for few-shot text classification. However, there lack systematic researches about adopting fewshot learning for NLP tasks. We propose FewRel: a new large-scale supervised Few-shot Relation Classification dataset. To address the wrong labeling problem in most distantly supervised RC datasets, we apply crowd-sourcing to manually remove the noise.i Besides constructing the dataset, we systematically implement the most recent state-of-theart few-shot learning methods and adapt them for i Many previous works, such as (Roth et al., 2013; Luo et al., 2017; Xin et al., 2018) have worked on automatically removing noise from distantly supervision. Instead, we use crowd-sourcing methods to achieve a high accuracy. RC. We conduct a detailed evaluation for all these models on our dataset. Though the state-of-theart few-shot learning methods have much lower results than humans on our challenging dataset, they significantly outperform the vanilla RC models, indicating that incorporating few-shot learning is promising and needs further research. In summary, our contribution is three-fold: (1) We formulate RC as a few-shot learning task, and propose a new large supervised"
D18-1514,N18-1109,0,0.0692706,"several few-shot benchmarks. Though meta-learning methods develop fast, most of these works evaluate on two popular datasets, Omniglot (Lake et al., 2015) and miniImageNet (Vinyals et al., 2016). Both the datasets concentrate on image classification. Many works in NLP mainly focus on the zero-shot/semisupervised scenario (Xie et al., 2016; Ma et al., 2016; Carlson et al., 2009), which incorporate extra information to classify objects never appearing in the training sets. However, the few-shot scenario needs models to classify objects with few instances without any extra information. Recently, Yu et al. (2018) propose a multi-metric method for few-shot text classification. However, there lack systematic researches about adopting fewshot learning for NLP tasks. We propose FewRel: a new large-scale supervised Few-shot Relation Classification dataset. To address the wrong labeling problem in most distantly supervised RC datasets, we apply crowd-sourcing to manually remove the noise.i Besides constructing the dataset, we systematically implement the most recent state-of-theart few-shot learning methods and adapt them for i Many previous works, such as (Roth et al., 2013; Luo et al., 2017; Xin et al., 2"
D18-1514,W02-1010,0,0.201432,"Missing"
D18-1514,D15-1203,0,0.119209,"nder four different settings. In recent research on few-shot learning, N way K shot setting is widely adopted. We follow this setting for the few-shot relation classification problem. To be exact, for N way K shot learning N = m = |R|, K = n1 = . . . = nm 3.2 (2) Experiment Settings We consider four types of few-shot tasks in our experiments: 5 way 1 shot, 5 way 5 shot, 10 way 1 shot, 10 way 5 shot. Under this setting, we evaluate different few-shot training strategies and stateof-the-art few-shot learning methods built upon two widely used instance encoders, CNN (Zeng et al., 2014) and PCNN (Zeng et al., 2015). For both CNN and PCNN, the sentence is first represented to the input vectors by transforming each word into concatenation of word embeddings and position embeddings. In CNN, the input vectors pass a convolution layer, a max-pooling layer, and a non-linear activation layer to get the final output sentence embedding. PCNN is a variant of CNN, which replaces the max-pooling operation with a piecewise max-pooling operation. To evaluate this two vanilla models in few-shot RC task, we first consider two training strategies, namely Finetune and kNN. For the Finetune baseline, it learns to classify"
D18-1514,C14-1220,0,0.364541,"viet writer. Test Instance (A) or (B) or (C) Euler was elected a foreign member of the Royal Swedish Academy of Sciences. Table 1: An example for a 3 way 2 shot scenario. Different colors indicate different entities, blue for head entity, and red for tail entity. Introduction Relation classification (RC) is an important task in NLP, aiming to determine the correct relation between two entities in a given sentence. Many works have been proposed for this task, including kernel methods (Zelenko et al., 2002; Mooney and Bunescu, 2006), embedding methods (Gormley et al., 2015), and neural methods (Zeng et al., 2014). The performance of these conventional models heavily depends on time-consuming and labor-intensive annotated data, which make themselves hard to generalize well. Adopting distant supervision is a primary approach to alleviate this problem for RC (Mintz et al.; Riedel et al.; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng ∗ The first four authors contribute equally. The order is determined by dice rolling. † Z. Wang is now at New York University. ‡ Correspondence author. et al., 2015; Lin et al., 2016), which heuristically aligns knowledge bases (KBs) and text to automatically annotate ad"
D18-1514,D17-1186,1,0.886952,"Missing"
D18-1514,D17-1004,0,0.0706106,"the top 100 relations. 2.3 Dataset #cls. #insts. 9 24 42 57 100 6, 674 16, 771 21, 784 143, 391 70, 000 Table 3: Comparison of FewRel with existing RC datasets. Note that negative (no relation) instances in some datasets are ignored. idation, and testing respectively. Table 2 provides a comparison of our FewRel dataset to two other popular few-shot classification datasets, Omniglot and mini-ImageNet. Table 3 provides a comparison of FewRel to the previous RC datasets, including SemEval-2010 Task 8 dataset (Hendrickx et al., 2009), ACE 2003-2004 dataset (Strassel et al., 2008), TACRED dataset (Zhang et al., 2017), and NYT-10 dataset (Riedel et al., 2010). While some RC datasets contain instances with no relations (negative), we ignore such instances for comparison. 3 Experiments We conduct comprehensive evaluations of vanilla RC models with simple strategies such as finetune or kNN on our new dataset. We also evaluate the recent state-of-the-art few-shot learning methods. 3.1 Task Formulation In few-shot relation classification, we intend to obtain a function F : (R, S, x) 7→ y. Here R = {r1 , . . . , rm } defines the relations that the instances are classified into. S is a support set S = {(x11 , r1"
D18-2024,P15-1067,0,0.573991,"nce and Technology, Beijing Normal University, Beijing, China Abstract knowledge embedding (KE) approaches have been proposed to embed both entities and relations in KGs into a continuous low-dimensional space, such as linear models (Bordes et al., 2011, 2012, 2014), latent factor models (Sutskever et al., 2009; Jenatton et al., 2012; Yang et al., 2015; Liu et al., 2017), neural models (Socher et al., 2013; Dong et al., 2014), matrix factorization models (Nickel et al., 2011, 2012, 2016; Trouillon et al., 2016), and translation models (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015). These models have achieved great performance on benchmark datasets. However, there exist two main issues which may lead to difficulty in full utilization and further development. On the one hand, the existing implementations are scattered and unsystematic to some extent. For example, the interfaces of these model implementations are inconsistent with each other. On the other hand, these model implementations mainly focus on model validation and are often time-consuming, which makes it difficult to apply them for realworld applications. Hence, it becomes urgent to develop an efficient and eff"
D19-1021,P08-1004,0,0.552062,"Missing"
D19-1021,P18-2065,0,0.125725,"-ended growth of new relation types in the open-domain corpora. To solve this problem, recently many efforts have been invested in exploring methods for open relation extraction (OpenRE), which aims to discover new relation types from unsupervised open-domain corpora. OpenRE methods can be roughly divided into two categories: taggingbased and clustering-based. Tagging-based methods cast OpenRE as a sequence labeling problem, and extract relational phrases consisting of words from sentences in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008) or supervised paradigms (Jia et al., 2018; Cui et al., 2018; Stanovsky et al., 2018). However, tagging-based methods often extract multiple overly-speciﬁc relational phrases for the same relation type, and cannot be readily utilized for downstream tasks. In comparison, conventional clustering-based OpenRE methods extract rich features for relation instances via external linguistic tools, and cluster semantic patterns into several relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012). Marcheggiani (2016) proposes a reconstructionbased model discrete-state variational autoencoder for OpenRE via unlabeled instances. Elsahar (2017) utilizes a clus"
D19-1021,D18-1514,1,0.907647,"01; Yao et al., 2011, 2012). Marcheggiani (2016) proposes a reconstructionbased model discrete-state variational autoencoder for OpenRE via unlabeled instances. Elsahar (2017) utilizes a clustering algorithm over linguistic features. In this paper, we focus on the clustering-based OpenRE methods, which have the advantage of discovering highly distinguishable relation types. Few-shot Learning. Few-shot learning aims to classify instances with a handful of labeled samples. Many efforts are devoted to few-shot image classiﬁcation (Koch et al., 2015) and relation classiﬁcation (Yuan et al., 2017; Han et al., 2018). Notably, (Koch et al., 2015) introduces Convolu220 Twain was a writer of America max FC vl classifier distance Kenji was a poet of Japan vd max FC a max-pooling layer, and a fully-connected (FC) layer. The embedding layer transforms the words in a sentence x and the positions of entities ehead and etail into pre-trained word embeddings and random-initialized position embeddings. Following (Zeng et al., 2014), we concatenate these embeddings to form a vector sequence. Next, a one-dimensional convolutional layer and a maxpooling layer transform the vector sequence into features. Finally, an FC"
D19-1021,P16-1200,1,0.917398,"Missing"
D19-1021,P14-5010,0,0.00429393,"via unlabeled instances. It optimizes a relation classiﬁer by reconstructing entities from pairing entities and predicted relation types. Rich features including entity words, context words, trigger words, dependency paths, and context POS tags are used to predict the relation type. RW-HAC and VAE both rely on external linguistic tools to extract rich features from plain texts. Speciﬁcally, we ﬁrst align entities to Wikidata and get their KB types. Next, we preprocess the instances with part-of-speech (POS) tagging, named-entity recognition (NER), and dependency parsing with Stanford CoreNLP (Manning et al., 2014). It is worth noting that these features are only used by baseline models. Our models, in contrast, only use sentences and entity pairs as inputs. Evaluation Protocol. In evaluation, we use B 3 metric (Bagga and Baldwin, 1998) as the scoring function. B 3 metric is a standard measure to balance the precision and recall of clustering tasks, and is commonly used in previous OpenRE works (Marcheggiani and Titov, 2016; Elsahar et al., 2017). To be speciﬁc, we use F1 measure, the harmonic mean of precision and recall. First, we report the result of supervised RSN with different clustering methods."
D19-1021,Q16-1017,0,0.394598,"ht our model’s ability to extract new relations, testing instances only contain new relations. indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) 219 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 219–228, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 ers OpenRE as a clustering task for extracting triples with new relation types. However, previous clustering-based OpenRE methods (Yao et al., 2011, 2012; Marcheggiani and Titov, 2016; Elsahar et al., 2017) are mostly unsupervised, and cannot effectively select meaningful relation patterns and discard irrelevant information. In this paper, we propose to take advantage of high-quality supervised data of pre-deﬁned relations for OpenRE. The approach is non-trivial, however, due to the considerable gap between the pre-deﬁned relations and novel relations of interest in open domain. To bridge the gap, we propose Relational Siamese Networks (RSNs) to learn transferable relational knowledge from supervised data for OpenRE. Speciﬁcally, RSNs learn relational similarity metrics fr"
D19-1021,P09-1113,0,0.147833,"relations. To the best of our knowledge, RSN is the ﬁrst model to consider knowledge transfer in clustering-based OpenRE task. (2) We further propose Semi-supervised RSNs and Distantly-supervised RSNs that can learn from various weakly supervised scenarios. The experimental results show that all these RSN models achieve signiﬁcant improvements in F-measure compared with state-of-the-art baselines. Related Work Open Relation Extraction. Relation extraction (RE) is an important task in NLP. Traditional RE methods mainly concentrate on classifying relational facts into pre-deﬁned relation types (Mintz et al., 2009; Yu et al., 2017). Zeng (2014) utilizes CNN encoders to build sentence representations with the help of position embeddings. Lin (2016) further improves RE performance on distantlysupervised data via instance-level attention. These methods take advantage of supervised or distantlysupervised data to learn neural sentence encoders for distributed representations, and have achieved promising results. However, these methods cannot handle the open-ended growth of new relation types in the open-domain corpora. To solve this problem, recently many efforts have been invested in exploring methods for"
D19-1021,D14-1162,0,0.0822568,"for convenience. Experimental results show that the sample ratio decides RSN’s tendency to predict larger or smaller clusters. In other words, it controls the granularity of the predicted relation types. This phenomenon suggests a potential application of our model in hierarchical relation extraction. However, we leave any serious discussion to future work. Hyperparameter Settings. Following (Lin et al., 2016) and (Zeng et al., 2014), we ﬁx the less inﬂuencing hyperparameters for sentence encoding as their reported optimal values. For word embeddings, we use pre-trained 50-dimensional Glove (Pennington et al., 2014) word embeddings. For position embeddings, we use randominitialized 5-dimensional position embeddings. During training, all the embeddings are trainable. For the neural network, the number of feature 4.3 Experiment Results on OpenRE In this section, we demonstrate the effectiveness of our RSN models by comparing our models with state-of-the-art clustering-based OpenRE methods. We also conduct ablation experiments to detailedly investigate the contributions of different mechanisms of Semi-supervised RSN and Distantly-supervised RSN. Baselines. Conventional clustering-based OpenRE models usually"
D19-1021,N18-1081,0,0.039658,"ew relation types in the open-domain corpora. To solve this problem, recently many efforts have been invested in exploring methods for open relation extraction (OpenRE), which aims to discover new relation types from unsupervised open-domain corpora. OpenRE methods can be roughly divided into two categories: taggingbased and clustering-based. Tagging-based methods cast OpenRE as a sequence labeling problem, and extract relational phrases consisting of words from sentences in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008) or supervised paradigms (Jia et al., 2018; Cui et al., 2018; Stanovsky et al., 2018). However, tagging-based methods often extract multiple overly-speciﬁc relational phrases for the same relation type, and cannot be readily utilized for downstream tasks. In comparison, conventional clustering-based OpenRE methods extract rich features for relation instances via external linguistic tools, and cluster semantic patterns into several relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012). Marcheggiani (2016) proposes a reconstructionbased model discrete-state variational autoencoder for OpenRE via unlabeled instances. Elsahar (2017) utilizes a clustering algorithm over lin"
D19-1021,D11-1135,0,0.870996,"(Unlabeled) 1 To highlight our model’s ability to extract new relations, testing instances only contain new relations. indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) 219 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 219–228, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 ers OpenRE as a clustering task for extracting triples with new relation types. However, previous clustering-based OpenRE methods (Yao et al., 2011, 2012; Marcheggiani and Titov, 2016; Elsahar et al., 2017) are mostly unsupervised, and cannot effectively select meaningful relation patterns and discard irrelevant information. In this paper, we propose to take advantage of high-quality supervised data of pre-deﬁned relations for OpenRE. The approach is non-trivial, however, due to the considerable gap between the pre-deﬁned relations and novel relations of interest in open domain. To bridge the gap, we propose Relational Siamese Networks (RSNs) to learn transferable relational knowledge from supervised data for OpenRE. Speciﬁcally, RSNs le"
D19-1021,P12-1075,0,0.11007,"n embeddings. During training, all the embeddings are trainable. For the neural network, the number of feature 4.3 Experiment Results on OpenRE In this section, we demonstrate the effectiveness of our RSN models by comparing our models with state-of-the-art clustering-based OpenRE methods. We also conduct ablation experiments to detailedly investigate the contributions of different mechanisms of Semi-supervised RSN and Distantly-supervised RSN. Baselines. Conventional clustering-based OpenRE models usually cluster instances by either clustering their linguistic features (Lin and Pantel, 2001; Yao et al., 2012; Elsahar et al., 2017) or imposing reconstruction constraints (Yao et al., 2011; Marcheggiani and Titov, 2016). To demonstrate 224 the effectiveness of our RSN models, we compare our models with two state-of-the-art models: (1) HAC with re-weighted word embeddings (RW-HAC) (Elsahar et al., 2017): RW-HAC is the state-of-the-art feature clustering model for OpenRE. The model ﬁrst extracts KB types and NER tags of entities as well as re-weighted word embeddings from sentences, then adopts principal component analysis (PCA) to reduce feature dimensionality, and ﬁnally uses HAC to cluster the conc"
D19-1021,I17-1086,0,0.0548203,"st of our knowledge, RSN is the ﬁrst model to consider knowledge transfer in clustering-based OpenRE task. (2) We further propose Semi-supervised RSNs and Distantly-supervised RSNs that can learn from various weakly supervised scenarios. The experimental results show that all these RSN models achieve signiﬁcant improvements in F-measure compared with state-of-the-art baselines. Related Work Open Relation Extraction. Relation extraction (RE) is an important task in NLP. Traditional RE methods mainly concentrate on classifying relational facts into pre-deﬁned relation types (Mintz et al., 2009; Yu et al., 2017). Zeng (2014) utilizes CNN encoders to build sentence representations with the help of position embeddings. Lin (2016) further improves RE performance on distantlysupervised data via instance-level attention. These methods take advantage of supervised or distantlysupervised data to learn neural sentence encoders for distributed representations, and have achieved promising results. However, these methods cannot handle the open-ended growth of new relation types in the open-domain corpora. To solve this problem, recently many efforts have been invested in exploring methods for open relation extr"
D19-1021,C14-1220,0,0.139899,"g aims to classify instances with a handful of labeled samples. Many efforts are devoted to few-shot image classiﬁcation (Koch et al., 2015) and relation classiﬁcation (Yuan et al., 2017; Han et al., 2018). Notably, (Koch et al., 2015) introduces Convolu220 Twain was a writer of America max FC vl classifier distance Kenji was a poet of Japan vd max FC a max-pooling layer, and a fully-connected (FC) layer. The embedding layer transforms the words in a sentence x and the positions of entities ehead and etail into pre-trained word embeddings and random-initialized position embeddings. Following (Zeng et al., 2014), we concatenate these embeddings to form a vector sequence. Next, a one-dimensional convolutional layer and a maxpooling layer transform the vector sequence into features. Finally, an FC layer with sigmoid activation maps features into a relational vector v. To summarize, we obtain a vector representation v for a relational sentence with our CNN module: 0.7 p vr word position embeddings embeddings Figure 2: The architecture of Relational Siamese Networks. The output is the similarity between two relational instances. v = CNN(s), in which we denote the joint information of a sentence x and two"
D19-1334,N18-1165,0,0.0352096,"ng et al. (2018) predict new facts under a challenging setting where only one training triple for a given relation r is available, which can be seen as a one-shot scenario. Although these models are effective, they lack interpretability for their decisions. Multi-Hop Reasoning over KGs aims to learn symbolic inference rules from relational paths in G and has been formulated as sequential decision problems in recent years. DeepPath (Xiong et al., 2017) first applies RL to search reasoning paths in KGs for a given query, which inspires much later work (e.g., MINERVA (Das et al., 2018) and DIVA (Chen et al., 2018)). Because it is hard to train an RL model, ReinforceWalk (Shen et al., 2018) proposes to solve the reward sparsity problem using off-policy learning. MultiHop (Lin et al., 2018) further extends MINERVA with reward shaping and action dropout, achieveing the state-of-the-art performance. These reasoning methods are intuitive and explainable. However, all of them have a weak performance in few-shot scenarios. In addition to multi-hop reasoning over KGs, there are also some multi-hop QA methods in recent years. Yang et al. (2018) proposes a high quality dataset, which greatly promotes the develop"
D19-1334,P19-1259,0,0.0136091,"(Shen et al., 2018) proposes to solve the reward sparsity problem using off-policy learning. MultiHop (Lin et al., 2018) further extends MINERVA with reward shaping and action dropout, achieveing the state-of-the-art performance. These reasoning methods are intuitive and explainable. However, all of them have a weak performance in few-shot scenarios. In addition to multi-hop reasoning over KGs, there are also some multi-hop QA methods in recent years. Yang et al. (2018) proposes a high quality dataset, which greatly promotes the development of this field. After that, many methods like CogQA (Ding et al., 2019) and DFGN (Xiao et al., 2019) are also proposed. Meta-Learning tries to solve the problem of “fast adaptation on a new training task”. It has been proved to be very successful on few-shot task (Lake et al., 2015; Gu et al., 2018). Previous metalearning models mainly focus on computer vision and imitation learning domains. In this paper, we propose a new model (Meta-KGR) using the metalearning algorithm MAML (Finn et al., 2017) for 3377 rLra ✓ Meta-Learning Algorithm 1 Meta-Learning for multi-hop reasoning over knowledge graphs &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(null)&lt;/latexit&gt; Fast Adaptation &lt;lat"
D19-1334,D18-1398,0,0.0413263,"These reasoning methods are intuitive and explainable. However, all of them have a weak performance in few-shot scenarios. In addition to multi-hop reasoning over KGs, there are also some multi-hop QA methods in recent years. Yang et al. (2018) proposes a high quality dataset, which greatly promotes the development of this field. After that, many methods like CogQA (Ding et al., 2019) and DFGN (Xiao et al., 2019) are also proposed. Meta-Learning tries to solve the problem of “fast adaptation on a new training task”. It has been proved to be very successful on few-shot task (Lake et al., 2015; Gu et al., 2018). Previous metalearning models mainly focus on computer vision and imitation learning domains. In this paper, we propose a new model (Meta-KGR) using the metalearning algorithm MAML (Finn et al., 2017) for 3377 rLra ✓ Meta-Learning Algorithm 1 Meta-Learning for multi-hop reasoning over knowledge graphs &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(null)&lt;/latexit&gt; Fast Adaptation &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(null)&lt;/latexit&gt; &lt;latexit rLrb &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(null)&lt;/latexit&gt; ✓ r3 &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(null)&lt;/latexit&gt; rLrc &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(null)&lt;/latexit&gt; ✓⇤ &lt;latexit sha1_base"
D19-1334,D18-1514,1,0.731523,"ing methods, which leverage the symbolic compositionality of relations in KGs to achieve explainable reasoning results. For example, when queried with (Mark Twain, nationality, ?), multi-hop reasoning models can give not only the target entity America but also multi-hop explainable paths (Mark Twain, bornIn, Florida) ∧ (Florida, locatedIn, America) as well. Most previous work assumes that there are enough triples to train an effective and robust reasoning models for each relation in KGs. However, as shown in Figure 1, a large portion of KG relations are actually long-tail (Xiong et al., 2018; Han et al., 2018) and only contain few triples, which can be called few-shot relations. Some pilot experiments show that the performance of 3376 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3376–3381, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics previous multi-hop reasoning models, e.g., MINERVA (Das et al., 2018) and MultiHop (Lin et al., 2018), on these few-shot relations will drop significantly. Note that, there are some knowledge graph embeddin"
D19-1334,D18-1362,0,0.357769,"me knowledge graph embedding methods (Bordes et al., 2013; Corresponding Author 0 2000 4000 6000 Relation frequency 8000 10000 Figure 1: The histogram of relation frequency in the real-world knowledge graph Wikidata. Introduction ∗ 400 Dettmers et al., 2018) have been proposed to embed entities and relations into semantic spaces to capture inner connections, and then use the learned embeddings for final predictions. Although these embedding-based methods have shown strong abilities in predicting target entities for queries, they only give answers and lack interpretability for their decisions (Lin et al., 2018). In order to make models more intuitive, Das et al. (2018) and Lin et al. (2018) propose multi-hop reasoning methods, which leverage the symbolic compositionality of relations in KGs to achieve explainable reasoning results. For example, when queried with (Mark Twain, nationality, ?), multi-hop reasoning models can give not only the target entity America but also multi-hop explainable paths (Mark Twain, bornIn, Florida) ∧ (Florida, locatedIn, America) as well. Most previous work assumes that there are enough triples to train an effective and robust reasoning models for each relation in KGs. H"
D19-1334,D15-1174,0,0.0874501,"te the meta policy network with parameters θ. Usually, we will go over many tasks in a batch and update θ as follows: X DQ θ = θ − β∇θ Lr (θr0 ), (5) Tr where β is the meta-learning rate. We detail the meta-learning algorithm in Algorithm 1. After previous meta-learning steps, Meta-KGR can fast adapt to a relation-specific policy network for every few-shot relation by using θ as wellinitialized parameters θ∗ . #Ent #Rel #Triples 14,448 3,078 63,524 2,951 200 37 170 30 268,039 4,076 115,454 2,680 Table 2: Statistics of datasets. 5 5.1 Experiments Datasets We use two typical datasets FB15K-237 (Toutanova et al., 2015) and NELL-995 (Xiong et al., 2017) for training and evaluation. Specifically, we set K = 137 and K = 114 to select few-shot relations from FB15K-237 and NELL995 respectively. Besides, we rebuild NELL-995 to generate few-shot relations in valid and test set. Statistics are given separately for normal relations and few-shot relations in Table 2. 5.2 Baselines We compare with four multi-hop reasoning models in experiments: (1) Neural Logical Programming (NerualLP) (Yang et al., 2017); (2) NTP-λ (Rockt¨aschel and Riedel, 2017); (3) MINERVA (Das et al., 2018) and (4) MultiHop (Lin et al., 2018). Fo"
D19-1334,P19-1617,0,0.0179871,"to solve the reward sparsity problem using off-policy learning. MultiHop (Lin et al., 2018) further extends MINERVA with reward shaping and action dropout, achieveing the state-of-the-art performance. These reasoning methods are intuitive and explainable. However, all of them have a weak performance in few-shot scenarios. In addition to multi-hop reasoning over KGs, there are also some multi-hop QA methods in recent years. Yang et al. (2018) proposes a high quality dataset, which greatly promotes the development of this field. After that, many methods like CogQA (Ding et al., 2019) and DFGN (Xiao et al., 2019) are also proposed. Meta-Learning tries to solve the problem of “fast adaptation on a new training task”. It has been proved to be very successful on few-shot task (Lake et al., 2015; Gu et al., 2018). Previous metalearning models mainly focus on computer vision and imitation learning domains. In this paper, we propose a new model (Meta-KGR) using the metalearning algorithm MAML (Finn et al., 2017) for 3377 rLra ✓ Meta-Learning Algorithm 1 Meta-Learning for multi-hop reasoning over knowledge graphs &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(null)&lt;/latexit&gt; Fast Adaptation &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(nu"
D19-1334,D17-1060,0,0.176655,"al., 2016; Shi and Weninger, 2018) incorporate additional entity descriptions to learn embeddings for unseen entities, which can be seen as zero-shot scenarios. Xiong et al. (2018) predict new facts under a challenging setting where only one training triple for a given relation r is available, which can be seen as a one-shot scenario. Although these models are effective, they lack interpretability for their decisions. Multi-Hop Reasoning over KGs aims to learn symbolic inference rules from relational paths in G and has been formulated as sequential decision problems in recent years. DeepPath (Xiong et al., 2017) first applies RL to search reasoning paths in KGs for a given query, which inspires much later work (e.g., MINERVA (Das et al., 2018) and DIVA (Chen et al., 2018)). Because it is hard to train an RL model, ReinforceWalk (Shen et al., 2018) proposes to solve the reward sparsity problem using off-policy learning. MultiHop (Lin et al., 2018) further extends MINERVA with reward shaping and action dropout, achieveing the state-of-the-art performance. These reasoning methods are intuitive and explainable. However, all of them have a weak performance in few-shot scenarios. In addition to multi-hop r"
D19-1334,D18-1223,0,0.311594,"ose multi-hop reasoning methods, which leverage the symbolic compositionality of relations in KGs to achieve explainable reasoning results. For example, when queried with (Mark Twain, nationality, ?), multi-hop reasoning models can give not only the target entity America but also multi-hop explainable paths (Mark Twain, bornIn, Florida) ∧ (Florida, locatedIn, America) as well. Most previous work assumes that there are enough triples to train an effective and robust reasoning models for each relation in KGs. However, as shown in Figure 1, a large portion of KG relations are actually long-tail (Xiong et al., 2018; Han et al., 2018) and only contain few triples, which can be called few-shot relations. Some pilot experiments show that the performance of 3376 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3376–3381, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics previous multi-hop reasoning models, e.g., MINERVA (Das et al., 2018) and MultiHop (Lin et al., 2018), on these few-shot relations will drop significantly. Note that, there are some knowl"
D19-1334,D18-1259,0,0.0386084,"nspires much later work (e.g., MINERVA (Das et al., 2018) and DIVA (Chen et al., 2018)). Because it is hard to train an RL model, ReinforceWalk (Shen et al., 2018) proposes to solve the reward sparsity problem using off-policy learning. MultiHop (Lin et al., 2018) further extends MINERVA with reward shaping and action dropout, achieveing the state-of-the-art performance. These reasoning methods are intuitive and explainable. However, all of them have a weak performance in few-shot scenarios. In addition to multi-hop reasoning over KGs, there are also some multi-hop QA methods in recent years. Yang et al. (2018) proposes a high quality dataset, which greatly promotes the development of this field. After that, many methods like CogQA (Ding et al., 2019) and DFGN (Xiao et al., 2019) are also proposed. Meta-Learning tries to solve the problem of “fast adaptation on a new training task”. It has been proved to be very successful on few-shot task (Lake et al., 2015; Gu et al., 2018). Previous metalearning models mainly focus on computer vision and imitation learning domains. In this paper, we propose a new model (Meta-KGR) using the metalearning algorithm MAML (Finn et al., 2017) for 3377 rLra ✓ Meta-Learn"
D19-1584,P13-1008,0,0.810312,"al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (C"
D19-1584,C10-1077,0,0.409617,"is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et"
D19-1584,P17-1038,0,0.138572,") rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pr"
D19-1584,P10-1081,0,0.697552,"is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et"
D19-1584,P15-1017,0,0.611756,"t al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same supero"
D19-1584,D15-1166,0,0.0125031,"tention score for each hidden embedding to model its correlation with the specific superordinate concept. As an argument role can belong to more than one superordinate concept, we set a logic union module to combine the scores from different superordinate modules together. For each argument role, we hierarchically compose its superordinate concept modules into the integrated hierarchical modular attention component to build its role-oriented embedding. Superordinate Concept Module For a specific superordinate concept c, we represent its semantic features with a trainable vector uc . Following Luong et al. (2015), we adopt a multi-layer perceptron to calculate the attention scores. We first calculate the hidden state, hci = tanh(Wa [hi ; uc ]). (3) Then, we apply a softmax operation to get the attention score for the hidden embedding hi , exp(Wb hci ) sci = Pn c , j=1 exp(Wb hj ) (4) where Wa and Wb are trainable matrices shared among different superordinate concept modules. Logic Union Module Given an argument role r ∈ R, we denote its k superordinate concepts as c1 , c2 , . . . , ck , and the corresponding attention n X sri hi . (6) i=1 2.3 Argument Role Classifier We concatenate the instance embedd"
D19-1584,N18-1076,0,0.0621074,"ller”. Most event extraction (EE) methods treat EE as a two-stage problem, including event detection (ED, to identify the trigger word and determine the event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguy"
D19-1584,N19-1423,0,0.0182971,"der We denote an instance as an n-word sequence x = {w1 , . . . , t, . . . , a, . . . , wn }, where t, a denote the trigger word and the candidate argument respectively. The trigger word is detected by the previous event detection models (independent of our work) and each named entity in the sentence is a candidate argument. Sentence Encoder is adopted to encode the word sequence into hidden embeddings,  {h1 , h2 . . . , hn } = E w1 , . . . , t, . . . , a, . . . , wn , (1) 5778 where E(·) is the neural network to encode the sentence. In this paper, we select CNN (Chen et al., 2015) and BERT (Devlin et al., 2019) as encoders. Feature Aggregator aggregates the hidden embeddings into an instance embedding. Our method is independent of the feature aggregator mechanism. Here, we follow Chen et al. (2015) and use dynamic multi-pooling as the feature aggregator: scores for hi are sci 1 , sci 2 , . . . , sci k computed by Eq. (4). As information about all the superordinate concepts should be retained in the role-oriented embedding, we calculate the mean of the attention scores as the role-oriented attention score, sri k 1 X cj = si , k (5) j=1 [x1,pt ]i = max{[h1 ]i , . . . , [hpt ]i }, [xpt +1,pa ]i = max{["
D19-1584,N16-1034,0,0.62235,"018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept"
D19-1584,D18-1247,1,0.781371,"score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we set a neural module network for each con"
D19-1584,D09-1016,0,0.244851,"e event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-mo"
D19-1584,P11-2105,0,0.0152203,"ng att score input embeddings + ATT att score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we"
D19-1584,P16-1116,0,0.285339,"BERT and HMEAE (BERT) are the same as the BERTBASE model. To utilize the event type information in our model, we append a special token into each input sequence for BERT to indicate the event type. Additional hyperparameters used in our experiments are shown in Table 2. Learning Rate Batch Size Kernel Size Warmup Rate uc dimension Wb dimension 6e-05 50 3 0.1 900 900 Table 2: Hyperparameter settings for BERT models. 3.2 Overall Evaluation Results We compare our models with various state-of-theart baselines on ACE 2005: (1) Feature-based methods, including Li’s joint (Li et al., 2013) and RBPB (Sha et al., 2016). (2) Vanilla neural network methods, including DMCNN (Chen et al., 2015) and JRNN (Nguyen et al., 2016). (3) Neural network with syntax information, like dbRNN (Sha et al., 2018) enhancing the recurrent neural network with dependency bridges to consider syntactically related information. On TAC KBP 2016, we compare our models with the top systems (Dubbin et al., 2016; Hsi et al., 2016; Ferguson et al., 2016) of the competition as well as DMCNN and DMBERT. 5780 Barry Diller on Wednesday quit as chief of Vivendi Universal Entertainment Argument Role Classification P R F1 Method Li’s Joint (Li e"
D19-1584,D18-1093,0,0.0221031,"embeddings + ATT att score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we set a neural module n"
D19-1584,P18-1201,0,0.114469,"(Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods"
D19-1584,N19-1105,1,0.884827,"ted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5777"
D19-1584,P18-2066,0,0.173398,"1 Argument Role Instance Event argument extraction (EAE) aims to identify the entities serving as event arguments and classify the roles they play in an event. For instance, given that the word “sold” triggers a Transfer-Ownership event in the sentence “Steve Jobs sold Pixar to Disney”, EAE aims to identify that “Steve Jobs” is an event argument and its argument role is “Seller”. Most event extraction (EE) methods treat EE as a two-stage problem, including event detection (ED, to identify the trigger word and determine the event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al.,"
D19-1649,W06-1615,0,0.189413,"can sample instances from the training set as supporting instances for NOTA relation (this method is described explicitly in Section 4). Also note that to better demonstrate the effects of the NOTA relation, we use the original FewRel dataset for fewshot NOTA, instead of the new test set, which can get rid of the influence of domain adaptation. 3 Approaches for Few-Shot DA Many efforts have been devoted for domain adaptation, like subspace mapping (Pan et al., 2010; Fernando et al., 2013), finding domain-invariant spaces (Baktashmotlagh et al., 2013; Ganin et al., 2016), feature augmentation (Blitzer et al., 2006) and minimax estimators (Provost and Fawcett, 2001). Among them, adversarial training (Goodfellow et al., 2015; Ganin et al., 2016; Wang et al., 2018) has been proved to be efficient in finding domain-invariant features. It is a game process between an encoder and a discriminator, where the encoder tries to generate domain-invariant features while the discriminator tries to tell which domain the features are from. Here we follow the adversarial training setting in Wang et al. (2018), where a two-layer perceptron network is used as the discriminator. While training the few-shot learning task, w"
D19-1649,P07-1073,0,0.0597172,"mpled from the test set, each of which consists of (R, S, x, r), where R = {r1 , r2 , ..., rN } is the sampled relation set, r ∈ R is the correct relation label for the query x, and S is the supporting set containing K instances for each relation, S = {(xjri , ri )}, 1 ≤ i ≤ N, 1 ≤ j ≤ K. (1) Models should predict the relation label y ∈ R for the query instance x based on the given S and R. Both of the following two challenges are based on this N -way K-shot setting. Both the training and test sets of the original FewRel dataset are constructed by manually annotating the distantly supervised (Bunescu and Mooney, 2007; Mintz et al., 2009) results on Wikipedia corpus and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) knowledge bases. In other words, they are from the same domain, yet in a real-world scenario, we might train models on one domain and perform few-shot learning on a different one. For example, we may train models on Wikipedia, which has large amounts of data and adequate annotations, and then perform few-shot learning on some domains suffering data sparsity, like literature, finance and medicine. Note that, not only do these corpora differ vastly from each other in morphology and syntax, but there"
D19-1649,N19-1423,0,0.0360439,"le NOTA is to regard it as an extra class in the N -way K-shot setting. To be more specific, we can sample instances outside the N relations as the supporting data of NOTA, and perform the (N + 1)-way K-shot learning. As compared to the current methods ignoring NOTA, this approach does not bring much improvements, since the supporting data for NOTA actually belong to several different relations and are scattered in the feature space, making it hard to perform classification. To better address few-shot NOTA, we propose a model named BERT-PAIR based on the sequence classification model in BERT (Devlin et al., 2019). We pair each query instance with all the supporting instances, concatenate each pair as one sequence, and send the concatenated sequence to the BERT sequence classification model to get the score of the two instances expressing the same relation. Denote the BERT model as B, the query instance as x and the paired supporting instance as xjr (the j-th supporting instance for the relation r), B(x, xjr ) outputs a two-element vector corresponding to scores of the pair sharing the same relation and not sharing the same relation. The probability over each relation in the few-shot scenario, includin"
D19-1649,P19-1279,0,0.106636,"Missing"
D19-1649,C18-1099,1,0.86151,"o better demonstrate the effects of the NOTA relation, we use the original FewRel dataset for fewshot NOTA, instead of the new test set, which can get rid of the influence of domain adaptation. 3 Approaches for Few-Shot DA Many efforts have been devoted for domain adaptation, like subspace mapping (Pan et al., 2010; Fernando et al., 2013), finding domain-invariant spaces (Baktashmotlagh et al., 2013; Ganin et al., 2016), feature augmentation (Blitzer et al., 2006) and minimax estimators (Provost and Fawcett, 2001). Among them, adversarial training (Goodfellow et al., 2015; Ganin et al., 2016; Wang et al., 2018) has been proved to be efficient in finding domain-invariant features. It is a game process between an encoder and a discriminator, where the encoder tries to generate domain-invariant features while the discriminator tries to tell which domain the features are from. Here we follow the adversarial training setting in Wang et al. (2018), where a two-layer perceptron network is used as the discriminator. While training the few-shot learning task, we feed the sentence encoder E and the discriminator D with the corpora from the training domain and the test domain, and optimize the min-max game, X"
D19-1649,D18-1514,1,0.42617,"Missing"
D19-1649,W09-2415,0,0.332326,"Missing"
D19-1649,P09-1113,0,0.589697,"ach of which consists of (R, S, x, r), where R = {r1 , r2 , ..., rN } is the sampled relation set, r ∈ R is the correct relation label for the query x, and S is the supporting set containing K instances for each relation, S = {(xjri , ri )}, 1 ≤ i ≤ N, 1 ≤ j ≤ K. (1) Models should predict the relation label y ∈ R for the query instance x based on the given S and R. Both of the following two challenges are based on this N -way K-shot setting. Both the training and test sets of the original FewRel dataset are constructed by manually annotating the distantly supervised (Bunescu and Mooney, 2007; Mintz et al., 2009) results on Wikipedia corpus and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) knowledge bases. In other words, they are from the same domain, yet in a real-world scenario, we might train models on one domain and perform few-shot learning on a different one. For example, we may train models on Wikipedia, which has large amounts of data and adequate annotations, and then perform few-shot learning on some domains suffering data sparsity, like literature, finance and medicine. Note that, not only do these corpora differ vastly from each other in morphology and syntax, but there are wide disparities"
D19-3029,N16-1030,0,0.0461089,"ween the entity pair. Hence Riedel et al. (2010) and Hoffmann et al. (2011) introduce to aggregate the sentences mentioning the same entity pair into Entity-Oriented Applications For extracting structured information from plain text, it requires to extract entities from text and then predict relations between entities. In normal RE scenarios, all entity mentions have been already annotated and RE models are just required to classify relations for all annotated entity pairs. Although the entity-oriented applications are not the focus of our toolkit, we still implement specific modules for NER (Lample et al., 2016) and EL (Han et al., 2011). The NER modules can detect words or phrases (also named entity mentions) representing real-world objects. In OpenNRE, we provide two approaches for NER, one is 170 Architecture of OpenNRE a entity-pair bag. As shown in Figure 1, synthesizing the features of different sentences in a bag can provide more reliable information and result in more accurate predictions. The Bag-level setting is widely applied by various distantly supervised RE methods (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), and thus it is also integrated into OpenNRE. Sentence-Level RE Da"
D19-3029,P16-1200,1,0.949986,"ng, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open infor"
D19-3029,P09-1113,0,0.489525,"sentence-level RE, bag-level RE, document-level RE, and few-shot RE. For completing a full pipeline of extracting structured information, we also enable OpenNRE to have the capacity of entity-oriented applications to a certain extent, e.g., NER and EL. The examples of these application scenarios are all shown in Figure 1. 2.1 Sentence-Level Relation Extraction 2.3 Bag-Level Relation Extraction The supervised RE methods suffer from several problems, especially their requirements of adequate annotated data for training. As manually labeling large amounts of data is expensive and time-consuming, Mintz et al. (2009) introduce distant supervision to automatically label large amounts of data for RE by aligning knowledge graphs and text. Although distant supervision brings sufficient auto-labeled data, it also leads to the wrong labeling problem. Considering an entity pair may occur several times in different sentences, and there is a significant probability that some of these sentences can express the relation between the entity pair. Hence Riedel et al. (2010) and Hoffmann et al. (2011) introduce to aggregate the sentences mentioning the same entity pair into Entity-Oriented Applications For extracting st"
D19-3029,W15-1506,0,0.0733895,"ao∗ , Yuan Yao, Demin Ye, Zhiyuan Liu† , Maosong Sun Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linki"
D19-3029,P15-1034,0,0.0609609,"Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction. Hence, it becomes necessary and significant to systematically develop an efficient and effective toolkit for RE. To this end, we develop an open and extensible toolkit for designing and implementing RE models, especially for NRE models, which is named “OpenNRE”. The toolkit prioritizes operational efficiency based on TensorFlow and PyTorch, which support quick model training and validation. Meanwhile, the toolkit maintains sufficient system encapsulation and model extensibility, which can meet some individual requirements of incorporating new models. To keep t"
D19-3029,N19-1423,0,0.0171983,"bniz was a member of the Prussian Academy of Sciences Samuel Langhorne Clemens, better known by his pen name Mark Twain child of place of death [Olivia Langdon] [writer] Newton served as the president of the Royal Society member of birth name sibling of spouse of occupation Query … Supporting Set member of In 1921, Ernest Hemingway married Hadley Richardson, the first of his four wives [Jean Clemens] Euler was elected a foreign member of the Royal Swedish Academy of Sciences Figure 1: The examples of all application scenarios in OpenNRE. based on spaCy, the other is based on fine-tuning BERT (Devlin et al., 2019). The EL modules can align those entity mentions to the entities in Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) based on TagMe (Ferragina and Scaiella, 2010). models, they can quickly start up their RE system based on OpenNRE, without knowing too many technical details and writing tedious glue code. An online system is also available to extract structured relational facts from the text with friendly interactive interfaces and fast reaction speed. We will provide long-term maintenance to fix bugs and meet new requests for OpenNRE, and we think both researchers and industry developers can benefit"
D19-3029,P19-1279,0,0.172058,"gy and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction. Hence, it becomes necessary and significant to systematically develop an efficient and effective toolkit f"
D19-3029,D17-1187,0,0.633015,"ired to be capable of accurately capturing relation patterns of these small amounts of training instances. Considering few-shot RE is important for handling long-tail relations, OpenNRE also provides a custom platform for further research in this direction. 3 can maximize the reuse of code to avoid unnecessary redundant model implementations. For operational efficiency, OpenNRE is based on TensorFlow and PyTorch, which enables developers to train models on GPUs. For model extensibility, we systematically implement various neural modules and some special algorithms (e.g., adversarial training (Wu et al., 2017) and reinforcement learning (Feng et al., 2018)). Hence, it is easy to implement new RE models based on OpenNRE. We also implement some typical RE models so as to conveniently train custom models for specific application scenarios. More specifically, OpenNRE attains the above four design objects through implementing the following five components. 3.1 Tokenization The tokenization component is responsible for tokenizing input text into several input tokens. In OpenNRE, we implement both word-level tokenization and subword-level tokenization. These two operations satisfy most tokenization demand"
D19-3029,P19-1074,1,0.835257,"te predictions. The Bag-level setting is widely applied by various distantly supervised RE methods (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), and thus it is also integrated into OpenNRE. Sentence-Level RE Data Loader Model Train Method Eval Method Model Encoder Forward Bag-Level RE Few-Shot RE … Softmax Classifier Instance-Level Attention Prototypical Networks … Module Tokenizer Forward Encoder Tokenization CNN BERT … Tokenization Word Tokenization Word Piece Tokenization … 2.4 Framework Framework Module Convolutional NN Pooling … Document-Level Relation Extraction Example Code Yao et al. (2019) have pointed out that multiple entities in documents often exhibit complex intersentence relations rather than intra-sentence relations. Besides, as shown in Figure 1, a large number of relational facts are expressed in multiple sentences, e.g., Langdon is the sibling of Jean Clemens. Hence, it is hard to extract these intersentence relations with both the sentence-level and bag-level settings. Although the document-level RE setting is not widely explored by the current work, we argue that this scenario remains an open problem for future research, and still integrate document-level RE into Op"
D19-3029,P19-1277,0,0.217226,"ntelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction. Hence, it becomes necessary and significant to systematically develop an efficient an"
D19-3029,D18-2024,1,0.917593,"e for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction."
D19-3029,D15-1203,0,0.816632,"a University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 20"
D19-3029,D18-1247,1,0.904919,"e for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction."
D19-3029,C14-1220,0,0.712368,"n Xu Han∗ , Tianyu Gao∗ , Yuan Yao, Demin Ye, Zhiyuan Liu† , Maosong Sun Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scai"
D19-3029,D18-1514,1,0.864567,"e for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction."
D19-3029,Y15-1009,0,0.292487,"hiyuan Liu† , Maosong Sun Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han"
D19-3029,W09-2415,0,0.128206,"Missing"
D19-3029,P19-1139,1,0.92994,"hua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction. Hence, it becomes necessary and significant to systematically develop an efficient and effective toolkit for RE. To this end, w"
D19-3029,P16-2034,0,0.322331,"g Sun Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for"
D19-3029,P11-1055,0,0.138259,"ements of adequate annotated data for training. As manually labeling large amounts of data is expensive and time-consuming, Mintz et al. (2009) introduce distant supervision to automatically label large amounts of data for RE by aligning knowledge graphs and text. Although distant supervision brings sufficient auto-labeled data, it also leads to the wrong labeling problem. Considering an entity pair may occur several times in different sentences, and there is a significant probability that some of these sentences can express the relation between the entity pair. Hence Riedel et al. (2010) and Hoffmann et al. (2011) introduce to aggregate the sentences mentioning the same entity pair into Entity-Oriented Applications For extracting structured information from plain text, it requires to extract entities from text and then predict relations between entities. In normal RE scenarios, all entity mentions have been already annotated and RE models are just required to classify relations for all annotated entity pairs. Although the entity-oriented applications are not the focus of our toolkit, we still implement specific modules for NER (Lample et al., 2016) and EL (Han et al., 2011). The NER modules can detect"
L18-1078,strotgen-gertz-2012-temporal,0,0.0760372,"Missing"
L18-1078,W16-5002,0,0.0709891,"Missing"
L18-1078,D11-1147,0,0.0885703,"Missing"
L18-1078,W08-0606,0,0.0330734,"first priority is to figure out the types of uncertainty in microblogs. Kiefer (2005) pointed out that uncertainty can be divided into Epistemic and Hypothetical. Epistemic contains Possible and Probable. In fact, the two subclasses are fairly similar in Chinese. Wei (2013) observed Question and External frequently appeared on these posts or comments which reveals uncertainty. Considering Dynamic, one sub-class of Hypothetical, hardly existing in Chinese microblogs, we removed this label in our annotation scheme. At present, there are several corpora in different domains: (1) BioScope corpus (Vincze et al., 2008) annotated uncertainty, negation sentences and their scope in biomedical texts containing 20,879 sentences from 3,236 documents. (2) the dataset for CoNLL’2010 shared task (Vincze, 2010) consisted of biological part of BioScope corpus and the selection of Wikipedia articles, which annotated uncertain sentences and cues. (3) Uncertainty Corpus in complex spoken dialogue systems derived from 120 digital dialogues recording from 60 students, totaling 2,171 turns for students and 2,531 turns for tutor. (4) The Scientific Literature Corpus for Chinese (Chen et al., 2013 ) including 19 full papers a"
L18-1078,W10-3001,0,0.0887126,"Missing"
L18-1078,J96-2004,0,0.756853,"Missing"
N19-1105,D15-1247,0,0.110478,"Missing"
N19-1105,C18-1075,0,0.479342,"Missing"
N19-1105,P98-1013,0,0.408429,"Missing"
N19-1105,D18-1021,1,0.869581,"Missing"
N19-1105,P18-1241,0,0.0663714,"Missing"
N19-1105,P17-1038,0,0.449592,"Missing"
N19-1105,P15-1017,0,0.704641,"Missing"
N19-1105,N18-1076,0,0.147721,"Missing"
N19-1105,I17-1036,0,0.128152,"Missing"
N19-1105,P16-2011,0,0.135941,"Missing"
N19-1105,N18-2058,0,0.476683,"Missing"
N19-1105,P16-2060,0,0.130043,"Missing"
N19-1105,P09-2093,0,0.207456,"Missing"
N19-1105,P11-1113,0,0.73024,"Missing"
N19-1105,P18-1048,0,0.155771,"Missing"
N19-1105,P16-1025,0,0.240411,"Missing"
N19-1105,C10-1077,0,0.711492,"Missing"
N19-1105,P10-1081,0,0.823295,"Missing"
N19-1105,P18-1145,0,0.0214274,"Missing"
N19-1105,D18-1127,0,0.406701,"Missing"
N19-1105,P16-1201,0,0.0323032,"Missing"
N19-1105,P17-1164,0,0.540835,"Missing"
N19-1105,P11-1163,0,0.257108,"Missing"
N19-1105,E12-1029,0,0.137736,"Missing"
N19-1105,P09-1113,0,0.272376,"Missing"
N19-1105,P08-1030,0,0.791729,"Missing"
N19-1105,P13-1008,0,0.756973,"Missing"
N19-1105,C18-1007,0,0.0627013,"Missing"
N19-1105,N16-1034,0,0.425474,"Missing"
N19-1105,N16-1033,0,0.379533,"Missing"
N19-1105,P15-2060,0,0.493355,"Missing"
N19-1105,P18-4009,0,0.10604,"Missing"
N19-1105,P18-1046,0,0.0622712,"Missing"
N19-1105,D15-1203,0,0.0776927,"Missing"
N19-1105,P18-2066,0,0.731835,"Missing"
N19-1105,C18-1099,1,0.90081,"Missing"
N19-1105,D17-1187,0,0.0611022,"Missing"
N19-1105,1983.tc-1.13,0,0.586136,"Missing"
P19-1074,P04-1035,0,0.0142302,"ngle sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-le"
P19-1074,P11-1055,0,0.148806,"Missing"
P19-1074,Q17-1008,0,0.237663,"level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-level RE systems. 7 Acknowledgement Fenia Christopoulou, Makoto Miwa, and Sophia Ananiadou. 2018. A walk-based model on entity graphs for relation extraction. In Proceedings of ACL, pages 81–88. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirect"
P19-1074,D14-1162,0,0.0928063,"dels differ only at the encoder used for encoding the document and will be explained in detail in the rest of this section. We refer the readers to the original paper for the details of the Context-Aware model for space limitation. The CNN/LSTM/BiLSTM based models first encode a document D = {wi }ni=1 consisting of n words into a hidden state vector sequence {hi }ni=1 with CNN/LSTM/BiLSTM as encoder, then compute the representations for entities, and finally predict relations for each entity pair. For each word, the features fed to the encoder is the concatenation of its GloVe word embedding (Pennington et al., 2014), entity type embedding and coreference embedding. The entity type embedding is obtained by mapping the entity type (e.g., PER, LOC, ORG) assigned to the word into a vector using an embedding matrix. The entity type is assigned by human for the humanannotated data, and by a fine-tuned BERT model for the distantly supervised data. Named entity mentions corresponding to the same entity are assigned with the same entity id, which is determined by the order of its first appearance in the document. And the entity ids are mapped into vectors as the coreference embeddings. For each named entity menti"
P19-1074,P17-1147,0,0.0349888,"uality datasets. However, these RE datasets limit relations to single sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which ma"
P19-1074,P10-1114,0,0.0111808,"rences Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-level RE systems. 7 Acknowledgeme"
P19-1074,D17-1082,0,0.028811,"ever, these RE datasets limit relations to single sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to"
P19-1074,D12-1110,0,0.0504203,"tion instances annotated for this example document are presented, with named entity mentions involved in these instances colored in blue and other named entity mentions underlined for clarity. Note that mentions of the same subject (e.g., Kungliga Hovkapellet and Royal Court Orchestra) are identified as shown in the first relation instance. work focuses on sentence-level RE, i.e., extracting relational facts from a single sentence. In recent years, various neural models have been explored to encode relational patterns of entities for sentence-level RE, and achieve state-of-theart performance (Socher et al., 2012; Zeng et al., 2014, 2015; dos Santos et al., 2015; Xiao and Liu, 2016; Cai et al., 2016; Lin et al., 2016; Wu et al., 2017; Qin et al., 2018; Han et al., 2018a). Despite these successful efforts, sentence-level RE suffers from an inevitable restriction in practice: a large number of relational facts are expressed in multiple sentences. Taking Figure 1 as an example, multiple entities are mentioned in the document and exhibit complex interactions. In Introduction The task of relation extraction (RE) is to identify relational facts between entities from plain text, which plays an important role"
P19-1074,D17-1188,0,0.159305,"n higher computational complexity such as (Sorokin Benchmark Settings We design two benchmark settings for supervised and weakly supervised scenarios respectively. For both settings, RE systems are evaluated on the high-quality human-annotated dataset, which provides more reliable evaluation results for document-level RE systems. The statistics of data used for the two settings are shown in Table 3. Supervised Setting. In this setting, only humanannotated data is used, which are randomly split 768 model, a bidirectional LSTM (BiLSTM) (Cai et al., 2016) based model and the Context-Aware model (Sorokin and Gurevych, 2017) originally designed for leveraging contextual relations to improve intra-sentence RE. The first three models differ only at the encoder used for encoding the document and will be explained in detail in the rest of this section. We refer the readers to the original paper for the details of the Context-Aware model for space limitation. The CNN/LSTM/BiLSTM based models first encode a document D = {wi }ni=1 consisting of n words into a hidden state vector sequence {hi }ni=1 with CNN/LSTM/BiLSTM as encoder, then compute the representations for entities, and finally predict relations for each entit"
P19-1074,D12-1042,0,0.159686,"Missing"
P19-1074,swampillai-stevenson-2010-inter,0,\N,Missing
P19-1074,P09-1113,0,\N,Missing
P19-1074,W03-0419,0,\N,Missing
P19-1074,C14-1220,0,\N,Missing
P19-1074,doddington-etal-2004-automatic,0,\N,Missing
P19-1074,P16-1072,0,\N,Missing
P19-1074,P16-1200,1,\N,Missing
P19-1074,D17-1004,0,\N,Missing
P19-1074,D17-1187,0,\N,Missing
P19-1074,P18-2014,0,\N,Missing
P19-1074,D18-1259,0,\N,Missing
P19-1074,D18-1247,1,\N,Missing
P19-1074,N19-1423,0,\N,Missing
P19-1074,D18-1514,1,\N,Missing
P19-1074,E17-1110,0,\N,Missing
P19-1074,P15-1061,0,\N,Missing
P19-1074,D15-1203,0,\N,Missing
P19-1074,C16-1119,0,\N,Missing
P19-1085,W13-3819,0,0.0691858,"Missing"
P19-1085,W17-5307,0,0.125125,"“SUPPORTED” example and “REFUTED” example, we cannot verify the given claims via checking any evidence in isolation. The claims can be verified only by understanding and reasoning over the multiple evidence. To integrate and reason over information from multiple pieces of evidence, we propose a Introduction Due to the rapid development of information extraction (IE), huge volumes of data have been extracted. How to automatically verify the data becomes a vital problem for various datadriven applications, e.g., knowledge graph completion (Wang et al., 2017) and open domain question answering (Chen et al., 2017a). Hence, many recent research efforts have been devoted to fact verification (FV), which aims to verify given claims with the evidence retrieved from plain text. † The Rodney King riots took place in the most populous county in the USA. Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) 892 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 892–901 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics graph-based evidence aggregating and reasoning (GEAR) framework. Specifically, we first build a fully-connected"
P19-1085,D17-1070,0,0.0306419,"guage Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competition on Codalab1 with a blind test set. Nie et al. (2019); Yoneda et al. (2018) and Hanselo"
P19-1085,N19-1423,0,0.49966,"r is not sufficient for the claim. Intuitively, by sufficiently exchanging and reasoning over evidence information on the evidence graph, the proposed model can make the best of the information for verifying claims. For example, by delivering the information “Los Angeles County is the most populous county in the USA” to “the Rodney King riots occurred in Los Angeles County” through the evidence graph, the synthetic information can support “The Rodney King riots took place in the most populous county in the USA”. Furthermore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESI"
P19-1085,W18-2501,0,0.0172141,"ose our Graph-based Evidence Aggregating and Reasoning (GEAR) framework in the final claim verification stage. The full pipeline of our method is illustrated in Figure 1. 3.1 Document Retrieval and Sentence Selection In this section, we describe our document retrieval and sentence selection components. Additionally, we add a threshold filter after the sentence selection component to filter out those noisy evidence. In the document retrieval step, we adopt the entity linking approach from Hanselowski et al. (2018). Given a claim, the method first utilizes the constituency parser from AllenNLP (Gardner et al., 2018) to extract potential entities from the claim. Then it uses the entities as search queries and finds relevant Wikipedia documents via the online MediaWiki API2 . The seven highest-ranked results for each query are stored to form a candidate article set. Finally, the method drops the articles which are not in the offline Wikipedia dump and filters the articles by the word overlap between their titles and the claim. The sentence selection component selects the most relevant evidence for the claim from all sentences in the retrieved documents. Hanselowski et al. (2018) modify the ESIM 3.2 Claim V"
P19-1085,J84-3009,0,0.693828,"Missing"
P19-1085,W18-5516,0,0.229796,"ounty in the USA”. Furthermore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competiti"
P19-1085,W18-5525,0,0.0407684,"tive pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018)"
P19-1085,D14-1059,0,0.0998272,"twork is an American basic cable and satellite television channel. Table 1: Some examples of reasoning over several pieces of evidence together for verification. The italic words are the key information to verify the claim. Both of the claims require to reason and aggregate multiple evidence sentences for verification. More specifically, given a claim, an FV system is asked to label it as “SUPPORTED”, “REFUTED”, or “NOT ENOUGH INFO”, which indicate that the evidence can support, refute, or is not sufficient for the claim. Existing FV methods formulate FV as a natural language inference (NLI) (Angeli and Manning, 2014) task. However, they utilize simple evidence combination methods such as concatenating the evidence or just dealing with each evidence-claim pair. These methods are unable to grasp sufficient relational and logical information among the evidence. In fact, many claims require to simultaneously integrate and reason over several pieces of evidence for verification. As shown in Table 1, for both of the “SUPPORTED” example and “REFUTED” example, we cannot verify the given claims via checking any evidence in isolation. The claims can be verified only by understanding and reasoning over the multiple"
P19-1085,D15-1075,0,0.0499913,"cially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018) propose the T WOW ING OS system which trains the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task"
P19-1085,W18-5526,0,0.190196,"Missing"
P19-1085,P17-1171,0,0.224451,"“SUPPORTED” example and “REFUTED” example, we cannot verify the given claims via checking any evidence in isolation. The claims can be verified only by understanding and reasoning over the multiple evidence. To integrate and reason over information from multiple pieces of evidence, we propose a Introduction Due to the rapid development of information extraction (IE), huge volumes of data have been extracted. How to automatically verify the data becomes a vital problem for various datadriven applications, e.g., knowledge graph completion (Wang et al., 2017) and open domain question answering (Chen et al., 2017a). Hence, many recent research efforts have been devoted to fact verification (FV), which aims to verify given claims with the evidence retrieved from plain text. † The Rodney King riots took place in the most populous county in the USA. Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) 892 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 892–901 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics graph-based evidence aggregating and reasoning (GEAR) framework. Specifically, we first build a fully-connected"
P19-1085,W18-5517,0,0.0467547,"hat the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018) propose the T WOW ING OS system which trains the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis"
P19-1085,E17-1002,0,0.0154565,"m verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competition on Codalab1 with a blind test set. Nie et"
P19-1085,W17-5308,0,0.0203664,"intly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competition on Codalab1 with a blind test set. Nie et al. (2019); Yoneda et"
P19-1085,D18-1010,0,0.0874661,"dey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018) propose the T WOW ING OS system which trains the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results ("
P19-1085,W18-5515,0,0.449838,"ore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these metho"
P19-1085,D16-1244,0,0.131708,"Missing"
P19-1085,N18-1202,0,0.0381169,"nd test set. Nie et al. (2019); Yoneda et al. (2018) and Hanselowski et al. (2018) have achieved the top three results among 23 teams. Existing methods mainly formulate FV as an NLI task. Thorne et al. (2018a) simply concatenate all evidence together, and then feed the concatenated evidence and the given claim into the NLI model. Luken et al. (2018) adopt the decomposable attention model (DAM) (Parikh et al., 2016) to generate NLI predictions for each claimevidence pair individually and then aggregate all 2.3 Pre-trained Language Models Pre-trained language representation models such as ELMo (Peters et al., 2018) and OpenAI GPT (Radford et al., 2018) are proven to be effective on many NLP tasks. BERT (Devlin et al., 2019) employs bidirectional transformer and welldesigned pre-training tasks to fuse bidirectional context information and obtains the state-of-theart results on the NLI task. In our experiments, we find the fine-tuned BERT model outperforms other NLI-based models on the claim verification subtask of FEVER. Hence, we use BERT as the sentence encoder in our framework to better encoding semantic information of evidence and claims. 1 https://competitions.codalab.org/ competitions/18814 893 Cla"
P19-1085,C16-1270,0,0.0227379,"ins the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competi"
P19-1085,D18-1185,0,0.0219262,"Missing"
P19-1085,N18-1074,0,0.605773,"the information for verifying claims. For example, by delivering the information “Los Angeles County is the most populous county in the USA” to “the Rodney King riots occurred in Los Angeles County” through the evidence graph, the synthetic information can support “The Rodney King riots took place in the most populous county in the USA”. Furthermore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great result"
P19-1085,W18-5501,0,0.12184,"Missing"
P19-1137,P17-1038,0,0.185671,"Missing"
P19-1137,D14-1164,0,0.0357125,"Missing"
P19-1137,P11-1055,0,0.181204,"a small number of human annotations; • presenting both significant and interpretable performance improvements as well as intuitive diagnostic analyses. Particularly, for one relation with severe false negative noises, we improve the F1 score by about 0.4. To the best of our knowledge, we are the first to explicitly reveal and address this severe noise problem for that dataset. 2 Related Work To reduce labeling noises of DS, earlier work attempted to design specific model architectures that can better tolerate labeling noises, such as the multi-instance learning paradigm (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et a"
P19-1137,P18-1161,1,0.894749,"Missing"
P19-1137,P16-1200,1,0.908825,"e a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction (NRE) models (Zeng et al., 2015; Lin et al., 2016). Although DS is both simple and effective in many cases, it inevitably introduces intolerable labeling noises. As Figure 1 shows, there are two types of error labels, false negatives and false positives. The reason for false negatives is that a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a result heavily weaken th"
P19-1137,N16-1104,0,0.153144,"a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a result heavily weaken the model performance (Riedel et al., 2010). Recent research has realized that introducing appropriate human efforts is essential for reducing such labeling noises. For example, Zhang et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorporate pattern-based labeling, where the k"
P19-1137,D17-1005,0,0.213057,"et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorporate pattern-based labeling, where the key idea was to regard both DS and pattern-based heuristics as the weak supervision sources and develop a weak-label-fusion (WLF) model to produce denoised labels. However, the major limitation of the WLF paradigm lies in the requirement of human experts to write relation-specific patterns. Unfortunately, writing good patterns is both a highskill and labor-intensive task that requires experts to learn detailed pattern-composing instructions, examine adequate examples, tune patterns for different corner cases, etc. For example, the sp"
P19-1137,D17-1189,0,0.680844,"et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorporate pattern-based labeling, where the key idea was to regard both DS and pattern-based heuristics as the weak supervision sources and develop a weak-label-fusion (WLF) model to produce denoised labels. However, the major limitation of the WLF paradigm lies in the requirement of human experts to write relation-specific patterns. Unfortunately, writing good patterns is both a highskill and labor-intensive task that requires experts to learn detailed pattern-composing instructions, examine adequate examples, tune patterns for different corner cases, etc. For example, the sp"
P19-1137,P17-1040,0,0.0220481,"et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al., 2017). Especially, the most recent work (Feng et al., 2018; Qin et al., 2018) adopted RL to train an agent that interacts with the NRE model to learn how to remove or alter noise labels. These methods work without human intervention by utilizing the consistency and difference between DS-generated labels and model-predicted ones. However, such methods can neither discover 1420 noise labels that coincide with the model predictions nor explain the reasons for removed or altered labels. As discussed in the introduction, introducing human efforts is a promising direction to contribute both significant a"
P19-1137,P09-1113,0,0.925853,"s defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction (NRE) models (Zeng et al., 2015; Lin et al., 2016). Although DS is both simple and effective in many cases, it inevitably introduces intolerable labeling noises. As Figure 1 shows, there are t"
P19-1137,D12-1104,0,0.0947384,"improved the performance of the vanilla LSTM (Hochreiter and Schmidhuber, 1997) by utilizing RL to discover structured representations and Li et al. (2016) interpreted the sentiment prediction of neural models by employing RL to find the decision-changing phrases. However, NRE models are unique because we only care about the semantic inter-entity relation mentioned in the sentence. To the best of our knowledge, we are the first to extract patterns from NRE models by RL. We also note that the relational-pattern mining has been extensively studied (Califf and Mooney, 1999; Carlson et al., 2010; Nakashole et al., 2012; Jiang et al., 2017). Different from those studies, our pattern-extraction method 1) is simply based on RL, 2) does not rely on any lexical or syntactic annotation, and 3) can be aware of the pattern importance via the prediction of NRE models. Besides, Takamatsu et al. (2012) inferred negative syntactic patterns via the example-pattern-relation co-occurrence and removed the false-positive labels accordingly. In contrast, built upon modern neural models, our method not only reduces negative patterns to alleviate false positives but also reinforces positive patterns to address false negatives"
P19-1137,C14-1220,0,0.100927,"atives (FN) and false positives (FP), caused by DS. Introduction Relation extraction aims to extract relational facts from the plain text and can benefit downstream knowledge-driven applications. A relational fact is defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent ne"
P19-1137,D14-1162,0,0.0810357,"Missing"
P19-1137,P12-1087,0,0.0275027,"Missing"
P19-1137,P14-2119,1,0.82305,"itives. The reason for false negatives is that a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a result heavily weaken the model performance (Riedel et al., 2010). Recent research has realized that introducing appropriate human efforts is essential for reducing such labeling noises. For example, Zhang et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorp"
P19-1137,P18-1199,0,0.0429382,"se models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al., 2017). Especially, the most recent work (Feng et al., 2018; Qin et al., 2018) adopted RL to train an agent that interacts with the NRE model to learn how to remove or alter noise labels. These methods work without human intervention by utilizing the consistency and difference between DS-generated labels and model-predicted ones. However, such methods can neither discover 1420 noise labels that coincide with the model predictions nor explain the reasons for removed or altered labels. As discussed in the introduction, introducing human efforts is a promising direction to contribute both significant and interpretable improvements, which is also the focus of this paper. As"
P19-1137,P05-1053,0,0.214768,": Two types of error labels, false negatives (FN) and false positives (FP), caused by DS. Introduction Relation extraction aims to extract relational facts from the plain text and can benefit downstream knowledge-driven applications. A relational fact is defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it"
P19-1137,P16-2034,0,0.0665773,"Missing"
P19-1137,P15-1061,0,0.0475427,"ositives (FP), caused by DS. Introduction Relation extraction aims to extract relational facts from the plain text and can benefit downstream knowledge-driven applications. A relational fact is defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction ("
P19-1137,D12-1042,0,0.111105,"n annotations; • presenting both significant and interpretable performance improvements as well as intuitive diagnostic analyses. Particularly, for one relation with severe false negative noises, we improve the F1 score by about 0.4. To the best of our knowledge, we are the first to explicitly reveal and address this severe noise problem for that dataset. 2 Related Work To reduce labeling noises of DS, earlier work attempted to design specific model architectures that can better tolerate labeling noises, such as the multi-instance learning paradigm (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al.,"
P19-1137,P12-1076,0,0.0302716,"models are unique because we only care about the semantic inter-entity relation mentioned in the sentence. To the best of our knowledge, we are the first to extract patterns from NRE models by RL. We also note that the relational-pattern mining has been extensively studied (Califf and Mooney, 1999; Carlson et al., 2010; Nakashole et al., 2012; Jiang et al., 2017). Different from those studies, our pattern-extraction method 1) is simply based on RL, 2) does not rely on any lexical or syntactic annotation, and 3) can be aware of the pattern importance via the prediction of NRE models. Besides, Takamatsu et al. (2012) inferred negative syntactic patterns via the example-pattern-relation co-occurrence and removed the false-positive labels accordingly. In contrast, built upon modern neural models, our method not only reduces negative patterns to alleviate false positives but also reinforces positive patterns to address false negatives at the same time. 3 Methodology Provided with DS-generated data and NRE models trained on them, DIAG-NRE can generate high-quality patterns for the WLF stage to produce denoised labels. As Figure 2 shows, DIAG-NRE contains two key stages in general: pattern extraction (Section"
P19-1137,D17-1187,0,0.107221,"le performance improvements as well as intuitive diagnostic analyses. Particularly, for one relation with severe false negative noises, we improve the F1 score by about 0.4. To the best of our knowledge, we are the first to explicitly reveal and address this severe noise problem for that dataset. 2 Related Work To reduce labeling noises of DS, earlier work attempted to design specific model architectures that can better tolerate labeling noises, such as the multi-instance learning paradigm (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al., 2017). Especially, the most recent work (Feng et al., 2"
P19-1137,D15-1203,0,0.38233,"vised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction (NRE) models (Zeng et al., 2015; Lin et al., 2016). Although DS is both simple and effective in many cases, it inevitably introduces intolerable labeling noises. As Figure 1 shows, there are two types of error labels, false negatives and false positives. The reason for false negatives is that a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a resul"
P19-1139,D15-1075,0,0.0415293,"nington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the fine-grained relations, such as composer and autho"
P19-1139,D18-1021,1,0.795851,"Missing"
P19-1139,P17-1149,0,0.0267538,"token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be aligned to an entity in"
P19-1139,P18-1224,0,0.0426992,"low ··· wn(i 1) 1962 (i 1) e1 (i 1) e2 Entity Input Bob Dylan Blowin’ in the Wind Token Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this"
P19-1139,P18-1009,0,0.0439758,"to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3. As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs. 4.3 Entity Typing Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1. We compare our model with the following baseline models for entity typing: NFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER. As this paper focuses on comparing the general language representation abilities of various n"
P19-1139,N19-1423,0,0.626127,"e Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results"
P19-1139,I05-5002,0,0.0118207,"ovide more information for relation classification than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller training set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE"
P19-1139,D18-1247,1,0.937706,"ty Input Bob Dylan Blowin’ in the Wind Token Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora a"
P19-1139,D18-1514,1,0.936476,"ty Input Bob Dylan Blowin’ in the Wind Token Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora a"
P19-1139,P18-1031,0,0.153256,"olume One is_a Bob Dylan Writer Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language represent"
P19-1139,P16-1200,1,0.845415,"from each class for the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation “no relation”) in TACRED. We compare our model with the following baseline models for relation classification: CNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to"
P19-1139,Q15-1023,0,0.119712,"Missing"
P19-1139,P18-1136,0,0.0322388,"oken Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language represen"
P19-1139,W03-0419,0,0.686759,"Missing"
P19-1139,W16-1313,0,0.0292746,"nd its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1. We compare our model with the following baseline models for entity typing: NFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER. As this paper focuses on comparing the general language representation abilities of various neural models, we thus do not use the hand-craft features in this work. UFET. For Open Entity, we add a new hybrid model UFET (Choi et al., 2018) for comparison. UFET is proposed with the Open Entity dataset, which uses a Bi-LSTM for context representation instead of two Bi-LSTMs separated by entity mentions in NFGEC. Besides NFGEC and UFET, we also report th"
P19-1139,D13-1170,0,0.0050581,"anguage models can provide more information for relation classification than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller training set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small"
P19-1139,speer-havasi-2012-representing,0,0.0818577,"Missing"
P19-1139,D15-1174,0,0.032581,"token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m is not equal to n in most"
P19-1139,P18-1076,0,0.0381481,"T). Radford et al. (2018) propose a generative pre-trained Transformer (Vaswani et al., 2017) (GPT) to learn language representations. Devlin et al. (2019) propose a deep bidirectional model with multiplelayer Transformers (BERT), which achieves the state-of-the-art results for various NLP tasks. Though both feature-based and fine-tuning language representation models have achieved great success, they ignore the incorporation of knowledge information. As demonstrated in recent work, injecting extra knowledge information can significantly enhance original models, such as reading comprehension (Mihaylov and Frank, 2018; Zhong et al., 2018), machine translation (Zaremoodi et al., 2018), natural language 1442 Token Output (i) Token Output Aggregator Information Fusion (i) e1 Entity Output e2 (i) (i) (i) w2 w1 w3 ··· wn(i) e1 ··· w˜n(i) e˜1 (i) e2 (i) e˜2 (i) Entity Output Aggregator K-Encoder Mx Information Fusion Multi-Head Attention Multi-Head Attention (i) (i) e˜2 e˜1 Feed Forward (i) (i) (i) w˜3 w˜2 w˜1 Transformer T-Encoder Nx (i) w4 (i) w˜4 (i) Entity Input Multi-Head Attention Multi-Head Attention Multi-Head Attention (i 1) Token Input w1 bob (i 1) w2 dylan (i 1) w3 wrote (i 1) w4 blow ··· wn(i 1) 1962"
P19-1139,P10-1040,0,0.0598698,"cific NLP tasks. These pre-training approaches can be divided into two classes, i.e., feature-based approaches and finetuning approaches. The early work (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings. Different from the above-mentioned featurebased language approaches only using the pretrained language representations as input features, Dai and Le (2015) train auto-encoders on unlabeled text, and then use the pre-trained model architecture and parameters as a starting point for other specific NLP models. Inspired by Dai and Le (2015)"
P19-1139,D14-1162,0,0.102807,"rresponding author: Z.Liu(liuzy@tsinghua.edu.cn) os er Song Book r tho au Chronicles: Volume One is_a Bob Dylan Writer Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015)"
P19-1139,P17-1161,0,0.0313441,"(liuzy@tsinghua.edu.cn) os er Song Book r tho au Chronicles: Volume One is_a Bob Dylan Writer Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classifica"
P19-1139,N18-1202,0,0.102763,"ture-based approaches and finetuning approaches. The early work (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings. Different from the above-mentioned featurebased language approaches only using the pretrained language representations as input features, Dai and Le (2015) train auto-encoders on unlabeled text, and then use the pre-trained model architecture and parameters as a starting point for other specific NLP models. Inspired by Dai and Le (2015), more pre-trained language representation models for fine-tuning have been proposed. How"
P19-1139,D16-1264,0,0.274955,"e representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly imp"
P19-1139,W18-5446,0,0.172541,") and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the fine-grained relations, such as composer and author on the relation classification task. For th"
P19-1139,D14-1167,0,0.0268165,"input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m i"
P19-1139,N18-1101,0,0.0178587,"ion than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller training set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE is better on CoLA and RTE, but worse on STS-"
P19-1139,D17-1187,0,0.0321162,"or the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation “no relation”) in TACRED. We compare our model with the following baseline models for relation classification: CNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to these three base"
P19-1139,D18-1121,1,0.825023,"Missing"
P19-1139,E17-1055,0,0.0423233,"Missing"
P19-1139,K16-1025,0,0.0384676,"concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be align"
P19-1139,P18-2104,0,0.0250317,"er (Vaswani et al., 2017) (GPT) to learn language representations. Devlin et al. (2019) propose a deep bidirectional model with multiplelayer Transformers (BERT), which achieves the state-of-the-art results for various NLP tasks. Though both feature-based and fine-tuning language representation models have achieved great success, they ignore the incorporation of knowledge information. As demonstrated in recent work, injecting extra knowledge information can significantly enhance original models, such as reading comprehension (Mihaylov and Frank, 2018; Zhong et al., 2018), machine translation (Zaremoodi et al., 2018), natural language 1442 Token Output (i) Token Output Aggregator Information Fusion (i) e1 Entity Output e2 (i) (i) (i) w2 w1 w3 ··· wn(i) e1 ··· w˜n(i) e˜1 (i) e2 (i) e˜2 (i) Entity Output Aggregator K-Encoder Mx Information Fusion Multi-Head Attention Multi-Head Attention (i) (i) e˜2 e˜1 Feed Forward (i) (i) (i) w˜3 w˜2 w˜1 Transformer T-Encoder Nx (i) w4 (i) w˜4 (i) Entity Input Multi-Head Attention Multi-Head Attention Multi-Head Attention (i 1) Token Input w1 bob (i 1) w2 dylan (i 1) w3 wrote (i 1) w4 blow ··· wn(i 1) 1962 (i 1) e1 (i 1) e2 Entity Input Bob Dylan Blowin’ in the Wind Token"
P19-1139,D18-1009,0,0.0262803,"including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the"
P19-1139,D15-1203,0,0.231948,"rom Devlin et al. (2019). The overall pre-training loss is the sum of the dEA, MLM and NSP loss. to fine-tune ERNIE for relation classification is to apply the pooling layer to the final output embeddings of the given entity mentions, and represent the given entity pair with the concatenation of their mention embeddings for classification. In this paper, we design another method, which modifies the input token sequence by adding two mark tokens to highlight entity mentions. These extra mark tokens play a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015). Then, we also take the [CLS] token embedding for classification. Note that we design different tokens [HD] and [TL] for head entities and tail entities respectively. The specific fine-tuning procedure for entity typing is a simplified version of relation classification. As previous typing models make full use of both context embeddings and entity mention embeddings (Shimaoka et al., 2016; Yaghoobzadeh and Sch¨utze, 2017; Xin et al., 2018), we argue that the modified input sequence with the mention mark token [ENT] can guide ERNIE to combine both context information and entity mention informa"
P19-1139,D18-1244,0,0.0362932,"n classification: CNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to these three baselines, we also finetune BERT with the same input format introduced in Section 3.5 for fair comparison. 1447 Model MNLI-(m/mm) 392k QQP 363k QNLI 104k SST-2 67k BERTBASE 84.6/83.4 71.2 - 93.5 ERNIE 84.0/83.2 71.2 91.3 93.5 Model CoLA 8.5k STS-B 5.7k MRPC 3.5k RTE 2.5k BERTB"
P19-1139,D17-1004,0,0.0498373,"labels more precisely. In summary, ERNIE effectively reduces the noisy label challenge in FIGER, which is a widely-used distantly supervised entity typing dataset, by injecting the information from KGs. Besides, ERNIE also outperforms the baselines on Open Entity which has gold annotations. Relation Classification Relation classification aims to determine the correct relation between two entities in a given sentence, which is an important knowledge-driven NLP task. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FewRel (Han et al., 2018c) and TACRED (Zhang et al., 2017). The statistics of these two datasets are shown in Table 4. As the original experimental setting of FewRel is few-shot learning, we rearrange the FewRel dataset for the common relation classification setting. Specifically, we sample 100 instances from each class for the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation “no relation”) in TACRED. We compare our model with the following baseline models for relation classification: CNN. With a convolution layer, a max-pooling lay"
P19-1139,W07-1401,0,\N,Missing
P19-1278,D15-1082,1,0.727498,"Missing"
P19-1278,N13-1008,0,0.0278094,"on of KBs (Suchanek et al., 2007; Bollacker et al., 2008; Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Banko et al., 2007; Zhu et al., 2009; Etzioni et al., 2011; Saha et al., 2017) and relation extraction (Riedel et al., 2013; Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Zeng et al., 2015; Lin et al., 2016), and relation prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b,a; Xie et al., 2016). For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so"
P19-1278,S12-1055,0,0.0259867,", researchers have empirically found there are various different categorizations of semantic relations among words and contexts. For promoting research on these different semantic relations, Bejar et al. (1991) explicitly defining these relations and Miller (1995) further systematically organize rich semantic relations between words via a database. For identifying correlation and distinction between different semantic relations so as to support learning semantic similarity, various methods have attempted to measure relational similarity (Turney, 2005, 2006; Zhila et al., 2013; Pedersen, 2012; Rink and Harabagiu, 2012; Mikolov et al., 2013b,a). 2889 With the ongoing development of information extraction and effective construction of KBs (Suchanek et al., 2007; Bollacker et al., 2008; Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy"
P19-1278,P17-2050,0,0.0462859,"Missing"
P19-1278,P16-1200,1,0.832915,"er defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Banko et al., 2007; Zhu et al., 2009; Etzioni et al., 2011; Saha et al., 2017) and relation extraction (Riedel et al., 2013; Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Zeng et al., 2015; Lin et al., 2016), and relation prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b,a; Xie et al., 2016). For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between similar relations, and merge those distant surface forms o"
P19-1278,P15-1061,0,0.0277445,"Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Banko et al., 2007; Zhu et al., 2009; Etzioni et al., 2011; Saha et al., 2017) and relation extraction (Riedel et al., 2013; Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Zeng et al., 2015; Lin et al., 2016), and relation prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b,a; Xie et al., 2016). For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between similar relations, a"
P19-1278,Q16-1017,0,0.018265,"ever, these extractors only yield relation patterns between entities, without aggregating and clustering their results. Accordingly, there are a fair amount of redundant relation patterns after extracting those relation patterns. Furthermore, the redundant patterns lead to |E| #Fact 112,946 194,556 14,951 29,943 426,067 266,645 483,142 68,124 Section §5 and §6.1 §6.2 §7.1 and §8 §7.2 and §9 Table 3: Statistics of the triple sets used in this paper. some redundant relations in KBs. Recently, some efforts are devoted to Open Relation Extraction (Open RE) (Lin and Pantel, 2001; Yao et al., 2011; Marcheggiani and Titov, 2016; ElSahar et al., 2017), aiming to cluster relation patterns into several relation types instead of redundant relation patterns. Whenas, these Open RE methods adopt distantly supervised labels as golden relation types, suffering from both false positive and false negative problems on the one hand. On the other hand, these methods still rely on the conventional similarity metrics mentioned above. In this section, we will show that our defined similarity quantification could help Open IE by identifying redundant relations. To be specific, we set a toy experiment to remove redundant relations in"
P19-1278,J06-3003,0,0.156138,"Missing"
P19-1278,S12-1070,0,0.033295,"1; Resnik, 1999), researchers have empirically found there are various different categorizations of semantic relations among words and contexts. For promoting research on these different semantic relations, Bejar et al. (1991) explicitly defining these relations and Miller (1995) further systematically organize rich semantic relations between words via a database. For identifying correlation and distinction between different semantic relations so as to support learning semantic similarity, various methods have attempted to measure relational similarity (Turney, 2005, 2006; Zhila et al., 2013; Pedersen, 2012; Rink and Harabagiu, 2012; Mikolov et al., 2013b,a). 2889 With the ongoing development of information extraction and effective construction of KBs (Suchanek et al., 2007; Bollacker et al., 2008; Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2"
P19-1278,D15-1206,0,0.0592151,"Missing"
P19-1278,D11-1135,0,0.0282773,"et al., 2015). However, these extractors only yield relation patterns between entities, without aggregating and clustering their results. Accordingly, there are a fair amount of redundant relation patterns after extracting those relation patterns. Furthermore, the redundant patterns lead to |E| #Fact 112,946 194,556 14,951 29,943 426,067 266,645 483,142 68,124 Section §5 and §6.1 §6.2 §7.1 and §8 §7.2 and §9 Table 3: Statistics of the triple sets used in this paper. some redundant relations in KBs. Recently, some efforts are devoted to Open Relation Extraction (Open RE) (Lin and Pantel, 2001; Yao et al., 2011; Marcheggiani and Titov, 2016; ElSahar et al., 2017), aiming to cluster relation patterns into several relation types instead of redundant relation patterns. Whenas, these Open RE methods adopt distantly supervised labels as golden relation types, suffering from both false positive and false negative problems on the one hand. On the other hand, these methods still rely on the conventional similarity metrics mentioned above. In this section, we will show that our defined similarity quantification could help Open IE by identifying redundant relations. To be specific, we set a toy experiment to"
P19-1278,N07-4013,0,0.0157421,". On the other hand, our model shows a stronger correlation (0.63) with human judgment, indicating that considering the probability over whole entity pair space helps to gain a similarity closer to human judgments. These results provide evidence for our claim raised in §3.2. 6 Wikidata ReVerb Extractions FB15K TACRED Open IE extracts concise token patterns from plain text to represent various relations between entities, e.g.„ (Mark Twain, was born in, Florida). As Open IE is significant for constructing KBs, many effective extractors have been proposed to extract triples, such as Text-Runner (Yates et al., 2007), ReVerb (Fader et al., 2011), and Standford Open IE (Angeli et al., 2015). However, these extractors only yield relation patterns between entities, without aggregating and clustering their results. Accordingly, there are a fair amount of redundant relation patterns after extracting those relation patterns. Furthermore, the redundant patterns lead to |E| #Fact 112,946 194,556 14,951 29,943 426,067 266,645 483,142 68,124 Section §5 and §6.1 §6.2 §7.1 and §8 §7.2 and §9 Table 3: Statistics of the triple sets used in this paper. some redundant relations in KBs. Recently, some efforts are devoted"
P19-1278,D15-1203,0,0.0346458,"relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Banko et al., 2007; Zhu et al., 2009; Etzioni et al., 2011; Saha et al., 2017) and relation extraction (Riedel et al., 2013; Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Zeng et al., 2015; Lin et al., 2016), and relation prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b,a; Xie et al., 2016). For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between similar relations, and merge those dist"
P19-1278,C14-1220,0,0.0489211,"cker et al., 2008; Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Banko et al., 2007; Zhu et al., 2009; Etzioni et al., 2011; Saha et al., 2017) and relation extraction (Riedel et al., 2013; Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Zeng et al., 2015; Lin et al., 2016), and relation prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b,a; Xie et al., 2016). For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between"
P19-1278,D17-1004,0,0.316785,"11) is a program that automatically identifies and extracts binary relationships from English sentences. We use the extractions from running ReVerb on Wikipedia9. We only keep the relations appear more than 10 times and their corresponding triples to construct our dataset. 4.1 4.3 4 Dataset Construction Wikidata In Wikidata (Vrandeˇci´c and Krötzsch, 2014), facts can be described as (Head item/property, Property, Tail item/property). To construct a dataset suitable for our task, we only consider the facts whose head FB15K and TACRED FB15K (Bordes et al., 2013) is a subset of freebase. TACRED (Zhang et al., 2017) is a large supervised relation extraction dataset obtained via crowdsourcing. We directly use these two dataset, no extra processing steps were applied. 8Embeddings used in this graph are from a trained TransE model. 2885 9http://reverb.cs.washington.edu/ 5 Triple Set Human Judgments Following Miller and Charles (1991); Resnik (1999) and the vast amount of previous work on semantic similarity, we ask nine undergraduate subjects to assess the similarity of 360 pairs of relations from a subset of Wikidata (Vrandeˇci´c and Krötzsch, 2014)10 that are chosen to cover from high to low levels of sim"
P19-1278,N13-1120,0,0.0183395,"ler and Charles, 1991; Resnik, 1999), researchers have empirically found there are various different categorizations of semantic relations among words and contexts. For promoting research on these different semantic relations, Bejar et al. (1991) explicitly defining these relations and Miller (1995) further systematically organize rich semantic relations between words via a database. For identifying correlation and distinction between different semantic relations so as to support learning semantic similarity, various methods have attempted to measure relational similarity (Turney, 2005, 2006; Zhila et al., 2013; Pedersen, 2012; Rink and Harabagiu, 2012; Mikolov et al., 2013b,a). 2889 With the ongoing development of information extraction and effective construction of KBs (Suchanek et al., 2007; Bollacker et al., 2008; Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtei"
S15-2111,S13-2053,0,0.0815975,"ment analysis module: creates a SVM classifier that incorporates the above features classify the polarity of each tweet. Finally, Twitter-OpinMiner outputs the polarity of each tweet. 2.2 Development Data and Lexicon The development data are necessary in our system. We fully utilize the training tweets provided by SemEval 2013. The dataset consists of 9,912 annotated tweets. Besides, for sentiment analysis, we also utilize several sentiment lexicons, including Liu’s sentiment lexicon (Liu, 2012), MPQA subjectivity lexicon (Wilson et al., 2005), and the sentiment lexicon generated from tweets (Mohammad et al., 2013). 665 Feature Extraction The objective of this task is to determine whether a given message is positive, negative, or neutral. We train sentiment classifiers with LibLinear (Fan et al., 2008) on the training set and dev set, and tune parameter −c, −wi of SVM on the test set of SemEval 2013. SVM is a popular machine learning algorithm, the effectiveness of which has been proved in sentiment analysis on formal texts in related work (Pang and Lee, 2002; Liu, 2012). Since the performance of SVM classifier will be greatly influenced by the features selection, we explore a variety of features in the"
S15-2111,S13-2052,0,0.043413,"eliminate polarity ambiguities in compound sentences where both positive and negative sentiments are appearing. Based on the SemEval 2014 and SemEval 2015 Twitter sentiment analysis task datasets, the experimental results show that the performance of Twitter-OpinMiner could effectively recognize opinionated messages and identify the polarities. 1 Introduction This year comes the third edition of SemEval Twitter sentiment analysis task consisting of new genres, including topic-based polarity classification, trends detection towards a topic, and the sentimental strength of association of terms (Nakov et al., 2013). Corresponding We only participated in the subtask of message sentiment analysis and built up a system, named Twitter-OpinMiner for the task. TwitterOpinMiner stems from two different angles: LDAbased topic detection for discovering the opinionated features of trending tweets’ topics and sentiment analysis based on a variety of features.  Topic detection Recent studies show that people often search Twitter to find temporally relevant information (Teevan et al., 2011), such as emergent events, trending topics. In fact, similar opinions were likely to express on the same topic/event in Twitte"
S15-2111,W02-1011,0,0.0164539,"Missing"
S15-2111,P14-1146,0,0.0195006,"ether the last token contains an exclamation or question mark; • Emoticons: the presence of positive and negative emoticons at any position in the tweet; whether the last token is an emoticon; • OOV: the ratio of words out of vocabulary; • Elongated words: the presence of sentiment words with one character repeated more than two times, for example, ‘cooool’; • URL: whether the tweet contains a URL. • Reply or Retweet: Is the current tweet a reply/retweet tweet 3.4 Word embedding We also utilize word embedding technique for feature extraction. We adopt sentiment-specific word embedding method (Tang et al., 2014) that could encode sentiment information in the continuous representation of words. In our approach, each term is extended into a 150 dimensional vector. 3.5 Discourse specific feature Since tweets are usually expressed informally, there are many compound sentences in a tweet, which always contain positive sentiment and negative sentiment with ambiguity. For example, It may not be the biggest squad in the last 10yrs, but Ancelotti is working for quality over quantity. Everyone... http://t.co/oCdPXQWggT. 666 In this case, there are two segments in the tweet that holds a Contrast discourse relat"
S15-2111,H05-1044,0,0.0478955,"res, sentiment distribution among topics, and word embedding. (3) Sentiment analysis module: creates a SVM classifier that incorporates the above features classify the polarity of each tweet. Finally, Twitter-OpinMiner outputs the polarity of each tweet. 2.2 Development Data and Lexicon The development data are necessary in our system. We fully utilize the training tweets provided by SemEval 2013. The dataset consists of 9,912 annotated tweets. Besides, for sentiment analysis, we also utilize several sentiment lexicons, including Liu’s sentiment lexicon (Liu, 2012), MPQA subjectivity lexicon (Wilson et al., 2005), and the sentiment lexicon generated from tweets (Mohammad et al., 2013). 665 Feature Extraction The objective of this task is to determine whether a given message is positive, negative, or neutral. We train sentiment classifiers with LibLinear (Fan et al., 2008) on the training set and dev set, and tune parameter −c, −wi of SVM on the test set of SemEval 2013. SVM is a popular machine learning algorithm, the effectiveness of which has been proved in sentiment analysis on formal texts in related work (Pang and Lee, 2002; Liu, 2012). Since the performance of SVM classifier will be greatly infl"
S15-2111,D11-1015,1,0.775819,"rking for quality over quantity. Everyone... http://t.co/oCdPXQWggT. 666 In this case, there are two segments in the tweet that holds a Contrast discourse relation, and the polarity is determined by “but” segment. In our system, we also take into consideration of intrasentence discourse relation features for processing compound sentences. Mann and Thompson (1988) defined a complete discourse scheme Rhetorical Structure Theory (RST). Since not all of the discourse relations in RST would help eliminate polarity ambiguities, the discourse relations were implemented in our system was on a subset (Zhou et al., 2011). In our system, we use cue-phrase based method for discourse relation identification. We maintain a cue phrase lexicon and the examples of the cue phrases were shown in Table 2. 4 Experiment We trained a SVM classifier on 9,912 annotated tweets (8,258 in the training set and 1,654 in the development set). We used the same evaluation metrics with SemEval 2013, including the macroaveraged F-score of the positive and negative classes. The experimental results obtained by our system on the training set (ten-fold cross validation), development set, and test sets on Twitter 2013 were shown in Table"
W13-2007,W13-2003,0,0.107565,"Missing"
W13-2007,W09-1401,0,0.0705542,"Rebholz-srv/GRO/GRO.html 54 tology in event extraction can be an interesting future work. entity strings, the distance between them, and the shortest path between the two entities in the dependency structure of the source sentence, which is identified by Enju parser (Sagae et al., 2007). 3.2 Table 6. Evaluation results (percentage) Evaluation scheme Strict matching Approximate boundary matching Approximate recursive matching Allowing parents Allowing grandparents Evaluation criteria The GRO task follows some of the evaluation criteria of the Genia Event Extraction (GE) task of BioNLP-ST 2009 (Kim et al., 2009), including strict and approximate matching, and also introduce new criteria that consider 1) the hierarchical structure of the GRO and 2) parent and/or grandparent of answer concept. We here explain these new criteria in detail. 1) In this scheme of evaluation, the event results of a participant are classified into the GRO concepts at the third level (see Table 4 for examples), which are ancestors of their labeled classes, and the evaluation results are accumulated for each of those concepts at the third level. This scheme may give us insights on which categories the participant system shows"
W13-2007,W13-2001,1,0.869399,"Missing"
W13-2007,P07-1079,0,0.0156029,"eRegulation NegativeRegulation Occurrent/Process/MolecularProcess IntraCellularProcess Occurrent/Process/PhysiologicalProcess OrganismalProcess Occurrent/Process/PhysicalInteraction Binding Occurrent/Process/Mutation Occurrent/Process/Localization 16 73 64 20 54 Count 782 217 186 422 189 418 143 312 296 82 77 http://www.ebi.ac.uk/Rebholz-srv/GRO/GRO.html 54 tology in event extraction can be an interesting future work. entity strings, the distance between them, and the shortest path between the two entities in the dependency structure of the source sentence, which is identified by Enju parser (Sagae et al., 2007). 3.2 Table 6. Evaluation results (percentage) Evaluation scheme Strict matching Approximate boundary matching Approximate recursive matching Allowing parents Allowing grandparents Evaluation criteria The GRO task follows some of the evaluation criteria of the Genia Event Extraction (GE) task of BioNLP-ST 2009 (Kim et al., 2009), including strict and approximate matching, and also introduce new criteria that consider 1) the hierarchical structure of the GRO and 2) parent and/or grandparent of answer concept. We here explain these new criteria in detail. 1) In this scheme of evaluation, the eve"
W13-2007,E12-2021,0,\N,Missing
