2021.findings-acl.360,{A} Formidable Ability: {D}etecting Adjectival Extremeness with {DSM}s,2021,-1,-1,3,0,8357,farhan samir,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2020.nlpcovid19-acl.13,Exploration of Gender Differences in {COVID-19} Discourse on {R}eddit,2020,-1,-1,3,0,16193,jai aggarwal,Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020,0,"Decades of research on differences in the language of men and women have established postulates about the nature of lexical, topical, and emotional preferences between the two genders, along with their sociological underpinnings. Using a novel dataset of male and female linguistic productions collected from the Reddit discussion platform, we further confirm existing assumptions about gender-linked affective distinctions, and demonstrate that these distinctions are amplified in social media postings involving emotionally-charged discourse related to COVID-19. Our analysis also confirms considerable differences in topical preferences between male and female authors in pandemic-related discussions."
2020.coling-main.454,Pick a Fight or Bite your Tongue: Investigation of Gender Differences in Idiomatic Language Usage,2020,-1,-1,3,1,8815,ella rabinovich,Proceedings of the 28th International Conference on Computational Linguistics,0,"A large body of research on gender-linked language has established foundations regarding cross-gender differences in lexical, emotional, and topical preferences, along with their sociological underpinnings. We compile a novel, large and diverse corpus of spontaneous linguistic productions annotated with speakers{'} gender, and perform a first large-scale empirical study of distinctions in the usage of figurative language between male and female authors. Our analyses suggest that (1) idiomatic choices reflect gender-specific lexical and semantic preferences in general language, (2) men{'}s and women{'}s idiomatic usages express higher emotion than their literal language, with detectable, albeit more subtle, differences between male and female authors along the dimension of dominance compared to similar distinctions in their literal utterances, and (3) contextual analysis of idiomatic expressions reveals considerable differences, reflecting subtle divergences in usage environments, shaped by cross-gender communication styles and semantic biases."
K19-1008,Say Anything: Automatic Semantic Infelicity Detection in {L}2 {E}nglish Indefinite Pronouns,2019,27,0,4,1,8815,ella rabinovich,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"Computational research on error detection in second language speakers has mainly addressed clear grammatical anomalies typical to learners at the beginner-to-intermediate level. We focus instead on acquisition of subtle semantic nuances of English indefinite pronouns by non-native speakers at varying levels of proficiency. We first lay out theoretical, linguistically motivated hypotheses, and supporting empirical evidence, on the nature of the challenges posed by indefinite pronouns to English learners. We then suggest and evaluate an automatic approach for detection of atypical usage patterns, demonstrating that deep learning architectures are promising for this task involving nuanced semantic anomalies."
D19-5558,{C}ode{S}witch-{R}eddit: Exploration of Written Multilingual Discourse in Online Discussion Forums,2019,0,0,3,1,8815,ella rabinovich,Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),0,"In contrast to many decades of research on oral code-switching, the study of written multilingual productions has only recently enjoyed a surge of interest. Many open questions remain regarding the sociolinguistic underpinnings of written code-switching, and progress has been limited by a lack of suitable resources. We introduce a novel, large, and diverse dataset of written code-switched productions, curated from topical threads of multiple bilingual communities on the Reddit discussion platform, and explore questions that were mainly addressed in the context of spoken language thus far. We investigate whether findings in oral code-switching concerning content and style, as well as speaker proficiency, are carried over into written code-switching in discussion forums. The released dataset can further facilitate a range of research and practical activities."
D19-1484,{C}ode{S}witch-{R}eddit: Exploration of Written Multilingual Discourse in Online Discussion Forums,2019,0,0,3,1,8815,ella rabinovich,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"In contrast to many decades of research on oral code-switching, the study of written multilingual productions has only recently enjoyed a surge of interest. Many open questions remain regarding the sociolinguistic underpinnings of written code-switching, and progress has been limited by a lack of suitable resources. We introduce a novel, large, and diverse dataset of written code-switched productions, curated from topical threads of multiple bilingual communities on the Reddit discussion platform, and explore questions that were mainly addressed in the context of spoken language thus far. We investigate whether findings in oral code-switching concerning content and style, as well as speaker proficiency, are carried over into written code-switching in discussion forums. The released dataset can further facilitate a range of research and practical activities."
W18-0105,Predicting and Explaining Human Semantic Search in a Cognitive Model,2018,0,0,3,0,28697,filip miscevic,Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics ({CMCL} 2018),0,None
W18-0106,Modeling bilingual word associations as connected monolingual networks,2018,24,0,3,0,10703,yevgen matusevych,Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics ({CMCL} 2018),0,None
D16-1010,Comparing Computational Cognitive Models of Generalization in a Language Acquisition Task,2016,29,4,3,1,24405,libby barak,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
W15-2412,"Perceptual, conceptual, and frequency effects on error patterns in {E}nglish color term acquisition",2015,-1,-1,2,1,8358,barend beekhuizen,Proceedings of the Sixth Workshop on Cognitive Aspects of Computational Language Learning,0,None
D15-1207,A Computational Cognitive Model of Novel Word Generalization,2015,22,4,3,1,8252,aida nematzadeh,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"A key challenge in vocabulary acquisition is learning which of the many possible meanings is appropriate for a word. The word generalization problem refers to how children associate a word such as dog with a meaning at the appropriate category level in a taxonomy of objects, such as Dalmatians, dogs, or animals. We present the first computational study of word generalization integrated within a word-learning model. The model simulates child and adult patterns of word generalization in a word-learning task. These patterns arise due to the interaction of type and token frequencies in the input data, an influence often observed in peoplexe2x80x99s generalization of linguistic categories."
W14-2005,Learning Verb Classes in an Incremental Model,2014,21,7,3,1,24405,libby barak,Proceedings of the Fifth Workshop on Cognitive Modeling and Computational Linguistics,0,"The ability of children to generalize over the linguistic input they receive is key to acquiring productive knowledge of verbs. Such generalizations help children extend their learned knowledge of constructions to a novel verb, and use it appropriately in syntactic patterns previously unobserved for that verbxe2x80x94a key factor in language productivity. Computational models can help shed light on the gradual development of more abstract knowledge during verb acquisition. We present an incremental Bayesian model that simultaneously and incrementally learns argument structure constructions and verb classes given naturalistic language input. We show how the distributional properties in the input language influence the formation of generalizations over the constructions and classes."
W14-2006,A Usage-Based Model of Early Grammatical Development,2014,22,3,4,1,8358,barend beekhuizen,Proceedings of the Fifth Workshop on Cognitive Modeling and Computational Linguistics,0,"The representations and processes yielding the limited length and telegraphic style of language production early on in acquisition have received little attention in acquisitional modeling. In this paper, we present a model, starting with minimal linguistic representations, that incrementally builds up an inventory of increasingly long and abstract grammatical representations (formmeaning pairings), in line with the usage-based conception of language acquisition. We explore its performance on a comprehension and a generation task, showing that, over time, the model better understands the processed utterances, generates longer utterances, and better expresses the situation these utterances intend to refer to."
D14-1031,A Cognitive Model of Semantic Network Learning,2014,33,6,3,1,8252,aida nematzadeh,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Child semantic development includes learning the meaning of words as well as the semantic relations among words. A presumed outcome of semantic development is the formation of a semantic network that reflects this knowledge. We present an algorithm for simultaneously learning word meanings and gradually growing a semantic network, which adheres to the cognitive plausibility requirements of incrementality and limited computations. We demonstrate that the semantic connections among words in addition to their context is necessary in forming a semantic network that resembles an adultxe2x80x99s semantic knowledge."
W13-3525,Acquisition of Desires before Beliefs: A Computional Investigation,2013,25,7,3,1,24405,libby barak,Proceedings of the Seventeenth Conference on Computational Natural Language Learning,0,"The acquisition of Belief verbs lags behind the acquisition of Desire verbs in children. Some psycholinguistic theories attribute this lag to conceptual differences between the two classes, while others suggest that syntactic differences are responsible. Through computational experiments, we show that a probabilistic verb learning model exhibits the pattern of acquisition, even though there is no difference in the model in the difficulty of the semantic or syntactic properties of Belief vs. Desire verbs. Our results point to the distributional properties of various verb classes as a potentially important, and heretofore unexplored, factor in the observed developmental lag of Belief verbs."
W12-1701,Modeling the Acquisition of Mental State Verbs,2012,30,11,3,1,24405,libby barak,Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics ({CMCL} 2012),0,"Children acquire mental state verbs (MSVs) much later than other, lower-frequency, words. One factor proposed to contribute to this delay is that children must learn various semantic and syntactic cues that draw attention to the difficult-to-observe mental content of a scene. We develop a novel computational approach that enables us to explore the role of such cues, and show that our model can replicate aspects of the developmental trajectory of MSV acquisition."
W12-1708,"A Computational Model of Memory, Attention, and Word Learning",2012,33,5,3,1,8252,aida nematzadeh,Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics ({CMCL} 2012),0,There is considerable evidence that people generally learn items better when the presentation of items is distributed over a period of time (the spacing effect). We hypothesize that both forgetting and attention to novelty play a role in the spacing effect in word learning. We build an incremental probabilistic computational model of word learning that incorporates a forgetting and attentional mechanism. Our model accounts for experimental results on children as well as several patterns observed in adults.
S12-1014,Unsupervised Disambiguation of Image Captions,2012,16,5,5,0,42596,wesley may,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"Given a set of images with related captions, our goal is to show how visual features can improve the accuracy of unsupervised word sense disambiguation when the textual context is very small, as this sort of data is common in news and social media. We extend previous work in unsupervised text-only disambiguation with methods that integrate text and images. We construct a corpus by using Amazon Mechanical Turk to caption sense-tagged images gathered from ImageNet. Using a Yarowsky-inspired algorithm, we show that gains can be made over text-only disambiguation, as well as multimodal approaches such as Latent Dirichlet Allocation."
W11-0910,Incorporating Coercive Constructions into a Verb Lexicon,2011,22,5,6,0,5184,claire bonial,Proceedings of the {ACL} 2011 Workshop on Relational Models of Semantics,0,"We take the first steps towards augmenting a lexical resource, VerbNet, with probabilistic information about coercive constructions. We focus on Causedmotion as an example construction occurring with verbs for which it is a typical usage or for which it must be interpreted as extending the event semantics through coercion, which occurs productively and adds substantially to the relational semantics of a verb. However, through annotation we find that VerbNet fails to accurately capture all usages of the construction. We use unsupervised methods to estimate probabilistic measures from corpus data for predicting usage of the construction across verb classes in the lexicon and evaluate against VerbNet. We discuss how these methods will form the basis for enhancements for VerbNet supporting more accurate analysis of the relational semantics of a verb across productive usages."
W10-2109,No Sentence Is Too Confusing To Ignore,2010,17,0,2,1,1013,paul cook,Proceedings of the 2010 Workshop on {NLP} and Linguistics: Finding the Common Ground,0,"We consider sentences of the form No X is too Y to Z, in which X is a noun phrase, Y is an adjective phrase, and Z is a verb phrase. Such constructions are ambiguous, with two possible (and opposite!) interpretations, roughly meaning either that Every X Zs, or that No X Zs. The interpretations have been noted to depend on semantic and pragmatic factors. We show here that automatic disambiguation of this pragmatically complex construction can be largely achieved by using features of the lexical semantic properties of the verb (i.e., Z) participating in the construction. We discuss our experimental findings in the context of construction grammar, which suggests a possible account of this phenomenon."
cook-stevenson-2010-automatically,Automatically Identifying Changes in the Semantic Orientation of Words,2010,22,36,2,1,1013,paul cook,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The meanings of words are not fixed but in fact undergo change, with new word senses arising and established senses taking on new aspects of meaning or falling out of usage. Two types of semantic change are amelioration and pejoration; in these processes a word sense changes to become more positive or negative, respectively. In this first computational study of amelioration and pejoration we adapt a web-based method for determining semantic orientation to the task of identifying ameliorations and pejorations in corpora from differing time periods. We evaluate our proposed method on a small dataset of known historical ameliorations and pejorations, and find it to perform better than a random baseline. Since this test dataset is small, we conduct a further evaluation on artificial examples of amelioration and pejoration, and again find evidence that our proposed method is able to identify changes in semantic orientation. Finally, we conduct a preliminary evaluation in which we apply our methods to the task of finding words which have recently undergone amelioration or pejoration."
J10-1002,A Graph-Theoretic Framework for Semantic Distance,2010,62,18,2,1,37093,vivian tsang,Computational Linguistics,0,"Many NLP applications entail that texts are classified based on their semantic distance (how similar or different the texts are). For example, comparing the text of a new document to that of documents of known topics can help identify the topic of the new text. Typically, a distributional distance is used to capture the implicit semantic distance between two pieces of text. However, such approaches do not take into account the semantic relations between words. In this article, we introduce an alternative method of measuring the semantic distance between texts that integrates distributional information and ontological knowledge within a network flow formalism. We first represent each text as a collection of frequency-weighted concepts within an ontology. We then make use of a network flow method which provides an efficient way of explicitly measuring the frequency-weighted ontological distance between the concepts across two texts. We evaluate our method in a variety of NLP tasks, and find that it performs well on two of three tasks. We develop a new measure of semantic coherence that enables us to account for the performance difference across the three data sets, shedding light on the properties of a data set that lends itself well to our method."
J10-1005,Automatically Identifying the Source Words of Lexical Blends in {E}nglish,2010,34,18,2,1,1013,paul cook,Computational Linguistics,0,"Newly coined words pose problems for natural language processing systems because they are not in a system's lexicon, and therefore no lexical information is available for such words. A common way to form new words is lexical blending, as in cosmeceutical, a blend of cosmetic and pharmaceutical. We propose a statistical model for inferring a blend's source words drawing on observed linguistic properties of blends; these properties are largely based on the recognizability of the source words in a blend. We annotate a set of 1,186 recently coined expressions which includes 515 blends, and evaluate our methods on a 324-item subset. In this first study of novel blends we achieve an accuracy of 40% on the task of inferring a blend's source words, which corresponds to a reduction in error rate of 39% over an informed baseline. We also give preliminary results showing that our features for source word identification can be used to distinguish blends from other kinds of novel words."
W09-2010,An Unsupervised Model for Text Message Normalization,2009,13,110,2,1,1013,paul cook,Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,0,"Cell phone text messaging users express themselves briefly and colloquially using a variety of creative forms. We analyze a sample of creative, non-standard text message word forms to determine frequent word formation processes in texting language. Drawing on these observations, we construct an unsupervised noisy-channel model for text message normalization. On a test set of 303 text message forms that differ from their standard form, our model achieves 59% accuracy, which is on par with the best supervised results reported on this dataset."
J09-1005,Unsupervised Type and Token Identification of Idiomatic Expressions,2009,60,95,3,1,10736,afsaneh fazly,Computational Linguistics,0,"Idiomatic expressions are plentiful in everyday language, yet they remain mysterious, as it is not clear exactly how people learn and understand them. They are of special interest to linguists, psycholinguists, and lexicographers, mainly because of their syntactic and semantic idiosyncrasies as well as their unclear lexical status. Despite a great deal of research on the properties of idioms in the linguistics literature, there is not much agreement on which properties are characteristic of these expressions. Because of their peculiarities, idiomatic expressions have mostly been overlooked by researchers in computational linguistics. In this article, we look into the usefulness of some of the identified linguistic properties of idioms for their automatic recognition. Specifically, we develop statistical measures that each model a specific property of idiomatic expressions by looking at their actual usage patterns in text. We use these statistical measures in a type-based classification task where we automatically separate idiomatic expressions (expressions with a possible idiomatic interpretation) from similar-on-the-surface literal phrases (for which no idiomatic interpretation is possible). In addition, we use some of the measures in a token identification task where we distinguish idiomatic and literal usages of potentially idiomatic expressions in context."
W08-2108,Fast Mapping in Word Learning: What Probabilities Tell Us,2008,20,13,3,1,12090,afra alishahi,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"Children can determine the meaning of a new word from hearing it used in a familiar context---an ability often referred to as fast mapping. In this paper, we study fast mapping in the context of a general probabilistic model of word learning. We use our model to simulate fast mapping experiments on children, such as referent selection and retention. The word learning model can perform these tasks through an inductive interpretation of the acquired probabilities. Our results suggest that fast mapping occurs as a natural consequence of learning more words, and provides explanations for the (occasionally contradictory) child experimental data."
W08-2112,An Incremental {B}ayesian Model for Learning Syntactic Categories,2008,17,33,3,0,44373,christopher parisien,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"We present an incremental Bayesian model for the unsupervised learning of syntactic categories from raw text. The model draws information from the distributional cues of words within an utterance, while explicitly bootstrapping its development on its own partially-learned knowledge of syntactic categories. Testing our model on actual child-directed data, we demonstrate that it is robust to noise, learns reasonable categories, manages lexical ambiguity, and in general shows learning behaviours similar to those observed in children."
J08-2001,Special Issue Introduction: Semantic Role Labeling: An Introduction to the Special Issue,2008,48,167,4,0,25372,lluis marquez,Computational Linguistics,0,"Semantic role labeling, the computational identification and labeling of arguments in text, has become a leading task in computational linguistics today. Although the issues for this task have been studied for decades, the availability of large resources and the development of statistical machine learning methods have heightened the amount of effort in this field. This special issue presents selected and representative work in the field. This overview describes linguistic background of the problem, the movement from linguistic theories to computational practice, the major resources that are being used, an overview of steps taken in computational systems, and a description of the key issues and results in semantic role labeling (as revealed in several international evaluations). We assess weaknesses in semantic role labeling and identify important challenges facing the field. Overall, the opportunities and the potential for useful further research in semantic role labeling are considerable."
W07-1102,Distinguishing Subtypes of Multiword Expressions Using Linguistically-Motivated Statistical Measures,2007,23,57,2,1,10736,afsaneh fazly,Proceedings of the Workshop on A Broader Perspective on Multiword Expressions,0,"We identify several classes of multiword expressions that each require a different encoding in a (computational) lexicon, as well as a different treatment within a computational system. We examine linguistic properties pertaining to the degree of semantic idiosyncrasy of these classes of expressions. Accordingly, we propose statistical measures to quantify each property, and use the measures to automatically distinguish the classes."
W07-1106,Pulling their Weight: Exploiting Syntactic Forms for the Automatic Identification of Idiomatic Expressions in Context,2007,16,70,3,1,1013,paul cook,Proceedings of the Workshop on A Broader Perspective on Multiword Expressions,0,"Much work on idioms has focused on type identification, i.e., determining whether a sequence of words can form an idiomatic expression. Since an idiom type often has a literal interpretation as well, token classification of potential idioms in context is critical for NLP. We explore the use of informative prior knowledge about the overall syntactic behaviour of a potentially-idiomatic expression (type-based knowledge) to determine whether an instance of the expression is used idiomatically or literally (token-based knowledge). We develop unsupervised methods for the task, and show that their performance is comparable to that of state-of-the-art supervised techniques."
W07-0606,A Cognitive Model for the Representation and Acquisition of Verb Selectional Preferences,2007,16,6,2,1,12090,afra alishahi,Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition,0,"We present a cognitive model of inducing verb selectional preferences from individual verb usages. The selectional preferences for each verb argument are represented as a probability distribution over the set of semantic properties that the argument can possess---a semantic profile. The semantic profiles yield verb-specific conceptualizations of the arguments associated with a syntactic position. The proposed model can learn appropriate verb profiles from a small set of noisy training data, and can use them in simulating human plausibility judgments and analyzing implicit object alternation."
W06-3815,Context Comparison as a Minimum Cost Flow Problem,2006,13,5,2,1,37093,vivian tsang,Proceedings of {T}ext{G}raphs: the First Workshop on Graph Based Methods for Natural Language Processing,0,"Comparing word contexts is a key component of many NLP tasks, but rarely is it used in conjunction with additional ontological knowledge. One problem is that the amount of overhead required can be high. In this paper, we provide a graphical method which easily combines an ontology with contextual information. We take advantage of the intrinsic graphical structure of an ontology for representing a context. In addition, we turn the ontology into a metric space, such that subgraphs within it, which represent contexts, can be compared. We develop two variants of our graphical method for comparing contexts. Our analysis indicates that our method performs the comparison efficiently and offers a competitive alternative to non-graphical methods."
W06-1207,Classifying Particle Semantics in {E}nglish Verb-Particle Constructions,2006,24,27,2,1,1013,paul cook,Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties,0,"Previous computational work on learning the semantic properties of verb-particle constructions (VPCs) has focused on their compositionality, and has left unaddressed the issue of which meaning of the component words is being used in a given VPC. We develop a feature space for use in classification of the sense contributed by the particle in a VPC, and test this on VPCs using the particle up. The features that capture linguistic properties of VPCs that are relevant to the semantics of the particle outperform linguistically uninformed word co-occurrence features in our experiments on unseen test VPCs."
E06-1043,Automatically Constructing a Lexicon of Verb Phrase Idiomatic Combinations,2006,24,78,2,1,10736,afsaneh fazly,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We investigate the lexical and syntactic flexibility of a class of idiomatic expressions. We develop measures that draw on such linguistic properties, and demonstrate that these statistical, corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones. We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation."
Y05-1003,Automatic Acquisition of Knowledge About Multiword Predicates,2005,41,4,2,1,10736,afsaneh fazly,"Proceedings of the 19th Pacific Asia Conference on Language, Information and Computation",0,"Human interpretation of natural language relies heavily on cognitive processes involving metaphorical and idiomatic meanings. One area of computational linguistics in which such processes play an important, but largely unaddressed, role is the determination of the properties of multiword predicates (MWPs). MWPs such as give a groan and cut taxes involve metaphorical meaning extensions of highly frequent, and highly polysemous, verbs. Tools for automatically identifying such MWPs, and extracting their lexical and syntactic properties, are crucial to the adequate treatment of text in a computational system, due to the productive nature of MWPs across many languages. This paper gives an overview of our work addressing these issues. We begin by relating linguistic properties of metaphorical uses of verbs to their distributional properties. We devise automatic methods for assessing whether a verb phrase is literal, metaphorical, or idiomatic. Since metaphorical MWPs are generally semi-productive, we also develop computational measures of their individual acceptability and of their productivity over semantically related combinations. Our results demonstrate that combining statistical approaches with linguistic information is beneficial, both for the acquisition of knowledge about metaphorical and idiomatic MWPs, and for the organization of such knowledge in a computational lexicon. 1. Metaphorical Multiword Predicates Metaphor is a powerful aspect of language, enabling creative expression in terms of familiar concepts, usually ones which are easily visualizable (Lakoff and Johnson, 1980; Johnson, 1987; Nunberg et al., 1994). Indeed, metaphor is such a central part of linguistic competence that many terms, especially multiword expressions, that are currently accepted as xe2x80x9cregularxe2x80x9d language have their origin in metaphorical uses (Newman, 1996). Some of these expressions are viewed as meaning extensions of their component words, which at least partly contribute their semantics, or a figurative version of their semantics. Others have become idioms with idiosyncratic semantics whose relation to their component words is not obvious (except possibly historically). In particular, it is common across languages for multiword predicates (MWPs) to form around certain high frequency verbs that easily undergo a process of metaphorization (Pauwels, 2000; Newman and Rice, 2004). In their literal uses, these so-called xe2x80x9cbasicxe2x80x9d verbs typically refer to states or acts that are central to human experience (e.g., cut, give, put, sit). Their metaphorical uses yield a range of meaning extensions, exhibited in MWPs such as those in 1(axe2x80x93d): Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation"
W05-1005,Automatically Distinguishing Literal and Figurative Usages of Highly Polysemous Verbs,2005,21,12,3,1,10736,afsaneh fazly,Proceedings of the {ACL}-{SIGLEX} Workshop on Deep Lexical Acquisition,0,"We investigate the meaning extensions of very frequent and highly polysemous verbs, both in terms of their compositional contribution to a light verb construction (LVC), and the patterns of acceptability of the resulting LVC. We develop compositionality and acceptability measures that draw on linguistic properties specific to LVCs, and demonstrate that these statistical, corpus-based measures correlate well with human judgments of each property."
W05-0510,The Acquisition and Use of Argument Structure Constructions: A {B}ayesian Model,2005,15,6,2,1,12090,afra alishahi,Proceedings of the Workshop on Psychocomputational Models of Human Language Acquisition,0,"We present a Bayesian model for the representation, acquisition and use of argument structure constructions, which is founded on a novel view of constructions as a mapping of a syntactic form to a probability distribution over semantic features. Our computational experiments demonstrate the feasibility of learning general constructions from individual examples of verb usage, and show that the acquired knowledge generalizes to novel or low-frequency situations in language use."
H05-1111,Exploiting a Verb Lexicon in Automatic Semantic Role Labelling,2005,11,30,2,0,51122,robert swier,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We develop an unsupervised semantic role labelling system that relies on the direct application of information in a predicate lexicon combined with a simple probability model. We demonstrate the usefulness of predicate lexicons for role labelling, as well as the feasibility of modifying an existing role-labelled corpus for evaluating a different set of semantic roles. We achieve a substantial improvement over an informed baseline."
W04-3213,Unsupervised Semantic Role Labellin,2004,17,73,2,0,51122,robert swier,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,None
W04-2605,Using Selectional Profile Distance to Detect Verb Alternations,2004,21,6,2,1,37093,vivian tsang,Proceedings of the Computational Lexical Semantics Workshop at {HLT}-{NAACL} 2004,0,"We propose a new method for detecting verb alternations, by comparing the probability distributions over WordNet classes occurring in two potentially alternating argument positions. Existing distance measures compute only the distributional distance, and do not take into account the semantic similarity between WordNet senses across the distributions. Our method compares two probability distributions over WordNet by measuring the semantic distance of the component nodes, weighted by their probability. To incorporate semantic similarity, we calculate the (dis)similarity between two probability distributions as a weighted distance travelled from one to the other through the WordNet hierarchy. We evaluate the measure on the causative alternation, and find that overall it outperforms existing distance measures."
W04-2411,Calculating Semantic Distance between Word Sense Probability Distributions,2004,25,11,2,1,37093,vivian tsang,Proceedings of the Eighth Conference on Computational Natural Language Learning ({C}o{NLL}-2004) at {HLT}-{NAACL} 2004,0,None
W04-0401,Statistical Measures of the Semi-Productivity of Light Verb Constructions,2004,16,49,1,1,8359,suzanne stevenson,Proceedings of the Workshop on Multiword Expressions: Integrating Processing,0,"We propose a statistical measure for the degree of acceptability of light verb constructions, such as take a walk, based on their linguistic properties. Our measure shows good correlations with human ratings on unseen test data. Moreover, we find that our measure correlates more strongly when the potential complements of the construction (such as walk, stroll, or run) are separated into semantically similar classes. Our analysis demonstrates the systematic nature of the semi-productivity of these constructions."
W03-0604,Towards a Framework for Learning Structured Shape Models from Text-Annotated Images,2003,16,13,2,0,35281,sven wachsmuth,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Learning Word Meaning from Non-Linguistic Data,0,"We present on-going work on the topic of learning translation models between image data and text (English) captions. Most approaches to this problem assume a one-to-one or a flat, one-to-many mapping between a segmented image region and a word. However, this assumption is very restrictive from the computer vision standpoint, and fails to account for two important properties of image segmentation: 1) objects often consist of multiple parts, each captured by an individual region; and 2) individual regions are often over-segmented into multiple subregions. Moreover, this assumption also fails to capture the structural relations among words, e.g., part/whole relations. We outline a general framework that accommodates a many-to-many mapping between image regions and words, allowing for structured descriptions on both sides. In this paper, we describe our extensions to the probabilistic translation model of Brown et al. (1993) (as in Duygulu et al. (2002)) that enable the creation of structured models of image objects. We demonstrate our work in progress, in which a set of annotated images is used to derive a set of labeled, structured descriptions in the presence of oversegmentation."
W03-0410,Semi-supervised Verb Class Discovery Using Noisy Features,2003,18,31,1,1,8359,suzanne stevenson,Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003,0,"We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs. The feature set was previously shown to work well in a supervised learning setting, using known English verb classes. In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task. We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties. We find that the unsupervised method we tried cannot be consistently applied to our data. However, the semi-supervised approach (using a seed set of sample verbs) overall outperforms not only the full set of features, but the hand-selected features as well."
E03-1040,A General Feature Space for Automatic Verb Classification,2003,38,23,2,0,13774,eric joanis,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We develop a general feature space for automatic classification of verbs into lexical semantic classes. Previous work was limited in scope by the need for manual selection of discriminating features, through a linguistic analysis of the target verb classes (Merlo and Stevenson, 2001). We instead analyze the classification structure at a higher level, using the possible defining characteristics of classes as the basis for our feature space. The general feature space achieves reductions in error rates of 42--69%, on a wider range of classes than investigated previously, with comparable performance to feature sets manually selected for the particular classification tasks. Our results show that the approach is generally applicable, and avoids the need for resource-intensive linguistic analysis for each new task."
P02-1027,A Multilingual Paradigm for Automatic Verb Classification,2002,21,37,2,0.664468,3347,paola merlo,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"We demonstrate the benefits of a multilingual approach to automatic lexical semantic verb classification based on statistical analysis of corpora in multiple languages. Our research incorporates two interrelated threads. In one, we exploit the similarities in the crosslinguistic classification of verbs, to extend work on English verb classification to a new language (Italian), and to new classes within that language, achieving an accuracy of 86.4% (baseline 33.9%). Our second strand of research exploits the differences across languages in the syntactic expression of semantic properties, to show that complementary information about English verbs can be extracted from their translations in a second language (Chinese). The use of multilingual features improves classification performance of the English verbs, achieving an accuracy of 83.5% (baseline 33.3%)."
C02-1146,Crosslinguistic Transfer in Automatic Verb Classification,2002,17,8,2,1,37093,vivian tsang,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"We investigate the use of multilingual data in the automatic classification of English verbs, and show that there is a useful transfer of information across languages. Specifically, we experiment with three lexical semantic classes of English verbs. We collect statistical features over a sample of English verbs from each of the classes, as well as over Chinese translations of those verbs. We use the English and Chinese data, alone and in combination, as training data for a machine learning algorithm whose output is an automatic verb classifier. We demonstrate that Chinese data is indeed useful in helping to classify the English verbs (at 82% accuracy), and furthermore that a multilingual combination of data outperforms the English data alone (85% accuracy). Moreover, our results using monolingual corpora show that it is not necessary to use a parallel corpus to extract the translations in order for this technique to be successful."
W01-0705,Automatic verb classification using multilingual resources,2001,16,4,2,1,37093,vivian tsang,Proceedings of the {ACL} 2001 Workshop on Computational Natural Language Learning ({C}on{LL}),0,"We propose the use of multilingual corpora in the automatic classification of verbs. We extend the work of (Merlo and Stevenson, 2001), in which statistics over simple syntactic features extracted from textual corpora were used to train an automatic classifier for three lexical semantic classes of English verbs. We hypothesize that some lexical semantic features that are difficult to detect superficially in English may manifest themselves as easily extractable surface syntactic features in another language. Our experimental results combining English and Chinese features show that a small bilingual corpus may provide a useful alternative to using a large monolingual corpus for verb classification."
J01-3003,Automatic Verb Classification Based on Statistical Distributions of Argument Structure,2001,48,170,2,0.734443,3347,paola merlo,Computational Linguistics,0,"Automatic acquisition of lexical knowledge is critical to a wide range of natural language processing tasks. Especially important is knowledge about verbs, which are the primary source of relational information in a sentence---the predicate-argument structure that relates an action or state to its participants (i.e., who did what to whom). In this work, we report on supervised learning experiments to automatically classify three major types of English verbs, based on their argument structure--specifically, the thematic roles they assign to participants. We use linguistically-motivated statistical indicators extracted from large annotated corpora to train the classifier, achieving 69.8% accuracy for a task whose baseline is 34%, and whose expert-based upper bound we calculate at 86.5%. A detailed analysis of the performance of the algorithm and of its errors confirms that the proposed features capture properties related to the argument structure of the verbs. Our results validate our hypotheses that knowledge about thematic relations is crucial for verb classification, and that it can be gleaned from a corpus by automatic means. We thus demonstrate an effective combination of deeper linguistic knowledge with the robustness and scalability of statistical techniques."
merlo-stevenson-2000-establishing,Establishing the Upper Bound and Inter-judge Agreement of a Verb Classification Task,2000,21,2,2,0.854143,3347,paola merlo,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Detailed knowledge about verbs is critical in many NLP and IR tasks, yet manual determination of such knowledge for large numbers of verbs is difficult, time-consuming and resource intensive. Recent responses to this problem have attempted to classify verbs automatically, as a first step to automatically build lexical resources. In order to estimate the upper bound of a verb classification task, which appears to be difficult and subject to variability among experts, we investigated the performance of human experts in controlled classification experiments. We report here the results of two experimentsxe2x80x94using a forced-choice task and a non-forced choice taskxe2x80x94which measure human expert accuracy (compared to a gold standard) in classifying verbs into three pre-defined classes, as well as inter-expert agreement. To preview, we find that the highest expert accuracy is 86.5% agreement with the gold standard, and that inter-expert agreement is not very high (K between .53 and .66). The two experiments show comparable results."
C00-2118,Automatic Lexical Acquisition Based on Statistical Distributions,2000,14,10,1,1,8359,suzanne stevenson,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"We automatically classify verbs into lexical semantic classes, based on distributions of indicators of verb alternations, extracted from a very large annotated corpus. We address a problem which is particularly difficult because the verb classes, although semantically different, show similar surface syntactic behavior. Five grammatical features are sufficient to reduce error rate by more than 50% over chance: we achieve almost 70% accuracy in a task whose baseline performance is 34%, and whose expert-based upper bound we calculated at 86.5%. We conclude that corpus-driven extraction of grammatical features is a promising methodology for find-grained verb classification."
W99-0503,Supervised Learning of Lexical Semantic Verb Classes Using Frequency Distributions,1999,0,15,1,1,8359,suzanne stevenson,{SIGLEX}99: Standardizing Lexical Resources,0,"Vve zeport a number of computatmnal experiments m supervised learning whose goal Is to automatmally classify a set of verbs into lexmal semanUc classes, based on frequency dls tnbutmn approxlmatmns of grammatical features extracted from a very large annotated corpus DlstnbuUons of five syntactic features that approximate tranmUvlty alternatmns and thematic role assignments are sufficient to reduce error rate by 56% over chance We conclude that corpus da ta is a usable repository of verb class mformatmn, and that corpusdriven extraction of grammaUcal features Is a promising methodology for automatm lexmal acqum,Uon"
E99-1007,Automatic Verb Classification Using Distributions of Grammatical Features,1999,20,32,1,1,8359,suzanne stevenson,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We apply machine learning techniques to classify automatically a set of verbs into lexical semantic classes, based on distributional approximations of diatheses, extracted from a very large annotated corpus. Distributions of four grammatical features are sufficient to reduce error rate by 50% over chance. We conclude that corpus data is a usable repository of verb class information, and that corpus-driven extraction of grammatical features is a promising methodology for automatic lexical acquisition."
W98-1116,What grammars tell us about corpora: the case of reduced relative clauses,1998,10,3,2,0.854143,3347,paola merlo,Sixth Workshop on Very Large Corpora,0,None
J98-4009,Book Reviews: The Architecture of the Language Faculty,1998,-1,-1,1,1,8359,suzanne stevenson,Computational Linguistics,0,None
P93-1036,A Competition-Based Explanation of Syntactic Attachment Preferences and Garden Path Phenomena,1993,12,20,1,1,8359,suzanne stevenson,31st Annual Meeting of the Association for Computational Linguistics,1,"This paper presents a massively parallel parser that predicts critical attachment behaviors of the human sentence processor, without the use of explicit preference heuristics or revision strategies. The processing of a syntactic ambiguity is modeled as an active, distributed competition among the potential attachments for a phrase. Computationally motivated constraints on the competitive mechanism provide a principled and uniform account of a range of human attachment preferences and garden path phenomena."
