2021.wanlp-1.19,Quranic Verses Semantic Relatedness Using {A}ra{BERT},2021,-1,-1,2,0,539,abdullah alsaleh,Proceedings of the Sixth Arabic Natural Language Processing Workshop,0,"Bidirectional Encoder Representations from Transformers (BERT) has gained popularity in recent years producing state-of-the-art performances across Natural Language Processing tasks. In this paper, we used AraBERT language model to classify pairs of verses provided by the QurSim dataset to either be semantically related or not. We have pre-processed The QurSim dataset and formed three datasets for comparisons. Also, we have used both versions of AraBERT, which are AraBERTv02 and AraBERTv2, to recognise which version performs the best with the given datasets. The best results was AraBERTv02 with 92{\%} accuracy score using a dataset comprised of label {`}2{'} and label '-1{'}, the latter was generated outside of QurSim dataset."
2020.lrec-1.415,Constructing a Bilingual Hadith Corpus Using a Segmentation Tool,2020,-1,-1,2,0,17551,shatha altammami,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This article describes the process of gathering and constructing a bilingual parallel corpus of Islamic Hadith, which is the set of narratives reporting different aspects of the prophet Muhammad{'}s life. The corpus data is gathered from the six canonical Hadith collections using a custom segmentation tool that automatically segments and annotates the two Hadith components with 92{\%} accuracy. This Hadith segmenter minimises the costs of language resource creation and produces consistent results independently from previous knowledge and experiences that usually influence human annotators. The corpus includes more than 10M tokens and will be freely available via the LREC repository."
2020.icon-main.4,Automatic Hadith Segmentation using {PPM} Compression,2020,-1,-1,2,0,19088,taghreed tarmom,Proceedings of the 17th International Conference on Natural Language Processing (ICON),0,"In this paper we explore the use of Prediction by partial matching (PPM) compression based to segment Hadith into its two main components (Isnad and Matan). The experiments utilized the PPMD variant of the PPM, showing that PPMD is effective in Hadith segmentation. It was also tested on Hadith corpora of different structures. In the first experiment we used the non- authentic Hadith (NAH) corpus for train- ing models and testing, and in the second experiment we used the NAH corpus for training models and the Leeds University and King Saud University (LK) Hadith cor- pus for testing PPMD segmenter. PPMD of order 7 achieved an accuracy of 92.76{\%} and 90.10{\%} in the first and second experiments, respectively."
2020.icon-main.34,{WEKA} in Forensic Authorship Analysis: A corpus-based approach of Saudi Authors,2020,-1,-1,2,0,19147,mashael alamr,Proceedings of the 17th International Conference on Natural Language Processing (ICON),0,"This is a pilot study that aims to explore the potential of using WEKA in forensic authorship analysis. It is a corpus-based research using data from Twitter collected from thirteen authors from Riyadh, Saudi Arabia. It examines the performance of unbalanced and balanced data sets using different classifiers and parameters of word grams. The attributes are dialect-specific linguistic features categorized as word grams. The findings further support previous studies in computational authorship identification."
W19-5605,Text Segmentation Using N-grams to Annotate Hadith Corpus,2019,-1,-1,2,0,17551,shatha altammami,Proceedings of the 3rd Workshop on Arabic Corpus Linguistics,0,None
W19-5607,Classifying {A}rabic dialect text in the Social Media {A}rabic Dialect Corpus ({SMADC}),2019,-1,-1,2,0,23806,areej alshutayri,Proceedings of the 3rd Workshop on Arabic Corpus Linguistics,0,None
L18-1621,Web-based Annotation Tool for Inflectional Language Resources,2018,0,0,2,0,30182,abdulrahman alosaimy,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"We present Wasim, a web-based tool for semi-automatic morphosyntactic annotation of inflectional languages resources. The tool features high flexibility in segmenting tokens, editing, diacritizing, and labelling tokens and segments. Text annotation of highly inflectional languages (including Arabic) requires key functionality which we could not see in a survey of existing tools. Wasim integrates with morphological analysers to speed up the annotation process by selecting one from their proposed analyses. It integrates as well with external POS taggers for kick-start annotation and adaptive predicting based on annotations made so far. It aims to speed up the annotation by completely relying on a keyboard interface, with no mouse interaction required. Wasim has been tested on four case studies and these features proved to be useful. The source-code is released under the MIT license."
W16-4826,{A}rabic Language {WEKA}-Based Dialect Classifier for {A}rabic Automatic Speech Recognition Transcripts,2016,0,5,2,0,23806,areej alshutayri,"Proceedings of the Third Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial3)",0,"This paper describes an Arabic dialect identification system which we developed for the Discriminating Similar Languages (DSL) 2016 shared task. We classified Arabic dialects by using Waikato Environment for Knowledge Analysis (WEKA) data analytic tool which contains many alternative filters and classifiers for machine learning. We experimented with several classifiers and the best accuracy was achieved using the Sequential Minimal Optimization (SMO) algorithm for training and testing process set to three different feature-sets for each testing process. Our approach achieved an accuracy equal to 42.85{\%} which is considerably worse in comparison to the evaluation scores on the training set of 80-90{\%} and with training set {``}60:40{''} percentage split which achieved accuracy around 50{\%}. We observed that Buckwalter transcripts from the Saarland Automatic Speech Recognition (ASR) system are given without short vowels, though the Buckwalter system has notation for these. We elaborate such observations, describe our methods and analyse the training dataset."
L16-1080,An Empirical Study of {A}rabic Formulaic Sequence Extraction Methods,2016,14,3,2,0,34819,ayman alghamdi,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper aims to implement what is referred to as the collocation of the Arabic keywords approach for extracting formulaic sequences (FSs) in the form of high frequency but semantically regular formulas that are not restricted to any syntactic construction or semantic domain. The study applies several distributional semantic models in order to automatically extract relevant FSs related to Arabic keywords. The data sets used in this experiment are rendered from a new developed corpus-based Arabic wordlist consisting of 5,189 lexical items which represent a variety of modern standard Arabic (MSA) genres and regions, the new wordlist being based on an overlapping frequency based on a comprehensive comparison of four large Arabic corpora with a total size of over 8 billion running words. Empirical n-best precision evaluation methods are used to determine the best association measures (AMs) for extracting high frequency and meaningful FSs. The gold standard reference FSs list was developed in previous studies and manually evaluated against well-established quantitative and qualitative criteria. The results demonstrate that the MI.log{\_}f AM achieved the highest results in extracting significant FSs from the large MSA corpus, while the T-score association measure achieved the worst results."
L16-1285,Compilation of an {A}rabic Children{'}s Corpus,2016,12,1,4,0,35013,latifa alsulaiti,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Inspired by the Oxford Children{'}s Corpus, we have developed a prototype corpus of Arabic texts written and/or selected for children. Our Arabic Children{'}s Corpus of 2950 documents and nearly 2 million words has been collected manually from the web during a 3-month project. It is of high quality, and contains a range of different children{'}s genres based on sources located, including classic tales from The Arabian Nights, and popular fictional characters such as Goha. We anticipate that the current and subsequent versions of our corpus will lead to interesting studies in text classification, language use, and ideology in children{'}s texts."
brierley-etal-2014-tools,Tools for {A}rabic Natural Language Processing: a case study in qalqalah prosody,2014,10,1,3,1,34820,claire brierley,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we focus on the prosodic effect of qalqalah or {``}vibration{''} applied to a subset of Arabic consonants under certain constraints during correct Qur{'}anic recitation or ta{\c{C}}{\S}w{\=\i}d, using our Boundary-Annotated QurÂan dataset of 77430 words (Brierley et al 2012; Sawalha et al 2014). These qalqalah events are rule-governed and are signified orthographically in the Arabic script. Hence they can be given abstract definition in the form of regular expressions and thus located and collected automatically. High frequency qalqalah content words are also found to be statistically significant discriminators or keywords when comparing Meccan and Medinan chapters in the Qur{'}an using a state-of-the-art Visual Analytics toolkit: Semantic Pathways. Thus we hypothesise that qalqalah prosody is one way of highlighting salient items in the text. Finally, we implement Arabic transcription technology (Brierley et al under review; Sawalha et al forthcoming) to create a qalqalah pronunciation guide where each word is transcribed phonetically in IPA and mapped to its chapter-verse ID. This is funded research under the EPSRC {``}Working Together{''} theme."
sharaf-atwell-2012-qurana,{Q}ur{A}na: Corpus of the Quran annotated with Pronominal Anaphora,2012,11,28,2,0,42845,abdulbaquee sharaf,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper presents QurAna: a large corpus created from the original Quranic text, where personal pronouns are tagged with their antecedence. These antecedents are maintained as an ontological list of concepts, which have proved helpful for information retrieval tasks. QurAna is characterized by: (a) comparatively large number of pronouns tagged with antecedent information (over 24,500 pronouns), and (b) maintenance of an ontological concept list out of these antecedents. We have shown useful applications of this corpus. This corpus is first of its kind considering classical Arabic text, which could be used for interesting applications for Modern Standard Arabic as well. This corpus would benefit researchers in obtaining empirical and rules in building new anaphora resolution approaches. Also, such corpus would be used to train, optimize and evaluate existing approaches."
sharaf-atwell-2012-qursim,{Q}ur{S}im: A corpus for evaluation of relatedness in short texts,2012,10,26,2,0,42845,abdulbaquee sharaf,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper presents a large corpus created from the original Quranic text, where semantically similar or related verses are linked together. This corpus will be a valuable evaluation resource for computational linguists investigating similarity and relatedness in short texts. Furthermore, this dataset can be used for evaluation of paraphrase analysis and machine translation tasks. Our dataset is characterised by: (1) superior quality of relatedness assignment; as we have incorporated relations marked by well-known domain experts, this dataset could thus be considered a gold standard corpus for various evaluation tasks, (2) the size of our dataset; over 7,600 pairs of related verses are collected from scholarly sources with several levels of degree of relatedness. This dataset could be extended to over 13,500 pairs of related verses observing the commutative property of strongly related pairs. This dataset was incorporated into online query pages where users can visualize for a given verse a network of all directly and indirectly related verses. Empirical experiments showed that only 33{\%} of related pairs shared root words, emphasising the need to go beyond common lexical matching methods, and incorporate -in addition- semantic, domain knowledge, and other corpus-based approaches."
sawalha-etal-2012-predicting,Predicting Phrase Breaks in Classical and {M}odern {S}tandard {A}rabic Text,2012,12,5,3,1,24095,majdi sawalha,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We train and test two probabilistic taggers for Arabic phrase break prediction on a purpose-built, Âgold standardÂ, boundary-annotated and PoS-tagged Qur'an corpus of 77430 words and 8230 sentences. In a related LREC paper (Brierley et al., 2012), we cover dataset build. Here we report on comparative experiments with off-the-shelf N-gram and HMM taggers and coarse-grained feature sets for syntax and prosody, where the task is to predict boundary locations in an unseen test set stripped of boundary annotations by classifying words as breaks or non-breaks. The preponderance of non-breaks in the training data sets a challenging baseline success rate: 85.56{\%}. However, we achieve significant gains in accuracy with the trigram tagger, and significant gains in performance recognition of minority class instances with both taggers via Balanced Classification Rate. This is initial work on a long-term research project to produce annotation schemes, language resources, algorithms, and applications for Classical and Modern Standard Arabic."
brierley-etal-2012-open,Open-Source Boundary-Annotated Corpus for {A}rabic Speech and Language Processing,2012,16,9,3,1,34820,claire brierley,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"A boundary-annotated and part-of-speech tagged corpus is a prerequisite for developing phrase break classifiers. Boundary annotations in English speech corpora are descriptive, delimiting intonation units perceived by the listener. We take a novel approach to phrase break prediction for Arabic, deriving our prosodic annotation scheme from Tajw{\=\i}d (recitation) mark-up in the Qur'an which we then interpret as additional text-based data for computational analysis. This mark-up is prescriptive, and signifies a widely-used recitation style, and one of seven original styles of transmission. Here we report on version 1.0 of our Boundary-Annotated Qur'an dataset of 77430 words and 8230 sentences, where each word is tagged with prosodic and syntactic information at two coarse-grained levels. In (Sawalha et al., 2012), we use the dataset in phrase break prediction experiments. This research is part of a larger-scale project to produce annotation schemes, language resources, algorithms, and applications for Classical and Modern Standard Arabic."
dukes-atwell-2012-lamp,{LAMP}: A Multimodal Web Platform for Collaborative Linguistic Analysis,2012,18,7,2,0.833333,38942,kais dukes,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes the underlying software platform used to develop and publish annotations for the Quranic Arabic Corpus (QAC). The QAC (Dukes, Atwell and Habash, 2011) is a multimodal language resource that integrates deep tagging, interlinear translation, multiple speech recordings, visualization and collaborative analysis for the Classical Arabic language of the Quran. Available online at http://corpus.quran.com, the website is a popular study guide for Quranic Arabic, used by over 1.2 million visitors over the past year. We provide a description of the underlying software system that has been used to develop the corpus annotations. The multimodal data is made available online through an accessible cross-referenced web interface. Although our Linguistic Analysis Multimodal Platform (LAMP) has been applied to the Classical Arabic language of the Quran, we argue that our annotation model and software architecture may be of interest to other related corpus linguistics projects. Work related to LAMP includes recent efforts for annotating other Classical languages, such as Ancient Greek and Latin (Bamman, Mambrini and Crane, 2009), as well as commercial systems (e.g. Logos Bible study) that provide access to syntactic tagging for the Hebrew Bible and Greek New Testament (Brannan, 2011)."
dukes-etal-2010-syntactic,Syntactic Annotation Guidelines for the Quranic {A}rabic Dependency Treebank,2010,9,29,2,0.833333,38942,kais dukes,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The Quranic Arabic Dependency Treebank (QADT) is part of the Quranic Arabic Corpus (http://corpus.quran.com), an online linguistic resource organized by the University of Leeds, and developed through online collaborative annotation. The website has become a popular study resource for Arabic and the Quran, and is now used by over 1,500 researchers and students daily. This paper presents the treebank, explains the choice of syntactic representation, and highlights key parts of the annotation guidelines. The text being analyzed is the Quran, the central religious book of Islam, written in classical Quranic Arabic (c. 600 CE). To date, all 77,430 words of the Quran have a manually verified morphological analysis, and syntactic analysis is in progress. 11,000 words of Quranic Arabic have been syntactically annotated as part of a gold standard treebank. Annotation guidelines are especially important to promote consistency for a corpus which is being developed through online collaboration, since often many people will participate from different backgrounds and with different levels of linguistic expertise. The treebank is available online for collaborative correction to improve accuracy, with suggestions reviewed by expert Arabic linguists, and compared against existing published books of Quranic Syntax."
sawalha-atwell-2010-fine,Fine-Grain Morphological Analyzer and Part-of-Speech Tagger for {A}rabic Text,2010,20,18,2,1,24095,majdi sawalha,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Morphological analyzers and part-of-speech taggers are key technologies for most text analysis applications. Our aim is to develop a part-of-speech tagger for annotating a wide range of Arabic text formats, domains and genres including both vowelized and non-vowelized text. Enriching the text with linguistic analysis will maximize the potential for corpus re-use in a wide range of applications. We foresee the advantage of enriching the text with part-of-speech tags of very fine-grained grammatical distinctions, which reflect expert interest in syntax and morphology, but not specific needs of end-users, because end-user applications are not known in advance. In this paper we review existing Arabic Part-of-Speech Taggers and tag-sets, and illustrate four different Arabic PoS tag-sets for a sample of Arabic text from the Quran. We describe the detailed fine-grained morphological feature tag set of Arabic, and the fine-grained Arabic morphological analyzer algorithm. We faced practical challenges in applying the morphological analyzer to the 100-million-word Web Arabic Corpus: we had to port the software to the National Grid Service, adapt the analyser to cope with spelling variations and errors, and utilise a Broad-Coverage Lexical Resource combining 23 traditional Arabic lexicons. Finally we outline the construction of a Gold Standard for comparative evaluation."
sawalha-atwell-2010-constructing,Constructing and Using Broad-coverage Lexical Resource for Enhancing Morphological Analysis of {A}rabic,2010,5,8,2,1,24095,majdi sawalha,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Broad-coverage language resources which provide prior linguistic knowledge must improve the accuracy and the performance of NLP applications. We are constructing a broad-coverage lexical resource to improve the accuracy of morphological analyzers and part-of-speech taggers of Arabic text. Over the past 1200 years, many different kinds of Arabic language lexicons were constructed; these lexicons are different in ordering, size and aim or goal of construction. We collected 23 machine-readable lexicons, which are freely available on the web. We combined lexical resources into one large broad-coverage lexical resource by extracting information from disparate formats and merging traditional Arabic lexicons. To evaluate the broad-coverage lexical resource we computed coverage over the QurÂan, the Corpus of Contemporary Arabic, and a sample from the Arabic Web Corpus, using two methods. Counting exact word matches between test corpora and lexicon scored about 65-68{\%}; Arabic has a rich morphology with many combinations of roots, affixes and clitics, so about a third of words in the corpora did not have an exact match in the lexicon. The second approach is to compute coverage in terms of use in a lemmatizer program, which strips clitics to look for a match for the underlying lexeme; this scored about 82-85{\%}."
brierley-atwell-2010-proposec,{P}ro{POSEC}: A Prosody and {P}o{S} Annotated Spoken {E}nglish Corpus,2010,12,4,2,1,34820,claire brierley,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We have previously reported on ProPOSEL, a purpose-built Prosody and PoS English Lexicon compatible with the Python Natural Language ToolKit. ProPOSEC is a new corpus research resource built using this lexicon, intended for distribution with the Aix-MARSEC dataset. ProPOSEC comprises multi-level parallel annotations, juxtaposing prosodic and syntactic information from different versions of the Spoken English Corpus, with canonical dictionary forms, in a query format optimized for Perl, Python, and text processing programs. The order and content of fields in the text file is as follows: (1) Aix-MARSEC file number; (2) word; (3) LOB PoS-tag; (4) C5 PoS-tag; (5) Aix SAM-PA phonetic transcription; (6) SAM-PA phonetic transcription from ProPOSEL; (7) syllable count; (8) lexical stress pattern; (9) default content or function word tag; (10) DISC stressed and syllabified phonetic transcription; (11) alternative DISC representation, incorporating lexical stress pattern; (12) nested arrays of phonemes and tonic stress marks from Aix. As an experimental dataset, ProPOSEC can be used to study correlations between these annotation tiers, where significant findings are then expressed as additional features for phrasing models integral to Text-to-Speech and Speech Recognition. As a training set, ProPOSEC can be used for machine learning tasks in Information Retrieval and Speech Understanding systems."
W08-1904,{P}ro{POSEL}: a human-oriented prosody and {P}o{S} {E}nglish lexicon for machine-learning and {NLP},2008,15,6,2,1,34820,claire brierley,Coling 2008: Proceedings of the Workshop on Cognitive Aspects of the Lexicon ({COGALEX} 2008),0,"ProPOSEL is a prosody and PoS English lexicon, purpose-built to integrate and leverage domain knowledge from several well-established lexical resources for machine learning and NLP applications. The lexicon of 104049 separate entries is in accessible text file format, is human and machine-readable, and is intended for open source distribution with the Natural Language ToolKit. It is therefore supported by Python software tools which transform ProPOSEL into a Python dictionary or associative array of linguistic concepts mapped to compound lookup keys. Users can also conduct searches on a subset of the lexicon and access entries by word class, phonetic transcription, syllable count and lexical stress pattern. ProPOSEL caters for a range of different cognitive aspects of the lexiconxc2xa9."
shawar-atwell-2008-ai,An {AI}-inspired intelligent agent/student architecture to combine Language Resources research and teaching,2008,10,0,2,1,48292,bayan shawar,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper describes experimental use of the multi-agent architecture to integrate Natural Language and Information Systems research and teaching, by casting a group of students as intelligent agents to collect and analyse English language resources from around the world. Section 2 and section 3 describe the hybrid intelligent information systems experiments at the University of Leeds and the results generated, including several research papers accepted at international conferences, and a finalist entry in the British Computer Society Machine Intelligence contest. Our proposals for applying the multi-agent idea in other universities such as the Arab Open University are presented in section 4. The conclusion is presented in section 5: the success of hybrid intelligent information systems experiments in generating research papers within a limited time."
brierley-atwell-2008-proposel-prosody,{P}ro{POSEL}: A Prosody and {POS} {E}nglish Lexicon for Language Engineering,2008,15,10,2,1,34820,claire brierley,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"ProPOSEL is a prototype prosody and PoS (part-of-speech) English lexicon for Language Engineering, derived from the following language resources: the computer-usable dictionary CUVPlus, the CELEX-2 database, the Carnegie-Mellon Pronouncing Dictionary, and the BNC, LOB and Penn Treebank PoS-tagged corpora. The lexicon is designed for the target application of prosodic phrase break prediction but is also relevant to other machine learning and language engineering tasks. It supplements the existing record structure for wordform entries in CUVPlus with syntactic annotations from rival PoS-tagging schemes, mapped to fields for default closed and open-class word categories and for lexical stress patterns representing the rhythmic structure of wordforms and interpreted as potential new text-based features for automatic phrase break classifiers. The current version of the lexicon comes as a textfile of 104052 separate entries and is intended for distribution with the Natural Language ToolKit; it is therefore accompanied by supporting Python software for manipulating the data so that it can be used for Natural Language Processing (NLP) and corpus-based research in speech synthesis and speech recognition."
C08-2027,Comparative Evaluation of {A}rabic Language Morphological Analysers and Stemmers,2008,2,57,2,1,24095,majdi sawalha,Coling 2008: Companion volume: Posters,0,"Arabic morphological analysers and stemming algorithms have become a popular area of research. Many computational linguists have designed and developed algorithms to solve the problem of morphology and stemming. Each researcher proposed his own gold standard, testing methodology and accuracy measurements to test and compute the accuracy of his algorithm. Therefore, we cannot make comparisons between these algorithms. In this paper we have accomplished two tasks. First, we proposed four different fair and precise accuracy measurements and two 1000-word gold standards taken from the Holy Qurxe2x80x99an and from the Corpus of Contemporary Arabic. Second, we combined the results from the morphological analysers and stemming algorithms by voting after running them on the sample documents. The evaluation of the algorithms shows that Arabic morphology is still a challenge."
W07-0313,Different measurement metrics to evaluate a chatbot system,2007,14,48,2,1,48292,bayan shawar,Proceedings of the Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies,0,"A chatbot is a software system, which can interact or chat with a human user in natural language such as English. For the annual Loebner Prize contest, rival chatbots have been assessed in terms of ability to fool a judge in a restricted chat session. We are investigating methods to train and adapt a chatbot to a specific user's language use or application, via a user-supplied training corpus. We advocate open-ended trials by real users, such as an example Afrikaans chatbot for Afrikaans-speaking researchers and students in South Africa. This is evaluated in terms of glass box dialogue efficiency metrics, and black box dialogue quality metrics and user satisfaction feedback. The other examples presented in this paper are the Qur'an and the FAQchat prototypes. Our general conclusion is that evaluation should be adapted to the application and to user needs."
shawar-atwell-2004-chatbot,A Chatbot as a Novel Corpus Visualization Tool,2004,4,4,2,1,48292,bayan shawar,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The classical way of viewing data set is using the visualization process, which maps the data from numerical or textual form to a visual representation that our mind can easily interpret such as: using graphical diagrams, charts, and geometric representation. In this paper we introduce a new idea to visualize a dialogue corpus using a chatbot interface tool. We developed a java program to convert a readable text (corpus) to AIML language to retrain ALICE. We use specific domains of the BNC spoken files to retrain ALICE, and visualise the data contents of these domains via chatting. Even that visualizing the corpus using a chatbot based on the chatting time and the size of the corpus, certain domains could be scanned easily if they are part of the current corpus or not. Our main conclusion is that it is possible to use the chatbot tool as a visualization process of a dialogue corpus, and to model different chatbot personalities."
2004.jeptalnrecital-poster.2,Le Regroupement de Types de Mots et l{'}Unification d{'}Occurrences de Mots dans des Cat{\\'e}gories grammaticales de mots (Clustering of Word Types and Unification of Word Tokens into Grammatical Word-Classes),2004,-1,-1,1,1,540,eric atwell,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Ce papier discute la N{\'e}oposie: l{'}inf{\'e}rence auto-adaptive de cat{\'e}gories grammaticales de mots de la langue naturelle. L{'}inf{\'e}rence grammaticale peut {\^e}tre divis{\'e}e en deux parties : l{'}inf{\'e}rence de cat{\'e}gories grammaticales de mots et l{'}inf{\'e}rence de la structure. Nous examinons les {\'e}l{\'e}ments de base de l{'}apprentissage auto-adaptif du marquage des cat{\'e}gories grammaticales, et discutons l{'}adaptation des trois types principaux de marqueurs des cat{\'e}gories grammaticales {\`a} l{'}inf{\'e}rence auto-adaptive de cat{\'e}gories grammaticales de mots. Des marqueurs statistiques de n-grammes sugg{\`e}rent une approche de regroupement statistique, mais le regroupement n{'}aide ni avec les types de mots peu fr{\'e}quents, ni avec les types de mots nombreux qui peuvent se pr{\'e}senter dans plus d{'}une cat{\'e}gorie grammaticale. Le marqueur alternatif d{'}apprentissage bas{\'e} sur la transformation sugg{\`e}re une approche bas{\'e}e sur la contrainte de l{'}unification de contextes d{'}occurrences de mots. Celle-ci pr{\'e}sente un moyen de regrouper des mots peu fr{\'e}quents, et permet aux occurrences diff{\'e}rentes d{'}un seul type de mot d{'}appartenir {\`a} des cat{\'e}gories diff{\'e}rentes selon les contextes grammaticaux o{\`u} ils se pr{\'e}sentent. Cependant, la simple unification de contextes d{'}occurrences de mots produit un nombre incroyablement grand de cat{\'e}gories grammaticales de mots. Nous avons essay{\'e} d{'}unifier plus de cat{\'e}gories en mod{\'e}rant le contexte de la correspondance pour permettre l{'}unification des cat{\'e}gories de mots aussi bien que des occurrences de mots, mais cela entra{\^\i}ne des unifications fausses. Nous concluons que l{'}avenir peut {\^e}tre un hybride qui comprend le regroupement de types de mots peu fr{\'e}quents, l{'}unification de contextes d{'}occurrences de mots, et le {`}seeding{'} avec une connaissance linguistique limit{\'e}e. Nous demandons un programme de nouvelles recherches pour d{\'e}velopper une valise pour la d{\'e}couverte de la langue naturelle."
elliott-etal-2004-fluency,A fluency error categorization scheme to guide automated machine translation evaluation,2004,11,10,3,0,51186,debbie elliott,Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Existing automated MT evaluation methods often require expert human translations. These are produced for every language pair evaluated and, due to this expense, subsequent evaluations tend to rely on the same texts, which do not necessarily reflect real MT use. In contrast, we are designing an automated MT evaluation system, intended for use by post-editors, purchasers and developers, that requires nothing but the raw MT output. Furthermore, our research is based on texts that reflect corporate use of MT. This paper describes our first step in system design: a hierarchical classification scheme of fluency errors in English MT output, to enable us to identify error types and frequencies, and guide the selection of errors for automated detection. We present results from the statistical analysis of 20,000 words of MT output, manually annotated using our classification scheme, and describe correlations between error frequencies and human scores for fluency and adequacy."
W00-1901,Comparing Linguistic Interpretation Schemes for {E}nglish Corpora,2000,25,1,1,1,540,eric atwell,Proceedings of the {COLING}-2000 Workshop on Linguistically Interpreted Corpora,0,"Project AMALGAM explored a range of Partof-Speech tagsets and phrase structure parsing schemes used in modern English corpus-based research. The PoS-tagging schemes and parsing schemes include some which have been used for hand annotation of corpora or manual postediting of automatic taggers or parsers; and others which are unedited output of a parsing program. Project deliverables include: a detailed description of each PoS-tagging scheme, and multi-tagged corpus; a xe2x80x9cCorpus-neutral xe2x80x9d tokenization scheme; a family of PoS-taggers, for 8 PoS-tagsets; a method for xe2x80x9cPoS-tagset conversionxe2x80x9d, a sample of texts parsed according to a range of parsing schemes: a MultiTreebank; an Internet service allowing researchers worldwide free access to the above resources, including a simple email-based method for PoS-tagging any English text with any or all PoS-tagset(s). We conclude that the range of tagging and parsing schemes in use is too varied to allow agreement on a standard; and that parserevaluation based on xe2x80x98bracket-matching xe2x80x99 is unfair to more sophisticated parsers."
W00-0705,Increasing our Ignorance{'} of Language: Identifying Language Structure in an Unknown {`}Signal{'},2000,9,6,2,0,54258,john elliot,Fourth Conference on Computational Natural Language Learning and the Second Learning Language in Logic Workshop,0,"This paper describes algorithms and software developed to characterise and detect generic intelligent language-like features in an input signal, using natural language learning techniques: looking for characteristic statistical language-signatures in test corpora. As a first step towards such species-independent language-detection, we present a suite of programs to analyse digital representations of a range of data, and use the results to extrapolate whether or not there are language-like structures which distinguish this data from other sources, such as music, images, and white noise. Outside our own immediate NLP sphere, generic communication techniques are of particular interest in the astronautical community, where two sessions are dedicated to SETI at their annual International conference with topics ranging from detecting ET technology to the ethics and logistics of message construction (Elliott and Atwell, 1999; Ollongren, 2000; Vakoch, 2000)."
menzel-etal-2000-isle,The {ISLE} Corpus of Non-Native Spoken {E}nglish,2000,4,38,2,0,5536,wolfgang menzel,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"For the purpose of developing pronunciation training tools for second language learning a corpus of non-native speech data has been collected, which consists of almost 18 hours of annotated speech signals spoken by Italian and German learners of English. The corpus is based on 250 utterances selected from typical second language learning exercises. It has been annotated at the word and the phone level, to highlight pronunciation errors such as phone realisation problems and misplaced word stress assignments. The data has been used to develop and evaluate several diagnostic components, which can be used to produce corrective feedback of unprecedented detail to a language learner."
demetriou-etal-2000-using,Using Lexical Semantic Knowledge from Machine Readable Dictionaries for Domain Independent Language Modelling,2000,15,4,2,0,48283,george demetriou,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Machine Readable Dictionaries (MRDs) have been used in a variety of language processing tasks including word sense disambiguation, text segmentation, information retrieval and information extraction. In this paper we describe the utilization of semantic knowledge acquired from an MRD for language modelling tasks in relation to speech recognition applications. A semantic model of language has been derived using the dictionary definitions in order to compute the semantic association between the words. The model is capable of capturing phenomena of latent semantic dependencies between the words in texts and reducing the language ambiguity by a considerable factor. The results of experiments suggest that the semantic model can improve the word recognition rates in xe2x80x9cnoisy-channelxe2x80x9d applications. This research provides evidence that limited or incomplete knowledge from lexical resources such as MRDs can be useful for domain independent language modelling."
C00-2150,Language Identification in Unknown Signals,2000,8,19,2,0,54650,john elliott,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"This paper describes algorithms and software developed to characterise and detect generic intelligent language-like features in an input signal, using Natural Language Learning techniques: looking for characteristic statistical language-signatures in test corpora. As a first step towards such species-independent language-detection, we present a suite of programs to analyse digital representations of a range of data, and use the results to extrapolate whether or not there are language-like structures which distinguish this data from other sources, such as music, images, and white noise. We assume that generic species-independent communication can be detected by concentrating on localised patterns and rhythms, identifying segments at the level of characters, words and phrases, without necessarily having to understand the content.We assume that a language-like signal will be encoded symbolically, i.e. some kind of character-stream. Our language-detection algorithm for symbolic input uses a number of statistical clues: data compression ratio, chunking to find character bit-length and boundaries, and matching against a Zipfian type-token distribution for letters and words. We do not claim extensive (let alone exhaustive) empirical evidence that our language-detection clues are correct; the only real test will come when the Search for Extra-Terrestrial Intelligence finds true alien signals. If and when true SETI signals are found, the first step to interpretation is to identify the language-like features, using techniques like the above. Our current research goal is to apply Natural Language Learning techniques to the identification of higher-level grammatical and semantic structure in a linguistic signal."
W97-0602,A Generic Template to evaluate integrated components in spoken dialogue systems,1997,16,0,2,0,55559,gavin churcher,Interactive Spoken Dialog Systems: Bringing Speech and {NLP} Together in Real Applications,0,"We present a generic template for spoken dialogue systems integrating speech recognition and synthesis with 'higher-level' natural language dialogue modelling components. The generic model is abstracted from a number of real application systems targetted at very different domains. Our research aim in developing this generic template is to investigate a new approach to the evaluation of Dialogue Management Systems. Rather than attempting to measure accuracy/speed of output, we propose principles for the evaluation of the underlying theoretical linguistic model of Dialogue Management in a given system, in terms of how well it fits our generic template for Dialogue Management Systems. This is a measure of 'genericness' or 'application-independence' of a given system, which can be used to moderate accuracy /speed scores in comparisons of very unlike DMSs serving different domains. This relates to (but is orthogonal to) Dialogue Management Systems evaluation in terms of naturalness and like measurable metrics (eg Dybkjaer et al 1995, Vilnat 1996, EAGLES 1994, Fraser 1995); it follows more closely emerging qualitative evaluation techniques for NL grammatical parsing schemes (Leech et al 1996, Atwell 1996)."
W94-0103,{AMALGAM}: Automatic Mapping Among Lexico-Grammatical Annotation Models,1994,9,20,1,1,540,eric atwell,The Balancing Act: Combining Symbolic and Statistical Approaches to Language,0,"Several Corpus Linguistics research groups have gone beyond collation of 'raw' text, to syntactic annotation of the text. However, linguists developing these linguistic resources have used quite different wordtagging and parse-tree labelling schemes in each of these annotated corpora. This restricts the accessibility of each corpus, making it impossible for speech and handwriting researchers to collate them into a single very large training set. This is particularly problematic as there is evidence that one of these parsed corpora on its own is too small for a general statistical model of grammatical structure, but the combined size of all the above annotated corpora should deliver a much more reliable model. We are developing a set of mapping algorithms to map between the main tagsets and phrase structure grammar schemes used in the above corpora. We plan to develop a Multi-tagged Corpus and a MultiTreebank, a single text-set annotated with all the above tagging and parsing schemes. The text-set is the Spoken English Corpus: this is a half-way house between formal written text and colloquial conversational speech. However, the main deliverable to the computational linguistics research community is not the SEC-based MultiTreebank, but the mapping suite used to produce it - this can be used to combine currently-incompatible syntactic training sets into a large unified multicorpus. Our architecture combines standard statistical language modelling and a rule-base derived from linguists' analyses of tagset-mappings, in a novel yet intuitive way. Our development of the mapping algorithms aims to distinguish notational from substantive differences in the annotation schemes, and we will be able to evaluate tagging schemes in terms of how well they fit standard statistical language models such as n-pos (Markov) models."
P88-1013,{P}roject {A}pril --- A Progress Report,1988,10,3,3,0,57843,robin haigh,26th Annual Meeting of the Association for Computational Linguistics,1,"Parsing techniques based on rules defining grammaticality are difficult to use with authentic inputs, which are often grammatically messy. Instead, the APRIL system seeks a labelled tree structure which maximizes a numerical measure of conformity to statistical norms derived from a sample of parsed text. No distinction between legal and illegal trees arises: any labelled tree has a value. Because the search space is large and has an irregular geometry, APRIL seeks the best tree using simulated annealing, a stochastic optimization technique. Beginning with an arbitrary tree, many randomly-generated local modifications are considered and adopted or rejected according to their effect on tree-value: acceptance decisions are made probabilistically, subject to a bias against adverse moves which is very weak at the outset but is made to increase as the random walk through the search space continues. This enables the system to converge on the global optimum without getting trapped in local optima. Performance of an early version of the APRIL system on authentic inputs is yielding analyses with a mean accuracy of 75.3% using a schedule which increases processing linearly with sentence-length; modifications currently being implemented should eliminate a high proportion of the remaining errors."
E87-1007,How to Detect Grammatical Errors in a Text Without Parsing It,1987,24,52,1,1,540,eric atwell,Third Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"The Constituent Likelihood Automatic Word-tagging System (CLAWS) was originally designed for the low-level grammatical analysis of the million-word LOB Corpus of English text samples. CLAWS does not attempt a full parse, but uses a first-order Markov model of language to assign word-class labels to words. CLAWS can be modified to detect grammatical errors, essentially by flagging unlikely word-class transitions in the input text. This may seem to be an intuitively implausible and theoretically inadequate model of natural language syntax, but nevertheless it can successfully pinpoint most grammatical errors in a text. Several modifications to CLAWS have been explored. The resulting system cannot detect all errors in typed documents; but then neither do far more complex systems, which attempt a full parse, requiring much greater computation."
E87-1010,Pattern Recognition Applied to the Acquisition of a Grammatical Classification System From Unrestricted {E}nglish Text,1987,14,13,1,1,540,eric atwell,Third Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Within computational linguistics, the use of statistical pattern matching is generally restricted to speech processing. We have attempted to apply statistical techniques to discover a grammatical classification system from a Corpus of 'raw' English text. A discovery procedure is simpler for a simpler language model; we assume a first-order Markov model, which (surprisingly) is shown elsewhere to be sufficient for practical applications. The extraction of the parameters of a standard Markov model is theoretically straightforward; however, the huge size of the standard model for a Natural Language renders it incomputable in reasonable time. We have explored various constrained models to reduce computation, which have yielded results of varying success."
1984.bcs-1.20,The grammatical tagging of unrestricted {E}nglish text,1984,-1,-1,3,0,49809,roger garside,Proceedings of the International Conference on Methodology and Techniques of Machine Translation: Processing from words to language,0,None
