2021.acl-long.9,Mention Flags ({MF}): Constraining Transformer-based Text Generators,2021,-1,-1,4,0,10656,yufei wang,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"This paper focuses on Seq2Seq (S2S) constrained text generation where the text generator is constrained to mention specific words which are inputs to the encoder in the generated outputs. Pre-trained S2S models or a Copy Mechanism are trained to copy the surface tokens from encoders to decoders, but they cannot guarantee constraint satisfaction. Constrained decoding algorithms always produce hypotheses satisfying all constraints. However, they are computationally expensive and can lower the generated text quality. In this paper, we propose Mention Flags (MF), which traces whether lexical constraints are satisfied in the generated outputs in an S2S decoder. The MF models can be trained to generate tokens in a hypothesis until all constraints are satisfied, guaranteeing high constraint satisfaction. Our experiments on the Common Sense Generation task (CommonGen) (Lin et al., 2020), End2end Restaurant Dialog task (E2ENLG) (Du{\v{}}sek et al., 2020) and Novel Object Captioning task (nocaps) (Agrawal et al., 2019) show that the MF models maintain higher constraint satisfaction and text quality than the baseline models and other constrained decoding algorithms, achieving state-of-the-art performance on all three tasks. These results are achieved with a much lower run-time than constrained decoding algorithms. We also show that the MF models work well in the low-resource setting."
2020.starsem-1.19,Large Scale Author Obfuscation Using {S}iamese Variational Auto-Encoder: The {S}iam{AO} System,2020,-1,-1,2,0,14538,chakaveh saedi,Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics,0,"Author obfuscation is the task of masking the author of a piece of text, with applications in privacy. Recent advances in deep neural networks have boosted author identification performance making author obfuscation more challenging. Existing approaches to author obfuscation are largely heuristic. Obfuscation can, however, be thought of as the construction of adversarial examples to attack author identification, suggesting that the deep learning architectures used for adversarial attacks could have application here. Current architectures are proposed to construct adversarial examples against classification-based models, which in author identification would exclude the high-performing similarity-based models employed when facing large number of authorial classes. In this paper, we propose the first deep learning architecture for constructing adversarial examples against similarity-based learners, and explore its application to author obfuscation. We analyse the output from both success in obfuscation and language acceptability, as well as comparing the performance with some common baselines, and showing promising results in finding a balance between safety and soundness of the perturbed texts."
P18-2072,Predicting accuracy on large datasets from smaller pilot data,2018,0,4,3,0,4047,mark johnson,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Because obtaining training data is often the most difficult part of an NLP or ML project, we develop methods for predicting how much data is required to achieve a desired test accuracy by extrapolating results from models trained on a small pilot training dataset. We model how accuracy varies as a function of training size on subsets of the pilot data, and use that model to predict how much training data would be required to achieve the desired accuracy. We introduce a new performance extrapolation task to evaluate how well different extrapolations predict accuracy on larger training sets. We show that details of hyperparameter optimisation and the extrapolation models can have dramatic effects in a document classification task. We believe this is an important first step in developing methods for estimating the resources required to meet specific engineering performance targets."
N18-5012,{V}n{C}ore{NLP}: A {V}ietnamese Natural Language Processing Toolkit,2018,17,6,4,1,13715,thanh vu,Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"We present an easy-to-use and fast toolkit, namely VnCoreNLP{---}a Java NLP annotation pipeline for Vietnamese. Our VnCoreNLP supports key natural language processing (NLP) tasks including word segmentation, part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing, and obtains state-of-the-art (SOTA) results for these tasks. We release VnCoreNLP to provide rich linguistic annotations to facilitate research work on Vietnamese NLP. Our VnCoreNLP is open-source and available at: \url{https://github.com/vncorenlp/VnCoreNLP}"
L18-1410,A Fast and Accurate {V}ietnamese Word Segmenter,2018,-1,-1,4,0.401092,3796,dat nguyen,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
J18-3003,Native Language Identification With Classifier Stacking and Ensembles,2018,55,5,2,0.709882,3599,shervin malmasi,Computational Linguistics,0,"Ensemble methods using multiple classifiers have proven to be among the most successful approaches for the task of Native Language Identification (NLI), achieving the current state of the art. However, a systematic examination of ensemble methods for NLI has yet to be conducted. Additionally, deeper ensemble architectures such as classifier stacking have not been closely evaluated. We present a set of experiments using three ensemble-based models, testing each with multiple configurations and algorithms. This includes a rigorous application of meta-classification models for NLI, achieving state-of-the-art results on several large data sets, evaluated in both intra-corpus and cross-corpus modes."
U17-1001,Stock Market Prediction with Deep Learning: A Character-based Neural Language Model for Event-based Trading,2017,0,7,2,0,32218,leonardo pinheiro,Proceedings of the Australasian Language Technology Association Workshop 2017,0,None
U17-1013,From Word Segmentation to {POS} Tagging for {V}ietnamese,2017,19,4,4,0.524108,3796,dat nguyen,Proceedings of the Australasian Language Technology Association Workshop 2017,0,"This paper presents an empirical comparison of two strategies for Vietnamese Part-of-Speech (POS) tagging from unsegmented text: (i) a pipeline strategy where we consider the output of a word segmenter as the input of a POS tagger, and (ii) a joint strategy where we predict a combined segmentation and POS tag for each syllable. We also make a comparison between state-of-the-art (SOTA) feature-based and neural network-based models. On the benchmark Vietnamese treebank (Nguyen et al., 2009), experimental results show that the pipeline strategy produces better scores of POS tagging from unsegmented text than the joint strategy, and the highest accuracy is obtained by using a feature-based model."
P17-2063,Feature Hashing for Language and Dialect Identification,2017,0,0,2,0.801702,3599,shervin malmasi,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We evaluate feature hashing for language identification (LID), a method not previously used for this task. Using a standard dataset, we first show that while feature performance is high, LID data is highly dimensional and mostly sparse ({\textgreater}99.5{\%}) as it includes large vocabularies for many languages; memory requirements grow as languages are added. Next we apply hashing using various hash sizes, demonstrating that there is no performance loss with dimensionality reductions of up to 86{\%}. We also show that using an ensemble of low-dimension hash-based classifiers further boosts performance. Feature hashing is highly useful for LID and holds great promise for future work in this area."
P17-1134,Unsupervised Text Segmentation Based on Native Language Characteristics,2017,29,1,2,0.801702,3599,shervin malmasi,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Most work on segmenting text does so on the basis of topic changes, but it can be of interest to segment by other, stylistically expressed characteristics such as change of authorship or native language. We propose a Bayesian unsupervised text segmentation approach to the latter. While baseline models achieve essentially random segmentation on our task, indicating its difficulty, a Bayesian model that incorporates appropriately compact language models and alternating asymmetric priors can achieve scores on the standard metrics around halfway to perfect segmentation."
K17-3014,A Novel Neural Network Model for Joint {POS} Tagging and Graph-based Dependency Parsing,2017,41,18,2,0.524108,3796,dat nguyen,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"We present a novel neural network model that learns POS tagging and graph-based dependency parsing jointly. Our model uses bidirectional LSTMs to learn feature representations shared for both POS tagging and dependency parsing tasks, thus handling the feature-engineering problem. Our extensive experiments, on 19 languages from the Universal Dependencies project, show that our model outperforms the state-of-the-art neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing, resulting in a new state of the art. Our code is open-source and available together with pre-trained models at: \url{https://github.com/datquocnguyen/jPTDP}"
W16-0314,Predicting Post Severity in Mental Health Forums,2016,12,16,3,1,3599,shervin malmasi,Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology,0,None
U16-1017,An empirical study for {V}ietnamese dependency parsing,2016,26,5,2,0.524108,3796,dat nguyen,Proceedings of the Australasian Language Technology Association Workshop 2016,0,"This paper presents an empirical comparison of different dependency parsers for Vietnamese, which has some unusual characteristics such as copula drop and verb serialization. Experimental results show that the neural network-based parsers perform significantly better than the traditional parsers. We report the highest parsing scores published to date for Vietnamese with the labeled attachment score (LAS) at 73.53% and the unlabeled attachment score (UAS) at 80.66%."
S16-1154,{LTG} at {S}em{E}val-2016 Task 11: Complex Word Identification with Classifier Ensembles,2016,13,3,2,1,3599,shervin malmasi,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"We present the description of the LTG entry in the SemEval-2016 Complex Word Identification (CWI) task, which aimed to develop systems for identifying complex words in English sentences. Our entry focused on the use of contextual language model features and the application of ensemble classification methods. Both of our systems achieved good performance, ranking in 2nd and 3rd place overall in terms of F-Score."
L16-1647,Modeling Language Change in Historical Corpora: The Case of {P}ortuguese,2016,0,1,3,0,622,marcos zampieri,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents a number of experiments to model changes in a historical Portuguese corpus composed of literary texts for the purpose of temporal text classification. Algorithms were trained to classify texts with respect to their publication date taking into account lexical variation represented as word n-grams, and morphosyntactic variation represented by part-of-speech (POS) distribution. We report results of 99.8{\%} accuracy using word unigram features with a Support Vector Machines classifier to predict the publication date of documents in time intervals of both one century and half a century. A feature analysis is performed to investigate the most informative features for this task and how they are linked to language change."
W15-5407,Language Identification using Classifier Ensembles,2015,0,10,2,1,3599,shervin malmasi,"Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects",0,None
W15-0620,Oracle and Human Baselines for Native Language Identification,2015,18,11,3,1,3599,shervin malmasi,Proceedings of the Tenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We examine different ensemble methods, including an oracle, to estimate the upper-limit of classification accuracy for Native Language Identification (NLI). The oracle outperforms state-of-the-art systems by over 10% and results indicate that for many misclassified texts the correct class label receives a significant portion of the ensemble votes, often being the runner-up. We also present a pilot study of human performance for NLI, the first such experiment. While some participants achieve modest results on our simplified setup with 5 L1s, they did not outperform our NLI system, and this performance gap is likely to widen on the standard NLI setup."
U15-1008,Clinical Information Extraction Using Word Representations,2015,4,5,3,1,3599,shervin malmasi,Proceedings of the Australasian Language Technology Association Workshop 2015,0,None
U15-1018,Cognate Identification using Machine Translation,2015,9,0,2,1,3599,shervin malmasi,Proceedings of the Australasian Language Technology Association Workshop 2015,0,"In this paper we describe an approach to automatic cognate identification in monolingual texts using machine translation. This system was used as our entry in the 2015 ALTA shared task, achieving an F1score of 63% on the test set. Our proposed approach takes an input text in a source language and uses statistical machine translation to create a word-aligned parallel text in the target language. A robust measure of string distance, the JaroWinkler distance in this case, is then applied to the pairs of aligned words to detect potential cognates. Further extensions to improve the method are also discussed."
R15-1053,{N}orwegian Native Language Identification,2015,27,6,2,1,3599,shervin malmasi,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"We present a study of Native Language Identification (NLI) using data from learners of Norwegian, a language not yet used for this task. NLI is the task of predicting a writerxe2x80x99s first language using only their writings in a learned language. We find that three feature types, function words, part-of-speech n-grams and a hybrid part-of-speech/function word mixture n-gram model are useful here. Our system achieves an accuracy of 79% against a baseline of 13% for predicting an authorxe2x80x99s L1. The same features can distinguish non-native writing with 99% accuracy. We also find that part-of-speech n-gram performance on this data deviates from previous NLI results, possibly due to the use of manually post-corrected tags."
N15-1160,Large-Scale Native Language Identification with Cross-Corpus Evaluation,2015,24,16,2,1,3599,shervin malmasi,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present a large-scale Native Language Identification (NLI) experiment on new data, with a focus on cross-corpus evaluation to identify corpusand genre-independent language transfer features. We test a new corpus and show it is comparable to other NLI corpora and suitable for this task. Cross-corpus evaluation on two large corpora achieves good accuracy and evidences the existence of reliable language transfer features, but lower performance also suggests that NLI models are not completely portable across corpora. Finally, we present a brief case study of features distinguishing Japanese learnersxe2x80x99 English writing, demonstrating the presence of cross-corpus and cross-genre language transfer features that are highly applicable to SLA and ESL research."
J15-2005,{S}quibs: Evaluating Human Pairwise Preference Judgments,2015,27,2,1,1,12694,mark dras,Computational Linguistics,0,"Human evaluation plays an important role in NLP, often in the form of preference judgments. Although there has been some use of classical non-parametric and bespoke approaches to evaluating these sorts of judgments, there is an entire body of work on this in the context of sensory discrimination testing and the human judgments that are central to it, backed by rigorous statistical theory and freely available software, that NLP can draw on. We investigate one approach, Log-Linear Bradley-Terry models, and apply it to sample NLP data."
W14-4606,Cross-lingual Transfer Parsing for Low-Resourced Languages: An {I}rish Case Study,2014,18,9,3,1,14284,teresa lynn,Proceedings of the First Celtic Language Technology Workshop,0,"We present a study of cross-lingual direct transfer parsing for the Irish language. Firstly we discuss mapping of the annotation scheme of the Irish Dependency Treebank to a universal dependency scheme. We explain our dependency label mapping choices and the structural changes required in the Irish Dependency Treebank. We then experiment with the universally annotated treebanks of ten languages from four language family groups to assess which languages are the most useful for cross-lingual parsing of Irish by using these treebanks to train delexicalised parsing models which are then applied to sentences from the Irish Dependency Treebank. The best results are achieved when using Indonesian, a language from the Austronesian language family."
W14-3708,From Visualisation to Hypothesis Construction for Second Language Acquisition,2014,28,2,2,1,3599,shervin malmasi,Proceedings of {T}ext{G}raphs-9: the workshop on Graph-based Methods for Natural Language Processing,0,"One research goal in Second Language Acquisition (SLA) is to formulate and test hypotheses about errors and the environments in which they are made, a process which often involves substantial effort; large amounts of data and computational visualisation techniques promise help here. In this paper we have defined a new task for finding contexts for errors that vary with the native language of the speaker that are potentially useful for SLA research. We propose four models for approaching this task, and find that one based only on error-feature cooccurrence and another based on determining maximum weight cliques in a feature association graph discover strongly distinguishing contexts, with an apparent trade-off between false positives and very specific contexts."
W14-3625,{A}rabic Native Language Identification,2014,27,24,2,1,3599,shervin malmasi,Proceedings of the {EMNLP} 2014 Workshop on {A}rabic Natural Language Processing ({ANLP}),0,"In this paper we present the first application of Native Language Identification (NLI) to Arabic learner data. NLI, the task of predicting a writerxe2x80x99s first language from their writing in other languages has been mostly investigated with English data, but is now expanding to other languages. We use L2 texts from the newly released Arabic Learner Corpus and with a combination of three syntactic features (CFG production rules, Arabic function words and Part-of-Speech n-grams), we demonstrate that they are useful for this task. Our system achieves an accuracy of 41% against a baseline of 23%, providing the first evidence for classifier-based detection of language transfer effects in L2 Arabic. Such methods can be useful for studying language transfer, developing teaching materials tailored to studentsxe2x80x99 native language and forensic linguistics. Future directions are discussed."
U14-1020,{F}innish Native Language Identification,2014,29,8,2,1,3599,shervin malmasi,Proceedings of the Australasian Language Technology Association Workshop 2014,0,"We outline the first application of Native Language Identification (NLI) to Finnish learner data. NLI is the task of predicting an authorxe2x80x99s first language using writings in an acquired language. Using data from a new learner corpus of Finnish xe2x80x94 a language typology quite different from others previously investigated, with its morphological richness potentially causing difficulties xe2x80x94 we show that a combination of three feature types is useful for this task. Our system achieves an accuracy of 70% against a baseline of 20% for predicting an authorxe2x80x99s L1. Using the same features we can also distinguish non-native writings with an accuracy of 97%. This methodology can be useful for studying language transfer effects, developing teaching materials tailored to studentsxe2x80x99 native language and also forensic linguistics."
E14-4019,{C}hinese Native Language Identification,2014,16,21,2,1,3599,shervin malmasi,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"We present the first application of Native Language Identification (NLI) to nonEnglish data. Motivated by theories of language transfer, NLI is the task of identifying a writerxe2x80x99s native language (L1) based on their writings in a second language (the L2). An NLI system was applied to Chinese learner texts using topicindependent syntactic models to assess their accuracy. We find that models using part-of-speech tags, context-free grammar production rules and function words are highly effective, achieving a maximum accuracy of 71% . Interestingly, we also find that when applied to equivalent English data, the model performance is almost identical. This finding suggests a systematic pattern of cross-linguistic transfer may exist, where the degree of transfer is independent of the L1 and L2."
D14-1144,Language Transfer Hypotheses with Linear {SVM} Weights,2014,29,25,2,1,3599,shervin malmasi,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Language transfer, the characteristic second language usage patterns caused by native language interference, is investigated by Second Language Acquisition (SLA) researchers seeking to find overused and underused linguistic features. In this paper we develop and present a methodology for deriving ranked lists of such features. Using very large learner data, we show our methodxe2x80x99s ability to find relevant candidates using sophisticated linguistic features. To illustrate its applicability to SLA research, we formulate plausible language transfer hypotheses supported by current evidence. This is the first work to extend Native Language Identification to a broader linguistic interpretation of learner data and address the automatic extraction of underused features on a per-native language basis."
W13-4901,Working with a small dataset - semi-supervised dependency parsing for {I}rish,2013,24,5,3,1,14284,teresa lynn,Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages,0,"We present a number of semi-supervised parsing experiments on the Irish language carried out using a small seed set of manually parsed trees and a larger, yet still relatively small, set of unlabelled sentences. We take two popular dependency parsers xe2x80x90 one graph-based and one transition-based xe2x80x90 and compare results for both. Results show that using semisupervised learning in the form of self-training and co-training yields only very modest improvements in parsing accuracy. We also try to use morphological information in a targeted way and fail to see any improvements."
W13-1716,{NLI} Shared Task 2013: {MQ} Submission,2013,27,27,3,1,3599,shervin malmasi,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Our submission for this NLI shared task used for the most part standard features found in recent work. Our focus was instead on two other aspects of our system: at a high level, on possible ways of constructing ensembles of multiple classifiers; and at a low level, on the granularity of part-of-speech tags used as features. We found that the choice of ensemble combination method did not lead to much difference in results, although exploiting the varying behaviours of linear versus logistic regression SVM classifiers could be promising in future work; but part-of-speech tagsets showed noticeable differences. We also note that the overall architecture, with its feature set and ensemble approach, had an accuracy of 83.1% on the test set when trained on both the training data and development data supplied, close to the best result of the task. This suggests that basically throwing together all the features of previous work will achieve roughly the state of the art."
U12-1005,Active Learning and the {I}rish Treebank,2012,28,7,3,1,14284,teresa lynn,Proceedings of the Australasian Language Technology Association Workshop 2012,0,"We report on our ongoing work in developing the Irish Dependency Treebank, describe the results of two Inter annotator Agreement (IAA) studies, demonstrate improvements in annotation consistency which have a knock-on effect on parsing accuracy, and present the final set of dependency labels. We then go on to investigate the extent to which active learning can play a role in treebank and parser development by comparing an active learning bootstrapping approach to a passive approach in which sentences are chosen at random for manual revision. We show that active learning outperforms passive learning, but when annotation effort is taken into account, it is not clear how much of an advantage the active learning approach has. Finally, we present results which suggest that adding automatic parses to the training data along with manually revised parses in an active learning setup does not greatly affect parsing accuracy."
U12-1007,Valence Shifting: Is It A Valid Task?,2012,-1,-1,2,1,42580,mary gardiner,Proceedings of the Australasian Language Technology Association Workshop 2012,0,None
lynn-etal-2012-irish,{I}rish Treebanking and Parsing: A Preliminary Evaluation,2012,40,9,5,1,14284,teresa lynn,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Language resources are essential for linguistic research and the development of NLP applications. Low-density languages, such as Irish, therefore lack significant research in this area. This paper describes the early stages in the development of new language resources for Irish â namely the first Irish dependency treebank and the first Irish statistical dependency parser. We present the methodology behind building our new treebank and the steps we take to leverage upon the few existing resources. We discuss language-specific choices made when defining our dependency labelling scheme, and describe interesting Irish language characteristics such as prepositional attachment, copula, and clefting. We manually develop a small treebank of 300 sentences based on an existing POS-tagged corpus and report an inter-annotator agreement of 0.7902. We train MaltParser to achieve preliminary parsing results for Irish and describe a bootstrapping approach for further stages of development."
D12-1064,Exploring {A}daptor {G}rammars for Native Language Identification,2012,17,24,2,1,41048,szemeng wong,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"The task of inferring the native language of an author based on texts written in a second language has generally been tackled as a classification problem, typically using as features a mix of n-grams over characters and part of speech tags (for small and fixed n) and unigram function words. To capture arbitrarily long n-grams that syntax-based approaches have suggested are useful, adaptor grammars have some promise. In this work we investigate their extension to identifying n-gram collocations of arbitrary length over a mix of PoS tags and words, using both maxent and induced syntactic language model approaches to classification. After presenting a new, simple baseline, we show that learned collocations used as features in a maxent model perform better still, but that the story is more mixed for the syntactic language model."
C12-1111,Is Bad Structure Better Than No Structure?: Unsupervised Parsing for Realisation Ranking,2012,35,1,2,0,43773,yasaman motazedi,Proceedings of {COLING} 2012,0,"In natural language generation using symbolic grammars, state-of-the-art realisation rankers use statistical models incorporating both language model and structural features. The rankers depend on multiple structures produced by the particular large-scale symbolic grammars to rank the output; for languages with smaller resources and in-development grammars, we look at the feasibility of an alternative source of structural features, unsupervised parsers. We show that, in spite of their lower quality of structure, raw sets of unsupervised parse features can be helpful with smaller language models; and that the parses do contain particular elements that can be highly useful, improving performance on our classification task by up to 10% on 60% of the test set leading to an overall improvement under a back-off model. Title and Abstract in French Une mauvaise structure est-elle mieux que pas de structure du tout? Lxe2x80x99analyse non supervisee pour la selection des realisations Dans plusieurs systemes recents de generation de texte bases sur des grammaires symboliques, les resultats sont ordonnes selon leur acceptabilite par des modeles statistiques qui incorporent des modeles de Markov et des traits structurels. Ces modules dxe2x80x99ordonnancement dependent de diverses structures produites par la grammaire, ce qui presuppose une grammaire suffisamment developpee. Pour les langues a faibles ressources ou pour les grammaires en cours de developpement, nous etudions ici la viabilite dxe2x80x99une source alternative de traits structurels: les analyseurs non supervises. Nous demontrons que, en depit de la faible qualite des structures produites, elles contiennent des elements qui peuvent etre tres utiles pour les langues peu dotees, permettant dxe2x80x99ameliorer de 10% la performance de notre classificateur pour 60% des phrases de notre corpus de test."
W11-2828,Detecting Interesting Event Sequences for Sports Reporting,2011,15,8,2,0,25263,franccois lareau,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"Hand-crafted approaches to content determination are expensive to port to new domains. Machine-learned approaches, on the other hand, tend to be limited to relatively simple selection of items from data sets. We observe that in time series domains, textual descriptions often aggregate a series of events into a compact description. We present a simple technique for automatically determining sequences of events that are worth reporting, and evaluate its effectiveness."
U11-1013,Collocations in Multilingual Natural Language Generation: Lexical Functions meet {L}exical {F}unctional {G}rammar,2011,27,6,2,0,25263,franccois lareau,Proceedings of the Australasian Language Technology Association Workshop 2011,0,"In a collocation, the choice of one lexical item depends on the choice made for another. This poses a problem for simple approaches to lexicalisation in natural language generation systems. In the Meaning-Text framework, recurrent patterns of collocations have been characterised by lexical functions, which offer an elegant way of describing these relationships. Previous work has shown that using lexical functions in the context of multilingual natural language generation allows for a more efficient development of linguistic resources. We propose a way to encode lexical functions in the Lexical Functional Grammar framework."
U11-1015,Topic Modeling for Native Language Identification,2011,20,13,2,1,41048,szemeng wong,Proceedings of the Australasian Language Technology Association Workshop 2011,0,"Native language identification (NLI) is the task of determining the native language of an author writing in a second language. Several pieces of earlier work have found that features such as function words, part-of-speech n-grams and syntactic structure are helpful in NLI, perhaps representing characteristic errors of different native language speakers. This paper looks at the idea of using Latent Dirichlet Allocation as a feature clustering technique over lexical features to see whether there is any evidence that these smaller-scale features do cluster into more coherent latent factors, and investigates their effect in a classification task. We find that although (not unexpectedly) classification accuracy decreases, there is some evidence of coherent clustering, which could help with much larger syntactic feature spaces."
P11-2067,Clause Restructuring For {SMT} Not Absolutely Helpful,2011,14,5,2,1,44619,susan howlett,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"There are a number of systems that use a syntax-based reordering step prior to phrase-based statistical MT. An early work proposing this idea showed improved translation performance, but subsequent work has had mixed results. Speculations as to cause have suggested the parser, the data, or other factors. We systematically investigate possible factors to give an initial answer to the question: Under what conditions does this use of syntax help PSMT?"
D11-1148,Exploiting Parse Structures for Native Language Identification,2011,42,57,2,1,41048,szemeng wong,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Attempts to profile authors according to their characteristics extracted from textual data, including native language, have drawn attention in recent years, via various machine learning approaches utilising mostly lexical features. Drawing on the idea of contrastive analysis, which postulates that syntactic errors in a text are to some extent influenced by the native language of an author, this paper explores the usefulness of syntactic features for native language identification. We take two types of parse substructure as features---horizontal slices of trees, and the more general feature schemas from discriminative parse reranking---and show that using this kind of syntactic feature results in an accuracy score in classification of seven native languages of around 80%, an error reduction of more than 30%."
U10-1007,Dual-Path Phrase-Based Statistical Machine Translation,2010,20,2,2,1,44619,susan howlett,Proceedings of the Australasian Language Technology Association Workshop 2010,0,"Preceding a phrase-based statistical machine translation (PSMT) system by a syntactically-informed reordering preprocessing step has been shown to improve overall translation performance compared to a baseline PSMT system. However, the improvement is not seen for every sentence. We use a lattice input to a PSMT system in order to translate simultaneously across both original and reordered versions of a sentence, and include a number of confidence features to support the system in choosing on a sentence-by-sentence basis whether to use the reordering process. In German-to-English translation, our best system achieves a BLEU score of 21.39, an improvement of 0.62."
U10-1011,Parser Features for Sentence Grammaticality Classification,2010,18,14,2,1,41048,szemeng wong,Proceedings of the Australasian Language Technology Association Workshop 2010,0,"Automatically judging sentences for their grammaticality is potentially useful for several purposes xe2x80x94 evaluating language technology systems, assessing language competence of second or foreign language learners, and so on. Previous work has examined parser xe2x80x98byproductsxe2x80x99, in particular parse probabilities, to distinguish grammatical sentences from ungrammatical ones. The aim of the present paper is to examine whether the primary output of a parser, which we characterise via CFG production rules embodied in a parse, contains useful information for sentence grammaticality classification; and also to examine which feature selection metrics are most useful in this task. Our results show that using gold standard production rules alone can improve over using parse probabilities alone. Combining parser-produced production rules with parse probabilities further produces an improvement of 1.6% on average in the overall classification accuracy."
W09-2508,Using Hypernymy Acquisition to Tackle (Part of) Textual Entailment,2009,23,6,2,0,46957,elena akhmatova,Proceedings of the 2009 Workshop on Applied Textual Inference ({T}ext{I}nfer),0,"Within the task of Recognizing Textual Entailment, various existing work has proposed the idea that tackling specific subtypes of entailment could be more productive than taking a generic approach to entailment. In this paper we look at one such subtype, where the entailment involves hypernymy relations, often found in Question Answering tasks. We investigate current work on hypernymy acquisition, and show that adapting one such approach leads to a marked improvement in entailment classification accuracy."
W09-2310,Coupling Hierarchical Word Reordering and Decoding in Phrase-Based Statistical Machine Translation,2009,25,3,3,0,17603,maxim khalilov,Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation ({SSST}-3) at {NAACL} {HLT} 2009,0,"In this paper, we start with the existing idea of taking reordering rules automatically derived from syntactic representations, and applying them in a preprocessing step before translation to make the source sentence structurally more like the target; and we propose a new approach to hierarchically extracting these rules. We evaluate this, combined with a lattice-based decoding, and show improvements over state-of-the-art distortion models."
U09-1008,Contrastive Analysis and Native Language Identification,2009,23,46,2,1,41048,szemeng wong,Proceedings of the Australasian Language Technology Association Workshop 2009,0,"Attempts to profile authors based on their characteristics, including native language, have drawn attention in recent years, via several approaches using machine learning with simple features. In this paper we investigate the potential usefulness to this task of contrastive analysis from second language acquistion research, which postulates that the (syntactic) errors in a text are influenced by an authorxe2x80x99s native language. We explore this, first, by conducting an analysis of three syntactic error types, through hypothesis testing and machine learning; and second, through adding in these errors as features to the replication of a previous machine learning approach. This preliminary study provides some support for the use of this kind of syntactic errors as a clue to identifying the native language of an author."
E09-1097,Improving Grammaticality in Statistical Sentence Generation: Introducing a Dependency Spanning Tree Algorithm with an Argument Satisfaction Model,2009,22,28,2,1,3122,stephen wan,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"Abstract-like text summarisation requires a means of producing novel summary sentences. In order to improve the grammaticality of the generated sentence, we model a global (sentence) level syntactic structure. We couch statistical sentence generation as a spanning tree problem in order to search for the best dependency tree spanning a set of chosen words. We also introduce a new search algorithm for this task that models argument satisfaction to improve the linguistic validity of the generated tree. We treat the allocation of modifiers to heads as a weighted bipartite graph matching (or assignment) problem, a well studied problem in graph theory. Using BLEU to measure performance on a string regeneration task, we found an improvement, illustrating the benefit of the spanning tree approach armed with an argument satisfaction model."
2009.eamt-1.27,A New Subtree-Transfer Approach to Syntax-Based Reordering for Statistical Machine Translation,2009,27,7,3,0,17603,maxim khalilov,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,"In this paper we address the problem of translating between languages with word order disparity. The idea of augmenting statistical machine translation (SMT) by using a syntax-based reordering step prior to translation, proposed in recent years, has been quite successful in improving translation quality. We present a new technique for extracting syntax-based reordering rules, which are derived through a syntactically augmented alignment of source and target texts. The parallel corpus with reordered source side is then passed to an N -gram-based machine translation system and the obtained results are contrasted with a monotone system performance. In experiments, we show significant improvement for the Chinese-to-English translation task."
U08-1021,Morphosyntactic Target Language Matching in Statistical Machine Translation,2008,16,1,2,1,44667,simon zwarts,Proceedings of the Australasian Language Technology Association Workshop 2008,0,"While the intuition that morphological preprocessing of languages in various applications can be beneficial appears to be often true, especially in the case of morphologically richer languages, it is not always the case. Previous work on translation between Nordic languages, including the morphologically rich Finnish, found that morphological analysis and preprocessing actually led to a decrease in translation quality below that of the unprocessed baseline. In this paper we investigate the proposition that the effect on translation quality depends on the kind of morphological preprocessing; and in particular that a specific kind of morphological preprocessing before translation could improve translation quality, a preprocessing that first transforms the source language to look more like the target, adapted from work on preprocessing via syntactically motivated reordering. We show that this is indeed the case in translating from Finnish, and that the results hold for different target languages and different morphological analysers."
D08-1057,Seed and Grow: {A}ugmenting Statistically Generated Summary Sentences using Schematic Word Patterns,2008,21,7,3,1,3122,stephen wan,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"We examine the problem of content selection in statistical novel sentence generation. Our approach models the processes performed by professional editors when incorporating material from additional sentences to support some initially chosen key summary sentence, a process we refer to as Sentence Augmentation. We propose and evaluate a method called Seed and Grow for selecting such auxiliary information. Additionally, we argue that this can be performed using schemata, as represented by word-pair co-occurrences, and demonstrate its use in statistical summary sentence generation. Evaluation results are supportive, indicating that a schemata model significantly improves over the baseline."
C08-1145,Choosing the Right Translation: A Syntactically Informed Classification Approach,2008,21,17,2,1,44667,simon zwarts,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"One style of Multi-Engine Machine Translation architecture involves choosing the best of a set of outputs from different systems. Choosing the best translation from an arbitrary set, even in the presence of human references, is a difficult problem; it may prove better to look at mechanisms for making such choices in more restricted contexts.n n In this paper we take a classification-based approach to choosing between candidates from syntactically informed translations. The idea is that using multiple parsers as part of a classifier could help detect syntactic problems in this context that lead to bad translations; these problems could be detected on either the source side---perhaps sentences with difficult or incorrect parses could lead to bad translations---or on the target side---perhaps the output quality could be measured in a more syntactically informed way, looking for syntactic abnormalities.n n We show that there is no evidence that the source side information is useful. However, a target-side classifier, when used to identify particularly bad translation candidates, can lead to significant improvements in BLEU score. Improvements are even greater when combined with existing language and alignment model approaches."
W07-2205,The Impact of Deep Linguistic Processing on Parsing Technology,2007,2,7,2,0,1468,timothy baldwin,Proceedings of the Tenth International Conference on Parsing Technologies,0,"As the organizers of the ACL 2007 Deep Linguistic Processing workshop (Baldwin et al., 2007), we were asked to discuss our perspectives on the role of current trends in deep linguistic processing for parsing technology. We are particularly interested in the ways in which efficient, broad coverage parsing systems for linguistically expressive grammars can be built and integrated into applications which require richer syntactic structures than shallow approaches can provide. This often requires hybrid technologies which use shallow or statistical methods for pre- or post-processing, to extend coverage, or to disambiguate the output."
U07-1004,Entailment due to Syntactically Encoded Semantic Relationships,2007,13,1,2,0,46957,elena akhmatova,Proceedings of the Australasian Language Technology Workshop 2007,0,The majority of the state-of-the-art approaches to recognizing textual entailment focus on defining a generic approach to RTE. A generic approach never works well for every single entailment pair: there are entailment pairs that are recognized poorly by all the generic systems. Automatic identification of such entailment pairs and applying to them an RTE algorithm that is specific to them could thus increase an overall performance of an entailment engine (that in this case will combine a generic RTE algorithm with a number of RTE algorithms for the problematic entailment pairs). We identify one subtype of entailment pairs and develop atwo-part probabilistic model for their classification into true and false entailments and evaluate it relative both to a baseline and to the RTE systems. We show that the model performs better than the baseline and the average of the systems from the RTE2on both the balanced and unbalanced datasets we have created for evaluation.
U07-1007,Exploring Approaches to Discriminating among Near-Synonyms,2007,13,10,2,1,42580,mary gardiner,Proceedings of the Australasian Language Technology Workshop 2007,0,"Near-synonyms are words that mean approximately the same thing, and which tend to be assigned to the same leaf in ontologies such as WordNet. However, they can differ from each other subtly in both meaning and usagexe2x80x94consider the pair of nearsynonyms frugal and stingyxe2x80x94and therefore choosing the appropriate near-synonym for a given context is not a trivial problem. Initial work by Edmonds (1997) suggested that corpus statistics methods would not be particularly effective, and led to subsequent work adopting methods based on specific lexical resources. In earlier work (Gardiner and Dras, 2007) we discussed the hypothesis that some kind of corpus statistics approach may still be effective in some situations, particularly if the near-synonyms differ in sentiment from each other, and we presented some preliminary confirmation of the truth of this hypothesis. This suggests that problems involving this type of nearsynonym may be particularly amenable to corpus statistics methods. In this paper we investigate whether this result extends to a different corpus statistics method and in addition we analyse the results with respect to a possible confounding factor discussed in the previous work: the skewness of the sets of near synonyms. Our results show that the relationship between success in prediction and the nature of the near-synonyms is method dependent and that skewness is a more significant factor."
U07-1019,Statistical Machine Translation of {A}ustralian Aboriginal Languages: Morphological Analysis with Languages of Differing Morphological Richness,2007,11,3,2,1,44667,simon zwarts,Proceedings of the Australasian Language Technology Workshop 2007,0,"Morphological analysis is often used during preprocessing in Statistical Machine Translation. Existing work suggests that the benefit would be greater for more highly inflected languages, although to our knowledge this has not been systematically tested on languages with comparable morphology. In this paper, two comparable languages with different amounts of inflection are tested, to see if the benefits of morphology used during the translation process, depends on the morphological richness of the language. For this work we use indigenous Australian languages: most Australian Aboriginal languages are highly inflected, where words can take a considerable number of postfixes when compared to Indo-European languages, and for languages in the same (Pama Nyungan) family, the morphological system works similarly. We show in this preliminary work that morphological analysis clearly benefits the richer of the two languages investigated, but is more equivocal in the case of the other."
P07-1044,{GLEU}: Automatic Evaluation of Sentence-Level Fluency,2007,16,33,2,0,49192,andrew mutton,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"In evaluating the output of language technology applicationsxe2x80x94MT, natural language generation, summarisationxe2x80x94automatic evaluation techniques generally conflate measurement of faithfulness to source content with fluency of the resulting text. In this paper we develop an automatic evaluation metric to estimate fluency alone, by examining the use of parser outputs as metrics, and show that they correlate with human judgements of generated text fluency. We then develop a machine learner based on these, and show that this performs better than the individual parser metrics, approaching a lower bound on human performance. We finally look at different language models for generating sentences, and show that while individual parser metrics can be xe2x80x98fooledxe2x80x99 depending on generation method, the machine learner provides a consistent estimator of fluency."
2007.mtsummit-papers.74,Syntax-based word reordering in phrase-based statistical machine translation: why does it work?,2007,16,14,2,1,44667,simon zwarts,Proceedings of Machine Translation Summit XI: Papers,0,None
U06-1019,Using Dependency-Based Features to Take the {'}Para-farce{'} out of Paraphrase,2006,19,103,2,1,3122,stephen wan,Proceedings of the Australasian Language Technology Workshop 2006,0,"As research in text-to-text paraphrase generation progresses, it has the potential to improve the quality of generated text. However, the use of paraphrase generation methods creates a secondary problem. We must ensure that generated novel sentences are not inconsistent with the text from which it was generated. We propose a machine learning approach be used to filter out inconsistent novel sentences, or False Paraphrases. To train such a filter, we use the Microsoft Research Paraphrase corpus and investigate whether features based on syntactic dependencies can aid us in this task. Like Finch et al. (2005), we obtain a classification accuracy of 75.6%, the best known performance for this corpus. We also examine the strengths and weaknesses of dependency based features and conclude that they may be useful in more accurately classifying cases of False Paraphrase."
U06-1021,This Phrase-Based {SMT} System is Out of Order: Generalised Word Reordering in Machine Translation,2006,12,6,2,1,44667,simon zwarts,Proceedings of the Australasian Language Technology Workshop 2006,0,"Many natural language processes have some degree of preprocessing of data: tokenisation, stemming and so on. In the domain of Statistical Machine Translation it has been shown that word reordering as a preprocessing step can help the translation process. Recently, hand-written rules for reordering in Germanxe2x80x93English translation have shown good results, but this is clearly a labour-intensive and language pair-specific approach. Two possible sources of the observed improvement are that (1) the reordering explicitly matches the syntax of the source language more closely to that of the target language, or that (2) it fits the data better to the mechanisms of phrasal SMT; but it is not clear which. In this paper, we apply a general principle based on dependency distance minimisation to produce reorderings. Our languageindependent approach achieves half of the improvement of a reimplementation of the handcrafted approach, and suggests that reason (2) is a possible explanation for why that reordering approach works. Help you I can, yes. Jedi Master Yoda"
W05-1628,Searching for Grammaticality: Propagating Dependencies in the {V}iterbi Algorithm,2005,12,11,3,1,3122,stephen wan,Proceedings of the Tenth {E}uropean Workshop on Natural Language Generation ({ENLG}-05),0,None
U05-1015,Formal Grammars for Linguistic Treebank Queries,2005,15,0,1,1,12694,mark dras,Proceedings of the Australasian Language Technology Workshop 2005,0,"There has been recent interest in looking at what is required for a tree query language for linguistic corpora. One approach is to start from existing formal machinery, such as tree grammars and automata, to see what kind of machine is an appropriate underlying one for the query language. The goal of the paper is then to examine what is an appropriate machine for a linguistic tree query language, with a view to future work defining a query language based on it. In this paper we review work relating XPath to regular tree grammars, and as the paperxe2x80x99s first contribution show how regular tree grammars can also be a basis for extensions proposed for XPath for common linguistic corpus querying. As the paperxe2x80x99s second contribution we demonstrate that, on the other hand, regular tree grammars cannot describe a number of structures of interest; we then show that, instead, a slightly more powerful machine is appropriate, and indicate how linguistic tree query languages might be augmented to include this extra power."
I05-5012,Towards Statistical Paraphrase Generation: Preliminary Evaluations of Grammaticality,2005,11,7,2,1,3122,stephen wan,Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005),0,"Summary sentences are often paraphrases of existing sentences. They may be made up of recycled fragments of text taken from important sentences in an input document. We investigate the use of a statistical sentence generation technique that recombines words probabilistically in order to create new sentences. Given a set of event-related sentences, we use an extended version of the Viterbi algorithm which employs dependency relation and bigram probabilities to find the most probable summary sentence. Using precision and recall metrics for verb arguments as a measure of grammaticality, we find that our system performs better than a bigram baseline, producing fewer spurious verb arguments."
2004.tmi-1.4,Non-contiguous tree parsing,2004,-1,-1,1,1,12694,mark dras,Proceedings of the 10th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
W03-1202,Using Thematic Information in Statistical Headline Generation,2003,21,17,2,1,3122,stephen wan,Proceedings of the {ACL} 2003 Workshop on Multilingual Summarization and Question Answering,0,"We explore the problem of single sentence summarisation. In the news domain, such a summary might resemble a headline. The headline generation system we present uses Singular Value Decomposition (SVD) to guide the generation of a headline towards the theme that best represents the document to be summarised. In doing so, the intuition is that the generated summary will more accurately reflect the content of the source document. This paper presents SVD as an alternative method to determine if a word is a suitable candidate for inclusion in the headline. The results of a recall based evaluation comparing three different strategies to word selection, indicate that thematic information does help improve recall."
U03-1012,Straight to the point: Discovering themes for summary generation,2003,19,3,2,1,3122,stephen wan,Proceedings of the Australasian Language Technology Workshop 2003,0,"This paper presents our approach to the problem of single sentence summarisation. We investigate the use of Singular Value Decomposition (SVD) to guide the generation of a summary towards the theme that is the focus of the document to be summarised. In doing so, the intuition is that the generated summary will more accurately reflect the content of the source document. Currently, we operate in the news domain and at present, our summaries are modelled on headlines. This paper presents SVD as an alternative method to determine if a word is a suitable candidate for inclusion in the headline. The results of a recall based evaluation comparing three different strategies to word selection, indicate that thematic information does help improve recall."
W02-2230,{K}orean-{E}nglish {MT} and {S}-{TAG},2002,15,3,1,1,12694,mark dras,Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+6),0,None
W00-2008,Some remarks on an extension of synchronous {TAG},2000,12,6,3,0,3180,david chiang,Proceedings of the Fifth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+5),0,"We explore some properties of the synchronous formalism introduced in Dras (1999), showing that it handles an interaction, noted in Schuler (1999), between bridge and raising verbs which is problematic for synchronous TAG. We also show that it has greater formal power than synchronous TAG and discuss its computational complexity."
W00-2035,How problematic are clitics for {S}-{TAG} translation?,2000,4,6,1,1,12694,mark dras,Proceedings of the Fifth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+5),0,None
P00-1057,Multi-Component {TAG} and Notions of Formal Power,2000,15,8,3,0,7110,william schuler,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents a restricted version of Set-Local Multi-Component TAGs (Weir, 1988) which retains the strong generative capacity of Tree-Local Multi-Component TAG (i.e. produces the same derived structures) but has a greater derivational generative capacity (i.e. can derive those structures in more ways). This formalism is then applied as a framework for integrating dependency and constituency based linguistic representations."
P99-1011,A Meta-Level Grammar: Redefining Synchronous {TAG} for Translation and Paraphrase,1999,12,22,1,1,12694,mark dras,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"In applications such as translation and paraphrase, operations are carried out on grammars at the meta level. This paper shows how a meta-grammar, defining structure at the meta level, is useful in the case of such operations; in particular, how it solves problems in the current definition of Synchronous TAG (Shieber, 1994) caused by ignoring such structure in mapping between grammars, for applications such as translation. Moreover, essential properties of the formalism remain unchanged."
P97-1070,Representing Paraphrases Using Synchronous {TAG}s,1997,6,4,1,1,12694,mark dras,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,This paper looks at representing paraphrases using the formalism of Synchronous TAGs; it looks particularly at comparisons with machine translation and the modifications it is necessary to make to Synchronous TAGs for paraphrasing. A more detailed version is in Dras (1997a).
