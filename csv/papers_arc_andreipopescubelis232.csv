2021.findings-emnlp.224,Subword Mapping and Anchoring across Languages,2021,-1,-1,2,0,6977,giorgos vernikos,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"State-of-the-art multilingual systems rely on shared vocabularies that sufficiently cover all considered languages. To this end, a simple and frequently used approach makes use of subword vocabularies constructed jointly over several languages. We hypothesize that such vocabularies are suboptimal due to false positives (identical subwords with different meanings across languages) and false negatives (different subwords with similar meanings). To address these issues, we propose Subword Mapping and Anchoring across Languages (SMALA), a method to construct bilingual subword vocabularies. SMALA extracts subword alignments using an unsupervised state-of-the-art mapping technique and uses them to create cross-lingual anchors based on subword similarities. We demonstrate the benefits of SMALA for cross-lingual natural language inference (XNLI), where it improves zero-shot transfer to an unseen language without task-specific data, but only by sharing subword embeddings. Moreover, in neural machine translation, we show that joint subword vocabularies obtained with SMALA lead to higher BLEU scores on sentences that contain many false positives and false negatives."
2020.lrec-1.672,Chat or Learn: a Data-Driven Robust Question-Answering System,2020,-1,-1,2,0,17991,gabriel luthier,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present a voice-based conversational agent which combines the robustness of chatbots and the utility of question answering (QA) systems. Indeed, while data-driven chatbots are typically user-friendly but not goal-oriented, QA systems tend to perform poorly at chitchat. The proposed chatbot relies on a controller which performs dialogue act classification and feeds user input either to a sequence-to-sequence chatbot or to a QA system. The resulting chatbot is a spoken QA application for the Google Home smart speaker. The system is endowed with general-domain knowledge from Wikipedia articles and uses coreference resolution to detect relatedness between questions. We present our choices of data sets for training and testing the components, and present the experimental results that helped us optimize the parameters of the chatbot. In particular, we discuss the appropriateness of using the SQuAD dataset for evaluating end-to-end QA, in the light of our system{'}s behavior."
2020.isa-1.7,A Consolidated Dataset for Knowledge-based Question Generation using Predicate Mapping of Linked Data,2020,-1,-1,3,0,18934,johanna melly,16th Joint ACL - ISO Workshop on Interoperable Semantic Annotation PROCEEDINGS,0,"In this paper, we present the ForwardQuestions data set, made of human-generated questions related to knowledge triples. This data set results from the conversion and merger of the existing SimpleDBPediaQA and SimpleQuestionsWikidata data sets, including the mapping of predicates from DBPedia to Wikidata, and the selection of {`}forward{'} questions as opposed to {`}backward{'} ones. The new data set can be used to generate novel questions given an unseen Wikidata triple, by replacing the subjects of existing questions with the new one and then selecting the best candidate questions using semantic and syntactic criteria. Evaluation results indicate that the question generation method using ForwardQuestions improves the quality of questions by about 20{\%} with respect to a baseline not using ranking criteria."
Q18-1044,Integrating Weakly Supervised Word Sense Disambiguation into Neural Machine Translation,2018,0,2,4,1,28965,xiao pu,Transactions of the Association for Computational Linguistics,0,"This paper demonstrates that word sense disambiguation (WSD) can improve neural machine translation (NMT) by widening the source context considered when modeling the senses of potentially ambiguous words. We first introduce three adaptive clustering algorithms for WSD, based on k-means, Chinese restaurant processes, and random walks, which are then applied to large word contexts represented in a low-rank space and evaluated on SemEval shared-task data. We then learn word vectors jointly with sense vectors defined by our best WSD method, within a state-of-the-art NMT system. We show that the concatenation of these vectors, and the use of a sense selection mechanism based on the weighted average of sense vectors, outperforms several baselines including sense-aware ones. This is demonstrated by translation on five language pairs. The improvements are more than 1 BLEU point over strong NMT baselines, +4{\%} accuracy over all ambiguous nouns and verbs, or +20{\%} when scored manually over several challenging words."
N18-1124,Self-Attentive Residual Decoder for Neural Machine Translation,2018,0,5,4,1,29447,lesly werlen,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Neural sequence-to-sequence networks with attention have achieved remarkable performance for machine translation. One of the reasons for their effectiveness is their ability to capture relevant source-side contextual information at each time-step prediction through an attention mechanism. However, the target-side context is solely based on the sequence model which, in practice, is prone to a recency bias and lacks the ability to capture effectively non-sequential dependencies among words. To address this limitation, we propose a target-side-attentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. The residual learning facilitates the flow of information from the distant past and is able to emphasize any of the previously translated words, hence it gains access to a wider context. The proposed model outperforms a neural MT baseline as well as a memory and self-attention network on three language pairs. The analysis of the attention learned by the decoder confirms that it emphasizes a wider context, and that it captures syntactic-like structures."
L18-1597,Machine Translation of Low-Resource Spoken Dialects: Strategies for Normalizing {S}wiss {G}erman,2018,-1,-1,2,0,30152,pierreedouard honnet,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-4802,Validation of an Automatic Metric for the Accuracy of Pronoun Translation ({APT}),2017,12,7,2,1,29447,lesly werlen,Proceedings of the Third Workshop on Discourse in Machine Translation,0,"In this paper, we define and assess a reference-based metric to evaluate the accuracy of pronoun translation (APT). The metric automatically aligns a candidate and a reference translation using GIZA++ augmented with specific heuristics, and then counts the number of identical or different pronouns, with provision for legitimate variations and omitted pronouns. All counts are then combined into one score. The metric is applied to the results of seven systems (including the baseline) that participated in the DiscoMT 2015 shared task on pronoun translation from English to French. The APT metric reaches around 0.993-0.999 Pearson correlation with human judges (depending on the parameters of APT), while other automatic metrics such as BLEU, METEOR, or those specific to pronouns used at DiscoMT 2015 reach only 0.972-0.986 Pearson correlation."
W17-4701,Sense-Aware Statistical Machine Translation using Adaptive Context-Dependent Clustering,2017,23,2,3,1,28965,xiao pu,Proceedings of the Second Conference on Machine Translation,0,None
W17-1505,Using Coreference Links to Improve {S}panish-to-{E}nglish Machine Translation,2017,0,2,2,1,29447,lesly werlen,Proceedings of the 2nd Workshop on Coreference Resolution Beyond {O}nto{N}otes ({CORBON} 2017),0,"In this paper, we present a proof-of-concept implementation of a coreference-aware decoder for document-level machine translation. We consider that better translations should have coreference links that are closer to those in the source text, and implement this criterion in two ways. First, we define a similarity measure between source and target coreference structures, by projecting the target ones onto the source and reusing existing coreference metrics. Based on this similarity measure, we re-rank the translation hypotheses of a baseline system for each sentence. Alternatively, to address the lack of diversity of mentions in the MT hypotheses, we focus on mention pairs and integrate their coreference scores with MT ones, resulting in post-editing decisions for mentions. The experimental results for Spanish to English MT on the AnCora-ES corpus show that the second approach yields a substantial increase in the accuracy of pronoun translation, with BLEU scores remaining constant."
I17-1102,Multilingual Hierarchical Attention Networks for Document Classification,2017,36,15,2,1,8895,nikolaos pappas,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Hierarchical attention networks have recently achieved remarkable performance for document classification in a given language. However, when multilingual document collections are considered, training such models separately for each language entails linear parameter growth and lack of cross-language transfer. Learning a single multilingual model with fewer parameters is therefore a challenging but potentially beneficial objective. To this end, we propose multilingual hierarchical attention networks for learning document structures, with shared encoders and/or shared attention mechanisms across languages, using multi-task learning and an aligned semantic space as input. We evaluate the proposed models on multilingual document classification with disjoint label sets, on a large dataset which we provide, with 600k news documents in 8 languages, and 5k labels. The multilingual models outperform monolingual ones in low-resource as well as full-resource settings, and use fewer parameters, thus confirming their computational efficiency and the utility of cross-language transfer."
E17-3029,The {SUMMA} Platform Prototype,2017,8,1,26,0,28433,renars liepins,Proceedings of the Software Demonstrations of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present the first prototype of the SUMMA Platform: an integrated platform for multilingual media monitoring. The platform contains a rich suite of low-level and high-level natural language processing technologies: automatic speech recognition of broadcast media, machine translation, automated tagging and classification of named entities, semantic parsing to detect relationships between entities, and automatic construction / augmentation of factual knowledge bases. Implemented on the Docker platform, it can easily be deployed, customised, and scaled to large volumes of incoming media streams."
E17-2100,Machine Translation of {S}panish Personal and Possessive Pronouns Using Anaphora Probabilities,2017,0,1,2,1,33006,ngoc luong,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"We implement a fully probabilistic model to combine the hypotheses of a Spanish anaphora resolution system with those of a Spanish-English machine translation system. The probabilities over antecedents are converted into probabilities for the features of translated pronouns, and are integrated with phrase-based MT using an additional translation model for pronouns. The system improves the translation of several Spanish personal and possessive pronouns into English, by solving translation divergencies such as {`}ella{'} vs. {`}she{'}/{`}it{'} or {`}su{'} vs. {`}his{'}/{`}her{'}/{`}its{'}/{`}their{'}. On a test set with 2,286 pronouns, a baseline system correctly translates 1,055 of them, while ours improves this by 41. Moreover, with oracle antecedents, possessives are translated with an accuracy of 83{\%}."
E17-1089,Consistent Translation of Repeated Nouns using Syntactic and Semantic Cues,2017,17,2,3,1,28965,xiao pu,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"We propose a method to decide whether two occurrences of the same noun in a source text should be translated consistently, i.e. using the same noun in the target text as well. We train and test classifiers that predict consistent translations based on lexical, syntactic, and semantic features. We first evaluate the accuracy of our classifiers intrinsically, in terms of the accuracy of consistency predictions, over a subset of the UN Corpus. Then, we also evaluate them in combination with phrase-based statistical MT systems for Chinese-to-English and German-to-English. We compare the automatic post-editing of noun translations with the re-ranking of the translation hypotheses based on the classifiers{'} output, and also use these methods in combination. This improves over the baseline and closes up to 50{\%} of the gap in BLEU scores between the baseline and an oracle classifier."
W16-6213,Human versus Machine Attention in Document Classification: A Dataset with Crowdsourced Annotations,2016,16,2,2,1,8895,nikolaos pappas,Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media,0,"Keywords: aspect-based sentiment analysis ; attention mechanism ; Document Classification Reference EPFL-CONF-222451 Record created on 2016-10-19, modified on 2017-05-10"
W16-3416,A Contextual Language Model to Improve Machine Translation of Pronouns by Re-ranking Translation Hypotheses,2016,15,2,2,1,33006,ngoc luong,Proceedings of the 19th Annual Conference of the {E}uropean Association for Machine Translation,0,None
W16-2345,Findings of the 2016 {WMT} Shared Task on Cross-lingual Pronoun Prediction,2016,20,11,9,0,5894,liane guillou,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"We describe the design, the evaluation setup, and the results of the 2016 WMT shared task on cross-lingual pronoun prediction. This is a classification task in which participants are asked to provi ..."
W16-2352,Pronoun Language Model and Grammatical Heuristics for Aiding Pronoun Prediction,2016,10,1,2,1,33006,ngoc luong,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
W16-2202,Improving Pronoun Translation by Modeling Coreference Uncertainty,2016,15,5,2,1,33006,ngoc luong,"Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers",0,None
L16-1355,Using the {TED} Talks to Evaluate Spoken Post-editing of Machine Translation,2016,28,0,2,0,5002,jeevanthi liyanapathirana,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents a solution to evaluate spoken post-editing of imperfect machine translation output by a human translator. We compare two approaches to the combination of machine translation (MT) and automatic speech recognition (ASR): a heuristic algorithm and a machine learning method. To obtain a data set with spoken post-editing information, we use the French version of TED talks as the source texts submitted to MT, and the spoken English counterparts as their corrections, which are submitted to an ASR system. We experiment with various levels of artificial ASR noise and also with a state-of-the-art ASR system. The results show that the combination of MT with ASR improves over both individual outputs of MT and ASR in terms of BLEU scores, especially when ASR performance is low."
W15-2513,Pronoun Translation and Prediction with or without Coreference Links,2015,18,6,3,1,33006,ngoc luong,Proceedings of the Second Workshop on Discourse in Machine Translation,0,"The Idiap NLP Group has participated in both DiscoMT 2015 sub-tasks: pronoun-focused translation and pronoun prediction. The system for the first sub-task combines two knowledge sources: gram matical constraints from the hypothesized coreference links, and candidate translations from an SMT decoder. The system for the second sub-task avoids hypothesizing a coreference link, and uses instead a large set of source-side and target-side features from the noun phrases surrounding the pronoun to train a pronoun predictor."
P15-3002,Leveraging Compounds to Improve Noun Phrase Translation from {C}hinese and {G}erman,2015,20,2,3,1,28965,xiao pu,Proceedings of the {ACL}-{IJCNLP} 2015 Student Research Workshop,0,"This paper presents a method to improve the translation of polysemous nouns, when a previous occurrence of the noun as the head of a compound noun phrase is available in a text. The occurrences are identified through pattern matching rules, which detect XY compounds followed closely by a potentially coreferent occurrence of Y , such as xe2x80x9cNordwand ... Wandxe2x80x9d. Two strategies are proposed to improve the translation of the second occurrence of Y : re-using the cached translation of Y from the XY compound, or post-editing the translation of Y using the head of the translation of XY . Experiments are performed on Chinese-toEnglish and German-to-French statistical machine translation, over the WIT3 and TextBerg corpora respectively, with 261 XY/Y pairs each. The results suggest that while the overall BLEU scores increase only slightly, the translations of the targeted polysemous nouns are significantly improved."
loaiciga-etal-2014-english,{E}nglish-{F}rench Verb Phrase Alignment in {E}uroparl for Tense Translation Modeling,2014,17,19,3,0,4241,sharid loaiciga,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents a method for verb phrase (VP) alignment in an English-French parallel corpus and its use for improving statistical machine translation (SMT) of verb tenses. The method starts from automatic word alignment performed with GIZA++, and relies on a POS tagger and a parser, in combination with several heuristics, in order to identify non-contiguous components of VPs, and to label the aligned VPs with their tense and voice on each side. This procedure is applied to the Europarl corpus, leading to the creation of a smaller, high-precision parallel corpus with about 320,000 pairs of finite VPs, which is made publicly available. This resource is used to train a tense predictor for translation from English into French, based on a large number of surface features. Three MT systems are compared: (1) a baseline phrase-based SMT; (2) a tense-aware SMT system using the above predictions within a factored translation model; and (3) a system using oracle predictions from the aligned VPs. For several tenses, such as the French {``}imparfait{''}, the tense-aware SMT system improves significantly over the baseline and is closer to the oracle system."
D14-1052,Explaining the Stars: Weighted Multiple-Instance Learning for Aspect-Based Sentiment Analysis,2014,40,27,2,1,8895,nikolaos pappas,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews. Each variable-length text is represented by several independent feature vectors; one word vector per sentence or paragraph. For learning from texts with known aspect ratings, the model performs multiple-instance regression (MIR) and assigns importance weights to each of the sentences or paragraphs of a text, uncovering their contribution to the aspect ratings. Next, the model is used to predict aspect ratings in previously unseen texts, demonstrating interpretability and explanatory power for its predictions. We evaluate the model on seven multi-aspect sentiment analysis data sets, improving over four MIR baselines and two strong bag-of-words linear models, namely SVR and Lasso, by more than 10% relative in terms of MSE."
C14-1056,Enforcing Topic Diversity in a Document Recommender for Conversations,2014,29,6,2,0,40246,maryam habibi,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"This paper addresses the problem of building concise, diverse and relevant lists of documents, which can be recommended to the participants of a conversation to fulfill their information needs without distracting them. These lists are retrieved periodically by submitting multiple implicit queries derived from the pronounced words. Each query is related to one of the topics identified in the conversation fragment preceding the recommendation, and is submitted to a search engine over the English Wikipedia. We propose in this paper an algorithm for diverse merging of these lists, using a submodular reward function that rewards the topical similarity of documents to the conversation words as well as their diversity. We evaluate the proposed method through crowdsourcing. The results show the superiority of the diverse merging technique over several others which not enforce the diversity of topics."
W13-3305,Detecting Narrativity to Improve {E}nglish to {F}rench Translation of Simple Past Verbs,2013,25,16,3,0.866667,39466,thomas meyer,Proceedings of the Workshop on Discourse in Machine Translation,0,"The correct translation of verb tenses ensures that the temporal ordering of events in the source text is maintained in the target text. This paper assesses the utility of automatically labeling English Simple Past verbs with a binary discursive feature, narrative vs. non-narrative, for statistical machine translation (SMT) into French. The narrativity feature, which helps deciding which of the French past tenses is a correct translation of the English Simple Past, can be assigned with about 70% accuracy (F1). The narrativity feature improves SMT by about 0.2 BLEU points when a factored SMT system is trained and tested on automatically labeled English-French data. More importantly, manual evaluation shows that verb tense translation and verb choice are improved by respectively 9.7% and 3.4% (absolute), leading to an overall improvement of verb translation of 17% (relative)."
P13-2115,Diverse Keyword Extraction from Conversations,2013,25,10,2,0,40246,maryam habibi,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"A new method for keyword extraction from conversations is introduced, which preserves the diversity of topics that are mentioned. Inspired from summarization, the method maximizes the coverage of topics that are recognized automatically in transcripts of conversation fragments. The method is evaluated on excerpts of the Fisher and AMI corpora, using a crowdsourcing platform to elicit comparative relevance judgments. The results demonstrate that the method outperforms two competitive baselines."
W12-0117,Using Sense-labeled Discourse Connectives for Statistical Machine Translation,2012,24,38,2,1,39466,thomas meyer,Proceedings of the Joint Workshop on Exploiting Synergies between Information Retrieval and Machine Translation ({ESIRMT}) and Hybrid Approaches to Machine Translation ({H}y{T}ra),0,"This article shows how the automatic disambiguation of discourse connectives can improve Statistical Machine Translation (SMT) from English to French. Connectives are firstly disambiguated in terms of the discourse relation they signal between segments. Several classifiers trained using syntactic and semantic features reach state-of-the-art performance, with F1 scores of 0.6 to 0.8 over thirteen ambiguous English connectives. Labeled connectives are then used into SMT systems either by modifying their phrase table, or by training them on labeled corpora. The best modified SMT systems improve the translation of connectives without degrading BLEU scores. A threshold-based SMT system using only high-confidence labels improves BLEU scores by 0.2--0.4 points."
popescu-belis-etal-2012-discourse,Discourse-level Annotation over {E}uroparl for Machine Translation: Connectives and Pronouns,2012,14,22,1,1,6978,andrei popescubelis,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes methods and results for the annotation of two discourse-level phenomena, connectives and pronouns, over a multilingual parallel corpus. Excerpts from Europarl in English and French have been annotated with disambiguation information for connectives and pronouns, for about 3600 tokens. This data is then used in several ways: for cross-linguistic studies, for training automatic disambiguation software, and ultimately for training and testing discourse-aware statistical machine translation systems. The paper presents the annotation procedures and their results in detail, and overviews the first systems trained on the annotated resources and their use for machine translation."
bunt-etal-2012-iso,{ISO} 24617-2: A semantically-based standard for dialogue annotation,2012,14,51,7,0.145504,16745,harry bunt,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper summarizes the latest, final version of ISO standard 24617-2 ``Semantic annotation framework, Part 2: Dialogue acts''''''''. Compared to the preliminary version ISO DIS 24617-2:2010, described in Bunt et al. (2010), the final version additionally includes concepts for annotating rhetorical relations between dialogue units, defines a full-blown compositional semantics for the Dialogue Act Markup Language DiAML (resulting, as a side-effect, in a different treatment of functional dependence relations among dialogue acts and feedback dependence relations); and specifies an optimally transparent XML-based reference format for the representation of DiAML annotations, based on the systematic application of the notion of `ideal concrete syntax'. We describe these differences and briefly discuss the design and implementation of an incremental method for dialogue act recognition, which proves the usability of the ISO standard for automatic dialogue annotation."
2012.amta-papers.20,Machine Translation of Labeled Discourse Connectives,2012,31,25,2,1,39466,thomas meyer,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"This paper shows how the disambiguation of discourse connectives can improve their automatic translation, while preserving the overall performance of statistical MT as measured by BLEU. State-of-the-art automatic classifiers for rhetorical relations are used prior to MT to label discourse connectives that signal those relations. These labels are used for MT in two ways: (1) by augmenting factored translation models; and (2) by using the probability distributions of labels in order to train and tune SMT. The improvement of translation quality is demonstrated using a new semi-automated metric for discourse connectives, on the English/French WMT10 data, while BLEU scores remain comparable to non-discourse-aware systems, due to the low frequency of discourse connectives."
2012.amta-caas14.1,Translating {E}nglish Discourse Connectives into {A}rabic: a Corpus-based Analysis and an Evaluation Metric,2012,14,7,2,0,18371,najeh hajlaoui,Fourth Workshop on Computational Approaches to Arabic-Script-based Languages,0,"Discourse connectives can often signal multiple discourse relations, depending on their context. The automatic identification of the Arabic translations of seven English discourse connectives shows how these connectives are differently translated depending on their actual senses. Automatic labelling of English source connectives can help a machine translation system to translate them more correctly. The corpus-based analysis of Arabic translations also enables the definition of a connective-specific evaluation metric for machine translation, which is here validated by human judges on sample English/Arabic translation data."
W11-2022,Multilingual Annotation and Disambiguation of Discourse Connectives for Machine Translation,2011,23,22,2,1,39466,thomas meyer,Proceedings of the {SIGDIAL} 2011 Conference,0,"Many discourse connectives can signal several types of relations between sentences. Their automatic disambiguation, i.e. the labeling of the correct sense of each occurrence, is important for discourse parsing, but could also be helpful to machine translation. We describe new approaches for improving the accuracy of manual annotation of three discourse connectives (two English, one French) by using parallel corpora. An appropriate set of labels for each connective can be found using information from their translations. Our results for automatic disambiguation are state-of-the-art, at up to 85% accuracy using surface features. Using feature analysis, contextual features are shown to be useful across languages and connectives."
W11-2045,A Just-in-Time Document Retrieval System for Dialogues or Monologues,2011,9,2,1,1,6978,andrei popescubelis,Proceedings of the {SIGDIAL} 2011 Conference,0,"The Automatic Content Linking Device is a just-in-time document retrieval system that monitors an ongoing dialogue or monologue and enriches it with potentially related documents from local repositories or from the Web. The documents are found using queries that are built from the dialogue words, obtained through automatic speech recognition. Results are displayed in real time to the dialogue participants, or to people watching a recorded dialogue or a talk. The system can be demonstrated in both settings."
W11-1211,How Comparable are Parallel Corpora? Measuring the Distribution of General Vocabulary and Connectives,2011,21,30,4,0,17415,bruno cartoni,Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web,0,"In this paper, we question the homogeneity of a large parallel corpus by measuring the similarity between various sub-parts. We compare results obtained using a general measure of lexical similarity based on xcfx872 and by counting the number of discourse connectives. We argue that discourse connectives provide a more sensitive measure, revealing differences that are not visible with the general measure. We also provide evidence for the existence of specific characteristics defining translated texts as opposed to non-translated ones, due to a universal tendency for explicitation."
W11-1105,Using a {W}ikipedia-based Semantic Relatedness Measure for Document Clustering,2011,20,8,2,0,3866,majid yazdani,Proceedings of {T}ext{G}raphs-6: Graph-based Methods for Natural Language Processing,0,"A graph-based distance between Wikipedia articles is defined using a random walk model, which estimates visiting probability (VP) between articles using two types of links: hyperlinks and lexical similarity relations. The VP to and from a set of articles is then computed, and approximations are proposed to make tractable the computation of semantic relatedness between every two texts in a large data set. The model is applied to document clustering on the 20 Newsgroups data set. Precision and recall are improved in comparison with previous textual distance algorithms."
P11-4014,A Speech-based Just-in-Time Retrieval System using Semantic Search,2011,16,13,1,1,6978,andrei popescubelis,Proceedings of the {ACL}-{HLT} 2011 System Demonstrations,0,"The Automatic Content Linking Device is a just-in-time document retrieval system which monitors an ongoing conversation or a monologue and enriches it with potentially related documents, including multimedia ones, from local repositories or from the Internet. The documents are found using keyword-based search or using a semantic similarity measure between documents and the words obtained from automatic speech recognition. Results are displayed in real time to meeting participants, or to users watching a recorded lecture or conversation."
bunt-etal-2010-towards,Towards an {ISO} Standard for Dialogue Act Annotation,2010,22,96,9,0.145504,16745,harry bunt,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper describes an ISO project which aims at developing a standard for annotating spoken and multimodal dialogue with semantic information concerning the communicative functions of utterances, the kind of semantic content they address, and their relations with what was said and done earlier in the dialogue. The project, ISO 24617-2 ''``Semantic annotation framework, Part 2: Dialogue acts'''', is currently at DIS stage. The proposed annotation schema distinguishes 9 orthogonal dimensions, allowing each functional segment in dialogue to have a function in each of these dimensions, thus accounting for the multifunctionality that utterances in dialogue often have. A number of core communicative functions is defined in the form of ISO data categories, available at http://semantic-annotation.uvt.nl/dialogue-acts/iso-datcats.pdf; they are divided into ''``dimension-specific'''' functions, which can be used only in a particular dimension, such as Turn Accept in the Turn Management dimension, and ''``general-purpose'''' functions, which can be used in any dimension, such as Inform and Request. An XML-based annotation language, ''``DiAML'''' is defined, with an abstract syntax, a semantics, and a concrete syntax."
popescu-belis-etal-2008-task,Task-Based Evaluation of Meeting Browsers: from Task Elicitation to User Behavior Analysis,2008,11,1,1,1,6978,andrei popescubelis,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents recent results of the application of the task-based Browser Evaluation Test (BET) to meeting browsers, that is, interfaces to multimodal databases of meeting recordings. The tasks were defined by browser-neutral BET observers. Two groups of human subjects used the Transcript-based Query and Browsing interface (TQB), and attempted to solve as many BET tasks - pairs of true/false statements to disambiguate - as possible in a fixed amount of time. Their performance was measured in terms of precision and speed. Results indicate that the browserÂs annotation-based search functionality is frequently used, in particular the keyword search. A more detailed analysis of each test question for each participant confirms that despite considerable variation across strategies, the use of queries is correlated to successful performance."
estrella-etal-2008-improving,Improving Contextual Quality Models for {MT} Evaluation Based on Evaluators{'} Feedback,2008,0,1,2,1,20864,paula estrella,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The Framework for the Evaluation for Machine Translation (FEMTI) contains guidelines for building a quality model that is used to evaluate MT systems in relation to the purpose and intended context of use of the systems. Contextual quality models can thus be constructed, but entering into FEMTI the knowledge required for this operation is a complex task. An experiment has been set up in order to transfer knowledge from MT evaluation experts into the FEMTI guidelines, by polling experts about the evaluation methods they would use in a particular context, then inferring from the results generic relations between characteristics of the context of use and quality characteristics. The results of this hands-on exercise, carried out as part of a conference tutorial, have served to refine FEMTIÂs Âgeneric contextual quality modelÂ and to obtain feedback on the FEMTI guidelines in general."
P07-2024,Generating Usable Formats for Metadata and Annotations in a Large Meeting Corpus,2007,8,6,1,1,6978,andrei popescubelis,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"The AMI Meeting Corpus is now publicly available, including manual annotation files generated in the NXT XML format, but lacking explicit metadata for the 171 meetings of the corpus. To increase the usability of this important resource, a representation format based on relational databases is proposed, which maximizes informativeness, simplicity and reusability of the metadata and annotations. The annotation files are converted to a tabular format using an easily adaptable XSLT-based mechanism, and their consistency is verified in the process. Metadata files are generated directly in the IMDI XML format from implicit information, and converted to tabular format using a similar procedure. The results and tools will be freely available with the AMI Corpus. Sharing the metadata using the Open Archives network will contribute to increase the visibility of the AMI Corpus."
2007.tmi-papers.8,A new method for the study of correlations between {MT} evaluation metrics,2007,20,8,2,1,20864,paula estrella,Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,"This paper aims at providing a reliable method for measuring the correlations between different scores of evaluation metrics applied to machine translated texts. A series of examples from recent MT evaluation experiments are first discussed, including results and data from the recent French MT evaluation campaign, CESTA, which is used here. To compute correlation, a set of 1,500 samples for each system and each evaluation metric are created using bootstrapping. Correlations between metrics, both automatic and applied by human judges, are then computed over these samples. The results confirm the previously observed correlations between some automatic metrics, but also indicate a lack of correlation between human and automatic metrics on the CESTA data, which raises a number of questions regarding their validity. In addition, the roles of the corpus size and of the selection procedure for bootstrapping (low vs. high scores) are also examined."
2007.sigdial-1.3,Contrasting the Automatic Identification of Two Discourse Markers in Multiparty Dialogues,2007,15,4,1,1,6978,andrei popescubelis,Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,0,"The identification of occurrences of like and well that serve as discourse markers (DMs) is a classification problem which is studied here on a corpus of dialogue transcripts with more than 4,000 occurrences of each item. Decision trees using item-specific lexical, prosodic, positional and sociolinguistic features are trained using the C4.5 method. The results demonstrate improvement over past experiments, reaching the same range as inter-annotator agreement scores. DM identification appears to benefit from itemspecific classifiers, which perform better than general purpose ones, thanks to the differentiated use of lexical features."
2007.mtsummit-ucnlg.10,Evaluation of {NLG}: some analogies and differences with machine translation and reference resolution,2007,-1,-1,1,1,6978,andrei popescubelis,Proceedings of the Workshop on Using corpora for natural language generation,0,None
2007.mtsummit-tutorials.3,Context-based evaluation of {MT} systems: principles and tools,2007,-1,-1,2,0,48288,maghi king,Proceedings of Machine Translation Summit XI: Tutorials,0,None
2007.mtsummit-papers.23,How much data is needed for reliable {MT} evaluation? Using bootstrapping to study human and automatic metrics,2007,16,6,3,1,20864,paula estrella,Proceedings of Machine Translation Summit XI: Papers,0,"Evaluating the output quality of machine translation system requires test data and quality metrics to be applied. Based on the results of the French MT evaluation campaign CESTA, this paper studies the statistical reliability of the scores depending on the amount of test data used to obtain them. Bootstrapping is used to compute standard deviation of scores assigned by human judges (mainly of adequacy) as well as of five automatic metrics. The reliability of the scores is measured using two formal criteria, and the minimal number of documents or segments needed to reach reliable scores is estimated. This number does not depend on the exact subset of documents that is used."
2007.mtsummit-papers.31,Assessing human and automated quality judgments in the {F}rench {MT} evaluation campaign {CESTA},2007,9,11,3,0.666667,27330,olivier hamon,Proceedings of Machine Translation Summit XI: Papers,0,"This paper analyzes the results of the French MT Evaluation Campaign, CESTA (2003-2006). The details of the campaign are first briefly described. The paper then focuses on the results of the two runs, which used human metrics, such as fluency or adequacy, as well as automated metrics, mainly based on n-gram comparison and word error rates. The results show that the quality of the systems can be reliably compared using these metrics, and that the adaptability of some systems to a given domain xe2x80x93 which was the focus of CESTA's second run xe2x80x93 is not strictly related to their intrinsic performance."
2007.mtsummit-aptme.1,The place of automatic evaluation metrics in external quality models for machine translation,2007,-1,-1,1,1,6978,andrei popescubelis,Proceedings of the Workshop on Automatic procedures in MT evaluation,0,None
popescu-belis-etal-2006-model,A Model for Context-Based Evaluation of Language Processing Systems and its Application to Machine Translation Evaluation,2006,0,1,1,1,6978,andrei popescubelis,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper, we propose a formal framework that takes into account the influence of the intended context of use of an NLP system on the procedure and the metrics used to evaluate the system. We introduce in particular the notion of a context-dependent quality model and explain how it can be adapted to a given context of use. More specifically, we define vector-space representations of contexts of use and of quality models, which are connected by a generic contextual quality model (GCQM). For each domain, experts in evaluation are needed to build a GCQM based on analytic knowledge and on previous evaluations, using the mechanism proposed here. The main inspiration source for this work is the FEMTI framework for the evaluation of machine translation, which implements partly the present model, and which is described briefly along with insights from other domains."
popescu-belis-georgescul-2006-tqb,{TQB}: Accessing Multimodal Data Using a Transcript-based Query and Browsing Interface,2006,0,8,1,1,6978,andrei popescubelis,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This article describes an interface for searching and browsing multimodal recordings of group meetings. We provide first an overall perspective of meeting processing and retrieval applications, and distinguish between the media/modalities that are recorded and the ones that are used for browsing. We then proceed to describe the data and the annotations that are stored in a meeting database. Two scenarios of use for the transcript-based query and browsing interface (TQB) are then outlined: search and browse vs. overview and browse. The main functionalities of TQB, namely the database backend and the multimedia rendering solutions are described. An outline of evaluation perspectives is finally provided, with a description of the user interaction features that will be monitored."
2006.jeptalnrecital-long.23,R{\\'e}solution des r{\\'e}f{\\'e}rences aux documents dans un corpus de dialogues humains,2006,-1,-1,1,1,6978,andrei popescubelis,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article {\'e}tudie la r{\'e}solution des r{\'e}f{\'e}rences {\`a} des entit{\'e}s lorsqu{'}une repr{\'e}sentation informatique de ces entit{\'e}s est disponible. Nous nous int{\'e}ressons {\`a} un corpus de dialogues entre humains, portant sur les grands titres de la presse francophone du jour, et proposons une m{\'e}thode pour d{\'e}tecter et r{\'e}soudre les r{\'e}f{\'e}rences faites par les locuteurs aux articles des journaux. La d{\'e}tection des expressions nominales qui r{\'e}f{\`e}rent {\`a} ces documents est r{\'e}alis{\'e}e gr{\^a}ce {\`a} une grammaire, alors que le probl{\`e}me de la d{\'e}tection des pronoms qui r{\'e}f{\`e}rent aux documents est abord{\'e} par des moyens statistiques. La r{\'e}solution de ces expressions, {\`a} savoir l{'}attribution des r{\'e}f{\'e}rents, fait quant {\`a} elle l{'}objet d{'}un algorithme inspir{\'e} de la r{\'e}solution des cor{\'e}f{\'e}rences. Ces propositions sont {\'e}valu{\'e}es par le biais de mesures quantitatives sp{\'e}cifiques."
2005.tc-1.3,Finding the System that Suits You Best: Towards the Normalization of {MT} Evaluation,2005,-1,-1,2,1,20864,paula estrella,Translating and the Computer 27,0,None
2005.mtsummit-papers.16,Evaluation of Machine Translation with Predictive Metrics beyond {BLEU}/{NIST}: {CESTA} Evaluation Campaign {\\#} 1,2005,-1,-1,5,0,49442,sylvain surcin,Proceedings of Machine Translation Summit X: Papers,0,"In this paper, we report on the results of a full-size evaluation campaign of various MT systems. This campaign is novel compared to the classical DARPA/NIST MT evaluation campaigns in the sense that French is the target language, and that it includes an experiment of meta-evaluation of various metrics claiming to better predict different attributes of translation quality. We first describe the campaign, its context, its protocol and the data we used. Then we summarise the results obtained by the participating systems and discuss the meta-evaluation of the metrics used."
W04-2313,Towards Automatic Identification of Discourse Markers in Dialogs: The Case of Like,2004,13,16,2,1,37151,sandrine zufferey,Proceedings of the 5th {SIG}dial Workshop on Discourse and Dialogue at {HLT}-{NAACL} 2004,0,"This article discusses the detection of discoursen  markers (DM) in dialog transcriptions, n by human annotators and by automated n means. After a theoretical discussion of the n definition of DMs and their relevance to naturaln  language processing, we focus on the role n of like as a DM. Results from experiments n with human annotators show that detection of n DMs is a difficult but reliable task, which requiresn  prosodic information from soundtracks. n Then, several types of features are defined for n automatic disambiguation of like: collocations,n  part-of-speech tags and duration-based n features. Decision-tree learning shows that for n like, nearly 70% precision can be reached, n with near 100% recall, mainly using collocationn  filters. Similar results hold for well, with n about 91% precision at 100% recall."
W04-2328,Multi-level Dialogue Act Tags,2004,11,45,2,0,2260,alexander clark,Proceedings of the 5th {SIG}dial Workshop on Discourse and Dialogue at {HLT}-{NAACL} 2004,0,"In this paper we discuss the use of multi-layered tagsets for dialogue acts, in the context of dialogue understanding for multi-party meeting recording and retrieval applications. We discuss some desiderata for such tagsets and critically examine some previous proposals. We then define MALTUS, a new tagset based on the ICSI-MR and Switchboard tagsets, which satisfies these requirements. We present some experiments using MALTUS which attempt to compare the merits of integrated versus multi-level classifiers for the detection of dialogue acts."
W04-0710,Reference Resolution over a Restricted Domain: References to Documents,2004,17,19,1,1,6978,andrei popescubelis,Proceedings of the Conference on Reference Resolution and Its Applications,0,"This article studies the resolution of references made by speakers to documents discussed during a meeting. The focus is on transcribed recordings of press review meetings, in French. After an overview of the required framework for reference resolutionxe2x80x94specification of the task, data annotation, and evaluation procedurexe2x80x94we propose, analyze and evaluate an algorithm for the resolution of references to documents (ref2doc) based on anaphora tracking and context matching. Applications to speech-to-document alignment and more generally to meeting processing and retrieval are finally discussed."
lisowska-etal-2004-user,User Query Analysis for the Specification and Evaluation of a Dialogue Processing and Retrieval System,2004,5,28,2,0,45940,agnes lisowska,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This article describes an experiment in user query elicitation for the design of a multimodal meeting processing and retrieval system (MPR). In the experiment, participants are asked to choose between several scenarios of use of an MPR system, then formulate (on paper) queries to the system within the context of their chosen scenario. The analysis of the queries provides us with an initial set of requirements for the design of an MPR system, which will be used to confirm a priori design considerations, and suggest improvements to existing interfaces. This elicitation-design-evaluation process will be iterated, where the next phase will involve experiments using the Wizard-of-Oz methodology."
popescu-belis-2004-abstracting,Abstracting a Dialog Act Tagset for Meeting Processing,2004,0,3,1,1,6978,andrei popescubelis,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,None
popescu-belis-etal-2004-online,Online Evaluation of Coreference Resolution,2004,13,13,1,1,6978,andrei popescubelis,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper presents the design of an online evaluation service for coreference resolution in texts. We argue that coreference, as an equivalence relation between referring expressions (RE) in texts, should be properly distinguished from anaphora and has therefore to be evaluated separately. The annotation model for coreference is based on links between REs. The program presented in this article compares two such annotations, which may be the output of coreference resolution tools or of human judgement. In order to evaluate the agreement between the two annotations, the evaluator first converts the input annotation format into a pivot format, then abstracts equivalence classes from the links and provides five scores representing in different ways the similarity between the two partitions: MUC, B3, Kappa, Core-discourse-entity, and Mutual-information. Although we consider that the identification of REs (i.e. the elements of the partition) should not be part of coreference resolution properly speaking, we propose several solutions for the frequent case when the input files do not agree on the elements of the text to consider as REs."
popescu-belis-etal-2004-building,Building and Using a Corpus of Shallow Dialogue Annotated Meetings,2004,13,9,1,1,6978,andrei popescubelis,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper we provide a framework for shallow dialog annotations (SDA), and for their use in the context of the processing and retrieval of multimodal meeting recordings. The SDA model groups the following elements: dialog segmentation into utterances and episodes, detection of dialog acts and adjacency pairs, and detection of referring expressions and coreference links, including references to documents. An instantiated XML annotation model based on boundaries, labels and links, is provided. The use of SDA data in a meeting retrieval interface is also described."
2003.mtsummit-papers.30,{FEMTI}: creating and using a framework for {MT} evaluation,2003,-1,-1,2,0,50182,margaret king,Proceedings of Machine Translation Summit IX: Papers,0,"This paper presents FEMTI, a web-based Framework for the Evaluation of Machine Translation in ISLE. FEMTI offers structured descriptions of potential user needs, linked to an overview of technical characteristics of MT systems. The description of possible systems is mainly articulated around the quality characteristics for software product set out in ISO/IEC standard 9126. Following the philosophy set out there and in the related 14598 series of standards, each quality characteristic bottoms out in metrics which may be applied to a particular instance of a system in order to judge how satisfactory the system is with respect to that characteristic. An evaluator can use the description of user needs to help identify the specific needs of his evaluation and the relations between them. He can then follow the pointers to system description to determine what metrics should be applied and how. In the current state of the framework, emphasis is on being exhaustive, including as much as possible of the information available in the literature on machine translation evaluation. Future work will aim at being more analytic, looking at characteristics and metrics to see how they relate to one another, validating metrics and investigating the correlation between particular metrics and human judgement."
2003.mtsummit-papers.41,An experiment in comparative evaluation: humans vs. computers,2003,-1,-1,1,1,6978,andrei popescubelis,Proceedings of Machine Translation Summit IX: Papers,0,"This paper reports results from an experiment that was aimed at comparing evaluation metrics for machine translation. Implemented as a workshop at a major conference in 2002, the experiment defined an evaluation task, description of the metrics, as well as test data consisting of human and machine translations of two texts. Several metrics, either applicable by human judges or automated, were used, and the overall results were analyzed. It appeared that most human metrics and automated metrics provided in general consistent rankings of the various candidate translations; the ranking of the human translations matched the one provided by translation professionals; and human translations were distinguished from machine translations."
hovy-etal-2002-computer,Computer-Aided Specification of Quality Models for Machine Translation Evaluation,2002,5,7,3,0,1043,eduard hovy,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This article describes the principles and mechanism of an integrative effort in machine translation (MT) evaluation. Building upon previous standardization initiatives, above all ISO/IEC 9126, 14598 and EAGLES, we attempt to classify into a coherent taxonomy most of the characteristics, attributes and metrics that have been proposed for MT evaluation. The main articulation of this flexible framework is the link between a taxonomy that helps evaluators define a context of use for the evaluated software, and a taxonomy of the quality characteristics and associated metrics. The article explains the theoretical grounds of this articulation, along with an overview of the taxonomies in their present state, and a perspective on ongoing work in MT evaluation standardization."
starlander-popescu-belis-2002-corpus,Corpus-based Evaluation of a {F}rench Spelling and Grammar Checker,2002,1,14,2,0,37884,marianne starlander,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This article describes an evaluation method for spelling and grammar checkers and gives the results of its application to two French checkers. The evaluation process follows closely the ISO/IEC and EAGLES guidelines, and defines precisely the evaluation metrics, so that they can be easily reproduced. The choice of professional translators as user profile entails the use of a corpus of spelling mistakes, which was collected and annotated. The metrics are divided into three sets: classification of perfect vs. imperfect sentences; detection of mistakes; correction of mistakes. The results show in which respect the two systems are the most adapted to the user needs, and the points on which they could be improved."
popescu-belis-etal-2002-electronic,"Electronic Dictionaries - from Publisher Data to a Distribution Server: the {D}ico{P}ro, {D}ico{E}ast and {RERO} Projects",2002,7,3,1,1,6978,andrei popescubelis,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
2001.mtsummit-eval.5,Towards a two-stage taxonomy for machine translation evaluation,2001,-1,-1,1,1,6978,andrei popescubelis,Workshop on MT Evaluation,0,None
P98-2172,Reference Resolution beyond Coreference: a Conceptual Frame and its Application,1998,15,15,1,1,6978,andrei popescubelis,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"A model for reference use in communication is proposed, from a representationist point of view. Both the sender and the receiver of a message handle representations of their common environment, including mental representations of objects. Reference resolution by a computer is viewed as the construction of object representations using referring expressions from the discourse, whereas often only coreference links between such expressions are looked for. Differences between these two approaches are discussed.The model has been implemented with elementary rules, and tested on complex narrative texts (hundreds to thousands of referring expressions). The results support the mental representations paradigm."
C98-2167,Reference Resolution beyond Coreference: a Conceptual Frame and its Application,1998,15,15,1,1,6978,andrei popescubelis,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"A model for reference use in communication is proposed, from a representationist point of view. Both the sender and the receiver of a message handle representations of their common environment, including mental representations of objects. Reference resolution by a computer is viewed as the construction of object representations using referring expressions from the discourse, whereas often only coreference links between such expressions are looked for. Differences between these two approaches are discussed.The model has been implemented with elementary rules, and tested on complex narrative texts (hundreds to thousands of referring expressions). The results support the mental representations paradigm."
W97-1314,Cooperation between pronoun and reference resolution for unrestricted texts,1997,12,10,1,1,6978,andrei popescubelis,"Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts",0,"Anaphora resolution is envisaged in this paper as part of the reference resolution process. A general open architecture is proposed, which can be particularized and configured in order to simulate some classic anaphora resolution methods. With the aim of improving pronoun resolution, the system takes advantage of elementary cues about characters of the text, which are represented through a particular data structure. In its most robust configuration, the system uses only a general lexicon, a local morphosyntactic parser and a dictionary of synonyms. A short comparative corpus analysis shows that narrative texts are the most suitable for testing such a system."
