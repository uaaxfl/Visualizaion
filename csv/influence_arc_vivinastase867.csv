C18-1293,C12-1025,0,0.289788,"even when a high proficiency level in a non-native language is achieved. 1 Introduction Native Language Identification (NLI) – identifying the native language (L1) of a person based on his/her writing in the second language (L2) – is useful for a variety of purposes, including security, marketing, and educational applications. The effect of native language phenomena seeping into texts produced in a different language is known as language transfer (Odlin, 1989). Numerous aspects of the language have been explored for NLI – character-level language models (Ionescu et al., 2014), lexical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribution (Markov et al., 2017b; Grieve, 2007)"
C18-1293,P17-2086,1,0.931542,"ntification (NLI) – identifying the native language (L1) of a person based on his/her writing in the second language (L2) – is useful for a variety of purposes, including security, marketing, and educational applications. The effect of native language phenomena seeping into texts produced in a different language is known as language transfer (Odlin, 1989). Numerous aspects of the language have been explored for NLI – character-level language models (Ionescu et al., 2014), lexical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribution (Markov et al., 2017b; Grieve, 2007). More generally, from a linguistic point of view, punctuation has been disputed as following prosodic"
C18-1293,W17-5049,0,0.660463,"Missing"
C18-1293,D14-1142,0,0.770328,"or, which do not diminish in influence even when a high proficiency level in a non-native language is achieved. 1 Introduction Native Language Identification (NLI) – identifying the native language (L1) of a person based on his/her writing in the second language (L2) – is useful for a variety of purposes, including security, marketing, and educational applications. The effect of native language phenomena seeping into texts produced in a different language is known as language transfer (Odlin, 1989). Numerous aspects of the language have been explored for NLI – character-level language models (Ionescu et al., 2014), lexical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribut"
C18-1293,W13-1732,0,0.0787801,"ency level in a non-native language is achieved. 1 Introduction Native Language Identification (NLI) – identifying the native language (L1) of a person based on his/her writing in the second language (L2) – is useful for a variety of purposes, including security, marketing, and educational applications. The effect of native language phenomena seeping into texts produced in a different language is known as language transfer (Odlin, 1989). Numerous aspects of the language have been explored for NLI – character-level language models (Ionescu et al., 2014), lexical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribution (Markov et al., 2017b; Grieve, 2007). More generally, from a lin"
C18-1293,W17-5007,0,0.388824,"Missing"
C18-1293,W17-5042,1,0.778619,"ical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribution (Markov et al., 2017b; Grieve, 2007). More generally, from a linguistic point of view, punctuation has been disputed as following prosodic principles or as a clarifier of grammatical structure (Baron, 2001; Bruthiaux, 1993). Moore (2016) finds a common ground for these two views by observing that prosody and punctuation realize the same function – revealing/emphasizing the information structure of an utterance – in the spoken and respectively written modes of language. Since grammar and prosodic structure are language specific, indicators that reveal them would be language specific as well. As with other aspects"
C18-1293,P13-1112,0,0.0182134,"s achieved. 1 Introduction Native Language Identification (NLI) – identifying the native language (L1) of a person based on his/her writing in the second language (L2) – is useful for a variety of purposes, including security, marketing, and educational applications. The effect of native language phenomena seeping into texts produced in a different language is known as language transfer (Odlin, 1989). Numerous aspects of the language have been explored for NLI – character-level language models (Ionescu et al., 2014), lexical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribution (Markov et al., 2017b; Grieve, 2007). More generally, from a linguistic point of view, punctuation has"
C18-1293,D17-1286,1,0.829035,"the second language (L2) – is useful for a variety of purposes, including security, marketing, and educational applications. The effect of native language phenomena seeping into texts produced in a different language is known as language transfer (Odlin, 1989). Numerous aspects of the language have been explored for NLI – character-level language models (Ionescu et al., 2014), lexical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribution (Markov et al., 2017b; Grieve, 2007). More generally, from a linguistic point of view, punctuation has been disputed as following prosodic principles or as a clarifier of grammatical structure (Baron, 2001; Bruthiaux, 1993). Moore (2016) finds a co"
C18-1293,W13-1718,0,0.16246,"(L1) of a person based on his/her writing in the second language (L2) – is useful for a variety of purposes, including security, marketing, and educational applications. The effect of native language phenomena seeping into texts produced in a different language is known as language transfer (Odlin, 1989). Numerous aspects of the language have been explored for NLI – character-level language models (Ionescu et al., 2014), lexical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribution (Markov et al., 2017b; Grieve, 2007). More generally, from a linguistic point of view, punctuation has been disputed as following prosodic principles or as a clarifier of grammatical structure"
C18-1293,C12-1158,0,0.361292,"ays were written in response to eight different writing prompts/topics (P0–P7), all of which appear in all 11 L1 groups. The dataset also contains information regarding the proficiency level (low, medium, high) of the essay authors. Dataset statistics in terms of proficiency levels and writing prompts are presented in Tables 1 and 2, respectively. ICLEv2 (Granger et al., 2009): the ICLEv2 dataset (henceforth, ICLE) consists of essays written by highly-proficient non-native college-level students of English. We used a 7-language subset of the corpus normalized for topic and character encoding (Tetreault et al., 2012; Ionescu et al., 2014)2 . This subset contains 110 essays (with an average of 747 tokens per essay after tokenization and removal of metadata) for each of the 7 languages: Bulgarian, Chinese, Czech, French, Japanese, Russian, and Spanish. 2 The authors express their gratitude to A. Cahill for providing the list of documents used in their paper. 3458 3.2 Features The suite of experiments we report were designed to investigate the impact of punctuation-based features on native language identification. The hypothesis we are testing is whether patterns of punctuation usage – possibly motivated by"
C18-1293,W13-1706,0,0.376823,"Missing"
C18-1293,W07-0602,0,0.0397926,"Missing"
C18-1293,U09-1008,0,0.0674874,"Missing"
D08-1080,C04-1084,0,0.0158902,"ntent relative to a topic based on lexical chains. The sentences intersected by the most and strongest chains are chosen for the extractive summary. Alternative sources for query expansion and document processing have also been explored. Amini & Usunier (2007) use the documents to be summarized themselves to cluster terms, and thus expanding the query “internally”. More advanced methods for query expansion use “topic signatures” – words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not (Lin & Hovy, 2000; Harabagiu, 2004). Graph-based methods for text summarization work usually at the level of sentences (Erkan & Radev, 2004; Mihalcea & Tarau, 2004). Edge weights between sentences represent a similarity measure, and a PageRank algorithm is used to determine the sentences that are the most salient from a collection of documents and closest to a given topic. At the word level, Leskovec et al. (2004) build a document graph using subject-verb-object triples, semantic normalization and coreference resolution. They use several methods (node degree, PageRank, Hubs, etc.) to compute statistics for the nodes in the netw"
D08-1080,C00-1072,0,0.147676,"model a text’s content relative to a topic based on lexical chains. The sentences intersected by the most and strongest chains are chosen for the extractive summary. Alternative sources for query expansion and document processing have also been explored. Amini & Usunier (2007) use the documents to be summarized themselves to cluster terms, and thus expanding the query “internally”. More advanced methods for query expansion use “topic signatures” – words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not (Lin & Hovy, 2000; Harabagiu, 2004). Graph-based methods for text summarization work usually at the level of sentences (Erkan & Radev, 2004; Mihalcea & Tarau, 2004). Edge weights between sentences represent a similarity measure, and a PageRank algorithm is used to determine the sentences that are the most salient from a collection of documents and closest to a given topic. At the word level, Leskovec et al. (2004) build a document graph using subject-verb-object triples, semantic normalization and coreference resolution. They use several methods (node degree, PageRank, Hubs, etc.) to compute statistics for the"
D08-1080,W04-3252,0,\N,Missing
D08-1080,W97-0703,0,\N,Missing
D09-1095,S07-1007,0,0.739036,"proach suffers from data sparseness. To address this problem, Nissim and Markert (2003) proposed a word similarity-based method. They use Lin’s thesaurus (Lin, 1998) to determine how close two lexical heads are, and use this instead of the more restrictive identity constraint when comparing two instances. This technique is complex, requiring smoothing, multiple iterations over the thesaurus and hybrid methods to allow a back-off to grammatical roles. The supervised approach to resolving metonymies was encouraged by the metonymy resolution task at the semantic evaluation exercise SemEval 2007 (Markert and Nissim, 2007). The participating systems in this task were varied. Most of them (four out of five) have used supervised machine learning techniques. The systems that beat the baseline used either the grammatical annotations provided by the organizers (Farkas et al., 2007; Nicolae et al., 2007), or a robust and deep (not freely available) parser (Brun et al., 2007). These systems represented instances in a manner similar to (Nissim and Markert, 2005). They used additional manually built resources – WordNet, FrameNet, Levin’s verb classes, manually built lists of “trigger” words – to generalize the existing"
D09-1095,S07-1109,0,0.196903,"ple iterations over the thesaurus and hybrid methods to allow a back-off to grammatical roles. The supervised approach to resolving metonymies was encouraged by the metonymy resolution task at the semantic evaluation exercise SemEval 2007 (Markert and Nissim, 2007). The participating systems in this task were varied. Most of them (four out of five) have used supervised machine learning techniques. The systems that beat the baseline used either the grammatical annotations provided by the organizers (Farkas et al., 2007; Nicolae et al., 2007), or a robust and deep (not freely available) parser (Brun et al., 2007). These systems represented instances in a manner similar to (Nissim and Markert, 2005). They used additional manually built resources – WordNet, FrameNet, Levin’s verb classes, manually built lists of “trigger” words – to generalize the existing features. Brun et al. (2007) also used the British National Corpus (BNC) for computing the distance between words based on their syntactic distribution. While lexical resources and corpora are used to estimate word similarity, all these systems rely exclusively on the data provided by the organizers – instance representation captures only information"
D09-1095,de-marneffe-etal-2006-generating,0,0.0127061,"Missing"
D09-1095,I08-2105,1,0.926103,"ing a PMW towards each of PMW’s possible readings. The technique employed is adapted from unsupervised word sense disambiguation (WSD). In short, we use the local grammatical context as it is commonly used in WSD approaches, to guide the system in choosing the reading that fits best. The benefits of using grammatical information for automatic WSD were first explored by Yarowsky (1995) and Resnik (1996) in unsupervised approaches to disambiguating single words in context. The method described here uses automatically induced selectional preferences, computed from sense-untagged data, similar to Nastase (2008). 3 Data We work with the data from the metonymy resolution task at SemEval 2007 (Markert and Nissim, 2007), generated based on a scheme developed by Markert and Nissim (2003). The metonymy resolution task at SemEval 2007 consisted of two subtasks – one for resolving country names, the other for companies. For each subtask there is a training and a test portion. Figure 1 shows the text fragment for one sample, and Table 1 the data statistics. The reading column shows the possible interpretations of a PMW for countries and companies respectively. For example, org-for-product would be the interp"
D09-1095,S07-1033,0,0.544548,"y constraint when comparing two instances. This technique is complex, requiring smoothing, multiple iterations over the thesaurus and hybrid methods to allow a back-off to grammatical roles. The supervised approach to resolving metonymies was encouraged by the metonymy resolution task at the semantic evaluation exercise SemEval 2007 (Markert and Nissim, 2007). The participating systems in this task were varied. Most of them (four out of five) have used supervised machine learning techniques. The systems that beat the baseline used either the grammatical annotations provided by the organizers (Farkas et al., 2007; Nicolae et al., 2007), or a robust and deep (not freely available) parser (Brun et al., 2007). These systems represented instances in a manner similar to (Nissim and Markert, 2005). They used additional manually built resources – WordNet, FrameNet, Levin’s verb classes, manually built lists of “trigger” words – to generalize the existing features. Brun et al. (2007) also used the British National Corpus (BNC) for computing the distance between words based on their syntactic distribution. While lexical resources and corpora are used to estimate word similarity, all these systems rely exclusiv"
D09-1095,S07-1101,0,0.577899,"stems in this task were varied. Most of them (four out of five) have used supervised machine learning techniques. The systems that beat the baseline used either the grammatical annotations provided by the organizers (Farkas et al., 2007; Nicolae et al., 2007), or a robust and deep (not freely available) parser (Brun et al., 2007). These systems represented instances in a manner similar to (Nissim and Markert, 2005). They used additional manually built resources – WordNet, FrameNet, Levin’s verb classes, manually built lists of “trigger” words – to generalize the existing features. Brun et al. (2007) also used the British National Corpus (BNC) for computing the distance between words based on their syntactic distribution. While lexical resources and corpora are used to estimate word similarity, all these systems rely exclusively on the data provided by the organizers – instance representation captures only information that can be derived from or between the data points provided. The approach presented here goes beyond the given data, and induces from corpora measures that allow the system to determine that appear as this verb’s subjects, and estimate from this the preferences excel has fo"
D09-1095,J91-1003,0,0.834572,"the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the literal reading is not appropriate and give clues about the intended metonymic interpretation. Markert and Nissim (2003) have combined observations from the linguistic analysis of metonymies with results of corpus studies. Linguistic research has postulated that (i) conven911 what are the preferences of the words surrounding a PMW towards each of PMW’s possible readings. The technique employed is adapted from unsu"
D09-1095,P03-1008,0,0.923863,"ontext, and are applied to the information/knowledge evoked by w. The local constraints come from the words with which w is (grammatically) related to. The global constraints come from the domain/topic of the text, discourse relations that span across sentences. Metonymic words have a rather small number of possible interpretations (also called readings) which occur frequently (Markert and Nissim, 2002). Idiosyncratic interpretations are also possible, but very rare. One can view the possible interpretations of a potentially metonymic word (PMW) as corresponding to the word’s possible senses (Nissim and Markert, 2003), bringing the task close to word sense disambiguation. The approach to metonymy resolution presented here is supervised, with unsupervised feature enrichment. We apply techniques inspired by unsupervised word sense disambiguation, which allow us to go beyond the annotated data provided in training, and quantify the restrictions imposed on the interpretation of a PMW by its grammatically related neighbours through collocation information extracted from corpora. The only annotation required for the corpora are automatically induced part-of-speech tags from which we obtain grammatical relations"
D09-1095,W98-0720,0,0.828656,"this context – organization-for-people or organization-for-product. The paper continues with related work in Section 2 and the description of the data in Section 3. The representation used is introduced in Section 4. The results and the discussion are presented in Section 5. The paper wraps up with conclusions and future work. 2 Related Work Analysis of metonymies as a linguistic phenomenon dates back at least to the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the lit"
D09-1095,P92-1047,0,0.829483,"work in Section 2 and the description of the data in Section 3. The representation used is introduced in Section 4. The results and the discussion are presented in Section 5. The paper wraps up with conclusions and future work. 2 Related Work Analysis of metonymies as a linguistic phenomenon dates back at least to the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the literal reading is not appropriate and give clues about the intended metonymic interpretation. Markert and Nissi"
D09-1095,J91-4003,0,0.170387,"stic phenomenon dates back at least to the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the literal reading is not appropriate and give clues about the intended metonymic interpretation. Markert and Nissim (2003) have combined observations from the linguistic analysis of metonymies with results of corpus studies. Linguistic research has postulated that (i) conven911 what are the preferences of the words surrounding a PMW towards each of PMW’s possible readings. The techni"
D09-1095,P93-1012,0,0.853879,"oduct. The paper continues with related work in Section 2 and the description of the data in Section 3. The representation used is introduced in Section 4. The results and the discussion are presented in Section 5. The paper wraps up with conclusions and future work. 2 Related Work Analysis of metonymies as a linguistic phenomenon dates back at least to the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the literal reading is not appropriate and give clues about the inten"
D09-1095,W02-1027,0,0.87833,"stract parts, the cockpit and front seat are some of them, and this provides the discourse links between the two sentences. Constraints on the interpretation of a word w in context comes both from the local and global context, and are applied to the information/knowledge evoked by w. The local constraints come from the words with which w is (grammatically) related to. The global constraints come from the domain/topic of the text, discourse relations that span across sentences. Metonymic words have a rather small number of possible interpretations (also called readings) which occur frequently (Markert and Nissim, 2002). Idiosyncratic interpretations are also possible, but very rare. One can view the possible interpretations of a potentially metonymic word (PMW) as corresponding to the word’s possible senses (Nissim and Markert, 2003), bringing the task close to word sense disambiguation. The approach to metonymy resolution presented here is supervised, with unsupervised feature enrichment. We apply techniques inspired by unsupervised word sense disambiguation, which allow us to go beyond the annotated data provided in training, and quantify the restrictions imposed on the interpretation of a PMW by its gram"
D09-1095,P95-1026,0,0.255858,"Markert and Nissim (2003) have combined observations from the linguistic analysis of metonymies with results of corpus studies. Linguistic research has postulated that (i) conven911 what are the preferences of the words surrounding a PMW towards each of PMW’s possible readings. The technique employed is adapted from unsupervised word sense disambiguation (WSD). In short, we use the local grammatical context as it is commonly used in WSD approaches, to guide the system in choosing the reading that fits best. The benefits of using grammatical information for automatic WSD were first explored by Yarowsky (1995) and Resnik (1996) in unsupervised approaches to disambiguating single words in context. The method described here uses automatically induced selectional preferences, computed from sense-untagged data, similar to Nastase (2008). 3 Data We work with the data from the metonymy resolution task at SemEval 2007 (Markert and Nissim, 2007), generated based on a scheme developed by Markert and Nissim (2003). The metonymy resolution task at SemEval 2007 consisted of two subtasks – one for resolving country names, the other for companies. For each subtask there is a training and a test portion. Figure 1"
D09-1142,N03-1006,0,0.158107,"word form on the production of nouns with the same or different grammatical gender, there is no study of the relation between word forms and their corresponding gender. In recent studies we have found on the relation between word form and its associated gender, the only phonological component of a word that is considered indicative is the ending. Spalek et al. (2008) experiment on French nouns, and test whether a noun’s ending is a strong clue for gender for native speakers of French. Vigliocco et al. (2004b) test cognitive aspects of grammatical gender of Italian nouns referring to animals. Cucerzan and Yarowsky (2003) present a bootstrapping process to predict gender for nouns in context. They show that context gives accurate clues to gender (in particular through determiners, quantifiers, adjectives), but when the context is not useful, the algorithm can fall back successfully on the word form. Cucerzan and Yarowsky model the word form for predicting gender using suffix trie models. When a new word is encountered, the word is mapped onto the trie starting from the last letter, and it is assigned the gender that has the highest probability based on the path it matches in the trie. In context nouns appear w"
D09-1142,barbu-2008-romanian,0,\N,Missing
D12-1017,S07-1109,0,0.0948328,"t of older work in metonymy resolution such as Hobbs et al. (1993), Fass (1991), Markert and Hahn (2002) or the use of a generative lexicon and its relations in Pustejovsky (1991), which also are unsupervised. However, these approaches lacked scalability due to the use of small hand-modeled knowledge bases which our use of a very large Wikipedia-derived ontology overcomes. In addition, most of these approaches (Fass, 1991; Hobbs et al., 1993; Pustejovsky, 1991; Harabagiu, 1998) rely on the view that metonymies violate selectional restrictions in their immediate, local context, usually those 1 Brun et al. (2007) is semi-supervised but again relies on the local grammatical context. 185 imposed by the verbs on their arguments. As can be seen in the Example 2, this misses metonymies which do not violate selectional restrictions. Nastase and Strube (2009) use more flexible probabilistic selectional preferences instead of strict constraint violations as well as WordNet as a larger taxonomy but are also restricted to the local context. Markert and Hahn (2002) do propose a treatment of metonymies that takes into account the larger discourse in the form of anaphoric relations between a metonymy and the prior"
D12-1017,S07-1033,0,0.279243,"ntext, dependent on the semantic class studied and (ii) that an unsupervised approach — although lower than the supervised one — outperforms the supervised most frequent reading baseline and performs close to a standard supervised model with the basic set of lexico-syntactic features (Nissim and Markert, 2005). 2 Related Work The word sense disambiguation setting for metonymy resolution as developed by Nissim and Markert (2005) and used for the SemEval 2007 task (Markert and Nissim, 2009) uses a small, prespecified number of frequently occurring readings. The approaches building on this work (Farkas et al., 2007; Nicolae et al., 2007, among others) are supervised, mostly using shallow surface features as well as grammatical relations.1 Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions"
D12-1017,J91-1003,0,0.806461,"ation in that the flexibility of our framework allows us to incorporate a wider context than in most prior approaches. Let us consider the indications for metonymic readings and its interpretation in Example 1, on the one hand, and Example 2, on the other hand. In Example 1, the grammatical relation to the verb defeat and the verb’s selectional preferences indicate the metonymy. We will call all such grammatically related words and the grammatical relations the local context of the PMW. Such types of local context have been used by most prior approaches (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991; Nastase and Strube, 2009, among others). However, Example 2 shows that the local context can be ambiguous or often weak, such as the verb to be. In these examples, the wider context (database, key184 word) is a better indication for a metonymy but has not been satisfactorily integrated in prior approaches (see Section 2). We here call all words surrounding the PMW but not grammatically related to it the global context. In our approach we integrate both the local and the global context in our probabilistic framework. For the local context, we compute the selectional preferences for the words"
D12-1017,W98-0720,0,0.49228,"s limited to interpretation. Our view of relations in a concept network being the interpretations of metonymies is strongly reminiscent of older work in metonymy resolution such as Hobbs et al. (1993), Fass (1991), Markert and Hahn (2002) or the use of a generative lexicon and its relations in Pustejovsky (1991), which also are unsupervised. However, these approaches lacked scalability due to the use of small hand-modeled knowledge bases which our use of a very large Wikipedia-derived ontology overcomes. In addition, most of these approaches (Fass, 1991; Hobbs et al., 1993; Pustejovsky, 1991; Harabagiu, 1998) rely on the view that metonymies violate selectional restrictions in their immediate, local context, usually those 1 Brun et al. (2007) is semi-supervised but again relies on the local grammatical context. 185 imposed by the verbs on their arguments. As can be seen in the Example 2, this misses metonymies which do not violate selectional restrictions. Nastase and Strube (2009) use more flexible probabilistic selectional preferences instead of strict constraint violations as well as WordNet as a larger taxonomy but are also restricted to the local context. Markert and Hahn (2002) do propose a"
D12-1017,P03-1069,0,0.0257898,"tures as well as grammatical relations.1 Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) can be inferred from a corpus. Utiyama et al. (2000), Lapata (2003) propose a probabilistic model for finding the correct interpretation of such metonymies in an unsupervised manner. However, these event type metonymies differ from the problem dealt with in our paper and the SemEval 2007 task in that their recognition (i.e. their distinction f"
D12-1017,S07-1031,0,0.309658,"tting for metonymy resolution as developed by Nissim and Markert (2005) and used for the SemEval 2007 task (Markert and Nissim, 2009) uses a small, prespecified number of frequently occurring readings. The approaches building on this work (Farkas et al., 2007; Nicolae et al., 2007, among others) are supervised, mostly using shallow surface features as well as grammatical relations.1 Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) c"
D12-1017,J98-2002,0,0.0128516,"ed with a parallelized version of Ensemble4 (Surdeanu and Manning, 2010), and we extracted G, the set of all grammatical relations of the type (verb, dependency, hyperlink) and (adjective, dependency, hyperlink), with the hyperlinks resolved to their corresponding node (concept) in the network ( |G |= 1,578,413 triples). For each verb and adjective in the extracted collocations, and for each of their dependency relations, their collocates were generalized in the network defined by the hypernym/hyponym relations in WikiNet following a method similar to the Minimum Description Length principle (Li and Abe, 1998). Essentially, we aimed to determine a small set of (more general) concepts that describe the set of collocates for a word w and grammatical relation r. Starting from the concept collocates gathered, we go upwards following WikiNet’s is a links, and for each node found that covers at least N concept collocates (N is a parameter, N=2 in the experiments presented here), the MDL score of the node is computed (Algorithm 2). We place a limit M on the number of upward steps in the hierarchy (M =3 in our experiments). The disjoint set of nodes that has the lowest overall MDL score is chosen (Γ), and"
D12-1017,D09-1095,1,0.884177,"t the flexibility of our framework allows us to incorporate a wider context than in most prior approaches. Let us consider the indications for metonymic readings and its interpretation in Example 1, on the one hand, and Example 2, on the other hand. In Example 1, the grammatical relation to the verb defeat and the verb’s selectional preferences indicate the metonymy. We will call all such grammatically related words and the grammatical relations the local context of the PMW. Such types of local context have been used by most prior approaches (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991; Nastase and Strube, 2009, among others). However, Example 2 shows that the local context can be ambiguous or often weak, such as the verb to be. In these examples, the wider context (database, key184 word) is a better indication for a metonymy but has not been satisfactorily integrated in prior approaches (see Section 2). We here call all words surrounding the PMW but not grammatically related to it the global context. In our approach we integrate both the local and the global context in our probabilistic framework. For the local context, we compute the selectional preferences for the words related to the PMW from a"
D12-1017,nastase-etal-2010-wikinet,1,0.885388,"Missing"
D12-1017,S07-1101,0,0.312209,"ork in metonymy resolution such as Hobbs et al. (1993), Fass (1991), Markert and Hahn (2002) or the use of a generative lexicon and its relations in Pustejovsky (1991), which also are unsupervised. However, these approaches lacked scalability due to the use of small hand-modeled knowledge bases which our use of a very large Wikipedia-derived ontology overcomes. In addition, most of these approaches (Fass, 1991; Hobbs et al., 1993; Pustejovsky, 1991; Harabagiu, 1998) rely on the view that metonymies violate selectional restrictions in their immediate, local context, usually those 1 Brun et al. (2007) is semi-supervised but again relies on the local grammatical context. 185 imposed by the verbs on their arguments. As can be seen in the Example 2, this misses metonymies which do not violate selectional restrictions. Nastase and Strube (2009) use more flexible probabilistic selectional preferences instead of strict constraint violations as well as WordNet as a larger taxonomy but are also restricted to the local context. Markert and Hahn (2002) do propose a treatment of metonymies that takes into account the larger discourse in the form of anaphoric relations between a metonymy and the prior"
D12-1017,S07-1093,0,0.266348,"my resolution as developed by Nissim and Markert (2005) and used for the SemEval 2007 task (Markert and Nissim, 2009) uses a small, prespecified number of frequently occurring readings. The approaches building on this work (Farkas et al., 2007; Nicolae et al., 2007, among others) are supervised, mostly using shallow surface features as well as grammatical relations.1 Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) can be inferred f"
D12-1017,J91-4003,0,0.727503,"anking showcases our second major innovation in that the flexibility of our framework allows us to incorporate a wider context than in most prior approaches. Let us consider the indications for metonymic readings and its interpretation in Example 1, on the one hand, and Example 2, on the other hand. In Example 1, the grammatical relation to the verb defeat and the verb’s selectional preferences indicate the metonymy. We will call all such grammatically related words and the grammatical relations the local context of the PMW. Such types of local context have been used by most prior approaches (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991; Nastase and Strube, 2009, among others). However, Example 2 shows that the local context can be ambiguous or often weak, such as the verb to be. In these examples, the wider context (database, key184 word) is a better indication for a metonymy but has not been satisfactorily integrated in prior approaches (see Section 2). We here call all words surrounding the PMW but not grammatically related to it the global context. In our approach we integrate both the local and the global context in our probabilistic framework. For the local context, we compute the select"
D12-1017,D11-1091,0,0.0239153,"Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) can be inferred from a corpus. Utiyama et al. (2000), Lapata (2003) propose a probabilistic model for finding the correct interpretation of such metonymies in an unsupervised manner. However, these event type metonymies differ from the problem dealt with in our paper and the SemEval 2007 task in that their recognition (i.e. their distinction from literal occurrences) is achieved simply by grammatic"
D12-1017,P09-3001,0,0.164701,"s grammatical relations.1 Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) can be inferred from a corpus. Utiyama et al. (2000), Lapata (2003) propose a probabilistic model for finding the correct interpretation of such metonymies in an unsupervised manner. However, these event type metonymies differ from the problem dealt with in our paper and the SemEval 2007 task in that their recognition (i.e. their distinction from literal occu"
D12-1017,N10-1091,0,0.0159107,"S // Remove hyponyms. for all {(c, c0 ) ∈ S 0 |(c0 , is a, c) ∈ W kN } do // update frequency f of c fc = fc + fc0 , f ∈ S delete c0 return S 0 187 discourse” assumption – a phrase that appears associated with a hyperlink once in the article body will be associated with the same hyperlink throughout the article (this applies to the article title as well, which is not hyperlinked in the article itself). This new version of the corpus was then split into sentences, and those without hyperlinks were removed. The remaining 18 million sentences were parsed with a parallelized version of Ensemble4 (Surdeanu and Manning, 2010), and we extracted G, the set of all grammatical relations of the type (verb, dependency, hyperlink) and (adjective, dependency, hyperlink), with the hyperlinks resolved to their corresponding node (concept) in the network ( |G |= 1,578,413 triples). For each verb and adjective in the extracted collocations, and for each of their dependency relations, their collocates were generalized in the network defined by the hypernym/hyponym relations in WikiNet following a method similar to the Minimum Description Length principle (Li and Abe, 1998). Essentially, we aimed to determine a small set of (mo"
D12-1017,C00-2128,0,0.0528555,"requent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) can be inferred from a corpus. Utiyama et al. (2000), Lapata (2003) propose a probabilistic model for finding the correct interpretation of such metonymies in an unsupervised manner. However, these event type metonymies differ from the problem dealt with in our paper and the SemEval 2007 task in that their recognition (i.e. their distinction from literal occurrences) is achieved simply by grammatical patterns (a noun instead of a gerund or to-infinitive following the verb) and the problem is limited to interpretation. Our view of relations in a concept network being the interpretations of metonymies is strongly reminiscent of older work in meto"
D12-1017,bentivogli-etal-2010-building,0,\N,Missing
D17-1286,C12-1025,0,0.0586323,"rote “All men are equal: but some are more equal than others”. How true is this today? In the words of the old song “Money is the root of all evil”. Europe. In the 19th century, Victor Hugo said: ”How sad it is to think that nature is calling out but humanity refuses to pay heed. ”Do you think it is still true nowadays ? Some people say that in our modern world, dominated by science technology and industrialization, there is no longer a place for dreaming and imagination. What is your opinion ? Table 2: Topics in the ICLE dataset. The suitability of the dataset above for NLI was questioned by Brooke and Hirst (2012). They have shown that the fact that the corpus consists of sets of essays on a number of topics causes an overes2704 Language Bulgarian Czech Dutch French German Italian Norwegian Polish Rusian Spanish Swedish Accuracy timation of the results of NLI when random splitting, particularly for groups of contributors that were presented with very different topics – e.g. students from Asia vs. students from Europe. We have analyzed the distribution of essays into topics using the essay titles, and observed that contributions from Europe (which are our focus) have similar distributions across the fea"
D17-1286,D11-1010,0,0.0203646,"rors caused by “etymological interference”; (iii) together with other interference phenomena, for the automatic corrections of language errors. 2 Related Work English is a widespread common language for communication in a variety of fields – science, news, entertainment, politics, etc. A consequence is that numerous people learn English as a second (or indeed nth language). The study of native language interference with the learning of English can be used in multiple ways, including devising methods to make the learning easier and correcting language errors (Leacock et al., 2014; Gamon, 2010; Dahlmeier and Ng, 2011). 2702 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2702–2707 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Massung and Zhai (2016) present an overview of approaches to the task of natural language identification (NLI). Various surface indicators hold clues about a speaker’s native language, that make their way into language production in a non-native language. Nagata and Whittaker (2013),Nagata (2014) find that grammatical patterns from the native language seep into the production of English texts."
D17-1286,N10-1019,0,0.0244574,"cal choice errors caused by “etymological interference”; (iii) together with other interference phenomena, for the automatic corrections of language errors. 2 Related Work English is a widespread common language for communication in a variety of fields – science, news, entertainment, politics, etc. A consequence is that numerous people learn English as a second (or indeed nth language). The study of native language interference with the learning of English can be used in multiple ways, including devising methods to make the learning easier and correcting language errors (Leacock et al., 2014; Gamon, 2010; Dahlmeier and Ng, 2011). 2702 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2702–2707 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Massung and Zhai (2016) present an overview of approaches to the task of natural language identification (NLI). Various surface indicators hold clues about a speaker’s native language, that make their way into language production in a non-native language. Nagata and Whittaker (2013),Nagata (2014) find that grammatical patterns from the native language seep into the prod"
D17-1286,D14-1142,0,0.162594,"their way into language production in a non-native language. Nagata and Whittaker (2013),Nagata (2014) find that grammatical patterns from the native language seep into the production of English texts. Tsur and Rappoport (2007) verify the hypothesis that lexical choice of non-native speakers is influenced by the phonology of their native language, and Wong and Dras (2009) propose the idea that (grammatical) errors are also influenced by the native language. One could draw the inference that character n-grams then could be indicative of the native language, and this was shown to be the case by Ionescu et al. (2014). The natural language identification task (Tetreault et al., 2013) attracted 29 participating teams, which used a variety of features to accomplish the NLI task as a classification exercise: n-grams of lexical tokens (words and POS tags), skip-grams, grammatical information (dependency parses, parse tree rules, preference for particular grammatical forms, e.g. active or passive voice), spelling errors. Apart from morphological, lexical, grammatical features, words also have an etymological dimension. The language family tree itself is drawn based on the analysis of the evolution of languages."
D17-1286,W13-1732,0,0.465352,"Missing"
D17-1286,W15-0606,0,0.0141736,"s, grammatical information (dependency parses, parse tree rules, preference for particular grammatical forms, e.g. active or passive voice), spelling errors. Apart from morphological, lexical, grammatical features, words also have an etymological dimension. The language family tree itself is drawn based on the analysis of the evolution of languages. Language evolution includes, or starts with, word etymologies. Word etymologies have been under-used for tasks related to NLI. They have been used implicitly in work that investigates cognate interference (Nicolai et al., 2013), and explicitly by (Malmasi and Cahill, 2015) who use words with Old English and Latin etymologies as unigram features in building classifiers for the T OEFL 11 dataset. Etymological information is obtained from the Etymological WordNet (de Melo and Weikum, 2010). We also investigate here the impact of etymological information, but unlike previous work, we do not extract unigram/n-gram features for classification, but we look at the collective evidence captured by the etymological “fingerprint” for each document and set of essays. 3 Etymological fingerprints To investigate the influence of etymological ancestor languages, we represent ea"
D17-1286,C14-1183,0,0.0205578,"ke the learning easier and correcting language errors (Leacock et al., 2014; Gamon, 2010; Dahlmeier and Ng, 2011). 2702 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2702–2707 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Massung and Zhai (2016) present an overview of approaches to the task of natural language identification (NLI). Various surface indicators hold clues about a speaker’s native language, that make their way into language production in a non-native language. Nagata and Whittaker (2013),Nagata (2014) find that grammatical patterns from the native language seep into the production of English texts. Tsur and Rappoport (2007) verify the hypothesis that lexical choice of non-native speakers is influenced by the phonology of their native language, and Wong and Dras (2009) propose the idea that (grammatical) errors are also influenced by the native language. One could draw the inference that character n-grams then could be indicative of the native language, and this was shown to be the case by Ionescu et al. (2014). The natural language identification task (Tetreault et al., 2013) attracted 29"
D17-1286,P13-1112,0,0.166073,"uding devising methods to make the learning easier and correcting language errors (Leacock et al., 2014; Gamon, 2010; Dahlmeier and Ng, 2011). 2702 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2702–2707 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Massung and Zhai (2016) present an overview of approaches to the task of natural language identification (NLI). Various surface indicators hold clues about a speaker’s native language, that make their way into language production in a non-native language. Nagata and Whittaker (2013),Nagata (2014) find that grammatical patterns from the native language seep into the production of English texts. Tsur and Rappoport (2007) verify the hypothesis that lexical choice of non-native speakers is influenced by the phonology of their native language, and Wong and Dras (2009) propose the idea that (grammatical) errors are also influenced by the native language. One could draw the inference that character n-grams then could be indicative of the native language, and this was shown to be the case by Ionescu et al. (2014). The natural language identification task (Tetreault et al., 2013)"
D17-1286,W13-1718,0,0.730005,"the native language such as pronunciation, vocabulary and grammar are well-attested, and the phenomenon is called native language interference (Odlin, 1989). At the lexical level, the choice as well as the spelling can be indicative of the native language, through the choice of cognates, true or false friends – e.g. a writer with native language German may choose bloom cognate with blume, while a French one may choose flower, cognate with fleur. Misspellings – cuestion instead of question are also indicative, as the writer will tend to spell words close to the form from her original language (Nicolai et al., 2013). In this paper we also look at native language interference starting from the lexical level, but abstract away from the actual word forms, and focus instead on the language of the etymological ancestors. The hypothesis we investigate is that the collective evidence of etymological ancestor languages are indicative of the language of the native speaker, and that this effect is sufficiently strong to allow us to rebuild an Indo-European language family tree. We use a corpus of essays written by English language learners, whose native language cover the languages from the Indo-European family. E"
D17-1286,W07-0602,0,0.102212,". 2702 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2702–2707 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Massung and Zhai (2016) present an overview of approaches to the task of natural language identification (NLI). Various surface indicators hold clues about a speaker’s native language, that make their way into language production in a non-native language. Nagata and Whittaker (2013),Nagata (2014) find that grammatical patterns from the native language seep into the production of English texts. Tsur and Rappoport (2007) verify the hypothesis that lexical choice of non-native speakers is influenced by the phonology of their native language, and Wong and Dras (2009) propose the idea that (grammatical) errors are also influenced by the native language. One could draw the inference that character n-grams then could be indicative of the native language, and this was shown to be the case by Ionescu et al. (2014). The natural language identification task (Tetreault et al., 2013) attracted 29 participating teams, which used a variety of features to accomplish the NLI task as a classification exercise: n-grams of lex"
D17-1286,W13-1706,0,0.284714,"ta and Whittaker (2013),Nagata (2014) find that grammatical patterns from the native language seep into the production of English texts. Tsur and Rappoport (2007) verify the hypothesis that lexical choice of non-native speakers is influenced by the phonology of their native language, and Wong and Dras (2009) propose the idea that (grammatical) errors are also influenced by the native language. One could draw the inference that character n-grams then could be indicative of the native language, and this was shown to be the case by Ionescu et al. (2014). The natural language identification task (Tetreault et al., 2013) attracted 29 participating teams, which used a variety of features to accomplish the NLI task as a classification exercise: n-grams of lexical tokens (words and POS tags), skip-grams, grammatical information (dependency parses, parse tree rules, preference for particular grammatical forms, e.g. active or passive voice), spelling errors. Apart from morphological, lexical, grammatical features, words also have an etymological dimension. The language family tree itself is drawn based on the analysis of the evolution of languages. Language evolution includes, or starts with, word etymologies. Wor"
D19-1122,Q16-1026,0,0.020277,"4 entities and 2078 relations. Entity detection and relation extraction are performed separately, using an SVM with lexical, POS and dependency features. These approaches rely on heuristics to detect the arguments of relations: use a predefined list of family relationships and diseases, or use as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation arguments is difficult because of their variable length. Sequence labelling approaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text, and use an encodedecoder LSTM-based model"
D19-1122,N16-1030,0,0.0134605,"duce a corpus of 477 sentences, with 4154 entities and 2078 relations. Entity detection and relation extraction are performed separately, using an SVM with lexical, POS and dependency features. These approaches rely on heuristics to detect the arguments of relations: use a predefined list of family relationships and diseases, or use as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation arguments is difficult because of their variable length. Sequence labelling approaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text"
D19-1122,P15-2047,0,0.0208728,"and diseases, or use as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation arguments is difficult because of their variable length. Sequence labelling approaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text, and use an encodedecoder LSTM-based model to predict for an input sentence the tags that combine relation and argument information. We will use a similar tag set, but in a two step approach. 3 Dataset Our dataset consists of answers to a family history questionnaire collected from volunteers through Amazon"
D19-1122,P16-1101,0,0.0191433,"sentences, with 4154 entities and 2078 relations. Entity detection and relation extraction are performed separately, using an SVM with lexical, POS and dependency features. These approaches rely on heuristics to detect the arguments of relations: use a predefined list of family relationships and diseases, or use as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation arguments is difficult because of their variable length. Sequence labelling approaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text, and use an encode"
D19-1122,P14-5010,0,0.00426101,"Missing"
D19-1122,P16-1105,0,0.0295519,"proaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text, and use an encodedecoder LSTM-based model to predict for an input sentence the tags that combine relation and argument information. We will use a similar tag set, but in a two step approach. 3 Dataset Our dataset consists of answers to a family history questionnaire collected from volunteers through Amazon Mechanical Turk (AMT) and from test cases of genetic counseling sessions (GCS).2 This questionnaire has been developed by cancer genetic counseling experts to construct a patient’s medical history (Wattendorf and Had"
D19-1122,D14-1162,0,0.0822833,"classification task. Each sentence is used to produce positive and negative instances for the relations it contains. For example, the sentence I have two sons John and Jim who are 23 and 26. will produce two positive instances: (John, age, 23) and (Jim, age, 26). Negative instances are produced by pairing up unrelated arguments (e.g. (John, age, 26)). We train a bi-directional LSTM classifier for this task and fine-tune the parameters using the validation set. The model is trained using the following feature representation: Word embeddings: we use pretrained 300 dimensional Glove embeddings (Pennington et al., 2014) Position features: binary flag representing the position of the target entities Annotation: the tag of the token from the entity identification step. 5 Results The system is trained on training data from both sources (AMT and GCS), and is evaluated on the test data. Since the relation classification model relies on the output of the entity identification model, we evaluate it using the automatically detected entities. Table 6 shows the performance of 1258 the tagger and relation classifier on the test set. We further analyze the performance of the relation classifier in two scenarios: i) when"
D19-1122,W18-5613,0,0.0221254,"llness history (a pre-specified set of 8 illnesses) in discharge summaries and outpatient visit notes. The system is run on 2000 reports randomly selected from 4 different hospitals’ clinical notes. 1000 of the relations detected by the system were manually inspected for evaluation. Lewis et al. (2011) rely on grammatical dependencies and patterns of dependency sequences to detect family history information, constraining one of the arguments of a relation to express a family member (e.g. mother, brother). The system is trained on 299 sentences, 77 of which contained 167 persondiagnosis pairs. Rama et al. (2018) iteratively develop an annotation schema and a synthetic corpus of clinical notes in Norwegian for family history annotation and extraction. They produce a corpus of 477 sentences, with 4154 entities and 2078 relations. Entity detection and relation extraction are performed separately, using an SVM with lexical, POS and dependency features. These approaches rely on heuristics to detect the arguments of relations: use a predefined list of family relationships and diseases, or use as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation argum"
D19-1122,W95-0107,0,0.190432,"n and extraction. They produce a corpus of 477 sentences, with 4154 entities and 2078 relations. Entity detection and relation extraction are performed separately, using an SVM with lexical, POS and dependency features. These approaches rely on heuristics to detect the arguments of relations: use a predefined list of family relationships and diseases, or use as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation arguments is difficult because of their variable length. Sequence labelling approaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme fo"
D19-1122,P15-1061,0,0.0270666,"as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation arguments is difficult because of their variable length. Sequence labelling approaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text, and use an encodedecoder LSTM-based model to predict for an input sentence the tags that combine relation and argument information. We will use a similar tag set, but in a two step approach. 3 Dataset Our dataset consists of answers to a family history questionnaire collected from volunteers through Amazon Mechanical Turk (AMT) and"
D19-1122,E12-2021,0,0.0642164,"Missing"
D19-1122,C14-1220,0,0.0138141,"amily relationships and diseases, or use as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation arguments is difficult because of their variable length. Sequence labelling approaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text, and use an encodedecoder LSTM-based model to predict for an input sentence the tags that combine relation and argument information. We will use a similar tag set, but in a two step approach. 3 Dataset Our dataset consists of answers to a family history questionnaire collected from voluntee"
D19-1122,P17-1113,0,0.0148418,"cessful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text, and use an encodedecoder LSTM-based model to predict for an input sentence the tags that combine relation and argument information. We will use a similar tag set, but in a two step approach. 3 Dataset Our dataset consists of answers to a family history questionnaire collected from volunteers through Amazon Mechanical Turk (AMT) and from test cases of genetic counseling sessions (GCS).2 This questionnaire has been developed by cancer genetic counseling experts to construct a patient’s medical history (Wattendorf and Hadley, 2005). It consis"
gella-etal-2014-mapping,perez-rosas-etal-2012-learning,0,\N,Missing
gella-etal-2014-mapping,W12-4210,0,\N,Missing
gella-etal-2014-mapping,fernando-stevenson-2012-mapping,0,\N,Missing
gella-etal-2014-mapping,magnini-cavaglia-2000-integrating,0,\N,Missing
I08-1034,P04-3034,0,0.0576114,"Missing"
I08-1034,P07-1102,0,0.119251,"of neuro-linguistic programming investigates how to program our language (among other things) to achieve a goal. In the 1980s, Rodger Bailey developed the Language and Behaviour Profile based on 60 meta-programs. Charvet (1997) presents a simplified approach with 14 metaprograms. This profile proposes that people’s language patterns are indicators of behavioural preferences. In the study of planning dialogues (ChuCarroll and Carberry, 2000), Searle’s theory of speech acts used through the discourse analysis also supports the fact that language carries much of people’s behaviour and emotions. Reitter and Moore (2007) studied repetitions in task-oriented conversations. They demonstrated that a speaker’s shortterm ability to copy the interlocutor’s syntax is autonomous from the success of the task, whereas long-term adaptation varies with such success. We consider a negotiation to be a communication in which the participants want to reach an agreement relative to the splitting/sharing of resources. Language is one of the tools used to reach the goal. We propose that not all messages exchanged throughout a negotiation have the same effect on the negotiation outcome. To test this hypothesis, we take an ever s"
I08-1034,D07-1048,0,0.147094,"model of behaviour (Koeszegi et al, 2007), which is common in faceto-face negotiations (Adair and Brett, 2005). Here is an example of behavioural phases in face-to-face negotiations: Perform Relational Positioning → Identify the Problem → Generate Solutions → Reach Agreement. Unexpected turns and moves – typical of human behaviour – make prediction of the negotiation outcome difficult. In case of electronic negotiation, the absence of the usual negotiation structure further complicates the outcome prediction. This distinguishes e-negotiations from agentcustomer phone conversations studied in (Takeuchi et al, 2007), where an agent follows the call flow pre-defined by his company’s policy. 258 The longer an e-negotiation takes, the more elaborate the structure of the e-negotiation process becomes. Simpler e-negotiation may involve an exchange of well-structured business documents such as pre-defined contract or retail transactions. A more complex process comprises numerous offers and counter-offers and has a high degree of uncertainty because of the possible unpredictability of negotiation moves. The next challenge stems from the limitations imposed by the use of electronic means. This overloads text mes"
I08-1034,H92-1073,0,0.0278682,"ble data of electronic negotiations. Our data come from the Web-based negotiAmong the wealth of data gathered by Inspire, we have focussed on the accompanying text messages, extracted from the transcripts of 2557 negotiations. Each negotiation had two different participants, and one person participated in only one negotiation. The total number of contributors was over 5000; most of them were not native English speakers. The data contain 1, 514, 623 word tokens and 27, 055 types. Compared with benchmark corpora, for example the Brown or the Wall Street Journal corpus (Francis and Kucera, 1997; Paul and Baker, 1992), this collection has a lower type-token ratio and a higher presence of content words among the most frequent words (this is typical of texts on a specific topic), and a high frequency of singular first- and second-person pronouns (this is typical of dialogues). We considered all messages from one negotiation to be a single negotiation text. We concatenated the messages in chronological order, keeping the punctuation and spelling unedited. Each negotiation had a unique label, either positive or negative, and was a training example in one of two classes – success259 Features negotiation-related"
I08-2105,P07-1005,0,0.0293326,"WordNet, the other using dependency relations from the British National Corpus. The best configuration uses the syntactically-constrained graph, selectional preferences computed from the corpus and a PageRank tie-breaking algorithm. We especially note good performance when disambiguating verbs with grammatically constrained links. 1 Introduction It has long been believed that being able to detect the correct sense of a word in a given context – performing word sense disambiguation (WSD) – will lead to improved performance of systems tackling high end applications such as machine translation (Chan et al., 2007) and summarization(Elhadad et al., 1997). In order for WSD methods to be useful, they must be robust, portable, scalable, and therefore preferably not reliant on manually tagged data. These desiderata have lead to an increased interest in developing unsupervised WSD methods, flexible relative to the word sense inventory, and which disambiguate all open-class words in a given context as opposed to a selected few. Particularly appropriate from this point of view are graph-based methods (Navigli and Lapata, 2007), which map the open-class words in a given context onto a highly interconnected grap"
I08-2105,J02-2003,0,0.0120342,"combination scores, based on the analyzed sense-tagged corpus, and similarities between the current words and those in tagged pairs with the same grammatical relation. Once such matrices are computed for all grammatically related word pairs, the sense preferences are propagated from the bottom of the parse tree towards the top, and the sense selection starts from the top and propagates downward. McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. In an approach inspired by the works of Li and Abe (1998) and Clark and Weir (2002), McCarthy and Carroll use grammatically connected words from a corpus to induce a distribution of senses over subtrees in the WordNet hierarchy. McCarthy et al. (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD. We build upon this previous research, and propose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges. We experiment with graph edge weights computed using WordNet, and weights computed using grammatical collocation information from a"
I08-2105,de-marneffe-etal-2006-generating,0,0.0261418,"Missing"
I08-2105,J97-2001,0,0.0508469,"relations from the British National Corpus. The best configuration uses the syntactically-constrained graph, selectional preferences computed from the corpus and a PageRank tie-breaking algorithm. We especially note good performance when disambiguating verbs with grammatically constrained links. 1 Introduction It has long been believed that being able to detect the correct sense of a word in a given context – performing word sense disambiguation (WSD) – will lead to improved performance of systems tackling high end applications such as machine translation (Chan et al., 2007) and summarization(Elhadad et al., 1997). In order for WSD methods to be useful, they must be robust, portable, scalable, and therefore preferably not reliant on manually tagged data. These desiderata have lead to an increased interest in developing unsupervised WSD methods, flexible relative to the word sense inventory, and which disambiguate all open-class words in a given context as opposed to a selected few. Particularly appropriate from this point of view are graph-based methods (Navigli and Lapata, 2007), which map the open-class words in a given context onto a highly interconnected graph. Each node in this graph represents a"
I08-2105,P07-1028,0,0.0372455,"Missing"
I08-2105,P03-1054,0,0.00715285,"rdNet and, in a second set-up, selectional preferences estimated from an (sense-)untagged corpus, for disambiguating together all words in the sentence. Grammatical information for the sentential context is obtained using the dependency relation output of the Stanford Parser (de Marneffe et al., 2006). Selectional preferences are estimated using grammatical collocation information from the British National Corpus (BNC), obtained with the Word Sketch Engine (WSE) (Kilgarriff et al., 2004). 2.1 Extracting grammatical relation information We parse the Senseval test data using the Stanford Parser(Klein and Manning, 2003) generating the output in dependency relation format (de Marneffe et al., 2006). Edges that do not connect open-class words are filtered out, words are lemmatized, and we reintroduce the copula (it is bypassed as a predicate) because the verb be must be disambiguated as well. To estimate selectional preferences from a senseuntagged corpus, for each grammatically related pair of words in a sentence we extract evidence consist758 Dependency relation nsubj(verb,noun) dobj(verb,noun) amod(noun,adj) nn(noun1 ,noun2 ) prep of(verb,noun) WSE relation subject(verb,noun) subject of(noun,verb) object(ve"
I08-2105,J98-2002,0,0.0358835,"ted a matrix of sense combination scores, based on the analyzed sense-tagged corpus, and similarities between the current words and those in tagged pairs with the same grammatical relation. Once such matrices are computed for all grammatically related word pairs, the sense preferences are propagated from the bottom of the parse tree towards the top, and the sense selection starts from the top and propagates downward. McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. In an approach inspired by the works of Li and Abe (1998) and Clark and Weir (2002), McCarthy and Carroll use grammatically connected words from a corpus to induce a distribution of senses over subtrees in the WordNet hierarchy. McCarthy et al. (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD. We build upon this previous research, and propose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges. We experiment with graph edge weights computed using WordNet, and weights computed using grammatical coll"
I08-2105,J03-4004,0,0.0418952,"ating all words in a sentence with sense association (or selectional) preferences computed from a sense-tagged corpus. An untagged grammatically linked word pair will have associated a matrix of sense combination scores, based on the analyzed sense-tagged corpus, and similarities between the current words and those in tagged pairs with the same grammatical relation. Once such matrices are computed for all grammatically related word pairs, the sense preferences are propagated from the bottom of the parse tree towards the top, and the sense selection starts from the top and propagates downward. McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. In an approach inspired by the works of Li and Abe (1998) and Clark and Weir (2002), McCarthy and Carroll use grammatically connected words from a corpus to induce a distribution of senses over subtrees in the WordNet hierarchy. McCarthy et al. (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD. We build upon this previous research, and propose an unsupervised WSD method in which senses for two grammatically related"
I08-2105,P04-1036,0,0.0571979,"Once such matrices are computed for all grammatically related word pairs, the sense preferences are propagated from the bottom of the parse tree towards the top, and the sense selection starts from the top and propagates downward. McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. In an approach inspired by the works of Li and Abe (1998) and Clark and Weir (2002), McCarthy and Carroll use grammatically connected words from a corpus to induce a distribution of senses over subtrees in the WordNet hierarchy. McCarthy et al. (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD. We build upon this previous research, and propose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges. We experiment with graph edge weights computed using WordNet, and weights computed using grammatical collocation information from a corpus. These weights are used to induce an initial scoring of the graph vertices, starting from the leaves and propagating upwards. The disambiguation process starts"
I08-2105,H05-1052,0,0.0593274,"Missing"
I08-2105,S01-1005,0,0.021868,"oosing a sense for the head of the sentence, and moves towards the leaves, propagating downward the chosen senses at each step, and using the edge weights and vertex scores to guide the sense selection process. We investigate two issues: (i) whether using in disambiguation only syntactically connected words leads to results on a par with, or better than, using all word-sense combinations, (ii) whether sense association strength induced from a sense-unlabeled corpus can rival relatedness measures induced from a lexical resource - in our case, WordNet. We evaluate this approach on the Senseval2(Palmer et al., 2001) and Senseval-3(Snyder and Palmer, 2004) English all-words test data. On the Senseval-2 data we obtain results on a par with the best unsupervised WSD systems, on the Senseval3 data, the results are lower overall, but for verbs higher than those obtained with other graph-based methods. In both situations, using only grammatically motivated edges leads to improved disambiguation of verbs compared to disambiguating in a graph with unrestricted connections. 2 Disambiguation Algorithm The disambiguation method described here uses grammatical information from the sentential context to constrain wor"
I08-2105,N04-3012,0,0.134013,"Missing"
I08-2105,W04-0811,0,0.0196378,"entence, and moves towards the leaves, propagating downward the chosen senses at each step, and using the edge weights and vertex scores to guide the sense selection process. We investigate two issues: (i) whether using in disambiguation only syntactically connected words leads to results on a par with, or better than, using all word-sense combinations, (ii) whether sense association strength induced from a sense-unlabeled corpus can rival relatedness measures induced from a lexical resource - in our case, WordNet. We evaluate this approach on the Senseval2(Palmer et al., 2001) and Senseval-3(Snyder and Palmer, 2004) English all-words test data. On the Senseval-2 data we obtain results on a par with the best unsupervised WSD systems, on the Senseval3 data, the results are lower overall, but for verbs higher than those obtained with other graph-based methods. In both situations, using only grammatically motivated edges leads to improved disambiguation of verbs compared to disambiguating in a graph with unrestricted connections. 2 Disambiguation Algorithm The disambiguation method described here uses grammatical information from the sentential context to constrain word pairs that are allowed to influence ea"
I08-2105,W98-0701,0,0.0374037,"nces. We study the impact on disambiguation performance when connections are restricted to pairs of word senses corresponding to words that are grammatically linked in the considered context. The benefits of using grammatical information for automatic WSD were first explored by Yarowsky (1995) and Resnik (1996), in unsupervised approaches to disambiguating single words in context. Sussna (1993) presents a first approach to disambiguating together words within a context. The focus is on nouns, and the sense combination that minimizes the overall distance in the WordNet nouns network is chosen. Stetina et al. (1998) present the first approach, supervised, to disambiguating all words in a sentence with sense association (or selectional) preferences computed from a sense-tagged corpus. An untagged grammatically linked word pair will have associated a matrix of sense combination scores, based on the analyzed sense-tagged corpus, and similarities between the current words and those in tagged pairs with the same grammatical relation. Once such matrices are computed for all grammatically related word pairs, the sense preferences are propagated from the bottom of the parse tree towards the top, and the sense se"
I08-2105,W04-0856,0,0.0578197,"Missing"
I08-2105,P95-1026,0,0.250247,"edges can contribute in a variety of ways to determine the best sense combination for the words in the considered 757 context. This approach leads to large and highly interconnected graphs, in which distant, unrelated (in the context) words, are nonetheless connected, and allowed to influence each other’s sense preferences. We study the impact on disambiguation performance when connections are restricted to pairs of word senses corresponding to words that are grammatically linked in the considered context. The benefits of using grammatical information for automatic WSD were first explored by Yarowsky (1995) and Resnik (1996), in unsupervised approaches to disambiguating single words in context. Sussna (1993) presents a first approach to disambiguating together words within a context. The focus is on nouns, and the sense combination that minimizes the overall distance in the WordNet nouns network is chosen. Stetina et al. (1998) present the first approach, supervised, to disambiguating all words in a sentence with sense association (or selectional) preferences computed from a sense-tagged corpus. An untagged grammatically linked word pair will have associated a matrix of sense combination scores,"
I08-2105,J06-1003,0,\N,Missing
I11-2001,O97-1002,0,0.259058,"Missing"
I11-2001,nastase-etal-2010-wikinet,1,0.745591,"sisting of graphical and textual browsing tools. This allows the user to inspect the knowledge base to which WikiNetTK is applied. The application-oriented part of the toolkit provides various functionalities: access to various types of information in the knowledge base as well as methods for computing association paths and relatedness measures. The system is applied to a large-scale multilingual concept network obtained by extracting and combining various sources of information from Wikipedia. 1 2 Data WikiNetTK is applied to WikiNet, a repository of world knowledge extracted from Wikipedia (Nastase et al., 2010). It is derived from the category and article network, disambiguation, redirect, cross-language, infobox and textual content of Wikipedia pages. It is organized as a concept network – it separates concepts and their lexicalizations, and contains relations between concepts – in a manner similar to WordNet. Concepts have lexicalizations in numerous languages. With WikiNet’s 3.7 Million concepts and 40 Million relations (instantiating 656 relation types), efficiency in data management becomes an issue. Manual analysis of the data is also problematic. WikiNetTK addresses both these issues. A fast"
I11-2001,J06-1003,0,\N,Missing
judea-etal-2012-concept,S07-1007,0,\N,Missing
judea-etal-2012-concept,N10-1091,0,\N,Missing
judea-etal-2012-concept,H92-1045,0,\N,Missing
judea-etal-2012-concept,I11-2001,1,\N,Missing
judea-etal-2012-concept,nastase-etal-2010-wikinet,1,\N,Missing
kassner-etal-2008-acquiring,A97-1014,0,\N,Missing
kassner-etal-2008-acquiring,kunze-lemnitzer-2002-germanet,0,\N,Missing
kassner-etal-2008-acquiring,P05-1045,0,\N,Missing
L18-1113,P15-1168,0,0.0279687,"ze the worst possible input acc3rding to ˜n arbitrary grammar st r ic t ly limited in expressive power, the researcher studying Natural Language Processing can be just i f ied in concerning himself more with issues of practical performance in parsing sentences encountered in language as humans Actually use i t using a grammar expressed in a form corve˜ie: to the human linguist who is writing i t . Figure 1: Excerpt from paper P81-1001, A Practical Comparison of Parsing Strategies by Jonathan Slocum proaches, and outperform statistical phrase-based machine translation models on the CoNLL data. Chen et al. (2015a; Chen et al. (2015b) explore the use of GRUs and LSTMs for Chinese word segmentation, and Zhang et al. (2016) approach the task using word and character context in a globally optimized beam-search framework for neural structured prediction. Yang et al. (2017) build on this previous work to produce a modular neural-based segmentation model for Chinese, using fivecharacter window, pre-trained based on a variety of external resources. Based on these previous analyses into the kind of architectures that perform well for different types of error correction, we adopt a character-level sequence-to-"
L18-1113,D15-1141,0,0.0632543,"Missing"
L18-1113,L16-1586,0,0.0695629,"Missing"
L18-1113,P16-1082,0,0.0588044,"Missing"
L18-1113,W14-1701,0,0.0125537,"rs, erroneous characters, non-canonical spellings (historical texts), grammatical errors, and probably more. For many of the above mentioned problems, neural-based approaches originally developed for machine translation have proved to be very successful. Yannakoudakis et al. (2017) use a machine translation inpired approach – N-best list ranking using neural sequence labelling models – for grammatical error correction. Word and character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Yang et al., 2017) have achieved good performance on the CoNLL-2014 shared task (Ng et al., 2014) on text correction (which covers a variety of errors made in written essays by second language learners). Schmaltz et al. (2017) show that character-level sequence-to-sequence models perform better than word-level models even with less training data than previous sequence-to-sequence ap1 Information about how the word re-segmented version of the ACL corpus is available will be posted at http://www.cl.uni-heidelberg.de/english/ research/downloads/resource_pages/ACL_ corrected/ACL_corrected.shtml. 706 INTRODUCTION Although the l i terature dealing with formal and natural languages abounds with"
L18-1113,W12-3201,0,0.0347551,"Missing"
L18-1113,W12-3210,0,0.0474353,"Missing"
L18-1113,D17-1298,0,0.0411902,"Missing"
L18-1113,E17-3017,1,0.846116,"Missing"
L18-1113,W12-3203,0,0.0248574,"Missing"
L18-1113,P17-1078,0,0.104966,"/phonetically written words (particuarly on social media), word segmentation errors, erroneous characters, non-canonical spellings (historical texts), grammatical errors, and probably more. For many of the above mentioned problems, neural-based approaches originally developed for machine translation have proved to be very successful. Yannakoudakis et al. (2017) use a machine translation inpired approach – N-best list ranking using neural sequence labelling models – for grammatical error correction. Word and character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Yang et al., 2017) have achieved good performance on the CoNLL-2014 shared task (Ng et al., 2014) on text correction (which covers a variety of errors made in written essays by second language learners). Schmaltz et al. (2017) show that character-level sequence-to-sequence models perform better than word-level models even with less training data than previous sequence-to-sequence ap1 Information about how the word re-segmented version of the ACL corpus is available will be posted at http://www.cl.uni-heidelberg.de/english/ research/downloads/resource_pages/ACL_ corrected/ACL_corrected.shtml. 706 INTRODUCTION Al"
L18-1113,D17-1297,0,0.0247156,"he corrected ACL collection will be offered to the ACL anthology editor to be made available to the community.1 2. Related Work Depending on their source, errors in unedited texts can fall into various categories: typos, deliberate misspellings including shortened/phonetically written words (particuarly on social media), word segmentation errors, erroneous characters, non-canonical spellings (historical texts), grammatical errors, and probably more. For many of the above mentioned problems, neural-based approaches originally developed for machine translation have proved to be very successful. Yannakoudakis et al. (2017) use a machine translation inpired approach – N-best list ranking using neural sequence labelling models – for grammatical error correction. Word and character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Yang et al., 2017) have achieved good performance on the CoNLL-2014 shared task (Ng et al., 2014) on text correction (which covers a variety of errors made in written essays by second language learners). Schmaltz et al. (2017) show that character-level sequence-to-sequence models perform better than word-level models even with less training data than previous s"
L18-1113,N16-1042,0,0.0266287,"eliberate misspellings including shortened/phonetically written words (particuarly on social media), word segmentation errors, erroneous characters, non-canonical spellings (historical texts), grammatical errors, and probably more. For many of the above mentioned problems, neural-based approaches originally developed for machine translation have proved to be very successful. Yannakoudakis et al. (2017) use a machine translation inpired approach – N-best list ranking using neural sequence labelling models – for grammatical error correction. Word and character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Yang et al., 2017) have achieved good performance on the CoNLL-2014 shared task (Ng et al., 2014) on text correction (which covers a variety of errors made in written essays by second language learners). Schmaltz et al. (2017) show that character-level sequence-to-sequence models perform better than word-level models even with less training data than previous sequence-to-sequence ap1 Information about how the word re-segmented version of the ACL corpus is available will be posted at http://www.cl.uni-heidelberg.de/english/ research/downloads/resource_pages/ACL_ corrected/AC"
L18-1113,P16-1040,0,0.0180203,"esearcher studying Natural Language Processing can be just i f ied in concerning himself more with issues of practical performance in parsing sentences encountered in language as humans Actually use i t using a grammar expressed in a form corve˜ie: to the human linguist who is writing i t . Figure 1: Excerpt from paper P81-1001, A Practical Comparison of Parsing Strategies by Jonathan Slocum proaches, and outperform statistical phrase-based machine translation models on the CoNLL data. Chen et al. (2015a; Chen et al. (2015b) explore the use of GRUs and LSTMs for Chinese word segmentation, and Zhang et al. (2016) approach the task using word and character context in a globally optimized beam-search framework for neural structured prediction. Yang et al. (2017) build on this previous work to produce a modular neural-based segmentation model for Chinese, using fivecharacter window, pre-trained based on a variety of external resources. Based on these previous analyses into the kind of architectures that perform well for different types of error correction, we adopt a character-level sequence-to-sequence model for the word segmentation of English texts. 3. The ACL collection The ACL collection we work wit"
L18-1217,W07-1430,0,0.0581955,"Missing"
L18-1217,N16-1098,0,0.0475776,"Missing"
L18-1217,P16-1119,0,0.150291,"lasses influence the omissibility of adjectives under truth-conditional aspects. Amoia and Gardent (2008) published a data set where these and other syntactic and semantic properties of adjectives are tested in an RTE (recognizing textual entailment) setting. In this work, the context in which the semantic effects of adjectives are tested is the sentence, and the relevant criterion is preservation of truth when, e.g., deleting the adjective or the head noun as in the following inference pairs: Daisy is a big mouse → Daisy is a mouse or Daisy is a big mouse → Daisy is big. Along similar lines, Stanovsky and Dagan (2016) describe the construction process and resulting dataset of non-restrictive noun phrase modification. Nonrestrictive modifiers – e.g. The speaker thanked president Obama who just came into the room – can be removed to shorten sentences. The dataset gathered and annotated trough crowd sourcing has no restrictions on the length of the modifiers, which often span phrases. The context provided for annotation is a sentence for each modifier. From a conceptual point of view, modifiers were studied with respect to the distortion effect they have on the concept denoted by the head noun (Murphy, 2002)."
L18-1217,P12-1107,0,0.0605082,"Missing"
L18-1217,D17-1062,0,0.016999,"– extracting a sentence from a document, deleting a phrase from a sentence or a sub-phrase from a larger chunk. Vanderwende et al. (2007), Zajic et al. (2007) propose syntax-based trimming, where branches of a syntactic tree are scored using a combination of features that 1 http://www.cl.uni-heidelberg.de/english/ research/downloads/resource_pages/deModify/ deModify_data.shtml 1357 marks them for potential deletion. Wubben et al. (2012) approach the problem of text simplification as a machine translation problem trained on pairs of texts from Wikipedia and SimpleWikipedia. Wang et al. (2016), Zhang and Lapata (2017) reformulate this approach in the form of neural encoder-decoder models. Focusing on the modifiers, modification can be viewed from many different perspectives. From a linguistic point of view, several typologies of modifiers have been proposed (McNally, 2013). Of these, the semantic impact of modifiers is taken into account in: (a) the entailmentbased typology, in which modifiers are grouped into three broad categories based on the inferences they license, which stem from potential interpretations of the extension of modifiers, head nouns and the compounds as sets: intersective modifiers (mal"
nastase-etal-2010-wikinet,W06-2810,0,\N,Missing
nastase-etal-2010-wikinet,wentland-etal-2008-building,0,\N,Missing
P12-2051,W02-1006,0,0.0354087,"Missing"
P12-2051,P96-1006,0,0.0446905,"Missing"
P12-2051,N03-1033,0,0.00583767,"pets returned by a search on Google Books for each of the three epochs we consider. 2 For each open class word we create ranked lists of words, where the ranking score is an adjusted tfidf score – the epochs correspond to documents. To choose words frequent only in one epoch, we choose the top words in the list, for words frequent in all epochs we choose the bottom words in this list. 3 A minimum of 30 total examples was required for a word to be considered in the dataset. All the extracted snippets are then processed: the text is tokenized and part-of-speech tagged using the Stanford tagger (Toutanova et al., 2003), and contexts that do not include the target word with the specified part-of-speech are removed. The position of the target word is also identified and recorded as an offset along with the example. For illustration, we show below an example drawn from each epoch for two different words, dinner: 1800: On reaching Mr. Crane’s house, dinner was set before us ; but as is usual here in many places on the Sabbath, it was both dinner and tea combined into a single meal. 1900: The average dinner of today consists of relishes; of soup, either a consomme (clear soup) or a thick soup. 2000: Preparing di"
P13-1064,Y09-2026,0,0.161141,"rted through micro-averaged precision, recall and F1-score for the targeted class, as well as overall accuracy. The high results, on a par with text categorization experiments in the field, validates our experimental set-up. For the cross language categorization experiments described in this paper, we use the data described above, and train on one language (English/Italian), and test on the other, using the same Word etymologies are a novel source of linguistic information in NLP, possibly because resources that capture this information in a machine readable format are also novel. Fang et al. (2009) used limited etymological information extracted from the Collins English Dictionary (CED) for text categorization on the British National Corpus (BNC): information on the provenance of words (ranges of 654 Categories quality of life made in Italy tourism culture and school Total Training 5759 5711 5731 3665 20866 English Test 1989 1864 1857 1245 6955 Total 7748 7575 7588 4910 27821 Training 5781 6111 6090 6284 24266 Italian Test 1901 2068 2015 2104 8088 Total 7682 8179 8105 8388 32354 Table 1: Dataset statistics monolingual BoW categorization Prec Rec F1 Train EN / Test EN 0.92 0.92 0.92 Trai"
P13-1064,W05-0802,1,0.63796,"corpora through latent semantic analysis (LSA). Most CLTC methods rely heavily on machine translation (MT). MT has been used: to cast the cross-language text 653 probability distribution of etymologies in different versions of Latin – New Latin, Late Latin, Medieval Latin) was used in a “home-made” range classifier. The experiments presented in this paper use the bag-of-word document representation with absolute frequency values. To this basic representation we add word etymological ancestors and run classification experiments. We then use LSA – previously shown by (Dumais et al., 1997) and (Gliozzo and Strapparava, 2005) to be useful for this task – to induce the latent semantic dimensions of documents and words respectively, hypothesizing that word etymological ancestors will lead to semantic dimensions that transcend language boundaries. The vectors obtained through LSA (on the training data only) for words that are shared by the English training data and the Italian test data (names, and most importantly, etymological ancestors of words in the original documents) are then used for rerepresenting the training and test data. The same process is applied for Italian training and English test data. Classificati"
P13-1064,P06-1070,1,0.879523,"Missing"
P13-1064,D10-1103,0,0.0521905,"Missing"
P13-1064,P10-1114,0,0.0421059,"Missing"
P13-1064,I08-1022,0,0.0394112,"Missing"
P17-2086,W13-1732,0,0.391403,"Missing"
P17-2086,C12-1025,0,0.181793,"Missing"
P17-2086,W13-1733,0,0.0322676,"Missing"
P17-2086,W13-1718,0,0.167874,"). For each target L1, the number of essays is equal (900 in the train set, 100 in the development set and 100 in the test set). The distribution of the number of essays per topic is not perfectly balanced across different L1s, but they are rather close: the average number of essays per topic is 1,513 and the standard deviation is 229. 543 Type of Feature (1) word ngrams (2) lemma ngrams (3) word error toefl (4) word error icle (5) char ngrams (6) char error icle (7) char error toefl (1) + (2) (1) + (2) + (3) (1) + (2) + (4) (1) + (2) + (5) (1) + (2) + (6) (1) + (2) + (7) Jarvis et al. (2013) Nicolai et al. (2013) Italian, German, Turkish, Chinese and Japanese). They are referred to here as Spelling error ICLE and Spelling error TOEFL in the later part of this paper. Spelling errors are binary features. Spelling errors as character n-grams Every misspelled word in a text will be represented as character n-grams, where n = 1..3. Special characters marking the start and end of a word will be part of the n-grams. The value of these features is their relative frequencies, as for character n-grams. 3.2 Classifiers Following the proven effectiveness of Support Vector Machine (SVM) by numerous experiments on"
P17-2086,W13-1729,0,0.0647926,"Missing"
P17-2086,W13-1714,0,0.150597,"relative frequency with respect to the set of n-grams (unigram, bigram, trigram) that they belong to: Using complete words to represent spelling errors would not capture regularities that go beyond a single misspelled instance – like the preference of using i instead of e by Italian writers. We investigate the representation of spelling errors through character n-grams with size up to 3. We assess the effectiveness of using such feature representation for NLI and its contribution when combined with word and lemma n-grams, whose effectiveness has already been established (Gyawali et al., 2013; Jarvis et al., 2013). We report high classification results when using only spelling errors, and an improvement of 1.2 percentage points in accuracy, compared to the best results obtained in NLI shared task, when using spelling errors in combination with word and lemma features. 2 Methods Data The experiments are performed on the TOEFL11 corpus (Blanchard et al., 2013) of English essays written by non-native English learners as part of the Test of English as a Foreign Language (TOEFL). We also use the ICLEv2 corpus (Granger et al., 2009) for extracting additional spelling errors. The TOEFL11 corpus is not the mos"
S07-1003,W04-3205,0,0.0491922,"ierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. Stephens et al. (2001) propose 17 classes targeted to relations between genes. Lapata (2002) presents a binary classification of relations in nominalizations. There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work for all applications. For example, the gene-gene relation scheme of Stephens et al. (2001), with relations like X phosphorylates Y, is unlikely to be transferred easily to general text. We hav"
S07-1003,C92-2082,0,0.0345238,"a variety of methods (since we work with relations between nominals, the part of speech is always noun). We have used WordNet 3.0 on the Web and sense index tags. We chose the following semantic relations: Cause-Effect, Content-Container, InstrumentAgency, Origin-Entity, Part-Whole, ProductProducer and Theme-Tool. We wrote seven detailed definitions, including restrictions and conventions, plus prototypical positive and near-miss negative examples. For each relation separately, we based data collection on wild-card search patterns that Google allows. We built the patterns manually, following Hearst (1992) and Nakov and Hearst (2006). Instances of the relation Content-Container, for example, come up in response to queries such as “* contains *”, “* holds *”, “the * in the *”. Following the model of the Senseval-3 English Lexical Sample Task, we set out to collect 140 training and at least 70 test examples per relation, so we had a number of different patterns to ensure variety. We also aimed to collect a balanced number of positive and negative examples. The use of heuristic patterns to search for both positive and negative examples 14 should naturally result in negative examples that are near"
S07-1003,J02-3004,0,0.0275234,"oun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. Stephens et al. (2001) propose 17 classes targeted to relations between genes. Lapata (2002) presents a binary classification of relations in nominalizations. There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work for all applications. For example, the gene-gene relation scheme of Stephens et al. (2001), with relations like X phosphorylates Y, is unlikely to be transferred easily to general text. We have created a benchmark data set to allow the evaluation of different semantic relation classification algorithms. We do not presume to propose a single classification scheme, however allurin"
S07-1003,W04-2609,1,0.951787,"cine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. Stephens et al. (2001) propose 17 classes targeted to relations between genes. Lapata (2002) presents a binary classification of relations in nominalizations. There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work f"
S07-1003,W01-0511,0,0.263052,"cer relation. The classification occurs in the context of a sentence in a written English text. Algorithms for classifying semantic relations can be applied in information retrieval, information extraction, text summarization, question answering and so on. The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications. The literature shows a wide variety of methods of nominal relation classification. They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2"
S07-1003,P02-1032,0,0.0943661,"traction, text summarization, question answering and so on. The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications. The literature shows a wide variety of methods of nominal relation classification. They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and"
S07-1003,H05-1047,0,0.0359726,"their results. There were 14 teams who submitted 15 systems. 1 Task Description and Related Work The theme of Task 4 is the classification of semantic relations between simple nominals (nouns or base noun phrases) other than named entities – honey bee, for example, shows an instance of the ProductProducer relation. The classification occurs in the context of a sentence in a written English text. Algorithms for classifying semantic relations can be applied in information retrieval, information extraction, text summarization, question answering and so on. The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications. The literature shows a wide variety of methods of nominal relation classification. They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, w"
S07-1003,H91-1061,0,\N,Missing
S15-1022,P05-1074,0,0.0262158,"consistent semantics (positive and negative). For English, WordNet and VerbOcean were used as lexical resources. Italian WordNet was used for Italian, and GermaNet and German DerivBase (Zeller et al., 2013) were used as lexical resources for German. 1 As a part of Excitement Open Platform for Textual Entailment. https://github.com/hltfbk/EOP-1.2.1/ wiki/AlignmentEDAP1 195 Paraphrase Aligner. The paraphrase aligner concentrates on surface forms rather than lemmas and can align sequences of them rather than just individual tokens. It uses paraphrase tables, e.g. extracted from parallel corpora (Bannard and Callison-Burch, 2005). The alignment process is similar to the lexical aligner: any two sequences of tokens in T and H are aligned if the pair is listed in the resource. The alignment links created by this aligner instantiate only one relation (“paraphrase”) but report the strength of the relation via the translation probability. We used the paraphrase tables provided by the METEOR MT evaluation package (Denkowski and Lavie, 2014), which are available for numerous languages. Lemma Identity Aligner. This aligner does not use any resources. It simply aligns identical lemmas between T and H and plays an important rol"
S15-1022,J12-1003,1,0.930469,"al algorithm development. We demonstrate that a pilot open-source implementation of multi-level alignment with minimal features competes with state-of-theart open-source TE engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge resources, it largely ignores the algorithmic level. In fact, TE algorithms themselves ar"
S15-1022,W14-5201,0,0.054806,"Missing"
S15-1022,W14-3348,0,0.0536698,"ncentrates on surface forms rather than lemmas and can align sequences of them rather than just individual tokens. It uses paraphrase tables, e.g. extracted from parallel corpora (Bannard and Callison-Burch, 2005). The alignment process is similar to the lexical aligner: any two sequences of tokens in T and H are aligned if the pair is listed in the resource. The alignment links created by this aligner instantiate only one relation (“paraphrase”) but report the strength of the relation via the translation probability. We used the paraphrase tables provided by the METEOR MT evaluation package (Denkowski and Lavie, 2014), which are available for numerous languages. Lemma Identity Aligner. This aligner does not use any resources. It simply aligns identical lemmas between T and H and plays an important role in practice to deal with named entities. 3.3 A Minimal Feature Set Similar to the aligners, we concentrate on a small set of four features in the pilot algorithm. Again, the features are completely language independent, even at the implementation level. This is possible because the linguistic annotations and the alignments, use a language-independent type system (cf. Section 3.1). All current features measur"
S15-1022,E09-1025,0,0.0131132,"gnments T multi-level alignment T-H pair enriched with various levels of alignments H 3-extracting features T-H pair expressed as a data point 4-classifying entailment Entailment Decision Figure 1: Dataflow for TE algorithms based on multi level alignment that alignment strength can be misleading (MacCartney et al., 2006), alignment was understood as an intermediate step whose outcome is a set of correspondences between parts of T and H that can be used to define (mis-)match features. Alignments can be established at the word level, phrase level (MacCartney et al., 2008), or dependency level (Dinu and Wang, 2009). Dagan et al. (2013) generalized this practical use to an architectural principle: They showed that various TE algorithms can be mapped onto a universal alignment-based schema with six steps: preprocessing, enrichment, candidate alignment generation, alignment selection, and classification. Proposal. Our proposal is similar to, but simpler than, Dagan et al.’s. Figure 1 shows the data flow. First, the text and the hypothesis are linguistically pre-processed. Then, the annotated T-H pair becomes 194 the input for various independent aligners, which have access to knowledge resources and can co"
S15-1022,S14-1009,1,0.744724,"Missing"
S15-1022,W07-1401,1,0.817868,"Missing"
S15-1022,P06-1114,0,0.0407371,"ntral, powerful representation for TE algorithms that encourages modular, reusable, multilingual algorithm development. We demonstrate that a pilot open-source implementation of multi-level alignment with minimal features competes with state-of-theart open-source TE engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge r"
S15-1022,P10-4008,0,0.0227067,"E engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge resources, it largely ignores the algorithmic level. In fact, TE algorithms themselves are generally not designed to be extensible or interoperable. Therefore, changes to the algorithms – like adding support for a new language or for new analysis aspect – are often"
S15-1022,N06-1006,0,0.105121,"Missing"
S15-1022,D08-1084,0,0.0240179,"igner2 aligner3 knowledge resource 2-adding alignments T multi-level alignment T-H pair enriched with various levels of alignments H 3-extracting features T-H pair expressed as a data point 4-classifying entailment Entailment Decision Figure 1: Dataflow for TE algorithms based on multi level alignment that alignment strength can be misleading (MacCartney et al., 2006), alignment was understood as an intermediate step whose outcome is a set of correspondences between parts of T and H that can be used to define (mis-)match features. Alignments can be established at the word level, phrase level (MacCartney et al., 2008), or dependency level (Dinu and Wang, 2009). Dagan et al. (2013) generalized this practical use to an architectural principle: They showed that various TE algorithms can be mapped onto a universal alignment-based schema with six steps: preprocessing, enrichment, candidate alignment generation, alignment selection, and classification. Proposal. Our proposal is similar to, but simpler than, Dagan et al.’s. Figure 1 shows the data flow. First, the text and the hypothesis are linguistically pre-processed. Then, the annotated T-H pair becomes 194 the input for various independent aligners, which ha"
S15-1022,P14-5008,1,0.767679,"Missing"
S15-1022,P12-3013,1,0.852425,"with state-of-theart open-source TE engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge resources, it largely ignores the algorithmic level. In fact, TE algorithms themselves are generally not designed to be extensible or interoperable. Therefore, changes to the algorithms – like adding support for a new language o"
S15-1022,D09-1082,0,0.0349226,"Missing"
S15-1022,P13-1118,1,0.780789,"Missing"
S17-1027,P15-1123,0,0.0608045,"re easily transferable. We present experiments for English and German that achieve competitive performance. We present a novel take on modeling and exploiting genre information and showcase the adaptation of our system from one language to another. 1 Introduction Semantic clause types, called Situation Entity (SE) types (Smith, 2003; Palmer et al., 2007) are linguistic characterizations of aspectual properties shown to be useful for argumentation structure analysis (Becker et al., 2016b), genre characterization (Palmer and Friedrich, 2014), and detection of generic and generalizing sentences (Friedrich and Pinkal, 2015). Recent work on automatic identification of SE types relies on feature-based classifiers for English that have been successfully applied to various textual genres (Friedrich et al., 2016), and also show that a sequence labeling approach that models contextual clause labels yields improved classification performance. 230 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 230–240, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics We present a novel take on modeling and exploiting genre information and test it on"
S17-1027,W16-2803,1,0.91818,"the advantage of our neural model is that it avoids the need to reproduce linguistic features for other languages and is thus more easily transferable. We present experiments for English and German that achieve competitive performance. We present a novel take on modeling and exploiting genre information and showcase the adaptation of our system from one language to another. 1 Introduction Semantic clause types, called Situation Entity (SE) types (Smith, 2003; Palmer et al., 2007) are linguistic characterizations of aspectual properties shown to be useful for argumentation structure analysis (Becker et al., 2016b), genre characterization (Palmer and Friedrich, 2014), and detection of generic and generalizing sentences (Friedrich and Pinkal, 2015). Recent work on automatic identification of SE types relies on feature-based classifiers for English that have been successfully applied to various textual genres (Friedrich et al., 2016), and also show that a sequence labeling approach that models contextual clause labels yields improved classification performance. 230 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 230–240, c Vancouver, Canada, August 3-4,"
S17-1027,P10-2013,0,0.0559985,"Missing"
S17-1027,P14-1062,0,0.0646515,"Missing"
S17-1027,D14-1181,0,0.0271982,"deling” ♠ Heidelberg University, Department of Computational Linguistics ♣ University of North Texas, Department of Linguistics {mbecker,staniek,nastase,frank}@cl.uni-heidelberg.de alexis.palmer@unt.edu Abstract Deep learning provides a powerful framework in which linguistic and semantic regularities can be implicitly captured through word embeddings (Mikolov et al., 2013b). Patterns in larger text fragments can be encoded and exploited by recurrent (RNNs) or convolutional neural networks (CNNs) which have been successfully used for various sentence-based classification tasks, e.g. sentiment (Kim, 2014) or relation classification (Vu et al., 2016; Tai et al., 2015). We frame the task of classifying clauses with respect to their aspectual properties – i.e., situation entity types – in a recurrent neural network architecture. We adopt a Gated Recurrent Unit (GRU)based RNN architecture that is well suited to modeling long sequences (Yin et al., 2017). This initial model is enhanced with an attention mechanism shown to be beneficial for sentence classification (Wang et al., 2016) and sequence modeling (Dong and Lapata, 2016). We explore the usefulness of attention in two settings: (i) the indivi"
S17-1027,J92-4003,0,0.43155,"Missing"
S17-1027,N16-1030,0,0.0618447,"state (memory) at time t, and h˜t is the candidate activation at time t. W∗ and U∗ are weights that are learned. denotes the element-wise multiplication of two vectors. rt = σ(Wr xt + Ur ht−1 ) h˜t = tanh(W xt + U (rt ht−1 )) zt = σ(Wz xt + Uz ht−1 ) 2014). RNN variations – with Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) – have since achieved state of the art performance in both sequence modeling and classification tasks. Recent work applies bi-LSTM models in sequence modeling (PoS tagging, Plank et al. (2016), NER Lample et al. (2016)) and structure prediction tasks (Semantic Role Labeling, Zhou and Xu (2015) or semantic parsing into logical forms Dong and Lapata (2016)). Tree-based LSTM models have been shown to often perform better than purely sequential bi-LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), but depend on parsed input. Attention. Attention has been established as an effective mechanism that allows models to focus on specific words in the larger context. A model with attention learns what input tokens or token sequences to attend to and thus does not need to capture the complete input information in its hidd"
S17-1027,W14-4012,0,0.0847139,"Missing"
S17-1027,P16-1004,0,0.18652,"successfully used for various sentence-based classification tasks, e.g. sentiment (Kim, 2014) or relation classification (Vu et al., 2016; Tai et al., 2015). We frame the task of classifying clauses with respect to their aspectual properties – i.e., situation entity types – in a recurrent neural network architecture. We adopt a Gated Recurrent Unit (GRU)based RNN architecture that is well suited to modeling long sequences (Yin et al., 2017). This initial model is enhanced with an attention mechanism shown to be beneficial for sentence classification (Wang et al., 2016) and sequence modeling (Dong and Lapata, 2016). We explore the usefulness of attention in two settings: (i) the individual classification task and (ii) in a setting approximating sequential labeling in which the attention vector provides features that describe the clauses preceding the current instance. Compared to the strong baseline provided by the feature based system of Friedrich et al. (2016), we achieve competitive performance and find that attention as well as context representation using predicted or goldstandard labels of the previous N clauses, and text genre information improve our model. A strong motivation for developing NN-b"
S17-1027,loaiciga-etal-2014-english,0,0.0424883,"Missing"
S17-1027,W14-4921,1,0.86387,"Missing"
S17-1027,J93-2004,0,0.060196,"Missing"
S17-1027,P16-1166,1,0.352268,"GRU)based RNN architecture that is well suited to modeling long sequences (Yin et al., 2017). This initial model is enhanced with an attention mechanism shown to be beneficial for sentence classification (Wang et al., 2016) and sequence modeling (Dong and Lapata, 2016). We explore the usefulness of attention in two settings: (i) the individual classification task and (ii) in a setting approximating sequential labeling in which the attention vector provides features that describe the clauses preceding the current instance. Compared to the strong baseline provided by the feature based system of Friedrich et al. (2016), we achieve competitive performance and find that attention as well as context representation using predicted or goldstandard labels of the previous N clauses, and text genre information improve our model. A strong motivation for developing NN-based systems is that they can be transferred with low cost to other languages without major feature engineering or use of hand-crafted linguistic knowledge resources. Given the highly-engineered feature sets used for SE classification so far (Friedrich et al., 2016), porting such classifiers to other languages is a non-trivial issue. We test the portab"
S17-1027,W15-2702,1,0.892449,"show that a sequence labeling approach that models contextual clause labels yields improved classification performance. 230 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 230–240, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics We present a novel take on modeling and exploiting genre information and test it on the English multi-genre corpus of Friedrich et al. (2016). distribution of SE types in text passages and discourse modes, e.g., narrative, informative, or argumentative (Palmer and Friedrich, 2014; Mavridou et al., 2015; Becker et al., 2016a). Our aims and contributions are: (i) We study the performance of GRU-based models enhanced with attention for modeling local and non-local characteristics of semantic clause types. (ii) We compare the effectiveness of the learned attention weights as features for a sequence labeling system to the explicitly defined syntactic-semantic features in (Friedrich et al., 2016). (iii) We define extensions of our models that integrate external knowledge about genre and show that this can be used to improve classification performance across genres. (iv) We test the portability of"
S17-1027,W15-1603,1,0.839673,"ing both attention and genre information leads to a 2.13 pp increase over the model that uses only attention. Adding context information beyond the local clause – a window of up to three previous clauses – improves the wordbased attention models slightly, but a wider window (four or more clauses) causes a major drop Baseline systems. The feature-based system of Palmer07 (Palmer et al., 2007) (Palmer07 in Table 2) simulates context through predicted labels from previous clauses. Friedrich et al. (2016) (Fried16 in Table 2) report results for their CRF-based SE 7 The Wiki texts were selected by Friedrich et al. (2015) precisely in order to target G ENERIC S ENTENCE clauses. 8 The cross validation splits of the data used by Friedrich et al. (2016) are not available. 235 GRU + att + gLab (1) GRU + att + gLab (2) GRU + att + gLab (3) GRU + att + gLab (4) GRU + att + gLab (5) GRU + att + gLab + genre (1) GRU + att + gLab + genre (2) GRU + att + gLab + genre (3) GRU + att + gLab + genre (4) GRU + att + gLab + genre (5) Acc 72.71 72.68 72.66 72.61 73.40 73.44 73.45 72.84 73.12 73.34 F1 65.37 66.51 65.03 64.33 66.39 66.76 66.51 66.29 66.21 66.13 Table 4: SE-type classification on English test set, sequence oracle"
S17-1027,N03-1030,0,0.361054,"Missing"
S17-1027,N13-1090,0,0.12279,"ling Context and Genre Characteristics with Recurrent Neural Networks and Attention Maria Becker♦♠ , Michael Staniek♦♠ , Vivi Nastase♦♠ , Alexis Palmer♣ , Anette Frank♦♠ ♦ Leibniz ScienceCampus “Empirical Linguistics and Computational Language Modeling” ♠ Heidelberg University, Department of Computational Linguistics ♣ University of North Texas, Department of Linguistics {mbecker,staniek,nastase,frank}@cl.uni-heidelberg.de alexis.palmer@unt.edu Abstract Deep learning provides a powerful framework in which linguistic and semantic regularities can be implicitly captured through word embeddings (Mikolov et al., 2013b). Patterns in larger text fragments can be encoded and exploited by recurrent (RNNs) or convolutional neural networks (CNNs) which have been successfully used for various sentence-based classification tasks, e.g. sentiment (Kim, 2014) or relation classification (Vu et al., 2016; Tai et al., 2015). We frame the task of classifying clauses with respect to their aspectual properties – i.e., situation entity types – in a recurrent neural network architecture. We adopt a Gated Recurrent Unit (GRU)based RNN architecture that is well suited to modeling long sequences (Yin et al., 2017). This initia"
S17-1027,P15-1150,0,0.094046,"Missing"
S17-1027,P16-1105,0,0.0374456,"with Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) – have since achieved state of the art performance in both sequence modeling and classification tasks. Recent work applies bi-LSTM models in sequence modeling (PoS tagging, Plank et al. (2016), NER Lample et al. (2016)) and structure prediction tasks (Semantic Role Labeling, Zhou and Xu (2015) or semantic parsing into logical forms Dong and Lapata (2016)). Tree-based LSTM models have been shown to often perform better than purely sequential bi-LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), but depend on parsed input. Attention. Attention has been established as an effective mechanism that allows models to focus on specific words in the larger context. A model with attention learns what input tokens or token sequences to attend to and thus does not need to capture the complete input information in its hidden state. Attention has been used successfully e.g. in aspect-based sentiment classification (Wang et al., 2016), for modeling relations between words or phrases in encoder-decoder models for translation (Bahdanau et al., 2015), or bi-clausal classification tasks such as textu"
S17-1027,N16-1065,0,0.0546958,"Missing"
S17-1027,P07-1113,1,0.66066,"ant context not only for the current instance, but also for the larger context. Apart from implicitly capturing task relevant features, the advantage of our neural model is that it avoids the need to reproduce linguistic features for other languages and is thus more easily transferable. We present experiments for English and German that achieve competitive performance. We present a novel take on modeling and exploiting genre information and showcase the adaptation of our system from one language to another. 1 Introduction Semantic clause types, called Situation Entity (SE) types (Smith, 2003; Palmer et al., 2007) are linguistic characterizations of aspectual properties shown to be useful for argumentation structure analysis (Becker et al., 2016b), genre characterization (Palmer and Friedrich, 2014), and detection of generic and generalizing sentences (Friedrich and Pinkal, 2015). Recent work on automatic identification of SE types relies on feature-based classifiers for English that have been successfully applied to various textual genres (Friedrich et al., 2016), and also show that a sequence labeling approach that models contextual clause labels yields improved classification performance. 230 Procee"
S17-1027,D16-1058,0,0.172418,"nal neural networks (CNNs) which have been successfully used for various sentence-based classification tasks, e.g. sentiment (Kim, 2014) or relation classification (Vu et al., 2016; Tai et al., 2015). We frame the task of classifying clauses with respect to their aspectual properties – i.e., situation entity types – in a recurrent neural network architecture. We adopt a Gated Recurrent Unit (GRU)based RNN architecture that is well suited to modeling long sequences (Yin et al., 2017). This initial model is enhanced with an attention mechanism shown to be beneficial for sentence classification (Wang et al., 2016) and sequence modeling (Dong and Lapata, 2016). We explore the usefulness of attention in two settings: (i) the individual classification task and (ii) in a setting approximating sequential labeling in which the attention vector provides features that describe the clauses preceding the current instance. Compared to the strong baseline provided by the feature based system of Friedrich et al. (2016), we achieve competitive performance and find that attention as well as context representation using predicted or goldstandard labels of the previous N clauses, and text genre information improve our"
S17-1027,P16-2067,0,0.0229775,"to keep. ht is the hidden state (memory) at time t, and h˜t is the candidate activation at time t. W∗ and U∗ are weights that are learned. denotes the element-wise multiplication of two vectors. rt = σ(Wr xt + Ur ht−1 ) h˜t = tanh(W xt + U (rt ht−1 )) zt = σ(Wz xt + Uz ht−1 ) 2014). RNN variations – with Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) – have since achieved state of the art performance in both sequence modeling and classification tasks. Recent work applies bi-LSTM models in sequence modeling (PoS tagging, Plank et al. (2016), NER Lample et al. (2016)) and structure prediction tasks (Semantic Role Labeling, Zhou and Xu (2015) or semantic parsing into logical forms Dong and Lapata (2016)). Tree-based LSTM models have been shown to often perform better than purely sequential bi-LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), but depend on parsed input. Attention. Attention has been established as an effective mechanism that allows models to focus on specific words in the larger context. A model with attention learns what input tokens or token sequences to attend to and thus does not need to capture the complete inp"
S17-1027,Q16-1027,0,0.0166683,"hbrenner et al., son/thing/situation the clause is about, often realized as its grammatical subject. 2 Code and data: https://github.com/annefried/sitent The main referent of a clause is roughly the per231 rent connections, which allow them to find patterns in – and thus model – sequences. Simple RNNs cannot capture long-term dependencies (Bengio et al., 1994) because the gradients tend to vanish or grow out of control with long sequences. Gated Recurrent Unit (GRU) RNNs, proposed by Cho et al. (2014), address this shortcoming. GRUs have fewer parameters and thus need less data to generalize (Zhou et al., 2016) than LSTM RNNs, and also outperform the LSTM in many cases (Yin et al., 2017), which makes them a good choice for our relatively small dataset.3 The relevant equations for a GRU are below. xt is the input at time t, rt is a reset gate which determines how to combine the new input with the previous memory, and the update gate zt defines how much of the previous memory to keep. ht is the hidden state (memory) at time t, and h˜t is the candidate activation at time t. W∗ and U∗ are weights that are learned. denotes the element-wise multiplication of two vectors. rt = σ(Wr xt + Ur ht−1 ) h˜t = tan"
S17-1027,P15-1109,0,0.0139583,"nd U∗ are weights that are learned. denotes the element-wise multiplication of two vectors. rt = σ(Wr xt + Ur ht−1 ) h˜t = tanh(W xt + U (rt ht−1 )) zt = σ(Wz xt + Uz ht−1 ) 2014). RNN variations – with Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) – have since achieved state of the art performance in both sequence modeling and classification tasks. Recent work applies bi-LSTM models in sequence modeling (PoS tagging, Plank et al. (2016), NER Lample et al. (2016)) and structure prediction tasks (Semantic Role Labeling, Zhou and Xu (2015) or semantic parsing into logical forms Dong and Lapata (2016)). Tree-based LSTM models have been shown to often perform better than purely sequential bi-LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), but depend on parsed input. Attention. Attention has been established as an effective mechanism that allows models to focus on specific words in the larger context. A model with attention learns what input tokens or token sequences to attend to and thus does not need to capture the complete input information in its hidden state. Attention has been used successfully e.g. in aspect-based sentimen"
S19-1016,D15-1173,0,0.0180427,"r the knowledge graph. To be used as features shared by multiple instances, the information about nodes on the paths is removed, transforming the actual paths into ”meta-paths”. The paths themselves can be incorporated in different ways in a model – as features (Lao et al., 2011; Gardner et al., 2014), as Horn clauses to provide rules for inference in KGs whether directly or through scores that represent the strength of the path as a direct relation (Neelakantan et al., 2015; Guu et al., 2015), also taking into account information about intermediary nodes (Das et al., 2017; Yin et al., 2018). Gardner and Mitchell (2015) perform link prediction using random walks but do not attempt to connect a source and target node, but rather to characterize the local structure around a (source or target) node using such localized paths. Using these subgraph features leads to better results for the knowledge graph completion task. We focus here on discovering useful and explanatory paths, not on optimizing or further improving the KGC task. Using paths can lead to interpretable models because the paths can help explain the predicted fact. Meng et al. (2015) present a method to automate the induction of metapaths in large h"
S19-1016,D14-1044,0,0.0723111,"ection, subset, superset) information between the different relations’ domains and ranges. This step drastically reduces the graph size, making many different graph processing approaches more tractable. We investigate whether in this graph that represents a more general version of the information in the original KG, good patterns/paths are stronger and easier to find, because the aggregated view compensates for individual missing edges throughout the graph. We test the extracted paths through the link prediction task on Freebase (Bollacker et al., 2008) and NELL (Carlson et al., 2010a), using Gardner et al. (2014)’s experimental set-up: pairs of nodes are represented using their connected paths as feaKnowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original exten"
S19-1016,D13-1080,0,0.0454623,"Missing"
S19-1016,D15-1038,0,0.036836,"Missing"
S19-1016,D14-1165,0,0.0353278,"haracteristic: they have strongly typed relations, i.e. the source and target of a relation have a very specific type. NELL for example, has relations such as like ActorStarredinMovie, StateHasLake, and Freebase has /film/film/rating, /book/literary series/author, whose arguments have type Person, Movie, State, etc. Previous work has shown that using node type information – provided in Freebase through the domain and range types for each relation – can help optimize computation for link prediction by filtering the entity matrix for each relation based on the relation’s domain and range types (Chang et al., 2014), improve prediction by adding a factor in the loss function that accounts for the type of the entities involved in a relation (Kotnis and Nastase, 2017), or improve predictions based on paths in the graph by using the types of intermediary entities (Yin et al., 2018). Entity types and the type of the domain and range of a relation have been proven to be useful for improving link prediction models. We investigate here the hypothesis that by relying on the fact that such strong constraints on the arguments of relations in Freebase exist, we can build an intensional graph of the knowledge reposi"
S19-1016,N18-1165,0,0.0130033,"xplain the predicted fact. Meng et al. (2015) present a method to automate the induction of metapaths in large heterogeneous information networks (a.k.a. knowledge graphs) for given node pairs, even if the given node pairs are not connected by a direct relation. Path information is also found to improve performance since paths help the model learn logical rules. However, mining paths from a large knowledge graph is often computationally expensive since it involves performing a traversal through the graph. To overcome this limitation (Das et al., 2017) proposed deep reinforcement learning and (Chen et al., 2018) proposed RNNS for generating paths. However, many datasets suffer from paths sparsity, lack of enough paths connecting source target pairs, resulting in poor performance for many relations. Wang et al. (2013) have a different approach – they start with patterns in the form of first-order probabilistic rules, which they then ground in a small subgraph of a large knowledge graph. The approach we present here combines different elements of these previous approaches in a novel way: we build an abstract graph to find pattures, and a model for predicting the direct relations is learned and tested o"
S19-1016,D11-1049,0,0.31383,"., 2016) for an overview) predict additional edges in the graph, based on induced node and edge representations that encode the structure of the graph and thus capture regularities (such as homophily). Lao and Cohen (2010) introduced a new method that predicts direct links based on paths that connect the source and target nodes. Such paths are not only useful for link prediction (Lao et al., 147 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 147–157 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics ner et al., 2014). Lao et al. (2011) obtain paths for given node pairs using random walks over the knowledge graph. To be used as features shared by multiple instances, the information about nodes on the paths is removed, transforming the actual paths into ”meta-paths”. The paths themselves can be incorporated in different ways in a model – as features (Lao et al., 2011; Gardner et al., 2014), as Horn clauses to provide rules for inference in KGs whether directly or through scores that represent the strength of the path as a direct relation (Neelakantan et al., 2015; Guu et al., 2015), also taking into account information about"
S19-1016,P18-1011,0,0.0416316,"Missing"
S19-1016,P15-1016,0,0.107164,"019. 2019 Association for Computational Linguistics ner et al., 2014). Lao et al. (2011) obtain paths for given node pairs using random walks over the knowledge graph. To be used as features shared by multiple instances, the information about nodes on the paths is removed, transforming the actual paths into ”meta-paths”. The paths themselves can be incorporated in different ways in a model – as features (Lao et al., 2011; Gardner et al., 2014), as Horn clauses to provide rules for inference in KGs whether directly or through scores that represent the strength of the path as a direct relation (Neelakantan et al., 2015; Guu et al., 2015), also taking into account information about intermediary nodes (Das et al., 2017; Yin et al., 2018). Gardner and Mitchell (2015) perform link prediction using random walks but do not attempt to connect a source and target node, but rather to characterize the local structure around a (source or target) node using such localized paths. Using these subgraph features leads to better results for the knowledge graph completion task. We focus here on discovering useful and explanatory paths, not on optimizing or further improving the KGC task. Using paths can lead to interpretable"
S19-1016,C18-1200,0,0.0273395,"Missing"
W02-2021,W98-1504,0,\N,Missing
W02-2021,P94-1013,0,\N,Missing
W02-2021,P01-1005,0,\N,Missing
W06-3805,O01-1003,0,0.0713771,"Missing"
W06-3805,H05-1049,0,0.0746317,"Missing"
W06-3805,P04-3020,0,0.0271221,"nformation that can be extracted from a topic description. In particular, we look at connections between open-class words. A dependency parser, MiniPar (Lin, 1998), builds a dependency relation graph for each sentence. We apply such graphs in two ways. We match a graph that covers the entire topic description against the graph for each sentence in the collection. We also extract all pairs of open-class words from the topic description, and check whether they are connected in the sentence graphs. Both methods let us rank sentences; the top-ranking ones go into a summary Erkan and Radev (2004), Mihalcea (2004), Mihalcea and Tarau (2004) introduced graph methods for summarization, word sense disambiguation and other NLP applications. The summarization graph-based systems implement a form of sentence ranking, based on the idea of prestige or centrality in social networks. In this case the network consists of sentences, and significantly similar sentences are interconnected. Various measures (such as node degree) help find the most central sentences, or to score each sentence. In topic-driven summarization, one or more sentences or questions describe an information need which the summaries must addres"
W06-3805,N04-1019,0,0.0391172,"ents for a given topic, one sentence per line, cleaned of XML tags. We process each file with MiniPar, and post-process the output similarly to the topics. For documents we keep the list of dependency relations but not a separate list of words. This processing also gives one file per topic, each sentence followed by its list of dependency relations. 3.3 Summary Content Units The DUC 2005 summary evaluation included an analysis based on Summary Content Units. SCUs are manually-selected topic-specific summaryworthy phrases which the summarization systems are expected to include in their output (Nenkova and Passonneau, 2004; Copeck and Szpakowicz, 2005). The SCUs for 20 of the test topics became available after the challenge. We use the SCU data to measure the performance of our graph-matching and path-search algorithms: the total number, weight and number of unique SCUs per summary, and the number of negative SCU sentences, explicitly marked as not relevant to the summary. 30 The overall score is S = SN + W eightF actor ∗ SE , where W eightF actor ∈ {0, 1, 2, ..., 15, 20, 50, 100}. Varying the weight factor allows us to find various combinations of node and edge score matches which work best for sentence extrac"
W06-3805,W04-3252,0,\N,Missing
W06-3813,P98-1013,0,0.0863126,"s an old idea. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 . He was a grammarian who analysed Sanskrit (Misra, 1966). The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesni`ere, 1959; Gruber, 1965; Fillmore, 1968). Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic 1 7 th The sources date his work variously between the 5th and century. role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005). Graph-like structures are a natural way of organising one’s impressions of a text seen from the perspective of connections between its simpler constituents of varying granularity, from sections through paragraphs, sentences, clauses, phrases, words to morphemes. In this work we pursue a well-known and often tacitly assumed line of thinking: connections at the syntactic level reflect connections at the semantic level (in other words, syntax carries meaning). Anecdotal s"
W06-3813,J02-3001,0,0.0131234,"sign a relation between two clauses, a verb and its arguments, or a noun and its modifier) and about the accuracy of the suggestion. Discussion and conclusions appear in Section 6. 2 Related Work Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998; Gildea and Jurafsky, 2002; Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996; Rosario et al., 2002)). Lists of semantic relations are designed to capture salient domain information. In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. It helps them build complex knowledge bases by combining components: events, entities and modifiers (Clark and Porter, 1997). The system’s interface facilitates the expert’s task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semant"
W06-3813,P05-1072,0,0.0123887,"anipulating structures which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semantic relation analysis, the focus is on semantic roles – relations between verbs and their arguments. Most approaches rely on VerbNet (Kipper et al., 2000) and FrameNet (Baker et al., 1998) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions (Carreras and Marquez, 2004; Carreras and Marquez, 2005) and also (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Shi and Mihalcea, 2005). These systems share two ideas which make them different from the approach presented here: they all analyse verb-argument relations, and they all use machine learning or probabilistic approaches (Pradhan et al., 2005) to assign a label to a new instance. Labelling every instance relies on the same previously encoded knowledge (see (Carreras and Marquez, 2004; Carreras and Marquez, 2005) for an overview of the systems in the semantic role labelling competitions from 2004 and 2005). Pradhan 82 et al. (2005) combine the outputs of multiple parsers to extract reliable syn"
W06-3813,W01-0511,0,0.0834081,"nd shows how often the heuristic was used when processing the input text. We show in detail our findings about syntactic levels (how often graph matching helped assign a relation between two clauses, a verb and its arguments, or a noun and its modifier) and about the accuracy of the suggestion. Discussion and conclusions appear in Section 6. 2 Related Work Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998; Gildea and Jurafsky, 2002; Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996; Rosario et al., 2002)). Lists of semantic relations are designed to capture salient domain information. In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. It helps them build complex knowledge bases by combining components: events, entities and modifiers (Clark and Porter, 1997). The system’s interface facilitates the expert"
W06-3813,C98-1013,0,\N,Missing
W06-3813,P02-1032,0,\N,Missing
W18-4518,D11-1142,0,0.016206,"thods comes from open information extraction (Open IE), particularly work where relations are described through POS and grammatical patterns (Banko et al., 2007), and using the assumption that binary relations often appear in a subject-verb-object format. Wu and Weld (2010) extend a previous distantly supervised system that learns using Wikipedia infobox relations, and describe relations (through POS and dependency patterns) as phrases consisting of at least one verb and/or a preposition. To counter noisy extractions (often phrases that are too long and too specific to constitute a relation), Fader et al. (2011) introduce ReVerb, where lexical and syntactic constraints on relation expressions serve to produce cleaner extractions, with less uninformative or incoherent expressions. To increase recall, Mausam et al. (2012) expand the patterns by allowing relational nouns (e.g. Bill Gates, co-founder of Microsoft ...). Patterns are gathered and generalized (e.g., from words to parts-of-speech), which boosts recall. All these relation extraction methods rely on redundancy in the data to verify the relation patterns and the extracted candidates through various reliability scores. This cannot be applied to"
W18-4518,W17-2207,0,0.0214532,"seconds of that are needed to process the data from our KG file, the remaining time is the rendering time. 11 166 this reason, we applied little and careful normalization to the regests. We concur that “the highly variable spelling found in many historical texts has remained one of the most troublesome issues for NLP” (p. 83), a fact that may be especially true with regard to the RI. Due to these issues, the research conducted in processing of (German) historical texts has been diverse and very task-specific, see i.a. (Massad et al., 2013; Mero˜no-Pe˜nuela et al., 2015; Seemann et al., 2017; Hench, 2017; Schulz and Keller, 2016). Regesta Imperii. Not much NLP research has been conducted on the RI. Kuczera (2015), in an example experiment, projects attributes and relations between entities from the times of Friedrich III. (i.e. a subset of the RI) into a graph database. He applies no NLP in these steps, but relies on the manually created person registers for the universe of Friedrich III. While currently only the registers for Friedrich III. are available, we think that they may be used in future work to fine tune the NER system to achieve better performance not only on the sub-corpus of Frie"
W18-4518,D12-1048,0,0.0174352,"ften appear in a subject-verb-object format. Wu and Weld (2010) extend a previous distantly supervised system that learns using Wikipedia infobox relations, and describe relations (through POS and dependency patterns) as phrases consisting of at least one verb and/or a preposition. To counter noisy extractions (often phrases that are too long and too specific to constitute a relation), Fader et al. (2011) introduce ReVerb, where lexical and syntactic constraints on relation expressions serve to produce cleaner extractions, with less uninformative or incoherent expressions. To increase recall, Mausam et al. (2012) expand the patterns by allowing relational nouns (e.g. Bill Gates, co-founder of Microsoft ...). Patterns are gathered and generalized (e.g., from words to parts-of-speech), which boosts recall. All these relation extraction methods rely on redundancy in the data to verify the relation patterns and the extracted candidates through various reliability scores. This cannot be applied to the RI corpus. We rely instead on the relatively simple structure of the regest texts and the grammatical information obtained from a parser. 6 Conclusions We induced a Knowledge Graph from the medieval Regesta I"
W18-4518,W16-2108,1,0.681363,"ich III. (i.e. a subset of the RI) into a graph database. He applies no NLP in these steps, but relies on the manually created person registers for the universe of Friedrich III. While currently only the registers for Friedrich III. are available, we think that they may be used in future work to fine tune the NER system to achieve better performance not only on the sub-corpus of Friedrich III. but also on the whole RI. A caveat is that the regests of the other emperors differ significantly not only in named entities but also in linguistic variety and thus such a system may fail to generalize. Opitz and Frank (2016) manually labeled 500 randomly sampled regests with 12 medieval themes and players of interest (e.g. nobles, spiritual institutions, war and peace or justice) and trained binary classifiers to in the end label all regests and compute statistics about the importance of the medieval themes and players with regard to time. Relation Extraction For relation extraction from the Regesta Imperii we work in an ”extreme” environment – no annotated data; few language resources (particularly because of the German language variations used in the corpus); no tools like named entity taggers or chunkers that"
W18-4518,W16-2105,0,0.0228105,"hat are needed to process the data from our KG file, the remaining time is the rendering time. 11 166 this reason, we applied little and careful normalization to the regests. We concur that “the highly variable spelling found in many historical texts has remained one of the most troublesome issues for NLP” (p. 83), a fact that may be especially true with regard to the RI. Due to these issues, the research conducted in processing of (German) historical texts has been diverse and very task-specific, see i.a. (Massad et al., 2013; Mero˜no-Pe˜nuela et al., 2015; Seemann et al., 2017; Hench, 2017; Schulz and Keller, 2016). Regesta Imperii. Not much NLP research has been conducted on the RI. Kuczera (2015), in an example experiment, projects attributes and relations between entities from the times of Friedrich III. (i.e. a subset of the RI) into a graph database. He applies no NLP in these steps, but relies on the manually created person registers for the universe of Friedrich III. While currently only the registers for Friedrich III. are available, we think that they may be used in future work to fine tune the NER system to achieve better performance not only on the sub-corpus of Friedrich III. but also on the"
W18-4518,W17-2206,0,0.0530842,"Missing"
W18-4518,P10-1013,0,0.0193165,"resources (particularly because of the German language variations used in the corpus); no tools like named entity taggers or chunkers that would help in identifying relations and relation arguments; and low or no redundancy in relation instances (for computing extraction reliability scores). Because of this, the inspiration for the applied methods comes from open information extraction (Open IE), particularly work where relations are described through POS and grammatical patterns (Banko et al., 2007), and using the assumption that binary relations often appear in a subject-verb-object format. Wu and Weld (2010) extend a previous distantly supervised system that learns using Wikipedia infobox relations, and describe relations (through POS and dependency patterns) as phrases consisting of at least one verb and/or a preposition. To counter noisy extractions (often phrases that are too long and too specific to constitute a relation), Fader et al. (2011) introduce ReVerb, where lexical and syntactic constraints on relation expressions serve to produce cleaner extractions, with less uninformative or incoherent expressions. To increase recall, Mausam et al. (2012) expand the patterns by allowing relational"
W18-6218,W17-5042,1,0.735025,"culture specific (Wierzbicka, 1994, 1999), and thus could be indicative of the native language of a speaker. 3.1 Experiment setup 3.3 Features 3.3.1 Part-of-speech tags and function words POS tag n-grams and function words (FWs) are considered core features in NLI research (Malmasi and Dras, 2015), not susceptible to topic bias, unlike word and character n-grams (Brooke and Hirst, 2011). POS n-grams, n=1..3 POS features capture the morpho-syntactic patterns in a text, and are indicative of the L1, especially when used in combination with other types of features (Cimino and Dell’Orletta, 2017; Markov et al., 2017). POS tags were obtained with TreeTagger (Schmid, 1999), which uses the Penn Treebank tagset (36 tags). Datasets We conduct experiments on two datasets commonly used in NLI research: TOEFL11 (Blanchard et al., 2013): the ETS Corpus of Non-Native Written English (TOEFL 11) contains 1,100 essays in English (avg. 348 tokens/essay) for each of the 11 L1s: Arabic (ARA), Chinese (CHI), French (FRE), German (GER), Hindi (HIN), Italian (ITA), Japanese (JPN), Korean (KOR), Spanish (SPA), Telugu (TEL), and Function words (FWs) n-grams, n=1..3 Function words clarify the relationships between the content-"
W18-6218,C18-1293,1,0.779736,"Missing"
W18-6218,W17-5049,0,0.719417,"Missing"
W18-6218,D14-1142,0,0.218691,"his by evaluating the impact of emotion-based features on classifying the L1 of the authors of essays written in L2. 3 Turkish (TUR). The essays were written in response to eight different writing prompts, all of which appear in all 11 L1 groups. The dataset contains information regarding the proficiency level (low, medium, high) of the authors. ICLE (Granger et al., 2009): the ICLEv2 dataset consists of essays written by highlyproficient non-native college-level students of English. We used a 7-language subset of the corpus normalized for topic and character encoding (Tetreault et al., 2012; Ionescu et al., 2014) to which we refer as ICLE. This subset contains 110 essays (avg. 747 tokens/essay after tokenization and removal of metadata) for each of the 7 languages: Bulgarian (BUL), Chinese (CHI), Czech (CZE), French (FRE), Japanese (JPN), Russian (RUS), and Spanish (SPA). 3.2 We used the (pre-)tokenized version of TOEFL 11 and tokenized ICLE with the Natural Language Toolkit (NLTK)1 tokenizer. ICLE metadata was removed in pre-processing. Each essay was represented through the sets of features described below, using term frequency (tf) and the liblinear scikit-learn (Pedregosa et al., 2011) implementat"
W18-6218,W13-1714,0,0.0640457,"h (SPA). 3.2 We used the (pre-)tokenized version of TOEFL 11 and tokenized ICLE with the Natural Language Toolkit (NLTK)1 tokenizer. ICLE metadata was removed in pre-processing. Each essay was represented through the sets of features described below, using term frequency (tf) and the liblinear scikit-learn (Pedregosa et al., 2011) implementation of Support Vector Machines (SVM) with OvR (one vs. the rest) multi-class strategy. We report classification accuracy on 10-fold cross-validation experiments. Emotion features for NLI The best performing features for NLI are word and character n-grams (Jarvis et al., 2013). They cover – and obscure – a wide range of phenomena, because language usage has multiple dimensions that can reveal information such as age, gender, cultural influences. In this study, we investigate the impact of words that have an emotion signal, since studies have shown that emotion is culture specific (Wierzbicka, 1994, 1999), and thus could be indicative of the native language of a speaker. 3.1 Experiment setup 3.3 Features 3.3.1 Part-of-speech tags and function words POS tag n-grams and function words (FWs) are considered core features in NLI research (Malmasi and Dras, 2015), not sus"
W18-6218,W14-0908,0,0.0296986,"Missing"
W18-6218,C12-1158,0,0.0958251,"zbicka, 1999). We test this by evaluating the impact of emotion-based features on classifying the L1 of the authors of essays written in L2. 3 Turkish (TUR). The essays were written in response to eight different writing prompts, all of which appear in all 11 L1 groups. The dataset contains information regarding the proficiency level (low, medium, high) of the authors. ICLE (Granger et al., 2009): the ICLEv2 dataset consists of essays written by highlyproficient non-native college-level students of English. We used a 7-language subset of the corpus normalized for topic and character encoding (Tetreault et al., 2012; Ionescu et al., 2014) to which we refer as ICLE. This subset contains 110 essays (avg. 747 tokens/essay after tokenization and removal of metadata) for each of the 7 languages: Bulgarian (BUL), Chinese (CHI), Czech (CZE), French (FRE), Japanese (JPN), Russian (RUS), and Spanish (SPA). 3.2 We used the (pre-)tokenized version of TOEFL 11 and tokenized ICLE with the Natural Language Toolkit (NLTK)1 tokenizer. ICLE metadata was removed in pre-processing. Each essay was represented through the sets of features described below, using term frequency (tf) and the liblinear scikit-learn (Pedregosa et"
W19-0801,W16-5311,0,0.0981394,"c relation classification covers a wide range of methods and learning paradigms for representing relation instances (see Nastase et al. 2013 for an overview). Typically, the data is presented to the learner as independent instances, with or without a sentential context. Relation classification models represent the meaning of the arguments (attributional features) and if context is available, also the relation (relational features). Recently Deep Learning has strongly influenced semantic relation learning. Word embeddings can provide attributional features for a variety of learning frameworks (Attia et al., 2016; Vylomova et al., 2016), and the sentential context – in its entirety, or only the structured (through grammatical relations) or unstructured phrase expressing the relation – can be modeled through a variety of neural architectures – CNN (Tan et al., 2018; Ren et al., 2018) or RNN variations (Zhang et al., 2018). 2.2 C ONCEPT N ET Relation Classification Speer et al. (2008) introduce AnalogySpace, a representation of concepts and relations in C ONCEPTN ET built by factorizing a matrix with concepts on one axis and their features or properties (according to C ONCEPT N ET) on the other. This lo"
W19-0801,S18-1172,0,0.0227727,"Missing"
W19-0801,D13-1072,0,0.111195,"amount of diverse but simple facts about the world, people and everyday life, e.g., Cars are used to travel or Birds can fly (Liebermann, 2008). Commonsense knowledge obtained from C ONCEPT N ET is increasingly used in advanced NLU tasks, such as textual entailment (Weissenborn et al., 2018), reading comprehension (Mihaylov and Frank, 2018), machine comprehension (Wang and Li, 2018; Jos´e-Angel Gonz´alez and Hurtado Oliver, Llu´ıs and Segarra, Encarna and Pla, Ferran, 2018), question answering (Ostermann et al., 2018) or dialogue modeling (Young et al., 2018) and also applications in vision (Le et al., 2013). Some of these approaches exploit embeddings learned from C ONCEPT N ET, others select specific relations from it, depending on the application. This paper proposes a multi-label neural approach for classifying C ONCEPT N ET relations, where the task is to predict one (or several) commonsense relations from a given set of relation types that hold between two given concepts from C ONCEPT N ET. In future work, the predicted relations can then be used for enriching C ONCEPT N ET by adding relations between concepts which are not yet linked in the network. We design the task of multi-label neural"
W19-0801,P16-1137,0,0.276295,"Missing"
W19-0801,K16-1006,0,0.0159662,"tings. The overall best performing model across all settings is FF NN +R NN (as opposed to FF NN with centroid argument representations) with relation-specific label prediction thresholds (as opposed to one global threshold value). In the OW setting we achieve overall F1-scores of 0.68 (OW-1) and 0.65 (OW-2). The CW setting leads to best results with 0.71 F1. The models improve by 4pp (OW-1), 7pp (OW-2) and 3 We additionally tested Numberbatch embeddings (Speer et al., 2017), GloVe embeddings trained on Wikipedia and Gigaword (Pennington et al., 2014), context2vec embeddings trained on UkWaC (Melamud et al., 2016). In our experiments we discovered that all of these alternatives perform worse than the word2vec embeddings. 6 Setting Model ISA H AS A AT L OCATION H AS P ROPERTY U SED F OR C APABLE O F R ECEIVES ACTION C AUS .D ES . D ESIRES M OTICATED B Y G OAL H AS P REREQUISITE H AS F IRST S UBEVENT H AS S UBEVENT C AUSES OTHER R ANDOM Weighted F1 FF .58 (.57) .67 (.66) .69 (.68) .66 (.65) .76 (.75) .61 (.61) .82 (.82) .87 (.87) .91 (.85) .61 (.60) .45 (.41) .54 (.53) .24 (.22) .60 (.59) .61 (.58) .64 (.63) OpenWorld OW-1 FF+RNN .62 (.60) .80 (.79) .78 (.78) .81 (.80) .78 (.77) .67 (.65) .91 (.91) .90 ("
W19-0801,P18-1076,1,0.824435,"complexity of argument types and relation ambiguity are the most important challenges to address. We design a customized evaluation method to address the incompleteness of the resource that can be expanded in future work. 1 Introduction Commonsense knowledge can be seen as a large amount of diverse but simple facts about the world, people and everyday life, e.g., Cars are used to travel or Birds can fly (Liebermann, 2008). Commonsense knowledge obtained from C ONCEPT N ET is increasingly used in advanced NLU tasks, such as textual entailment (Weissenborn et al., 2018), reading comprehension (Mihaylov and Frank, 2018), machine comprehension (Wang and Li, 2018; Jos´e-Angel Gonz´alez and Hurtado Oliver, Llu´ıs and Segarra, Encarna and Pla, Ferran, 2018), question answering (Ostermann et al., 2018) or dialogue modeling (Young et al., 2018) and also applications in vision (Le et al., 2013). Some of these approaches exploit embeddings learned from C ONCEPT N ET, others select specific relations from it, depending on the application. This paper proposes a multi-label neural approach for classifying C ONCEPT N ET relations, where the task is to predict one (or several) commonsense relations from a given set of re"
W19-0801,N13-1090,0,0.0168912,"of the arguments, coverage and completeness. A successful relation classification system should take these into account. Given the heterogeneity of sources of C ONCEPT N ET, we focus on its core part, in particular C N -O MCS C LN, a subset selected from C N -O MCS that includes ca. 180K triples from 36 relation types, restricted to known vocabulary from the GoogleNews Corpus (see §4.1 for further details). 3.3.1 Representing the Inputs Word embeddings have been shown to provide useful semantic representations, capturing lexical properties of words and relative positioning in semantic space (Mikolov et al., 2013b), which has been exploited for semantic relation classification (Vylomova et al., 2016; Attia et al., 2016). Following this work, we represent a pair of concepts hci , cj i whose relation we want to classify through their embeddings vci and vcj . These argument representations can be combined by subtraction (v ci − v cj ) (DiffVec; cf. wee , rol ), addition v ci + v cj (AddVec) or concatenation [v ci , v cj ] (ConcatVec, cf. bar ). One of the issues in using such representations for C ONCEPT N ET is the fact that most C ONCEPTN ET concepts are multi-word expressions (1.93 words on average, c"
W19-0801,S18-1119,0,0.0956526,"ce that can be expanded in future work. 1 Introduction Commonsense knowledge can be seen as a large amount of diverse but simple facts about the world, people and everyday life, e.g., Cars are used to travel or Birds can fly (Liebermann, 2008). Commonsense knowledge obtained from C ONCEPT N ET is increasingly used in advanced NLU tasks, such as textual entailment (Weissenborn et al., 2018), reading comprehension (Mihaylov and Frank, 2018), machine comprehension (Wang and Li, 2018; Jos´e-Angel Gonz´alez and Hurtado Oliver, Llu´ıs and Segarra, Encarna and Pla, Ferran, 2018), question answering (Ostermann et al., 2018) or dialogue modeling (Young et al., 2018) and also applications in vision (Le et al., 2013). Some of these approaches exploit embeddings learned from C ONCEPT N ET, others select specific relations from it, depending on the application. This paper proposes a multi-label neural approach for classifying C ONCEPT N ET relations, where the task is to predict one (or several) commonsense relations from a given set of relation types that hold between two given concepts from C ONCEPT N ET. In future work, the predicted relations can then be used for enriching C ONCEPT N ET by adding relations betwee"
W19-0801,D14-1162,0,0.0788332,"ble 4 summarizes the results in open (OW) and closed world (CW) settings. The overall best performing model across all settings is FF NN +R NN (as opposed to FF NN with centroid argument representations) with relation-specific label prediction thresholds (as opposed to one global threshold value). In the OW setting we achieve overall F1-scores of 0.68 (OW-1) and 0.65 (OW-2). The CW setting leads to best results with 0.71 F1. The models improve by 4pp (OW-1), 7pp (OW-2) and 3 We additionally tested Numberbatch embeddings (Speer et al., 2017), GloVe embeddings trained on Wikipedia and Gigaword (Pennington et al., 2014), context2vec embeddings trained on UkWaC (Melamud et al., 2016). In our experiments we discovered that all of these alternatives perform worse than the word2vec embeddings. 6 Setting Model ISA H AS A AT L OCATION H AS P ROPERTY U SED F OR C APABLE O F R ECEIVES ACTION C AUS .D ES . D ESIRES M OTICATED B Y G OAL H AS P REREQUISITE H AS F IRST S UBEVENT H AS S UBEVENT C AUSES OTHER R ANDOM Weighted F1 FF .58 (.57) .67 (.66) .69 (.68) .66 (.65) .76 (.75) .61 (.61) .82 (.82) .87 (.87) .91 (.85) .61 (.60) .45 (.41) .54 (.53) .24 (.22) .60 (.59) .61 (.58) .64 (.63) OpenWorld OW-1 FF+RNN .62 (.60) ."
W19-0801,C18-1100,0,0.0480483,"Missing"
W19-0801,K18-1014,0,0.311075,"Missing"
W19-0801,W16-5413,0,0.0587866,"Missing"
W19-0801,P16-1158,0,0.120832,"ation covers a wide range of methods and learning paradigms for representing relation instances (see Nastase et al. 2013 for an overview). Typically, the data is presented to the learner as independent instances, with or without a sentential context. Relation classification models represent the meaning of the arguments (attributional features) and if context is available, also the relation (relational features). Recently Deep Learning has strongly influenced semantic relation learning. Word embeddings can provide attributional features for a variety of learning frameworks (Attia et al., 2016; Vylomova et al., 2016), and the sentential context – in its entirety, or only the structured (through grammatical relations) or unstructured phrase expressing the relation – can be modeled through a variety of neural architectures – CNN (Tan et al., 2018; Ren et al., 2018) or RNN variations (Zhang et al., 2018). 2.2 C ONCEPT N ET Relation Classification Speer et al. (2008) introduce AnalogySpace, a representation of concepts and relations in C ONCEPTN ET built by factorizing a matrix with concepts on one axis and their features or properties (according to C ONCEPT N ET) on the other. This low-dimensional representa"
W19-0801,S18-1120,0,0.232442,"ty are the most important challenges to address. We design a customized evaluation method to address the incompleteness of the resource that can be expanded in future work. 1 Introduction Commonsense knowledge can be seen as a large amount of diverse but simple facts about the world, people and everyday life, e.g., Cars are used to travel or Birds can fly (Liebermann, 2008). Commonsense knowledge obtained from C ONCEPT N ET is increasingly used in advanced NLU tasks, such as textual entailment (Weissenborn et al., 2018), reading comprehension (Mihaylov and Frank, 2018), machine comprehension (Wang and Li, 2018; Jos´e-Angel Gonz´alez and Hurtado Oliver, Llu´ıs and Segarra, Encarna and Pla, Ferran, 2018), question answering (Ostermann et al., 2018) or dialogue modeling (Young et al., 2018) and also applications in vision (Le et al., 2013). Some of these approaches exploit embeddings learned from C ONCEPT N ET, others select specific relations from it, depending on the application. This paper proposes a multi-label neural approach for classifying C ONCEPT N ET relations, where the task is to predict one (or several) commonsense relations from a given set of relation types that hold between two given c"
W19-0801,S10-1006,0,\N,Missing
W19-4429,W15-0620,0,0.448395,"Missing"
W19-4429,P07-1083,0,0.0438934,"of expressions and the extracted vocabularies is provided in Table 2. We apply the following algorithm: 3.3.2 Misspelled cognates, L2-ed words and other misspellings We build features that gather information from misspelled words in the essays in the data. The information about which L1 a cognate or L2-ed word hints to is used as an attribute of the word. 1. For each misspelled English word wm identify its closest word in some L1: 2. For wf in each L1: Misspelled cognates. Several studies applied discriminative string similarity to the task of cognate identification (Mann and Yarowsky, 2001; Bergsma and Kondrak, 2007; Nicolai et al., 2013). Following the work by Nicolai et al. (2013), we detect cognates by identifying the cases where the closest correctly spelled L2 word we to the misspelled word wm has a translation in an L1 wf to which it is close in form, and wm is closer to wf than to we . Formally: (a) Replace diacritics in wf with the corresponding Latin equivalent (e.g., “´e” → “e”). (b) Compute the Levenshtein distance D(wm , wf ). (c) Identify the L1 with the smallest D(wm , wf ) value, and if D(wm , wf ) < 5 then take wm to be an L2-ed version 4 We use Python’s translation tool: https://pypi.org"
W19-4429,N01-1020,0,0.0668213,"ge in terms of the number of expressions and the extracted vocabularies is provided in Table 2. We apply the following algorithm: 3.3.2 Misspelled cognates, L2-ed words and other misspellings We build features that gather information from misspelled words in the essays in the data. The information about which L1 a cognate or L2-ed word hints to is used as an attribute of the word. 1. For each misspelled English word wm identify its closest word in some L1: 2. For wf in each L1: Misspelled cognates. Several studies applied discriminative string similarity to the task of cognate identification (Mann and Yarowsky, 2001; Bergsma and Kondrak, 2007; Nicolai et al., 2013). Following the work by Nicolai et al. (2013), we detect cognates by identifying the cases where the closest correctly spelled L2 word we to the misspelled word wm has a translation in an L1 wf to which it is close in form, and wm is closer to wf than to we . Formally: (a) Replace diacritics in wf with the corresponding Latin equivalent (e.g., “´e” → “e”). (b) Compute the Levenshtein distance D(wm , wf ). (c) Identify the L1 with the smallest D(wm , wf ) value, and if D(wm , wf ) < 5 then take wm to be an L2-ed version 4 We use Python’s transla"
W19-4429,W17-5042,1,0.646129,"he intended word we in L1.4 (b) Replace diacritics in wf with the corresponding Latin equivalent (e.g., “´e” → “e”). (c) Compute the Levenshtein distance D between we and wf . (d) If D(we , wf ) < 3 then wf is assumed to be a cognate of we .5 (e) If wf is a cognate and D(wm , wf ) < D(we , wf ) then consider the L1 as a clue of the native language of the author.6 3.3.1 Part-of-speech tags and function words POS features capture the morpho-syntactic patterns in a text, and are indicative of the L1, especially when used in combination with other types of features (Cimino and Dell’Orletta, 2017; Markov et al., 2017). POS tags were obtained with TreeTagger (Schmid, 1999), which uses the Penn Treebank tagset (36 tags). FWs clarify the relationships between the content-carrying elements of a sentence, and introduce syntactic structures like verbal complements, relative clauses, and questions (Smith and Witten, 1993). The FW feature set consists of 318 English FWs from the scikit-learn package (Pedregosa et al., 2011). L2-ed words. To identify the L2-ed, in our case anglicized, words we take a misspelled word and look for forms close to it in the L1 vocabularies. The idea is that a misspelled word may be an"
W19-4429,C18-1293,1,0.823977,"osa et al., 2011) implementation of Support Vector Machines (SVM) with OvR (one vs. the rest) multi-class strategy. We report classification accuracy on 10-fold cross-validation experiments. Methodology To investigate the impact of L2-ed words and cognates, we use the native language identification task: we perform multi-class classification of essays written in L2 (English in our case) by people with different native languages (L1s) – with L1 as the class labels – using a representation of these essays through features that capture these 3.3 Features Following previous studies on NLI, e.g., (Markov et al., 2018a,b), we evaluate the impact of L2-ed words and cognates in combination with the part2 277 http://www.nltk.org of-speech (POS) tag and function word (FW) representations. POS tags and function words (FWs) are considered core features in NLI research (Malmasi and Dras, 2015), not susceptible to topic bias, unlike word and character n-grams (Brooke and Hirst, 2011). An essay will be represented through various combinations of the feature sets we consider: POS & FW n-grams; n-grams from POS & FW sequences including word-level L1 information; character n-grams that represent misspelled words. 2. F"
W19-4429,P17-2086,1,0.880127,"Missing"
W19-4429,W18-6218,1,0.684474,"osa et al., 2011) implementation of Support Vector Machines (SVM) with OvR (one vs. the rest) multi-class strategy. We report classification accuracy on 10-fold cross-validation experiments. Methodology To investigate the impact of L2-ed words and cognates, we use the native language identification task: we perform multi-class classification of essays written in L2 (English in our case) by people with different native languages (L1s) – with L1 as the class labels – using a representation of these essays through features that capture these 3.3 Features Following previous studies on NLI, e.g., (Markov et al., 2018a,b), we evaluate the impact of L2-ed words and cognates in combination with the part2 277 http://www.nltk.org of-speech (POS) tag and function word (FW) representations. POS tags and function words (FWs) are considered core features in NLI research (Malmasi and Dras, 2015), not susceptible to topic bias, unlike word and character n-grams (Brooke and Hirst, 2011). An essay will be represented through various combinations of the feature sets we consider: POS & FW n-grams; n-grams from POS & FW sequences including word-level L1 information; character n-grams that represent misspelled words. 2. F"
W19-4429,W17-5049,0,0.0234891,"Missing"
W19-4429,Q18-1024,0,0.0183207,"rm with the vocabulary of the native language L1. Examples of this process are cognates, which are words that have the same ancestors or were derived from the same sources, that we often approximate in computational approaches as words having similar forms and similar meaning in L1 and L2, for example, S PA . religi´on and E NG . religion. Research in psycholinguistics and native language identification have shown that using cognates when producing L2 is common and shared across native speakers of the same L1 to the degree that a quite accurate phylogenetic language tree can be reconstructed (Rabinovich et al., 2018). 275 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 275–284 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics native language has been explored in various ways through the task of native language identification.1 Nicolai et al. (2013) add cognate-based features to frequently used ones (e.g., character and word n-grams, syntax production rules, misspelling features) for the NLI shared task 2013 (Tetreault et al., 2013). Cognates were detected by identifying misspelled words whose form is closer to an"
W19-4429,D17-1286,1,0.643296,"Missing"
W19-4429,W13-1718,0,0.017321,"racted vocabularies is provided in Table 2. We apply the following algorithm: 3.3.2 Misspelled cognates, L2-ed words and other misspellings We build features that gather information from misspelled words in the essays in the data. The information about which L1 a cognate or L2-ed word hints to is used as an attribute of the word. 1. For each misspelled English word wm identify its closest word in some L1: 2. For wf in each L1: Misspelled cognates. Several studies applied discriminative string similarity to the task of cognate identification (Mann and Yarowsky, 2001; Bergsma and Kondrak, 2007; Nicolai et al., 2013). Following the work by Nicolai et al. (2013), we detect cognates by identifying the cases where the closest correctly spelled L2 word we to the misspelled word wm has a translation in an L1 wf to which it is close in form, and wm is closer to wf than to we . Formally: (a) Replace diacritics in wf with the corresponding Latin equivalent (e.g., “´e” → “e”). (b) Compute the Levenshtein distance D(wm , wf ). (c) Identify the L1 with the smallest D(wm , wf ) value, and if D(wm , wf ) < 5 then take wm to be an L2-ed version 4 We use Python’s translation tool: https://pypi.org/project/translate/ 5 F"
W19-4429,W14-3907,0,0.0699155,"Missing"
W19-4429,W13-1706,0,0.232758,"Missing"
W19-4444,P15-2011,0,0.0401206,"Missing"
W19-4444,P18-1004,0,0.0273742,"Missing"
W19-4444,W18-0520,0,0.0749514,"is database contains up to 26 (psycho)linguistic attributes for 150,837 words. Concreteness ratings were extracted from a collection of English Abstractness/Concreteness ratings (K¨oper and Schulte im Walde, 2017). We extracted the values for the word forms if present in the databases and for the respective lemmas otherwise. For a number of words, the values are missing (see Table 6). De Hertog and Tack (2018) use the third and first quartile values for Imageability and Concreteness, respectively, following an assumption that rarer words tend to have lower imageability and concreteness, while Gooding and Kochmar (2018) use the null value. We decided to assign instead a “neutral” value: the median value for each feature based on the ratings in the MRC. Part of speech: The part of speech (POS) tagging was done using the NLTK toolkit14 (Bird et al., 2009). The POS tags were then manually corrected where necessary. The two possible values are noun and verb. Vector space word representations: We obtained vector space representations for each annotated word using Google’s pre-trained word2vec model (Mikolov et al., 2013).15 Word embeddings have been successfully used in metaphor identification (e.g. Dinh and Gure"
W19-4444,P16-1018,0,0.0606501,"Missing"
W19-4444,P14-2075,0,0.0236719,"and Clausen, 2017). Related work Text simplification has numerous facets, and can be approached from different angles. The general need for simplification can be predicted based on the readability of a text, from the point of view ˇ of sentence complexity (Stajner et al., 2017) or a combination of lexical, syntactic and semantic text characteristics (De Clercq and Hoste, 2016). Simplification can be targeted by identifying complex words (e.g. Paetzold and Specia, 2016; Yimam et al., 2018), and then performing lexical simplifiˇ cation (e.g. Glavaˇs and Stajner, 2015; Glavaˇs and Vuli´c, 2018; Horn et al., 2014; Kriz et al., 2018). Lexical simplification systems often build on sentence-aligned simplification corpora and propose substitutes for complex words from a number of synonyms based on the words’ frequency, length and suitability for the original context (De Belder and Moens, 2010; Drndarevi´c and Saggion, 2012; Vu et al., 2014). Approaches influenced by machine translation have also been explored, as lexical simplification can be viewed as monolingual translation (e.g. Nisioi et al., 2017; Xu et al., 2016; Zhu et al., 2010). Other neural based models have also been developed, which exploit wo"
W19-4444,J16-3004,0,0.0452872,"Missing"
W19-4444,S12-1066,0,0.0254113,"SVM implementation in scikit-learn (Pedregosa et al., 2011): https://scikit-learn.org/ stable/modules/generated/sklearn.svm. LinearSVC.html 13 Standardization was performed with the StandardScaler in scikit-learn: http://scikit-learn. org/stable/modules/generated/sklearn. preprocessing.StandardScaler.html 427 general features are part of speech, vector space word representations, Age of Acquisition, word frequency and Familiarity. The feature types used and their coverage in our dataset are described below. metaphor types. These features were successfully used in lexical simplification (e.g. Jauhar and Specia 2012; Vajjala and Meurers 2014). Imageability and Familiarity ratings were obtained from the MRC Psycholinguistic Database (Wilson, 1988). This database contains up to 26 (psycho)linguistic attributes for 150,837 words. Concreteness ratings were extracted from a collection of English Abstractness/Concreteness ratings (K¨oper and Schulte im Walde, 2017). We extracted the values for the word forms if present in the databases and for the respective lemmas otherwise. For a number of words, the values are missing (see Table 6). De Hertog and Tack (2018) use the third and first quartile values for Image"
W19-4444,W18-0539,0,0.0601472,"Missing"
W19-4444,W16-1104,0,0.0145839,"ochmar (2018) use the null value. We decided to assign instead a “neutral” value: the median value for each feature based on the ratings in the MRC. Part of speech: The part of speech (POS) tagging was done using the NLTK toolkit14 (Bird et al., 2009). The POS tags were then manually corrected where necessary. The two possible values are noun and verb. Vector space word representations: We obtained vector space representations for each annotated word using Google’s pre-trained word2vec model (Mikolov et al., 2013).15 Word embeddings have been successfully used in metaphor identification (e.g. Dinh and Gurevych 2016; Guti´errez et al. 2016) as well as in lexical simplification tasks (e.g. Glavaˇs and ˇ Stajner 2015; Glavaˇs and Vuli´c 2018). Age of Acquisition: Age of Acquisition (AoA) ratings were obtained from the AoA norms database of 51,715 English words (Kuperman et al., 2012). AoA denotes the approximate age at which a word is learned. The simplified news articles used in this study are intended for classroom use by 9-10 year old children. Words usually acquired after this age should be more readily changed/removed in the simplified version. We extracted the AoA ratings by matching both word forms"
W19-4444,W12-2202,0,0.0805182,"Missing"
W19-4444,W17-1903,0,0.0299689,"Missing"
W19-4444,N18-1019,0,0.0524186,"Missing"
W19-4444,E12-2021,0,0.0189978,"Missing"
W19-4444,D11-1063,0,0.0275635,"changed, we group the features based on the type of information they capture: • IFC (Imageability + Familiarity + Concreteness) – informative for metaphoric words • WN+IC/CC (WordNet sense + word’s context) – different aspect of metaphor relevance • Freq+AoA (word frequency + Age of Acquisition) – relevant for both metaphoric and literal items Word’s context: This feature reflects the discrepancy between the level of abstractness of a metaphoric word and its context. It was operationalized with ratings of Concreteness (K¨oper and Schulte im Walde, 2017) and Imageability from the MRC database. Turney et al. (2011) have shown that a word’s degree of abstractness, relative to the context it appears in, can be successfully used to distinguish between literal and metaphoric meanings. Broadwell et al. (2013) used Imageability ratings to discover metaphors based on the assumption that they stand out of their context as being highly imageable. We considered a symmetrical seven-word window centered on the target word. A word w’s Concreteness context (CC) value is computed as: bn/2c P Available 2,258 3,503 The F-score results on the full dataset (1,277 changed, 2,232 preserved instances) for different feature c"
W19-4444,P17-2014,0,0.0127019,"8), and then performing lexical simplifiˇ cation (e.g. Glavaˇs and Stajner, 2015; Glavaˇs and Vuli´c, 2018; Horn et al., 2014; Kriz et al., 2018). Lexical simplification systems often build on sentence-aligned simplification corpora and propose substitutes for complex words from a number of synonyms based on the words’ frequency, length and suitability for the original context (De Belder and Moens, 2010; Drndarevi´c and Saggion, 2012; Vu et al., 2014). Approaches influenced by machine translation have also been explored, as lexical simplification can be viewed as monolingual translation (e.g. Nisioi et al., 2017; Xu et al., 2016; Zhu et al., 2010). Other neural based models have also been developed, which exploit word embeddings and their closeness in the vector space as clues for substitution candidates. ˇ Glavaˇs and Stajner (2015) produce word simplifications in a large regular corpus using word embeddings to perform lexical substitution tasks. The simplification candidates are ranked based on features such as semantic and context similarity, and 1 Data 3.1 The dataset We use a parallel corpus of 1,130 Newsela articles by Xu et al. (2015), where each original article has been aligned with its four"
W19-4444,W17-5035,1,0.67817,"ch words. Regarding metaphors, the instructions are brief and seem to draw attention to idioms rather than metaphors: “be literal in lower versions. No straight out metaphors, as in no ‘paint into a corner’ in 5th grade or below.”. Each Newsela article has five versions of different difficulty levels determined based on the Lexile2 readability scores, which are used to measure the complexity of texts and assign them to appropriate grade levels. Using these parallel news texts allows for the quick identification of changed items to produce a dataset to which metaphor information is then added (Wolska and Clausen, 2017). Related work Text simplification has numerous facets, and can be approached from different angles. The general need for simplification can be predicted based on the readability of a text, from the point of view ˇ of sentence complexity (Stajner et al., 2017) or a combination of lexical, syntactic and semantic text characteristics (De Clercq and Hoste, 2016). Simplification can be targeted by identifying complex words (e.g. Paetzold and Specia, 2016; Yimam et al., 2018), and then performing lexical simplifiˇ cation (e.g. Glavaˇs and Stajner, 2015; Glavaˇs and Vuli´c, 2018; Horn et al., 2014;"
W19-4444,Q15-1021,0,0.0599226,"Missing"
W19-4444,S13-1040,0,0.0275289,"arevi´c and Saggion, 2012; Toruno˘glu-Selamet et al., 2016; Vu et al., 2014). The automatic handling of metaphorical language has also been researched extensively. However, the studies have mainly investigated the possibilities of automatic metaphor identification. Simplification of metaphorical language has not been explicitly addressed yet. This could be attributed to the fact that metaphor simplification is a challenging task for automatic implementation (cf. Drndarevi´c and Saggion, 2012). Some approaches have considered the problem of automatic metaphor interpretation (e.g. Bollegala and Shutova, 2013; Shutova, 2013), which aims to find literal paraphrases for metaphorical expressions. It is not clear though whether the literal version is easier to understand than the original metaphor. Sometimes lexical simplifications for complex words can be too basic to convey the original meaning (cf. Vu et al., 2014). We take a step towards filling the gap in metaphor simplification research. We combine information (in the form of features) from text simplification, and characteristics of metaphors to investigate whether there are specific features that can predict whether metaphors should be changed"
W19-4444,Q16-1029,0,0.0543508,"Missing"
W19-4444,C10-1152,0,0.0328801,"fiˇ cation (e.g. Glavaˇs and Stajner, 2015; Glavaˇs and Vuli´c, 2018; Horn et al., 2014; Kriz et al., 2018). Lexical simplification systems often build on sentence-aligned simplification corpora and propose substitutes for complex words from a number of synonyms based on the words’ frequency, length and suitability for the original context (De Belder and Moens, 2010; Drndarevi´c and Saggion, 2012; Vu et al., 2014). Approaches influenced by machine translation have also been explored, as lexical simplification can be viewed as monolingual translation (e.g. Nisioi et al., 2017; Xu et al., 2016; Zhu et al., 2010). Other neural based models have also been developed, which exploit word embeddings and their closeness in the vector space as clues for substitution candidates. ˇ Glavaˇs and Stajner (2015) produce word simplifications in a large regular corpus using word embeddings to perform lexical substitution tasks. The simplification candidates are ranked based on features such as semantic and context similarity, and 1 Data 3.1 The dataset We use a parallel corpus of 1,130 Newsela articles by Xu et al. (2015), where each original article has been aligned with its four simplified versions at the sentence"
