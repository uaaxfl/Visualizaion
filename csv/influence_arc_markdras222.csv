2004.tmi-1.4,J94-4004,0,0.0404578,"icient dynamic programming algorithm based on TAG and drawing on compiler theory for a decomposition into appropriate groupings. We then examine the formal properties of this algorithm, and show that it is linear in the number of nodes in the tree and has the same complexity as existing algorithms requiring only groupings of contiguous nodes. 1 Introduction There are many situations in which it is necessary to relate two sets of structures: machine translation, paraphrase, mapping between syntax and semantics, and so on. Often these are trees, and often structural divergences are significant. Dorr (1994) presents a classification of divergences in MT, including the structural, and uses the extent of the divergences to argue for an explicit semantic representation. Because of structural differences, it is necessary to use some transformation operation in the pairing of the trees. In some cases this is dealt with in an ad hoc manner, although there are several different models for dealing algorithmically with these structural differences that have been proposed. For example, in the structure-pairing formalism based on context-free derivations proposed for MT by Wu (1997), re-ordering of rightha"
2004.tmi-1.4,W00-2035,1,0.839608,"wo strings to be paired. In TAG, for each DERIVED TREE derived from smaller elementary trees, there is a corresponding DERIVATION TREE which describes the history of the derivation. This derivation tree has a number of similarities to dependency trees, but is not exactly the same (Rambow and Joshi, 1997). In general there will not be an isomorphism between two such trees for any of the above applications, hence Shieber’s proposed extension to allow “bounded subderivation” (which correspond to gCNs in the context of derivation trees). However, he also notes the possibility, further explored in Dras and Bleam (2000), that the pairing of gNCNs will be necessary. An example taken from the latter is in (2). (2) El m´edico le quiero poder . . . examinar los dientes. The doctor him–DAT wants to-be-able . . . to-examine the teeth. The doctor wants to be able . . . to examine his teeth. In this Spanish-English example, the clitic can climb over an unlimited number of ‘trigger verbs’ (Aissen and Perlmutter, 1976) (indicated by the ellipses in the example), and for certain TAG grammars this can correspond to a pair of derivation trees as in Figure 2. In this pair of trees, his corresponds to both los and the clit"
2004.tmi-1.4,P99-1011,1,0.904977,"small .. . cak-ayo .. . C’ $A[class:adj] B’ .. .  , BE[] II $A .. . Figure 1: Paired non-isomorphic structures, with transfer rule Current models of tree transformation, however, allow only the grouping of contiguous nodes for the purpose of pairing, and there are situations where groupings of non-contiguous nodes—but not just any arbitrary groups of non-contiguous nodes—are required. ‘Parsing’ a tree with a grammar based on some formalism other than CFGs or TSGs will then permit the mapping of such groupings; this can be viewed as applying to the tree a meta-level grammar along the lines of Dras (1999). For cases of parsing trees with groupings of contiguous nodes, there are standard efficient algorithms in compiler theory; however, these do not exist for pairing of groupings of non-contiguous nodes, and this would at first glance appear to require more powerful and slower mechanisms. In this paper we use Tree Adjoining Grammar as the formalism for capturing non-contiguous groupings of nodes required by pairings; it has properties that, given certain conditions, allow an efficient tree parsing algorithm. In Section 2 we examine some examples of the types of groupings required; in Section 3"
2004.tmi-1.4,P03-2041,0,0.141903,"to argue for an explicit semantic representation. Because of structural differences, it is necessary to use some transformation operation in the pairing of the trees. In some cases this is dealt with in an ad hoc manner, although there are several different models for dealing algorithmically with these structural differences that have been proposed. For example, in the structure-pairing formalism based on context-free derivations proposed for MT by Wu (1997), re-ordering of righthand sides in context-free grammar rules is allowed in order to represent differences in structure; more recently, Eisner (2003) has used a model of Synchronous Tree Substitution Grammars (S-TSGs) as the basis for a stochastic mapping induction system. Broadly, this takes a group of nodes in each tree and treats them as a single unit in order to be able to pair trees of different structure. Abeill´e et al. (1990), in presenting Synchronous Tree Adjoining Grammar (S-TAG) as a formalism for representing MT, note that the extent of the divergences and consequent restructuring will depend on the formalism chosen: with a formalism such as S-TAG, with its extended domain of locality which incorporates predicate–argument stru"
2004.tmi-1.4,han-etal-2000-handling,1,0.934817,"n unlimited number of ‘trigger verbs’ (Aissen and Perlmutter, 1976) (indicated by the ellipses in the example), and for certain TAG grammars this can correspond to a pair of derivation trees as in Figure 2. In this pair of trees, his corresponds to both los and the clitic le. Both his and los are fixed in relation to the root of the tree, but le is an unbounded distance from it, so it is not possible to form a gCN in the Spanish tree for pairing without the unbounded and unrelated recursively-inserted verbs, hence requiring infinitely many transfer rules. Paired dependency trees The system of Han et al. (2000) pairs two dependency trees based on a Deep Syntactic Structure (DSyntS) of Meaning Text Theory (MTT) (Mel’ˇcuk, 1988), a dependency representation composed of nodes labeled by lexemes that correspond to meaning-bearing words (nouns, verbs, adjectives, adverbs) and directed arcs with dependency relation labels. Transfer rules are also represented by DSyntS trees, with variables.3 The goal of this particular dependency representation is to minimise ‘spurious’ structural divergences, such as when a preposition in one language is 2 We use the romanization of Han et al. (2000), for consistency wit"
2004.tmi-1.4,P94-1022,0,0.041721,"cost 10 (4 for the pattern tree, 6 for the intervening auxiliary trees). The algorithm in Figure 8 is modified so that any annotation in an annotation set with the same type but non-minimal cost is discarded. Thus the derivation of the optimal tree parse, top-down, would be 1 with an adjunction of 1 which in turn has an adjunction of 1 . By observation, and just as the standard algorithm, this extension is also O (n) time and space complexity in the number of nodes. This is not surprising, as the restriction on non-embedding of gNCNs occurs if a TAG grammar is restricted to the normal form of Rogers (1994), so that the tree set is recS A A A A A A A b b b : S : A A e 0: ANA A;NA ANA B b A a a b C a A;NA a a e Figure 9: Embedding pattern trees ognizable: in brief, in this normal form auxiliary trees cannot embed, and so the grammar is in effect an equivalent but variant form of CFG whose syntax allows a limited degree of non-local behavior. (In the given example, it can be seen that it is not possible to embed recursive material, as all the auxiliary trees are only of height 1.) However, despite linear complexity in the number of nodes, the work done and space used are proportional to the numb"
2004.tmi-1.4,J97-3002,0,0.0670574,"nces are significant. Dorr (1994) presents a classification of divergences in MT, including the structural, and uses the extent of the divergences to argue for an explicit semantic representation. Because of structural differences, it is necessary to use some transformation operation in the pairing of the trees. In some cases this is dealt with in an ad hoc manner, although there are several different models for dealing algorithmically with these structural differences that have been proposed. For example, in the structure-pairing formalism based on context-free derivations proposed for MT by Wu (1997), re-ordering of righthand sides in context-free grammar rules is allowed in order to represent differences in structure; more recently, Eisner (2003) has used a model of Synchronous Tree Substitution Grammars (S-TSGs) as the basis for a stochastic mapping induction system. Broadly, this takes a group of nodes in each tree and treats them as a single unit in order to be able to pair trees of different structure. Abeill´e et al. (1990), in presenting Synchronous Tree Adjoining Grammar (S-TAG) as a formalism for representing MT, note that the extent of the divergences and consequent restructurin"
2007.mtsummit-papers.74,J93-2003,0,0.0325789,"Missing"
2007.mtsummit-papers.74,E06-1032,0,0.0848125,"Missing"
2007.mtsummit-papers.74,P05-1066,0,0.161664,"knowledge; they use pattern learning in their reordering system. In their work they parse and align sentences in the training phase and derive reordering patterns. From the English-French Canadian Hansard they extract 56,000 different transformations for translation. In the decoding phase they use these transformations on the source language. The main focus then is monotonic decoding. Both of these two cited works assume that explicitly matching the word order of the target language is the key. Syntactically motivated rules based on clause restructuring are also used in reordering models. In Collins et al. (2005) six hand-written rules for reordering source sentences are defined. These rules operate on the output of an automatic parser. The reordering rules however are language-pair (German-English) specific and hand-written. Syntactical reordering has also shown to improve machine translation quality for translation outside European languages, that is from an Indo-European language (English) to an Asian language (Chinese) (Wang et al., 2007). This shows the importance of reordering models as preprocessing in SMT, and reinforces the question as to why reordering works. We note that a common characteri"
2007.mtsummit-papers.74,W06-1609,0,0.118215,"Missing"
2007.mtsummit-papers.74,N03-1017,0,0.0345649,"Missing"
2007.mtsummit-papers.74,koen-2004-pharaoh,0,0.396047,"and are based on parser results. (The effects of the parsers used are discussed later in this paper. For Dutch we have also done the counting for human annotated text, which showed little difference from the parser results.) In the 5 and 10 word ranges, we find in Dutch roughly double the proportion of words which exceed this limit compared to English, increasing for even longer dependencies. Match target language word order Although most decoders are capable of generating words in a different order than the source language, usually only simple models are used for this reordering. In Pharaoh (Koehn, 2004), for example, every word reordering between languages is penalised and only the language model can encourage a different order. If we can match the word order of the target language to a certain degree, we might expect an increase in translation quality, because we already have more explicitly used information of what the new word ordering should be. Fitting phrase window The achievement of PhraseBased SMT (PSMT) (Koehn et al., 2003) was to combine different words into one phrase and treat them as one unit. Yet PSMT only manages to do this if the words all fit together in the same phrase-wind"
2007.mtsummit-papers.74,W06-1606,0,0.0154001,"Reordering of words on a sentence level as a more extensive step for preprocessing has succeeded in improving results in Statistical Machine Translation (SMT). Here, in both the training and decoding phases, sentences in the source language are reordered before being processed. Although in the Machine Translation (MT) community it is still an open question, syntactic information seems to be able to help in SMT when applied correctly. An example of this is the work of Quirk et al. (2005) where a dependency parser was used to learn certain translation phrases, in their work on ‘treelets’. Also Marcu et al. (2006) present a syntax-based approach with phrases that achieves a convincing quality improvement over phrases without these syntax rules. In this paper, however, we take as a starting point approaches which use syntax in a preprocessing step. Word reordering can be done based on rules over word alignment learnt statistically, for example Costa-Juss`a and Fonollosa (2006). In this work an improvement in overall translation quality in a Spanish-English MT system was achieved by using statistical word classes and a word-based distortion model to reorder words in the source language. Reordering here i"
2007.mtsummit-papers.74,J03-1002,0,0.00308836,"tuents marked as cp by Alpino which have a cmp on its first position. 3. Move Subject In every clause we move the subject to immediately precede the head. Clauses are recognised by Alpino as ssub and smain. 4. Particles Verb particles are moved to immediately precede the verb. Verb particles are explicitly marked by Alpino. B LEU Dutch to English Not Reordered Alpino Collins Full Limited 0.207 0.198 0.208 0.196 0.203 Table 2: Automatic evaluation metrics for the dependency minimising models interpolated Kneser-Ney discounting. For a baseline we used the Pharaoh translation made with a GIZA++ (Och and Ney, 2003) training on unchanged text, and the same phrase extractor was used for all of our models. As an automated scoring metric we used the B LEU (Papineni et al., 2002) and the F-Measure (Turian et al., 2003) evaluation metrics.3 For our training data we used the Dutch and English portions of most of the Europarl Corpus. Because one section of the Europarl corpus was not available in a parsed form, this was left out. After sentence aligning the Dutch and the English part we divided the corpus into a training and a testing part. From the original available Dutch parses we selected every 200th senten"
2007.mtsummit-papers.74,P02-1040,0,0.0744499,"ses are recognised by Alpino as ssub and smain. 4. Particles Verb particles are moved to immediately precede the verb. Verb particles are explicitly marked by Alpino. B LEU Dutch to English Not Reordered Alpino Collins Full Limited 0.207 0.198 0.208 0.196 0.203 Table 2: Automatic evaluation metrics for the dependency minimising models interpolated Kneser-Ney discounting. For a baseline we used the Pharaoh translation made with a GIZA++ (Och and Ney, 2003) training on unchanged text, and the same phrase extractor was used for all of our models. As an automated scoring metric we used the B LEU (Papineni et al., 2002) and the F-Measure (Turian et al., 2003) evaluation metrics.3 For our training data we used the Dutch and English portions of most of the Europarl Corpus. Because one section of the Europarl corpus was not available in a parsed form, this was left out. After sentence aligning the Dutch and the English part we divided the corpus into a training and a testing part. From the original available Dutch parses we selected every 200th sentence for testing, until we had 1500 sentences. We have a little over half a million sentences in our training section. 3.5 Results 5. Infinitives In the work of Coll"
2007.mtsummit-papers.74,P05-1034,0,0.0146367,"aster Yoda 1 Introduction Preprocessing—tokenisation, stemming and so on—is an essential step in natural language applications. Reordering of words on a sentence level as a more extensive step for preprocessing has succeeded in improving results in Statistical Machine Translation (SMT). Here, in both the training and decoding phases, sentences in the source language are reordered before being processed. Although in the Machine Translation (MT) community it is still an open question, syntactic information seems to be able to help in SMT when applied correctly. An example of this is the work of Quirk et al. (2005) where a dependency parser was used to learn certain translation phrases, in their work on ‘treelets’. Also Marcu et al. (2006) present a syntax-based approach with phrases that achieves a convincing quality improvement over phrases without these syntax rules. In this paper, however, we take as a starting point approaches which use syntax in a preprocessing step. Word reordering can be done based on rules over word alignment learnt statistically, for example Costa-Juss`a and Fonollosa (2006). In this work an improvement in overall translation quality in a Spanish-English MT system was achieved"
2007.mtsummit-papers.74,A97-1011,0,0.246028,"Missing"
2007.mtsummit-papers.74,2003.mtsummit-papers.51,0,0.0227668,"main. 4. Particles Verb particles are moved to immediately precede the verb. Verb particles are explicitly marked by Alpino. B LEU Dutch to English Not Reordered Alpino Collins Full Limited 0.207 0.198 0.208 0.196 0.203 Table 2: Automatic evaluation metrics for the dependency minimising models interpolated Kneser-Ney discounting. For a baseline we used the Pharaoh translation made with a GIZA++ (Och and Ney, 2003) training on unchanged text, and the same phrase extractor was used for all of our models. As an automated scoring metric we used the B LEU (Papineni et al., 2002) and the F-Measure (Turian et al., 2003) evaluation metrics.3 For our training data we used the Dutch and English portions of most of the Europarl Corpus. Because one section of the Europarl corpus was not available in a parsed form, this was left out. After sentence aligning the Dutch and the English part we divided the corpus into a training and a testing part. From the original available Dutch parses we selected every 200th sentence for testing, until we had 1500 sentences. We have a little over half a million sentences in our training section. 3.5 Results 5. Infinitives In the work of Collin et al., some verbs were not yet in th"
2007.mtsummit-papers.74,D07-1077,0,0.0961504,"matching the word order of the target language is the key. Syntactically motivated rules based on clause restructuring are also used in reordering models. In Collins et al. (2005) six hand-written rules for reordering source sentences are defined. These rules operate on the output of an automatic parser. The reordering rules however are language-pair (German-English) specific and hand-written. Syntactical reordering has also shown to improve machine translation quality for translation outside European languages, that is from an Indo-European language (English) to an Asian language (Chinese) (Wang et al., 2007). This shows the importance of reordering models as preprocessing in SMT, and reinforces the question as to why reordering works. We note that a common characteristic of the Collins et al. (2005) rules is that they reduce the distances of a certain class of long-distance dependencies in German with respect to English, which is known to have quite short distances between heads and dependants. Thus this approach has the effect of reducing semantically linked elements into the same phrasal window, in addition to matching the word order of English. To tease out the effects of these two possible ca"
2007.mtsummit-papers.74,C04-1073,0,0.213967,"provement over phrases without these syntax rules. In this paper, however, we take as a starting point approaches which use syntax in a preprocessing step. Word reordering can be done based on rules over word alignment learnt statistically, for example Costa-Juss`a and Fonollosa (2006). In this work an improvement in overall translation quality in a Spanish-English MT system was achieved by using statistical word classes and a word-based distortion model to reorder words in the source language. Reordering here is purely a statistical process and no syntactic knowledge of the language is used. Xia and McCord (2004) do use syntactic knowledge; they use pattern learning in their reordering system. In their work they parse and align sentences in the training phase and derive reordering patterns. From the English-French Canadian Hansard they extract 56,000 different transformations for translation. In the decoding phase they use these transformations on the source language. The main focus then is monotonic decoding. Both of these two cited works assume that explicitly matching the word order of the target language is the key. Syntactically motivated rules based on clause restructuring are also used in reord"
2007.mtsummit-papers.74,U06-1021,1,0.789257,"that Dutch has longer dependency distances on average, by going from English to Dutch we reverse the reordering rule with regard to the length of dependency lengths. Preferably we would have done it without moving to another language pair (direction), but since we want to conduct our experiments in natural languages, without an artificial long-distancedependency-English as end-product, we need to do this. To test the effect of reducing head dependency distance without explicitly matching word order, we describe a reordering based on minimising head dependant distances, an idea we proposed in Zwarts and Dras (2006), and apply this to Dutch-English translation. We note that minimising of dependency distances is a general principle appearing in a number of guises in psycholinguistics, for example the work of Hawkins (1990). In this paper we exploit this idea to develop one general syntactically motivated reordering rule subsuming those of Collins et al. (2005). German to English, as intuition suggests that German has longer head-dependant distances than English. If this were the case, reordering of German, or of a similar language like Dutch with longer dependency distances, would thus move related words"
2009.eamt-1.27,W08-0406,0,0.0142705,"lternative to the phrase-based state-of-the-art Moses1 system. 2 Related work In practice, a reordering model operates on a sentence level and is carried out based on word reordering rules derived from the training corpus. Reordering patterns can be purely statistical (see Costa-jussà and Fonollosa (2006), for example), use language-based syntactic information (Collins et al., 2005); the reordering can be driven by a lat1 www.statmt.org/moses/ Proceedings of the 13th Annual Conference of the EAMT, pages 197–204, Barcelona, May 2009 197 tice of syntactically motivated alternative translations (Elming, 2008) or be based on automatically extracted patterns driven by syntactical structure of the languages (see Crego and Mariño (2007b) as an example). Another recent implementation of the preprocessing approach to syntax-based reordering though an n-best list generation can be found in Li et al. (2007). Word class-based reordering patterns were part of Och’s Alignment Template system (Och et al., 2004). The modern state-of-the-art phrase-based translation system Moses, along with a distance based distortion model (Koehn et al., 2003), implements the phrase-based reordering (Tillmann and Zhang, 2005)."
2009.eamt-1.27,P05-1066,0,0.140942,"Missing"
2009.eamt-1.27,W06-1609,1,0.941672,"t from the reordering rules representing the order of child nodes, a set of additional rewrite rules based on a deep topdown subtree analysis is considered, which is another novel aspect of the paper. We used the N -gram-based SMT system of Mariño et al. (2006) to test the proposed syntaxbased reordering model, which is an alternative to the phrase-based state-of-the-art Moses1 system. 2 Related work In practice, a reordering model operates on a sentence level and is carried out based on word reordering rules derived from the training corpus. Reordering patterns can be purely statistical (see Costa-jussà and Fonollosa (2006), for example), use language-based syntactic information (Collins et al., 2005); the reordering can be driven by a lat1 www.statmt.org/moses/ Proceedings of the 13th Annual Conference of the EAMT, pages 197–204, Barcelona, May 2009 197 tice of syntactically motivated alternative translations (Elming, 2008) or be based on automatically extracted patterns driven by syntactical structure of the languages (see Crego and Mariño (2007b) as an example). Another recent implementation of the preprocessing approach to syntax-based reordering though an n-best list generation can be found in Li et al. (20"
2009.eamt-1.27,2006.iwslt-evaluation.18,1,0.868743,"Missing"
2009.eamt-1.27,2007.mtsummit-papers.16,0,0.0319675,"es on a sentence level and is carried out based on word reordering rules derived from the training corpus. Reordering patterns can be purely statistical (see Costa-jussà and Fonollosa (2006), for example), use language-based syntactic information (Collins et al., 2005); the reordering can be driven by a lat1 www.statmt.org/moses/ Proceedings of the 13th Annual Conference of the EAMT, pages 197–204, Barcelona, May 2009 197 tice of syntactically motivated alternative translations (Elming, 2008) or be based on automatically extracted patterns driven by syntactical structure of the languages (see Crego and Mariño (2007b) as an example). Another recent implementation of the preprocessing approach to syntax-based reordering though an n-best list generation can be found in Li et al. (2007). Word class-based reordering patterns were part of Och’s Alignment Template system (Och et al., 2004). The modern state-of-the-art phrase-based translation system Moses, along with a distance based distortion model (Koehn et al., 2003), implements the phrase-based reordering (Tillmann and Zhang, 2005). Reordering algorithms specifically developed for an N -gram system include a constrained distance-based distortion model (Co"
2009.eamt-1.27,2005.iwslt-1.23,1,0.909742,"Missing"
2009.eamt-1.27,2005.mtsummit-papers.35,0,0.195464,"system performance. In experiments, we show significant improvement for the Chinese-to-English translation task. 1 Introduction One of the most challenging problems facing machine translation (MT) is how to place the translated words in the natural order of the target language. A monotone SMT system suffers from weakness in the distortion model, even if it is able to generate correct word-by-word translation. In this study we propose a reordering model that involves both source- and target-side syntax information in the word reordering process. Our work is inspired by the approach proposed in Imamura et al. (2005), where a complete syntaxdriven SMT system based on a two-side subtree transfer is described. In their approach they construct a probabilistic non-isomorphic tree mapping model based on a context-free breakdown of the source and target parse trees; extract alignment templates that incorporate the constraints of the parse trees; and apply syntax-based decoding. We c 2009 European Association for Machine Translation. ° Mark Dras Macquarie University North Ryde NSW 2109, Sydney, Australia madras@ics.mq.edu.au propose to use a similar non-isomorphic subtree mapping to extract reordering rules, but"
2009.eamt-1.27,W08-0315,1,0.851424,"rds is translated into the reordered sequence using SMT techniques. In Xia and Mccord (2004) the authors present a hybrid system for French-English translation, based on the principle of automatic rewrite patterns extraction using a parse tree and phrase alignments. This method differs from the one presented in this paper, among other distinctions, by a lexical model underlying the subtree syntax transfer and a different statistical model used for translation. 3 Baseline SMT system N -gram-based SMT has proved to be competitive with the state-of-the-art systems in recent evaluation campaigns (Khalilov et al., 2008; Lambert et al., 2007). According to the N -gram-based approach, the translation process is considered as an arg max searching for the translation hypothesis eˆI1 maximizing a log-linear combination of a translation model (TM) and a set of feature models: ( M ) X λm hm (eI1 , f1J ) (1) eˆI1 = arg max eI1 m=1 where the feature functions hm refer to the system models and the set of λm refers to the weights corresponding to these models. A detailed description of the N -gram-based approach can be found in Mariño et al. (2006). As decoder, we used MARIE2 (Crego et al., 2005), a beam-search decode"
2009.eamt-1.27,P03-1054,0,0.0379698,"t al. (2006). As decoder, we used MARIE2 (Crego et al., 2005), a beam-search decoder implementing a distance-based constrained distortion model, limited by two parameters: m - a maximum distance measured number in words that a phrase can be reordered and j - a maximum number of ""jumps"" within a sentence (Costa-jussà et al., 2006). 4 Syntax-based reordering Our syntax-based reordering (SBR) system requires access to source and target language parse trees, along with the source-to-target and targetto-source word alignments intersection. In the framework of the study we used the Stanford Parser (Klein and Manning, 2003) for both languages, however the system permits using any other natural language parser allowing for different formal grammars for the source and the target languages. 4.1 Notation SBR operates with source and target parse trees that represent the syntactic structure of a string in source and target languages in a Context-Free Grammar (CFG) fashion. This representation is called ""CFG form"", and is formally defined in the usual way as G = hN, T, R, Si, where N is a set of nonterminal symbols (corresponding to source-side phrase and partof-speech tags); T is a set of source-side terminals (the l"
2009.eamt-1.27,N03-1017,0,0.00437198,"a, May 2009 197 tice of syntactically motivated alternative translations (Elming, 2008) or be based on automatically extracted patterns driven by syntactical structure of the languages (see Crego and Mariño (2007b) as an example). Another recent implementation of the preprocessing approach to syntax-based reordering though an n-best list generation can be found in Li et al. (2007). Word class-based reordering patterns were part of Och’s Alignment Template system (Och et al., 2004). The modern state-of-the-art phrase-based translation system Moses, along with a distance based distortion model (Koehn et al., 2003), implements the phrase-based reordering (Tillmann and Zhang, 2005). Reordering algorithms specifically developed for an N -gram system include a constrained distance-based distortion model (Costa-jussà et al., 2006) and a linguistically motivated reordering model employing monotonic search graph extension (Crego and Mariño, 2007a). An example of a word order monotonization strategy can be found in Costa-jussà and Fonollosa (2006), where a monotone sequence of source words is translated into the reordered sequence using SMT techniques. In Xia and Mccord (2004) the authors present a hybrid syst"
2009.eamt-1.27,W04-3250,0,0.10282,"Missing"
2009.eamt-1.27,J06-4004,1,0.897523,"Missing"
2009.eamt-1.27,J03-1002,0,0.00299387,"on|p1 (2) where ηi ∈ N for all 0 ≤ i ≤ k; (do . . . dk ) is a permutation of (0 . . . k); Lexicon includes the source-side set of words for each ηi ; and p1 is a probability associated with the rule. Figure 1 gives two examples of the rule format. 4.2 Rule extraction Concept. Inspired by the ideas presented in Imamura et al. (2005), where monolingual correspon2 198 http://gps-tsc.upc.es/veu/soft/soft/marie/ dences of syntactic nodes are used during decoding, we extract a set of bilingual patterns allowing for reordering as described below: (1) align the monotone bilingual corpus with GIZA++3 (Och and Ney, 2003) and find the intersection of direct and inverse word alignments, resulting in the construction of the projection matrix P (see below); (2) parse the source and the target parts of the parallel corpus; (3) extract reordering patterns from the parallel non-isomorphic CFG-trees based on the word alignment intersection. Step 2 is straightforward; we explain aspects of Steps 1 and 3 in more detail below. Figure 1 shows an example of the generation of two lexicalized rules; we use this below in our explanations. Given two parse trees and a word alignment intersection, a projection matrix P is defin"
2009.eamt-1.27,N04-1021,0,0.0321988,"rdering can be driven by a lat1 www.statmt.org/moses/ Proceedings of the 13th Annual Conference of the EAMT, pages 197–204, Barcelona, May 2009 197 tice of syntactically motivated alternative translations (Elming, 2008) or be based on automatically extracted patterns driven by syntactical structure of the languages (see Crego and Mariño (2007b) as an example). Another recent implementation of the preprocessing approach to syntax-based reordering though an n-best list generation can be found in Li et al. (2007). Word class-based reordering patterns were part of Och’s Alignment Template system (Och et al., 2004). The modern state-of-the-art phrase-based translation system Moses, along with a distance based distortion model (Koehn et al., 2003), implements the phrase-based reordering (Tillmann and Zhang, 2005). Reordering algorithms specifically developed for an N -gram system include a constrained distance-based distortion model (Costa-jussà et al., 2006) and a linguistically motivated reordering model employing monotonic search graph extension (Crego and Mariño, 2007a). An example of a word order monotonization strategy can be found in Costa-jussà and Fonollosa (2006), where a monotone sequence of s"
2009.eamt-1.27,P05-1069,0,0.017168,"ranslations (Elming, 2008) or be based on automatically extracted patterns driven by syntactical structure of the languages (see Crego and Mariño (2007b) as an example). Another recent implementation of the preprocessing approach to syntax-based reordering though an n-best list generation can be found in Li et al. (2007). Word class-based reordering patterns were part of Och’s Alignment Template system (Och et al., 2004). The modern state-of-the-art phrase-based translation system Moses, along with a distance based distortion model (Koehn et al., 2003), implements the phrase-based reordering (Tillmann and Zhang, 2005). Reordering algorithms specifically developed for an N -gram system include a constrained distance-based distortion model (Costa-jussà et al., 2006) and a linguistically motivated reordering model employing monotonic search graph extension (Crego and Mariño, 2007a). An example of a word order monotonization strategy can be found in Costa-jussà and Fonollosa (2006), where a monotone sequence of source words is translated into the reordered sequence using SMT techniques. In Xia and Mccord (2004) the authors present a hybrid system for French-English translation, based on the principle of automa"
2009.eamt-1.27,C04-1073,0,0.198444,"Missing"
2009.eamt-1.27,P01-1067,0,0.117426,", so the rule output functioning as an input to the next rule can lead to situations reverting the change of word order that the previously applied rule made. Therefore, the rules that can be ambiguous when applied sequentially are pruned according to the higher probability principle. For example, for the pair of patterns with the same lexicon (which is empty for a general rule leading to a recurring contradiction NP@0 VP@1 → VP@1 NP@0 p1, VP@0 NP@1 → NP@1 VP@0 p2 ), the less probable rule is removed. Finally, there are three resulting parameter tables analogous to the ""r-table"" as stated in (Yamada and Knight, 2001), consisting of POS- and constituent-based patterns allowing for reordering and monotone distortion. 4.5 Source-side monotonization Rule application is performed as a bottom-up parse tree traversal following two principles: (1) the longest possible rule is applied, i.e. among a set of nested rules, the rule with a longest left-side covering is selected. For example, in the case of the appearance of an NN JJ RB sequence and presence of the two reordering rules NN@0 JJ@1 → ... and NN@0 JJ@1 RB@2 → ... the latter pattern will be applied. (2) the rule containing the maximum lexical information is"
2009.eamt-1.27,N04-1035,0,\N,Missing
2009.eamt-1.27,2006.iwslt-evaluation.17,1,\N,Missing
2009.eamt-1.27,C08-1027,0,\N,Missing
2009.eamt-1.27,P02-1040,0,\N,Missing
2009.eamt-1.27,P04-1083,0,\N,Missing
2009.eamt-1.27,W05-0909,0,\N,Missing
2009.eamt-1.27,P07-1091,0,\N,Missing
2009.eamt-1.27,P05-1033,0,\N,Missing
2009.eamt-1.27,D07-1079,0,\N,Missing
2009.eamt-1.27,W06-3119,0,\N,Missing
2020.starsem-1.19,D18-1316,0,0.065551,"a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 1 https://bit.ly/2ycdnVV https://pan.webis.de/: shared tasks that are run annually on various aspects of authorship related tasks. 2 179 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 179–189 Barcelona, Spain (Online), December 12–13, 2020 Author obfuscation can be seen as the generation of adversarial examples to attack an author identification system. Work in other areas of adversarial example generation (Iyyer et al., 2018; Alzantot et al., 2018; Xiao et al., 2020; Bai et al., 2020) has seen rapid progress with the application of deep learning, and could potentially be adapted here. For example, Zhao et al. (2018b) define a GAN-style architecture to generate ‘natural’ adversarial examples that — unlike approaches searching the input space — works on the dense representation of each data point. Dense representations lie on the manifold that defines the data distribution and finding close points to them leads to natural adversarial examples. They apply this both to image classification tasks and a standard three-class natural language"
2020.starsem-1.19,P19-1104,0,0.125171,"his task is fairly challenging even for humans (McDonald et al., 2012), as authors are often not aware of hidden patterns in their writing; and the computational task is relatively underexplored. Some work has been carried out as part of a PAN authorship obfuscation task, since 2016, while other research has been independent of this. These approaches have included using backtranslation or heuristic application of paraphrase rules (Rosso et al., 2016; Hagen et al., 2017; Potthast et al., 2018), and more recently applying heuristic solution methods to the task framed as an optimization problem (Bevendorff et al., 2019; Li et al., 2019). This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 1 https://bit.ly/2ycdnVV https://pan.webis.de/: shared tasks that are run annually on various aspects of authorship related tasks. 2 179 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 179–189 Barcelona, Spain (Online), December 12–13, 2020 Author obfuscation can be seen as the generation of adversarial examples to attack an author identification system. Work in other areas of adve"
2020.starsem-1.19,E14-1022,0,0.0113232,"iamese decision by replacing Vi . It shows whether the Siamese author verification’s original decision on (V1 , V2 ) is different from the decision on (V1 , V20 ) and (V10 , V2 ). This gives a preliminary result: if the system cannot produce vectors that can fool the decider, it will not produce successfully perturbed texts. considering the texts as strings and counting the minimum number of operations required to transform the original texts into their perturbed counterparts (Przybocki et al., 2006; Li et al., 2019). This metric is believed to be used in commercial translation memory models (Bloodgood and Strauss, 2014). (3) Euclidean Distance (EC) between the vector representations: closeness in vector space typically corresponds to greater semantic similarity (Li et al., 2019; Alzantot et al., 2018). Perturbation Win in Text Space The author identification work of Saedi and Dras (2019) had as its primary evaluation, following the first work on deep Siamese networks (Koch et al., 2015), N -way one-shot classification: a ‘query’ text is compared against texts by N authors, one of whom is also the author of the query text. The N -way task is tackled by assigning pairwise similarities to the query text and eac"
2020.starsem-1.19,K16-1002,0,0.0289111,"Such architectures have so far only been defined for producing adversarial examples against classification-based learners (limited number of classes). In author identification, this would exclude the high-performing similarity-based approaches. In this paper we introduce S IAM AO, an architecture that can generate adversarial examples against a similarity-based learner (specifically a deep Siamese network (Saedi and Dras, 2019)) and evaluate whether it can obfuscate against authorship identification. S IAM AO draws on ideas from Variational Autoencoders (VAEs), and the specific use of them by Bowman et al. (2016) for generating novel sentences close to some input, and from the Adversarially Regularized Autoencoders (ARAEs) of Zhao et al. (2018b): the intuition here is for the autoencoder to regenerate close to the original text but with some perturbation to fool an authorship identification system. Our main contributions are: (i) A method for integrating Siamese networks into VAEs in order to generate adversaries against similarity based models, and testing it under author obfuscation. (ii) A performance comparison on properties of the obfuscated text between our model and baselines: our focus is on h"
2020.starsem-1.19,N19-1423,0,0.0177095,"d be natural-looking, in terms of grammaticality / acceptability. Prediction of language acceptability is now a standard NLP task, e.g. the CoLA task that is part of the GLUE benchmark (Wang et al., 2019). However, that is a binary task: sentences are judged acceptable or not. There is, instead, a notion of gradient grammaticality, where sentence grammaticality is measured on a scale of 0 to 1 (Lau et al., 2014); this could be more suited to capturing the changes we might see in our adversarial examples. BERT has previously been fine-tuned to produce a high-performing model for the CoLA task (Devlin et al., 2019). For gradient grammaticality, a variety of models predating BERT have been trained on the Statistical Models of Grammaticality (SMOG) dataset,7 and have been shown to correlate fairly well with human judgements (Lau et al., 2014, 2017). Given the improvements over earlier models shown by BERT on the CoLA task, we built our model of language naturalness by fine-tuning BERT-large on the SMOG dataset. We refer to this model as BERT-SMOG. To validate our BERTSMOG, we compare with models proposed in Lau et al. (2017) on the original dataset: its Pearson’s r correlation with human judgements is aro"
2020.starsem-1.19,N18-1170,0,0.137997,"k is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 1 https://bit.ly/2ycdnVV https://pan.webis.de/: shared tasks that are run annually on various aspects of authorship related tasks. 2 179 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 179–189 Barcelona, Spain (Online), December 12–13, 2020 Author obfuscation can be seen as the generation of adversarial examples to attack an author identification system. Work in other areas of adversarial example generation (Iyyer et al., 2018; Alzantot et al., 2018; Xiao et al., 2020; Bai et al., 2020) has seen rapid progress with the application of deep learning, and could potentially be adapted here. For example, Zhao et al. (2018b) define a GAN-style architecture to generate ‘natural’ adversarial examples that — unlike approaches searching the input space — works on the dense representation of each data point. Dense representations lie on the manifold that defines the data distribution and finding close points to them leads to natural adversarial examples. They apply this both to image classification tasks and a standard three-"
2020.starsem-1.19,P06-2058,0,0.635967,"based approach to author identification has been proposed by Saedi and Dras (2019), using a Siamese network. This approach outperforms alternatives on up to 5000 authors, and is suitable for our work. 2.2 Author Obfuscation Author obfuscation is a less explored area which shares interest with fields including style transfer (Prabhumoye et al., 2018) or attribute masking (Reddy and Knight, 2016). The goal is to change or perturb a text, so that the accuracy of a specific authorship inference mechanism is worsened while the modified text conveys the original message. Early research like that of Kacmarcik and Gamon (2006) worked at the level of machine learning features, proposing to eliminate those that are more effective in classification; this, however, resulted in mostly unreadable texts. At the level of working directly with text, one approach uses backtranslation: input text is translated to a pivot language and translated back to the original one, producing a more or less similar text. The result is greatly affected by the availability of a successful bidirectional machine translator (Rao et al., 2000; Prabhumoye et al., 2018). Other approaches have been largely rule-based or heuristic in nature. Most r"
2020.starsem-1.19,P18-2005,0,0.0895474,"significantly revealing when authors follow textual authorial patterns, which can lead to disclosure of sensitive information. This has led to real-world problems, such as with Amazon’s machine learning-based recruitment system, Mark Dras Department of Computing, Macquarie University, Sydney, Australia mark.dras@mq.edu.au which was discontinued when it turned out to disadvantage female candidates.1 Cases like this have generated interest in NLP in concealing authorial characteristics such as gender or age, for example by producing representations that make this information difficult to infer (Li et al., 2018). Author identification is the task of inferring the actual identity of the author. The potential number of author candidates can be very large, making author identification different from author profiling where the possible values of an attribute (e.g. gender) are typically limited to a small closed set, as in standard classification tasks. Depending on the number of included authorial classes, approaches in author identification are either classification-based or similarity-based, in the framing of Stamatatos (2009). Similarity-based approaches are proven to be better suited when facing larg"
2020.starsem-1.19,P02-1040,0,0.112276,"LMs). In this evaluation category, we also provide the scores for the more common binary acceptability. For this, we fine-tuned BERT only on the CoLA dataset (BERT-CoLA). Evaluating BERT-CoLA on CoLA testset, our results are in line with the published benchmarks (Devlin et al., 2019). Final evaluations are done on a subset of 700 randomly selected sentences from the Fanfiction database going through backtranslation, RAND modification and S IAM AO. 4.1.2 Utility: Text similarity We use the following measures to quantify the similarity between original and perturbed texts. (1) Bleu score (BL) (Papineni et al., 2002), measuring n-gram overlap between original and generated texts, previously used to assess difference in style transfer (Shen et al., 2017). (2) Edit distance (ED), 4.1.3 184 Utility: Language acceptability 7 Project website: https://clasp.gu.se/about/ people/shalom-lappin/smog. 4.2 Data Several datasets have been used for author identification, including various PAN datasets. We use the dataset from Saedi and Dras (2019) consisting of 10000 authors from the domain of fanfiction,8 as one that is large enough to train a deep learning system. We followed the FF -5K (5000 author) dataset setup un"
2020.starsem-1.19,P18-1080,0,0.0887867,"asures distances between documents over repeated samples by various fixed metrics (e.g. cosine similarity, Ruzicka). An end-to-end trainable deep learning author obfuscation architecture needs a deep learning component for author identification. A deep learning similarity-based approach to author identification has been proposed by Saedi and Dras (2019), using a Siamese network. This approach outperforms alternatives on up to 5000 authors, and is suitable for our work. 2.2 Author Obfuscation Author obfuscation is a less explored area which shares interest with fields including style transfer (Prabhumoye et al., 2018) or attribute masking (Reddy and Knight, 2016). The goal is to change or perturb a text, so that the accuracy of a specific authorship inference mechanism is worsened while the modified text conveys the original message. Early research like that of Kacmarcik and Gamon (2006) worked at the level of machine learning features, proposing to eliminate those that are more effective in classification; this, however, resulted in mostly unreadable texts. At the level of working directly with text, one approach uses backtranslation: input text is translated to a pivot language and translated back to the"
2020.starsem-1.19,przybocki-etal-2006-edit,0,0.00941472,"(V1 , V2 ) as their corresponding representations in vector space, Vi is perturbed to Vi0 which is then sent back to Siamese decision by replacing Vi . It shows whether the Siamese author verification’s original decision on (V1 , V2 ) is different from the decision on (V1 , V20 ) and (V10 , V2 ). This gives a preliminary result: if the system cannot produce vectors that can fool the decider, it will not produce successfully perturbed texts. considering the texts as strings and counting the minimum number of operations required to transform the original texts into their perturbed counterparts (Przybocki et al., 2006; Li et al., 2019). This metric is believed to be used in commercial translation memory models (Bloodgood and Strauss, 2014). (3) Euclidean Distance (EC) between the vector representations: closeness in vector space typically corresponds to greater semantic similarity (Li et al., 2019; Alzantot et al., 2018). Perturbation Win in Text Space The author identification work of Saedi and Dras (2019) had as its primary evaluation, following the first work on deep Siamese networks (Koch et al., 2015), N -way one-shot classification: a ‘query’ text is compared against texts by N authors, one of whom i"
2020.starsem-1.19,W16-5603,0,0.207552,"d samples by various fixed metrics (e.g. cosine similarity, Ruzicka). An end-to-end trainable deep learning author obfuscation architecture needs a deep learning component for author identification. A deep learning similarity-based approach to author identification has been proposed by Saedi and Dras (2019), using a Siamese network. This approach outperforms alternatives on up to 5000 authors, and is suitable for our work. 2.2 Author Obfuscation Author obfuscation is a less explored area which shares interest with fields including style transfer (Prabhumoye et al., 2018) or attribute masking (Reddy and Knight, 2016). The goal is to change or perturb a text, so that the accuracy of a specific authorship inference mechanism is worsened while the modified text conveys the original message. Early research like that of Kacmarcik and Gamon (2006) worked at the level of machine learning features, proposing to eliminate those that are more effective in classification; this, however, resulted in mostly unreadable texts. At the level of working directly with text, one approach uses backtranslation: input text is translated to a pivot language and translated back to the original one, producing a more or less simila"
2021.acl-long.9,D17-1098,1,0.925029,"9) require the models to mention all or some of the input keywords, key-value pairs and image object labels (respectively), potentially with linguistic variants, in the generated outputs. Large (pre-trained) Transformer-based S2S models such as T5 (Raffel et al., 2019) can be trained (fine-tuned) to perform this task. However, they only learn to copy the surface tokens from encoder inputs to the decoder outputs and there is no underlying mechanism guaranteeing good constraint satisfaction (the ratio of satisfied lexical constraints to given lexical constraints). Constrained Beam Search (CBS) (Anderson et al., 2017) and related algorithms can guarantee outputs satisfying all constraints, however they are much slower than the standard beam search algorithm. In addition, as they are all inference-based algorithms, their corresponding models are not aware of the 103 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 103–113 August 1–6, 2021. ©2021 Association for Computational Linguistics constraint words or phrases, the resulting generation could be poor. Ideally, a method for producing co"
2021.acl-long.9,W17-3518,0,0.0188628,"example, the Mention Flag for flower is set (indicated by orange dots) from the third token because it is generated at the second step. Both token and Mention Flag embeddings are the input to the decoder, but Mention Flags are injected into the decoder in a different way to the tokens (see Fig. 3). Note that task specific encoder inputs have been omitted for brevity. Introduction This paper focuses on Seq2Seq (S2S) constrained text generation where a set of encoder input tokens are required to be present in the generated outputs. For example, Keyword-to-Text (Lin et al., 2020), Data-to-Text (Gardent et al., 2017; Duˇsek et al., 2020) and Image-to-Text (Lin et al., 2014; 1 The source code for this paper is released at https: //github.com/GaryYufei/ACL2021MF Agrawal et al., 2019) require the models to mention all or some of the input keywords, key-value pairs and image object labels (respectively), potentially with linguistic variants, in the generated outputs. Large (pre-trained) Transformer-based S2S models such as T5 (Raffel et al., 2019) can be trained (fine-tuned) to perform this task. However, they only learn to copy the surface tokens from encoder inputs to the decoder outputs and there is no un"
2021.acl-long.9,P16-1154,0,0.0386084,"k well in low-resource settings. Our MF models set a new Background Training S2S Models S2S models can implicitly capture the co-occurrence between encoder and decoder sequences, particularly pre-trained ones such as T5 (Raffel et al., 2019) and BART (Lewis et al., 2020). Wen et al. (2015) uses a special gate to control what information will be generated in the following steps. Kale and Rastogi (2020) have shown that the T5 models achieve state-of-the-art results in various Data-to-Text tasks, requiring copying from encoder to decoder, after fine-tuning. As an alternative, the Copy Mechanism (Gu et al., 2016) explicitly learns where to copy the input constraints into the output by adding an extra copy pathway to the models. However, these approaches cannot control or guarantee their constraint satisfaction. Lin et al. (2020) also have observed lower constraint satisfaction in the above methods, compared to the constrained decoding approaches. Constrained Decoding These algorithms, including Constrained Beam Search (CBS) (Anderson et al., 2017) and Grid Beam Search (GBS) (Hokamp and Liu, 2017), maintain a set of states which have their own size-k beams and only allow hypotheses satisfying specific"
2021.acl-long.9,P17-1141,0,0.021949,"ext tasks, requiring copying from encoder to decoder, after fine-tuning. As an alternative, the Copy Mechanism (Gu et al., 2016) explicitly learns where to copy the input constraints into the output by adding an extra copy pathway to the models. However, these approaches cannot control or guarantee their constraint satisfaction. Lin et al. (2020) also have observed lower constraint satisfaction in the above methods, compared to the constrained decoding approaches. Constrained Decoding These algorithms, including Constrained Beam Search (CBS) (Anderson et al., 2017) and Grid Beam Search (GBS) (Hokamp and Liu, 2017), maintain a set of states which have their own size-k beams and only allow hypotheses satisfying specific constraints to be considered during inference. Each CBS state corresponds to the hypotheses satisfying different constraints (exponential in the number of constraints) and the GBS states correspond to the hypotheses satisfying the same number of constraints (linear to constraint number). Balakrishnan et al. (2019); Juraska et al. (2018); Duˇsek and Jurˇc´ıcˇ ek (2016) also modify their inference algorithm in a similar way to fulfill specific output requirements. However, they significantl"
2021.acl-long.9,P19-1080,0,0.0168116,"s, compared to the constrained decoding approaches. Constrained Decoding These algorithms, including Constrained Beam Search (CBS) (Anderson et al., 2017) and Grid Beam Search (GBS) (Hokamp and Liu, 2017), maintain a set of states which have their own size-k beams and only allow hypotheses satisfying specific constraints to be considered during inference. Each CBS state corresponds to the hypotheses satisfying different constraints (exponential in the number of constraints) and the GBS states correspond to the hypotheses satisfying the same number of constraints (linear to constraint number). Balakrishnan et al. (2019); Juraska et al. (2018); Duˇsek and Jurˇc´ıcˇ ek (2016) also modify their inference algorithm in a similar way to fulfill specific output requirements. However, they significantly increase the inference run-time and memory and can produce sub-optimal outputs. 3 Method This section first formulates constrained text generation tasks, then introduces Mention Flags and their 104 integration with Transformer-based text generators. 3.1 S2S Constrained Text Generation 7 3 7 3 3 In the S2S constrained text generation tasks, we are given encoder inputs x = [x1 , . . . , xlx ] ∈ X that describe the task"
2021.acl-long.9,W05-0909,0,0.0976559,"Missing"
2021.acl-long.9,W19-8652,0,0.0315495,"Missing"
2021.acl-long.9,N18-1014,0,0.0124725,"ed decoding approaches. Constrained Decoding These algorithms, including Constrained Beam Search (CBS) (Anderson et al., 2017) and Grid Beam Search (GBS) (Hokamp and Liu, 2017), maintain a set of states which have their own size-k beams and only allow hypotheses satisfying specific constraints to be considered during inference. Each CBS state corresponds to the hypotheses satisfying different constraints (exponential in the number of constraints) and the GBS states correspond to the hypotheses satisfying the same number of constraints (linear to constraint number). Balakrishnan et al. (2019); Juraska et al. (2018); Duˇsek and Jurˇc´ıcˇ ek (2016) also modify their inference algorithm in a similar way to fulfill specific output requirements. However, they significantly increase the inference run-time and memory and can produce sub-optimal outputs. 3 Method This section first formulates constrained text generation tasks, then introduces Mention Flags and their 104 integration with Transformer-based text generators. 3.1 S2S Constrained Text Generation 7 3 7 3 3 In the S2S constrained text generation tasks, we are given encoder inputs x = [x1 , . . . , xlx ] ∈ X that describe the task, where some xi corresp"
2021.acl-long.9,2020.inlg-1.14,0,0.0748271,"he non-pre-trained and pre-trained S2S Transformer-based models. Furthermore, our experiments show that the MF models can satisfy novel constraints (i.e, involving words or phrases not seen during training) and they work well in low-resource settings. Our MF models set a new Background Training S2S Models S2S models can implicitly capture the co-occurrence between encoder and decoder sequences, particularly pre-trained ones such as T5 (Raffel et al., 2019) and BART (Lewis et al., 2020). Wen et al. (2015) uses a special gate to control what information will be generated in the following steps. Kale and Rastogi (2020) have shown that the T5 models achieve state-of-the-art results in various Data-to-Text tasks, requiring copying from encoder to decoder, after fine-tuning. As an alternative, the Copy Mechanism (Gu et al., 2016) explicitly learns where to copy the input constraints into the output by adding an extra copy pathway to the models. However, these approaches cannot control or guarantee their constraint satisfaction. Lin et al. (2020) also have observed lower constraint satisfaction in the above methods, compared to the constrained decoding approaches. Constrained Decoding These algorithms, includin"
2021.acl-long.9,P16-2008,0,0.0438952,"Missing"
2021.acl-long.9,2020.acl-main.703,0,0.212736,"atural Language Processing, pages 103–113 August 1–6, 2021. ©2021 Association for Computational Linguistics constraint words or phrases, the resulting generation could be poor. Ideally, a method for producing constrained text should: a) generate high-quality text; b) achieve high constraint satisfaction; c) have an efficient inference procedure. state-of-the-art in these three tasks. 2 In this paper, we focus on constraining transformerbased text generation models due to their popularity and success in various domains, especially in largescale pre-trained language models (Raffel et al., 2019; Lewis et al., 2020). Previous work can be roughly categorized into two streams: S2S training approaches and Constrained decoding approaches: To this end, we propose Mention Flags (MF), which trace whether a lexical constraint has been realized in partial decoder outputs. Specifically, each decoder input token is provided with a set of flags indicating which constraints have been satisfied up to that token. As shown in Fig 1, the Mention Flags for flower is set from the third step, because flower is generated at the second step. We represent the three possible Mention Flags as separate trainable embeddings and in"
2021.acl-long.9,2020.findings-emnlp.165,0,0.427516,"surface tokens from encoders to decoders, but they cannot guarantee constraint satisfaction. Constrained decoding algorithms always produce hypotheses satisfying all constraints. However, they are computationally expensive and can lower the generated text quality. In this paper, we propose Mention Flags (MF), which trace whether lexical constraints are satisfied in the generated outputs of an S2S decoder. The MF models are trained to generate tokens until all constraints are satisfied, guaranteeing high constraint satisfaction. Our experiments on the Common Sense Generation task (CommonGen) (Lin et al., 2020), End2end Data-to-Text task (E2ENLG) (Duˇsek et al., 2020) and Novel Object Captioning task (nocaps) (Agrawal et al., 2019) show that the MF models maintain higher constraint satisfaction and text quality than the baseline models and other constrained text generation algorithms, achieving state-of-the-art performance on all three tasks. These results are achieved with a much lower run-time than constrained decoding algorithms. We also show that the MF models work well in the low-resource setting. 1 1 Figure 1: An overview of the Mention Flag mechanism for Transformer-based S2S models. Here, th"
2021.acl-long.9,N03-1020,0,0.0525378,"Missing"
2021.acl-long.9,P02-1040,0,0.119361,"Missing"
2021.acl-long.9,P16-1162,0,0.0190521,"in y:t . We use conventions from the relevant data set to determine whether a constraint is a multi-word constraint. This avoids false update when the models only generate the prefix of the constraints, rather than the full constraints. For example, given constraint “washing machine”, the output could be “I put my washing in the new washing machine.” The situation becomes more complicated when both washing and washing machine are given lexical constraints. When we find this case, we delay the value 2 update for washing until the word in is generated. Modern tokenization methods, such as BPE (Sennrich et al., 2016), make this situation frequent. Definition of Mentions We deliberately allow a flexible notion of mentions in the Function m(). We can define various types of mentions to fulfill the requirements of different applications and tasks. With this flexibility, the end-users can use Mention Flags in many constraint scenarios. For tasks with strict constraints, we define mentions to be the exact string match in y:t . Otherwise, inflectional variants or synonyms of words in the lexical constraints are allowed when checking for mentions. Our Mention Flag mechanism thus supports lexical constraints with"
2021.acl-long.9,N18-2074,0,0.0218551,"e that the CA module in the Transformer decoder already uses y:t as query and he as key. The resulting query-key similarity matrix has the same size of our Mention Flag matrix, making it suitable to incorporate F . Standard S2S Transformer Model The encoder input tokens x is fed into the Transformer Encoder he = Enc(x) where he ∈ Rlx ×d and d is the model hidden size. In the Transformer decoder, there are two self-attention modules, Self MultiHead Attention (SA) which handles the current decoder input sequence y:t , and Cross Multi-Head 106 Mention Flag Matrix as Relative Position Inspired by Shaw et al. (2018) which incorporates token relative positions into the SA module, we propose to inject Mention Flags as the “relative positions” between encoder output he and current decoder input y:t in the CA module. In each decoder layer, we represent F as two sets of trainable embeddings Mention Flag key mk = Ek (F ) and Mention Flag Value mv = Ev (F ) where Ek , Ev ∈ R3×d are the Mention Flag embedding tables. mk and mv ∈ Rlx ×t×d . We have separated Mention Flags representations for each decoder layer. Eq. 4 is changed to: layer of the T5 decoder. This parameters freezing technology is applied to both T5"
2021.acl-long.9,N19-1410,0,0.0383923,"Missing"
2021.acl-long.9,2020.acl-main.325,0,0.0391574,"Missing"
2021.acl-long.9,2021.eacl-main.104,1,0.896614,"objects jointly (Anderson et al., 2018). However, Puduppully et al. (2019) shows the benefits of separating content selection and text planning steps for general data-to-text tasks. Following this, we propose to first select salient objects and incorporate the selected objects into the description using Mention Flags. m(C, ε) = [0, 0, · · · , 1, · · · , 0, 0, · · · , 1] where only salient object labels receive value 1. m() allows inflectional variants to satisfy lexical constraints. We use T5-base model in this experiment. The T5 + C and T5 + MF + C models are constrained with CBS. Following Wang et al. (2021), we report CIDEr and SPICE as output text quality metrics and constraint satisfaction for novel constraints (Novel) and all constraints (ALL). We present the performance for all evaluation images 108 (Overall) and for the challenging images with only novel objects (out-of-domain split). model, indicating that the MF model correctly captures more long-range relationships (calculated by the parsing trees used in SPICE) among the (novel) objects than CBS. Our T5 + MF model outperforms the existing state-of-the-art end-to-end single-stage image captioning systems (Agrawal et al., 2019; Li et al.,"
2021.acl-long.9,D15-1199,0,0.066173,"Missing"
C08-1145,C02-1076,0,0.0672665,"source and target language sides respectively. Section 7 concludes. 2 Related work In this section we briefly review some relevant work on deciding between translation candidates in ‘sentence-level’ MEMT. Most common is the use of language models, or voting which may be based on some kind of alignment, or a combination. Callison-Burch and Flournoy (2001) use a trigram language model (LM) on MT outputs to decide the best candidate, looking at nine systems across four language directions and domains, and treating them as black boxes; evaluation is by human judges and on a fairly small data set. Akiba et al. (2002) score MT outputs by a combination of a standard LM and an alignment model (here IBM 4), and then use statistical tests to determine rankings of MT system outputs. Eisele (2005) uses a heuristic voting scheme based on n-gram overlap of the different outputs, and adds an LM to make decisions; the LM reportedly achieves further improvement. Rosti et al. (2007) look at sentence-level combinations (as well as word- and phrase-level), using reranking of n-best lists and confidence scores derived from generalised linear models with probabilistic features from n-best lists. Huang and Papineni (2007)"
C08-1145,2001.mtsummit-papers.12,0,0.169042,"al gain in correctly choosing the better translation candidate in our context. In Section 4 we build a classifier using an approximation to fairly standard language and alignment model features, mostly for use as a comparator, while Sections 5 and 6 present our models based on source and target language sides respectively. Section 7 concludes. 2 Related work In this section we briefly review some relevant work on deciding between translation candidates in ‘sentence-level’ MEMT. Most common is the use of language models, or voting which may be based on some kind of alignment, or a combination. Callison-Burch and Flournoy (2001) use a trigram language model (LM) on MT outputs to decide the best candidate, looking at nine systems across four language directions and domains, and treating them as black boxes; evaluation is by human judges and on a fairly small data set. Akiba et al. (2002) score MT outputs by a combination of a standard LM and an alignment model (here IBM 4), and then use statistical tests to determine rankings of MT system outputs. Eisele (2005) uses a heuristic voting scheme based on n-gram overlap of the different outputs, and adds an LM to make decisions; the LM reportedly achieves further improveme"
C08-1145,E06-1032,0,0.0346552,"Missing"
C08-1145,2003.mtsummit-papers.6,0,0.0211423,"ual error analysis, taking advantage of character1154 istics specific to their MT system and parser. Nomoto (2003) uses a LM and an IBM-based alignment model, and then constructs separate SVMs for regression based on these, each with a single feature (i.e. the LM value or the alignment model value); the SVM is thus not strictly used as a classifier, but as a regression tool. Nomoto (2004) extends this by deciding on the best LM through a voting scheme. Other related work not in an MEMT context that uses parsers to distinguish better from worse translations are on syntax-based language models (Charniak et al., 2003) and on syntactically informed reranking (Och et al., 2003). Both use only single parsers and work only with candidate translations generated inside an SMT system (either all candidates or n-best). 3 Potential Gain The type of system we focus on in this paper operates in two stages. First, syntactically based reordering takes place to make the source sentence more similar in structure to the syntax of the target language. This is then passed to a Phrase-based SMT (PSMT) component (Pharaoh (Koehn, 2004) in the cited work). For German to English (Collins et al., 2005) and Dutch to English (Zwart"
C08-1145,P05-1066,0,0.227807,"of the main idea of this paper is that there are two ways in which problematic translations might be detected. One is on the source side: perhaps sentences with difficult or incorrect parses could lead to bad use of syntax and hence bad translations, and this could be detected by a classifier. The other is that on the target side, perhaps the output quality could be measured in a more syntactically informed way, looking for syntactic abnormalities. As to the particular system, in this paper we look at a specific type of MT, the output of systems that use syntactic reordering as preprocessing (Collins et al., 2005; Wang et al., 2007; Zwarts and Dras, 2007). In these systems, the source language is reordered to mirror the syntax of the target language in certain respects, leading to an improvement in the aggregate quality of the output over the baseline, although it is not always the case that each individual sentence in the reordered version is better. This could then be framed as an MEMT, where the reordered candidate is considered the default one, backing off to the baseline where the reordered one is worse, based on the decision of a classifier. Given the ‘unnatural’ order of the preprocessed source"
C08-1145,P01-1020,0,0.0723559,"using reranking of n-best lists and confidence scores derived from generalised linear models with probabilistic features from n-best lists. Huang and Papineni (2007) propose a hierarchical model for word, phrase and sentence level combination; they use LMs and interestingly find that incorporating rudimentary linguistic information like Part-of-Speech is helpful. Riezler and Maxwell (2006) combine transfer-based and statistical MT; they back off to the SMT translation when the grammar is inadequate, analysing the grammar to determine this. Other work, like ours, uses a classifier. The goal of Corston-Oliver et al. (2001) is slightly different, in that it aims to distinguish human translations from MT output. The classifier uses syntactic features derived from a manual error analysis, taking advantage of character1154 istics specific to their MT system and parser. Nomoto (2003) uses a LM and an IBM-based alignment model, and then constructs separate SVMs for regression based on these, each with a single feature (i.e. the LM value or the alignment model value); the SVM is thus not strictly used as a classifier, but as a regression tool. Nomoto (2004) extends this by deciding on the best LM through a voting sche"
C08-1145,W05-0828,0,0.061405,"sentence-level’ MEMT. Most common is the use of language models, or voting which may be based on some kind of alignment, or a combination. Callison-Burch and Flournoy (2001) use a trigram language model (LM) on MT outputs to decide the best candidate, looking at nine systems across four language directions and domains, and treating them as black boxes; evaluation is by human judges and on a fairly small data set. Akiba et al. (2002) score MT outputs by a combination of a standard LM and an alignment model (here IBM 4), and then use statistical tests to determine rankings of MT system outputs. Eisele (2005) uses a heuristic voting scheme based on n-gram overlap of the different outputs, and adds an LM to make decisions; the LM reportedly achieves further improvement. Rosti et al. (2007) look at sentence-level combinations (as well as word- and phrase-level), using reranking of n-best lists and confidence scores derived from generalised linear models with probabilistic features from n-best lists. Huang and Papineni (2007) propose a hierarchical model for word, phrase and sentence level combination; they use LMs and interestingly find that incorporating rudimentary linguistic information like Part"
C08-1145,A94-1016,0,0.330476,"Missing"
C08-1145,1995.iwpt-1.15,0,0.0222129,"Missing"
C08-1145,D07-1029,0,0.029946,"a set. Akiba et al. (2002) score MT outputs by a combination of a standard LM and an alignment model (here IBM 4), and then use statistical tests to determine rankings of MT system outputs. Eisele (2005) uses a heuristic voting scheme based on n-gram overlap of the different outputs, and adds an LM to make decisions; the LM reportedly achieves further improvement. Rosti et al. (2007) look at sentence-level combinations (as well as word- and phrase-level), using reranking of n-best lists and confidence scores derived from generalised linear models with probabilistic features from n-best lists. Huang and Papineni (2007) propose a hierarchical model for word, phrase and sentence level combination; they use LMs and interestingly find that incorporating rudimentary linguistic information like Part-of-Speech is helpful. Riezler and Maxwell (2006) combine transfer-based and statistical MT; they back off to the SMT translation when the grammar is inadequate, analysing the grammar to determine this. Other work, like ours, uses a classifier. The goal of Corston-Oliver et al. (2001) is slightly different, in that it aims to distinguish human translations from MT output. The classifier uses syntactic features derived"
C08-1145,koen-2004-pharaoh,0,0.0161004,"to distinguish better from worse translations are on syntax-based language models (Charniak et al., 2003) and on syntactically informed reranking (Och et al., 2003). Both use only single parsers and work only with candidate translations generated inside an SMT system (either all candidates or n-best). 3 Potential Gain The type of system we focus on in this paper operates in two stages. First, syntactically based reordering takes place to make the source sentence more similar in structure to the syntax of the target language. This is then passed to a Phrase-based SMT (PSMT) component (Pharaoh (Koehn, 2004) in the cited work). For German to English (Collins et al., 2005) and Dutch to English (Zwarts and Dras, 2007) this reordering involves moving some long-distance dependencies closer together, such as clause-final participles and verb-second auxiliaries. This improves translation quality by compensating for the weakness in PSMT of long-distance word reordering: Collins et al. (2005) report a 1.6 Bleu percentage point improvement, Zwarts and Dras (2007) a 1.0 Bleu percentage point improvement. However, individual sentences translated from the original non-reordered source sentences are sometimes"
C08-1145,P07-1044,1,0.855875,"utput over the baseline, although it is not always the case that each individual sentence in the reordered version is better. This could then be framed as an MEMT, where the reordered candidate is considered the default one, backing off to the baseline where the reordered one is worse, based on the decision of a classifier. Given the ‘unnatural’ order of the preprocessed source side, there is reason to expect that bad or unsuccessful reordered translations might be detectable. The second part of the main idea of the paper is that a classifier could use a combination of multiple parsers, as in Mutton et al. (2007), to indicate problems. In that work, designed to assess fluency of output of generation systems, metrics were developed from various parsers—log probability of most likely parse, number of tree fragments, and so on— that correlated with human judgements, and that could be combined in a classifier to produce a better evaluation metric. We take such an approach as a starting point for developing classifiers to indicate problematic source and target sides within a reordering MT system. In Section 2 we review some related work. In Section 3 we investigate the potential gain in correctly choosing"
C08-1145,2003.mtsummit-papers.36,0,0.298558,"ind that incorporating rudimentary linguistic information like Part-of-Speech is helpful. Riezler and Maxwell (2006) combine transfer-based and statistical MT; they back off to the SMT translation when the grammar is inadequate, analysing the grammar to determine this. Other work, like ours, uses a classifier. The goal of Corston-Oliver et al. (2001) is slightly different, in that it aims to distinguish human translations from MT output. The classifier uses syntactic features derived from a manual error analysis, taking advantage of character1154 istics specific to their MT system and parser. Nomoto (2003) uses a LM and an IBM-based alignment model, and then constructs separate SVMs for regression based on these, each with a single feature (i.e. the LM value or the alignment model value); the SVM is thus not strictly used as a classifier, but as a regression tool. Nomoto (2004) extends this by deciding on the best LM through a voting scheme. Other related work not in an MEMT context that uses parsers to distinguish better from worse translations are on syntax-based language models (Charniak et al., 2003) and on syntactically informed reranking (Och et al., 2003). Both use only single parsers an"
C08-1145,P04-1063,0,0.118019,"ork, like ours, uses a classifier. The goal of Corston-Oliver et al. (2001) is slightly different, in that it aims to distinguish human translations from MT output. The classifier uses syntactic features derived from a manual error analysis, taking advantage of character1154 istics specific to their MT system and parser. Nomoto (2003) uses a LM and an IBM-based alignment model, and then constructs separate SVMs for regression based on these, each with a single feature (i.e. the LM value or the alignment model value); the SVM is thus not strictly used as a classifier, but as a regression tool. Nomoto (2004) extends this by deciding on the best LM through a voting scheme. Other related work not in an MEMT context that uses parsers to distinguish better from worse translations are on syntax-based language models (Charniak et al., 2003) and on syntactically informed reranking (Och et al., 2003). Both use only single parsers and work only with candidate translations generated inside an SMT system (either all candidates or n-best). 3 Potential Gain The type of system we focus on in this paper operates in two stages. First, syntactically based reordering takes place to make the source sentence more si"
C08-1145,N06-1032,0,0.0134585,"g scheme based on n-gram overlap of the different outputs, and adds an LM to make decisions; the LM reportedly achieves further improvement. Rosti et al. (2007) look at sentence-level combinations (as well as word- and phrase-level), using reranking of n-best lists and confidence scores derived from generalised linear models with probabilistic features from n-best lists. Huang and Papineni (2007) propose a hierarchical model for word, phrase and sentence level combination; they use LMs and interestingly find that incorporating rudimentary linguistic information like Part-of-Speech is helpful. Riezler and Maxwell (2006) combine transfer-based and statistical MT; they back off to the SMT translation when the grammar is inadequate, analysing the grammar to determine this. Other work, like ours, uses a classifier. The goal of Corston-Oliver et al. (2001) is slightly different, in that it aims to distinguish human translations from MT output. The classifier uses syntactic features derived from a manual error analysis, taking advantage of character1154 istics specific to their MT system and parser. Nomoto (2003) uses a LM and an IBM-based alignment model, and then constructs separate SVMs for regression based on"
C08-1145,N07-1029,0,0.0423554,"a trigram language model (LM) on MT outputs to decide the best candidate, looking at nine systems across four language directions and domains, and treating them as black boxes; evaluation is by human judges and on a fairly small data set. Akiba et al. (2002) score MT outputs by a combination of a standard LM and an alignment model (here IBM 4), and then use statistical tests to determine rankings of MT system outputs. Eisele (2005) uses a heuristic voting scheme based on n-gram overlap of the different outputs, and adds an LM to make decisions; the LM reportedly achieves further improvement. Rosti et al. (2007) look at sentence-level combinations (as well as word- and phrase-level), using reranking of n-best lists and confidence scores derived from generalised linear models with probabilistic features from n-best lists. Huang and Papineni (2007) propose a hierarchical model for word, phrase and sentence level combination; they use LMs and interestingly find that incorporating rudimentary linguistic information like Part-of-Speech is helpful. Riezler and Maxwell (2006) combine transfer-based and statistical MT; they back off to the SMT translation when the grammar is inadequate, analysing the grammar"
C08-1145,D07-1077,0,0.0593701,"is paper is that there are two ways in which problematic translations might be detected. One is on the source side: perhaps sentences with difficult or incorrect parses could lead to bad use of syntax and hence bad translations, and this could be detected by a classifier. The other is that on the target side, perhaps the output quality could be measured in a more syntactically informed way, looking for syntactic abnormalities. As to the particular system, in this paper we look at a specific type of MT, the output of systems that use syntactic reordering as preprocessing (Collins et al., 2005; Wang et al., 2007; Zwarts and Dras, 2007). In these systems, the source language is reordered to mirror the syntax of the target language in certain respects, leading to an improvement in the aggregate quality of the output over the baseline, although it is not always the case that each individual sentence in the reordered version is better. This could then be framed as an MEMT, where the reordered candidate is considered the default one, backing off to the baseline where the reordered one is worse, based on the decision of a classifier. Given the ‘unnatural’ order of the preprocessed source side, there is rea"
C08-1145,U06-1021,1,0.899603,"Missing"
C08-1145,2007.mtsummit-papers.74,1,0.915813,"ere are two ways in which problematic translations might be detected. One is on the source side: perhaps sentences with difficult or incorrect parses could lead to bad use of syntax and hence bad translations, and this could be detected by a classifier. The other is that on the target side, perhaps the output quality could be measured in a more syntactically informed way, looking for syntactic abnormalities. As to the particular system, in this paper we look at a specific type of MT, the output of systems that use syntactic reordering as preprocessing (Collins et al., 2005; Wang et al., 2007; Zwarts and Dras, 2007). In these systems, the source language is reordered to mirror the syntax of the target language in certain respects, leading to an improvement in the aggregate quality of the output over the baseline, although it is not always the case that each individual sentence in the reordered version is better. This could then be framed as an MEMT, where the reordered candidate is considered the default one, backing off to the baseline where the reordered one is worse, based on the decision of a classifier. Given the ‘unnatural’ order of the preprocessed source side, there is reason to expect that bad o"
C08-1145,N03-5008,0,\N,Missing
C08-1145,J03-4003,0,\N,Missing
C12-1111,W05-1601,0,0.0408358,"Missing"
C12-1111,P10-1131,0,0.0428875,"Missing"
C12-1111,W02-1503,0,0.0793487,"ui à la suite de Velldal and Oepen (2006), ont développé un modèle log-linéaire de réalisation pour une grammaire Lexical Functional Grammar (LFG). Comme eux, nous utilisons la plateforme Xerox Linguistics Environment (XLE) et construisons une banque d’arbres symétrique en analysant puis en regénérant les phrases. Ceci nous fournit à la fois des exemples positifs et négatifs. Nous utilisons les sections 2 à 21 du Penn Treebank (38 008 phrases) pour l’entraînement et la section 23 (2245 phrases) pour l’évaluation. Nous les avons analysées en utilisant la grammaire ParGram de l’anglais sur XLE (Butt et al., 2002), puis nous les avons regénérées en renversant la même grammaire pour produire de multiples paraphrases candidates. Nous avons rejeté les cas où la grammaire ne regénérait pas plus d’une phrase, ou plus de 1000, pour obtenir un corpus d’entraînement de 20 613 ensembles de paraphrases et un corpus de test de 1168 ensembles de paraphrases. Souvent, la grammaire n’arrivait pas à reproduire la phrase originale. Nous avons donc calculé la distance d’édition pour chaque paraphrase. Nous utilisons comme cas positif pour l’entraînement et pour le test la phrase regénérée ayant la plus petite distance"
C12-1111,W07-2303,0,0.0926388,"ent category and Part of Speech (POS) tags (Langkilde-Geary, 2002). This idea was developed further for handcrafted symbolic grammars based on linguistic formalisms that have been used for generation. In the Head-driven Phrase Structure Grammar (HPSG) system LOGON, Velldal and Oepen (2005) improve on this by using a maximum entropy model incorporating both a LM and features derived from HPSG structures for the candidate outputs; this drew on earlier developments in the field of statistical parsing that used such features for reranking (Johnson et al., 1999; Riezler et al., 2002, for example). Cahill et al. (2007) and White and Rajkumar (2009) similarly show that ranking for Lexical Functional Grammar (LFG)- and Combinatory Categorial Grammar (CCG)-based systems respectively can be improved by models incorporating a LM and structural features. All of these rankers depend on multiple structures produced by the respective large-scale symbolic grammars to rank the output. For much smaller symbolic grammars, and those in the process of development — e.g. LFG grammars for Indonesian (Arka et al., 2009) or Arrernte (Dras et al., 2012), or HPSG grammars for Persian (Müller, 2010) or Wambaya (Bender, 2008) — t"
C12-1111,P01-1017,0,0.0599094,"est. Keywords: natural language generation, realization ranking, unsupervised parsing. Keywords in French: génération de langue naturelle, analyseur statistique non supervisé. Proceedings of COLING 2012: Technical Papers, pages 1811–1830, COLING 2012, Mumbai, December 2012. 1811 1 Condensed 2-page version in French Les systèmes de génération de textes peuvent produire plusieurs textes de qualité variable pour les mêmes données. Cela a mené à la création d’algorithmes de sélection pour choisir le meilleur texte (Langkilde-Geary, 2000; Velldal and Oepen, 2005; Cahill et al., 2007, par exemple). Charniak (2001) a proposé d’utiliser les analyseurs statistiques comme source de structures sur lesquelles baser les algorithmes de sélection. À notre connaissance, seuls des analyseurs supervisés ont été utilisés à cet effet. Cependant, pour les systèmes basés sur des grammaires symboliques de modeste envergure ou en développement, trop peu de données sont disponibles, et les modèles existants s’appliquent mal. Dans cet article, nous étudions donc la possibilité d’utiliser un analyseur statistique non supervisé (Naseem et al., 2010) comme source alternative d’information pour la sélection automatique des te"
C12-1111,P05-1022,0,0.0230884,"and Language Models Before answering any questions about the usefulness of unsupervised parsers, we address the question, Do supervised parsers produce structural information that is useful for realisation ranking? If the answer is yes (and based on the work cited in Section 3 and others, we would think it likely), the supervised parsers and a large LM would constitute an upper bound on the efficacy of using parsers to rank candidate sentences generated by XLE. The supervised parsers we use are the Stanford parser (Klein and Manning, 2003) and the Charniak and Johnson (henceforth C&J) parser (Charniak and Johnson, 2005). These parsers are both quite accurate: the Stanford parser gets a labelled f-score of 85.61 on the WSJ, and the C&J 91.09. From the Stanford parser we examined both horizontal slices of parse trees, in effect treating them as sets of CFG production rules, and dependencies; the production rules and dependencies were either lexicalised or unlexicalised, and the dependency relations either named or unnamed. This gives two constituency and four dependency feature representations from the Stanford parser: prod-rule-lex and prod-rule-unlex; and dep-lex-named, dep-lex-unnamed, dep-unlex-named and d"
C12-1111,2003.mtsummit-papers.6,0,0.0224041,"helpful. White and Rajkumar (2012) found that explicitly representing dependency lengths led to shorter average dependencies, in line with psycholinguistic evidence for human-produced sentences, and to better generated text. And Filippova and Strube (2009) found that while trigram LMs are appropriate for phrase linearisation in German, at a clause level (longer) dependency features produced better results. Parser-based Ranking Charniak (2001) proposed the idea of using statistical parsers as a kind of structural language model. The idea has been applied a number of times in MT, for example by Charniak et al. (2003) or Post and Gildea (2008); it has also been applied in NLG, where Mutton et al. (2007) showed that a combination of parser-based metrics correlated with human judgements of the quality of generated text. To our knowledge, all work applying parser-based ranking has used supervised parsers. There is also an interesting piece of work by Cherry and Quirk (2008) where implicit discriminative syntactic LMs are constructed, using a latent Support Vector Machine (SVM) to train an unlexicalised parser to judge sentences produced by an MT system. The authors discuss some similarities to unsupervised pa"
C12-1111,2008.amta-papers.4,0,0.0151841,"nger) dependency features produced better results. Parser-based Ranking Charniak (2001) proposed the idea of using statistical parsers as a kind of structural language model. The idea has been applied a number of times in MT, for example by Charniak et al. (2003) or Post and Gildea (2008); it has also been applied in NLG, where Mutton et al. (2007) showed that a combination of parser-based metrics correlated with human judgements of the quality of generated text. To our knowledge, all work applying parser-based ranking has used supervised parsers. There is also an interesting piece of work by Cherry and Quirk (2008) where implicit discriminative syntactic LMs are constructed, using a latent Support Vector Machine (SVM) to train an unlexicalised parser to judge sentences produced by an MT system. The authors discuss some similarities to unsupervised parsing, in that both sorts of parsers are trained on sentences without the benefit of annotated parse trees. Using unsupervised parse trees as we do in this paper, however, can benefit from lexicalisation and, potentially, linguistic knowledge embodied in the parser (see below). 1815 Unsupervised Parsers The first unsupervised parser that convincingly beat fa"
C12-1111,N09-2057,0,0.0166609,"d LM scores. Structural features here contributed more strongly than for English. Which structural features are effective in realisation ranking is still an open question. For example, in the context of CCG, Rajkumar and White (2010) found that features reflecting animacy agreement between nouns and relative clauses, and number agreement between subject and verb, were helpful. White and Rajkumar (2012) found that explicitly representing dependency lengths led to shorter average dependencies, in line with psycholinguistic evidence for human-produced sentences, and to better generated text. And Filippova and Strube (2009) found that while trigram LMs are appropriate for phrase linearisation in German, at a clause level (longer) dependency features produced better results. Parser-based Ranking Charniak (2001) proposed the idea of using statistical parsers as a kind of structural language model. The idea has been applied a number of times in MT, for example by Charniak et al. (2003) or Post and Gildea (2008); it has also been applied in NLG, where Mutton et al. (2007) showed that a combination of parser-based metrics correlated with human judgements of the quality of generated text. To our knowledge, all work ap"
C12-1111,P07-1050,0,0.0495937,"Missing"
C12-1111,N09-1012,0,0.231243,"Missing"
C12-1111,P99-1069,0,0.0158786,"d later by injecting new features like headwords, constituent category and Part of Speech (POS) tags (Langkilde-Geary, 2002). This idea was developed further for handcrafted symbolic grammars based on linguistic formalisms that have been used for generation. In the Head-driven Phrase Structure Grammar (HPSG) system LOGON, Velldal and Oepen (2005) improve on this by using a maximum entropy model incorporating both a LM and features derived from HPSG structures for the candidate outputs; this drew on earlier developments in the field of statistical parsing that used such features for reranking (Johnson et al., 1999; Riezler et al., 2002, for example). Cahill et al. (2007) and White and Rajkumar (2009) similarly show that ranking for Lexical Functional Grammar (LFG)- and Combinatory Categorial Grammar (CCG)-based systems respectively can be improved by models incorporating a LM and structural features. All of these rankers depend on multiple structures produced by the respective large-scale symbolic grammars to rank the output. For much smaller symbolic grammars, and those in the process of development — e.g. LFG grammars for Indonesian (Arka et al., 2009) or Arrernte (Dras et al., 2012), or HPSG grammar"
C12-1111,P04-1061,0,0.125248,"ian (Arka et al., 2009) or Arrernte (Dras et al., 2012), or HPSG grammars for Persian (Müller, 2010) or Wambaya (Bender, 2008) — these multiple structures will not be available or will be quite impoverished. The goal of this paper then is to investigate using other sources of structural information that will be available for such languages: specifically — since such languages are also often unlikely to have treebanks — unsupervised statistical parsers. Unsupervised parsers have been motivated by possible application in contexts where large treebanks for training supervised parsers are absent (Klein and Manning, 2004, for example), but given their very modest performance with respect to supervised alternatives it is an open question as to whether the structures they produce have any extrinsic application. We aim to give an answer to this question in the context of realisation ranking. In Section 3 we look at some previous work on reranking in realization in more depth, and more briefly note work on parser-based models for ranking and on unsupervised parsing. In Section 4 we describe our models based on statistical parsers, and the experimental set-up for investigating them. In Section 5 we discuss our res"
C12-1111,P03-1054,0,0.0581206,"l’entraînement et pour le test la phrase regénérée ayant la plus petite distance par rapport à la phrase originale. Comme exemple négatif, nous avons comparé deux alternatives : la phrase ayant la plus grande distance (greatest, dans nos tableaux), et une phrase choisie au hasard parmi celles qui n’avaient pas la plus petite distance (random, dans nos tableaux). Pour la classification, nous avons utilisé le système à entropie maximale MegaM (cinquième version) de Hal Daumé III. Pour fins de comparaison, nous avons reproduit la même méthode avec deux analyseurs supervisés : celui de Stanford (Klein and Manning, 2003) et celui de Charniak et Johnson (2005) (ci-après, C&J). Nous couplons les analyseurs à des modèles de Markov : un grand, construit avec SRILM (Stolcke, 2002) sur le corpus Gigaword, et un petit, afin d’imiter les conditions de développement pour des petites grammaires. Nous utilisons ces modèles de Markov à la fois comme modèles de base et en combinaison avec les modèles structurels. En outre, parce que le principal obstacle à l’utilisation d’un analyseur non supervisé est la faible qualité des analyses qu’il produit (voir Figure 1 pour une analyse non supervisée, par opposition à l’analyse s"
C12-1111,A00-2023,0,0.0498443,"s, knowledge bases or logical forms and produce human-like text. The realization involves making many decisions on the surface representation of a sentence, including lexical selection, use of referring expressions, and word order. In general, such systems can thus generate multiple outputs, some of which are genuine good paraphrases, some of which may sound somewhat marked or odd out of context, and some of which may be (due to system limitations) incorrect. This observation led to the idea of taking the possible outputs and ranking them, in the earliest work by use of a language model (LM) (Langkilde-Geary, 2000) and later by injecting new features like headwords, constituent category and Part of Speech (POS) tags (Langkilde-Geary, 2002). This idea was developed further for handcrafted symbolic grammars based on linguistic formalisms that have been used for generation. In the Head-driven Phrase Structure Grammar (HPSG) system LOGON, Velldal and Oepen (2005) improve on this by using a maximum entropy model incorporating both a LM and features derived from HPSG structures for the candidate outputs; this drew on earlier developments in the field of statistical parsing that used such features for rerankin"
C12-1111,W02-2103,0,0.0111017,"representation of a sentence, including lexical selection, use of referring expressions, and word order. In general, such systems can thus generate multiple outputs, some of which are genuine good paraphrases, some of which may sound somewhat marked or odd out of context, and some of which may be (due to system limitations) incorrect. This observation led to the idea of taking the possible outputs and ranking them, in the earliest work by use of a language model (LM) (Langkilde-Geary, 2000) and later by injecting new features like headwords, constituent category and Part of Speech (POS) tags (Langkilde-Geary, 2002). This idea was developed further for handcrafted symbolic grammars based on linguistic formalisms that have been used for generation. In the Head-driven Phrase Structure Grammar (HPSG) system LOGON, Velldal and Oepen (2005) improve on this by using a maximum entropy model incorporating both a LM and features derived from HPSG structures for the candidate outputs; this drew on earlier developments in the field of statistical parsing that used such features for reranking (Johnson et al., 1999; Riezler et al., 2002, for example). Cahill et al. (2007) and White and Rajkumar (2009) similarly show"
C12-1111,J93-4001,0,0.0769914,"alisations are automatically specified by comparing the yields of the generated sentences with the original strings in the treebank (for them, the Redwoods English treebank); this gives the sets of positive and negative (i.e. all non-original) examples for the maxent learner. Cahill et al. (2007) drew on this to develop a log-linear model for realisation in an LFG context. They worked with a large-scale hand-crafted grammar of German, as they were investigating applicability of the approach to a language with freer word order than English, and generated from f-structures using the XLE system (Maxwell and Kaplan, 1993). They similarly constructed a symmetric treebank, and also had as their goal to re-generate the strings in the original treebank. For structural features in their model, the authors defined 186,731 instantiated templates based on Riezler et al. (2002), Rohrer and Forst (2006) and Riezler and Vasserman (2004); 1,471 of these actually occurred in their training data. The feature templates included information from both c-structure (e.g. simple features such as number of times a particular category label occurs, or compound features such as number of times it dominates another) and f-structure ("
C12-1111,P07-1044,1,0.838021,"led to shorter average dependencies, in line with psycholinguistic evidence for human-produced sentences, and to better generated text. And Filippova and Strube (2009) found that while trigram LMs are appropriate for phrase linearisation in German, at a clause level (longer) dependency features produced better results. Parser-based Ranking Charniak (2001) proposed the idea of using statistical parsers as a kind of structural language model. The idea has been applied a number of times in MT, for example by Charniak et al. (2003) or Post and Gildea (2008); it has also been applied in NLG, where Mutton et al. (2007) showed that a combination of parser-based metrics correlated with human judgements of the quality of generated text. To our knowledge, all work applying parser-based ranking has used supervised parsers. There is also an interesting piece of work by Cherry and Quirk (2008) where implicit discriminative syntactic LMs are constructed, using a latent Support Vector Machine (SVM) to train an unlexicalised parser to judge sentences produced by an MT system. The authors discuss some similarities to unsupervised parsing, in that both sorts of parsers are trained on sentences without the benefit of an"
C12-1111,D10-1120,0,0.23503,"ngkilde-Geary, 2000; Velldal and Oepen, 2005; Cahill et al., 2007, par exemple). Charniak (2001) a proposé d’utiliser les analyseurs statistiques comme source de structures sur lesquelles baser les algorithmes de sélection. À notre connaissance, seuls des analyseurs supervisés ont été utilisés à cet effet. Cependant, pour les systèmes basés sur des grammaires symboliques de modeste envergure ou en développement, trop peu de données sont disponibles, et les modèles existants s’appliquent mal. Dans cet article, nous étudions donc la possibilité d’utiliser un analyseur statistique non supervisé (Naseem et al., 2010) comme source alternative d’information pour la sélection automatique des textes générés, malgré la faible qualité des structures qu’ils produisent. Nos travaux sont basés notamment sur ceux de Cahill et al. (2007), qui à la suite de Velldal and Oepen (2006), ont développé un modèle log-linéaire de réalisation pour une grammaire Lexical Functional Grammar (LFG). Comme eux, nous utilisons la plateforme Xerox Linguistics Environment (XLE) et construisons une banque d’arbres symétrique en analysant puis en regénérant les phrases. Ceci nous fournit à la fois des exemples positifs et négatifs. Nous"
C12-1111,2008.amta-papers.16,0,0.0146347,"r (2012) found that explicitly representing dependency lengths led to shorter average dependencies, in line with psycholinguistic evidence for human-produced sentences, and to better generated text. And Filippova and Strube (2009) found that while trigram LMs are appropriate for phrase linearisation in German, at a clause level (longer) dependency features produced better results. Parser-based Ranking Charniak (2001) proposed the idea of using statistical parsers as a kind of structural language model. The idea has been applied a number of times in MT, for example by Charniak et al. (2003) or Post and Gildea (2008); it has also been applied in NLG, where Mutton et al. (2007) showed that a combination of parser-based metrics correlated with human judgements of the quality of generated text. To our knowledge, all work applying parser-based ranking has used supervised parsers. There is also an interesting piece of work by Cherry and Quirk (2008) where implicit discriminative syntactic LMs are constructed, using a latent Support Vector Machine (SVM) to train an unlexicalised parser to judge sentences produced by an MT system. The authors discuss some similarities to unsupervised parsing, in that both sorts"
C12-1111,C10-2119,0,0.0155292,"iezler and Vasserman (2004); 1,471 of these actually occurred in their training data. The feature templates included information from both c-structure (e.g. simple features such as number of times a particular category label occurs, or compound features such as number of times it dominates another) and f-structure (e.g. frequency of relative order of subject and object), sentence length and LM scores. Structural features here contributed more strongly than for English. Which structural features are effective in realisation ranking is still an open question. For example, in the context of CCG, Rajkumar and White (2010) found that features reflecting animacy agreement between nouns and relative clauses, and number agreement between subject and verb, were helpful. White and Rajkumar (2012) found that explicitly representing dependency lengths led to shorter average dependencies, in line with psycholinguistic evidence for human-produced sentences, and to better generated text. And Filippova and Strube (2009) found that while trigram LMs are appropriate for phrase linearisation in German, at a clause level (longer) dependency features produced better results. Parser-based Ranking Charniak (2001) proposed the id"
C12-1111,P02-1035,0,0.178824,"ew features like headwords, constituent category and Part of Speech (POS) tags (Langkilde-Geary, 2002). This idea was developed further for handcrafted symbolic grammars based on linguistic formalisms that have been used for generation. In the Head-driven Phrase Structure Grammar (HPSG) system LOGON, Velldal and Oepen (2005) improve on this by using a maximum entropy model incorporating both a LM and features derived from HPSG structures for the candidate outputs; this drew on earlier developments in the field of statistical parsing that used such features for reranking (Johnson et al., 1999; Riezler et al., 2002, for example). Cahill et al. (2007) and White and Rajkumar (2009) similarly show that ranking for Lexical Functional Grammar (LFG)- and Combinatory Categorial Grammar (CCG)-based systems respectively can be improved by models incorporating a LM and structural features. All of these rankers depend on multiple structures produced by the respective large-scale symbolic grammars to rank the output. For much smaller symbolic grammars, and those in the process of development — e.g. LFG grammars for Indonesian (Arka et al., 2009) or Arrernte (Dras et al., 2012), or HPSG grammars for Persian (Müller,"
C12-1111,rohrer-forst-2006-improving,0,0.0224082,"hill et al. (2007) drew on this to develop a log-linear model for realisation in an LFG context. They worked with a large-scale hand-crafted grammar of German, as they were investigating applicability of the approach to a language with freer word order than English, and generated from f-structures using the XLE system (Maxwell and Kaplan, 1993). They similarly constructed a symmetric treebank, and also had as their goal to re-generate the strings in the original treebank. For structural features in their model, the authors defined 186,731 instantiated templates based on Riezler et al. (2002), Rohrer and Forst (2006) and Riezler and Vasserman (2004); 1,471 of these actually occurred in their training data. The feature templates included information from both c-structure (e.g. simple features such as number of times a particular category label occurs, or compound features such as number of times it dominates another) and f-structure (e.g. frequency of relative order of subject and object), sentence length and LM scores. Structural features here contributed more strongly than for English. Which structural features are effective in realisation ranking is still an open question. For example, in the context of"
C12-1111,2005.mtsummit-papers.15,0,0.0206162,"h may sound somewhat marked or odd out of context, and some of which may be (due to system limitations) incorrect. This observation led to the idea of taking the possible outputs and ranking them, in the earliest work by use of a language model (LM) (Langkilde-Geary, 2000) and later by injecting new features like headwords, constituent category and Part of Speech (POS) tags (Langkilde-Geary, 2002). This idea was developed further for handcrafted symbolic grammars based on linguistic formalisms that have been used for generation. In the Head-driven Phrase Structure Grammar (HPSG) system LOGON, Velldal and Oepen (2005) improve on this by using a maximum entropy model incorporating both a LM and features derived from HPSG structures for the candidate outputs; this drew on earlier developments in the field of statistical parsing that used such features for reranking (Johnson et al., 1999; Riezler et al., 2002, for example). Cahill et al. (2007) and White and Rajkumar (2009) similarly show that ranking for Lexical Functional Grammar (LFG)- and Combinatory Categorial Grammar (CCG)-based systems respectively can be improved by models incorporating a LM and structural features. All of these rankers depend on mult"
C12-1111,W06-1661,0,0.15029,"s analyseurs supervisés ont été utilisés à cet effet. Cependant, pour les systèmes basés sur des grammaires symboliques de modeste envergure ou en développement, trop peu de données sont disponibles, et les modèles existants s’appliquent mal. Dans cet article, nous étudions donc la possibilité d’utiliser un analyseur statistique non supervisé (Naseem et al., 2010) comme source alternative d’information pour la sélection automatique des textes générés, malgré la faible qualité des structures qu’ils produisent. Nos travaux sont basés notamment sur ceux de Cahill et al. (2007), qui à la suite de Velldal and Oepen (2006), ont développé un modèle log-linéaire de réalisation pour une grammaire Lexical Functional Grammar (LFG). Comme eux, nous utilisons la plateforme Xerox Linguistics Environment (XLE) et construisons une banque d’arbres symétrique en analysant puis en regénérant les phrases. Ceci nous fournit à la fois des exemples positifs et négatifs. Nous utilisons les sections 2 à 21 du Penn Treebank (38 008 phrases) pour l’entraînement et la section 23 (2245 phrases) pour l’évaluation. Nous les avons analysées en utilisant la grammaire ParGram de l’anglais sur XLE (Butt et al., 2002), puis nous les avons r"
C12-1111,I11-1140,0,0.0223407,"Missing"
C12-1111,D09-1043,0,0.0210069,"Speech (POS) tags (Langkilde-Geary, 2002). This idea was developed further for handcrafted symbolic grammars based on linguistic formalisms that have been used for generation. In the Head-driven Phrase Structure Grammar (HPSG) system LOGON, Velldal and Oepen (2005) improve on this by using a maximum entropy model incorporating both a LM and features derived from HPSG structures for the candidate outputs; this drew on earlier developments in the field of statistical parsing that used such features for reranking (Johnson et al., 1999; Riezler et al., 2002, for example). Cahill et al. (2007) and White and Rajkumar (2009) similarly show that ranking for Lexical Functional Grammar (LFG)- and Combinatory Categorial Grammar (CCG)-based systems respectively can be improved by models incorporating a LM and structural features. All of these rankers depend on multiple structures produced by the respective large-scale symbolic grammars to rank the output. For much smaller symbolic grammars, and those in the process of development — e.g. LFG grammars for Indonesian (Arka et al., 2009) or Arrernte (Dras et al., 2012), or HPSG grammars for Persian (Müller, 2010) or Wambaya (Bender, 2008) — these multiple structures will"
C12-1111,D12-1023,0,0.0119381,"such as number of times a particular category label occurs, or compound features such as number of times it dominates another) and f-structure (e.g. frequency of relative order of subject and object), sentence length and LM scores. Structural features here contributed more strongly than for English. Which structural features are effective in realisation ranking is still an open question. For example, in the context of CCG, Rajkumar and White (2010) found that features reflecting animacy agreement between nouns and relative clauses, and number agreement between subject and verb, were helpful. White and Rajkumar (2012) found that explicitly representing dependency lengths led to shorter average dependencies, in line with psycholinguistic evidence for human-produced sentences, and to better generated text. And Filippova and Strube (2009) found that while trigram LMs are appropriate for phrase linearisation in German, at a clause level (longer) dependency features produced better results. Parser-based Ranking Charniak (2001) proposed the idea of using statistical parsers as a kind of structural language model. The idea has been applied a number of times in MT, for example by Charniak et al. (2003) or Post and"
C12-1111,zeman-etal-2012-hamledt,0,0.0220979,"Missing"
D08-1057,N04-1015,0,0.199313,"ith [host families]2 in the districts; A total or partial destruction of over 3,000 homes in Dili affecting at least 14,000 IDPs Figure 2: Examples of the pattern hDisplacedPersons[1], HostingCommunities[2]i. ure 2 showed that mentions of the plight of internationally displaced persons are often followed by descriptions of the impact on the host communities that look after them. In this particular example, this is realised lexically in the co-occurrences of the words displaced and host. Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. However, to our knowledge, using word co-occurrences in this manner to represent schematic knowledge for the purposes of selecting content in a statistically-generated summary sentence has not previously been explored. This paper seeks to determine whether or not such patterns exist in homogeneous data; and furthermore, whether such patterns can be used to better select words from auxiliary sentences. In particular, we propose the “Seed and Grow” approach for this task. The results show that even simple modelling"
D08-1057,J05-3002,0,0.176208,"mations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be realised in the summary sentence, identified by finding the common elements in the input sentences. We see this as an example of conservation. In our work, this general proposition is equivalent to the core information for the summary sentence before the incorporation of supplementary material. In contrast to both compression and conservation work, we focus on augmenting th"
D08-1057,J96-1002,0,0.0370589,"Missing"
D08-1057,D07-1001,0,0.0182264,"nformation. Roughly, text-to-text transformations fall into three categories: those in which information is compressed, conserved, and augmented. We use these distinctions to organise this overview of the literature. In Sentence Compression work, a single sentence undergoes pruning to shorten its length. Previous approaches have focused on statistical syntactic transformations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be r"
D08-1057,P02-1057,0,0.0720033,"Missing"
D08-1057,J05-4004,0,0.0500513,"Missing"
D08-1057,J95-2003,0,0.0080821,"ns fall into three categories: those in which information is compressed, conserved, and augmented. We use these distinctions to organise this overview of the literature. In Sentence Compression work, a single sentence undergoes pruning to shorten its length. Previous approaches have focused on statistical syntactic transformations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be realised in the summary sentence, identifie"
D08-1057,P03-1069,0,0.0983791,"ing in camps and with [host families]2 in the districts; A total or partial destruction of over 3,000 homes in Dili affecting at least 14,000 IDPs Figure 2: Examples of the pattern hDisplacedPersons[1], HostingCommunities[2]i. ure 2 showed that mentions of the plight of internationally displaced persons are often followed by descriptions of the impact on the host communities that look after them. In this particular example, this is realised lexically in the co-occurrences of the words displaced and host. Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. However, to our knowledge, using word co-occurrences in this manner to represent schematic knowledge for the purposes of selecting content in a statistically-generated summary sentence has not previously been explored. This paper seeks to determine whether or not such patterns exist in homogeneous data; and furthermore, whether such patterns can be used to better select words from auxiliary sentences. In particular, we propose the “Seed and Grow” approach for this task. The results show"
D08-1057,N03-1020,0,0.12957,"e what words were actually chosen in the summary sentence of the aligned sentence tuple. We are specifically interested in open-class words, and so a stopword list of closed-class words is used to filter the sentences in each test case. We evaluate against the set of open-class words in the human-authored summary sentence using recall and precision metrics. Recall is the size of the intersection of the selected and gold-standard sets, normalised by the length of the gold-standard sentence (in words). This recall metric is similar to the ROUGE-1 metric, the unigram version of the ROUGE metric (Lin and Hovy, 2003) used in the Document Understanding Conferences2 (DUC). Precision is the size of the intersection normalised by the number of words selected. We also report the F-measure, which is the harmonic mean of the recall and precision scores. Recall, precision and F-measure are measured at various values of n ranging from 1 to the number of open-class words in the gold-standard summary sentence for a particular test case. For the purposes of evaluation, differences in tokens due to morphology were explored crudely via the use of Porter’s stemming algorithm. However, the results from stemming are not t"
D08-1057,J91-1002,0,0.0540133,"which information is compressed, conserved, and augmented. We use these distinctions to organise this overview of the literature. In Sentence Compression work, a single sentence undergoes pruning to shorten its length. Previous approaches have focused on statistical syntactic transformations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be realised in the summary sentence, identified by finding the common elements in the inpu"
D08-1057,P05-1009,0,0.0682517,"Missing"
D08-1057,P08-2033,1,0.823543,"least one auxiliary sentence). Of the 580 cases, 50 cases were set aside for testing. The remaining 530 cases were used for training. Statistics for the training portion of the sentence augmentation set are provided in Table 1. In this paper, aligned sentence tuples are obtained via manual annotation. Automatic construction of these sentence-level alignments is possible and has been explored by Jing and McKeown (1999). We also envisage using tools for scoring sentence similarity (for example, see Hatzivassiloglou et al. (2001)) for automatically constructing them; this is the focus of work by Wan and Paris (2008). 3 http://ochaonline3.un.org/humanitarianappeal/index.htm 549 5.2 The Baselines Three baselines were used in this work: the random, tf-idf and position baselines. A random word selector shows what performance might be achieved in the absence of any linguistic knowledge. We also sorted all words in the aligned source sentences by their weighted tf-idf scores. This baseline selects words in order until the desired word limit is reached. This baseline is referred to as the tf-idf baseline. Finally, we selected words based on their sentence order, choosing first those words from the key sentence."
D08-1057,I05-5012,1,0.846344,"Missing"
D11-1148,P05-1022,0,0.237029,"so we do not present results for the others. Bi-normal separation (Forman, 2003), often competitive with IG, is only suitable for binary classification. It is worth noting that the production rules being used here are all non-lexicalised ones, except those lexicalised with function words and punctuation, to avoid topic-related clues. Reranking Features As opposed to the horizontal parse production rules, features used for discriminative reranking are cross-sections of parse trees that might capture other aspects of ungrammatical structures. For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al. (1999) and Collins (2000). Examples of these feature schemas include tuples covering head-to-head dependencies, preterminals together with their closest maximal projection ancestors, and subtrees rooted in the least common ancestor. These feature schemas are not the only possible ones—they were empirically selected for the specific purpose of augmenting the Charniak parser. However, much subsequent work has tended to use these same features, albeit sometimes with extensions for specific purpos"
D11-1148,J07-4004,0,0.0182728,"ins (2000). Examples of these feature schemas include tuples covering head-to-head dependencies, preterminals together with their closest maximal projection ancestors, and subtrees rooted in the least common ancestor. These feature schemas are not the only possible ones—they were empirically selected for the specific purpose of augmenting the Charniak parser. However, much subsequent work has tended to use these same features, albeit sometimes with extensions for specific purposes (e.g. Johnson and Ural (2010) for the Berkeley parser (Petrov et al., 2006), Ng et al. (2010) for the C&C parser (Clark and Curran, 2007)). We also use this standard set, specifically the set of instantiated feature schemas from the parser from Charniak and Johnson (2005) as trained on the Wall Street Journal (WSJ), which gives 1,333,837 potential features. 4 4.1 Experimental Setup Data We use the International Corpus of Learner English (ICLE) compiled by Granger et al. (2009) for the precise purpose of studying the English writings of non-native English learners from diverse countries. All the contributors to the corpus are claimed to possess similar English proficiency levels (ranging from intermediate to advanced learners) a"
D11-1148,P08-2056,0,0.0483568,"Missing"
D11-1148,C04-1088,0,0.0405566,"Missing"
D11-1148,N10-1095,0,0.0110123,"ere inspired by earlier work in discriminative estimation techniques, such as Johnson et al. (1999) and Collins (2000). Examples of these feature schemas include tuples covering head-to-head dependencies, preterminals together with their closest maximal projection ancestors, and subtrees rooted in the least common ancestor. These feature schemas are not the only possible ones—they were empirically selected for the specific purpose of augmenting the Charniak parser. However, much subsequent work has tended to use these same features, albeit sometimes with extensions for specific purposes (e.g. Johnson and Ural (2010) for the Berkeley parser (Petrov et al., 2006), Ng et al. (2010) for the C&C parser (Clark and Curran, 2007)). We also use this standard set, specifically the set of instantiated feature schemas from the parser from Charniak and Johnson (2005) as trained on the Wall Street Journal (WSJ), which gives 1,333,837 potential features. 4 4.1 Experimental Setup Data We use the International Corpus of Learner English (ICLE) compiled by Granger et al. (2009) for the precise purpose of studying the English writings of non-native English learners from diverse countries. All the contributors to the corpus"
D11-1148,P99-1069,0,0.0603706,"ly suitable for binary classification. It is worth noting that the production rules being used here are all non-lexicalised ones, except those lexicalised with function words and punctuation, to avoid topic-related clues. Reranking Features As opposed to the horizontal parse production rules, features used for discriminative reranking are cross-sections of parse trees that might capture other aspects of ungrammatical structures. For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al. (1999) and Collins (2000). Examples of these feature schemas include tuples covering head-to-head dependencies, preterminals together with their closest maximal projection ancestors, and subtrees rooted in the least common ancestor. These feature schemas are not the only possible ones—they were empirically selected for the specific purpose of augmenting the Charniak parser. However, much subsequent work has tended to use these same features, albeit sometimes with extensions for specific purposes (e.g. Johnson and Ural (2010) for the Berkeley parser (Petrov et al., 2006), Ng et al. (2010) for the C&C"
D11-1148,P03-1054,0,0.020162,"the present study, we have 95 essays per native language. For the same reason as highlighted by Wong and Dras (2009), we intentionally use fewer essays as compared to Koppel et al. (2005)3 with a view to reserving more data for future work. We divide these into training sets of 70 essays per lan3 Koppel et al. (2005) took all 258 texts per language from ICLE Version 1 and evaluated using 10-fold cross valiadation. 1604 guage, with a held-out test set of 25 essays per language. There are 17,718 training sentences and 6,791 testing sentences. 4.2 Parsers We use two parsers: the Stanford parser (Klein and Manning, 2003) and the Charniak and Johnson (henceforth C&J) parser (Charniak and Johnson, 2005). Both are widely used, and produce relatively accurate parses: the Stanford parser gets a labelled f-score of 85.61 on the WSJ, and the C&J 91.09. With the Stanford parser, there are 26,284 unique parse production rules extractable from our ICLE training set of 490 texts, while the C&J parser produces 27,705. For reranking, we use only the C&J parser—since the parser stores these features during parsing, we can use them directly as classification features. On the ICLE training data, there are 6,230 features with"
D11-1148,P07-1044,1,0.88276,"Missing"
D11-1148,U10-1014,0,0.0147112,"uch as Johnson et al. (1999) and Collins (2000). Examples of these feature schemas include tuples covering head-to-head dependencies, preterminals together with their closest maximal projection ancestors, and subtrees rooted in the least common ancestor. These feature schemas are not the only possible ones—they were empirically selected for the specific purpose of augmenting the Charniak parser. However, much subsequent work has tended to use these same features, albeit sometimes with extensions for specific purposes (e.g. Johnson and Ural (2010) for the Berkeley parser (Petrov et al., 2006), Ng et al. (2010) for the C&C parser (Clark and Curran, 2007)). We also use this standard set, specifically the set of instantiated feature schemas from the parser from Charniak and Johnson (2005) as trained on the Wall Street Journal (WSJ), which gives 1,333,837 potential features. 4 4.1 Experimental Setup Data We use the International Corpus of Learner English (ICLE) compiled by Granger et al. (2009) for the precise purpose of studying the English writings of non-native English learners from diverse countries. All the contributors to the corpus are claimed to possess similar English proficiency levels (rangi"
D11-1148,P06-1055,0,0.0138347,"timation techniques, such as Johnson et al. (1999) and Collins (2000). Examples of these feature schemas include tuples covering head-to-head dependencies, preterminals together with their closest maximal projection ancestors, and subtrees rooted in the least common ancestor. These feature schemas are not the only possible ones—they were empirically selected for the specific purpose of augmenting the Charniak parser. However, much subsequent work has tended to use these same features, albeit sometimes with extensions for specific purposes (e.g. Johnson and Ural (2010) for the Berkeley parser (Petrov et al., 2006), Ng et al. (2010) for the C&C parser (Clark and Curran, 2007)). We also use this standard set, specifically the set of instantiated feature schemas from the parser from Charniak and Johnson (2005) as trained on the Wall Street Journal (WSJ), which gives 1,333,837 potential features. 4 4.1 Experimental Setup Data We use the International Corpus of Learner English (ICLE) compiled by Granger et al. (2009) for the precise purpose of studying the English writings of non-native English learners from diverse countries. All the contributors to the corpus are claimed to possess similar English profici"
D11-1148,P07-1011,0,0.0227101,"Missing"
D11-1148,P10-2065,0,0.0441638,"Missing"
D11-1148,W07-0602,0,0.744784,"ternet users into giving away confidential details. One class of countermeasures to phishing consists of technical methods such as email authentication; another looks at profiling of the text’s author(s) (Fette et al., 2007; Zheng et al., 2003), to find any indications of the source of the text. In this paper we investigate classification of a text with respect to an author’s native language, where this is not the language that that text is written in (which is often the case in phishing); we refer to this as native language identification. Initial work by Koppel et al. (2005) was followed by Tsur and Rappoport (2007), Estival et al. (2007), van Halteren (2008), and Wong and Dras (2009). By and large, the problem was tackled using various supervised machine learning approaches, with mostly lexical features over characters, words, and parts of speech, as well as some document structure. Syntactic features, in contrast, in particular those that capture grammatical errors, which might potentially be useful for this task, have received little attention. Koppel et al. (2005) did suggest using syntactic errors in their work but did not investigate them in any detail. Wong and Dras (2009) noted the relevance of t"
D11-1148,C08-1118,0,0.10534,"Missing"
D11-1148,U09-1008,1,0.687463,"sures to phishing consists of technical methods such as email authentication; another looks at profiling of the text’s author(s) (Fette et al., 2007; Zheng et al., 2003), to find any indications of the source of the text. In this paper we investigate classification of a text with respect to an author’s native language, where this is not the language that that text is written in (which is often the case in phishing); we refer to this as native language identification. Initial work by Koppel et al. (2005) was followed by Tsur and Rappoport (2007), Estival et al. (2007), van Halteren (2008), and Wong and Dras (2009). By and large, the problem was tackled using various supervised machine learning approaches, with mostly lexical features over characters, words, and parts of speech, as well as some document structure. Syntactic features, in contrast, in particular those that capture grammatical errors, which might potentially be useful for this task, have received little attention. Koppel et al. (2005) did suggest using syntactic errors in their work but did not investigate them in any detail. Wong and Dras (2009) noted the relevance of the concept of contrastive analysis (Lado, 1957), which postulates that"
D11-1148,U10-1011,1,0.908945,"Missing"
D12-1064,D11-1131,1,0.787979,"Missing"
D12-1064,P05-1022,1,0.597221,"Missing"
D12-1064,D10-1028,0,0.104201,"ul in NLI (Koppel et al., 2005; Tsur and Rappoport, 2007; Estival et al., 2007). The recent work of Wong and Dras (2011), motivated by ideas from Second Language Acquisition (SLA), has shown that syntactic features — potentially capturing syntactic erAdaptor grammars (Johnson, 2010), a hierarchical non-parametric extension of PCFGs (and also interpretable as an extension of LDA-based topic models), hold out some promise here. In that initial work, Johnson’s model learnt collocations of arbitrary length such as gradient descent and cost function, under a topic associated with machine learning. Hardisty et al. (2010) applied this idea to perspective classification, learning collocations such as palestinian violence and palestinian freedom, the use of which as features was demonstrated to help the classification of texts from the Bitter Lemons corpus as either Palestinian or Israeli perspective. Typically in NLI and other authorship attribution tasks, the feature sets exclude content words, to avoid unfair cues due to potentially different domains of discourse. In our context, then, what we are interested in are ‘quasi-syntactic collocations’ of either pure PoS (e.g. NN IN NN) or a mixture of PoS with func"
D12-1064,P10-1117,1,0.882795,"second language — native language identification (NLI) — has, since the seminal work of Koppel et al. (2005), been primarily tackled as a text classification task using supervised machine learning techniques. Lexical features, such as function words, character n-grams, and part-ofspeech (PoS) n-grams, have been proven to be useful in NLI (Koppel et al., 2005; Tsur and Rappoport, 2007; Estival et al., 2007). The recent work of Wong and Dras (2011), motivated by ideas from Second Language Acquisition (SLA), has shown that syntactic features — potentially capturing syntactic erAdaptor grammars (Johnson, 2010), a hierarchical non-parametric extension of PCFGs (and also interpretable as an extension of LDA-based topic models), hold out some promise here. In that initial work, Johnson’s model learnt collocations of arbitrary length such as gradient descent and cost function, under a topic associated with machine learning. Hardisty et al. (2010) applied this idea to perspective classification, learning collocations such as palestinian violence and palestinian freedom, the use of which as features was demonstrated to help the classification of texts from the Bitter Lemons corpus as either Palestinian o"
D12-1064,D07-1072,0,0.0243615,"Missing"
D12-1064,W07-0602,0,0.340293,"just lead to feature sparsity problems, but also computational efficiency issues. Some form of feature selection should then come into play. Introduction The task of inferring the native language of an author based on texts written in a second language — native language identification (NLI) — has, since the seminal work of Koppel et al. (2005), been primarily tackled as a text classification task using supervised machine learning techniques. Lexical features, such as function words, character n-grams, and part-ofspeech (PoS) n-grams, have been proven to be useful in NLI (Koppel et al., 2005; Tsur and Rappoport, 2007; Estival et al., 2007). The recent work of Wong and Dras (2011), motivated by ideas from Second Language Acquisition (SLA), has shown that syntactic features — potentially capturing syntactic erAdaptor grammars (Johnson, 2010), a hierarchical non-parametric extension of PCFGs (and also interpretable as an extension of LDA-based topic models), hold out some promise here. In that initial work, Johnson’s model learnt collocations of arbitrary length such as gradient descent and cost function, under a topic associated with machine learning. Hardisty et al. (2010) applied this idea to perspective"
D12-1064,C08-1118,0,0.152166,"Missing"
D12-1064,D11-1148,1,0.326815,"ciency issues. Some form of feature selection should then come into play. Introduction The task of inferring the native language of an author based on texts written in a second language — native language identification (NLI) — has, since the seminal work of Koppel et al. (2005), been primarily tackled as a text classification task using supervised machine learning techniques. Lexical features, such as function words, character n-grams, and part-ofspeech (PoS) n-grams, have been proven to be useful in NLI (Koppel et al., 2005; Tsur and Rappoport, 2007; Estival et al., 2007). The recent work of Wong and Dras (2011), motivated by ideas from Second Language Acquisition (SLA), has shown that syntactic features — potentially capturing syntactic erAdaptor grammars (Johnson, 2010), a hierarchical non-parametric extension of PCFGs (and also interpretable as an extension of LDA-based topic models), hold out some promise here. In that initial work, Johnson’s model learnt collocations of arbitrary length such as gradient descent and cost function, under a topic associated with machine learning. Hardisty et al. (2010) applied this idea to perspective classification, learning collocations such as palestinian violen"
D12-1064,U11-1015,1,0.926022,"r, when character n-grams are used as features, some special characters used in some ICLE texts might affect performance. For our case, this should not be of much issue since they will not appear in our collocations. the grammar of Johnson (2010) as presented in Section 2.2.2, except that the vocabulary differs: either w ∈ Vpos or w ∈ Vpos+f w . For Vpos , there are 119 distinct PoS tags based on the Brown tagset. Vpos+f w is extended with 398 function words as per Wong and Dras (2011). m = 490 is the number of documents, and t = 25 the number of topics (chosen as the best performing one from Wong et al. (2011)). Rules of the form Docj → Docj Topici that encode the possible topics that are associated with a document j are given similar α priors as used in LDA (α = 5/t where t = 25 in our experiments). Likewise, similar β priors from LDA are placed on the adapted rules expanding from Topici → Words, representing the possible sequences of words that each topic comprises (β = 0.01).3 The inference algorithm for the adaptor grammars are based on the Markov Chain Monte Carlo technique made available online by Johnson (2010).4 3.1.3 Classification models with n-gram features Based on the two adaptor gramm"
D14-1144,de-marneffe-etal-2006-generating,0,0.00289728,"Missing"
D14-1144,W14-3625,1,0.761165,"work also has a backwards link in this regard by providing qualitative evidence about the underpinning linguistic theories that make NLI work. 1388 The work presented here has a number of applications; chief among them is the development of tools for SLA researchers. This would enable them to not just provide new evidence for previous findings, but to also perform semi-automated data-driven generation of new and viable hypotheses. This, in turn, can help reduce expert effort and involvement in the process, particularly as such studies expand to more corpora and emerging language like Chinese (Malmasi and Dras, 2014b) and Arabic (Malmasi and Dras, 2014a). The brief analysis included here represents only a tiny portion of what can be achieved with this methodology. We included but a few of the thousands of features revealed by this method; practical SLA tools based on this would have a great impact on current research. In addition to language transfer hypotheses, such systems could also be applied to aid development of pedagogical material within a needsbased and data-driven approach. Once language use patterns are uncovered, they can be assessed for teachability and used to create tailored, L1specific ex"
D14-1144,E14-4019,1,0.747522,"work also has a backwards link in this regard by providing qualitative evidence about the underpinning linguistic theories that make NLI work. 1388 The work presented here has a number of applications; chief among them is the development of tools for SLA researchers. This would enable them to not just provide new evidence for previous findings, but to also perform semi-automated data-driven generation of new and viable hypotheses. This, in turn, can help reduce expert effort and involvement in the process, particularly as such studies expand to more corpora and emerging language like Chinese (Malmasi and Dras, 2014b) and Arabic (Malmasi and Dras, 2014a). The brief analysis included here represents only a tiny portion of what can be achieved with this methodology. We included but a few of the thousands of features revealed by this method; practical SLA tools based on this would have a great impact on current research. In addition to language transfer hypotheses, such systems could also be applied to aid development of pedagogical material within a needsbased and data-driven approach. Once language use patterns are uncovered, they can be assessed for teachability and used to create tailored, L1specific ex"
D14-1144,W13-1716,1,0.667772,"irst and second. One possibility is that this relates to argumentation styles that are possibly influenced by cultural norms. More broadly, this effect could also be teaching rather than transfer related. For example, it may be case that a widely-used text book for learning English in Korea happens to overuse this construction. Some recent findings from the 2013 NLI Shared Task found that L1 Hindi and Telugu learners of English had similar transfer effects and their writings were difficult to distinguish. It has been posited that this is likely due to shared culture and teaching environments (Malmasi et al., 2013). Despite some clearcut instances of overuse,9 more research is required to determine the causal factors. We hope to expand on this in future work using more data. 9 More than half of the Korean scripts contained a sentence-initial however. 5 Discussion and Conclusion Using the proposed methodology, we generated lists of linguistic features overused and underused by English learners of various L1 backgrounds. Through an analysis of the top items in these ranked lists, we demonstrated the high applicability of the output by formulating plausible language transfer hypotheses supported by current"
D14-1144,E14-4033,0,0.369465,"e and positive examples.2 More discriminative features have higher scores. Another alternative method is Information Gain (Yang and Pedersen, 1997). As defined in equation (2), it measures the entropy gain associated with feature t in assigning the class label c. G(t) = − + + Pm i=1 Pr (t) Pr (t¯) Pr (ci ) log Pr (ci ) Pm i=1 Pr (ci |t) log Pr (ci |t) i=1 Pr (ci |t¯) log Pr (ci |t¯) Pm (2) However, these methods are limited: they do not provide ranked lists per-L1 class, and more importantly, they do not explicitly capture underuse. Among the efflorescence of NLI work, a new trend explored by Swanson and Charniak (2014) aims to extract lists of candidate language transfer features by comparing L2 data against the writer’s L1 to find features where the L1 use is mirrored in L2 use. This allows the detection of obvious effects, but Jarvis and Crossley (2012) note (p. 183) that many transfer effects are “too complex” to observe in this manner. Moreover, this method is unable to detect underuse, is only suitable for syntactic features, and has only been applied to very small data (4,000 sentences) over three L1s. Addressing these issues is the focus of the present work. 3 Experimental Setup 3.1 Corpus Features A"
D14-1144,P10-1117,0,0.0103612,"r to discover arbitrary length n-gram collocations. We explore both the pure part-of-speech (POS) n-grams as 2 3 Sentence → Docj Docj → j Docj → Docj T opici T opici → W ords W ords → W ord W ords → W ords W ord W ord → w j ∈ 1, . . . , m j ∈ 1, . . . , m i ∈ 1, . . . , t; j ∈ 1, . . . , m i ∈ 1, . . . , t w ∈ Vpos ; w ∈ Vpos+f w Vpos contains 119 distinct POS tags based on the Brown tagset and Vpos+f w is extended with 398 function words. The number of topics t is set to 50. The inference algorithm for the adaptor grammars are based on the Markov Chain Monte Carlo technique made available by Johnson (2010).4 Stanford dependencies We use Stanford dependencies as a syntactic feature: for each text we extract all the basic dependencies returned by the Stanford Parser (de Marneffe et al., 2006). We then generate all the variations for each of the dependencies (grammatical relations) by substituting each lemma with its corresponding POS tag. For instance, a grammatical relation of det(knowledge, the) yields the following variations: det(NN, the), det(knowledge, DT), and det(NN, DT). Lexical features Content and function words are also considered as two feature types related to learner’s vocabulary a"
D14-1144,C12-1158,0,0.251498,"overuse, the extensive use of some linguistic structures, and underuse, the underutilization of particular structures, also known as avoidance (Gass and Selinker, 2008). While there have been some attempts in SLA to use computational approaches on small-scale data,1 these still use fairly elementary techniques and have several shortcomings, including in the manual approaches to annotation and the computational artefacts derived from these. Conversely, NLI work has focused on automatic learner L1 classification using Machine Learning with large-scale data and sophisticated linguistic features (Tetreault et al., 2012). Here, feature ranking could be performed with relevancy methods such as the F-score: 1 E.g. Chen (2013), Lozan´o and Mendikoetxea (2010) and Di´ez-Bedmar and Papp (2008). 1385 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1385–1390, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2  (−) ¯j − x ¯j + x   n+  n−  P P (+) (+) 2 (−) (−) 2 ¯j ¯j xi,j − x xi,j − x + n−1−1  F (j) ≡ 1 n+ −1 (+) ¯j x ¯j −x 2 i=1 (1) i=1 The F-score (Fisher score) measures the ratio between the intraclass and interclass v"
D14-1144,W13-1706,0,0.387579,"Missing"
D14-1144,W07-0602,0,0.345237,"007, for example) that the English spoken in India still retains characteristics of the English that was spoken during the time of the Raj and the East India Company that have disappeared from other English varieties, so it sounds more formal to other speakers, or retains traces of an archaic business correspondence style; the features noted fit that pattern. The second list includes content words overused by Arabic L1 learners. Analysis of content words here, and for other L1s in our data, reveals very frequent misspellings which are believed to be due to orthographic or phonetic influences (Tsur and Rappoport, 2007; Odlin, 1989). Since Arabic does not share orthography with English, we believe most of these are due to phonetics. Looking at items 1, 3 and 5 we can see a common pattern: the English letter u which has various phonetic realizations is being replaced by a vowel that more often represents that sound. Items 2 and 5 are also phonetically similar to the intended words. For Spanish L1 authors we provide both underuse and overuse lists of syntactic dependencies. The top 3 overuse rules show the word that is very often used as the subject of verbs. This is almost 6 certainly a consequence of the pr"
D14-1144,D12-1064,1,0.856292,"ate language transfer features by comparing L2 data against the writer’s L1 to find features where the L1 use is mirrored in L2 use. This allows the detection of obvious effects, but Jarvis and Crossley (2012) note (p. 183) that many transfer effects are “too complex” to observe in this manner. Moreover, this method is unable to detect underuse, is only suitable for syntactic features, and has only been applied to very small data (4,000 sentences) over three L1s. Addressing these issues is the focus of the present work. 3 Experimental Setup 3.1 Corpus Features Adaptor grammar collocations Per Wong et al. (2012), we utilize an adaptor grammar to discover arbitrary length n-gram collocations. We explore both the pure part-of-speech (POS) n-grams as 2 3 Sentence → Docj Docj → j Docj → Docj T opici T opici → W ords W ords → W ord W ords → W ords W ord W ord → w j ∈ 1, . . . , m j ∈ 1, . . . , m i ∈ 1, . . . , t; j ∈ 1, . . . , m i ∈ 1, . . . , t w ∈ Vpos ; w ∈ Vpos+f w Vpos contains 119 distinct POS tags based on the Brown tagset and Vpos+f w is extended with 398 function words. The number of topics t is set to 50. The inference algorithm for the adaptor grammars are based on the Markov Chain Monte Carl"
E09-1097,C00-1007,0,0.0278773,"reinserted. Since the LMO baseline reduces to bigram generation when concatenating single words, we test a second language model baseline which always uses a 4-gram window size. A Viterbi-like generator with a 4-gram model and a beam of 100 is used to generate a sequence. For this baseline, referred to as the Viterbi baseline, base noun phrases were separated into their constituent words and included in the input word set. 6 Related Work 6.1 Statistical Surface Realisers The work in this paper is similar to research in statistical surface realisation (for example, Langkilde and Knight (1998); Bangalore and Rambow (2000); Filippova and Strube (2008)). These start with a semantic representation for which a specific rendering, an ordering of words, must be determined, often using language models to govern tree traversal. The task in this paper is different as it is a text-to-text scenario and does not begin with a representation of semantics. 5.4 Results The results are presented in Table 2. Significance was measured using the sign test and the sampling method outlined in (Collins et al., 2005). We will examine the results in the PTB-LM column first. The gain of 10 BLEU points by the LMO baseline over the Viter"
E09-1097,P99-1071,0,0.00878955,"with a fixed set of semantic labels. Additionally, our end goal is a dependency tree that encodes word precedence order, bypassing the surface realisation stage. The dependency model and the LMO linearisation algorithm are based heavily on word order statistics. As such, the utility of this approach is limited to human languages with minimal use of inflections, such as English. Approaches for other language types, for example German, have been explored (Filippova and Strube, 2007). 6.2 Text-to-Text Generation As a text-to-text approach, our work is more similar to work on Information Fusion (Barzilay et al., 1999), a sub-problem in multi-document summarisation. In this work, sentences presenting the same information, for example multiple news articles describing the same event, are merged to form a single summary by aligning repeated words and phrases across sentences. Other text-to-text approaches for generating novel sentences also aim to recycle sentence fragments where possible, as we do. Work on phrasebased statistical machine translation has been applied to paraphrase generation (Bannard and Callison-Burch, 2005) and multi-sentence alignment in summarisation (Daum´e III and Marcu, 2004). These ap"
E09-1097,W07-2216,0,0.0153048,"ely if called on an 1 Adapted from (McDonald et al., 2005) and . The difference concerns the direction of the edge and the edge weight function. We have also folded the function ‘contract’ in McDonald et al. (2005) into the main algorithm. Again following that work, we treat the function s as a data structure permitting storage of updated edge weights. probLM O (w0 . . . wj ) http://www.ce.rit.edu/˜ sjyeec/dmst.html = j−k−1 Y probM LE (wi+k |wi . . . wi+k−1 ) i=0 (3) 854 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 surrounding tree context. This makes the search for an optimal tree an NP-hard problem (McDonald and Satta, 2007) as all possible trees must be considered to find an optimal solution. Consequently, we must choose a heuristic search algorithm for finding the locally optimum spanning tree. By representing argument positions that can be filled only once, we allow modifiers to compete for argument positions and vice versa. The CLE algorithm only considers this competition in one direction. In line 3 of Algorithm 1, only heads compete for modifiers, and thus the solution will be sub-optimal. In Wan et al. (2007), we showed that introducing a model of argument positions into a greedy spanning tree algorithm ha"
E09-1097,H05-1066,0,0.586947,". This is not surprising as these methods are unable to model grammaticality at the sentence level, unless the size of n is sufficiently large. In practice, the lack of sufficient training data means that n is often smaller than the average sentence length. Even if data exists, increasing the size of n corresponds to a higher degree polynomial complexity search for the best word sequence. In response, we introduce an algorithm for searching for the best word sequence in a way that attempts to model grammaticality at the sentence level. Mirroring the use of spanning tree algorithms in parsing (McDonald et al., 2005), we present an approach to statistical sentence generation. Given a set of scrambled words, the approach searches for the most probable dependency tree, as defined by some corpus, such that it contains each word of the input set. The tree is then traversed to obtain the final word ordering. In particular, we present two spanning tree algorithms. We first adapt the Chu-Liu-Edmonds (CLE) algorithm (see Chu and Liu (1965) and Edmonds (1967)), used in McDonald et al. (2005), to include a basic argument model, added to keep track of linear precedence between heads and modifiers. While our adapted"
E09-1097,P07-1044,1,0.690541,"surement of grammaticality, we use the string regeneration task. Beginning with a human-authored sentence with its word order randomised, the goal is to regenerate the original sentence. Success is indicated by the proportion of the original sentence regenerated, as measured by any string comparison method: in our case, using the BLEU metric (Papineni et al., 2002). One benefit to this evaluation is that content selection, as a factor, is held constant. Specifically, the probability of word selection is uniform for all words. 3 Alternative grammaticality measures have been developed recently (Mutton et al., 2007). We are currently exploring the use of this and other metrics. 4 This would correspond to the use of a chunking algorithm or a named-entity recogniser to find noun phrases that could be re-used for sentence generation. 857 Algorithms Viterbi baseline LMO baseline CLE AB PTB-LM 14.9 24.3 26.4 33.6 AB: the dow at this point was down about 35 points CLE: was down about this point 35 points the dow at LMO: was this point about at down the down 35 points Viterbi: the down 35 points at was about this point down BLLIP-LM 18.0 26.0 26.8 33.7 Original: at this point, the dow was down about 35 points F"
E09-1097,P04-1030,0,0.0114709,"nal (WSJ) with human annotations of syntactic structures. Dependency events were sourced from the events file of the Collins parser package, which contains the dependency events found in training sections 2-22 of the corpus. Development was done on section 00 and testing was performed on section 23. A 4-gram language model (LM) was also obtained from the PTB training data, referred to as PTB-LM. Additionally, a 4-gram language model was obtained from a subsection of the BLLIP’99 Corpus (LDC number: LDC2000T43) containing three years of WSJ data from 1987 to 1989 (Charniak et al., 1999). As in Collins et al. (2004), the 1987 portion of the BLLIP corpus containing 20 million words was also used to create a language model, referred to here as BLLIP-LM. Ngram models were smoothed using Katz’s method, backing off to smaller values of n. For this evaluation, tokenisation was based on that provided by the PTB data set. This data set also delimits base noun phrases (noun phrases without nested constituents). Base noun phrases were treated as single tokens, and the rightmost word assumed to be the head. For the algorithms tested, the input set for any test case consisted of the single tokens identified by the P"
E09-1097,P05-1066,0,0.0320327,"this paper is similar to research in statistical surface realisation (for example, Langkilde and Knight (1998); Bangalore and Rambow (2000); Filippova and Strube (2008)). These start with a semantic representation for which a specific rendering, an ordering of words, must be determined, often using language models to govern tree traversal. The task in this paper is different as it is a text-to-text scenario and does not begin with a representation of semantics. 5.4 Results The results are presented in Table 2. Significance was measured using the sign test and the sampling method outlined in (Collins et al., 2005). We will examine the results in the PTB-LM column first. The gain of 10 BLEU points by the LMO baseline over the Viterbi baseline shows the performance improvement that can be gained when reinserting the base noun phrases. 858 structures whilst searching for the best word ordering. As a result, an argument model is needed to identify linguistically plausible spanning trees. We treated the alignment of modifiers to head words as a bipartite graph matching problem. This is similar to work in semantic role labelling by Pad´o and Lapata (2006). The alignment of answers to question types as a sema"
E09-1097,P06-1146,0,0.0168536,"Missing"
E09-1097,P96-1025,0,0.199273,"rs. Adopting an approach similar to Johnson (2007), we look at the direction (left or right) of the head with respect to the modifier; we consequently define a set D = {l, r} to represent this. Set D represents the linear precedence of the words in the dependency relation; consequently, it partially approximates the distinction between syntactic roles like subject and object. Each edge has a pair of associated weights, one for each direction, defined by the function s : E × D → R, based on a probabilistic model of dependency relations. To calculate the edge weights, we adapt the definition of Collins (1996) to use direction rather than relation type (represented in the original as triples of non-terminals). Given a corpus, for some edge e = (u, v) ∈ E and direction d ∈ D, we calculate the edge weight as: not always correspond with a linguistically valid dependency tree, primarily because it does not attempt to ensure that words in the tree have plausible numbers of arguments. We propose an alternative dependencyspanning tree algorithm which uses a more fine-grained argument model representing argument positions. To find the best modifiers for argument positions, we treat the attachment of edges"
E09-1097,P02-1040,0,0.0950057,"density up to a fixed maximum, in this case 7 argument positions, and assume zero probability beyond that. 5 Evaluation 5.1 String Generation Task The best-performing word ordering algorithm is one that makes fewest grammatical errors. As a surrogate measurement of grammaticality, we use the string regeneration task. Beginning with a human-authored sentence with its word order randomised, the goal is to regenerate the original sentence. Success is indicated by the proportion of the original sentence regenerated, as measured by any string comparison method: in our case, using the BLEU metric (Papineni et al., 2002). One benefit to this evaluation is that content selection, as a factor, is held constant. Specifically, the probability of word selection is uniform for all words. 3 Alternative grammaticality measures have been developed recently (Mutton et al., 2007). We are currently exploring the use of this and other metrics. 4 This would correspond to the use of a chunking algorithm or a named-entity recogniser to find noun phrases that could be re-used for sentence generation. 857 Algorithms Viterbi baseline LMO baseline CLE AB PTB-LM 14.9 24.3 26.4 33.6 AB: the dow at this point was down about 35 poin"
E09-1097,W04-3216,0,0.0625091,"Missing"
E09-1097,D07-1002,0,0.0191291,"ain of 10 BLEU points by the LMO baseline over the Viterbi baseline shows the performance improvement that can be gained when reinserting the base noun phrases. 858 structures whilst searching for the best word ordering. As a result, an argument model is needed to identify linguistically plausible spanning trees. We treated the alignment of modifiers to head words as a bipartite graph matching problem. This is similar to work in semantic role labelling by Pad´o and Lapata (2006). The alignment of answers to question types as a semantic role labelling task using similar methods was explored by Shen and Lapata (2007). Our work is also strongly related to that of Wong and Mooney (2007) which constructs symbolic semantic structures via an assignment process in order to provide surface realisers with input. Our approach differs in that we do not begin with a fixed set of semantic labels. Additionally, our end goal is a dependency tree that encodes word precedence order, bypassing the surface realisation stage. The dependency model and the LMO linearisation algorithm are based heavily on word order statistics. As such, the utility of this approach is limited to human languages with minimal use of inflections,"
E09-1097,P05-1009,0,0.127674,"ame information, for example multiple news articles describing the same event, are merged to form a single summary by aligning repeated words and phrases across sentences. Other text-to-text approaches for generating novel sentences also aim to recycle sentence fragments where possible, as we do. Work on phrasebased statistical machine translation has been applied to paraphrase generation (Bannard and Callison-Burch, 2005) and multi-sentence alignment in summarisation (Daum´e III and Marcu, 2004). These approaches typically use n-gram models to find the best word sequence. The WIDL formalism (Soricut and Marcu, 2005) was proposed to efficiently encode constraints that restricted possible word sequences, for example dependency information. Though similar, our work here does not explicitly represent the word lattice. For these text-to-text systems, the order of elements in the generated sentence is heavily based on the original order of words and phrases in the input sentences from which lattices are built. Our approach has the benefit of considering all possible orderings of words, corresponding to a wider range of paraphrases, provided with a suitable dependency model is available. 7 Conclusions In this p"
E09-1097,P07-1041,0,0.0565153,"semantic structures via an assignment process in order to provide surface realisers with input. Our approach differs in that we do not begin with a fixed set of semantic labels. Additionally, our end goal is a dependency tree that encodes word precedence order, bypassing the surface realisation stage. The dependency model and the LMO linearisation algorithm are based heavily on word order statistics. As such, the utility of this approach is limited to human languages with minimal use of inflections, such as English. Approaches for other language types, for example German, have been explored (Filippova and Strube, 2007). 6.2 Text-to-Text Generation As a text-to-text approach, our work is more similar to work on Information Fusion (Barzilay et al., 1999), a sub-problem in multi-document summarisation. In this work, sentences presenting the same information, for example multiple news articles describing the same event, are merged to form a single summary by aligning repeated words and phrases across sentences. Other text-to-text approaches for generating novel sentences also aim to recycle sentence fragments where possible, as we do. Work on phrasebased statistical machine translation has been applied to parap"
E09-1097,D08-1019,0,0.0364206,"eline reduces to bigram generation when concatenating single words, we test a second language model baseline which always uses a 4-gram window size. A Viterbi-like generator with a 4-gram model and a beam of 100 is used to generate a sequence. For this baseline, referred to as the Viterbi baseline, base noun phrases were separated into their constituent words and included in the input word set. 6 Related Work 6.1 Statistical Surface Realisers The work in this paper is similar to research in statistical surface realisation (for example, Langkilde and Knight (1998); Bangalore and Rambow (2000); Filippova and Strube (2008)). These start with a semantic representation for which a specific rendering, an ordering of words, must be determined, often using language models to govern tree traversal. The task in this paper is different as it is a text-to-text scenario and does not begin with a representation of semantics. 5.4 Results The results are presented in Table 2. Significance was measured using the sign test and the sampling method outlined in (Collins et al., 2005). We will examine the results in the PTB-LM column first. The gain of 10 BLEU points by the LMO baseline over the Viterbi baseline shows the perform"
E09-1097,P07-1022,0,0.0163055,"irected edges. For each sentence w = w1 . . . wn , we define the digraph Gw = (Vw , Ew ) where Vw = {w0 , w1 , . . . , wn }, with w0 a dummy root vertex, and Ew = {(u, v)|u ∈ Vw , v ∈ Vw  {w0 }}. The graph is fully connected (except for the root vertex w0 which is only fully connected outwards) and is a representation of possible dependencies. For an edge (u, v), we refer to u as the head and v as the modifier. We extend the original formulation of McDonald et al. (2005) by adding a notion of argument positions for a word, providing points to attach modifiers. Adopting an approach similar to Johnson (2007), we look at the direction (left or right) of the head with respect to the modifier; we consequently define a set D = {l, r} to represent this. Set D represents the linear precedence of the words in the dependency relation; consequently, it partially approximates the distinction between syntactic roles like subject and object. Each edge has a pair of associated weights, one for each direction, defined by the function s : E × D → R, based on a probabilistic model of dependency relations. To calculate the edge weights, we adapt the definition of Collins (1996) to use direction rather than relati"
E09-1097,N07-1022,0,0.0160194,"hows the performance improvement that can be gained when reinserting the base noun phrases. 858 structures whilst searching for the best word ordering. As a result, an argument model is needed to identify linguistically plausible spanning trees. We treated the alignment of modifiers to head words as a bipartite graph matching problem. This is similar to work in semantic role labelling by Pad´o and Lapata (2006). The alignment of answers to question types as a semantic role labelling task using similar methods was explored by Shen and Lapata (2007). Our work is also strongly related to that of Wong and Mooney (2007) which constructs symbolic semantic structures via an assignment process in order to provide surface realisers with input. Our approach differs in that we do not begin with a fixed set of semantic labels. Additionally, our end goal is a dependency tree that encodes word precedence order, bypassing the surface realisation stage. The dependency model and the LMO linearisation algorithm are based heavily on word order statistics. As such, the utility of this approach is limited to human languages with minimal use of inflections, such as English. Approaches for other language types, for example Ge"
E09-1097,W98-1426,0,\N,Missing
E09-1097,P05-1074,0,\N,Missing
E09-1097,W00-1401,0,\N,Missing
E14-4019,W13-1706,0,0.378307,"Missing"
E14-4019,C12-1025,0,0.340093,"Missing"
E14-4019,W07-0602,0,0.284166,"Missing"
E14-4019,C12-1027,0,0.150967,"age Acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages (Ortega, 2009). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. Rising numbers of language learners have led to an increasing need 2 Related Work NLI is a fairly recent, but rapidly growing area of research. While some research was conducted in the early 2000s, the most significant work has only appeared in the last few years (Wong and Dras, 2009; Wong and Dras, 2011; Swanson and Charniak, 2012; Tetreault et al., 2012; Bykh and Meurers, 2012). Most studies approach NLI as a multi-class supervised classification task. In this experimental design, the L1 metadata are used as class labels 95 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 95–99, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics and the individual writings are used as training and testing data. Using lexical and syntactic features of increasing sophistication, researchers have obtained good results under this paradigm. While a detailed exposition of NLI has been omi"
E14-4019,U09-1008,1,0.432497,"ue. This relates to Cross-Linguistic Influence (CLI), a key topic in the field of Second Language Acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages (Ortega, 2009). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. Rising numbers of language learners have led to an increasing need 2 Related Work NLI is a fairly recent, but rapidly growing area of research. While some research was conducted in the early 2000s, the most significant work has only appeared in the last few years (Wong and Dras, 2009; Wong and Dras, 2011; Swanson and Charniak, 2012; Tetreault et al., 2012; Bykh and Meurers, 2012). Most studies approach NLI as a multi-class supervised classification task. In this experimental design, the L1 metadata are used as class labels 95 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 95–99, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics and the individual writings are used as training and testing data. Using lexical and syntactic features of increasing sophistication, researche"
E14-4019,D11-1148,1,0.849433,"ross-Linguistic Influence (CLI), a key topic in the field of Second Language Acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages (Ortega, 2009). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. Rising numbers of language learners have led to an increasing need 2 Related Work NLI is a fairly recent, but rapidly growing area of research. While some research was conducted in the early 2000s, the most significant work has only appeared in the last few years (Wong and Dras, 2009; Wong and Dras, 2011; Swanson and Charniak, 2012; Tetreault et al., 2012; Bykh and Meurers, 2012). Most studies approach NLI as a multi-class supervised classification task. In this experimental design, the L1 metadata are used as class labels 95 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 95–99, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics and the individual writings are used as training and testing data. Using lexical and syntactic features of increasing sophistication, researchers have obtained good"
E14-4019,W13-1716,1,0.422025,"till relatively young and many fundamental questions have yet to be answered. All of the tested models are effective, and they appear to be complementary as combining them improves overall accuracy. We also note the difference in the efficacy of the feature representations and see a clear preference for frequency-based feature values. Others have found that binary features are the most effective for English NLI (Brooke and Hirst, 2012), but our results indicate frequency information is more informative in this task. The combination of both feature types has also been reported to be effective (Malmasi et al., 2013). To see how these models perform across languages, we also compare the results against the TOEFL11 corpus used in the NLI2013 shared task. We perform the same experiments on that dataset using the English CoreNLP models, Penn Treebank PoS tagset and a set of 400 English function words. Figure 2 shows the results side by side. Remarkably, we see that the model results closely mirror each other across corpora. This is a highly interesting finding from our study that merits further investigation. There is a systematic pattern occurring across data from learners of completely different L1-L2 pair"
E14-4019,P12-2038,0,0.0919377,"ence (CLI), a key topic in the field of Second Language Acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages (Ortega, 2009). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. Rising numbers of language learners have led to an increasing need 2 Related Work NLI is a fairly recent, but rapidly growing area of research. While some research was conducted in the early 2000s, the most significant work has only appeared in the last few years (Wong and Dras, 2009; Wong and Dras, 2011; Swanson and Charniak, 2012; Tetreault et al., 2012; Bykh and Meurers, 2012). Most studies approach NLI as a multi-class supervised classification task. In this experimental design, the L1 metadata are used as class labels 95 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 95–99, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics and the individual writings are used as training and testing data. Using lexical and syntactic features of increasing sophistication, researchers have obtained good results under this paradigm"
E14-4019,C12-1158,0,0.301215,"he field of Second Language Acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages (Ortega, 2009). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. Rising numbers of language learners have led to an increasing need 2 Related Work NLI is a fairly recent, but rapidly growing area of research. While some research was conducted in the early 2000s, the most significant work has only appeared in the last few years (Wong and Dras, 2009; Wong and Dras, 2011; Swanson and Charniak, 2012; Tetreault et al., 2012; Bykh and Meurers, 2012). Most studies approach NLI as a multi-class supervised classification task. In this experimental design, the L1 metadata are used as class labels 95 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 95–99, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics and the individual writings are used as training and testing data. Using lexical and syntactic features of increasing sophistication, researchers have obtained good results under this paradigm. While a detailed expos"
I05-5012,W98-1426,0,\N,Missing
I05-5012,C00-1007,0,\N,Missing
I05-5012,P98-1035,0,\N,Missing
I05-5012,C98-1035,0,\N,Missing
I05-5012,P96-1025,0,\N,Missing
I05-5012,P99-1071,0,\N,Missing
J15-2005,W11-2101,0,0.168157,"Missing"
J15-2005,W07-0718,0,0.573394,"proaches to evaluating these sorts of judgments, there is an entire body of work on this in the context of sensory discrimination testing and the human judgments that are central to it, backed by rigorous statistical theory and freely available software, that NLP can draw on. We investigate one approach, Log-Linear Bradley-Terry models, and apply it to sample NLP data. 1. Introduction Human evaluation is a key aspect of many NLP technologies. Automatic metrics that correlate with human judgments have been developed, especially in Machine Translation, to relieve some of the burden. Neverthess, Callison-Burch et al. (2007) note in their meta-evaluation that in MT they still “consider the human evaluation to be primary.” Whereas MT has traditionally used a Likert scale score for the criteria of adequacy and fluency, this meta-evaluation noted that these are “seemingly difficult things for judges to agree on”; consequently, asking judges to express a preference between alternative translations is increasingly used on the grounds of ease and intuitiveness. Further, where the major empirical results of a paper are from automatic metrics, it is still useful to supplement them: As two examples, Collins, Koehn, and Ku"
J15-2005,W08-0309,0,0.0717525,"Missing"
J15-2005,P05-1066,0,0.0153106,"ociated freely available R software, and it permits questions like the following to be asked: Can we say that the judges are expressing a preference at all, as opposed to no preference? Is there an effect from judge disagreement or inconsistency? ∗ Department of Computing, Macquarie University, NSW 2109, Australia. E-mail: mark.dras@mq.edu.au. Submission received: 10 April 2014; accepted for publication: 18 July 2014. doi:10.1162/COLI a 00222 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 Figure 1 (a) Results for two human judges (annotators) in Collins et al. (2005), Table 2. R gives counts corresponding to preferences for reordered translation system; B, preferences for baseline; E, no preference. (b) Results for human judges for baseline Moses versus cluster-based reranker in Lewis and Steedman (2013), Table 5. (c) Count data of pairwise preferences from Collins et al. (2005) and Lewis and Steedman (2013) plus artificial data. We describe our sample data (Section 2), sketch a classical non-parametric approach (Section 3) and discuss the issues that arise from this, and look at some of the approaches used in MT (Section 4). We then (Section 5) introduce"
J15-2005,P13-1139,0,0.63298,"erannotator agreement. Lopez (2012) extends the analysis of Bojar et al. and casts the problem as “finding the minimum feedback arc set in a tournament, a well-known NPcomplete problem.” He advocates using the pairwise rankings themselves, rather than aggregate statistics like Vilar et al. (2007), and aims to minimize the number of violations among these. Koehn (2012) evaluates empirically the approaches of both Bojar et al. (2011) and Lopez (2012), with a focus on determining which systems are statistically distinguishable in terms of performance, defining confidence bounds for this purpose. Hopkins and May (2013) recently advocated a focus on finding the extent to which particular rankings could be trusted. They proposed a model based on Item Response Theory (IRT), which underlies many standardized tests. They draw an analogy with judges assessing students on the basis of an underlying distribution of the student’s ability, with items authored by students having a quality drawn from the student’s ability distribution. They note in passing that a Gaussian parameterization of their IRT models resembles Thurstone and Bradley-Terry models; this leads us to the topic of Section 5. Overall, then, there are"
J15-2005,2012.iwslt-papers.5,0,0.253027,"testing. However, in aggregating this way information about ties is lost. Bojar et al. (2011) critique the earlier WMT evaluations, citing issues with the ignoring of non-top ranks (noted in Section 3 herein also), with ties and also with interannotator agreement. Lopez (2012) extends the analysis of Bojar et al. and casts the problem as “finding the minimum feedback arc set in a tournament, a well-known NPcomplete problem.” He advocates using the pairwise rankings themselves, rather than aggregate statistics like Vilar et al. (2007), and aims to minimize the number of violations among these. Koehn (2012) evaluates empirically the approaches of both Bojar et al. (2011) and Lopez (2012), with a focus on determining which systems are statistically distinguishable in terms of performance, defining confidence bounds for this purpose. Hopkins and May (2013) recently advocated a focus on finding the extent to which particular rankings could be trusted. They proposed a model based on Item Response Theory (IRT), which underlies many standardized tests. They draw an analogy with judges assessing students on the basis of an underlying distribution of the student’s ability, with items authored by student"
J15-2005,W12-3101,0,0.154309,"blem as one where, given an order relationship is-better-than between pairs of systems, the goal is to find an ordering of all the systems: They see this as the fundamental computer science problem of sorting. They define an aggregate evaluation score for comparing systems, estimating expected value and standard error for hypothesis testing. However, in aggregating this way information about ties is lost. Bojar et al. (2011) critique the earlier WMT evaluations, citing issues with the ignoring of non-top ranks (noted in Section 3 herein also), with ties and also with interannotator agreement. Lopez (2012) extends the analysis of Bojar et al. and casts the problem as “finding the minimum feedback arc set in a tournament, a well-known NPcomplete problem.” He advocates using the pairwise rankings themselves, rather than aggregate statistics like Vilar et al. (2007), and aims to minimize the number of violations among these. Koehn (2012) evaluates empirically the approaches of both Bojar et al. (2011) and Lopez (2012), with a focus on determining which systems are statistically distinguishable in terms of performance, defining confidence bounds for this purpose. Hopkins and May (2013) recently adv"
J15-2005,P09-1034,0,0.0130309,"stem in the sentence ranking evaluation.” They assessed inter-annotator agreement, and—with a key goal of the meta-evaluation being to find the automatic evaluation metric that best matched human evaluations—calculated Spearman’s rank correlation coefficient between the two types of assessment. The 2008 shared task (Callison-Burch 311 Computational Linguistics Volume 41, Number 2 et al. 2008) took the same approach, but noted that in ranking, “[h]ow best to treat these is an open discussion, and certainly warrants further thought,” in particular because of ties “further complicating matters.” Pado et al. (2009) modified the systemlevel predictions approach to become “tie-aware,” and noted that that this “makes a considerable practical difference, improving correlation figures by 5–10 points.” At around the same time Vilar et al. (2007) examined the use of pairwise comparisons in MT evaluation. They pose the problem as one where, given an order relationship is-better-than between pairs of systems, the goal is to find an ordering of all the systems: They see this as the fundamental computer science problem of sorting. They define an aggregate evaluation score for comparing systems, estimating expected"
J15-2005,W07-0713,0,0.196544,"+ over − and not too many null preferences (ModPref), a distribution of equal preferences over all three categories (EqualPref), a distribution with mostly null preferences and equal n+ and n− (NoPref), and a distribution with very few null preferences (StrongPref). Multiple Pairwise Comparison. As noted in Section 1, there has been a trend to using human preference judgments, particularly in the workshops on statistical machine translation from Callison-Burch et al. (2007) onwards. Schemes have included asking humans to rank random samples of five translations, each from a different system. Vilar et al. (2007) propose using binary rather than n-ary rankings, arguing that this is a natural and easy task for judges. Here we present some artificial data of pairwise (binary) rankings to illustrate the techniques we discuss in Section 5, although these techniques can be extended to n-ary comparisons. In our example, there are four systems A, B, C, D and four judges J1, J2, J3, J4. The judges have pairwise ranked 240 translation pairs from systems x and y, indicating whether the translation of x is better than y (x  y), worse than y (x ≺ y), or similar in quality to y (x = y); see Table 1. An overall im"
J15-2005,D13-1064,0,\N,Missing
J18-3003,W17-5025,0,0.0414871,"Missing"
J18-3003,P06-4020,0,0.0185625,"Missing"
J18-3003,brooke-hirst-2012-measuring,0,0.346229,"can be applied for NLI. We aim to examine several different ensemble and meta-classification architectures, each of which can utilize different configurations and algorithms. Furthermore, previous ensemble methods have not been tested on different data sets, making the ability of these models to generalize for NLI unclear. Ideally, the same method should be tested across multiple corpora to assess its validity. To this end, we also apply our methods to multiple data sets, in both English and other languages, to evaluate their generalization capacity. In addition, following the observation of Brooke and Hirst (2012b) that patterns utilized by machine learners can be corpus-specific and influenced by topic, even with apparently topic-neutral features—something that could also be true in our multi-level classifier context—we carry out cross-corpus experiments along the lines of Brooke and Hirst (2012b), Malmasi and Dras (2015), and Ionescu, Popescu, and Cahill (2016). To summarize, the principal aims of the present study are the following: 1. Apply several advanced ensemble combination methods to NLI and evaluate their performance against previously used ensemble methods. 1 Traditional text classification"
J18-3003,C12-1025,0,0.0420896,"Missing"
J18-3003,C14-1185,0,0.0367403,"Missing"
J18-3003,W13-1726,0,0.0502666,"Missing"
J18-3003,W17-5023,0,0.0367823,"Missing"
J18-3003,W17-5049,0,0.221304,"Missing"
J18-3003,W13-1727,0,0.0506005,"Missing"
J18-3003,W17-5041,0,0.0299776,"Missing"
J18-3003,W13-1712,0,0.0558704,"Missing"
J18-3003,W13-1729,0,0.049149,"Missing"
J18-3003,W13-1713,0,0.050284,"Missing"
J18-3003,W13-1730,0,0.0605592,"Missing"
J18-3003,W17-5024,0,0.184364,"Missing"
J18-3003,D14-1142,0,0.12137,"Missing"
J18-3003,J16-3005,0,0.0296618,"Missing"
J18-3003,W17-5021,0,0.461653,"Missing"
J18-3003,W13-1714,0,0.0445402,"Missing"
J18-3003,W17-5043,0,0.0399461,"Missing"
J18-3003,W17-5044,0,0.0385807,"Missing"
J18-3003,W15-0606,1,0.788838,"rmed well on a range of classification tasks and comment that “whatever exotic tools are the rage of the day, we should always have available these two simple tools.” 5. Features As its focus is on classifier architecture, this study just utilizes a standard set of NLI features used in previous work, as noted in, for example, the overview of the systems in the NLI shared task of 2013 (Tetreault, Blanchard, and Cahill 2013). We combine these various feature types because it has been shown that both lexical and syntactic features each capture diverse types of information that are complementary (Malmasi and Cahill 2015). Different feature types are extracted for each of our data sets, as shown in Table 4. Table 4 An overview of the features used for each data set. Feature Word/Lemma n-grams Character n-grams Function word unigrams Function word bigrams POS n-grams Dependencies CFG Rules Adaptor Grammars TSG Fragments T OEFL11 X X X X X X X X X E F C AM D AT Chinese Norwegian X X X X X X X X X X X X 419 Computational Linguistics Volume 44, Number 3 The feature types for each data set were chosen based on properties of the data set and the availability of NLP resources for the L2. For stylistic classification"
J18-3003,W14-3625,1,0.859836,"cal relations between other words; consequently, they have been widely used in stylistic classification tasks. In this work, the English word list was obtained from the Onix Text Retrieval Toolkit.18 For Norwegian we used a list of 176 function words obtained from the distribution of the Apache Lucene search engine software.19 This list includes stop words for the Bokm˚al variant of the language and contains entries such as hvis [whose], ikke [not], jeg [I], s˚a [so], and hj˚a [at]. We also make this list available on our Web site.20 For Chinese, we utilize the function word list described in Malmasi and Dras (2014b). 17 It is true that other features can still detect topic bias: Brooke and Hirst (2011) showed that the ICLE corpus suffered from this, as, for example, the fact that French texts were more about philosophy and religion, whereas Japanese texts were more about personal experiences, such as language learning and travel, had related register differences that could be detected by non-content features. 18 http://www.lextek.com/manuals/onix/stopwords1.html. 19 https://github.com/apache/lucene-solr. 20 http://web.science.mq.edu.au/~smalmasi/data/norwegian-funcwords.txt. 420 Malmasi and Dras Native"
J18-3003,E14-4019,1,0.846416,"cal relations between other words; consequently, they have been widely used in stylistic classification tasks. In this work, the English word list was obtained from the Onix Text Retrieval Toolkit.18 For Norwegian we used a list of 176 function words obtained from the distribution of the Apache Lucene search engine software.19 This list includes stop words for the Bokm˚al variant of the language and contains entries such as hvis [whose], ikke [not], jeg [I], s˚a [so], and hj˚a [at]. We also make this list available on our Web site.20 For Chinese, we utilize the function word list described in Malmasi and Dras (2014b). 17 It is true that other features can still detect topic bias: Brooke and Hirst (2011) showed that the ICLE corpus suffered from this, as, for example, the fact that French texts were more about philosophy and religion, whereas Japanese texts were more about personal experiences, such as language learning and travel, had related register differences that could be detected by non-content features. 18 http://www.lextek.com/manuals/onix/stopwords1.html. 19 https://github.com/apache/lucene-solr. 20 http://web.science.mq.edu.au/~smalmasi/data/norwegian-funcwords.txt. 420 Malmasi and Dras Native"
J18-3003,U14-1020,1,0.85752,"cal relations between other words; consequently, they have been widely used in stylistic classification tasks. In this work, the English word list was obtained from the Onix Text Retrieval Toolkit.18 For Norwegian we used a list of 176 function words obtained from the distribution of the Apache Lucene search engine software.19 This list includes stop words for the Bokm˚al variant of the language and contains entries such as hvis [whose], ikke [not], jeg [I], s˚a [so], and hj˚a [at]. We also make this list available on our Web site.20 For Chinese, we utilize the function word list described in Malmasi and Dras (2014b). 17 It is true that other features can still detect topic bias: Brooke and Hirst (2011) showed that the ICLE corpus suffered from this, as, for example, the fact that French texts were more about philosophy and religion, whereas Japanese texts were more about personal experiences, such as language learning and travel, had related register differences that could be detected by non-content features. 18 http://www.lextek.com/manuals/onix/stopwords1.html. 19 https://github.com/apache/lucene-solr. 20 http://web.science.mq.edu.au/~smalmasi/data/norwegian-funcwords.txt. 420 Malmasi and Dras Native"
J18-3003,N15-1160,1,0.882698,"Missing"
J18-3003,W15-0620,1,0.882544,"Missing"
J18-3003,W13-1716,1,0.9035,"Missing"
J18-3003,W16-4801,1,0.877455,"Missing"
J18-3003,P14-5010,0,0.005809,"Missing"
J18-3003,W17-5042,0,0.0222264,"Missing"
J18-3003,de-marneffe-etal-2006-generating,0,0.0128688,"Missing"
J18-3003,W17-5047,0,0.0450588,"Missing"
J18-3003,W13-1735,0,0.0534509,"Missing"
J18-3003,W17-5027,0,0.0454599,"Missing"
J18-3003,P12-2038,0,0.0326668,"tanford parser (de Marneffe, Maccartney, and Manning 2006). We then generate all the variations for each of the dependencies (grammatical relations) by substituting each lemma with its corresponding POS tag. For instance, a grammatical relation of det(knowledge, the) yields the following variations: det(NN, the), det(knowledge, DT), and det(NN, DT). CFG Rules. For English and Chinese, we obtain a constituent-based parse using the Stanford parser, and extract the production rules that constitute those trees to use as features. Tree Substitution Grammar Fragments. TSG fragments were proposed by Swanson and Charniak (2012) as another type of syntactic feature for NLI that captures a broader syntactic context than the single-level fragments of phrase-structure trees that constitute the CFG rules. We only extract TSG fragments for the T OEFL11 data as they include lexical terminal nodes. 21 http://nlp.stanford.edu/software/corenlp.shtml. 22 http://users.sussex.ac.uk/~johnca/rasp/. 421 Computational Linguistics Volume 44, Number 3 6. Classification Models We conduct a set of three experiments, each based on different ensemble structures that we describe in this section. The first model is based on a traditional pa"
J18-3003,tenfjord-etal-2006-ask,0,0.188231,"Missing"
J18-3003,W13-1706,0,0.0869832,"Missing"
J18-3003,C12-1158,0,0.219699,"corpora (Malmasi and Dras 2014a,c). Ensemble Methods: The 2013 Shared Task. As mentioned earlier, some of the most successful approaches to NLI have used ensemble learning methods, and investigating ensemble configurations is a key goal of this article. We now therefore present an overview of work in NLI that has used ensembles, focusing on the way in which ensembles were defined and what benefit they brought. Most of this work leads into, is part of, or is positioned with respect to the first NLI shared task of 2013. We discuss briefly after this some observations from the 2017 shared task. Tetreault et al. (2012) were the first to propose the use of classifier ensembles for NLI, and they performed a comprehensive evaluation of the feature types used until that point. In their study they used an ensemble of logistic regression learners, using a wide range of features that included character and word n-grams, function words, parts of speech, spelling errors, and writing quality markers. With regard to syntactic features, they also investigated the use of Tree Substitution Grammars and dependency features extracted using the Stanford parser. Furthermore, they also proposed using language models for this"
J18-3003,N01-1031,0,0.0900484,"d to various classification tasks with good results. Not surprisingly, researchers have attempted to use them for improving the performance of NLI, as we discuss in the next section. 2.2 Native Language Identification General Background. Determining the identity of, or properties of, an author of a text is a problem of longstanding interest, and attempts to solve it are similarly longstanding (Koppel, Schler, and Argamon 2009). NLI is the subtype of this task focusing on identifying the L1 of the writer. To our knowledge, the first work tackling this task in a computational manner was that of Tomokiyo and Jones (2001), who worked with speech data recorded for this task. Their main aim was to detect non-native speech, using part-of-speech and lexical features in a na¨ıve Bayes classifier, and to also determine the native language of the non-native speakers. Another early work was that of Jarvis, Castaneda-Jim´enez, and Nielsen (2004), who presented an early empirical approach to investigating L1 transfer using lexical style and word choice, which they termed wordprints. Working with a written corpus compiled for the task and focusing exclusively on lexical transfer, they collected the 30 most frequent words"
J18-3003,W15-0614,1,0.905428,"Missing"
J18-3003,U09-1008,1,0.84359,"Missing"
J18-3003,D12-1064,1,0.884234,"Missing"
J18-3003,W17-5028,0,\N,Missing
J18-3003,J15-4006,0,\N,Missing
K17-3014,P16-1231,0,0.0892725,"Missing"
K17-3014,D15-1041,0,0.0586343,"ny external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs better than strong baselines and especially outperforms the neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016), achieving a new state of the art. 2 BiLSTM-based latent feature representations: Given an input sentence s consisting of n word tokens w1 , w2 , ..., wn , we represent each word wi in (•) s by an embedding ewi . Plank et al. (2016) and Ballesteros et al. (2015) show that character-based representations of words help improve POS tagging and dependency parsing performances. So, we also use a sequence BiLSTM (BiLSTMseq ) to compute a character-based vector representation for each word wi in s. For a word type w consisting of k characters w = c1 c2 ...ck , the input to the sequence BiLSTM consists of k character embeddings c1:k in which each embedding vector cj represents the j th character cj in w; and the output (∗) is the character-based embedding ew of the word type w, computed as: Our joint model e(∗) w = BiLSTMseq (c1:k ) In this section, we descr"
K17-3014,D12-1133,0,0.0393744,"ation for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project"
K17-3014,D15-1159,0,0.273183,"cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs better than strong baselines and especially ou"
K17-3014,A00-1031,0,0.217047,"0 73.4 75.4 79.2 82.5 10.6 81.3 79.9 79.5 81.3 80.4 79.3 3.4 80.2 80.1 79.4 80.7 81.8 81.7 9.2 78.5 76.6 76.6 77.6 78.9 79.6 4.4 74.8 69.6 72.0 73.9 73.8 75.8 4.5 Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctuation) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates. UDPipe is the trainable pipeline for processing CoNLL-U files (Straka et al., 2016). TnT denotes the second order HMM-based TnT tagger (Brants, 2000). CRF denotes the Conditional random fields-based tagger, presented in Plank et al. (2014). BiLSTM-aux refers to the state-of-the-art (SOTA) BiLSTMbased POS tagging model with an additional auxiliary loss for rare words (Plank et al., 2016). Note that the (old) language code for Hebrew “iw” is referred to as “he” as in Plank et al. (2016). [⊕]: Results are reported in Plank et al. (2016). Stack-prop refers to the SOTA Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016). 5-Chars denotes the absolute accuracy decrease of our jPTDP, when t"
K17-3014,W06-2920,0,0.112905,"to achieve the best accuracy. Alternatively, joint learning both POS tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 1 Introduction Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006, 2007 and 2017 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a; Zeman et al., 2017). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada 134 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e"
K17-3014,D14-1082,0,0.0290936,"Missing"
K17-3014,P11-1089,0,0.0526243,"142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Univers"
K17-3014,D16-1238,0,0.0563387,"Missing"
K17-3014,P14-1130,0,0.0298879,"k et al. (2016). Stack-prop refers to the SOTA Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016). 5-Chars denotes the absolute accuracy decrease of our jPTDP, when the character-based representations of words are not taken into account. B’15 denotes the character-based stack LSTM model for transition-based dependency parsing (Ballesteros et al., 2015). PipelinePtag refers to a greedy version of the approach proposed by Alberti et al. (2015). RBGParser refers to the graph-based dependency parser with tensor decomposition, presented in Lei et al. (2014). [*]: Results are reported in Zhang and Weiss (2016). 3.2 Implementation details ment set is when using 64-dimensional character embeddings, 128-dimensional word embeddings, 128-dimensional BiLSTM states, 2 BiLSTM layers and 100 hidden nodes in MLPs with one hidden layer.4 We then apply those hyper-parameters to all 18 remaining languages. Our jPTDP is implemented using DYNET v2.0 (Neubig et al., 2017).3 We optimize the objective function using Adam (Kingma and Ba, 2014) with default DYNET parameter settings and no mini-batches. We use a fixed random seed, and we do not utilize pre-trained em"
K17-3014,P13-1104,0,0.0177322,"Missing"
K17-3014,D11-1109,0,0.0242839,"to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental r"
K17-3014,I17-1007,0,0.0423582,"Missing"
K17-3014,P15-1033,0,0.0179515,"Missing"
K17-3014,P13-2109,0,0.0304432,"Missing"
K17-3014,C96-1058,0,0.755432,"g performance and ii) the the syntactic context of a parse tree could help resolve POS Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 1 Introduction Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006, 2007 and 2017 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a; Zeman et al., 2017). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada 134 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed int"
K17-3014,P05-1012,0,0.124355,"and ii) the the syntactic context of a parse tree could help resolve POS Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 1 Introduction Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006, 2007 and 2017 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a; Zeman et al., 2017). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada 134 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer percept"
K17-3014,J11-1007,0,0.0469794,"tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 1 Introduction Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006, 2007 and 2017 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a; Zeman et al., 2017). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada 134 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustrati"
K17-3014,I11-1136,0,0.040234,"endencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 language"
K17-3014,E06-1011,0,0.360014,"Missing"
K17-3014,P82-1020,0,0.835619,"Missing"
K17-3014,P16-2091,0,0.0520545,"Missing"
K17-3014,Q16-1032,0,0.0276542,"Missing"
K17-3014,U16-1017,1,0.896512,"Missing"
K17-3014,Q16-1023,0,0.513708,"07b; Bohnet, 2010; Zhang and Nivre, 2011; Martins et al., 2013; Choi and McCallum, 2013). Recent work shows that using deep learning in dependency parsing has obtained state-of-the-art performances. Several authors represent the core features with dense vector embeddings and then feed them as inputs to neural network-based classifiers (Chen and Manning, 2014; Weiss et al., 2015; Pei et al., 2015; Andor et al., 2016). In addition, others propose novel neural architectures for parsing to handle feature-engineering (Dyer et al., 2015; Cheng et al., 2016; Zhang et al., 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016a,b; Dozat and Manning, 2017; Ma and Hovy, 2017; Peng et al., 2017). We present a novel neural network model that learns POS tagging and graph-based dependency parsing jointly. Our model uses bidirectional LSTMs to learn feature representations shared for both POS tagging and dependency parsing tasks, thus handling the feature-engineering problem. Our extensive experiments, on 19 languages from the Universal Dependencies project, show that our model outperforms the state-of-the-art neural networkbased Stack-propagation model for joint POS tagging and transition-based dependency parsing, result"
K17-3014,W03-3017,0,0.1546,"Missing"
K17-3014,P10-1001,0,0.0222126,"ic context of a parse tree could help resolve POS Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 1 Introduction Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006, 2007 and 2017 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a; Zeman et al., 2017). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada 134 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden laye"
K17-3014,P14-1069,0,0.0119884,"r, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs bett"
K17-3014,P15-1032,0,0.0279264,"Missing"
K17-3014,W03-3023,0,0.190146,"Missing"
K17-3014,P15-1031,0,0.0311799,"Missing"
K17-3014,P17-1186,0,0.0153844,"Missing"
K17-3014,petrov-etal-2012-universal,0,0.0164754,"t of all possible dependency trees for the input sentence s while scorearc (h, m) measures the score of the arc between the head hth word and the modifier mth word in s. Following Kiperwasser and Goldberg (2016b), we score an arc by using a MLP with one-node output layer (MLParc ) on top of the BiLSTMctx : 3 scorearc (h, m) = MLParc (v h ◦ v m ) Experiments 3.1 Experimental setup Following Zhang and Weiss (2016) and Plank et al. (2016), we conduct multilingual experiments on 19 languages from the Universal Dependencies (UD) treebanks1 v1.2 (Nivre et al., 2015), using the universal POS tagset (Petrov et al., 2012) instead of the language specific POS tagset.2 For dependency parsing, the evaluation metric is the labeled attachment score (LAS). LAS is the percentage of words which are correctly assigned both dependency arc and relation type. where v h and v m are the shared BiLSTM-based feature vectors representing the hth and mth words in s, respectively. We then compute a marginbased hinge loss Larc with loss-augmented inference to maximize the margin between the gold unlabeled parse tree and the highest scoring incorrect tree (Kiperwasser and Goldberg, 2016b). Dependency relation types are predicted i"
K17-3014,C14-1168,0,0.0255266,"8 81.7 9.2 78.5 76.6 76.6 77.6 78.9 79.6 4.4 74.8 69.6 72.0 73.9 73.8 75.8 4.5 Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctuation) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates. UDPipe is the trainable pipeline for processing CoNLL-U files (Straka et al., 2016). TnT denotes the second order HMM-based TnT tagger (Brants, 2000). CRF denotes the Conditional random fields-based tagger, presented in Plank et al. (2014). BiLSTM-aux refers to the state-of-the-art (SOTA) BiLSTMbased POS tagging model with an additional auxiliary loss for rare words (Plank et al., 2016). Note that the (old) language code for Hebrew “iw” is referred to as “he” as in Plank et al. (2016). [⊕]: Results are reported in Plank et al. (2016). Stack-prop refers to the SOTA Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016). 5-Chars denotes the absolute accuracy decrease of our jPTDP, when the character-based representations of words are not taken into account. B’15 denotes the c"
K17-3014,N15-1005,0,0.0152957,"4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs better than strong basel"
K17-3014,P16-2067,0,0.17323,"uber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs better than strong baselines and especially outperforms the neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016), achieving a new state of the art. 2 BiLSTM-based latent feature representations: Given an input sentence s consisting of n word tokens w1 , w2 , ..., wn , we represent each word wi in (•) s by an embedding ewi . Plank et al. (2016) and Ballesteros et al. (2015) show that character-based representations of words help improve POS tagging and dependency parsing performances. So, we also use a sequence BiLSTM (BiLSTMseq ) to compute a character-based vector representation for each word wi in s. For a word type w consisting of k characters w = c1 c2 ...ck , the input to the sequence BiLSTM consists of k character embeddings c1:k in which each embedding vector cj represents the j th character cj in w; and the output (∗) is the character-based embedding ew of the word type w, computed as: Our joint model e(∗) w = BiLSTMseq (c1"
K17-3014,P16-1147,0,0.0149578,"Missing"
K17-3014,D12-1046,0,0.015649,"Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint"
K17-3014,P11-2033,0,0.142463,"Missing"
K17-3014,P16-1131,0,0.0218431,"Missing"
K17-3014,L16-1680,0,0.129015,"3 69.5 70.3 72.4 73.6 66.8 5.4 84.5 82.4 83.6 83.9 84.7 84.9 2.3 79.4 78.0 73.4 75.4 79.2 82.5 10.6 81.3 79.9 79.5 81.3 80.4 79.3 3.4 80.2 80.1 79.4 80.7 81.8 81.7 9.2 78.5 76.6 76.6 77.6 78.9 79.6 4.4 74.8 69.6 72.0 73.9 73.8 75.8 4.5 Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctuation) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates. UDPipe is the trainable pipeline for processing CoNLL-U files (Straka et al., 2016). TnT denotes the second order HMM-based TnT tagger (Brants, 2000). CRF denotes the Conditional random fields-based tagger, presented in Plank et al. (2014). BiLSTM-aux refers to the state-of-the-art (SOTA) BiLSTMbased POS tagging model with an additional auxiliary loss for rare words (Plank et al., 2016). Note that the (old) language code for Hebrew “iw” is referred to as “he” as in Plank et al. (2016). [⊕]: Results are reported in Plank et al. (2016). Stack-prop refers to the SOTA Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016). 5"
K17-3014,P16-1218,0,0.0176882,"Missing"
L16-1647,P12-1011,0,0.0220187,"estion of representing time intervals for temporal text classification. Related Work Modeling temporal information in text is a relevant task to a number of NLP applications. Information Retrieval (IR) methods, for example, often have to process temporal information in both queries and documents to deal with dynamicity of the content found in data repositories and the Web (Dakka et al., 2012; Preotiuc-Pietro, 2014; Kanhabua et al., 2015; Zhao and Hauff, 2015b; Zhao and Hauff, 2015a). Time expressions (e.g. after 2010), can help algorithms to identify the approximate publication date of texts (Chambers, 2012), but there are a number of cases in which they are not present in text and one alternative is to use features related to language change as we propose in this paper. As will be evidenced in this section, even though there were a number of attempts to approach temporal text classification, to our knowledge this task was not substantially explored as other text classification tasks. The work by de Jong et al. (2005) uses unigram language models combined with smoothing techniques and log-likelihood ratio measure (NLLR) (Kraaij, 2004) to classify documents within different time spans. The method"
L16-1647,R13-1018,0,0.0129348,"investigate the most informative features for this task and how they are linked to language change. Keywords: Language Change, Temporal Text Classification, Support Vector Machines, Text Categorization 1. Introduction 2. It is well-known that language changes over time both in spoken and in written forms. Changes in written language can be manifested in many ways such as the use of the lexicon, grammatical structures, and textual stylistics. Recent studies have shown that it is possible to use language change to predict the approximate publication date of texts in diachronic text collections (Ciobanu et al., 2013a; Popescu and Strapparava, 2015). This task is called temporal text classification and to our knowledge it has not been substantially explored in the literature as other text classification tasks. This paper contributes in this direction. In this paper we investigate the use of supervised machine learning classifiers to predict when a text was written using lexical and morphosyntactic information. The classifiers were trained and tested on a sample of a historical Portuguese corpus called Colonia (Zampieri and Becker, 2013) which contains texts spanning from the 16th to the early 20th century"
L16-1647,W13-2714,0,0.0164411,"investigate the most informative features for this task and how they are linked to language change. Keywords: Language Change, Temporal Text Classification, Support Vector Machines, Text Categorization 1. Introduction 2. It is well-known that language changes over time both in spoken and in written forms. Changes in written language can be manifested in many ways such as the use of the lexicon, grammatical structures, and textual stylistics. Recent studies have shown that it is possible to use language change to predict the approximate publication date of texts in diachronic text collections (Ciobanu et al., 2013a; Popescu and Strapparava, 2015). This task is called temporal text classification and to our knowledge it has not been substantially explored in the literature as other text classification tasks. This paper contributes in this direction. In this paper we investigate the use of supervised machine learning classifiers to predict when a text was written using lexical and morphosyntactic information. The classifiers were trained and tested on a sample of a historical Portuguese corpus called Colonia (Zampieri and Becker, 2013) which contains texts spanning from the 16th to the early 20th century"
L16-1647,W06-0903,0,0.343169,"e a number of attempts to approach temporal text classification, to our knowledge this task was not substantially explored as other text classification tasks. The work by de Jong et al. (2005) uses unigram language models combined with smoothing techniques and log-likelihood ratio measure (NLLR) (Kraaij, 2004) to classify documents within different time spans. The method was tested on a collection of Dutch journalistic texts published from January 1999 to February 2005. Other methods, such as Kumar et al. (2011), make use of information gain to estimate the best features in classification. In Dalli and Wilks (2006) researchers train a classifier to predict the publication date of texts within a time span of nine years. The method uses words as features and it is aided by words which increase their frequency at some point of time, particularly named entities. Another study that works under a similar assumption is the one published by Abe and Tsumoto (2010). The authors proposed the use of similarity metrics to categorize texts based on keywords calculated using tf-idf (term frequency - inverse document frequency). Garcia-Fernandez et al. (2011) presents a method to predict the publication dates of excerp"
L16-1647,Q16-1003,0,0.0689138,"Missing"
L16-1647,E14-4019,1,0.882439,"tion of historical corpora. Although Colonia is to our knowledge the biggest Portuguese corpus of its kind, it does not contain many texts from each period. The number of documents varies between 13 from the 16th century and 38 from the 19th century. For text classification, however, the number of documents available (especially at the training stage) is very important to provide enough information to achieve high classification performance. To circumvent this limitation, in this paper we propose the use of composite documents made of sentences from various texts. Following the methodology of Malmasi and Dras (2014a), we randomly select and combine the sentences from the same class (time period) to generate artificial texts of approximately 330 tokens on average, creating a set of documents for training and testing. This methodology ensures that the texts for each class are a mix of different authorship styles and topics. It also means that all documents are similar and comparable in length making the task more challenging. Previous work in other text classification tasks has shown that longer texts can be easier to classify (Malmasi et al., 2015). In our experiments we model two dimensions of language"
L16-1647,D14-1144,1,0.86872,"tion of historical corpora. Although Colonia is to our knowledge the biggest Portuguese corpus of its kind, it does not contain many texts from each period. The number of documents varies between 13 from the 16th century and 38 from the 19th century. For text classification, however, the number of documents available (especially at the training stage) is very important to provide enough information to achieve high classification performance. To circumvent this limitation, in this paper we propose the use of composite documents made of sentences from various texts. Following the methodology of Malmasi and Dras (2014a), we randomly select and combine the sentences from the same class (time period) to generate artificial texts of approximately 330 tokens on average, creating a set of documents for training and testing. This methodology ensures that the texts for each class are a mix of different authorship styles and topics. It also means that all documents are similar and comparable in length making the task more challenging. Previous work in other text classification tasks has shown that longer texts can be easier to classify (Malmasi et al., 2015). In our experiments we model two dimensions of language"
L16-1647,P12-2051,0,0.075944,"Missing"
L16-1647,E14-4004,1,0.951879,"The authors concluded that the use of lexical features is the best source of information for this task. An important issue to take into account when working on temporal text classification is how to represent time. Most studies, including our own, model the task as supervised classification in which algorithms are trained to assign texts to an n number of classes. Each of these n classes represent an arbitrarily defined time interval, for example: a month, a year, or a decade. However, there have been a few attempts to approach this task without relying on predefined time spans. The study by Niculae et al. (2014) approached the task using ranking and pairwise comparisons to predict for each pair of documents which one is older and finally to produce a rank of all documents in a collection from older to newer. Another recent study to tackle the issue of time intervals is Efremova et al. (2015). In this study authors apply clustering methods to automatically obtain optimal time partitions in a dataset of historical Dutch notary acts. We return to this question in Section 6.1. of this paper. The style of texts also changes over time and it can be a good indicator to predict the publication date of a docˇ"
L16-1647,I13-1040,0,0.041697,"Missing"
L16-1647,S15-2147,0,0.336739,"formative features for this task and how they are linked to language change. Keywords: Language Change, Temporal Text Classification, Support Vector Machines, Text Categorization 1. Introduction 2. It is well-known that language changes over time both in spoken and in written forms. Changes in written language can be manifested in many ways such as the use of the lexicon, grammatical structures, and textual stylistics. Recent studies have shown that it is possible to use language change to predict the approximate publication date of texts in diachronic text collections (Ciobanu et al., 2013a; Popescu and Strapparava, 2015). This task is called temporal text classification and to our knowledge it has not been substantially explored in the literature as other text classification tasks. This paper contributes in this direction. In this paper we investigate the use of supervised machine learning classifiers to predict when a text was written using lexical and morphosyntactic information. The classifiers were trained and tested on a sample of a historical Portuguese corpus called Colonia (Zampieri and Becker, 2013) which contains texts spanning from the 16th to the early 20th century. The approach we propose here is"
L16-1647,S15-2142,0,0.0219181,"Missing"
L16-1647,S15-2148,0,0.327017,"Missing"
L16-1647,S15-2144,1,0.861705,"Missing"
L18-1410,neubig-mori-2010-word,0,0.024758,"ag such as B (Begin 1 In the traditional underscore-based representation in the Vietnamese word segmentation task (Nguyen et al., 2009), white space is only used to separate words while underscore is used to separate syllables inside a word. of a word) or I (Inside of a word). Another promising approach is joint word segmentation and POS tagging (Takahashi and Yamamoto, 2016; Nguyen et al., 2017b), which assigns a combined segmentation and POS tag to each syllable. Furthermore, Luu and Kazuhide (2012), Liu and Lin (2014) and Nguyen and Le (2016) proposed methods based on pointwise prediction (Neubig and Mori, 2010), where a binary classifier is trained to identify whether or not there is a word boundary between two syllables. In this paper, we propose a novel method to Vietnamese word segmentation. Our method automatically constructs a Single Classification Ripple Down Rules (SCRDR) tree (Compton and Jansen, 1990) to correct wrong segmentations given by a longest matchingbased word segmenter. On the benchmark Vietnamese treebank (Nguyen et al., 2009), experimental results show that our method obtains better accuracy and performance speed than the previous state-of-the-art methods JVnSegmenter (Nguyen et"
L18-1410,Y06-1028,0,0.813957,"elves (Thang et al., 2008; Le et al., 2008), thus creating challenges in Vietnamese word segmentation (Nguyen et al., 2012). Many approaches are proposed for the Vietnamese word segmentation task. Le et al. (2008), Pham et al. (2009) and Tran et al. (2012) applied the maximum matching strategy (NanYuan and YanBin, 1991) to generate all possible segmentations for each input sentence; then to select the best segmentation, Le et al. (2008) and Tran et al. (2012) used n-gram language models while Pham et al. (2009) employed part-ofspeech (POS) information from an external POS tagger. In addition, Nguyen et al. (2006), Dinh and Vu (2006) and Tran et al. (2010) considered this segmentation task as a sequence labeling task, using either a linear-chain CRF, SVM or MaxEnt model to assign each syllable a segmentation tag such as B (Begin 1 In the traditional underscore-based representation in the Vietnamese word segmentation task (Nguyen et al., 2009), white space is only used to separate words while underscore is used to separate syllables inside a word. of a word) or I (Inside of a word). Another promising approach is joint word segmentation and POS tagging (Takahashi and Yamamoto, 2016; Nguyen et al., 2017b)"
L18-1410,W09-3035,0,0.228624,"ll possible segmentations for each input sentence; then to select the best segmentation, Le et al. (2008) and Tran et al. (2012) used n-gram language models while Pham et al. (2009) employed part-ofspeech (POS) information from an external POS tagger. In addition, Nguyen et al. (2006), Dinh and Vu (2006) and Tran et al. (2010) considered this segmentation task as a sequence labeling task, using either a linear-chain CRF, SVM or MaxEnt model to assign each syllable a segmentation tag such as B (Begin 1 In the traditional underscore-based representation in the Vietnamese word segmentation task (Nguyen et al., 2009), white space is only used to separate words while underscore is used to separate syllables inside a word. of a word) or I (Inside of a word). Another promising approach is joint word segmentation and POS tagging (Takahashi and Yamamoto, 2016; Nguyen et al., 2017b), which assigns a combined segmentation and POS tag to each syllable. Furthermore, Luu and Kazuhide (2012), Liu and Lin (2014) and Nguyen and Le (2016) proposed methods based on pointwise prediction (Neubig and Mori, 2010), where a binary classifier is trained to identify whether or not there is a word boundary between two syllables."
L18-1410,R11-1056,1,0.803088,"OS tag of the word “anticipate” instead of the initial POS tag “VB.” To correct a wrong conclusion returned for a given case, a new node containing a new exception rule may be attached to the last node in the evaluation path. If the last node’s rule is the last satisfied rule given the case, the new node is added as its child with the “except” edge; otherwise, the new node is attached with the “if-not” edge. SCRDR has been successfully applied in NLP tasks for temporal relation extraction (Pham and Hoffmann, 2006), word lemmatization (Plisson et al., 2008), POS tagging (Xu and Hoffmann, 2010; Nguyen et al., 2011b; Nguyen et al., 2014; Nguyen et al., 2016), named entity recognition (Nguyen and Pham, 2012) and question answering (Nguyen et al., 2011a; Nguyen et al., 2013; Nguyen et al., 2017a). The works by Plisson et al. (2008), Nguyen et al. (2011b), Nguyen et al. (2014) and Nguyen et al. (2016) build the tree autoFigure 2: Diagram of our approach. matically, while others manually construct the tree. 3. Our approach This section describes our new error-driven approach to automatically construct a SCRDR tree to correct wrong segmentations produced by an initial word segmenter. Following Nguyen et al."
L18-1410,W12-5005,0,0.0366239,"Missing"
L18-1410,E14-2005,1,0.884766,"ticipate” instead of the initial POS tag “VB.” To correct a wrong conclusion returned for a given case, a new node containing a new exception rule may be attached to the last node in the evaluation path. If the last node’s rule is the last satisfied rule given the case, the new node is added as its child with the “except” edge; otherwise, the new node is attached with the “if-not” edge. SCRDR has been successfully applied in NLP tasks for temporal relation extraction (Pham and Hoffmann, 2006), word lemmatization (Plisson et al., 2008), POS tagging (Xu and Hoffmann, 2010; Nguyen et al., 2011b; Nguyen et al., 2014; Nguyen et al., 2016), named entity recognition (Nguyen and Pham, 2012) and question answering (Nguyen et al., 2011a; Nguyen et al., 2013; Nguyen et al., 2017a). The works by Plisson et al. (2008), Nguyen et al. (2011b), Nguyen et al. (2014) and Nguyen et al. (2016) build the tree autoFigure 2: Diagram of our approach. matically, while others manually construct the tree. 3. Our approach This section describes our new error-driven approach to automatically construct a SCRDR tree to correct wrong segmentations produced by an initial word segmenter. Following Nguyen et al. (2006) and Tran et al."
L18-1410,U17-1013,1,0.664336,"Nguyen et al. (2006), Dinh and Vu (2006) and Tran et al. (2010) considered this segmentation task as a sequence labeling task, using either a linear-chain CRF, SVM or MaxEnt model to assign each syllable a segmentation tag such as B (Begin 1 In the traditional underscore-based representation in the Vietnamese word segmentation task (Nguyen et al., 2009), white space is only used to separate words while underscore is used to separate syllables inside a word. of a word) or I (Inside of a word). Another promising approach is joint word segmentation and POS tagging (Takahashi and Yamamoto, 2016; Nguyen et al., 2017b), which assigns a combined segmentation and POS tag to each syllable. Furthermore, Luu and Kazuhide (2012), Liu and Lin (2014) and Nguyen and Le (2016) proposed methods based on pointwise prediction (Neubig and Mori, 2010), where a binary classifier is trained to identify whether or not there is a word boundary between two syllables. In this paper, we propose a novel method to Vietnamese word segmentation. Our method automatically constructs a Single Classification Ripple Down Rules (SCRDR) tree (Compton and Jansen, 1990) to correct wrong segmentations given by a longest matchingbased word s"
L18-1410,dinh-etal-2008-word,0,0.574669,"Missing"
lynn-etal-2012-irish,W98-1507,0,\N,Missing
lynn-etal-2012-irish,ui-dhonnchadha-van-genabith-2010-partial,1,\N,Missing
lynn-etal-2012-irish,gupta-etal-2010-partial,0,\N,Missing
lynn-etal-2012-irish,nivre-etal-2006-maltparser,0,\N,Missing
lynn-etal-2012-irish,W10-2910,0,\N,Missing
lynn-etal-2012-irish,W02-1503,0,\N,Missing
lynn-etal-2012-irish,W01-1203,0,\N,Missing
lynn-etal-2012-irish,W04-0210,0,\N,Missing
lynn-etal-2012-irish,W08-1301,0,\N,Missing
lynn-etal-2012-irish,P10-1075,0,\N,Missing
lynn-etal-2012-irish,J08-4003,0,\N,Missing
lynn-etal-2012-irish,W10-1401,1,\N,Missing
lynn-etal-2012-irish,P05-1012,0,\N,Missing
lynn-etal-2012-irish,J08-4010,0,\N,Missing
lynn-etal-2012-irish,J08-3003,0,\N,Missing
lynn-etal-2012-irish,W11-4649,0,\N,Missing
lynn-etal-2012-irish,P06-1063,1,\N,Missing
lynn-etal-2012-irish,J08-4004,0,\N,Missing
lynn-etal-2012-irish,P05-1034,0,\N,Missing
lynn-etal-2012-irish,W11-2917,0,\N,Missing
lynn-etal-2012-irish,dzeroski-etal-2006-towards,0,\N,Missing
lynn-etal-2012-irish,mieskes-strube-2006-part,0,\N,Missing
lynn-etal-2012-irish,W11-2929,0,\N,Missing
lynn-etal-2012-irish,ui-dhonnchadha-van-genabith-2006-part,0,\N,Missing
N15-1160,brooke-hirst-2012-measuring,0,0.140452,"y first NLI Shared Task that was held in 2013 (Tetreault et al., 2013). Most English NLI work has been done using two corpora. The International Corpus of Learner En1 An ideal NLI corpus should have multiple L1s, be balanced by topic, proficiency, texts per L1 and be large in size. 1403 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1403–1409, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics glish (Granger et al., 2009) was widely used until recently, despite its shortcomings2 being widely noted (Brooke and Hirst, 2012a). More recently, T OEFL11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a; Malmasi and Dras, 2014c). Recently, Malmasi and Dras (2014b) introduced the Chinese Learner Corpus for NLI and their results indicate that feature performance may be similar across corpora and even L1L2 pairs. This is a claim that we will test here. NLI is now also moving towards using feat"
N15-1160,C12-1025,0,0.0479453,"y first NLI Shared Task that was held in 2013 (Tetreault et al., 2013). Most English NLI work has been done using two corpora. The International Corpus of Learner En1 An ideal NLI corpus should have multiple L1s, be balanced by topic, proficiency, texts per L1 and be large in size. 1403 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1403–1409, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics glish (Granger et al., 2009) was widely used until recently, despite its shortcomings2 being widely noted (Brooke and Hirst, 2012a). More recently, T OEFL11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a; Malmasi and Dras, 2014c). Recently, Malmasi and Dras (2014b) introduced the Chinese Learner Corpus for NLI and their results indicate that feature performance may be similar across corpora and even L1L2 pairs. This is a claim that we will test here. NLI is now also moving towards using feat"
N15-1160,W13-1727,0,0.144938,"Missing"
N15-1160,W00-1322,0,0.0193141,"Missing"
N15-1160,W01-0521,0,0.113463,"similar across corpora and even L1L2 pairs. This is a claim that we will test here. NLI is now also moving towards using features to generate SLA hypotheses. Swanson and Charniak (2014) approach this by using both L1 and L2 data to identify features exhibiting non-uniform usage in both datasets, creating lists of candidate transfer features. Malmasi and Dras (2014d) propose a different method, using linear SVM weights to extract lists of overused and underused linguistic features for each L1 group. Cross-corpus studies have been conducted for various data-driven NLP tasks, including parsing (Gildea, 2001), WSD (Escudero et al., 2000) and NER (Nothman et al., 2009). While most such experiments show a drop in performance, the effect varies widely across tasks, making it hard to predict the expected drop for NLI. We aim to address this question using large training and testing data. 3 EFCamDat: A new corpus for NLI The EF Cambridge Open Language Database (E F C AM DAT) is an English L2 corpus that was released recently (Geertzen et al., 2013). It is composed of texts submitted to Englishtown, an online school used by thousands of learners daily. This corpus is notable for its size, containing som"
N15-1160,W15-0606,1,0.335813,"Missing"
N15-1160,W14-3625,1,0.731842,"hnologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1403–1409, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics glish (Granger et al., 2009) was widely used until recently, despite its shortcomings2 being widely noted (Brooke and Hirst, 2012a). More recently, T OEFL11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a; Malmasi and Dras, 2014c). Recently, Malmasi and Dras (2014b) introduced the Chinese Learner Corpus for NLI and their results indicate that feature performance may be similar across corpora and even L1L2 pairs. This is a claim that we will test here. NLI is now also moving towards using features to generate SLA hypotheses. Swanson and Charniak (2014) approach this by using both L1 and L2 data to identify features exhibiting non-uniform usage in both datasets, creating lists of candidate transfer features. Malmasi and Dras (2014d) propose a different method, using linear SVM weights to extrac"
N15-1160,E14-4019,1,0.718241,"hnologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1403–1409, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics glish (Granger et al., 2009) was widely used until recently, despite its shortcomings2 being widely noted (Brooke and Hirst, 2012a). More recently, T OEFL11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a; Malmasi and Dras, 2014c). Recently, Malmasi and Dras (2014b) introduced the Chinese Learner Corpus for NLI and their results indicate that feature performance may be similar across corpora and even L1L2 pairs. This is a claim that we will test here. NLI is now also moving towards using features to generate SLA hypotheses. Swanson and Charniak (2014) approach this by using both L1 and L2 data to identify features exhibiting non-uniform usage in both datasets, creating lists of candidate transfer features. Malmasi and Dras (2014d) propose a different method, using linear SVM weights to extrac"
N15-1160,U14-1020,1,0.701847,"hnologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1403–1409, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics glish (Granger et al., 2009) was widely used until recently, despite its shortcomings2 being widely noted (Brooke and Hirst, 2012a). More recently, T OEFL11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a; Malmasi and Dras, 2014c). Recently, Malmasi and Dras (2014b) introduced the Chinese Learner Corpus for NLI and their results indicate that feature performance may be similar across corpora and even L1L2 pairs. This is a claim that we will test here. NLI is now also moving towards using features to generate SLA hypotheses. Swanson and Charniak (2014) approach this by using both L1 and L2 data to identify features exhibiting non-uniform usage in both datasets, creating lists of candidate transfer features. Malmasi and Dras (2014d) propose a different method, using linear SVM weights to extrac"
N15-1160,D14-1144,1,0.667583,"hnologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1403–1409, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics glish (Granger et al., 2009) was widely used until recently, despite its shortcomings2 being widely noted (Brooke and Hirst, 2012a). More recently, T OEFL11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a; Malmasi and Dras, 2014c). Recently, Malmasi and Dras (2014b) introduced the Chinese Learner Corpus for NLI and their results indicate that feature performance may be similar across corpora and even L1L2 pairs. This is a claim that we will test here. NLI is now also moving towards using features to generate SLA hypotheses. Swanson and Charniak (2014) approach this by using both L1 and L2 data to identify features exhibiting non-uniform usage in both datasets, creating lists of candidate transfer features. Malmasi and Dras (2014d) propose a different method, using linear SVM weights to extrac"
N15-1160,W13-1716,1,0.210371,"baseline used by Malmasi et al. (2015). The oracle correctly classifies a text if any ensemble member correctly predicts its label and defines an upper-bound for classification accuracy. We avoid using lexical features as E F C AM DAT is not topic balanced. We extract the following topicindependent feature types: Function words are topic-independent grammatical words such as prepositions which indicate the relations between other words. They are known to be useful for NLI. Frequencies of 400 English function words3 are extracted as features. We also apply function word bigrams as described in Malmasi et al. (2013). Context-free Grammar Production Rules are extracted after parsing each sentence. Each rule is a classification feature (Wong and Dras, 2011) and captures global syntactic patterns. 3 Like previous work, this also includes stop words, which we sourced from the Onix Text Retrieval Toolkit: http://www.lextek.com/manuals/onix/stopwords1.html Confusion Matrix ARA PR CLC 480 ITA 400 JPN 320 KOR 240 SPA Within-Corpus Evaluation Our first experiment applies 10-fold cross-validation within the corpus to assess feature efficacy. The results are shown in the first column of Table 2. All features perfor"
N15-1160,W15-0620,1,0.806188,"e table also indicates the 9 classes common to both corpora. This subset of common classes enables us to perform large-scale cross-corpus validation experiments that have not been possible until now. 4 Methodology We use the standard NLI classification approach. A linear Support Vector Machine is used for classification and feature vectors are created using relative frequency values. We also combine features with a mean probability ensemble classifier (Polikar, 2006, §4.2) using the probabilities assigned to each class. We compare results with a random baseline and the oracle baseline used by Malmasi et al. (2015). The oracle correctly classifies a text if any ensemble member correctly predicts its label and defines an upper-bound for classification accuracy. We avoid using lexical features as E F C AM DAT is not topic balanced. We extract the following topicindependent feature types: Function words are topic-independent grammatical words such as prepositions which indicate the relations between other words. They are known to be useful for NLI. Frequencies of 400 English function words3 are extracted as features. We also apply function word bigrams as described in Malmasi et al. (2013). Context-free Gr"
N15-1160,E09-1070,0,0.0336972,"a claim that we will test here. NLI is now also moving towards using features to generate SLA hypotheses. Swanson and Charniak (2014) approach this by using both L1 and L2 data to identify features exhibiting non-uniform usage in both datasets, creating lists of candidate transfer features. Malmasi and Dras (2014d) propose a different method, using linear SVM weights to extract lists of overused and underused linguistic features for each L1 group. Cross-corpus studies have been conducted for various data-driven NLP tasks, including parsing (Gildea, 2001), WSD (Escudero et al., 2000) and NER (Nothman et al., 2009). While most such experiments show a drop in performance, the effect varies widely across tasks, making it hard to predict the expected drop for NLI. We aim to address this question using large training and testing data. 3 EFCamDat: A new corpus for NLI The EF Cambridge Open Language Database (E F C AM DAT) is an English L2 corpus that was released recently (Geertzen et al., 2013). It is composed of texts submitted to Englishtown, an online school used by thousands of learners daily. This corpus is notable for its size, containing some 550k texts from numerous nationalities, making it an ideal"
N15-1160,E14-4033,0,0.198827,"author based on a second language (L2) text, has received much attention recently. Much of this is motivated by Second Language Acquisition (SLA) as NLI, often accomplished via machine learning methods, can be used to study language transfer effects. Most NLI research hitherto has focused on identifying linguistic phenomena that can capture transfer effects, with little effort towards interpreting discriminant features. Some researchers have now shifted their focus to developing data-driven methods for the automatic extraction and ranking of linguistic features that distinguish specific L1s (Swanson and Charniak, 2014). Such methods could be used not only to confirm existing SLA hypotheses, but also to create new ones. This hypothesis formulation is an inherently Mark Dras Centre for Language Technology Macquarie University Sydney, NSW, Australia mark.dras@mq.edu.au difficult problem requiring copious amounts of data. Contrary to this requirement, researchers have long noted the paucity of suitable corpora1 for this task (Brooke and Hirst, 2011). This is one of the research issues addressed by this work. Furthermore, deriving SLA hypotheses from a single corpus may not be entirely useful for SLA research. M"
N15-1160,C12-1158,0,0.105171,"Missing"
N15-1160,W13-1706,0,0.0749477,"NLI, (2) perform within-corpus evaluation with a comparative analysis against equivalent corpora, (3) perform cross-corpus evaluation to determine the efficiency of corpus independent features and (4) analyze the features’ utility for SLA & ESL research. 2 Background and Motivation NLI work has been growing in recent years, using a wide range of syntactic and more recently, lexical features to distinguish the L1. A detailed review of NLI methods is omitted here for reasons of space, but a thorough exposition is presented in the report from the very first NLI Shared Task that was held in 2013 (Tetreault et al., 2013). Most English NLI work has been done using two corpora. The International Corpus of Learner En1 An ideal NLI corpus should have multiple L1s, be balanced by topic, proficiency, texts per L1 and be large in size. 1403 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1403–1409, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics glish (Granger et al., 2009) was widely used until recently, despite its shortcomings2 being widely noted (Brooke and Hirst, 2012a). More recently, T OEFL11, the first corpus de"
N15-1160,D11-1148,1,0.637281,"Missing"
N18-5012,Y06-1028,0,0.124858,"Missing"
N18-5012,N16-1031,0,0.0230142,"Missing"
N18-5012,P15-1038,0,0.0700121,"Missing"
N18-5012,U16-1017,1,0.928447,"is a need for building an NLP pipeline, such as the Stanford CoreNLP toolkit (Manning et al., 2014), for those key tasks to assist users and to support researchers and tool developers of downstream tasks. Nguyen et al. (2010) and Le et al. (2013) built Vietnamese NLP pipelines by wrapping existing word segmenters and POS taggers including: JVnSegmenter (Nguyen et al., 2006), vnTokenizer (Le et al., 2008), JVnTagger (Nguyen et al., 2010) and vnTagger (Le-Hong et al., 2010). However, these word segmenters and POS taggers are no longer considered SOTA models for Vietnamese (Nguyen and Le, 2016; Nguyen et al., 2016b). • Easy-to-use – All VnCoreNLP components are wrapped into a single .jar file, so users do not have to install external dependencies. Users can run processing pipelines from either the command-line or the Java API. • Fast – VnCoreNLP is fast, so it can be used for dealing with large-scale data. Also it benefits users suffering from limited computation resources (e.g. users from Vietnam). • Accurate – VnCoreNLP components obtain higher results than all previous published results on the same benchmark datasets. 56 Proceedings of NAACL-HLT 2018: Demonstrations, pages 56–60 c New Orleans, Louis"
N18-5012,K17-3014,1,0.886035,"Missing"
N18-5012,Q16-1023,0,0.121699,"Missing"
N18-5012,N16-1030,0,0.0236039,"ns present evaluations for the NER (ner) and dependency parsing (parse) components. 4.1 Models: We make an empirical comparison between the VnCoreNLP’s NER component and the following neural network-based models: • BiLSTM-CRF (Huang et al., 2015) is a sequence labeling model which extends the BiLSTM model with a CRF layer. • BiLSTM-CRF + CNN-char, i.e. BiLSTMCNN-CRF, is an extension of BiLSTM-CRF, using CNN to derive character-based word representations (Ma and Hovy, 2016). • BiLSTM-CRF + LSTM-char is an extension of BiLSTM-CRF, using BiLSTM to derive the character-based word representations (Lample et al., 2016). Named entity recognition We make a comparison between SOTA featurebased and neural network-based models, which, to the best of our knowledge, has not been done in any prior work on Vietnamese NER. • BiLSTM-CRF+POS is another extension to BiLSTM-CRF, incorporating embeddings of automatically predicted POS tags (Reimers and Gurevych, 2017). Dataset: The NER shared task at the 2016 VLSP workshop provides a set of 16,861 manually annotated sentences for training and development, and a set of 2,831 manually annotated sentences for test, with four NER labels PER, LOC, ORG and MISC. Note that in bo"
N18-5012,L18-1410,1,0.651006,"Ông Nguyễn Khắc Chúc đang làm việc tại Đại học Quốc gia Hà Nội.""); pipeline.annotate(annotation); String annotatedStr = annotation. toString(); • wseg – Unlike English where white space is a strong indicator of word boundaries, when written in Vietnamese white space is also used to separate syllables that constitute words. So word segmentation is referred to as the key first step in Vietnamese NLP. We have proposed a transformation rule-based learning model for Vietnamese word segmentation, which obtains better segmentation accuracy and speed than all previous word segmenters. See details in Nguyen et al. (2018). Listing 1: Minimal code for an analysis pipeline. In addition, Listing 2 provides a more realistic and complete example code, presenting key components of the toolkit. Here an annotation pipeline can be used for any text rather than just a single sentence, e.g. for a paragraph or entire news story. 3 Components This section briefly describes each component of VnCoreNLP. Note that our goal is not to develop 57 • pos – To label words with their POS tag, we apply MarMoT which is a generic CRF framework and a SOTA POS and morphological tagger (Mueller et al., 2013).1 gold POS tags are not availa"
N18-5012,U17-1013,1,0.89339,"Missing"
N18-5012,W09-3035,0,0.236133,"Missing"
N18-5012,2010.jeptalnrecital-long.36,0,0.116472,"dependency treebank was published in 2014 (Nguyen et al., 2014); and an NER dataset was released for the second VLSP campaign in 2016. So there is a need for building an NLP pipeline, such as the Stanford CoreNLP toolkit (Manning et al., 2014), for those key tasks to assist users and to support researchers and tool developers of downstream tasks. Nguyen et al. (2010) and Le et al. (2013) built Vietnamese NLP pipelines by wrapping existing word segmenters and POS taggers including: JVnSegmenter (Nguyen et al., 2006), vnTokenizer (Le et al., 2008), JVnTagger (Nguyen et al., 2010) and vnTagger (Le-Hong et al., 2010). However, these word segmenters and POS taggers are no longer considered SOTA models for Vietnamese (Nguyen and Le, 2016; Nguyen et al., 2016b). • Easy-to-use – All VnCoreNLP components are wrapped into a single .jar file, so users do not have to install external dependencies. Users can run processing pipelines from either the command-line or the Java API. • Fast – VnCoreNLP is fast, so it can be used for dealing with large-scale data. Also it benefits users suffering from limited computation resources (e.g. users from Vietnam). • Accurate – VnCoreNLP components obtain higher results than all"
N18-5012,P16-1101,0,0.0801222,"LP) tasks including word segmentation, part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing, and obtains state-of-the-art (SOTA) results for these tasks. We release VnCoreNLP to provide rich linguistic annotations to facilitate research work on Vietnamese NLP. Our VnCoreNLP is open-source and available at: https:// github.com/vncorenlp/VnCoreNLP. 1 Figure 1: In pipeline architecture of VnCoreNLP, annotations are performed on an Annotation object. Pham et al. (2017) built the NNVLP toolkit for Vietnamese sequence labeling tasks by applying a BiLSTM-CNN-CRF model (Ma and Hovy, 2016). However, Pham et al. (2017) did not make a comparison to SOTA traditional feature-based models. In addition, NNVLP is slow with a processing speed at about 300 words per second, which is not practical for real-world application such as dealing with large-scale data. In this paper, we present a Java NLP toolkit for Vietnamese, namely VnCoreNLP, which aims to facilitate Vietnamese NLP research by providing rich linguistic annotations through key NLP components of word segmentation, POS tagging, NER and dependency parsing. Figure 1 describes the overall system architecture. The following items"
N18-5012,P14-5010,0,0.0129689,"tively explored in the last decade, boosted by the successes of the 4-year KC01.01/2006-2010 national project on Vietnamese language and speech processing (VLSP). Over the last 5 years, standard benchmark datasets for key Vietnamese NLP tasks are publicly available: datasets for word segmentation and POS tagging were released for the first VLSP evaluation campaign in 2013; a dependency treebank was published in 2014 (Nguyen et al., 2014); and an NER dataset was released for the second VLSP campaign in 2016. So there is a need for building an NLP pipeline, such as the Stanford CoreNLP toolkit (Manning et al., 2014), for those key tasks to assist users and to support researchers and tool developers of downstream tasks. Nguyen et al. (2010) and Le et al. (2013) built Vietnamese NLP pipelines by wrapping existing word segmenters and POS taggers including: JVnSegmenter (Nguyen et al., 2006), vnTokenizer (Le et al., 2008), JVnTagger (Nguyen et al., 2010) and vnTagger (Le-Hong et al., 2010). However, these word segmenters and POS taggers are no longer considered SOTA models for Vietnamese (Nguyen and Le, 2016; Nguyen et al., 2016b). • Easy-to-use – All VnCoreNLP components are wrapped into a single .jar file,"
N18-5012,I17-3010,0,0.0466753,"ely VnCoreNLP—a Java NLP annotation pipeline for Vietnamese. Our VnCoreNLP supports key natural language processing (NLP) tasks including word segmentation, part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing, and obtains state-of-the-art (SOTA) results for these tasks. We release VnCoreNLP to provide rich linguistic annotations to facilitate research work on Vietnamese NLP. Our VnCoreNLP is open-source and available at: https:// github.com/vncorenlp/VnCoreNLP. 1 Figure 1: In pipeline architecture of VnCoreNLP, annotations are performed on an Annotation object. Pham et al. (2017) built the NNVLP toolkit for Vietnamese sequence labeling tasks by applying a BiLSTM-CNN-CRF model (Ma and Hovy, 2016). However, Pham et al. (2017) did not make a comparison to SOTA traditional feature-based models. In addition, NNVLP is slow with a processing speed at about 300 words per second, which is not practical for real-world application such as dealing with large-scale data. In this paper, we present a Java NLP toolkit for Vietnamese, namely VnCoreNLP, which aims to facilitate Vietnamese NLP research by providing rich linguistic annotations through key NLP components of word segmentat"
N18-5012,P05-1012,0,0.395501,"Missing"
N18-5012,D17-1035,0,0.0299672,"Missing"
N18-5012,D13-1032,0,0.135735,"Missing"
P00-1057,E91-1005,0,0.0110272,"nd l , call the set of nodes dominated by one node but not strictly dominated by the other the site-segment hh ; l i.  Removing a site-segment must not deprive a tree of its foot node. That is, no site-segment hh ; l i may contain a foot node unless l is itself the foot node.  If two tree sets adjoin into the same tree, the two site-segments must be simultaneously removable. That is, the two sitesegments must be disjoint, or one must contain the other. Because of the rst restriction, we depict tree sets with the components connected by a dominance link (dotted line), in the manner of (Becker et al., 1991). As written, the above rules only allow tree-local adjunction; we can generalize them to allow set-local adjunction by treating this dominance link like an ordinary arc. But this would increase the weak generative capacity of the system. For present purposes it is sucient just to allow one type of set-local adjunction: adjoin the upper tree to the upper foot, and the lower tree to the lower root (see Figure 5). This does not increase the weak generative capacity, as will be shown in Section 2.3. Observe that the set-local TAG given in Figure 5 obeys the above restrictions. 2.2 2LTAG For the"
P00-1057,W00-2008,1,0.8006,"ed later. Of course, in our case the stack"" has at most one element. The Pop rule does the reverse: every completed elementary tree set must contain a site-segment, and the Pop rule places it back where the site-segment came from, emptying the stack."" The Pop-push rule performs setlocal adjunction: a completed elementary tree set is placed between the two trees of yet another elementary tree set, and the stack"" is unchanged. Pop-push is computationally the most expensive rule; since it involves six indices and three di erent elementary trees, its running time is O(n6 G3 ). It was noted in (Chiang et al., 2000) that for synchronous RF-2LTAG, parse forests could not be transferred in time O(n6 ). This fact turns out to be connected to several properties of RF-TAG (Rogers, 1994).3 3 Thanks to Anoop Sarkar for pointing out the rst The CKY-style parser for regular form TAG described in (Rogers, 1994) essentially keeps track of adjunctions using stacks, and the regular form constraint ensures that the stack depth is bounded. The only kinds of adjunction that can occur to arbitrary depth are root and foot adjunction, which are treated similarly to substitution and do not a ect the stacks. The reader will"
P00-1057,P99-1011,1,0.850223,"node has Gorn address , then its ith child has Gorn address 2 Figure 6: Adjoining into by removing .  An auxiliary tree may adjoin anywhere.  When a tree is adjoined at a node ,  is rewritten as , and the foot of inherits the label of . The tree set of hG; G0 i, T (hG; G0 i), is f G [T (G0)], where f G is the yield function of G and T (G0 ) is the tree set of G0. Thus, the elementary trees of G0 are combined to form a derived tree, which is then interpreted as a derivation tree for G, which gives instructions for combining elementary trees of G into the nal derived tree. It was shown in Dras (1999) that when the meta-level grammar is in the regular form of Rogers (1994) the formalism is weakly equivalent to TAG. 2.3 Reducing restricted R-MCTAG to RF-2LTAG Consider the case of a multicomponent tree set f 1 ; 2 g adjoining into an initial tree (Figure 6). Recall that we de ned a sitesegment of a pair of adjunction sites to be all the nodes which are dominated by the upper site but not the lower site. Imagine that the site-segment is excised from , and that 1 and 2 are fused into a single elementary tree. Now we can simulate the multi-component adjunction by ordinary adjunction: adjoin the"
P00-1057,P95-1021,0,0.3527,"of tree-local. seem : sleep sleep : S does S likely S .. .. . . VP John VP seem VP seem VP likely VP to sleep S likely : Figure 4: SL-MCTAG generable derivation Unfortunately, unrestricted set-local multicomponent TAGs not only have more derivational generative capacity than TAGs, but they also have more weak generative capacity: SL-MCTAGs can generate the quadruple copy language wwww, for example, which does not correspond to any known linguistic phenomenon. Other formalisms aiming to model dependency correctly similarly expand weak generative capacity, notably D-tree Substitution Grammar (Rambow et al., 1995), and consequently end up with much greater parsing complexity. The work in this paper follows another Figure 5: Set-local adjunction. line of research which has focused on squeezing as much strong generative capacity as possible out of weakly TAG-equivalent formalisms. Tree-local multicomponent TAG (Weir, 1988), nondirectional composition (Joshi and Vijay-Shanker, 1999), and segmented adjunction (Kulick, 2000) are examples of this approach, wherein the constraint on weak generative capacity naturally limits the expressivity of these systems. We discuss the relation of the formalism of this pa"
P00-1057,P94-1022,0,0.333071,": Adjoining into by removing .  An auxiliary tree may adjoin anywhere.  When a tree is adjoined at a node ,  is rewritten as , and the foot of inherits the label of . The tree set of hG; G0 i, T (hG; G0 i), is f G [T (G0)], where f G is the yield function of G and T (G0 ) is the tree set of G0. Thus, the elementary trees of G0 are combined to form a derived tree, which is then interpreted as a derivation tree for G, which gives instructions for combining elementary trees of G into the nal derived tree. It was shown in Dras (1999) that when the meta-level grammar is in the regular form of Rogers (1994) the formalism is weakly equivalent to TAG. 2.3 Reducing restricted R-MCTAG to RF-2LTAG Consider the case of a multicomponent tree set f 1 ; 2 g adjoining into an initial tree (Figure 6). Recall that we de ned a sitesegment of a pair of adjunction sites to be all the nodes which are dominated by the upper site but not the lower site. Imagine that the site-segment is excised from , and that 1 and 2 are fused into a single elementary tree. Now we can simulate the multi-component adjunction by ordinary adjunction: adjoin the fused 1 and 2 into what is left of ; then replace by adjoining it betwee"
P00-1057,1985.tmi-1.17,0,0.0253703,"erence between the two is in the kinds of languages that they are able to describe: DSG is both less and more restrictive than R-MCTAG. DSG can generate the language count-k for some arbitrary k (that is, fa1 n a2 n : : : ak n g), which makes it extremely powerful, whereas R-MCTAG can only generate count-4. However, DSG cannot generate the copy language (that is, fww j w 2 g with  some terminal alphabet), whereas R-MCTAG can; this may be problematic for a formalism modeling natural language, given the key role of the copy language in demonstrating that natural language is not context-free (Shieber, 1985). RMCTAG is thus a more constrained relaxation of the notion of immediate dominance in favor of non-immediate dominance than is the case for DSG. Another formalism of particular interest here is the Segmented Adjoining Grammar of (Kulick, 2000). This generalization of TAG is characterized by an extension of the adjoining operation, motivated by evidence in scrambling, clitic climbing and subject-to-subject raising. Most interestingly, this extension to TAG, proposed on empirical grounds, is de ned by a composition operation with constrained non-immediate dominance links that looks quite simila"
P00-1057,W00-2015,0,\N,Missing
P07-1044,J99-2004,0,0.00523097,", we introduced varying amounts of structure into the generation process. 5.1 Structural Generation Methods PoStag In the first of these, we constructed a rough approximation of typical sentence grammar structure by taking bigrams over part-of-speech tags.6 Then, given a string of PoS tags of length n, t1 . . . tn , we start by assigning the probabilities for the word in position 1, w1 , according to the conditional probability P (w1 |t1 ). Then, for position j (2 ≤ j ≤ n), we assign to candidate words the value P (wj |tj ) × P (wj |wj−1 ) to score word sequences. 6 We used the supertagger of Bangalore and Joshi (1999). 349 So, for example, we might generate the PoS tag template Det NN Adj Adv, take all the words corresponding to each of these parts of speech, and combine bigram word sequence probability with the conditional probability of words with respect to these parts of speech. We then use a Viterbi-style algorithm to find the most likely word sequence. In this model we violate the Markov assumption of independence in much the same way as Witbrock and Mittal (1999) in their combination of content and language model probabilities, by backtracking at every state in order to discourage repeated words and"
P07-1044,W00-1401,0,0.227088,"the letters for correct spelling, good grammar, rhythm and flow, appropriateness of tone, and several other specific characteristics of good text. In terms of automatic evaluation, we are not aware of any technique that measures only fluency or similar characteristics, ignoring content, apart from that of Wan et al. (2005). Even in NLG, where, given the variability of the input representations (and hence difficulty in verifying faithfulness), it might be expected that such measures would be available, the available metrics still conflate content and form. For example, the metrics proposed in Bangalore et al. (2000), such as Simple Accuracy and Generation Accuracy, measure changes with respect to a reference string based on the idea of string-edit distance. Similarly, B LEU has been used in NLG, for example by Langkilde-Geary (2002). 3 Parsers as Evaluators There are three parts to verifying the usefulness of parsers as evaluators: choosing the parsers and the metrics derived from them; generating some texts for human and parser evaluation; and, the key part, getting human judgements on these texts and correlating them with parser metrics. 1 http://projects.ldc.upenn.edu/TIDES/ Translation/TranAssessSpec"
P07-1044,E06-1032,0,0.00508865,"xt Generation Methods The method used to generate text in Section 3.2 is a variation of the standard n-gram language model. A question that arises is: Are any of the metrics defined above strongly influenced by the type of language model used to generate the text? It may be the case, for example, that a parser implementation uses its own language model that predisposes it to favour a similar model in the text generation process. This is a phenomenon seen in MT, where B LEU seems to favour text that has been produced using a similar statistical n-gram language model over other symbolic models (Callison-Burch et al., 2006). Our previous approach used only sequences of words concatenated together. To define some new methods for generating text, we introduced varying amounts of structure into the generation process. 5.1 Structural Generation Methods PoStag In the first of these, we constructed a rough approximation of typical sentence grammar structure by taking bigrams over part-of-speech tags.6 Then, given a string of PoS tags of length n, t1 . . . tn , we start by assigning the probabilities for the word in position 1, w1 , according to the conditional probability P (w1 |t1 ). Then, for position j (2 ≤ j ≤ n),"
P07-1044,1995.iwpt-1.15,0,0.0668745,"parsers as evaluators: choosing the parsers and the metrics derived from them; generating some texts for human and parser evaluation; and, the key part, getting human judgements on these texts and correlating them with parser metrics. 1 http://projects.ldc.upenn.edu/TIDES/ Translation/TranAssessSpec.pdf 3.1 The Parsers In testing the idea of using parsers to judge fluency, we use three parsers, from which we derive four parser metrics, to investigate the general applicability of the idea. Those chosen were the Connexor parser,2 the Collins parser (Collins, 1999), and the Link Grammar parser (Grinberg et al., 1995). Each produces output that can be taken as representing degree of ungrammaticality, although this output is quite different for each. Connexor is a commercially available dependency parser that returns head–dependant relations as well as stemming information, part of speech, and so on. In the case of an ungrammatical sentence, Connexor returns tree fragments, where these fragments are defined by transitive head–dependant relations: for example, for the sentence Everybody likes big cakes do it returns fragments for Everybody likes big cakes and for do. We expect that the number of fragments sh"
P07-1044,J00-4006,0,0.00141925,"echnologies can be characterised as having at least two aspects: how well the generated text reflects the source data, whether it be text in another language for machine translation (MT), a natural language generation (NLG) input representation, a document to be summarised, and so on; and how well it conforms to normal human language usage. These two aspects are often made explicit in approaches to creating the text. For example, in statistical MT the translation model and the language model are treated separately, characterised as faithfulness and fluency respectively (as in the treatment in Jurafsky and Martin (2000)). Similarly, the ultrasummarisation model of Witbrock and Mittal (1999) consists of a content model, modelling the probability that a word in the source text will be in the summary, and a language model. Evaluation methods can be said to fall into two categories: a comparison to gold reference, or an appeal to human judgements. Automatic evaluation methods carrying out a comparison to gold reference tend to conflate the two aspects of faithfulness and fluency in giving a goodness score for generated output. B LEU (Papineni et al., 2002) is a canonical example: in matching n-grams in a candida"
P07-1044,2004.tmi-1.8,0,0.0447561,"o use something like these as indicators of generated sentence fluency. The aim of the next section is to build a better predictor than the individual parser metrics alone. Metric Humans Collins Parser Connexor Link Grammar Nulled Tokens Link Grammar Invalid Parses G LEU Corr. 0.6529 0.4057 -0.3804 -0.3310 0.1619 0.4606 In MT, one problem with most metrics like B LEU is that they are intended to apply only to documentlength texts, and any application to individual sentences is inaccurate and correlates poorly with human judgements. A neat solution to poor sentence-level evaluation proposed by Kulesza and Shieber (2004) is to use a Support Vector Machine, using features such as word error rate, to estimate sentence-level translation quality. The two main insights in applying SVMs here are, first, noting that human translations are generally good and machine translations poor, that binary training data can be created by taking the human translations as positive training instances and machine translations as negative ones; and second, that a non-binary metric of translation goodness can be derived by the distance from a test instance to the support vectors. In an empirical evaluation, Kulesza and Shieber found"
P07-1044,W02-2103,0,0.0171253,"easures only fluency or similar characteristics, ignoring content, apart from that of Wan et al. (2005). Even in NLG, where, given the variability of the input representations (and hence difficulty in verifying faithfulness), it might be expected that such measures would be available, the available metrics still conflate content and form. For example, the metrics proposed in Bangalore et al. (2000), such as Simple Accuracy and Generation Accuracy, measure changes with respect to a reference string based on the idea of string-edit distance. Similarly, B LEU has been used in NLG, for example by Langkilde-Geary (2002). 3 Parsers as Evaluators There are three parts to verifying the usefulness of parsers as evaluators: choosing the parsers and the metrics derived from them; generating some texts for human and parser evaluation; and, the key part, getting human judgements on these texts and correlating them with parser metrics. 1 http://projects.ldc.upenn.edu/TIDES/ Translation/TranAssessSpec.pdf 3.1 The Parsers In testing the idea of using parsers to judge fluency, we use three parsers, from which we derive four parser metrics, to investigate the general applicability of the idea. Those chosen were the Conne"
P07-1044,P02-1040,0,0.111683,"ess and fluency respectively (as in the treatment in Jurafsky and Martin (2000)). Similarly, the ultrasummarisation model of Witbrock and Mittal (1999) consists of a content model, modelling the probability that a word in the source text will be in the summary, and a language model. Evaluation methods can be said to fall into two categories: a comparison to gold reference, or an appeal to human judgements. Automatic evaluation methods carrying out a comparison to gold reference tend to conflate the two aspects of faithfulness and fluency in giving a goodness score for generated output. B LEU (Papineni et al., 2002) is a canonical example: in matching n-grams in a candidate translation text with those in a reference text, the metric measures faithfulness by counting the matches, and fluency by implicitly using the reference n-grams as a language model. Often we are interested in knowing the quality of the two aspects separately; many human judgement frameworks ask specifically for separate judgements on elements of the task that correspond to faithfulness and to fluency. In addition, the need for reference texts for an evaluation metric can be problematic, and intuitively seems unnecessary for characteri"
P07-1044,2003.mtsummit-papers.51,0,0.0346475,"nerally good and machine translations poor, that binary training data can be created by taking the human translations as positive training instances and machine translations as negative ones; and second, that a non-binary metric of translation goodness can be derived by the distance from a test instance to the support vectors. In an empirical evaluation, Kulesza and Shieber found that their SVM gave a correlation of 0.37, which was an improvement of around half the gap between the B LEU correlations with the human judgements (0.25) and the lowest pairwise human inter-judge correlation (0.46) (Turian et al., 2003). We take a similar approach here, using as features the four parser metrics described in Section 3. We trained an SVM,4 taking as positive training data the 1000 instances of sentences of sequence length 24 (i.e. sentences extracted from the corpus) and as negative training data the 1000 sentences of sequence length 1. We call this learner G LEU. 5 As a check on the ability of the G LEU SVM to distinguish these ‘positive’ sentences from ‘negative’ ones, we evaluated its classification accuracy on a (new) test set of size 300, split evenly between sentences of sequence length 24 and sequence l"
P07-1044,W05-1628,1,0.8657,"d using their own language model, rather than human-authored sentences already existing in a test corpus; and so it is not obvious what language model would be an objective assessment of sentence naturalness. In the case of evaluating a single system, using the language model that generated the sentence will only confirm that the sentence does fit the language model; in situations such as comparing two systems which each generate text using a different language model, it is not obvious that there is a principled way of deciding on a fair language model. Quite a different idea was suggested in Wan et al. (2005), of using the grammatical judgement of a parser to assess fluency, giving a measure independent of the language model used to generate the text. The idea is that, assuming the parser has been trained on an appropriate corpus, the poor performance of the parser on one sentence relative to another might be an indicator of some degree of ungrammaticality and possibly disfluency. In that work, however, correlation with human judgements was left uninvestigated. The goal of this paper is to take this idea and develop it. In Section 2 we look at some related work on metrics, in particular for NLG. I"
P07-1044,C96-1043,0,\N,Missing
P07-1044,J03-4003,0,\N,Missing
P11-2067,P05-1066,0,0.861461,"ly investigate possible factors to give an initial answer to the question: Under what conditions does this use of syntax help PSMT? 1 Introduction Phrase-based statistical machine translation (PSMT) translates documents from one human language to another by dividing text into contiguous sequences of words (phrases), translating each, and finally reordering them according to a distortion model. The PSMT distortion model typically does not consider linguistic information, and as such encounters difficulty in language pairs that require specific long-distance reorderings, such as German–English. Collins et al. (2005) address this problem by reordering German sentences to more closely parallel English word order, prior to translation by a PSMT system. They find that this reordering-aspreprocessing approach results in a significant improvement in translation performance over the baseline. However, there have been several other systems using the reordering-as-preprocessing approach, and they have met with mixed success. We systematically explore possible explanations for these contradictory results, and conclude that, while reordering is helpful for some sentences, potential improvement can be eroded by many"
P11-2067,P03-1013,0,0.0720245,"Missing"
P11-2067,2007.mtsummit-papers.29,0,0.498806,"Missing"
P11-2067,W08-0510,0,0.0249512,"em reuses the recasing and language models of its corresponding baseline system, to eliminate one source of possible variation. Training the parser with less data affects only the reordered systems; for experiments using these models, the corresponding baselines (and thus the shared models) are not retrained. For each system pair, we also run the HD oracle. System Variations CKK uses the PSMT system Pharaoh (Koehn et al., 2003), whereas HD uses its successor Moses (Koehn et al., 2007). In itself, this should not cause a dramatic difference in performance, as the two systems perform similarly (Hoang and Koehn, 2008). However, there are a number of other differences between the two systems. Koehn et al. (2003) (and thus presumably CKK) use an unlexicalised distortion model, whereas HD uses a lexicalised model. CKK does not include a tuning (minimum error rate training) phase, unlike HD. Finally, HD uses a 5gram language model. The CKK language model is unspecified; we assume a 3-gram model would be 1 Reord. 26.73 26.63 Diff. +1.15 +1.05 Oracle 28.11 28.03 more likely for the time. We explore combinations of all these choices. 4.2 additional models trained by the same method. The first is trained on the sa"
P11-2067,U10-1007,1,0.79659,"antial before it outweighs other factors. In all cases, the oracle outperforms both baseline and reordered systems by a large margin. Its selections show that, in changing test sets, the balance shifts from one system to the other, but both still contribute strongly. This shows that improvements are possible across the board if it is possible to correctly choose which sentences will benefit from reordering. 6 Conclusion Collins et al. (2005) reported that a reorderingas-preprocessing approach improved overall performance in German-to-English translation. The reimplementation of this system by Howlett and Dras (2010) came to the opposite conclusion. We have systematically varied several aspects of the Howlett and Dras (2010) system and reproduced results close to both papers, plus a full range in between. Our results show that choices in the PSMT system can completely erode potential gains of the reordering preprocessing step, with the largest effect due to simple choice of data. We have shown that a lack of overall improvement using reordering-aspreprocessing need not be due to the usual suspects, language pair and reordering process. Significantly, our oracle experiments show that in all cases the reord"
P11-2067,N03-1017,0,0.0099742,"uct a number of experiments with the HD system to attempt to replicate the CKK and HD findings. All parts of the system are available online.1 Each experiment is paired: the reordered system reuses the recasing and language models of its corresponding baseline system, to eliminate one source of possible variation. Training the parser with less data affects only the reordered systems; for experiments using these models, the corresponding baselines (and thus the shared models) are not retrained. For each system pair, we also run the HD oracle. System Variations CKK uses the PSMT system Pharaoh (Koehn et al., 2003), whereas HD uses its successor Moses (Koehn et al., 2007). In itself, this should not cause a dramatic difference in performance, as the two systems perform similarly (Hoang and Koehn, 2008). However, there are a number of other differences between the two systems. Koehn et al. (2003) (and thus presumably CKK) use an unlexicalised distortion model, whereas HD uses a lexicalised model. CKK does not include a tuning (minimum error rate training) phase, unlike HD. Finally, HD uses a 5gram language model. The CKK language model is unspecified; we assume a 3-gram model would be 1 Reord. 26.73 26.6"
P11-2067,P07-2045,0,0.00809034,"to replicate the CKK and HD findings. All parts of the system are available online.1 Each experiment is paired: the reordered system reuses the recasing and language models of its corresponding baseline system, to eliminate one source of possible variation. Training the parser with less data affects only the reordered systems; for experiments using these models, the corresponding baselines (and thus the shared models) are not retrained. For each system pair, we also run the HD oracle. System Variations CKK uses the PSMT system Pharaoh (Koehn et al., 2003), whereas HD uses its successor Moses (Koehn et al., 2007). In itself, this should not cause a dramatic difference in performance, as the two systems perform similarly (Hoang and Koehn, 2008). However, there are a number of other differences between the two systems. Koehn et al. (2003) (and thus presumably CKK) use an unlexicalised distortion model, whereas HD uses a lexicalised model. CKK does not include a tuning (minimum error rate training) phase, unlike HD. Finally, HD uses a 5gram language model. The CKK language model is unspecified; we assume a 3-gram model would be 1 Reord. 26.73 26.63 Diff. +1.15 +1.05 Oracle 28.11 28.03 more likely for the"
P11-2067,P02-1040,0,0.0807985,"a news test set, the reordered system performed significantly better than the baseline regardless of the maximum length of phrases. However, this improvement was only apparent with monotonic decoding; when using a distortion model, the difference disappeared. Xia and McCord attribute the drop-off in performance on the Hansard set to similarity of training and test data. Collins et al. (2005) (German-to-English) use six hand-crafted reordering rules targeting the placement of verbs, subjects, particles and negation. They train and evaluate their system on Europarl text and obtain a BLEU score (Papineni et al., 2002) of 26.8, with the baseline PSMT system achieving 25.2. A human evaluation confirms that reordered translations are generally (but not universally) better. On Web text, Xu et al. (2009) report significant improvements applying one set of hand-crafted rules to translation from English to each of five SOV lan384 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 384–388, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics guages: Korean, Japanese, Hindi, Urdu and Turkish. Training on news text, Wang et al. ("
P11-2067,W08-1005,0,0.0199143,"ses the Berkeley parser (Petrov et al., 2006), trained on Negra’s successor, the larger Tiger corpus (Brants et al., 2002). Refer to Table 1 for precision and recall for each model. Note that the CKK reordering requires not just category labels (e.g. NP) but also function labels (e.g. SB for subject); parser performance typically goes down when these are learnt, due to sparsity. All models in Table 1 include function labels. Dubey and Keller (2003) train and test on the Negra corpus, with 18,602 sentences for training, 1,000 development and 1,000 test, removing sentences longer than 40 words. Petrov and Klein (2008) train and test the Berkeley parser on part of the Tiger corpus, with 20,894 sentences for training and 2,611 sentences for each of development and test, all at most 40 words long. The parsing model used by HD is trained on the full Tiger corpus, unrestricted for length, with 38,020 sentences for training and 2,000 sentences for development. The figures reported in Table 1 are the model’s performance on this development set. With twice as much data, the increase in performance is unsurprising. From these figures, we conclude that sheer parser grunt is unlikely to be responsible for the discrep"
P11-2067,P06-1055,0,0.0600351,"Missing"
P11-2067,A97-1014,0,0.150629,"Missing"
P11-2067,D07-1077,0,0.189153,"Missing"
P11-2067,C04-1073,0,0.109401,"ordering is helpful for some sentences, potential improvement can be eroded by many aspects of the PSMT system, independent of the reordering. Prior Work Reordering-as-preprocessing systems typically involve three steps. First, the input sentence is parsed. Second, the parse is used to permute the words according to some reordering rules, which may be automatically or manually determined. Finally, a phrase-based SMT system is trained and tested using the reordered sentences as input, in place of the original sentences. Many such systems exist, with results being mixed; we review several here. Xia and McCord (2004) (English-to-French translation, using automatically-extracted reordering rules) train on the Canadian Hansard. On a Hansard test set, an improvement over the baseline was only seen if the translation system’s phrase table was restricted to phrases of length at most four. On a news test set, the reordered system performed significantly better than the baseline regardless of the maximum length of phrases. However, this improvement was only apparent with monotonic decoding; when using a distortion model, the difference disappeared. Xia and McCord attribute the drop-off in performance on the Hans"
P11-2067,N09-1028,0,0.0702201,"decoding; when using a distortion model, the difference disappeared. Xia and McCord attribute the drop-off in performance on the Hansard set to similarity of training and test data. Collins et al. (2005) (German-to-English) use six hand-crafted reordering rules targeting the placement of verbs, subjects, particles and negation. They train and evaluate their system on Europarl text and obtain a BLEU score (Papineni et al., 2002) of 26.8, with the baseline PSMT system achieving 25.2. A human evaluation confirms that reordered translations are generally (but not universally) better. On Web text, Xu et al. (2009) report significant improvements applying one set of hand-crafted rules to translation from English to each of five SOV lan384 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 384–388, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics guages: Korean, Japanese, Hindi, Urdu and Turkish. Training on news text, Wang et al. (2007) (Chinese-to-English, hand-crafted rules) report a significant improvement over the baseline system on the NIST 2006 test set, using a distance-based distortion model. Similar resu"
P11-2067,2007.mtsummit-papers.74,1,0.879233,"Missing"
P17-1134,P15-2082,0,0.030673,"e as an example the case that in Hebrew there are seven synonyms for the word fear, and that different authors may choose consistently from among them. Then, having constructed their own synsets using available biblical resources and annotations, they represent texts by vectors of synonyms and apply a modified cosine similarity measure to compare and cluster these vectors. While the general task is relevant to this paper, the particular notion of synonymy here means the approach is specific to this problem, although their approach is extended to other kinds of text in Akiva and Koppel (2013). Aldebei et al. (2015) proposed a new approach motivated by this work, similarly clustering sentences, then using a Naive Bayes classifier with modified prior probabilities to classify sentences. Poetry Voice Detection Brooke et al. (2012) perform stylistic segmentation of a well-known poem, The Waste Land by T.S. Eliot. This poem is renowned for the great number of voices that appear throughout the text and has been the subject of much literary analysis (Bedient and Eliot, 1986; Cooper, 1987). These distinct voices, conceived of as representing different characters, have differing tones, lexis and grammatical styl"
P17-1134,N10-1083,0,0.0984604,"Missing"
P17-1134,W12-2504,0,0.33007,"our task, indicating its difficulty, a Bayesian model that incorporates appropriately compact language models and alternating asymmetric priors can achieve scores on the standard metrics around halfway to perfect segmentation. 1 Introduction Most work on automatically segmenting text has been on the basis of topic: segment boundaries correspond to topic changes (Hearst, 1997). There are various contexts, however, in which it is of interest to identify changes in other characteristics; for example, there has been work on identifying changes in authorship (Koppel et al., 2011) and poetic voice (Brooke et al., 2012). In this paper we investigate text segmentation on the basis of change in the native language of the writer. Two illustrative contexts where this task might be of interest are patchwriting detection and literary analysis. Patchwriting is the heavy use of text from a different source with some modification and insertion of additional words and sentences to form a new text. Pecorari (2003) notes that this is a kind of textual plagiarism, but is a strategy for learning to write in an appropriate language and style, rather than for deception. Keck (2006), Gilmore et al. (2010) and Vieyra et al. ("
P17-1134,W13-1406,0,0.0505659,"Missing"
P17-1134,C14-1185,0,0.247824,"Missing"
P17-1134,N13-1019,1,0.877831,"lexical cohesion — expressed in this context as topic segments having compact and consistent lexical distributions — and implements this within a probabilistic framework by modelling words within each segment as draws from a multinomial language model associated with that segment. Much other subsequent work either uses this as a baseline, or extends it in some way: Jeong and Titov (2010), for example, who propose a model for joint discourse segmentation and alignment for documents with parallel structures, such as a text with commentaries or presenting alternative views on the same topic; or Du et al. (2013), who use hierarchical topic structure to improve the linear segmentation. Bible Authorship Koppel et al. (2011) consider the task of decomposing a document into its authorial components based on their stylistic properties and propose an unsupervised method for doing so. The authors use as their data two biblical books, Jeremiah and Ezekiel, that are generally believed to be single-authored: their task was to segment a single artificial text constructed by interleaving chapters of the two books. Their most successful method used work in biblical scholarship on lexical choice: they give as an e"
P17-1134,N09-1040,0,0.176558,"hat incorporating knowledge about which features are associated with which L1 could potentially help improve the results. One approach to do this is the use of asymmetric priors. We note that features associated with an L1 often dominate in a segment. Accordingly, priors can represent evidence external to the data that some some aspect should be weighted more strongly: for us, this is evidence from the NLI classification task. The segmentation models discussed so far only make use of a symmetric prior but later work mentions that it would be possible to modify this to use an asymmetric prior (Eisenstein, 2009). Given that priors are effective for incorporating external information, recent work has highlighted the importance of optimizing over such priors, and in particular, the use of asymmetric priors. Key work on this is by Wallach et al. (2009) on LDA, who report that “an asymmetric Dirichlet prior over the document-topic distributions has substantial advantages over a symmetric prior”, 1462 with prior values being determined through hyperparameter optimization. Such methods have since been applied in other tasks such as sentiment analysis (Lin and He, 2009; Lin et al., 2012) to achieve substant"
P17-1134,D08-1035,0,0.195164,"ns, sequences of related words (defined via thesaurus), to break up a text into topical segments: breaks in lexical chains indicate breaks in topic. The TextTiling algorithm (Hearst, 1994, 1997) took a related approach, defining a function over lexical frequency and distribution information to determine topic boundaries, and assuming that each topic has its own vocabulary and that large shifts in this vocabulary usage correspond to topic shifts. There have been many approaches since that time. A key one, which is the basis for our own work, is the unsupervised Bayesian technique BAYES S EG of Eisenstein and Barzilay (2008), based on a generative model that assumes that each segment has its own language model. Under this assumption the task can be framed as predicting boundaries at points which maximize the probability of a text being generated by a given language model. Their method is based on lexical cohesion — expressed in this context as topic segments having compact and consistent lexical distributions — and implements this within a probabilistic framework by modelling words within each segment as draws from a multinomial language model associated with that segment. Much other subsequent work either uses t"
P17-1134,W13-1729,0,0.0166827,"ate the importance of inducing a compact distribution, which we did here by reducing the vocabulary size by stripping non-informative features. 5.4 Applying Two Asymmetric Priors Our final model, L1S EG -A SYM P, assesses whether setting different priors for each L1 can improve performance. Our grid search over two priors gives 900 possible prior combinations. These combinations also include cases where θa and θb are symmetric, which is equivalent to the L1S EG -C OMP model. We observe (Table 1) that 8 In §2 we noted the comparison of PTB and CLAWS2 tagsets in Malmasi and Cahill (2015); also, Gyawali et al. (2013) compared Penn Treebank and Universal POS tagsets and found that the more fine-grained PTB ones did better. 1464 L1 A native L1s. There isn’t yet a topic-balanced corpus like T OEFL11 which includes native speaker writing for evaluation, although we expect (given recent results on distinguishing native from nonnative text in Malmasi and Dras (2015)) that the techniques should carry over. For the literary analysis, as well, to bridge the gap between work like Morzinski (1994) and a computational application, it remains to be seen how precise an annotation is possible for this task. Additionally"
P17-1134,P94-1002,0,0.847302,"esion (Halliday and Hasan, 1976) is an important concept here: the principle that text is not formed by a random set of words and sentences but rather logically ordered sets of related words that together form a topic. In addition to the semantic relation between words, other methods such as back-references and conjunctions also help achieve cohesion. Based on this, Morris and Hirst (1991) proposed the use of lexical chains, sequences of related words (defined via thesaurus), to break up a text into topical segments: breaks in lexical chains indicate breaks in topic. The TextTiling algorithm (Hearst, 1994, 1997) took a related approach, defining a function over lexical frequency and distribution information to determine topic boundaries, and assuming that each topic has its own vocabulary and that large shifts in this vocabulary usage correspond to topic shifts. There have been many approaches since that time. A key one, which is the basis for our own work, is the unsupervised Bayesian technique BAYES S EG of Eisenstein and Barzilay (2008), based on a generative model that assumes that each segment has its own language model. Under this assumption the task can be framed as predicting boundarie"
P17-1134,J97-1003,0,0.895698,"er, stylistically expressed characteristics such as change of authorship or native language. We propose a Bayesian unsupervised text segmentation approach to the latter. While baseline models achieve essentially random segmentation on our task, indicating its difficulty, a Bayesian model that incorporates appropriately compact language models and alternating asymmetric priors can achieve scores on the standard metrics around halfway to perfect segmentation. 1 Introduction Most work on automatically segmenting text has been on the basis of topic: segment boundaries correspond to topic changes (Hearst, 1997). There are various contexts, however, in which it is of interest to identify changes in other characteristics; for example, there has been work on identifying changes in authorship (Koppel et al., 2011) and poetic voice (Brooke et al., 2012). In this paper we investigate text segmentation on the basis of change in the native language of the writer. Two illustrative contexts where this task might be of interest are patchwriting detection and literary analysis. Patchwriting is the heavy use of text from a different source with some modification and insertion of additional words and sentences to"
P17-1134,D14-1142,0,0.143057,"Missing"
P17-1134,P10-2028,0,0.0212724,"t each segment has its own language model. Under this assumption the task can be framed as predicting boundaries at points which maximize the probability of a text being generated by a given language model. Their method is based on lexical cohesion — expressed in this context as topic segments having compact and consistent lexical distributions — and implements this within a probabilistic framework by modelling words within each segment as draws from a multinomial language model associated with that segment. Much other subsequent work either uses this as a baseline, or extends it in some way: Jeong and Titov (2010), for example, who propose a model for joint discourse segmentation and alignment for documents with parallel structures, such as a text with commentaries or presenting alternative views on the same topic; or Du et al. (2013), who use hierarchical topic structure to improve the linear segmentation. Bible Authorship Koppel et al. (2011) consider the task of decomposing a document into its authorial components based on their stylistic properties and propose an unsupervised method for doing so. The authors use as their data two biblical books, Jeremiah and Ezekiel, that are generally believed to"
P17-1134,P06-1004,0,0.539462,"j} |θ0 ) = Segmentation Models For all of our segmentation we use as a starting point the unsupervised Bayesian method of Eisenstein and Barzilay (2008); see §2.3 We recap the important technical definitions here. In Equation 1 of their work they define the observation likelihood as, p(X |z, Θ) = T Y t p(xt |θzt ), (1) where X is the set of all T sentences, z is the vector of segment assignments for each sentence, xt is the bag of words drawn from the language model and Θ is the set of all K language models Θ1 . . . ΘK . As is standard in segmentation work, K is assumed to be fixed and known (Malioutov and Barzilay, 2006); it is set to the actual number of segments. The authors also impose an additional constraint, that zt must be equal to either zt−1 (the previous sentence’s segment) or zt−1 + 1 (the next segment), in order to ensure a linear segmentation. This segmentation model has two parameters: the set of language models Θ and the segment assignment indexes z. The authors note that since this task is only concerned with the segment assignments, searching in the space of language models is not desirable. They offer two alternatives to overcome this: (1) taking point estimates of the language models, which"
P17-1134,W15-0606,1,0.938516,"of 5 segments each for the single L1 pair, and from all language pairs (L1S EG -PTB-A LL D EV, L1S EG -PTB-A LL -T EST). We would expect that these datasets should not be segmentable by topic, as all the segments are on the same topic; the segments should however, differ in stylistic characteristics related to the L1. L1S EG -CLAWS2 This dataset is generated using the same methodology as L1S EG -PTB, with the exception that the essays are tagged using the RASP tagger which uses the more fine-grained CLAWS2 tagset, noting that the CLAWS2 tagset performed better in the NLI classification task (Malmasi and Cahill, 2015). 3.3 Evaluation We use the standard Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) metrics, which (broadly speaking) select sentences using a moving window of size k and determines whether these sentences correctly or incorrectly fall into the same or different reference segmentations. Pk and WD scores range between 0 and 1, with a lower score indicating better performance, and 0 a perfect segmentation. It has been noted that some “degenerate” algorithms — such as placing boundaries randomly or at every possible position — can score 0.5 (Pevzner and Hearst, 2002)."
P17-1134,D14-1144,1,0.910428,"Missing"
P17-1134,J91-1002,0,0.270624,"ation based on authorial characteristics, applied to native language. 2 Related Work Topic Segmentation The most widelyresearched text segmentation task has as its goal to divide a text into topically coherent segments. Lexical cohesion (Halliday and Hasan, 1976) is an important concept here: the principle that text is not formed by a random set of words and sentences but rather logically ordered sets of related words that together form a topic. In addition to the semantic relation between words, other methods such as back-references and conjunctions also help achieve cohesion. Based on this, Morris and Hirst (1991) proposed the use of lexical chains, sequences of related words (defined via thesaurus), to break up a text into topical segments: breaks in lexical chains indicate breaks in topic. The TextTiling algorithm (Hearst, 1994, 1997) took a related approach, defining a function over lexical frequency and distribution information to determine topic boundaries, and assuming that each topic has its own vocabulary and that large shifts in this vocabulary usage correspond to topic shifts. There have been many approaches since that time. A key one, which is the basis for our own work, is the unsupervised"
P17-1134,J02-1002,0,0.450824,"L -T EST). We would expect that these datasets should not be segmentable by topic, as all the segments are on the same topic; the segments should however, differ in stylistic characteristics related to the L1. L1S EG -CLAWS2 This dataset is generated using the same methodology as L1S EG -PTB, with the exception that the essays are tagged using the RASP tagger which uses the more fine-grained CLAWS2 tagset, noting that the CLAWS2 tagset performed better in the NLI classification task (Malmasi and Cahill, 2015). 3.3 Evaluation We use the standard Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) metrics, which (broadly speaking) select sentences using a moving window of size k and determines whether these sentences correctly or incorrectly fall into the same or different reference segmentations. Pk and WD scores range between 0 and 1, with a lower score indicating better performance, and 0 a perfect segmentation. It has been noted that some “degenerate” algorithms — such as placing boundaries randomly or at every possible position — can score 0.5 (Pevzner and Hearst, 2002). WD scores are typically similar to Pk , correcting for differential penalties between false positive boundaries"
P17-1134,N13-1009,0,0.041223,"Missing"
P17-1134,W13-1706,0,0.105765,"Missing"
P17-1134,C12-1158,0,0.107886,"Missing"
P17-1134,D12-1064,1,0.884455,"Missing"
P17-2063,hughes-etal-2006-reconsidering,0,0.211293,"LID and holds great promise for future work in this area. 1 Mark Dras Macquarie University Sydney, Australia mark.dras@mq.edu.au Introduction Language Identification (LID) is the task of determining the language of a text, at the document, sub-document or even sentence level. LID is a fundamental preprocessing task in NLP and is also used in lexicography, machine translation and information retrieval. It is widely used for filtering to select documents in a specific language; e.g. LID can filter webpages or tweets by language. Although LID has been widely studied, several open issues remain (Hughes et al., 2006). Current goals include developing models that can identify thousands of languages; extending the task to more fine-grained dialect identification; and making LID functionality more readily available to users/developers. A common challenge among these goals is dealing with high dimensional feature spaces. LID differs from traditional text categorization tasks in some important aspects. Standard tasks, such as topic classification, are usually 399 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 399–403 c Vancouver, Canada, July 30 -"
P17-2063,W15-5407,1,0.871205,"Missing"
P17-2063,W16-4801,1,0.852423,"Missing"
P17-2063,D14-1069,0,0.0621414,"Missing"
P18-2072,D17-1156,0,0.0582927,"Missing"
P18-2072,H01-1052,0,0.176018,"ects in a document classification task. We believe this is an important first step in developing methods for estimating the resources required to meet specific engineering performance targets. 1 Introduction An engineering discipline should be able to predict the cost of a project before the project is started. Because training data is often the most expensive part of an NLP or ML project, it is important to estimate how much training data required for a system to achieve a target accuracy. Unfortunately our field only offers fairly impractical advice, e.g., that more data increases accuracy (Banko and Brill, 2001); we currently have no practical methods for estimating how much data or what quality of data is required to achieve a target accuracy goal. Imagine if bridge construction was planned the way we build our systems! 2 Related work Power analysis (Cohen, 1992) is widely-used statistical technique (e.g., in biomedical trials) for predicting the number of measurements required in an experimental design; we aim to develop sim450 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 450–455 c Melbourne, Australia, July 15 - 20, 2018. 2018 Associ"
P97-1070,C90-3001,0,0.234823,"Missing"
P97-1070,C96-2183,0,0.0245516,"in terms of a mapping between syntax trees describing each of the paraphrase alternatives--have been chosen for their general applicability. Three examples are: (1) a. b. (2) a. b. (3) a. b. The salesman made an attempt to wear Steven down. The salesman attempted to wear Steven down. The compere who put the contestant to the lie detector gained the cheers of the audience. The compere put the contestant to the lie detector test. He gained the cheers of the audience. The smile broke his composure. His composure was broken by the smile. A possible approach for representing paraphrases is that of Chandrasekar et al (1996) in the context of text simplification. This involves a fairly straightforward representation, as the focus is on paraphrases which simplify sentences by breaking them apart. However, for purposes other than sentence simplification, where paraphrases like (1) are used, a more complex representation is needed. 516 A paraphrase representation can be thought of as comprising two p a r t s - - a representation for each of the source and target texts, and a representation for mapping between them. Tree Adjoining Grammars (TAGs) cover the first part: as a formalism for describing the syntactic aspec"
P97-1070,C94-2149,0,0.0131121,"ke (1) are used, a more complex representation is needed. 516 A paraphrase representation can be thought of as comprising two p a r t s - - a representation for each of the source and target texts, and a representation for mapping between them. Tree Adjoining Grammars (TAGs) cover the first part: as a formalism for describing the syntactic aspects of text, they have a number of desirable features. The properties of the formalism are well established (Joshi et al, 1975), and the research has also led to the development of a large standard grammar (XTAG Research Group, 1995), and a parser XTAG (Doran et al, 1994). Mapping between source and target texts is achieved by an extension to the TAG formalism known as Synchronous TAG, introduced by Shieber and Schabes (1990). Synchronous TAGs (STAGs) comprise a pair of trees plus links between nodes of the trees. The original paper of Shieber and Schabes proposed using STAGs to map from a syntactic to a semantic representation, while another paper by Abeill@ (1990) proposed their use in machine translation. The use in machine translation is quite close to the use proposed here, hence the comparison in the following section; instead of mapping between possibly"
P97-1070,C90-3045,0,0.10331,"ntation for each of the source and target texts, and a representation for mapping between them. Tree Adjoining Grammars (TAGs) cover the first part: as a formalism for describing the syntactic aspects of text, they have a number of desirable features. The properties of the formalism are well established (Joshi et al, 1975), and the research has also led to the development of a large standard grammar (XTAG Research Group, 1995), and a parser XTAG (Doran et al, 1994). Mapping between source and target texts is achieved by an extension to the TAG formalism known as Synchronous TAG, introduced by Shieber and Schabes (1990). Synchronous TAGs (STAGs) comprise a pair of trees plus links between nodes of the trees. The original paper of Shieber and Schabes proposed using STAGs to map from a syntactic to a semantic representation, while another paper by Abeill@ (1990) proposed their use in machine translation. The use in machine translation is quite close to the use proposed here, hence the comparison in the following section; instead of mapping between possibly different trees in different languages, there is a mapping between trees in the same language with very different syntactic properties. 2 Paraphrasing with"
P97-1070,W90-0102,0,\N,Missing
P99-1011,C90-3001,0,0.504859,"Missing"
P99-1011,P97-1070,1,0.848518,"Missing"
P99-1011,P98-1106,0,0.0262621,"k continues in the area, for example using S-TAG for English-Korean machine translation in a practical system (Palmer et al, 1998). Introduction A grammar is, among other things, a device by which it is possible to express structure in a set of entities; a grammar formalism, the constraints on how a grammar is allowed to express this. Once a grammar has been used to express structural relationships, in many applications there are operations which act at a 'meta level' on the structures expressed by the grammar: for example, lifting rules on a dependency grammar to achieve pseudo-projectivity (Kahane et al, 1998), and mapping between synchronised Tree Adjoining Grammars (TAGs) (Shieber and Schabes, 1990; Shieber 1994) as in machine translation or syntax-to-semantics transfer. At this meta level, however, the operations do not themselves exploit any structure. This paper explores how, in the TAG case, using a meta-level grammar to define meta-level structure resolves the flaws in the ability of Synchronous TAG (S-TAG) to be a representation for applications such as machine translation or paraphrase. In mapping between, say, English and French, there is a lexicalised TAG for each language (see XTAG, 199"
P99-1011,P94-1022,0,0.299417,"abelled trees would just adjoin in. So the meta-level auxiliary trees are rooted in c~-labelled trees; but they have only ~labelled trees in the spine, as they aim to represent the minimal amount of recursive material. Notwithstanding these conditions, the construction is quite straightforward. 5 Generative 5~i_1; the generative capacity of ~i is a superset of ~'i-1- Thus using a TAG meta-grammar, as described in Section 4, would suggest that the generative capacity of the object-level formalism would necessarily have been increased over that of TAG. However, there is a regular form for TAGs (Rogers, 1994), such that the trees of TAGs in this regular form are local sets; that is, they are context free. The meta-level TAG built by Construction 1 with the appropriate conditions on slots is in this regular form. A proof of this is in Dras (forthcoming); a sketch is as follows. If adjunction may not occur along the spine of another auxiliary tree, the grammar is in regular form. This kind of adjunction does not occur under Construction 1 because all meta-level auxiliary trees are rooted in c~-labelled trees (object-level auxiliary trees), while their spines consist only of p-labelled trees (object-"
P99-1011,J94-1004,0,0.0302297,"trees, leaving dependencies together. tion tree would be as in Figure 2. 2 Weir (1988) terms the derived tree, and its component elementary trees, OBJECT-LEVEL TREES; the derivation tree is t e r m e d a METALEVEL T R E E , since it describes the object-level trees. The derivation trees are context free (Weir, 1988), that is, they can be expressed by a CFG; Weir showed that applying a TAG yield function to a context free derivation tree (that is, reading the labels off the tree, and substituting or adjoining the corresponding objectlevel trees as appropriate) will uniquely specify a TAG tree. Schabes and Shieber (1994) characterise this as a function 7) from derivation trees to derived trees. The idea behind S-TAG is to take two TAGs and link t h e m in an appropriate way so that when substitution or adjunction occurs in a tree in one grammar, t h e n a corresponding composition operation occurs in a tree in the other grammar. Because of the way TAG's EDL captures dependencies, it is not problematic to have translations more complex t h a n word-for-word mappings (Abeill~ et al, 1990). For example, from the Abeill~ et al paper, handling argument swap, as in (1), is straightforward. These would be represente"
P99-1011,C90-3045,0,0.756378,"such as translation and paraphrase, operations are carried out on grammars at the meta level. This paper shows how a meta-grammar, defining structure at the meta level, is useful in the case of such operations; in particular, how it solves problems in the current definition of Synchronous TAG (Shieber, 1994) caused by ignoring such structure in mapping between grammars, for applications such as translation. Moreover, essential properties of the formalism remain unchanged. 1 2 S-TAG and Machine Translation Synchronous TAG, the mapping between two Tree Adjoining Grammars, was first proposed by Shieber and Schabes (1990). An application proposed concurrently with the definition of S-TAG was that of machine translation, mapping between English and French (Abeill~ et al, 1990); work continues in the area, for example using S-TAG for English-Korean machine translation in a practical system (Palmer et al, 1998). Introduction A grammar is, among other things, a device by which it is possible to express structure in a set of entities; a grammar formalism, the constraints on how a grammar is allowed to express this. Once a grammar has been used to express structural relationships, in many applications there are oper"
P99-1011,C98-1102,0,\N,Missing
P99-1011,W90-0102,0,\N,Missing
R15-1053,brooke-hirst-2012-measuring,0,0.0527535,"Missing"
R15-1053,guthrie-etal-2006-closer,0,0.0312481,"choices. In this work we used a list of 176 function words obtained from the distribution of the Apache Lucene search engine software.4 This list includes stop words for the Bokm˚al variant of the language and contains entries such as hvis (whose), ikke (not), jeg (I), s˚a (so) and hj˚a (at). We also make this list available on our website.5 In addition to single function words, we also extract function word bigrams, as described by Malmasi et al. (2013). Function word bigrams are a type of word n-gram where content words are skipped: they are thus a specific subtype of skipgram discussed by Guthrie et al. (2006). For example, the sentence We should all start taking the bus would be reduced to we should all the, from which we would extract the n-grams. Given its many morphosyntactic markers and detailed categories, the ASK dataset has a rich tagset with over 300 unique tags. 4 Experimental Methodology In this study we employ a supervised multi-class classification approach. The learner texts are organized into classes according to the author’s L1 and these documents are used for training and testing in our experiments. A diagram conceptualizing our NLI system is shown in Figure 2. 4.1 Classifier We us"
R15-1053,W13-1716,1,0.846813,"English function words have been found to be useful in studies of authorship attribution and NLI. Unlike POS tags, this model analyzes the author’s specific word choices. In this work we used a list of 176 function words obtained from the distribution of the Apache Lucene search engine software.4 This list includes stop words for the Bokm˚al variant of the language and contains entries such as hvis (whose), ikke (not), jeg (I), s˚a (so) and hj˚a (at). We also make this list available on our website.5 In addition to single function words, we also extract function word bigrams, as described by Malmasi et al. (2013). Function word bigrams are a type of word n-gram where content words are skipped: they are thus a specific subtype of skipgram discussed by Guthrie et al. (2006). For example, the sentence We should all start taking the bus would be reduced to we should all the, from which we would extract the n-grams. Given its many morphosyntactic markers and detailed categories, the ASK dataset has a rich tagset with over 300 unique tags. 4 Experimental Methodology In this study we employ a supervised multi-class classification approach. The learner texts are organized into classes according to the author’"
R15-1053,W15-0620,1,0.899129,"Missing"
R15-1053,W15-0606,1,0.871575,"Missing"
R15-1053,E14-4033,0,0.0367768,"Missing"
R15-1053,W14-3625,1,0.846376,"point to consider here is that this additional complexity also increases the possibility and number of potential learner errors. A more in-depth exposition of Norwegian syntax and morphology can be found in Haugen (2009). 3 150 Mean = 310.55 Std. Dev. = 15.291 … Frequency 100 50 0 275.00 295.00 315.00 335.00 355.00 Document Length (tokens) Figure 1: A histogram of the number of tokens per document in the dataset that we generated. Data In this work we extracted 750k tokens of text from the ASK corpus in the form of individual sentences. Following the methodology of Brooke and Hirst (2011) and Malmasi and Dras (2014b), we randomly select and combine the sentences from the same L1 to generate texts of approximately 300 tokens on average, creating a set of documents suitable for NLI. This methodology ensures that the texts for each L1 are a mix of different authorship styles, topics and proficiencies. It also means that all documents are similar and comparable in length. The 10 native languages and the number of texts generated per class are listed in Table 1. In addition to these we also generate 250 control texts written by natives. A histogram of the number of tokens per document is shown in Figure 1. T"
R15-1053,E14-4019,1,0.864397,"point to consider here is that this additional complexity also increases the possibility and number of potential learner errors. A more in-depth exposition of Norwegian syntax and morphology can be found in Haugen (2009). 3 150 Mean = 310.55 Std. Dev. = 15.291 … Frequency 100 50 0 275.00 295.00 315.00 335.00 355.00 Document Length (tokens) Figure 1: A histogram of the number of tokens per document in the dataset that we generated. Data In this work we extracted 750k tokens of text from the ASK corpus in the form of individual sentences. Following the methodology of Brooke and Hirst (2011) and Malmasi and Dras (2014b), we randomly select and combine the sentences from the same L1 to generate texts of approximately 300 tokens on average, creating a set of documents suitable for NLI. This methodology ensures that the texts for each L1 are a mix of different authorship styles, topics and proficiencies. It also means that all documents are similar and comparable in length. The 10 native languages and the number of texts generated per class are listed in Table 1. In addition to these we also generate 250 control texts written by natives. A histogram of the number of tokens per document is shown in Figure 1. T"
R15-1053,tenfjord-etal-2006-ask,0,0.59785,"Missing"
R15-1053,U14-1020,1,0.77517,"point to consider here is that this additional complexity also increases the possibility and number of potential learner errors. A more in-depth exposition of Norwegian syntax and morphology can be found in Haugen (2009). 3 150 Mean = 310.55 Std. Dev. = 15.291 … Frequency 100 50 0 275.00 295.00 315.00 335.00 355.00 Document Length (tokens) Figure 1: A histogram of the number of tokens per document in the dataset that we generated. Data In this work we extracted 750k tokens of text from the ASK corpus in the form of individual sentences. Following the methodology of Brooke and Hirst (2011) and Malmasi and Dras (2014b), we randomly select and combine the sentences from the same L1 to generate texts of approximately 300 tokens on average, creating a set of documents suitable for NLI. This methodology ensures that the texts for each L1 are a mix of different authorship styles, topics and proficiencies. It also means that all documents are similar and comparable in length. The 10 native languages and the number of texts generated per class are listed in Table 1. In addition to these we also generate 250 control texts written by natives. A histogram of the number of tokens per document is shown in Figure 1. T"
R15-1053,W15-5407,1,0.869865,"Missing"
R15-1053,C12-1158,0,0.0771052,"Missing"
R15-1053,N15-1160,1,0.663033,"Missing"
R15-1053,W13-1706,0,0.273777,"in distinguishing the different syntactic patterns used by different L1 groups. The availability of post-corrected POS tags in our data, as described in §3, can provide some insight into how much this issue affects NLI by comparing its performance with previously reported results. NLI work has been growing in recent years, using a wide range of syntactic and more recently, lexical features to distinguish the L1. A detailed review of NLI methods is omitted here for reasons of space, but a thorough exposition is presented in the report from the very first NLI Shared Task that was held in 2013 (Tetreault et al., 2013). Most English NLI work has been done using two corpora. The International Corpus of Learner English (Granger et al., 2009) was widely used until recently, despite its shortcomings1 being widely noted (Brooke and Hirst, 2012). More recently, T OEFL11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a; Malmasi and Dras, 2014c). Recently, Malmasi and Dras (2014b) introdu"
R15-1053,N03-1033,0,0.0671314,"Missing"
R15-1053,W15-0614,1,0.875239,"Missing"
R15-1053,D11-1148,1,0.802039,"Missing"
R15-1053,D12-1064,1,0.874612,"Missing"
R15-1053,D14-1144,1,\N,Missing
S16-1154,E14-4019,1,0.88592,"Missing"
S16-1154,P13-3015,0,0.106496,"ormed by human annotators required to label the data.1 We present the description of the LTG entry in the SemEval-2016 Complex Word Identification (CWI) task, which aimed to develop systems for identifying complex words in English sentences. Our entry focused on the use of contextual language model features and the application of ensemble classification methods. Both of our systems achieved good performance, ranking in 2nd and 3rd place overall in terms of F-Score. 1 2 Introduction Complex Word Identification (CWI) is the task of identifying complex words in texts using computational methods (Shardlow, 2013). The task is usually carried out as part of lexical and text simplification systems. Shardlow (2014) considers CWI as the first processing step in lexical simplification pipelines. Complex or difficult words should first be identified so they can be later substituted by simpler ones to improve text readability. CWI has gained more importance in the last decade as lexical and text simplification systems have been developed or tailored for a number of purposes. They have been applied to make texts more accessible to language learners (Petersen and Ostendorf, 2007); other researchers have explor"
U03-1012,P00-1041,0,0.0212721,"vation behind discovering segments in a text is that a sentence extraction summary should choose the most representative sentence for each segment, resulting in a comprehensive summary. In the view of Gong and Liu (2001), segments form the main themes of a document. They present a theme interpretation of the SVD analysis, as it is used for discourse segmentation, upon which our use of the technique is based. However, Gong and Liu use SVD for creating sentence extraction summaries, not for generating a single sentence summary by re-using words. In subsequent work to Witbrock and Mittal (1999), Banko et al. (2000) describe the use of information about the position of words within four quarters of the source document. The headline candidacy score of a word is weighted by its position in one of the quarters. We interpret this use of position information as a means of guiding the generation of a headline towards the central theme of the document, which for news articles typically occurs in the first quarter. SVD potentially offers a more general mechanism for handling the discovery of the central themes and their positions within the document. Jin et al. (2002) have also examined a statistical model for h"
U03-1012,W97-0704,0,0.0468846,"generates summaries greater in length than a sentence. We direct the reader to Paice (1990) for an overview of summarisation based on sentence extraction. Examples of recent systems include Kupiec et al. (1995) and Brandow et al. (1995). For examples of work in producing abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction and natural language processing. Hybrid methods for abstract-like summarisation, which combine statistical and symbolic approaches, have also been explored; see, for example, McKeown et al. (1999), Jing and McKeown (1999), and Hovy and Lin (1997). Statistical single sentence summarisation has been explored by a number of researchers (see for example, Witbrock and Mittal, 1999; Zajic et al., 2002). Interestingly, in the work of Witbrock and Mittal (1999), the selection of words for inclusion in the headline is decided solely on the basis of corpus statistics and does not use statistical information about the distribution of words in the document itself. Our work differs in that we utilise an SVD analysis to provide information about the document to be summarized, specifically its main theme. Discourse segmentation for sentence extracti"
U03-1012,J00-2011,0,0.290873,"mentation, where sentences naturally cluster in terms of their ‘aboutness’. $ is about, a characteristic described by Borko as being indicative. 5 Using Singular Value Decomposition for Content Selection As an alternative to the Conditional probability, we examine the use of SVD in determining the Content Selection probability. Before we outline the procedure for basing this probability on SVD, we will first outline our interpretation of the SVD analysis, based on that of Gong and Liu (2001). Our description is not intended to be a comprehensive explanation of SVD, and we direct the reader to Manning and Schütze (2000) for a description of how SVD is used in information retrieval. Conceptually, when used to analyse documents, SVD can discover relationships between word co-occurrences in a corpus of text. For example, in the context of information retrieval, this provides one way to retrieve additional documents that contain synonyms of query terms, where synonymy is defined by similarity of word co-occurrences. By discovering patterns in word co-occurrences, SVD also provides information that can be used to cluster documents based on similarity of themes. In the context of single document summarisation, we"
U03-1012,J98-3005,0,0.171898,"ppropriate, given that the performance does not differ considerably. In such a situation, a collection of documents is only necessary for collecting bigram statistics. 7 Related Work As the focus of this paper is on statistical singlesentence summarisation we will not focus on preceding work which generates summaries greater in length than a sentence. We direct the reader to Paice (1990) for an overview of summarisation based on sentence extraction. Examples of recent systems include Kupiec et al. (1995) and Brandow et al. (1995). For examples of work in producing abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction and natural language processing. Hybrid methods for abstract-like summarisation, which combine statistical and symbolic approaches, have also been explored; see, for example, McKeown et al. (1999), Jing and McKeown (1999), and Hovy and Lin (1997). Statistical single sentence summarisation has been explored by a number of researchers (see for example, Witbrock and Mittal, 1999; Zajic et al., 2002). Interestingly, in the work of Witbrock and Mittal (1999), the selection of words for inclusion in the headline is decided solely on the basis of corpus"
U03-1012,W03-1202,1,0.166119,"ence summaries using SVD. In Section 6, we present our experimental design in which we evaluated our approach, along with the results and corresponding discussion. Section 7, provides an overview of related work. Finally, in Section 8, we present our conclusions and future work. 2 Searching for a Probable Headline We re-implemented the work described in Witbrock and Mittal (1999) to provide a single sentence summarisation mechanism. For full details of their approach, we direct the reader to their paper (Witbrock and Mittal, 1999). For an overview of our implementation of their algorithm, see Wan et al. (2003). For convenience, a brief description is presented here. In a search, n words are selected on the basis of the two criteria. Conceptually, the task is twofold. Witbrock and Mittal (1999) label these two tasks as Content Selection and Realisation. Each criterion is scored probabilistically, whereby the probability is estimated by prior collection of corpus statistics. To estimate Content Selection probability for each word, we use the Maximum Likelihood Estimate (MLE). In an offline training stage, the system counts the number of times a word is used in a headline, with the condition that it o"
U05-1015,U04-1019,0,0.0247618,"s first contribution show how regular tree grammars can also be a basis for extensions proposed for XPath for common linguistic corpus querying. As the paper’s second contribution we demonstrate that, on the other hand, regular tree grammars cannot describe a number of structures of interest; we then show that, instead, a slightly more powerful machine is appropriate, and indicate how linguistic tree query languages might be augmented to include this extra power. 1 Introduction There has been recent interest in looking at what is required for a query language for annotated linguistic corpora (Lai and Bird, 2004). These corpora are used in a range of areas of natural language processing (NLP)—parsing, machine translation, and so on—where they form the basis of training data for statistical methods; and also in linguistics, where they are used to extract examples of particular phenomena for analysis and testing of hypotheses. As noted by Lai and Bird, the prototypical hierarchical linguistic annotation is the syntax tree, and Steve Cassidy Centre for Language Technology Macquarie University Steve.Cassidy@mq.edu.au consequently the type of query language that is of interest is a tree query language. One"
U05-1015,J97-2003,0,0.0209666,"e latter. For example, an algorithm for recognition exists that is linear in the number of nodes in the tree and the size of the automaton; it is decidable whether the set of matches will be empty; and so on (Comon et al., 1997). Further, there is the promise of the availability of standard tools and efficient techniques that could be used by a tree query language. This is the case for formal machines over strings (for example, the library of finite-state string transducers Proceedings of the Australasian Language Technology Workshop 2005, pages 96–104, Sydney, Australia, December 2005. 96 of Mohri (1997)), although not yet for trees. A number of researchers have looked at this complementary approach. One alternative is to design from scratch a tree query language derived from a tree grammar or automaton; this is taken by, for example, Chidlovskii (2000). Another is to relate an existing query language to a formal machine: Murata et al. (2000) present a taxonomy of XML schema languages using formal language theory. In this paper, we follow the second of these alternatives. Existing work, mostly focussed on XML, has looked only at regular tree grammars and automata for modelling query languages"
U05-1015,C69-0101,0,0.727908,"at they do not discuss that is also consistent with the data, differing only in the placement of the subtree headed with the V 0 ; this is the rightmost tree in Figure 5. The same pumping lemma shows that the set of these trees also cannot be generated by an RTG. K0 → NP K1 NP VP V’ NP V1 → K1 x2 x3 K1 K1 x1 x1 V’ VP NP 102 V’ V3 Context-Free Tree Grammar What, then, can generate these sets of trees? One possibility is a context-free tree grammar (CFTG). The basic idea of these is that, whereas trees generated by RTGs have regular paths, CTFGs have context-free paths. CFTGs were introduced in Rounds (1970). We do not have space for a full formal presentation of them, and we would also note that unlike the RTGs defined above they are defined over ranked trees. However, here is a brief definition, along with an example. A context-free tree grammar G is a 4-tuple (F, Φ, P, K0 ) where F is a finite ranked alphabet; Φ = {K0 , K1 , . . . , Kn } is a finite ranked alphabet of nonterminals; P is the set of production rules, a finite set of pairs (Ki (x1 , . . . , xm ), tx ), where i = 0, . . . , n, K i ∈ F, xi are variables, and tx is a tree over F , Φ and variables xi ; and K0 is the initial nontermin"
U05-1015,1985.tmi-1.17,0,0.252187,"e theory. In this paper, we follow the second of these alternatives. Existing work, mostly focussed on XML, has looked only at regular tree grammars and automata for modelling query languages; we examine the extent to which this is the case for linguistic treebanks, and what other machinery might be appropriate for a linguistic tree query language. We argue that while regular tree grammars might be satisfactory for a querying a broad range of phenomena, not all queries over trees representing natural language can be based on a regular tree grammar. This is a structural analogue of the work of Shieber (1985), which showed that natural language as a string language cannot be generated by a context-free grammar, which corresponds at the tree level to a regular tree grammar. In Section 2 we give the definition of regular tree grammars, along with an approach used to relate them to XPath. In Section 3 we look at extensions to XPath that Cassidy (2003) argues are necessary for linguistic corpus querying, and show that these can be captured by regular tree grammars. In Section 4, however, we present some examples from Dutch from the work of Bresnan et al. (1982) to show that not all desired queries can"
U06-1019,P05-1074,0,0.019642,"known performance for this corpus. We also examine the strengths and weaknesses of dependency based features and conclude that they may be useful in more accurately classifying cases of False Paraphrase. 1 Introduction In recent years, interest has grown in paraphrase generation methods. The use of paraphrase generation tools has been envisaged for applications ranging from abstract-like summarisation (see for example, Barzilay and Lee (2003), Daum´e and Marcu (2005), Wan et al. (2005)), questionanswering (for example, Marsi and Krahmer (2005)) and Machine Translation Evaluation (for example, Bannard and Callison-Burch (2005) and Yves Lepage (2005)). These approaches all employ a loose definition of paraphrase attributable to Dras (1999), who defines a ‘paraphrase pair’ operationally to be “a pair of units of text deemed to be interchangeable”. Notably, such a definition of paraphrase lends itself easily to corpora based methods. Furthermore, what the more modern approaches share is the fact that often they generate new paraphrases from raw text not semantic representations. The generation of paraphrases from raw text is a specific type of what is commonly referred to as text-to-text generation (Barzilay and Lee,"
U06-1019,W05-1612,0,0.0149558,"Missing"
U06-1019,N03-1003,0,0.0184087,"Missing"
U06-1019,U03-1014,0,0.036602,"Missing"
U06-1019,P96-1025,0,0.0188936,"Missing"
U06-1019,P02-1040,0,0.0775439,"equence overlap, where tokenisation is delimited by white space. We considered unigram overlap and explored two metrics, recall (feature 1) and precision (feature 2), where a precision score is defined as: precision = word-overlap(sentence1 , sentence2 ) word-count(sentence1 ) and recall is defined as: recall = word-overlap(sentence1 , sentence2 ) word-count(sentence2 ) nexor parser2 which provides lemmatisation information. For both sentences, each original word is replaced by its lemma. We then calculated our unigram precision and recall scores as before (features 3 and 4). The Bleu metric (Papineni et al., 2002), which uses the geometric average of unigram, bigram and trigram precision scores, is implemented as feature 5. The score was obtained using the original Bleu formula3 with a brevity penalty set to 1 (that is, the brevity penalty is ignored). Note that in our usage, there is only one ’reference’ sentence. By reversing which sentence was considered the ‘test’ sentence and which was considered the ‘reference’, a recall version of Bleu was obtained (feature 6). Lemmatised versions provided features 7 and 8. Finally, because of the bi-directionality property of paraphrase, the F-Measure4 , which"
U06-1019,W05-1203,0,0.0338777,"Missing"
U06-1019,C04-1051,0,0.253524,"Missing"
U06-1019,W06-1603,0,0.516548,"Missing"
U06-1019,I05-5012,1,0.793999,"nd sentence was generated from an input news article statistically using a four-gram language model and a probabilistic word selection module. Although other paraphrase generation approaches differ in their underlying mechanisms1 , most generate a novel sentence that cannot be found verbatim in the input text. The generated second sentence of the example is intended to be a paraphrase of the article headline. One might be convinced that the first exam1 The details of the generation algorithm used for this example are peripheral to the focus of this paper and we direct the interested reader to Wan et al. (2005) for more details. Proceedings of the 2006 Australasian Language Technology Workshop (ALTW2006), pages 131–138. 131 2 Paraphrase Classification and Related Work Example 1: Original Headline: European feeds remain calm on higher dollar. Generated Sentence: The European meals and feeds prices were firm on a stronger dollar; kept most buyers in this market. Example 2: Original Headline: India’s Gujral says too early to recognise Taleban. Generated Sentence: Prime Minister Inder Kumar Gujral of India and Pakistan to recognise the Taleban government in Kabul. Figure 1: Two examples of generated nov"
U06-1019,I05-5008,0,0.0109142,"examine the strengths and weaknesses of dependency based features and conclude that they may be useful in more accurately classifying cases of False Paraphrase. 1 Introduction In recent years, interest has grown in paraphrase generation methods. The use of paraphrase generation tools has been envisaged for applications ranging from abstract-like summarisation (see for example, Barzilay and Lee (2003), Daum´e and Marcu (2005), Wan et al. (2005)), questionanswering (for example, Marsi and Krahmer (2005)) and Machine Translation Evaluation (for example, Bannard and Callison-Burch (2005) and Yves Lepage (2005)). These approaches all employ a loose definition of paraphrase attributable to Dras (1999), who defines a ‘paraphrase pair’ operationally to be “a pair of units of text deemed to be interchangeable”. Notably, such a definition of paraphrase lends itself easily to corpora based methods. Furthermore, what the more modern approaches share is the fact that often they generate new paraphrases from raw text not semantic representations. The generation of paraphrases from raw text is a specific type of what is commonly referred to as text-to-text generation (Barzilay and Lee, 2003). As techniques fo"
U06-1019,U05-1023,0,0.485398,"Missing"
U06-1019,I05-5003,0,\N,Missing
U06-1021,W06-1609,0,0.0847568,"Missing"
U06-1021,koen-2004-pharaoh,0,0.468079,"ositioned to the head. The longer the NP gets the further away back is pushed. The theory is that languages tend to minimise the distance, so if the NP gets too long, we prefer 2 over 1, because we want to have back close to its head give. 2.3 Reordering Based on Minimising Dependency Distances Regarding the work of Collins et al., we suggest two possible sources for the improvement obtained. Match target language word order Although most decoders are capable of generating words in a different order than the source language, usually only simple models are used for this reordering. In Pharaoh (Koehn, 2004), for example, every word reordering between languages is penalised and only the language model can encourage a different order. If we can match the word order of the target language to a certain degree, we might expect an increase in translation quality, because we already have more explicitly used information of what the new word ordering should be. Fitting phrase length The achievement of Phrase-Based SMT (PSMT) (Koehn et al., 2003) was to combine different words into one phrase and treat them as one unit. Yet PSMT only manages to do this if the words all fit together in the same phrase-win"
U06-1021,J03-1002,0,0.00363425,"972 0.973 0.964 0.958 B LEU 0.207 0.198 0.196 0.203 0.208 brevity penalty of B LEU Table 1: Automatic Evaluation Metrics for the different Models pact of our parser and our choice of language pair. 4 Experimental Setup For our experiments we used the decoder Pharaoh (Koehn, 2004). For the phrase extraction we used our implementation of the algorithm which is described in the manual of Pharaoh. As a language model we used the SRILM (Stolcke, 2002) toolkit. We used a trigram model with interpolated Kneser-Ney discounting. For a baseline we used the Pharaoh translation made with a normal GIZA++ (Och and Ney, 2003) training on unchanged text, and the same phrase extractor we used for our other four models. As an automated scoring metric we used the B LEU (Papineni et al., 2002) and the FMeasure (Turian et al., 2003)3 method. For our training data we used the Dutch and English portions of most of the Europarl Corpus (Koehn, 2003). Because one section of the Europarl corpus was not available in a parsed form, this was left out. After sentence aligning the Dutch and the English part we divided the corpus into a training and a testing part. From the original available Dutch parses we selected every 200th se"
U06-1021,P02-1040,0,0.0781602,"er and our choice of language pair. 4 Experimental Setup For our experiments we used the decoder Pharaoh (Koehn, 2004). For the phrase extraction we used our implementation of the algorithm which is described in the manual of Pharaoh. As a language model we used the SRILM (Stolcke, 2002) toolkit. We used a trigram model with interpolated Kneser-Ney discounting. For a baseline we used the Pharaoh translation made with a normal GIZA++ (Och and Ney, 2003) training on unchanged text, and the same phrase extractor we used for our other four models. As an automated scoring metric we used the B LEU (Papineni et al., 2002) and the FMeasure (Turian et al., 2003)3 method. For our training data we used the Dutch and English portions of most of the Europarl Corpus (Koehn, 2003). Because one section of the Europarl corpus was not available in a parsed form, this was left out. After sentence aligning the Dutch and the English part we divided the corpus into a training and a testing part. From the original available Dutch parses we selected every 200th sentence for testing, until we had 1500 sentences. We have a little over half a million sentences in our training section. 3 In this article, we used our own implementa"
U06-1021,P05-1034,0,0.0420643,"unlike ‘arbitrary’ hand written language-pair specific rules (Collins et al., 2005) or thousands of different transformations (Xia and McCord, 2004)). If one is able to show that one general syntactically informed rule can lead to translation quality this is evidence in favour of the theory used explaining how languages themselves operate. 151 Explicitly using syntactic knowledge Although in the Machine Translation (MT) community it is still a controversial point, syntactical information of languages seems to be able to help in MT when applied correctly. Another example of this is the work of Quirk et al. (2005) where a dependency parser was used to learn certain translation phrases, in their work on “treelets”. When we can show that reordering based on an elegant rule using syntactical language information can enhance translation quality, it is another small piece of evidence supporting the idea that syntactical information is useful for MT. Starting point in search space Finally, most (P)SMT approaches are based on a huge search space which cannot be fully investigated. Usually hill climbing techniques are used to handle this large search space. Since hill climbing does not guarantee reaching globa"
U06-1021,2003.mtsummit-papers.51,0,0.0463876,"erimental Setup For our experiments we used the decoder Pharaoh (Koehn, 2004). For the phrase extraction we used our implementation of the algorithm which is described in the manual of Pharaoh. As a language model we used the SRILM (Stolcke, 2002) toolkit. We used a trigram model with interpolated Kneser-Ney discounting. For a baseline we used the Pharaoh translation made with a normal GIZA++ (Och and Ney, 2003) training on unchanged text, and the same phrase extractor we used for our other four models. As an automated scoring metric we used the B LEU (Papineni et al., 2002) and the FMeasure (Turian et al., 2003)3 method. For our training data we used the Dutch and English portions of most of the Europarl Corpus (Koehn, 2003). Because one section of the Europarl corpus was not available in a parsed form, this was left out. After sentence aligning the Dutch and the English part we divided the corpus into a training and a testing part. From the original available Dutch parses we selected every 200th sentence for testing, until we had 1500 sentences. We have a little over half a million sentences in our training section. 3 In this article, we used our own implementations of the B LEU and the F-Measure sc"
U06-1021,C04-1073,0,0.262458,"ered before being processed. This reordering can be done based on rules over word alignment learnt statistically; (Costa-Juss and Donollosa, 2006), for example, describe such a system. In this work an improvement in overall translation quality in a Spanish-English MT Mark Dras Centre for Language Technology Macquarie University Sydney, Australia madras@ics.mq.edu.au system was achieved by using statistical word classes and a word-based distortion model to reorder words in the source language. Reordering here is purely a statistical process and no syntactical knowledge of the language is used. Xia and McCord (2004) do use syntactical knowledge; they use pattern learning in their reordering system. In their work in the training phase they parse and align sentences and derive reordering patterns. From the English-French Canadian Hansard they extract 56,000 different transformations for translation. In the decoding phase they use these transformations on the source language. The main focus then is monotonic decoding (that is, decoding while roughly keeping the same order in the target language as in the source language — reordering done within phrases, for example, is an exception). Syntactically motivated"
U06-1021,N03-1017,0,\N,Missing
U06-1021,P05-1066,0,\N,Missing
U07-1004,P05-1074,0,0.108855,"Missing"
U07-1004,C96-2187,0,0.0816127,"Missing"
U07-1004,N01-1009,0,0.186384,"ther this will improve the overall performance of the RTE engine. In this paper we are looking at one subtype of entailment pairs where a semantic relation expressed in the hypothesis is implicitly represented by a syntactic construction in the text. There are several reasons to work with this type of entailment pairs. First, it proves possible to recognize them well automatically and distinguish them from other entailment pairs using machine learning. Second, narrowing down the entailment pairs to this subset allows us to draw an analogy with, and develop an algorithm related to, the work by Lapata (2001) that finds the implicit relation between attributes to a head noun in the noun group. That together with a conditional probability model in a parallel with SMT will be taken as the basis of an algorithm for classification of entailment pairs of the chosen type. We evaluate the approach on the RTE2 annotated dataset. The layout of the paper follows the general flow of the research. Section 2 defines the chosen type of entailment pairs. Section 3 describes an automatic classifier which distinguishes the desired type of the entailment pairs. Section 4 describes an algorithm for recognizing true"
U07-1004,A97-2017,0,\N,Missing
U07-1007,N03-1003,0,0.0225798,"em. Introduction Choosing an appropriate word or phrase from among candidate near-synonyms or paraphrases is a significant language generation problem since even though near-synonyms and paraphrases are close in meaning, they differ in connotation and denotation in ways that may be significant to the desired effect of the generation output: for example, word choice can change a sentence from advice to admonishment. Particular applications that have been cited as having a use for modules which make effective word and phrase choices among closely related options are summarisation and rewriting (Barzilay and Lee, 2003). Inkpen and Hirst (2006) extended the generation system HALogen (Langkilde and Knight, 1998; Langkilde, 2000) to include such a module. We discuss a particular aspect of choice between closely related words and phrases: choice between words when there is any difference in meaning or attitude. Typical examples are frugal and stingy; slender and skinny; and error and blunder. In this paper, as in Gardiner and Dras (2007), we explore whether corpus statistics methods have promise in discriminating between near-synonyms with attitude differences, particularly compared to near-synonyms that do not"
U07-1007,P97-1067,0,0.281626,"m HALogen (Langkilde and Knight, 1998; Langkilde, 2000) to include such a module. We discuss a particular aspect of choice between closely related words and phrases: choice between words when there is any difference in meaning or attitude. Typical examples are frugal and stingy; slender and skinny; and error and blunder. In this paper, as in Gardiner and Dras (2007), we explore whether corpus statistics methods have promise in discriminating between near-synonyms with attitude differences, particularly compared to near-synonyms that do not differ in attitude. In our work, we used the work of (Edmonds, 1997), the first to attempt to distinguish among near-synonyms, adopting a corpus statistics approach. Based on that work, we found that there was a significant difference in attitudinal versus non-attitudinal nearsynonyms. However, the Edmonds algorithm produced on the whole poor results, only a little above the given baseline, if at all. According to (Inkpen, 2007), the poor results were due to the way the alInitial work by Edmonds (1997) suggested that corpus statistics methods would not be particularly effective, and led to subsequent work adopting methods based on specific lexical resources. I"
U07-1007,U07-1008,1,0.886896,"Missing"
U07-1007,S07-1100,0,0.015614,"ords in the text. Inkpen estimated C(x), C(y) and C(x, y) by issuing queries to the Waterloo MultiText System (Clarke and Terra, 2003). She defined C(x, y) the number of times where x is followed by y within a certain query frame of length q within a corpus, so 33 Our variation of Inkpen’s approach 41 These n-gram counts allow us to estimate C(x, y) for a given window width k by summing the Web 1T counts of k-grams in which words x and y occur and x is followed by y. Counts are computed using a an especially developed version of the Web 1T processing software “Get 1T”1 originally described in Hawker (2007) and detailed in Hawker et. al (2007). The Get 1T software allows n-gram queries of the form in the following examples, where &lt;*&gt; is a wildcard which 1 Available at http://get1t.sf.net/ will match any token in that place in the n-gram. In order to find the number of n-grams with fresh and mango we need to construct three queries: (11) &lt;*&gt; fresh mango (12) fresh &lt;*&gt; mango (13) fresh mango &lt;*&gt; Since Web 1T contains 5-gram counts, we can use query frame sizes from q = 1 (words x and y must be adjacent, that is, occur in the 2-gram counts) to q = 4. 3.4 Baseline method However, in order to find fr"
U07-1007,J06-2003,0,0.111769,"an appropriate word or phrase from among candidate near-synonyms or paraphrases is a significant language generation problem since even though near-synonyms and paraphrases are close in meaning, they differ in connotation and denotation in ways that may be significant to the desired effect of the generation output: for example, word choice can change a sentence from advice to admonishment. Particular applications that have been cited as having a use for modules which make effective word and phrase choices among closely related options are summarisation and rewriting (Barzilay and Lee, 2003). Inkpen and Hirst (2006) extended the generation system HALogen (Langkilde and Knight, 1998; Langkilde, 2000) to include such a module. We discuss a particular aspect of choice between closely related words and phrases: choice between words when there is any difference in meaning or attitude. Typical examples are frugal and stingy; slender and skinny; and error and blunder. In this paper, as in Gardiner and Dras (2007), we explore whether corpus statistics methods have promise in discriminating between near-synonyms with attitude differences, particularly compared to near-synonyms that do not differ in attitude. In o"
U07-1007,W98-1426,0,0.0188637,"s or paraphrases is a significant language generation problem since even though near-synonyms and paraphrases are close in meaning, they differ in connotation and denotation in ways that may be significant to the desired effect of the generation output: for example, word choice can change a sentence from advice to admonishment. Particular applications that have been cited as having a use for modules which make effective word and phrase choices among closely related options are summarisation and rewriting (Barzilay and Lee, 2003). Inkpen and Hirst (2006) extended the generation system HALogen (Langkilde and Knight, 1998; Langkilde, 2000) to include such a module. We discuss a particular aspect of choice between closely related words and phrases: choice between words when there is any difference in meaning or attitude. Typical examples are frugal and stingy; slender and skinny; and error and blunder. In this paper, as in Gardiner and Dras (2007), we explore whether corpus statistics methods have promise in discriminating between near-synonyms with attitude differences, particularly compared to near-synonyms that do not differ in attitude. In our work, we used the work of (Edmonds, 1997), the first to attempt"
U07-1007,A00-2023,0,0.0134834,"icant language generation problem since even though near-synonyms and paraphrases are close in meaning, they differ in connotation and denotation in ways that may be significant to the desired effect of the generation output: for example, word choice can change a sentence from advice to admonishment. Particular applications that have been cited as having a use for modules which make effective word and phrase choices among closely related options are summarisation and rewriting (Barzilay and Lee, 2003). Inkpen and Hirst (2006) extended the generation system HALogen (Langkilde and Knight, 1998; Langkilde, 2000) to include such a module. We discuss a particular aspect of choice between closely related words and phrases: choice between words when there is any difference in meaning or attitude. Typical examples are frugal and stingy; slender and skinny; and error and blunder. In this paper, as in Gardiner and Dras (2007), we explore whether corpus statistics methods have promise in discriminating between near-synonyms with attitude differences, particularly compared to near-synonyms that do not differ in attitude. In our work, we used the work of (Edmonds, 1997), the first to attempt to distinguish amo"
U07-1019,J93-2003,0,0.0133443,"bes an approach where Czech was turned into ‘Czech-prime’ as a preprocessing step. For Indo-European languages, Czech is highly inflected and has a relatively free word order. In their approach they first completely discarded inflective information like number, tense and gender. Later they used this information to artificially enhance their statistical model, by enriching the vocabulary of their statistical look-up table by adding new tokens based on seen roots of words with known morphology. Note that this work was not done in the PSMT paradigm, but using the original IBM statistical models (Brown et al., 1993) for MT. An example of a fairly comprehensive analysis of the use of morphological analysis as a preprocessing step has been done on Arabic (Sadat and Habash, 2006). An Arabic morphological analyser was used to obtain an analysis of the build-up of 3 These texts were made available to us by the Aboriginal Studies Electronic Data Archive (ASEDA). Arabic words. Several models were presented which preprocessed the Arabic text. The key idea was to split off word parts based on specific analysis of the word. For example, pronominal clitics are split into several words. However, Arabic morphology is"
U07-1019,P05-1066,0,0.20919,"want to investigate the effect of applying special morphological treatment within SMT for languages with varying degree of morpho134 logical richness. Without any morphological preprocessing, individual word counts can be quite low in highly inflected languages, causing more data sparseness than necessary, and ignoring some information which might be useful in Natural Language Processing. Preprocessing before SMT has been used as a way of improving results. This ranges from basic tokenisation (e.g. separating possessive ’s on English before training) to extensive syntax-based reordering (e.g. Collins et al. (2005)). Often, the choice of preprocessing proceeds without consideration of the type of language; consider for example recent work on Arabic (Sadat and Habash, 2006), where the various combinations of different preprocessing strategies are systematically worked through, with no particular attention to the characteristics of Arabic. In most work, there is an intuitive notion that there is a connection between morphological richness of a language and the usefulness of morphological preprocessing. This is suggested in its use in parsing for Korean (Han and Sarkar, 2002) and Turkish (Eryi˘git and Ofla"
U07-1019,E06-1012,0,0.165895,"Missing"
U07-1019,C04-1073,0,0.0292902,"version. Overall our corpus is very small for SMT models, and we are trying to obtain more data. For the moment we are interested in relative machine translation quality, and hope that translation quality will improve when provided with more bilingual data. 137 3.1 Related approaches To treat morphologically rich indigenous languages we want to do morphological analyses before translating. We do this as a preprocessing step in Phrase Based SMT (PSMT), leaving all the other PSMT steps untouched. Preprocessing before applying PSMT has shown to be able to improve overall MT quality. As examples, Xia and McCord (2004), Collins et al. (2005) and Zwarts and Dras (2006) present an PSMT approach with word reordering as a preprocessing step, and demonstrate improved results in translation quality. Work in Czech, done during the 1999 Summer Workshop at John Hopkins University (Al-Onaizan et al., 1999), describes an approach where Czech was turned into ‘Czech-prime’ as a preprocessing step. For Indo-European languages, Czech is highly inflected and has a relatively free word order. In their approach they first completely discarded inflective information like number, tense and gender. Later they used this informat"
U07-1019,W02-2207,0,0.302767,"syntax-based reordering (e.g. Collins et al. (2005)). Often, the choice of preprocessing proceeds without consideration of the type of language; consider for example recent work on Arabic (Sadat and Habash, 2006), where the various combinations of different preprocessing strategies are systematically worked through, with no particular attention to the characteristics of Arabic. In most work, there is an intuitive notion that there is a connection between morphological richness of a language and the usefulness of morphological preprocessing. This is suggested in its use in parsing for Korean (Han and Sarkar, 2002) and Turkish (Eryi˘git and Oflazer, 2006), and MT for Czech (AlOnaizan et al., 1999). But in this body of work, as well as the body of work mentioned in section 3.1, only analysis of one language is performed. Moreover there is no specific measure of richness of morphology; it is not obvious how to compare the morphology of different languages such as English, Arabic, Turkish or Korean with their different combinations of prefixing, suffixing and infixing. In this paper, to examine this idea, we look at two Australian Aboriginal languages sharing a similar morphological system, but with differ"
U07-1019,koen-2004-pharaoh,0,0.0448715,"minga man was not know no again I is speaks Again he denied it with an oath I don’t know the man Table 7: Wik Mungkan improvement: example translation set: (B)aseline, (S)plit, (R)eference both corpora come with verse information. Aligning them on a sentence level within verses was found to be extremely hard, especially since the same information was probably distributed over different sentence in a way problematic for the statistical machinery. We use the normal tools to for PSMT: GIZA++ (Och and Ney, 2003) to statistically derive a sentence alignment on token level; and the decoder Pharaoh (Koehn, 2004), a beam search decoder for PSMT. Phrases are extracted by our own Phrase Builder, which extracts phrases based on the GIZA++ alignment as described in the Pharaoh manual. We used a trigram model with interpolated Kneser-Ney discounting as a language model. The language model was built using Biblical text and was enriched with extracts from the European Parliament in order to reduce data sparseness. The SRILM (Stolcke, 2002) toolkit was used to build this language model. Our system still suffers from quite some considerable noise. This is not uncommon for a statisti139 cal approach, but partic"
U07-1019,N04-4015,0,0.0234671,"able to us by the Aboriginal Studies Electronic Data Archive (ASEDA). Arabic words. Several models were presented which preprocessed the Arabic text. The key idea was to split off word parts based on specific analysis of the word. For example, pronominal clitics are split into several words. However, Arabic morphology is not as extensive as in languages like Warlpiri. Riesa et al. (2006) is another example where the use of morphological information boosts SMT quality. In this approach the tokens are separated from prefixes and postfixes based on a predefined list, derived from a grammar book. Lee (2004) similarly works on Arabic to English translation and separates prefixes and suffixes from the word stem. In contrast with our data, where we do not need to differentiate between different affixes. We only have postfixes, although stacked on each other and playing different roles, so we treat all morphology uniformly. 3.2 Data characteristics We want to apply morphological preprocessing to Aboriginal languages to investigate its effect on morphologically rich languages as opposed to morphologically poorer ones. In Warlpiri it is possible to explicitly mark suffixes. We separate the suffixes fr"
U07-1019,J03-1002,0,0.00359971,"ll this as I would like to know by inaniu I uuyaminga but when Peter to uuama he said truly I head a uuyaminga man was not know no again I is speaks Again he denied it with an oath I don’t know the man Table 7: Wik Mungkan improvement: example translation set: (B)aseline, (S)plit, (R)eference both corpora come with verse information. Aligning them on a sentence level within verses was found to be extremely hard, especially since the same information was probably distributed over different sentence in a way problematic for the statistical machinery. We use the normal tools to for PSMT: GIZA++ (Och and Ney, 2003) to statistically derive a sentence alignment on token level; and the decoder Pharaoh (Koehn, 2004), a beam search decoder for PSMT. Phrases are extracted by our own Phrase Builder, which extracts phrases based on the GIZA++ alignment as described in the Pharaoh manual. We used a trigram model with interpolated Kneser-Ney discounting as a language model. The language model was built using Biblical text and was enriched with extracts from the European Parliament in order to reduce data sparseness. The SRILM (Stolcke, 2002) toolkit was used to build this language model. Our system still suffers"
U07-1019,P06-1001,0,0.26455,"any morphological preprocessing, individual word counts can be quite low in highly inflected languages, causing more data sparseness than necessary, and ignoring some information which might be useful in Natural Language Processing. Preprocessing before SMT has been used as a way of improving results. This ranges from basic tokenisation (e.g. separating possessive ’s on English before training) to extensive syntax-based reordering (e.g. Collins et al. (2005)). Often, the choice of preprocessing proceeds without consideration of the type of language; consider for example recent work on Arabic (Sadat and Habash, 2006), where the various combinations of different preprocessing strategies are systematically worked through, with no particular attention to the characteristics of Arabic. In most work, there is an intuitive notion that there is a connection between morphological richness of a language and the usefulness of morphological preprocessing. This is suggested in its use in parsing for Korean (Han and Sarkar, 2002) and Turkish (Eryi˘git and Oflazer, 2006), and MT for Czech (AlOnaizan et al., 1999). But in this body of work, as well as the body of work mentioned in section 3.1, only analysis of one langu"
U07-1019,U06-1021,1,0.786445,"models, and we are trying to obtain more data. For the moment we are interested in relative machine translation quality, and hope that translation quality will improve when provided with more bilingual data. 137 3.1 Related approaches To treat morphologically rich indigenous languages we want to do morphological analyses before translating. We do this as a preprocessing step in Phrase Based SMT (PSMT), leaving all the other PSMT steps untouched. Preprocessing before applying PSMT has shown to be able to improve overall MT quality. As examples, Xia and McCord (2004), Collins et al. (2005) and Zwarts and Dras (2006) present an PSMT approach with word reordering as a preprocessing step, and demonstrate improved results in translation quality. Work in Czech, done during the 1999 Summer Workshop at John Hopkins University (Al-Onaizan et al., 1999), describes an approach where Czech was turned into ‘Czech-prime’ as a preprocessing step. For Indo-European languages, Czech is highly inflected and has a relatively free word order. In their approach they first completely discarded inflective information like number, tense and gender. Later they used this information to artificially enhance their statistical mode"
U08-1021,P05-1066,0,0.253871,"preprocessed Finnish (a Uralic language, typologically between inflected and agglutinative, and consequently morphologically rich) for translation into Danish and Swedish (both Indo-European languages, quite different in many morphosyntactic respects from Finnish, including being morphologically less rich). In doing this, they found that translation quality was generally worse than baseline, and never better. What this paper looks at is whether it is the way in which the morphology is preprocessed that is important. We draw on an idea from other work in machine translation first presented by Collins et al. (2005), where the source language is reordered so that its syntax is more like that of the target language, leading to an improvement in translation quality; here, we see whether that idea can apply at the level of morphology. In particular, where there are phenomena that are handled via morphology in one language and by syntax in another, we investigate whether an adaptation of this reordering idea can improve translation quality. In Section 2 we briefly review relevant literature. In Section 3 we describe the important characteristics of Finnish, followed by various models for morphological prepro"
U08-1021,W06-1609,0,0.0736492,"Missing"
U08-1021,E06-1012,0,0.0306491,"Missing"
U08-1021,C00-1042,0,0.0346836,"preprocessing that first transforms the source language to look more like the target, adapted from work on preprocessing via syntactically motivated reordering. We show that this is indeed the case in translating from Finnish, and that the results hold for different target languages and different morphological analysers. 1 Introduction In many NLP applications, morphological preprocessing such as stemming is intuitively felt to be important, especially when dealing with morphologically rich languages. In fairly early work on morphological disambiguation of the agglutinative language Turkish, Hakkani-Tur et al. (2000) cite the note of Hankamer (1989) that a single Turkish root Mark Dras Centre for Language Technology Macquarie University Sydney, Australia madras@ics.mq.edu.au can potentially have over a million inflected variants; such a proliferation of forms could exacerbate any data sparsity problems. Zwarts and Dras (2007a) showed, for languages that differ in morphological richness but are otherwise structurally similar, that, as expected, morphological preprocessing in an Statistical Machine Translation (SMT) environment confers greater benefits on the morphologically richer language than on the morp"
U08-1021,W02-2207,0,0.0198991,"e the important characteristics of Finnish, followed by various models for morphological preprocessing, including our method for transforming the morphology of the Finnish source to more closely match the target language. In Section 4 we discuss the results, and in Section 5 we conclude. 2 Literature Review 2.1 Morphological Analysis So many systems make some use of morphological preprocessing, particularly stemming, that we only point to a few specific instances here. In the context of parsing, morphological preprocessing has been shown to be necessary for the agglutinative languages Korean (Han and Sarkar, 2002) and Turkish (Eryi˘git and Oflazer, 2006). In SMT, use of morphological preprocessing has been fairly ad hoc. One quite systematic comparison of morphological preprocessing parameters was carried out by Sadat and Habash (2006) for the language pair Arabic-English; their approach was just to search the whole space of parameter combinations, rather than looking at any characteristics of the pairing of the specific languages. One earlier work looking at Czech-English, Al-Onaizan et al. (1999), did carry out morphological analysis that looked at characteristics of the pair of languages, transformi"
U08-1021,koen-2004-pharaoh,0,0.0312362,"SUF, ksi/SUF, tta/SUF, ne/SUF. See Table 1 for how these suffixes are acquired. istuntokauden uudelleenavaaminen turns into: n/SUF istutokauden lle/SUF en/SUF uudeavaamin For example: istuntokauden uudelleenavaaminen turns into: istu/STM n/SUF tokauden/STM uude/STM lle/SUF en/SUF avaam/STM in/SUF en/SUF 3.3 Data We use as our data the Europarl corpus, language pairs Finnish-English and Finnish-Dutch. In both cases the baseline is a normal sentence-aligned corpus, which is trained with GIZA++. After this training, the phrases are derived via the standard method described in the Pharaoh manual (Koehn, 2004). The language model is trained on Europarl text only and is an n-gram language model. We use Pharaoh to translate a test set of 10k sentences while we have 774k sentences in the training corpus. The same approach is used for the various other models, with the text passed to GIZA++ and Pharaoh being the preprocessed version of the source. As Virpioja et al. (2007), we do not perform Minimum Error Rate Training (MERT) (Och, 2003) or optimise variables. Finnish B. Original (baseline) Connexor C1. Noun Case Isolated C2. Stemmed, separate morph. C3. Stemmed C4. Stemmed and de-compounded Morfessor"
U08-1021,P03-1021,0,0.00367065,"suggests that the original baseline is undergenerating words. It is possible that the large brevity penalty is Finnish B. Original (baseline) Connexor C1. Noun Case Isolated C2. Stemmed, separate morph. C3. Stemmed C4. Stemmed and de-compounded to Dutch 0.132 0.138 0.089 0.101 0.102 Table 4: B LEU scores for different morphological analysis 1-gram 2-gram 3-gram 4-gram Brevity Penalty B LEU Model B 0.5167 0.2276 0.1114 0.0564 0.7717 0.1273 Model C1 0.4945 0.2135 0.1029 0.0500 0.9453 0.1443 Table 5: Decomposition of B LEU scores for Model B and Model C1 a consequence of not carrying out MERT.5 Och (2003) notes that using standard optimisation criteria (rather than optimising for B LEU score) can “prefer shorter translations which are heavily penalized by the B LEU and NIST brevity penalty”. However, all of the models are comparable, being implemented without MERT, so they are all affected in the same way. In attempting to understand it, we note that what the morphological preprocessing has achieved is to generate more correct words. We can conclude that in the normal (baseline) translation the problem is not so much to find the right translation for the words: even when morphology is attached"
U08-1021,P06-1001,0,0.0221681,"Section 4 we discuss the results, and in Section 5 we conclude. 2 Literature Review 2.1 Morphological Analysis So many systems make some use of morphological preprocessing, particularly stemming, that we only point to a few specific instances here. In the context of parsing, morphological preprocessing has been shown to be necessary for the agglutinative languages Korean (Han and Sarkar, 2002) and Turkish (Eryi˘git and Oflazer, 2006). In SMT, use of morphological preprocessing has been fairly ad hoc. One quite systematic comparison of morphological preprocessing parameters was carried out by Sadat and Habash (2006) for the language pair Arabic-English; their approach was just to search the whole space of parameter combinations, rather than looking at any characteristics of the pairing of the specific languages. One earlier work looking at Czech-English, Al-Onaizan et al. (1999), did carry out morphological analysis that looked at characteristics of the pair of languages, transforming some Czech morphemes into pseudo-prepositions. The specific work we cited in the Introduction as having found that morphological preprocessing did not help, that of Virpioja et al. (2007), used Morfessor, a morphological an"
U08-1021,A97-1011,0,0.026553,"Missing"
U08-1021,2007.mtsummit-papers.65,0,0.0607105,"Missing"
U08-1021,D07-1077,0,0.021219,"e restructuring. They define six hand-written rules for reordering source sentences in German for translation to English, which operate on the output of an automatic parser. The rules cover German phenomena such as the location of verbs, separable verb prefixes, negations and subjects, several of which represent long-distance relationships in German (e.g. where the inflected verb is in second position in the clause and its uninflected dependendent verbs are at the end of the clause). The approach has also been applied successfully to Dutch-English (Zwarts and Dras, 2007b) and Chinese-English (Wang et al., 2007), among others. Zwarts and Dras (2007b) found that there are at least two sources of the translation improvement: one is the explicit matching of target language syntax, while the other is the moving of heads and dependants closer together to take advantage of the phrasal window of PSMT. 3 Morphological Target Language Matching 3.1 Languages Taking the work of Virpioja et al. (2007) as a broad starting point, we use Finnish as our source language. As noted above, Finnish is part of the FinnoUgric branch of the Uralic language family rather than part of the majority Indo-European family, and co"
U08-1021,C04-1073,0,0.0292463,"ystem using words as the most fine-grained translation units. 2.2 Source-Side Reordering as Preprocessing There are a number of different approaches to word reordering. It can be done based on rules over word alignment learnt statistically, for example Costa-Juss`a and Fonollosa (2006). In this work an improvement in overall translation quality in a Spanish-English MT system was achieved by using statistical word classes and a word-based distortion model to reorder words in the source language. Reordering here is purely a statistical process and no syntactic knowledge of the language is used. Xia and McCord (2004) on the other hand use syntactic knowledge; they use pattern learning in their reordering system. In their work they parse and align sentences in the training phase and derive reordering patterns. From the English-French Canadian Hansard they extract 56,000 different transformations for translation. In the decoding phase they use these transformations on the source language. The main focus then is monotonic decoding. Both of these two cited works assume that explicitly matching the word order of the target language is the key. The work that we draw on in this paper is that of Collins et al. (2"
U08-1021,U07-1019,1,0.828341,"sers. 1 Introduction In many NLP applications, morphological preprocessing such as stemming is intuitively felt to be important, especially when dealing with morphologically rich languages. In fairly early work on morphological disambiguation of the agglutinative language Turkish, Hakkani-Tur et al. (2000) cite the note of Hankamer (1989) that a single Turkish root Mark Dras Centre for Language Technology Macquarie University Sydney, Australia madras@ics.mq.edu.au can potentially have over a million inflected variants; such a proliferation of forms could exacerbate any data sparsity problems. Zwarts and Dras (2007a) showed, for languages that differ in morphological richness but are otherwise structurally similar, that, as expected, morphological preprocessing in an Statistical Machine Translation (SMT) environment confers greater benefits on the morphologically richer language than on the morphologically poorer one. However, it is not always the case that morphological preprocessing of a morphologically rich language does provide a benefit. Again in an SMT context, Virpioja et al. (2007) preprocessed Finnish (a Uralic language, typologically between inflected and agglutinative, and consequently morpho"
U08-1021,2007.mtsummit-papers.74,1,0.855258,"sers. 1 Introduction In many NLP applications, morphological preprocessing such as stemming is intuitively felt to be important, especially when dealing with morphologically rich languages. In fairly early work on morphological disambiguation of the agglutinative language Turkish, Hakkani-Tur et al. (2000) cite the note of Hankamer (1989) that a single Turkish root Mark Dras Centre for Language Technology Macquarie University Sydney, Australia madras@ics.mq.edu.au can potentially have over a million inflected variants; such a proliferation of forms could exacerbate any data sparsity problems. Zwarts and Dras (2007a) showed, for languages that differ in morphological richness but are otherwise structurally similar, that, as expected, morphological preprocessing in an Statistical Machine Translation (SMT) environment confers greater benefits on the morphologically richer language than on the morphologically poorer one. However, it is not always the case that morphological preprocessing of a morphologically rich language does provide a benefit. Again in an SMT context, Virpioja et al. (2007) preprocessed Finnish (a Uralic language, typologically between inflected and agglutinative, and consequently morpho"
U08-1021,C08-1145,1,0.879868,"Missing"
U09-1008,C04-1088,0,0.0644578,"Missing"
U09-1008,W07-0602,0,0.679878,"epartments; the application that motivates the current work is profiling of phishing texts, texts that are designed to deceive a user into giving away confidential details (Fette et al., 2007; Zheng et al., 2003). The particular characteristic of interest in this paper is the native language of an author, where this is not the language that the text is written in. There has been only a relatively small amount of other research investigating this question, notably Koppel et Mark Dras Centre for Language Technology Macquarie University Sydney, NSW, Australia madras@science.mq.edu.au al. (2005), Tsur and Rappoport (2007), Estival et al. (2007), and van Halteren (2008). In general these tackle the problem as a text classification task using machine learning, with features over characters, words, parts of speech, and document structures. Koppel et al. (2005) also suggest syntactic features, although they do not use them in that work. The goal of this paper is to make a preliminary investigation into the use of syntactic errors in native language identification. The research drawn on for this work comes from the field of contrastive analysis in second language acquisition (SLA). According to the contrastive anal"
U09-1008,C08-1118,0,0.0834117,"Missing"
U10-1007,W02-1018,0,0.0291934,"g et al. (2007) (manual rules, Chinese-to-English), Habash (2007) (automatic rules, Arabic-to-English) and Popovi´c and Ney (2006) (manual rules, Spanish/English-toSpanish/English/German). Despite the success of the reordering-aspreprocessing approach overall, Collins et al. (2005) found that in a human evaluation on 100 sentences, there were still several cases in which the baseline system translation was preferred over that produced with the reordering. The authors note this finding but do not analyse it further. Phrase-based statistical MT Phrase-based statistical MT (PSMT) systems such as Marcu and Wong (2002) and Koehn et al. (2007) have set the standard for statistical MT for many years. While successful, these systems make limited use of linguistic information about the syntax of the languages which, intuitively, would seem to be useful. Much research is now focused on how to incorporate syntax into statistical MT, for example by using linguistic parse trees on one or both sides of the translation (e.g. Yamada and Knight (2001), Quirk et al. (2005)) or by incorporating only select aspects of syntactic structure, such as recursive structure (Chiang, 2007) or discontinuous phrases (Galley and Mann"
U10-1007,P02-1040,0,0.078781,"as dual-path PSMT (§3). We then augment the model with a number of confidence features to enable it to evaluate which of the two paths is more likely to yield the best translation (§3.2). We reimplement the Collins et al. (2005) reordering preprocessing step and conduct some preliminary experiments in German-toEnglish translation (§4). Our results (§5) do not replicate the finding of Collins et al. (2005) that the preprocessing step produces better translation results overall. However, results for our dual-path PSMT system do show an improvement, with our plain system achieving a BLEU score (Papineni et al., 2002) of 21.39, an increase of 0.62 over the baseline. We therefore conclude that a syntactically-informed reordering preprocessing step is inconsistently of use in PSMT, and that enabling the system to choose when to use the reordering leads to improved translation performance. Susan Howlett and Mark Dras. 2010. Dual-Path Phrase-Based Statistical Machine Translation. In Proceedings of Australasian Language Technology Association Workshop, pages 32−40 2 2.1 Background and Related Work tem for French–English translation that, instead of using hand-crafted reordering rules, automatically learns reord"
U10-1007,P06-1055,0,0.0949076,"ocess itself. Unlike Xiong et al. (2010), we use these features directly in translation. 2.4 Dual-Path PSMT In this paper, we develop a dual-path PSMT system. §3.1 introduces the lattice input format, by which we provide the system with two variants of the input sentence: the original and the reordered alternative produced by the preprocessing step. §3.2 outlines the confidence features that we include in the translation model to help the system choose between the two alternatives. Our system is built upon the PSMT system Moses (Koehn et al., 2007). For reordering, we use the Berkeley parser (Petrov et al., 2006) and the rules given by Collins et al. (2005), but any reordering preprocessing step could equally be used. Further details of our systems are given in §4. Reordering lattices Our work also bears some similarity to recent work using a reordering lattice or reordering forest as input to the translation system. Examples include Ge (2010), Dyer and Resnik (2010) and Zhang et al. (2007). In these systems, the input structure simultaneously represents many possible reorderings of a sentence, which are produced from a single parse and capture all possible combinations of individual reordering choice"
U10-1007,popovic-ney-2006-pos,0,0.0533327,"Missing"
U10-1007,P05-1034,0,0.035341,"dering. The authors note this finding but do not analyse it further. Phrase-based statistical MT Phrase-based statistical MT (PSMT) systems such as Marcu and Wong (2002) and Koehn et al. (2007) have set the standard for statistical MT for many years. While successful, these systems make limited use of linguistic information about the syntax of the languages which, intuitively, would seem to be useful. Much research is now focused on how to incorporate syntax into statistical MT, for example by using linguistic parse trees on one or both sides of the translation (e.g. Yamada and Knight (2001), Quirk et al. (2005)) or by incorporating only select aspects of syntactic structure, such as recursive structure (Chiang, 2007) or discontinuous phrases (Galley and Manning, 2010). One area where this lack of syntactic information is felt is the distortion model of the PSMT system. This component governs the relative movement of phrases and is the primary means of dealing with word order differences in translation. Common options are distance-based models (where movement is penalised proportionally to the distance moved) and lexicalised models (where probability of movement is conditioned upon the phrase being m"
U10-1007,D07-1077,0,0.0811222,"improved translation performance. Susan Howlett and Mark Dras. 2010. Dual-Path Phrase-Based Statistical Machine Translation. In Proceedings of Australasian Language Technology Association Workshop, pages 32−40 2 2.1 Background and Related Work tem for French–English translation that, instead of using hand-crafted reordering rules, automatically learns reordering patterns from the corpus. Automatically-acquired rules may be noisier and less intuitive than hand-crafted rules but the approach has the advantage of being more easily extended to new language pairs. Other examples of systems include Wang et al. (2007) (manual rules, Chinese-to-English), Habash (2007) (automatic rules, Arabic-to-English) and Popovi´c and Ney (2006) (manual rules, Spanish/English-toSpanish/English/German). Despite the success of the reordering-aspreprocessing approach overall, Collins et al. (2005) found that in a human evaluation on 100 sentences, there were still several cases in which the baseline system translation was preferred over that produced with the reordering. The authors note this finding but do not analyse it further. Phrase-based statistical MT Phrase-based statistical MT (PSMT) systems such as Marcu and Wong"
U10-1007,C04-1073,0,0.755345,"ce into sequences of adjacent words called phrases, then translating each phrase and reordering the phrases according to a distortion model. The distortion model may be lexicalised but does not typically incorporate information about the syntactic structure of the sentence. As such, although PSMT has been very successful, it suffers from the lack of a principled mechanism for handling long-distance reordering phenomena due to word order differences between languages. One method for addressing this difficulty is the reordering-as-preprocessing approach, exemplified by Collins et al. (2005) and Xia and McCord (2004), where PSMT is coupled with a preprocessing step that reorders input sentences to more closely parallel the target language word order. Although this leads to improved performance overall, Collins et al. (2005) show that the reordering-as-preprocessing system does not consistently provide better translations than the PSMT baseline on a sentence-by-sentence basis. One possible reason could be errors in the parse or the consequent reordering. Chiang et al. (2009) used features indicating problematic use of syntax to improve performance within hierarchical and syntax-based translation. In this w"
U10-1007,P10-1062,0,0.0157254,"reordering-aspreprocessing approach of Collins et al. (2005). Working with German-to-English translation, Collins et al. (2005) parse input sentences with a constituent-structure parser and apply six hand-crafted rules to reorder the German text toward English word order. These rules target the placement of non-finite and finite verbs, subjects, particles and negation. The authors demonstrate a statistically significant improvement in BLEU score over the baseline PSMT system. Many other reordering-as-preprocessing systems exist. Xia and McCord (2004) present a sys33 3 non-terminal in a rule. Xiong et al. (2010) derive features from the Link Grammar parser, in combination with word posterior probabilities, to detect MT errors (in order to subsequently improve translation quality). Unlike Chiang et al. (2009), we work with PSMT and use features that consider the parse tree as a whole or aspects of the reordering process itself. Unlike Xiong et al. (2010), we use these features directly in translation. 2.4 Dual-Path PSMT In this paper, we develop a dual-path PSMT system. §3.1 introduces the lattice input format, by which we provide the system with two variants of the input sentence: the original and th"
U10-1007,P01-1067,0,0.0716585,"hat produced with the reordering. The authors note this finding but do not analyse it further. Phrase-based statistical MT Phrase-based statistical MT (PSMT) systems such as Marcu and Wong (2002) and Koehn et al. (2007) have set the standard for statistical MT for many years. While successful, these systems make limited use of linguistic information about the syntax of the languages which, intuitively, would seem to be useful. Much research is now focused on how to incorporate syntax into statistical MT, for example by using linguistic parse trees on one or both sides of the translation (e.g. Yamada and Knight (2001), Quirk et al. (2005)) or by incorporating only select aspects of syntactic structure, such as recursive structure (Chiang, 2007) or discontinuous phrases (Galley and Manning, 2010). One area where this lack of syntactic information is felt is the distortion model of the PSMT system. This component governs the relative movement of phrases and is the primary means of dealing with word order differences in translation. Common options are distance-based models (where movement is penalised proportionally to the distance moved) and lexicalised models (where probability of movement is conditioned up"
U10-1007,W07-0401,0,0.0401195,"Missing"
U10-1007,C08-1145,1,0.884174,"ystem. This component governs the relative movement of phrases and is the primary means of dealing with word order differences in translation. Common options are distance-based models (where movement is penalised proportionally to the distance moved) and lexicalised models (where probability of movement is conditioned upon the phrase being moved). Without syntactic information here, PSMT systems lack a principled way to manage long-distance movements, leading to difficulty in language pairs where this is needed, such as English and Japanese or German. 2.2 2.3 Features for improved translation Zwarts and Dras (2008) explore the Collins et al. (2005) finding by examining whether machine learning techniques can be used to predict, on a sentence-by-sentence basis, whether the translation of the reordered sentence is to be preferred over the alternative. For features, they use sentence length, parse probability from the Collins parser and unlinked fragment count from the Link Grammar parser on the English side of the translation. The authors find that, when used on the source side (in English-to-Dutch translation), these features provide no significant improvement in BLEU score, while as target-side features"
U10-1007,N09-1025,0,0.0420869,"Missing"
U10-1007,J07-2003,0,0.0450015,"istical MT (PSMT) systems such as Marcu and Wong (2002) and Koehn et al. (2007) have set the standard for statistical MT for many years. While successful, these systems make limited use of linguistic information about the syntax of the languages which, intuitively, would seem to be useful. Much research is now focused on how to incorporate syntax into statistical MT, for example by using linguistic parse trees on one or both sides of the translation (e.g. Yamada and Knight (2001), Quirk et al. (2005)) or by incorporating only select aspects of syntactic structure, such as recursive structure (Chiang, 2007) or discontinuous phrases (Galley and Manning, 2010). One area where this lack of syntactic information is felt is the distortion model of the PSMT system. This component governs the relative movement of phrases and is the primary means of dealing with word order differences in translation. Common options are distance-based models (where movement is penalised proportionally to the distance moved) and lexicalised models (where probability of movement is conditioned upon the phrase being moved). Without syntactic information here, PSMT systems lack a principled way to manage long-distance moveme"
U10-1007,P05-1066,0,0.0678397,"ceeds by dividing a sentence into sequences of adjacent words called phrases, then translating each phrase and reordering the phrases according to a distortion model. The distortion model may be lexicalised but does not typically incorporate information about the syntactic structure of the sentence. As such, although PSMT has been very successful, it suffers from the lack of a principled mechanism for handling long-distance reordering phenomena due to word order differences between languages. One method for addressing this difficulty is the reordering-as-preprocessing approach, exemplified by Collins et al. (2005) and Xia and McCord (2004), where PSMT is coupled with a preprocessing step that reorders input sentences to more closely parallel the target language word order. Although this leads to improved performance overall, Collins et al. (2005) show that the reordering-as-preprocessing system does not consistently provide better translations than the PSMT baseline on a sentence-by-sentence basis. One possible reason could be errors in the parse or the consequent reordering. Chiang et al. (2009) used features indicating problematic use of syntax to improve performance within hierarchical and syntax-ba"
U10-1007,N10-1128,0,0.0377186,"Missing"
U10-1007,N10-1140,0,0.0164827,"and Wong (2002) and Koehn et al. (2007) have set the standard for statistical MT for many years. While successful, these systems make limited use of linguistic information about the syntax of the languages which, intuitively, would seem to be useful. Much research is now focused on how to incorporate syntax into statistical MT, for example by using linguistic parse trees on one or both sides of the translation (e.g. Yamada and Knight (2001), Quirk et al. (2005)) or by incorporating only select aspects of syntactic structure, such as recursive structure (Chiang, 2007) or discontinuous phrases (Galley and Manning, 2010). One area where this lack of syntactic information is felt is the distortion model of the PSMT system. This component governs the relative movement of phrases and is the primary means of dealing with word order differences in translation. Common options are distance-based models (where movement is penalised proportionally to the distance moved) and lexicalised models (where probability of movement is conditioned upon the phrase being moved). Without syntactic information here, PSMT systems lack a principled way to manage long-distance movements, leading to difficulty in language pairs where t"
U10-1007,N10-1127,0,0.0691319,"ep. §3.2 outlines the confidence features that we include in the translation model to help the system choose between the two alternatives. Our system is built upon the PSMT system Moses (Koehn et al., 2007). For reordering, we use the Berkeley parser (Petrov et al., 2006) and the rules given by Collins et al. (2005), but any reordering preprocessing step could equally be used. Further details of our systems are given in §4. Reordering lattices Our work also bears some similarity to recent work using a reordering lattice or reordering forest as input to the translation system. Examples include Ge (2010), Dyer and Resnik (2010) and Zhang et al. (2007). In these systems, the input structure simultaneously represents many possible reorderings of a sentence, which are produced from a single parse and capture all possible combinations of individual reordering choices. Like these systems, we use a complex input structure to translate across multiple variations of the sentence simultaneously, and choose between the resulting translations within the translation model itself. However, like Zwarts and Dras (2008), we consider only two possibilities: the original sentence and the sentence with a partic"
U10-1007,2007.mtsummit-papers.29,0,0.344297,"k Dras. 2010. Dual-Path Phrase-Based Statistical Machine Translation. In Proceedings of Australasian Language Technology Association Workshop, pages 32−40 2 2.1 Background and Related Work tem for French–English translation that, instead of using hand-crafted reordering rules, automatically learns reordering patterns from the corpus. Automatically-acquired rules may be noisier and less intuitive than hand-crafted rules but the approach has the advantage of being more easily extended to new language pairs. Other examples of systems include Wang et al. (2007) (manual rules, Chinese-to-English), Habash (2007) (automatic rules, Arabic-to-English) and Popovi´c and Ney (2006) (manual rules, Spanish/English-toSpanish/English/German). Despite the success of the reordering-aspreprocessing approach overall, Collins et al. (2005) found that in a human evaluation on 100 sentences, there were still several cases in which the baseline system translation was preferred over that produced with the reordering. The authors note this finding but do not analyse it further. Phrase-based statistical MT Phrase-based statistical MT (PSMT) systems such as Marcu and Wong (2002) and Koehn et al. (2007) have set the standa"
U10-1007,P07-2045,0,0.0725022,"verall, Collins et al. (2005) show that the reordering-as-preprocessing system does not consistently provide better translations than the PSMT baseline on a sentence-by-sentence basis. One possible reason could be errors in the parse or the consequent reordering. Chiang et al. (2009) used features indicating problematic use of syntax to improve performance within hierarchical and syntax-based translation. In this work, we want to see whether syntax-related features can help choose between original and reordered sentence translations in PSMT. We use as our starting point the PSMT system Moses (Koehn et al., 2007). In order to use features within the system’s log-linear model to assess the reliability of syntax, it is necessary to input both variants simultaneously. To do this, we adapt in a novel way the lattice input of Moses; we refer to this new system as dual-path PSMT (§3). We then augment the model with a number of confidence features to enable it to evaluate which of the two paths is more likely to yield the best translation (§3.2). We reimplement the Collins et al. (2005) reordering preprocessing step and conduct some preliminary experiments in German-toEnglish translation (§4). Our results (§"
U10-1011,2005.eamt-1.15,0,0.14096,"Missing"
U10-1011,I08-1059,0,0.0610847,"Missing"
U10-1011,P06-1032,0,0.0357918,"Missing"
U10-1011,han-etal-2010-using,0,0.0614885,"the overall classification accuracy. 1 Introduction Automatically judging sentences for their grammaticality has been a long-standing research problem within the natural language processing community. The ability of distinguishing grammatical sentences from ungrammatical ones has many potential applications, which include evaluating language technology systems such as natural language generation (Mutton et al., 2007) and machine translation (Gamon et al., 2005), as well as assessing language competence of second language or foreign language learners (Brockett et al., 2006; Gamon et al., 2008; Han et al., 2010). Various approaches have been proposed in the past to address this typical classification problem. Mark Dras Centre for Language Technology Macquarie University Sydney, NSW, Australia mark.dras@mq.edu.au A number of these existing studies attempt to exploit some form of ‘parser byproduct’ as classification features for machine learning: for instance, (log) probability of a parse tree, number of partial (incomplete) parse trees, parsing duration, and such (Mutton et al., 2007; Sun et al., 2007; Foster et al., 2008; Wagner et al., 2009). The aim of this paper is to examine whether the primary o"
U10-1011,P03-1054,0,0.0224597,"roduced by native English speakers. Dickinson (2010), in other recent corpus-based research aiming to address morphological errors found in highly inflecting languages, creates learner-like morphological errors from a segmented lexicon. 3 3.2 In order to enable a parser to be able to parse ungrammatical sentences, we re-train a probabilistic parser on both grammatical and ungrammatical corpora. This idea is adopted from Foster et al. (2008). By and large, we replicate the experiments conducted in Foster et al. (2008) with the exception that the parser used in our study is the Stanford Parser (Klein and Manning, 2003), chosen for ease of re-training. In this first stage, we conduct five experiments to re-train the Stanford Parser to induce a more robust parser capable of parsing both grammatical and ungrammatical sentences. In the first three experiments, three models of parser are induced by training on three different sets of corpora — first on the original WSJ (PureWSJ); second on the noisy WSJ (NoisyWSJ); and third on both the original and noisy WSJ (PureWSJ plus NoisyWSJ). We denote these three parser models as PureParser, NoisyParser, and MixedParser. In order to gauge its ability of parsing both gra"
U10-1011,P05-1022,0,0.117068,"8). Their intention is to improve the robustness of a probabilistic parser which might not be initially designed to handle ungrammatical sentences. Retraining on both grammatical and ungrammatical sentences enabled a parser to parse ungrammatical sentences at a relatively satisfactory level without compromising its initial performance on grammatical sentences. To attain an optimal parsing accuracy, the parser output of a sentence is chosen according to the highest parse probability of the most likely parse tree returned by the three induced models of the Charniak and Johnson reranking parser (Charniak and Johnson, 2005) trained across three different corpora — grammatical, ungrammatical, and a combination of both. Their experiments show that their parse probability-based classifier which can be considered as an integration of two parsers (one trained on grammatical data and the other trained on some ungrammatical data) is able to parse ungrammatical sentences better than the original parser trained exclusively on a grammatical corpus. The grammatical corpus used by Foster et al. (2008) is the Wall Street Journal (WSJ) treebank, and the ungrammatical version is one that they generated (see Section 2.2). In a"
U10-1011,C10-1030,0,0.0205354,"Missing"
U10-1011,P07-1044,1,0.872082,"Missing"
U10-1011,P07-1010,0,0.304628,"Missing"
U10-1011,W09-2112,0,0.344534,"Missing"
U10-1011,P07-1011,0,0.420604,"Missing"
U10-1011,P08-2056,0,0.0971332,"Missing"
U11-1013,U11-1013,1,0.0512755,"Missing"
U11-1013,C90-3059,0,0.853246,"Missing"
U11-1013,W02-1503,0,0.334737,"Missing"
U11-1013,cahill-etal-2000-enabling,0,0.711189,"Missing"
U11-1013,W07-2303,0,0.328285,"Missing"
U11-1013,E89-1018,0,0.73144,"Missing"
U11-1013,C92-3158,0,0.687291,"Missing"
U11-1013,W00-1403,0,0.695306,"involved in building a new system from scratch (Bateman et al., 1999). Such claims have been made since the very first MNLG systems; the FoG system generating weather forecasts in English and French (Bourbeau et al., 1990) is a case in point. Consequently, MNLG has been applied for a large number of text types: government statistics reports (Iordanskaja et al., 1992), technical instruction manuals (Paris et al., 1995), fairy tales (Callaway and Lester, 2002), museum tours (Callaway et al., 2005), medical terminology (Rassinoux et al., 2007), codes of practice (Evans et al., 2008), and so on. Marcu et al. (2000), in reviewing some of the earlier work, comment that MNLG systems need to abstract as much as possible away from the individual language generated: If an [MNLG] system needs to develop language dependent knowledge bases, and language dependent algorithms for content selection, text planning, and sentence planning, it is difficult to justify its economic viability. However, if most of these components are language independent and/or much of the code can be reused, an [MNLG] system becomes a viable option. Bateman et al. (1999) similarly emphasise the importance of reducing language dependence."
U11-1013,C90-1021,0,\N,Missing
U11-1013,J93-4001,0,\N,Missing
U11-1015,D11-1131,1,0.830287,"Missing"
U11-1015,P05-1022,1,0.590474,"(Granger et al., 2009), across seven languages (those of Koppel et al. (2005) with the two Asian languages Chinese and Japanese). The later work of Wong and Dras (2011), on the same data, further explored the usefulness of syntactic features in a broader sense by characterising syntactic errors with cross sections of parse trees obtained from statistical parsing. More specifically, they utilised two types of parse tree substructure to use as classification features — horizontal slices of the trees as sets of CFG production rules and the feature schemas used in discriminative parse reranking (Charniak and Johnson, 2005). It was demonstrated that using these kinds of syntactic features performs significantly better than lexical features alone. One key phenomenon observed by Wong and Dras (2011) was that there were different proportions of parse production rules indicative of particular native languages. One example is the production rule NP → NN NN, which appears to be very common amongst Chinese speakers compared with other native language groups; they claim that this is likely to reflect determiner-noun agreement errors, as that rule is used at the expense of one headed by a plural noun (NP → NN NNS). Our i"
U11-1015,P10-1117,1,0.83616,"to estimate the posterior topic distributions θ j for each document Dj as well as the posterior word distributions φi for each topic i that maximise the log likelihood of the corpus. As exact inference of these posterior distributions is generally intractable, there is a wide variety of means of approximate inference for LDA models which include approximation algorithms such as Variational Bayes (Blei et al., 2003) and expectation propagation (Minka and Lafferty, 2002) as well as Markov Chain Monte Carlo inference algorithm with Gibbs sampling (Griffiths and Steyvers, 2004). 3.1.2 LDA as PCFG Johnson (2010) showed that LDA topic models can be regarded as a specific type of probabilistic context-free grammar (PCFG), and that Bayesian inference for PCFGs can be used to learn LDA models where the inferred distributions of PCFGs correspond to those distributions of LDA. A general schema used for generating PCFG rule instances for representing m documents with t topics is as follows:2 Sentence → Doc0j Doc0j → j Doc0j → Doc0j Docj Docj → T opici T opici → w of LDA. Similarly, inference on the posterior rule distributions can be approximated with Variational Bayes and Gibbs sampling. We use this PCFG f"
U11-1015,W11-0321,0,0.164211,"exactly these that are used in NLI, so the above work does guarantee that an LDAbased approach will be helpful here. Two particularly relevant pieces of work on using LDA in classification are for the related task of authorship attribution, determining which author wrote a particular document. Rajkumar et al. (2009) claim that models with stopwords (function words) alone are sufficient to achieve high accuracy in classification, which seems to peak at 25 topics, and outperform content word-based models; the results presented in Table 2 and the discussion are, however, somewhat contradictory. Seroussi et al. (2011) also include both function words and content words in their models; they find that filtering words by frequency is almost always harmful, suggesting that function words are helping in this task.1 In this paper we will explore both function words and PoS n-grams, the latter of which is quite novel to our knowledge in terms of classification using LDA, to investigate whether clustering shows any potential for our task. 3 Experimental Setup 3.1 3.1.1 Mechanics of LDA General Definition Formally, each document is formed from a fixed set of vocabulary V and fixed set of topics T (|T |= t). Followi"
U11-1015,P08-1036,0,0.114902,"c subfields. It has also been augmented in various ways: supervised LDA, where topic models are integrated with a response variable, was introduced by Blei and McAuliffe (2008) and applied to predicting sentiment scores from movie review data, treating it as a regression problem rather than a classification problem. Work by Wang et al. (2009) followed from that, extending it to classification problems, and applying it to the simultaneous classification and annotation of images. An alternative approach to joint models of text and response variables for sentiment classification of review texts (Titov and McDonald, 2008), with a particular focus on constructing topics related to aspects of reviews (e.g. food, decor, or service for restaurant reviews), found that LDA topics were predictively useful and seemed qualitatively intuitive. In all of this preceding work, a document to be classified is represented by an exchangeable set of (content) words: function words are generally removed, and are not typically found in topics useful for classification. It is exactly these that are used in NLI, so the above work does guarantee that an LDAbased approach will be helpful here. Two particularly relevant pieces of work"
U11-1015,W07-0602,0,0.2051,"racy decreases, there is some evidence of coherent clustering, which could help with much larger syntactic feature spaces. 1 Introduction Native language identification (NLI), the task of determining the native language of an author writing in a second language, typically English, has gained increased attention in recent years. The problem was first phrased as a text classification task by Koppel et al. (2005), using a machine learner with fundamentally lexical features — function words, character ngrams, and part-of-speech (PoS) n-grams. A number of subsequent pieces of work, such as that of Tsur and Rappoport (2007), Estival et al. (2007), Wong and Dras (2009) and Wong and Dras (2011), have taken that as a starting point, typically along with a wider range of features, such as document structure or syntactic structure. Wong and Dras (2011) looked particularly at syntactic structure, in the form of production rules and parse reranking templates. They noted that they did not find the expected instances of clearly ungrammatical elements of syntactic structure indicating non-native speaker errors; instead there were just different distributions over regular elements of grammatical structure for different nat"
U11-1015,U09-1008,1,0.916533,"clustering, which could help with much larger syntactic feature spaces. 1 Introduction Native language identification (NLI), the task of determining the native language of an author writing in a second language, typically English, has gained increased attention in recent years. The problem was first phrased as a text classification task by Koppel et al. (2005), using a machine learner with fundamentally lexical features — function words, character ngrams, and part-of-speech (PoS) n-grams. A number of subsequent pieces of work, such as that of Tsur and Rappoport (2007), Estival et al. (2007), Wong and Dras (2009) and Wong and Dras (2011), have taken that as a starting point, typically along with a wider range of features, such as document structure or syntactic structure. Wong and Dras (2011) looked particularly at syntactic structure, in the form of production rules and parse reranking templates. They noted that they did not find the expected instances of clearly ungrammatical elements of syntactic structure indicating non-native speaker errors; instead there were just different distributions over regular elements of grammatical structure for different native languages. Our intuition is that it is se"
U11-1015,D11-1148,1,0.587838,"help with much larger syntactic feature spaces. 1 Introduction Native language identification (NLI), the task of determining the native language of an author writing in a second language, typically English, has gained increased attention in recent years. The problem was first phrased as a text classification task by Koppel et al. (2005), using a machine learner with fundamentally lexical features — function words, character ngrams, and part-of-speech (PoS) n-grams. A number of subsequent pieces of work, such as that of Tsur and Rappoport (2007), Estival et al. (2007), Wong and Dras (2009) and Wong and Dras (2011), have taken that as a starting point, typically along with a wider range of features, such as document structure or syntactic structure. Wong and Dras (2011) looked particularly at syntactic structure, in the form of production rules and parse reranking templates. They noted that they did not find the expected instances of clearly ungrammatical elements of syntactic structure indicating non-native speaker errors; instead there were just different distributions over regular elements of grammatical structure for different native languages. Our intuition is that it is several elements together t"
U12-1005,J08-4004,0,0.0441666,"Missing"
U12-1005,W04-3202,0,0.0199897,"og-linear model with features derived from the HPSG structure. The supervised framework requires sentences annotated with parses, which is where active learning can play a role. Osborne and Baldridge (2004) apply both QBU with an ensemble of models, and QBC, and show that this decreases annotation cost, measured both in number of sentences to achieve a particular level of parse selection accuracy, and in a measure of sentence complexity, with respect to random selection. However, this differs from the task of constructing a resource that is intended to be reused in a number of ways. First, as Baldridge and Osborne (2004) show, when “creating labelled training material (speciﬁcally, for them, for HPSG parse se27 Model Form+POS: Lemma+POS: Form+Lemma+POS: Form+CPOS: Form+Lemma+CPOS: Form+CPOS+POS: Lemma+CPOS+POS: Lemma+CPOS: Form+Lemma+CPOS+POS: LAS-1 60.6 61.3 61.5 62.1 62.9 63.0 63.1 63.3 63.3 UAS-1 70.3 70.8 70.8 72.5 72.6 72.9 72.4 72.7 73.1 LAS-2 64.4 64.6 64.6 65.0 66.1 66.0 66.0 65.1 66.5 UAS-2 74.2 74.3 74.5 76.1 76.2 76.0 76.2 75.7 76.3 Table 3: Preliminary MaltParser experiments with the Irish Dependency Treebank: Pre- and post-IAA-2 results lection) and later reusing it with other models, gains from"
U12-1005,D09-1031,0,0.0235575,"64.4 64.6 64.6 65.0 66.1 66.0 66.0 65.1 66.5 UAS-2 74.2 74.3 74.5 76.1 76.2 76.0 76.2 75.7 76.3 Table 3: Preliminary MaltParser experiments with the Irish Dependency Treebank: Pre- and post-IAA-2 results lection) and later reusing it with other models, gains from active learning may be negligible or even negative”: the simulation of active learning on an existing treebank under a particular model, with the goal of improving parser accuracy, may not correspond to a useful approach to constructing a treebank. Second, in the actual task of constructing a resource — interlinearized glossed text — Baldridge and Palmer (2009) show that the usefulness of particular example selection techniques in active learning varies with factors such as annotation expertise. They also note the importance of measures that are sensitive to the cost of annotation: the sentences that active learning methods select are often difﬁcult to annotate as well, and may result in no effective savings in time or other measures. To our knowledge, active learning has not yet been applied to the actual construction of a treebank: that is one of our goals. Further, most active learning work in NLP has used variants of QBU and QBC where instances"
U12-1005,baldwin-etal-2004-road,0,0.0693434,"et al., 2001), text categorisation (Lewis and Gale, 1994; Hoi et al., 2006) and word sense disambiguation (Chen et al., 2006). Olsson (2009) provides a survey of various approaches to active learning in NLP. For our work, the most relevant application of active learning to NLP is in parsing, for example, Thompson et al. (1999), Hwa et al. (2003), Osborne and Baldridge (2004) and Reichart and Rappoport (2007). Taking Osborne and Baldridge (2004) as an illustration, the goal of that work was to improve parse selection for HPSG: for all the analyses licensed by the HPSG English Resource Grammar (Baldwin et al., 2004) for a particular sentence, the task is to choose the best one using a log-linear model with features derived from the HPSG structure. The supervised framework requires sentences annotated with parses, which is where active learning can play a role. Osborne and Baldridge (2004) apply both QBU with an ensemble of models, and QBC, and show that this decreases annotation cost, measured both in number of sentences to achieve a particular level of parse selection accuracy, and in a measure of sentence complexity, with respect to random selection. However, this differs from the task of constructing"
U12-1005,C10-1011,0,0.0386426,"rs Thus, we rank the set of X trees (up ) based on their disagreement with a second reference parser.3 The 3 This assessment of disagreement between two trees is based on the number of dependency relations they disagree on, which is the fundamental idea of the F-complement measure of Ngai and Yarowsky (2000). Disagreement between 28 top Y trees from this ordered set are manually revised and added to the training set for the next iteration. We use MaltParser as the only parser in the passive learning setup and the main parser in the active learning setup. We use another dependency parser Mate (Bohnet, 2010) as our second parser in the active learning setup. Since we have 450 gold trees, we split them into a seed training set of 150 trees, a development set of 150 and a test set of 150. Due to time constraints we run the two versions of the algorithm for four iterations, and on each iteration 50 (Y) parse trees are handcorrected from a set of 200 (X). This means that the ﬁnal training set size for both setups is 350 trees (150 + (4*50)). However, the 4*50 training trees added to the seed training set of 150 are not the same for both setups. The set of 200 unseen sentences in each iteration is the"
U12-1005,N06-1016,0,0.0324761,"general technique applicable to many tasks involving machine learning. Two broad approaches are Query By Uncertainty (QBU) (Cohn et al., 1994), where examples about which the learner is least conﬁdent are selected for manual annotation; and Query By Committee (QBC) (Seung et al., 1992), where disagreement among a committee of learners is the criterion for selecting examples for annotation. Active learning has been used in a number of areas of NLP such as information extraction (Scheffer et al., 2001), text categorisation (Lewis and Gale, 1994; Hoi et al., 2006) and word sense disambiguation (Chen et al., 2006). Olsson (2009) provides a survey of various approaches to active learning in NLP. For our work, the most relevant application of active learning to NLP is in parsing, for example, Thompson et al. (1999), Hwa et al. (2003), Osborne and Baldridge (2004) and Reichart and Rappoport (2007). Taking Osborne and Baldridge (2004) as an illustration, the goal of that work was to improve parse selection for HPSG: for all the analyses licensed by the HPSG English Resource Grammar (Baldwin et al., 2004) for a particular sentence, the task is to choose the best one using a log-linear model with features de"
U12-1005,W08-1301,0,0.0917332,"arser (Nivre et al., 2006) on their original treebank of 300 sentences. Following the changes we made to the labelling scheme as a result of the second IAA study, we re-ran the same parsing experiments on the newly updated seed set of 300 sentences. We used 10fold cross-validation on the same feature sets (various combinations of form, lemma, ﬁne-grained POS and coarse-grained POS). The improved results, as shown in the ﬁnal two columns of Table 3, reﬂect the value of undertaking an analysis of IAA-1 results. 2 This label is also used in the English Stanford Dependency Scheme (de Marneffe and Manning, 2008) 3 Active Learning Experiments Now that the annotation scheme and guide have reached a stable state, we turn our attention to the role of active learning in parser and treebank development. Before describing our preliminary work in this area, we discuss related work . 3.1 Related Work Active learning is a general technique applicable to many tasks involving machine learning. Two broad approaches are Query By Uncertainty (QBU) (Cohn et al., 1994), where examples about which the learner is least conﬁdent are selected for manual annotation; and Query By Committee (QBC) (Seung et al., 1992), where"
U12-1005,dzeroski-etal-2006-towards,0,0.0278089,"ppropriate labelling scheme, as well as considerable manual annotation (parsing). In general, manual annotation is desired to ensure high quality treebank data. Yet, as is often encountered when working with language, the task of manually annotating text can become repetitive, involving frequent encounters with similar linguistic structures. In an effort to speed up the creation of treebanks, there has been an increased focus towards automating, or at least, semi-automating the process using various bootstrapping techniques. A basic bootstrapping approach such as that outlined by Judge et al. (2006) involves several steps. Firstly a parser is trained on a set of gold standard trees. This parser is then used to parse a new set of unseen sentences. When these new trees are reviewed and corrected, they are combined with the ﬁrst set of trees and used to train a new parsing model. These steps are repeated until all sentences are parsed. By adding to the training data on each iteration, the parser is expected to improve progressively. The process of correcting Teresa Lynn, Jennifer Foster, Mark Dras and Elaine U´ı Dhonnchadha. 2012. Active Learning and the Irish Treebank. In Proceedings of Au"
U12-1005,P06-1063,0,0.0779378,"Missing"
U12-1005,lynn-etal-2012-irish,1,0.776429,"Missing"
U12-1005,P00-1016,0,0.0540267,"atch of X unseen sentences. In the active learning variant, we select these trees based on a notion of how informative they are, i.e. how much the parser might be improved if it knew how to parse them correctly. We approximate informativeness based on QBC, speciﬁcally, disagreement between a committee of two parsers Thus, we rank the set of X trees (up ) based on their disagreement with a second reference parser.3 The 3 This assessment of disagreement between two trees is based on the number of dependency relations they disagree on, which is the fundamental idea of the F-complement measure of Ngai and Yarowsky (2000). Disagreement between 28 top Y trees from this ordered set are manually revised and added to the training set for the next iteration. We use MaltParser as the only parser in the passive learning setup and the main parser in the active learning setup. We use another dependency parser Mate (Bohnet, 2010) as our second parser in the active learning setup. Since we have 450 gold trees, we split them into a seed training set of 150 trees, a development set of 150 and a test set of 150. Due to time constraints we run the two versions of the algorithm for four iterations, and on each iteration 50 (Y"
U12-1005,nivre-etal-2006-maltparser,0,0.248823,"Missing"
U12-1005,N04-1012,0,0.0207355,"Query By Committee (QBC) (Seung et al., 1992), where disagreement among a committee of learners is the criterion for selecting examples for annotation. Active learning has been used in a number of areas of NLP such as information extraction (Scheffer et al., 2001), text categorisation (Lewis and Gale, 1994; Hoi et al., 2006) and word sense disambiguation (Chen et al., 2006). Olsson (2009) provides a survey of various approaches to active learning in NLP. For our work, the most relevant application of active learning to NLP is in parsing, for example, Thompson et al. (1999), Hwa et al. (2003), Osborne and Baldridge (2004) and Reichart and Rappoport (2007). Taking Osborne and Baldridge (2004) as an illustration, the goal of that work was to improve parse selection for HPSG: for all the analyses licensed by the HPSG English Resource Grammar (Baldwin et al., 2004) for a particular sentence, the task is to choose the best one using a log-linear model with features derived from the HPSG structure. The supervised framework requires sentences annotated with parses, which is where active learning can play a role. Osborne and Baldridge (2004) apply both QBU with an ensemble of models, and QBC, and show that this decrea"
U12-1005,P07-1052,0,0.0243787,"t al., 1992), where disagreement among a committee of learners is the criterion for selecting examples for annotation. Active learning has been used in a number of areas of NLP such as information extraction (Scheffer et al., 2001), text categorisation (Lewis and Gale, 1994; Hoi et al., 2006) and word sense disambiguation (Chen et al., 2006). Olsson (2009) provides a survey of various approaches to active learning in NLP. For our work, the most relevant application of active learning to NLP is in parsing, for example, Thompson et al. (1999), Hwa et al. (2003), Osborne and Baldridge (2004) and Reichart and Rappoport (2007). Taking Osborne and Baldridge (2004) as an illustration, the goal of that work was to improve parse selection for HPSG: for all the analyses licensed by the HPSG English Resource Grammar (Baldwin et al., 2004) for a particular sentence, the task is to choose the best one using a log-linear model with features derived from the HPSG structure. The supervised framework requires sentences annotated with parses, which is where active learning can play a role. Osborne and Baldridge (2004) apply both QBU with an ensemble of models, and QBC, and show that this decreases annotation cost, measured both"
U12-1007,W10-0701,0,0.0207043,"of sentences for their relative ﬂuency and negativity. Mechanical Turk Our subjects were recruited through Amazon Mechanical Turk.2 Mechanical Turk is a web service providing cheap de2 45 http://www.mturk.com/ centralised work units called Human Intelligence Tasks (HITs), which have been used by computational linguistics research for experimentation. Snow et al. (2008) cite a number of studies at that time which used Mechanical Turk as an annotation tool, including several which used Mechanical Turk rather than expert annotators to produce a gold standard annotation to evaluate their systems. Callison-Burch and Dredze (2010) provide guidelines in appropriate design of tasks for Mechanical Turk, which we broadly follow. We ameliorate potential risks of using Mechanical Turk by conﬁning ourselves to asking workers for numerical ratings of sentences, rather than any more complex tasks, well within the type of tasks which Snow et al. (2008) reported success with; and like Molla and Santiago-Martinez (2011), giving all subjects two elimination questions in which the sentences within each pair were identical, that is, in which there was no lexical substitution. These, being identical, should receive identical scores—we"
U12-1007,esuli-sebastiani-2006-sentiwordnet,0,0.0654994,"uation of valence shifting We ﬁrst describe the construction of our test data, followed by the process for eliciting human judgements on the test data. 3.1 3.1.1 Test data Selection of negativity word pairs Quite a number of lexical resources related to sentiment have been developed, and it may seem likely that there would be an appropriate one for choosing pairs of near-synonyms where one is more negative and the other less. However, none are really suitable. • Several resources based on WordNet contain synsets annotated with sentiment information in some fashion: these include SentiWordNet (Esuli and Sebastiani, 2006), MicroWNOP (Cerini et al., 2007) and WordNet Affect (Strapparava and Valitutti, 2004), and a dataset of subjectivity- and polarityannotated WordNet senses by Su and Markert (2008). Individual words within a synset are not, however, given individual scores, which is what we need. • The General Inquirer word list (Stone et al., 1966), which contains unscored words in certain categories including positive (1915 words) and negative (2291 words), does not group words into sets of near-synonyms. • The subjectivity lexicon that is part of the MPQA Opinion Corpus does assign terms to categories, in t"
U12-1007,guerini-etal-2008-valentino,0,0.60927,"ask, and show that sentiment-based lexical paraphrases do consistently change the sentiment for readers. We then also show that the Kullback-Leibler divergence makes a useful preliminary measure of valence that corresponds to human judgements. 1 Introduction This paper looks at the problem of VALENCE SHIFTING , rewriting a text to preserve much of its meaning but alter its sentiment characteristics. For example, starting with the sentence If we have to have another parody of a post-apocalyptic America, does it have to be this bad?, we could make it more negative by changing bad to abominable. Guerini et al. (2008) say about valence shifting that it would be conceivable to exploit NLP techniques to slant original writings toward speciﬁc biased orientation, keeping as much as possible the same meaning . . . as an element of a persuasive system. For instance a strategic planner may decide to intervene on a draft text with the goal of “coloring” it emotionally. Mark Dras Centre for Language Technology Macquarie University mark.dras@mq.edu.au There is only a relatively small amount of work on this topic, which we review in Section 2. From this work, the task appears more difﬁcult than researchers originally"
U12-1007,N01-1009,0,0.241805,"(1996) for adapting to grammaticality judgements. In this experimental modality, subjects are asked evaluate stimuli based not on a ﬁxed rating scale, but on an arbitrary rating scale in comparison with an initial stimulus. For example, subjects might initially be asked to judge the acceptability of The cat by chased the dog. Assuming that the subject gives this an acceptability score of N , they will be asked to assign a multiplicative score to other sentences, that is, 2N to a sentence that is twice as acceptable and N2 to one half as acceptable. This same experimental modality was used by Lapata (2001) in which subjects evaluated the acceptability of paraphrases of adjectival phrases, for example, considering the acceptability of each of (4b) and (4c) as paraphrases of (4a): (4) a. a difﬁcult customer b. a customer that is difﬁcult to satisfy c. a customer that is difﬁcult to drive In a standard design and analysis of a ME experiment (Marks, 1974), all the stimuli given to the subjects have known relationships (for example, in the original psychophysics context, that the power level for one heat stimulus was half that of another stimulus), and the experimenter is careful to provide subjects"
U12-1007,W10-0204,0,0.0129151,"General Inquirer word list (Stone et al., 1966), which contains unscored words in certain categories including positive (1915 words) and negative (2291 words), does not group words into sets of near-synonyms. • The subjectivity lexicon that is part of the MPQA Opinion Corpus does assign terms to categories, in this case positive, negative, both or neutral, but does not score the strength of their affective meaning, although this corpus does rate their effectiveness as a cue for subjectivity analysis (Wiebe et al., 2005; Wilson et al., 2005). The closest work to that described here is that of Mohammad and Turney (2010) and Mohammad and Turney (forthcoming), who describe in detail the creation of EmoLex, a large polarity lexicon, using Mechanical Turk. Mohammad and Turney (forthcoming), rather than asking annotators to evaluate words in context as we are proposing here, instead ask them directly for their analysis of the word, ﬁrst using a synonym-ﬁnding task in order to give the worker the correct word sense to evaluate. Part of a sample annotation question given by Mohammad and Turney (forthcoming) is given in Table 1. The word source used is the Macquarie Thesaurus (Bernard, 1986). Our work differs from t"
U12-1007,U11-1012,0,0.0252643,"er of studies at that time which used Mechanical Turk as an annotation tool, including several which used Mechanical Turk rather than expert annotators to produce a gold standard annotation to evaluate their systems. Callison-Burch and Dredze (2010) provide guidelines in appropriate design of tasks for Mechanical Turk, which we broadly follow. We ameliorate potential risks of using Mechanical Turk by conﬁning ourselves to asking workers for numerical ratings of sentences, rather than any more complex tasks, well within the type of tasks which Snow et al. (2008) reported success with; and like Molla and Santiago-Martinez (2011), giving all subjects two elimination questions in which the sentences within each pair were identical, that is, in which there was no lexical substitution. These, being identical, should receive identical scores—we also explicitly pointed this out in the instructions—and therefore we could easily eliminate workers who did not read the instructions from the pool. Eliciting subjects’ responses We considered both categorical responses (e.g. Is sentence variant A more or less negative than sentence variant B, or are A and B equally negative?) and Magnitude Estimation (ME). Categorical responses o"
U12-1007,P05-1015,0,0.0503139,"the desired effect when a word is used in context. We therefore turn to hand-crafted data to test our hypotheses: words chosen so as to be noticeably negative, with a neutral or slightly negative near synonym. We chose 20 such word pairs, shown in Table 2. The more negative word of the pair is from the sentiment lists developed by Nielsen (2011),1 typically rated about 3 for negativity on his scale (where 5 is reserved for obscenities) and the less negative chosen by us. 3.1.2 Selection of sentences Our corpus for sentence selection is the SCALE dataset v1.0 movie review data set (SCALE 1.0) (Pang and Lee, 2005), a set of 5000 short movie reviews by four authors on the World Wide Web, and widely used in sentiment classiﬁcation tasks. Each movie review is accompanied by both a three and four degree sentiment rating (that is, a rating on a scale of 0 to 2, and on a scale of 0 to 3) together with original rating assigned by the author to their own review on a scale of 0 to 10. 1 Available from http://www2.imm.dtu.dk/ pubdb/views/publication_details.php?id= 6010 44 Question Which word is closest in meaning (most related) to startle? How positive (good, praising) is the word startle? How negative (bad, cr"
U12-1007,D08-1027,0,0.107075,"Missing"
U12-1007,strapparava-valitutti-2004-wordnet,0,0.0622729,"llowed by the process for eliciting human judgements on the test data. 3.1 3.1.1 Test data Selection of negativity word pairs Quite a number of lexical resources related to sentiment have been developed, and it may seem likely that there would be an appropriate one for choosing pairs of near-synonyms where one is more negative and the other less. However, none are really suitable. • Several resources based on WordNet contain synsets annotated with sentiment information in some fashion: these include SentiWordNet (Esuli and Sebastiani, 2006), MicroWNOP (Cerini et al., 2007) and WordNet Affect (Strapparava and Valitutti, 2004), and a dataset of subjectivity- and polarityannotated WordNet senses by Su and Markert (2008). Individual words within a synset are not, however, given individual scores, which is what we need. • The General Inquirer word list (Stone et al., 1966), which contains unscored words in certain categories including positive (1915 words) and negative (2291 words), does not group words into sets of near-synonyms. • The subjectivity lexicon that is part of the MPQA Opinion Corpus does assign terms to categories, in this case positive, negative, both or neutral, but does not score the strength of their"
U12-1007,C08-1104,0,0.0153889,"egativity word pairs Quite a number of lexical resources related to sentiment have been developed, and it may seem likely that there would be an appropriate one for choosing pairs of near-synonyms where one is more negative and the other less. However, none are really suitable. • Several resources based on WordNet contain synsets annotated with sentiment information in some fashion: these include SentiWordNet (Esuli and Sebastiani, 2006), MicroWNOP (Cerini et al., 2007) and WordNet Affect (Strapparava and Valitutti, 2004), and a dataset of subjectivity- and polarityannotated WordNet senses by Su and Markert (2008). Individual words within a synset are not, however, given individual scores, which is what we need. • The General Inquirer word list (Stone et al., 1966), which contains unscored words in certain categories including positive (1915 words) and negative (2291 words), does not group words into sets of near-synonyms. • The subjectivity lexicon that is part of the MPQA Opinion Corpus does assign terms to categories, in this case positive, negative, both or neutral, but does not score the strength of their affective meaning, although this corpus does rate their effectiveness as a cue for subjectivi"
U12-1007,W10-0211,0,0.339404,"rd list (Stone et al., 1966), which contains unscored words in certain categories including positive (1915 words) and negative (2291 words), does not group words into sets of near-synonyms. • The subjectivity lexicon that is part of the MPQA Opinion Corpus does assign terms to categories, in this case positive, negative, both or neutral, but does not score the strength of their affective meaning, although this corpus does rate their effectiveness as a cue for subjectivity analysis (Wiebe et al., 2005; Wilson et al., 2005). The closest work to that described here is that of Mohammad and Turney (2010) and Mohammad and Turney (forthcoming), who describe in detail the creation of EmoLex, a large polarity lexicon, using Mechanical Turk. Mohammad and Turney (forthcoming), rather than asking annotators to evaluate words in context as we are proposing here, instead ask them directly for their analysis of the word, ﬁrst using a synonym-ﬁnding task in order to give the worker the correct word sense to evaluate. Part of a sample annotation question given by Mohammad and Turney (forthcoming) is given in Table 1. The word source used is the Macquarie Thesaurus (Bernard, 1986). Our work differs from t"
U12-1007,H05-1044,0,0.0154137,"t are not, however, given individual scores, which is what we need. • The General Inquirer word list (Stone et al., 1966), which contains unscored words in certain categories including positive (1915 words) and negative (2291 words), does not group words into sets of near-synonyms. • The subjectivity lexicon that is part of the MPQA Opinion Corpus does assign terms to categories, in this case positive, negative, both or neutral, but does not score the strength of their affective meaning, although this corpus does rate their effectiveness as a cue for subjectivity analysis (Wiebe et al., 2005; Wilson et al., 2005). The closest work to that described here is that of Mohammad and Turney (2010) and Mohammad and Turney (forthcoming), who describe in detail the creation of EmoLex, a large polarity lexicon, using Mechanical Turk. Mohammad and Turney (forthcoming), rather than asking annotators to evaluate words in context as we are proposing here, instead ask them directly for their analysis of the word, ﬁrst using a synonym-ﬁnding task in order to give the worker the correct word sense to evaluate. Part of a sample annotation question given by Mohammad and Turney (forthcoming) is given in Table 1. The word"
U14-1020,C12-1025,0,0.0932506,"ed for topic, we do not consider the use of purely lexical features such as word n-grams in this study. Topic bias can occur as a result of the subject matters or topics of the texts to be classified not not evenly distributed across the classes. For example, if in our training data all the texts written by English L1 speakers are on topic A, while all the French L1 authors write about topic B, then we have implicitly trained our classifier on the topics as well. In this case the classifier learns to distinguish our target variable through another confounding variable. Others researchers like Brooke and Hirst (2012), however, argue that lexical features cannot be simply ignored. Given the small size of our data and 4 http://www.csie.ntu.edu.tw/˜cjlin/ liblinear/ 141 Finnish Function Words Part-of-Speech tag n-grams In this model POS n-grams of size 1–3 were extracted. These n-grams capture small and very local syntactic patterns of language production and were used as classification features. Previous work and our experiments showed that sequences of size 4 or greater achieve lower accuracy, possibly due to data sparsity, so we do not include them. 5.3 Character n-grams This is a sub-lexical feature that"
U14-1020,W14-3625,1,0.431244,"ast few years. This surge of interest, coupled with the inaugural shared task in 2013 have resulted in NLI becoming a well-established NLP task. The NLI Shared Task in 2013 was attended by 29 teams from the NLP and SLA areas. An overview of the shared task results and a review of prior NLI work can be found in Tetreault et al. (2013). While there exists a large body of literature produced in the last decade, almost all of this work has focused exclusively on L2 English. The most recent work in this field has successfully presented the first applications of NLI to a large non-English datasets (Malmasi and Dras, 2014b; Malmasi and Dras, 2014a), evidencing the usefulness of syntactic features in distinguishing L2 Chinese and L2 Arabic texts. Finnish poses a particular challenge. In terms of morphological complexity, it is among the world’s most extreme: its number of cases, for example, places it in the highest category in the comparative World Atlas of Language Structures (Iggesen, 2013). Comrie (1989) proposed two scales for characterising morphology, the index of synthesis (based on the number of categories expressed per morpheme) and the index of fusion (based on the number of categories expressed per"
U14-1020,E14-4019,1,0.894263,"ast few years. This surge of interest, coupled with the inaugural shared task in 2013 have resulted in NLI becoming a well-established NLP task. The NLI Shared Task in 2013 was attended by 29 teams from the NLP and SLA areas. An overview of the shared task results and a review of prior NLI work can be found in Tetreault et al. (2013). While there exists a large body of literature produced in the last decade, almost all of this work has focused exclusively on L2 English. The most recent work in this field has successfully presented the first applications of NLI to a large non-English datasets (Malmasi and Dras, 2014b; Malmasi and Dras, 2014a), evidencing the usefulness of syntactic features in distinguishing L2 Chinese and L2 Arabic texts. Finnish poses a particular challenge. In terms of morphological complexity, it is among the world’s most extreme: its number of cases, for example, places it in the highest category in the comparative World Atlas of Language Structures (Iggesen, 2013). Comrie (1989) proposed two scales for characterising morphology, the index of synthesis (based on the number of categories expressed per morpheme) and the index of fusion (based on the number of categories expressed per"
U14-1020,W14-3708,1,0.792786,"ast few years. This surge of interest, coupled with the inaugural shared task in 2013 have resulted in NLI becoming a well-established NLP task. The NLI Shared Task in 2013 was attended by 29 teams from the NLP and SLA areas. An overview of the shared task results and a review of prior NLI work can be found in Tetreault et al. (2013). While there exists a large body of literature produced in the last decade, almost all of this work has focused exclusively on L2 English. The most recent work in this field has successfully presented the first applications of NLI to a large non-English datasets (Malmasi and Dras, 2014b; Malmasi and Dras, 2014a), evidencing the usefulness of syntactic features in distinguishing L2 Chinese and L2 Arabic texts. Finnish poses a particular challenge. In terms of morphological complexity, it is among the world’s most extreme: its number of cases, for example, places it in the highest category in the comparative World Atlas of Language Structures (Iggesen, 2013). Comrie (1989) proposed two scales for characterising morphology, the index of synthesis (based on the number of categories expressed per morpheme) and the index of fusion (based on the number of categories expressed per"
U14-1020,W13-1716,1,0.704144,"sh function words5 were extracted from the learner texts and used as features in this model. 5.2 4.2 Evaluation Methodology Consistent with most previous NLI studies and the NLI 2013 shared task, we report results as classification accuracy under k-fold cross-validation, with k = 10. In recent years this has become a de facto standard for reporting NLI results. 5 Experiments We experiment using three different feature types described in this section. Previous NLI research on English data has utilized a range of features types varying from surface features to more sophisticated syntactic ones (Malmasi et al., 2013). However, in most such studies the use of such deeper features is predicated on the availability of NLP tools and models for extracting those features. This, unfortunately, is not the case for Finnish and it was decided to make use of a simpler feature set in this preliminary study. As our data is not balanced for topic, we do not consider the use of purely lexical features such as word n-grams in this study. Topic bias can occur as a result of the subject matters or topics of the texts to be classified not not evenly distributed across the classes. For example, if in our training data all th"
U14-1020,E14-4033,0,0.126596,"Missing"
U14-1020,C12-1158,0,0.0549664,"ning improvements (Laufer and Girsai, 2008). There are a number of avenues for future work. A key limitation of this study, although beyond our control, is the limited amount of data used. We hope to evaluate our system on larger data as it becomes available. The application of more linguistically sophisticated features also merits further investigation, but this is limited by the availability of Finnish NLP tools and resources. Another possible improvement is the use of classifier ensembles to improve classification accuracy. This has previously been applied to English NLI with good results (Tetreault et al., 2012). We would also like to point to the failure to distinguish between the L2 and any other acquired languages as a more general criticism of the NLI literature to date. The current body of NLI literature fails to distinguish whether the learner language is in fact the writer’s second language, or whether it is possibly a third language (L3). It has been noted in the SLA literature that when acquiring an L3, there may be instances of both L1- and L2-based transfer effects on L3 production (Ringbom, 2001). Studies of such second language transfer effects during third language acquisition have been"
U14-1020,W13-1706,0,0.554553,"ive computational models in NLP (Jarvis and Crossley, 2012). Such analyses have traditionally been conducted manually by researchers, and the issues that arise when they are attempted on large corpora are well known (Ellis, 2008). Recently, researchers have noted that NLP has the tools to use large amounts of data to automate this analysis, using complex feature types. This has motivated studies in Native Language Identification (NLI), a subtype of text classification where the goal is to determine the native language (L1) of an author using texts they have written in a second language or L2 (Tetreault et al., 2013). Most work in SLA, NLI and NLP for that matter has dealt with English. This is largely due to the fact that since World War II, the world has witnessed the ascendancy of English as its lingua franca. While English is the native language of over 400 million people in the U.S., U.K. and the Commonwealth, there are also over a billion people who speak English as their second or foreign language (Guo and Beckett, 2007). This has created a global environment where learning multiple languages is not exceptional and this has fueled the growing research into language acquisition. However, while Engli"
U14-1020,D14-1144,1,\N,Missing
U15-1008,J92-4003,0,0.622882,"ructure and the sequence of sentences in the text. Domain-specific 3 Word Representations Word representations are mathematical objects associated with words. This representation is often, but not always, a vector where each dimension is a word feature (Turian et al., 2010). Various methods for inducing word representations have been proposed. These include distributional represen67 3.2 tations, such as LSA, LSI and LDA, as well as distributed representations, also known as word embeddings. Yet another type of representation is based on inducing a clustering over words, with Brown clustering (Brown et al., 1992) being the most well known method. This is the approach that we take in the present study. Recent work has demonstrated that unsupervised word representations induced from large unlabelled data can be used to improve supervised tasks, a type of semi-supervised learning. Examples of tasks where this has been applied include dependency parsing (Koo et al., 2008), Named Entity Recognition (NER) (Miller et al., 2004), sentiment analysis (Maas et al., 2011) and chunking (Turian et al., 2010). Such an approach could also be applied to the clinical information extraction task where although we only h"
U15-1008,U15-1006,1,0.883866,"Missing"
U15-1008,U11-1012,0,0.323456,"Missing"
U15-1008,P09-1056,0,0.0605632,"Missing"
U15-1008,E09-1070,0,0.0400976,"Missing"
U15-1008,N13-1039,0,0.0619871,"Missing"
U15-1008,P08-1068,0,0.0446688,"ributional represen67 3.2 tations, such as LSA, LSI and LDA, as well as distributed representations, also known as word embeddings. Yet another type of representation is based on inducing a clustering over words, with Brown clustering (Brown et al., 1992) being the most well known method. This is the approach that we take in the present study. Recent work has demonstrated that unsupervised word representations induced from large unlabelled data can be used to improve supervised tasks, a type of semi-supervised learning. Examples of tasks where this has been applied include dependency parsing (Koo et al., 2008), Named Entity Recognition (NER) (Miller et al., 2004), sentiment analysis (Maas et al., 2011) and chunking (Turian et al., 2010). Such an approach could also be applied to the clinical information extraction task where although we only have a very limited amount of labelled data, large-scale unlabelled data — hundreds of millions of tokens — is readily available to us. Researchers have noted a number of advantages to using word representations in supervised learning tasks. They produce substantially more compact models compared to fully lexicalized approaches where feature vectors have the sa"
U15-1008,P11-1015,0,0.0399372,"ations, also known as word embeddings. Yet another type of representation is based on inducing a clustering over words, with Brown clustering (Brown et al., 1992) being the most well known method. This is the approach that we take in the present study. Recent work has demonstrated that unsupervised word representations induced from large unlabelled data can be used to improve supervised tasks, a type of semi-supervised learning. Examples of tasks where this has been applied include dependency parsing (Koo et al., 2008), Named Entity Recognition (NER) (Miller et al., 2004), sentiment analysis (Maas et al., 2011) and chunking (Turian et al., 2010). Such an approach could also be applied to the clinical information extraction task where although we only have a very limited amount of labelled data, large-scale unlabelled data — hundreds of millions of tokens — is readily available to us. Researchers have noted a number of advantages to using word representations in supervised learning tasks. They produce substantially more compact models compared to fully lexicalized approaches where feature vectors have the same length as the entire vocabulary and suffer from sparsity. They better estimate the values f"
U15-1008,E14-4019,1,0.770194,"Missing"
U15-1008,P10-1040,0,0.580479,"pt has been made to leverage large-scale unlabelled data. The main aim of this work is to evaluate the feasibility of such an approach. been devised to automatically recognise these scientific artefacts in publications (Hassanzadeh et al., 2014a). The most common approach, as discussed in §2, is the use of supervised learning to classify sentences into the various categories. Separately, another recent trend in Natural Language Processing (NLP) has been the use of word representations to integrate large amounts of unlabelled data into such supervised tasks, a form of semi-supervised learning (Turian et al., 2010). This is something that has not been applied to scientific artefacts extraction. Accordingly, the primary aim of the present work is to draw together the two areas, evaluating the utility of word representations for this task and comparing them against the most commonly used features to see if they can enhance accuracy. A secondary goal is to inspect the induced word representations and the resulting discriminative models to gain further insights about this approach. The paper is structured as follows. We present related work on biomedical information extraction in §2. Word representations ar"
U15-1008,N15-1160,1,0.860043,"Missing"
U15-1008,D12-1053,0,0.0547617,"Missing"
U15-1008,W15-0620,1,0.785459,"s, they did not provide any improvement over the standard Brown features. 4.4 Evaluation We report our results as classification accuracy under k-fold cross-validation, with k = 10. These results are compared against a majority baseline and an oracle. The oracle considers the predictions by all the classifiers in Table 3 and will assign the correct class label for an instance if at least one of the the classifiers produces the correct label for that data point. This approach can help us quantify the potential upper limit of a classification system’s performance on the given data and features (Malmasi et al., 2015). In sum, these results show that Brown clusters, using far fewer features, can outperform the widely used word features. 7 e.g. Our own analysis showed that O UTCOME sentences contained substantially more past tense verbs, comparative adverbs and comparative adjectives. 70 Class Clusters of words BACKGROUND [have has had] — [describes presents examines discusses summarizes addresses] [objectives goal] — [emerged evolved attracted fallen arisen risen proliferated] I NTERVENTION [received underwent undergoing taking] — [gel cream spray ointment] [orally intravenously subcutaneously intramuscula"
U15-1008,N04-1043,0,0.321998,"and LDA, as well as distributed representations, also known as word embeddings. Yet another type of representation is based on inducing a clustering over words, with Brown clustering (Brown et al., 1992) being the most well known method. This is the approach that we take in the present study. Recent work has demonstrated that unsupervised word representations induced from large unlabelled data can be used to improve supervised tasks, a type of semi-supervised learning. Examples of tasks where this has been applied include dependency parsing (Koo et al., 2008), Named Entity Recognition (NER) (Miller et al., 2004), sentiment analysis (Maas et al., 2011) and chunking (Turian et al., 2010). Such an approach could also be applied to the clinical information extraction task where although we only have a very limited amount of labelled data, large-scale unlabelled data — hundreds of millions of tokens — is readily available to us. Researchers have noted a number of advantages to using word representations in supervised learning tasks. They produce substantially more compact models compared to fully lexicalized approaches where feature vectors have the same length as the entire vocabulary and suffer from spa"
U15-1008,W01-0521,0,\N,Missing
U15-1018,N01-1014,0,0.13991,"Missing"
U15-1018,E14-4019,1,0.891074,"Missing"
U15-1018,N15-1160,1,0.857019,"Missing"
U15-1018,W13-1718,0,0.0675242,"Missing"
U15-1018,U14-1003,0,0.0368465,"Missing"
U16-1017,P16-1231,0,0.0545678,"Missing"
U16-1017,W06-2920,0,0.344542,"Missing"
U16-1017,D14-1082,0,0.165994,"Missing"
U16-1017,P15-1038,0,0.113202,"Missing"
U16-1017,P15-1033,0,0.0502673,"Missing"
U16-1017,P11-2125,0,0.0214393,"uses#1 himself#2 , accuses#3 himself .” Oracle Tree Arc With punct. LAS UAS LS 79.20 85.22 88.38 85.98 90.50 92.67 Without punct. LAS UAS LS 79.33 86.24 86.66 85.96 91.14 91.57 Table 5: Upper bound of ensemble performance. One simple approach to improve parsing performance for Vietnamese is to separately use the graph-based parser BistG for short sentences and the transition-based parser BistT for longer sentences. Another approach is to use system combination (Nivre and McDonald, 2008; Zhang and Clark, 2008), e.g. building ensemble systems (Sagae and Tsujii, 2007; Surdeanu and Manning, 2010; Haffari et al., 2011). Table 5 presents an upper bound of oracle ensemble performance, using the DEPENDABLE toolkit (Choi et al., 2015). DEPENDABLE assumes that either the best tree or the best arc can be determined by an oracle. 4 Conclusions We have presented an empirical comparison for Vietnamese dependency parsing. Experimental results on the Vietnamese dependency treebank VnDT (Nguyen et al., 2014b) show that the neural network-based parsers (Kiperwasser and Goldberg, 2016b) obtain significantly higher scores than the traditional parsers (McDonald et al., 2005; Nivre et al., 2007b). More specifically, in each"
U16-1017,P82-1020,0,0.823,"Missing"
U16-1017,Q16-1032,0,0.0674042,"Missing"
U16-1017,Q16-1023,0,0.0898462,"Missing"
U16-1017,P13-2109,0,0.102004,"Missing"
U16-1017,J11-1007,0,0.169542,"Missing"
U16-1017,P05-1012,0,0.526091,"Missing"
U16-1017,E14-2005,1,0.927337,"results across many languages. Chen and Manning (2014), Weiss et al. (2015), Pei et al. (2015), and Andor et al. (2016) represent the core features with dense vector embeddings and then feed them as inputs to neural network-based classifiers, while Dyer et al. (2015), Kiperwasser and Goldberg (2016a), and Kiperwasser and Goldberg (2016b) propose novel neural network architectures to solve the feature-engineering problem. Dependency parsing for Vietnamese has not been actively explored. One main reason is because there is no manually labeled dependency treebank available. Thi et al. (2013) and Nguyen et al. (2014b) propose constituent-to-dependency conversion approaches to automatically translate the manually built constituent treebank for Vietnamese (Nguyen et al., 2009) to dependency treebanks. The converted dependency treebanks are then used in later works on Vietnamese dependency parsing, including Vu-Manh et al. (2015), Le-Hong et al. (2015) and Nguyen and Nguyen (2015). All of the previous research works use either the MSTparser (McDonald et al., 2005) or the Maltparser (Nivre et al., 2007b) for their parsing experiments. Among them, Nguyen et al. (2014b) report the highest results with LAS at 7"
U16-1017,P08-1108,0,0.0387999,"ttach the indexed3 word to be dependent to the indexed-2 word by the label nmod. This sentence is translated to English as “He who excuses#1 himself#2 , accuses#3 himself .” Oracle Tree Arc With punct. LAS UAS LS 79.20 85.22 88.38 85.98 90.50 92.67 Without punct. LAS UAS LS 79.33 86.24 86.66 85.96 91.14 91.57 Table 5: Upper bound of ensemble performance. One simple approach to improve parsing performance for Vietnamese is to separately use the graph-based parser BistG for short sentences and the transition-based parser BistT for longer sentences. Another approach is to use system combination (Nivre and McDonald, 2008; Zhang and Clark, 2008), e.g. building ensemble systems (Sagae and Tsujii, 2007; Surdeanu and Manning, 2010; Haffari et al., 2011). Table 5 presents an upper bound of oracle ensemble performance, using the DEPENDABLE toolkit (Choi et al., 2015). DEPENDABLE assumes that either the best tree or the best arc can be determined by an oracle. 4 Conclusions We have presented an empirical comparison for Vietnamese dependency parsing. Experimental results on the Vietnamese dependency treebank VnDT (Nguyen et al., 2014b) show that the neural network-based parsers (Kiperwasser and Goldberg, 2016b) obtai"
U16-1017,P15-1032,0,0.0567917,"Missing"
U16-1017,D08-1059,0,0.0257571,"be dependent to the indexed-2 word by the label nmod. This sentence is translated to English as “He who excuses#1 himself#2 , accuses#3 himself .” Oracle Tree Arc With punct. LAS UAS LS 79.20 85.22 88.38 85.98 90.50 92.67 Without punct. LAS UAS LS 79.33 86.24 86.66 85.96 91.14 91.57 Table 5: Upper bound of ensemble performance. One simple approach to improve parsing performance for Vietnamese is to separately use the graph-based parser BistG for short sentences and the transition-based parser BistT for longer sentences. Another approach is to use system combination (Nivre and McDonald, 2008; Zhang and Clark, 2008), e.g. building ensemble systems (Sagae and Tsujii, 2007; Surdeanu and Manning, 2010; Haffari et al., 2011). Table 5 presents an upper bound of oracle ensemble performance, using the DEPENDABLE toolkit (Choi et al., 2015). DEPENDABLE assumes that either the best tree or the best arc can be determined by an oracle. 4 Conclusions We have presented an empirical comparison for Vietnamese dependency parsing. Experimental results on the Vietnamese dependency treebank VnDT (Nguyen et al., 2014b) show that the neural network-based parsers (Kiperwasser and Goldberg, 2016b) obtain significantly higher s"
U16-1017,P11-2033,0,0.109133,"Missing"
U16-1017,P15-1031,0,0.0956167,"Missing"
U16-1017,D07-1111,0,0.0426341,"his sentence is translated to English as “He who excuses#1 himself#2 , accuses#3 himself .” Oracle Tree Arc With punct. LAS UAS LS 79.20 85.22 88.38 85.98 90.50 92.67 Without punct. LAS UAS LS 79.33 86.24 86.66 85.96 91.14 91.57 Table 5: Upper bound of ensemble performance. One simple approach to improve parsing performance for Vietnamese is to separately use the graph-based parser BistG for short sentences and the transition-based parser BistT for longer sentences. Another approach is to use system combination (Nivre and McDonald, 2008; Zhang and Clark, 2008), e.g. building ensemble systems (Sagae and Tsujii, 2007; Surdeanu and Manning, 2010; Haffari et al., 2011). Table 5 presents an upper bound of oracle ensemble performance, using the DEPENDABLE toolkit (Choi et al., 2015). DEPENDABLE assumes that either the best tree or the best arc can be determined by an oracle. 4 Conclusions We have presented an empirical comparison for Vietnamese dependency parsing. Experimental results on the Vietnamese dependency treebank VnDT (Nguyen et al., 2014b) show that the neural network-based parsers (Kiperwasser and Goldberg, 2016b) obtain significantly higher scores than the traditional parsers (McDonald et al., 200"
U16-1017,N10-1091,0,0.0484834,"Missing"
U16-1017,P81-1022,0,0.467235,"Missing"
U16-1017,P13-1104,0,\N,Missing
U17-1001,D14-1148,0,0.450023,"recently proposed a reinforcement learning method to predict negation scope and showed that it improved the accuracy on a dataset from the financial news domain. dict changes to the S&P 500 index. The architecture here was also multichannel, and incorporated a technical analysis1 input channel. The results from both pieces of work outperformed the former manual feature engineering approaches. To the best of our knowledge, characterlevel sequence modeling has not been applied to stock price forecasting so far; neither has the use of language model pre-training. We note that the event models of Ding et al. (2014) and Ding et al. (2015) make use of generalization and back-o↵ techniques to deal with data sparsity in terms of named entities etc, which as mentioned earlier characterlevel representations could help address. Also, character-level inputs are potentially complementary to other sorts such as word-level inputs or event representations, in particular with the multichannel architectures used for the work described above: research such as that of Kim (2014) has shown that multiple input representation can be usefully combined, and work using this kind of model such as Ruder et al. (2016) has speci"
U17-1001,J81-4005,0,0.742086,"Missing"
U17-1001,P82-1020,0,0.840701,"Missing"
U17-1001,D14-1181,0,0.0115078,"Missing"
U17-1001,P04-1035,0,0.045734,"Missing"
U17-1001,D13-1170,0,0.00320951,"tion is a promising area of research (Ding et al., 2015). Finding the most informative representation of the data in a text classification problem is still an open area of research. In the last few years a range of di↵erent neural networks architectures have been proposed for text classification, each one with strong results on di↵erLeonardo Dos Santos Pinheiro and Mark Dras. 2017. Stock Market Prediction with Deep Learning: A Character-based Neural Language Model for Event-based Trading. In Proceedings of Australasian Language Technology Association Workshop, pages 6 15. ent benchmarks (e.g. Socher et al. (2013), Kim (2014) and Kumar et al. (2016)), and each one proposing di↵erent ways to encode the textual information. character level language model. The remainder of the paper is structured as follows: In Section 2, we describe event-driven trading and review the relevant literature. In Section 3 we describe our model and the experimental setup used in this work. Section 5 presents and discuss the results. Finally, in Section 6 we summarize our work and suggest directions for future research. One of the most commonly used architectures for modeling text data is the Recurrent Neural Network (RNN). On"
U17-1013,W03-0316,0,0.14222,"Missing"
U17-1013,P09-1058,0,0.0273785,"a written text “thu∏ thu nh™p cá nhân” (individualcá_nhân incomethu_nh™p taxthu∏ ) consisting of 5 syllables, the word segmenter returns a two-word phrase “thu∏_thu_nh™p cá_nhân.”1 Then given the input segmented text “thu∏_thu_nh™p cá_nhân”, the POS tagger returns “thu∏_thu_nh™p/N cá_nhân/N.” A class of approaches to POS tagging from unsegmented text that has been actively explored in other languages, such as in Chinese and Japanese, is joint word segmentation and POS tagging (Zhang and Clark, 2008). A possible joint strategy is to assign a combined segmentation and POS tag to each syllable (Kruengkrai et al., 2009). For example, given the input text “thu∏ thu nh™p cá nhân”, the joint strategy would produce “thu∏/BN thu/I-N nh™p/I-N cá/B-N nhân/I-N”, where B refers to the beginning of a word and I refers to the inside of a word. Shao et al. (2017) showed that this joint strategy gives SOTA results for Chinese POS tagging by utilizing a BiLSTM-CNNCRF model (Ma and Hovy, 2016). In this paper, we present the first empirical study comparing the joint and pipeline strategies for Vietnamese POS tagging from unsegmented text. In addition, we make a comparison between SOTA feature-based and neural networkbased m"
U17-1013,N16-1030,0,0.082684,"tion rule-based learning model which obtained the highest accuracy at the VLSP 2013 POS tagging shared task.4 • MarMoT (Mueller et al., 2013) is a generic CRF framework and a SOTA POS and morphological tagger.5 • BiLSTM-CRF (Huang et al., 2015) is a sequence labeling model which extends the BiLSTM model with a CRF layer. • BiLSTM-CRF + CNN-char, i.e. BiLSTMCNN-CRF, is an extension of the BiLSTMCRF, using CNN to derive character-based representations (Ma and Hovy, 2016). • BiLSTM-CRF + LSTM-char is another extension of the BiLSTM-CRF, using BiLSTM to derive the character-based representations (Lample et al., 2016). Here, for the pipeline strategy, we train these models to predict POS tags with respect to (w.r.t.) gold word segmentation. In addition, we also retrain the fast and accurate Vietnamese word segmenter RDRsegmenter (Nguyen et al., 2017b) using the training set of 27k sentences.6 3 The data was officially used for the Vietnamese POS tagging shared task at the second VLSP 2013 workshop. 4 http://rdrpostagger.sourceforge.net 5 http://cistern.cis.lmu.de/marmot 6 RDRsegmenter obtains a segmentation speed at 60k words per second, computed on a personal computer of Intel Core i7 2.2 GHz. RDRsegmente"
U17-1013,2010.jeptalnrecital-long.36,0,0.579301,"Missing"
U17-1013,D13-1032,0,0.167826,"Missing"
U17-1013,Y06-1028,0,0.805919,"Missing"
U17-1013,K17-3014,1,0.90322,"2008), Pham et al. (2009) and Tran et al. (2012) used the maximum matching method (NanYuan and YanBin, 1991) to generate all possible segmentations for each input sentence; then to select the best segmentation, Le et al. (2008) and Tran et al. (2012) applied ngram language model while Pham et al. (2009) employed POS information from an external POS tagger. Later, Liu and Lin (2014) and Nguyen and Le (2016) proposed approaches based on pointwise prediction, where a binary classifier is trained to identify whether or not there is a word boundary at each point between two syllables. Furthermore, Nguyen et al. (2017b) proposed a rule-based approach which gets the highest results to date in terms of both segmentation accuracy and speed. 2.2 POS tagging Regarding Vietnamese POS tagging, Dien and Kiem (2003) projected POS annotations from English to Vietnamese via a bilingual corpus of word alignments. As a standard sequence labeling task, previous research has applied the CRF, SVM or MaxEnt model to assign each word a POS tag (Nghiem et al., 2008; Tran et al., 2009; Le-Hong et al., 2010; Nguyen et al., 2010; Tran et al., 2010; Bach et al., 2013). In addition, Nguyen et al. (2011) proposed a rule-based appr"
U17-1013,E14-2005,1,0.665315,"labeling models on the syllable-based transformed corpus. 3.2 Dataset The Vietnamese treebank (Nguyen et al., 2009) is the largest annotated corpus for Vietnamese, providing a set of 27,870 manually POS-annotated sentences for training and development (about 23 words per sentence on average) and a test set of 2120 manually POS-annotated sentences (about 31 words per sentence).3 From the set of 27,870 sentences, we use the first 27k sentences for training and the last 870 sentences for development. 3.3 Models For both joint and pipeline strategies, we use the following models: • RDRPOSTagger (Nguyen et al., 2014a) is a transformation rule-based learning model which obtained the highest accuracy at the VLSP 2013 POS tagging shared task.4 • MarMoT (Mueller et al., 2013) is a generic CRF framework and a SOTA POS and morphological tagger.5 • BiLSTM-CRF (Huang et al., 2015) is a sequence labeling model which extends the BiLSTM model with a CRF layer. • BiLSTM-CRF + CNN-char, i.e. BiLSTMCNN-CRF, is an extension of the BiLSTMCRF, using CNN to derive character-based representations (Ma and Hovy, 2016). • BiLSTM-CRF + LSTM-char is another extension of the BiLSTM-CRF, using BiLSTM to derive the character-based"
U17-1013,I17-3010,0,0.256196,"Missing"
U17-1013,D17-1035,0,0.019357,"second, computed on a personal computer of Intel Core i7 2.2 GHz. RDRsegmenter is available at: https: //github.com/datquocnguyen/RDRsegmenter 110 Pipeline 100 100 150 Joint 200 250 250 Table 1: Optimal number of LSTM units. 3.4 Implementation details We use the original pure Java implementations of RDRPOSTagger and MarMoT with default hyperparameter settings in our experiments. Instead of using implementations independently provided by authors of BiLSTM-CRF, BiLSTM-CRF + CNNchar7 and BiLSTM-CRF + LSTM-char, we use a reimplementation which is optimized for performance of all these models from Reimers and Gurevych (2017).8 For three BiLSTM-CRF-based models, we use default hyper-parameters provided by Reimers and Gurevych (2017) with the following exceptions: we use a dropout rate at 0.5 (Ma and Hovy, 2016) with the frequency threshold of 5 for unknown word and syllable types. We initialize word and syllable embeddings with 100-dimensional pretrained embeddings,9 then learn them together with other model parameters during training by using Nadam (Dozat, 2016). For training, we run for 100 epochs. We perform a grid search of hyperparameters to select the number of BiLSTM layers from {1, 2, 3} and the number of"
U17-1013,I17-1018,0,0.0204126,"the POS tagger returns “thu∏_thu_nh™p/N cá_nhân/N.” A class of approaches to POS tagging from unsegmented text that has been actively explored in other languages, such as in Chinese and Japanese, is joint word segmentation and POS tagging (Zhang and Clark, 2008). A possible joint strategy is to assign a combined segmentation and POS tag to each syllable (Kruengkrai et al., 2009). For example, given the input text “thu∏ thu nh™p cá nhân”, the joint strategy would produce “thu∏/BN thu/I-N nh™p/I-N cá/B-N nhân/I-N”, where B refers to the beginning of a word and I refers to the inside of a word. Shao et al. (2017) showed that this joint strategy gives SOTA results for Chinese POS tagging by utilizing a BiLSTM-CNNCRF model (Ma and Hovy, 2016). In this paper, we present the first empirical study comparing the joint and pipeline strategies for Vietnamese POS tagging from unsegmented text. In addition, we make a comparison between SOTA feature-based and neural networkbased models, which, to the best of our knowledge, has not done in any prior work on Vietnamese. On the benchmark Vietnamese treebank (Nguyen et al., 2009), we show that the pipeline strategy produces better scores than the joint strategy. We"
U17-1013,N03-1033,0,0.183629,"Missing"
U17-1013,K17-3001,0,0.0615701,"Missing"
U17-1013,P08-1101,0,0.0409698,"ed the word-segmented text—which is the output of the word segmenter—as the input to a POS tagger. For example, given a written text “thu∏ thu nh™p cá nhân” (individualcá_nhân incomethu_nh™p taxthu∏ ) consisting of 5 syllables, the word segmenter returns a two-word phrase “thu∏_thu_nh™p cá_nhân.”1 Then given the input segmented text “thu∏_thu_nh™p cá_nhân”, the POS tagger returns “thu∏_thu_nh™p/N cá_nhân/N.” A class of approaches to POS tagging from unsegmented text that has been actively explored in other languages, such as in Chinese and Japanese, is joint word segmentation and POS tagging (Zhang and Clark, 2008). A possible joint strategy is to assign a combined segmentation and POS tag to each syllable (Kruengkrai et al., 2009). For example, given the input text “thu∏ thu nh™p cá nhân”, the joint strategy would produce “thu∏/BN thu/I-N nh™p/I-N cá/B-N nhân/I-N”, where B refers to the beginning of a word and I refers to the inside of a word. Shao et al. (2017) showed that this joint strategy gives SOTA results for Chinese POS tagging by utilizing a BiLSTM-CNNCRF model (Ma and Hovy, 2016). In this paper, we present the first empirical study comparing the joint and pipeline strategies for Vietnamese PO"
W00-2008,P99-1011,1,0.572462,"Missing"
W00-2008,P94-1022,0,0.528315,"Missing"
W00-2008,E93-1045,0,0.08135,"Missing"
W00-2008,P99-1012,1,\N,Missing
W00-2008,P96-1016,0,\N,Missing
W00-2035,W90-0102,0,\N,Missing
W00-2035,C90-3045,0,\N,Missing
W00-2035,P99-1011,1,\N,Missing
W00-2035,P94-1022,0,\N,Missing
W02-2230,J94-4004,0,0.101261,"Missing"
W02-2230,P99-1011,1,0.884585,"esentation. However, many of these divergences are not probIems for an S-TAG-based approach. Synchronous TAG translation models thus allow us to explore the question of the extent to which a semantic representation is actually necessary. S-TAG was redefined by Shieber ( 1994) for both theoretical and practical reasons, introducing the requirement that the derivation trees of target and source be isomorphic. Under this definition it has been noted (Shieber, 1994; Dras and Bleam, 2000) that there are mappings that cannot be described under S-TAG. This was the motivation for meta-level grammars (Dras, 1999), by which two TAG grammars can be paired while retaining their original properties, as under standard S-TAG, allowing for a description of mappings that include unbounded non-isomorphisms (Dras and Bleam, 2000). This work on exploring how S-TAG (with and without meta-level grammars) can be used for MT has only been applied to languages that are closely related-English, French,&quot;Italian and Spanish. In this paper we aim to take a much more widely differing pair of languages, English anq 'Korean, to investigate the extent to which syntactic mappings are satisfactory. English and Korean have a wi"
W02-2230,W00-2035,1,0.830129,"al, thematic, conflational, structural and lexical divergences-have been used to argue for the necessity of an explicit semantic representation. However, many of these divergences are not probIems for an S-TAG-based approach. Synchronous TAG translation models thus allow us to explore the question of the extent to which a semantic representation is actually necessary. S-TAG was redefined by Shieber ( 1994) for both theoretical and practical reasons, introducing the requirement that the derivation trees of target and source be isomorphic. Under this definition it has been noted (Shieber, 1994; Dras and Bleam, 2000) that there are mappings that cannot be described under S-TAG. This was the motivation for meta-level grammars (Dras, 1999), by which two TAG grammars can be paired while retaining their original properties, as under standard S-TAG, allowing for a description of mappings that include unbounded non-isomorphisms (Dras and Bleam, 2000). This work on exploring how S-TAG (with and without meta-level grammars) can be used for MT has only been applied to languages that are closely related-English, French,&quot;Italian and Spanish. In this paper we aim to take a much more widely differing pair of languages"
W02-2230,C90-3045,0,0.118804,"Missing"
W02-2230,W00-1307,0,0.0597279,"Missing"
W02-2230,han-etal-2000-handling,1,\N,Missing
W02-2230,C90-3001,0,\N,Missing
W02-2230,W90-0102,0,\N,Missing
W02-2230,P95-1013,0,\N,Missing
W03-1202,P00-1041,0,0.0570253,"vation behind discovering segments in a text is that a sentence extraction summary should choose the most representative sentence for each segment, resulting in a comprehensive summary. In the view of Gong and Liu (2001), segments form the main themes of a document. They present a theme interpretation of the SVD analysis, as it is used for discourse segmentation, upon which our use of the technique is based. However, Gong and Liu use SVD for creating sentence extraction summaries, not for generating a single sentence summary by re-using words. In subsequent work to Witbrock and Mittal (1999), Banko et al. (2000) describe the use of information about the position of words within four quarters of the source document. The headline candidacy score of a word is weighted by its position in one of quarters. We interpret this use of position information as a means of guiding the generation of a headline towards the central theme of the document, which for news articles typically occurs in the first quarter. SVD potentially offers a more general mechanism for handling the discovery of the central themes and their positions within the document. Jin et al. (2002) have also examined a statistical model for headl"
W03-1202,W97-0704,0,0.072912,"g abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction 1 Theme is a term that is used in many ways by many researchers, and generally without any kind of formal definition. Our use of the term here is akin to the notion that underlies work on text segmentation, where sentences naturally cluster in terms of their ‘aboutness’. and natural language processing. Hybrid methods for abstract-like summarisation which combine statistical and symbolic approaches have also been explored; see, for example, McKeown et al. (1999), Jing and McKeown (1999), and Hovy and Lin (1997). Statistical single sentence summarisation has been explored by a number of researchers (see for example, Witbrock and Mittal, 1999; Zajic et al., 2002). We build on the approach employed by Witbrock and Mittal (1999) which we will describe in more detail in Section 3. Interestingly, in the work of Witbrock and Mittal (1999), the selection of words for inclusion in the headline is decided solely on the basis of corpus statistics and does not use statistical information about the distribution of words in the document itself. Our work differs in that we utilise an SVD analysis to provide inform"
W03-1202,J00-2011,0,0.557111,"th probabilities, directing the search towards the more probable word sequences first. The use of repeated words in the path is not permitted. 5.2 Using Singular Value Decomposition for Content Selection As an alternative to the Conditional probability, we examine the use of SVD in determining the Content Selection probability. Before we outline the procedure for basing this probability on SVD, we will first outline our interpretation of the SVD analysis, based on that of Gong and Liu (2001). Our description is not intended to be a comprehensive explanation of SVD, and we direct the reader to Manning and Schütze (2000) for a description of how SVD is used in information retrieval. Conceptually, when used to analyse documents, SVD can discover relationships between word co-occurrences in a collection of text. For example, in the context of information retrieval, this provides one way to retrieve additional documents that contain synonyms of query terms, where synonymy is defined by similarity of word co-occurrences. By discovering patterns in word co-occurrences, SVD also provides information that can be used to cluster documents based on similarity of themes. In the context of single document summarisation,"
W03-1202,J98-3005,0,0.0911491,"erated headline using SVD: “singapore shares fall” Figure 2. The headline generated using an SVDbased word selection criterion. The movement in share price is correct. 4 Related Work As the focus of this paper is on statistical singlesentence summarisation we will not focus on preceding work which generates summaries greater in length than a sentence. We direct the reader to Paice (1990) for an overview of summarisation based on sentence extraction. Examples of recent systems include Kupiec et al. (1995) and Brandow et al. (1995). For examples of work in producing abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction 1 Theme is a term that is used in many ways by many researchers, and generally without any kind of formal definition. Our use of the term here is akin to the notion that underlies work on text segmentation, where sentences naturally cluster in terms of their ‘aboutness’. and natural language processing. Hybrid methods for abstract-like summarisation which combine statistical and symbolic approaches have also been explored; see, for example, McKeown et al. (1999), Jing and McKeown (1999), and Hovy and Lin (1997). Statistical single sentence summar"
W03-1202,X98-1026,0,\N,Missing
W05-1628,C00-1007,0,0.0816988,"Missing"
W05-1628,P99-1071,0,0.0814331,"Missing"
W05-1628,P96-1025,0,0.137913,"Missing"
W05-1628,W04-3216,0,0.0395668,"Missing"
W05-1628,W03-1202,1,0.866597,"Missing"
W07-2205,W02-1503,1,0.818483,"Baldwin University of Melbourne tim@csse.unimelb.edu.au Julia Hockenmaier University of Pennsylvania juliahr@cis.upenn.edu Mark Dras Macquarie University madras@ics.mq.edu.au Tracy Holloway King PARC thking@parc.com Abstract Gertjan van Noord University of Groningen vannoord@let.rug.nl dependencies or the underlying predicate-argument structure directly. Aspects of this research have often had their own separate fora, such as the ACL 2005 workshop on deep lexical acquisition (Baldwin et al., 2005), as well as the TAG+ (Kallmeyer and Becker, 2006), Alpino (van der Beek et al., 2005), ParGram (Butt et al., 2002) and DELPH-IN (Oepen et al., 2002) projects and meetings. However, the fundamental approaches to building a linguistically-founded system and many of the techniques used to engineer efficient systems are common across these projects and independent of the specific grammar formalism chosen. As such, we felt the need for a common meeting in which experiences could be shared among a wider community, similar to the role played by recent meetings on grammar engineering (Wintner, 2006; Bender and King, 2007). As the organizers of the ACL 2007 Deep Linguistic Processing workshop (Baldwin et al., 2007"
W09-2310,W05-0909,0,0.0428613,"as 26 POS and 23 syntactic categories. As mentioned above, specific rules are not pruned away due to a limited amount of training material we set the thresholds kpart and kgener to relatively low values, 1 and 3, respectively. Evaluation conditions were case-insensitive and with punctuation marks considered. The targetside 4-gram language model was estimated using the SRILM toolkit (Stolcke, 2002) and modified Kneser-Ney discounting with interpolation. The highest BLEU score (Papineni et al., 2002) was chosen as the optimization criterion. Apart from BLEU, a standard automatic measure METEOR (Banerjee and Lavie, 2005) was used for evaluation. 5.4 Table 1: Basic statistics of the BTEC training corpus. Arabic data preprocessing Results The scores considered are: BLEU scores obtained for the development set as the final point of the MERT procedure (Dev), and BLEU and METEOR scores obtained on test dataset (Test). We present BTEC results (Tables 2), characterized by relatively short sentence length, and the results obtained on the NIST corpus (Tables 3) with much longer sentences and much need of global reordering. Plain BL SBR SBR+lattice Dev BLEU 48.31 48.46 48.75 48.90 BLEU 45.02 47.10 47.52 48.78 Test METE"
W09-2310,P05-1066,0,0.277951,"Missing"
W09-2310,W06-1609,1,0.883808,"approach is described in Nießen and Ney (2004), where morpho-syntactic information was used to account for the reorderings needed. A representative set of similar systems includes: a set of hand-crafted reordering patterns for German-to-English (Collins et al., 2005) and Chinese-English (Wang et al., 2007) translations, emphasizing the distinction between German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-jussà and Fonollosa, 2006). Coupling of SMR algorithm and the search space extension via generating a set of weighted reordering hypotheses has demonstrated a significant improvement, as shown in Costa-jussà and Fonollosa (2008). The technique proposed in this study is most similar to the one proposed for French-to-English translation task in Xia and McCord (2004), where the authors present a hybrid system for FrenchEnglish translation based on the principle of automatic rewrite patterns extraction using a parse tree and phrase alignments. We propose using a word distortion model not only to monotonize the source part"
W09-2310,2008.amta-papers.6,1,0.850574,"reordering patterns for German-to-English (Collins et al., 2005) and Chinese-English (Wang et al., 2007) translations, emphasizing the distinction between German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-jussà and Fonollosa, 2006). Coupling of SMR algorithm and the search space extension via generating a set of weighted reordering hypotheses has demonstrated a significant improvement, as shown in Costa-jussà and Fonollosa (2008). The technique proposed in this study is most similar to the one proposed for French-to-English translation task in Xia and McCord (2004), where the authors present a hybrid system for FrenchEnglish translation based on the principle of automatic rewrite patterns extraction using a parse tree and phrase alignments. We propose using a word distortion model not only to monotonize the source part of the corpus (using a different approach to rewrite rule organization from Xia and McCord), but also to extend the search space during decoding. 3 4 Syntax-based reordering coupled with word graph Our"
W09-2310,2006.iwslt-evaluation.18,1,0.905153,"Missing"
W09-2310,2007.mtsummit-papers.16,0,0.0292467,"ins for the reordering search as described in Kanthak et al. (2005) and Crego et al. (2005). The main criticism of such systems is that the constraints are not lexicalized. Recently there has been interest in SMT exploiting non-monotonic decoding which allow for extension of the search space and linguistic information involvement. The variety of such models includes a constrained distance-based reordering (Costa-jussà et al., 2006); and a constrained version of distortion model where the reordering search problem is tackled through a set of linguistically motivated rules used during decoding (Crego and Mariño, 2007). Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 78–86, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics A quite popular class of reordering algorithms is a monotonization of the source part of the parallel corpus prior to translation. The first work on this approach is described in Nießen and Ney (2004), where morpho-syntactic information was used to account for the reorderings needed. A representative set of similar systems includes: a set of hand-crafted reordering patterns for German-to-English (Collins et al.,"
W09-2310,2005.mtsummit-papers.37,0,0.117726,"Missing"
W09-2310,J04-2003,0,0.0331549,"tance-based reordering (Costa-jussà et al., 2006); and a constrained version of distortion model where the reordering search problem is tackled through a set of linguistically motivated rules used during decoding (Crego and Mariño, 2007). Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 78–86, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics A quite popular class of reordering algorithms is a monotonization of the source part of the parallel corpus prior to translation. The first work on this approach is described in Nießen and Ney (2004), where morpho-syntactic information was used to account for the reorderings needed. A representative set of similar systems includes: a set of hand-crafted reordering patterns for German-to-English (Collins et al., 2005) and Chinese-English (Wang et al., 2007) translations, emphasizing the distinction between German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-jussà and Fonollosa, 2006). Coupling of"
W09-2310,N06-2013,0,0.0595218,"s characteristic in global reordering studies. A training set statistics can be found in Table 1. Sentences Words ASL Voc BTEC Ar En 24.9 K 24.9 K 225 K 210 K 9.05 8.46 11.4 K 7.6 K NIST50K Ar En 50 K 50 K 1.2 M 1.35 M 24.61 26.92 55.3 36.3 used to test the translation quality has 500 sentences, 4.1 K words and is also provided with 6 reference translations. The NIST50K development set consists of 1353 sentences and 43 K words; the test data contains 1056 sentences and 33 K running words. Both datasets have 4 reference translations per sentence. 5.2 We took a similar approach to that shown in Habash and Sadat (2006), using the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.3 The BTEC development dataset consists of 489 sentences and 3.8 K running words, with 6 humanmade reference translations per sentence; the dataset 83 Experimental setup We used the Stanford Parser (Klein and Manning, 2003) for both langua"
W09-2310,P08-2020,0,0.0131323,"tions are motivated by the crossed links found in the word alignment and, conIn order to improve reordering power of the translation system, we implemented an additional reordering as described in Crego and Mariño (2006). Multiple word segmentations is encoded in a lattice, which is then passed to the input of the decoder, containing reordering alternatives consistent with the previously extracted rules. The decoder takes the n-best reordering of a source sentence coded in the form of a word lattice. This approach is in line with recent research tendencies in SMT, as described for example in (Hildebrand et al., 2008; Xu et al., 2005). Originally, word lattice algorithms do not involve syntax into reordering process, there(a) Monotonic search, plain text: >n +h mTEm *w tAryx Eryq >n S +h 1 mTEm 2 *w 3 tAryx 4 Eryq 5 L (b) Word lattice, plain text: >n +h mTEm *w tAryx Eryq mTEm +h mTEm Eryq tAryx *w >n S 2 1 4 3 >n 7 6 5 >n L 10 9 8 mTEm mTEm tAryx tAryx +h Eryq *w (c) Word lattice, reordered text: >n +h mTEm *w Eryq tAryx tAryx tAryx Eryq *w *w *w +h Eryq mTEm mTEm mTEm >n S tAryx 1 2 3 >n 4 5 6 7 10 9 8 11 12 >n 13 14 L *w mTEm +h *w *w Eryq Eryq Eryq Eryq Figure 5: Comparative example of a monotone sear"
W09-2310,2005.mtsummit-papers.35,0,0.0339199,"ed local orientations enriched with probabilities are learned from training data. During decoding, translation is viewed as a monotone block sequence generation process with the possibility to swap a pair of neighbor blocks. http://www.statmt.org/moses/ 79 ηd0 @d0 . . . ηdk @dk |Lexicon|p1 (1) where ηi ∈ N for all 0 ≤ i ≤ k; (do . . . dk ) is a permutation of (0 . . . k); Lexicon comes from the source-side set of words for each ηi ; and p1 is a probability associated with the rule. Figure 1 gives two examples of the rule format. 4.2 Rules extraction Concept. Inspired by the ideas presented in Imamura et al. (2005), where monolingual correspondences of syntactic nodes are used during decoding, we extract a set of bilingual patterns allowing for reordering as described below: (1) align the monotone bilingual corpus with GIZA++ (Och and Ney, 2003) and find the intersection of direct and inverse word alignments, resulting in the construction of the projection matrix P (see below)); (2) parse the source and the target parts of the parallel corpus; (3) extract reordering patterns from the parallel non-isomorphic CFG-trees based on the word alignment intersection. Step 2 is straightforward; we explain aspects"
W09-2310,W05-0831,0,0.0927259,"ave appeared over the past few years. Word class-based reordering was a part of Och’s Alignment Template system (Och et al., 2004); the main criticism of this approach is that it shows bad performance for the pair of languages with very distinct word order. The state-of-the-art SMT system Moses implements a distance-based reordering model (Koehn et al., 2003) and a distortion model, operating with rewrite patterns extracted from a phrase alignment table (Tillman, 2004). Many SMT models implement the brute force approach, introducing several constrains for the reordering search as described in Kanthak et al. (2005) and Crego et al. (2005). The main criticism of such systems is that the constraints are not lexicalized. Recently there has been interest in SMT exploiting non-monotonic decoding which allow for extension of the search space and linguistic information involvement. The variety of such models includes a constrained distance-based reordering (Costa-jussà et al., 2006); and a constrained version of distortion model where the reordering search problem is tackled through a set of linguistically motivated rules used during decoding (Crego and Mariño, 2007). Proceedings of SSST-3, Third Workshop on S"
W09-2310,P03-1054,0,0.00299978,"to that shown in Habash and Sadat (2006), using the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.3 The BTEC development dataset consists of 489 sentences and 3.8 K running words, with 6 humanmade reference translations per sentence; the dataset 83 Experimental setup We used the Stanford Parser (Klein and Manning, 2003) for both languages, Penn English Treebank (Marcus et al., 1993) and Penn Arabic Treebank set (Kulick et al., 2006). The English Treebank is provided with 48 POS and 14 syntactic tags, the Arabic Treebank has 26 POS and 23 syntactic categories. As mentioned above, specific rules are not pruned away due to a limited amount of training material we set the thresholds kpart and kgener to relatively low values, 1 and 3, respectively. Evaluation conditions were case-insensitive and with punctuation marks considered. The targetside 4-gram language model was estimated using the SRILM toolkit (Stolcke,"
W09-2310,N03-1017,0,0.0203752,"Missing"
W09-2310,P07-2045,0,0.00740083,"sentation &quot;CFG form&quot;. We formally define a CFG in the usual way as G = hN, T, R, Si, where N is a set of nonterminal symbols (corresponding to source-side phrase and partof-speech tags); T is a set of source-side terminals (the lexicon), R is a set of production rules of the form η → γ, with η ∈ N and γ, which is a sequence of terminal and nonterminal symbols; and S ∈ N is the distinguished symbol. The reordering rules then have the form η0 @0 . . . ηk @k → Baseline phrase-based SMT systems The reference system which was used as a translation mechanism is the state-of-the-art Moses-based SMT (Koehn et al., 2007). The training and weights tuning procedures can be found on the Moses web page1 . Classical phrase-based translation is considered as a three step algorithm: (1) the source sequence of words is segmented into phrases, (2) each phrase is translated into the target language using a translation table, (3) the target phrases are reordered to fit the target language. The probabilities of the phrases are estimated by relative frequencies of their appearance in the training corpus. 1 In baseline experiments we used a phrase dependent lexicalized reordering model, as proposed in Tillmann (2004). Acco"
W09-2310,W04-3250,0,0.325388,"Missing"
W09-2310,J93-2004,0,0.0311303,"m for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.3 The BTEC development dataset consists of 489 sentences and 3.8 K running words, with 6 humanmade reference translations per sentence; the dataset 83 Experimental setup We used the Stanford Parser (Klein and Manning, 2003) for both languages, Penn English Treebank (Marcus et al., 1993) and Penn Arabic Treebank set (Kulick et al., 2006). The English Treebank is provided with 48 POS and 14 syntactic tags, the Arabic Treebank has 26 POS and 23 syntactic categories. As mentioned above, specific rules are not pruned away due to a limited amount of training material we set the thresholds kpart and kgener to relatively low values, 1 and 3, respectively. Evaluation conditions were case-insensitive and with punctuation marks considered. The targetside 4-gram language model was estimated using the SRILM toolkit (Stolcke, 2002) and modified Kneser-Ney discounting with interpolation. T"
W09-2310,J03-1002,0,0.00410782,"atmt.org/moses/ 79 ηd0 @d0 . . . ηdk @dk |Lexicon|p1 (1) where ηi ∈ N for all 0 ≤ i ≤ k; (do . . . dk ) is a permutation of (0 . . . k); Lexicon comes from the source-side set of words for each ηi ; and p1 is a probability associated with the rule. Figure 1 gives two examples of the rule format. 4.2 Rules extraction Concept. Inspired by the ideas presented in Imamura et al. (2005), where monolingual correspondences of syntactic nodes are used during decoding, we extract a set of bilingual patterns allowing for reordering as described below: (1) align the monotone bilingual corpus with GIZA++ (Och and Ney, 2003) and find the intersection of direct and inverse word alignments, resulting in the construction of the projection matrix P (see below)); (2) parse the source and the target parts of the parallel corpus; (3) extract reordering patterns from the parallel non-isomorphic CFG-trees based on the word alignment intersection. Step 2 is straightforward; we explain aspects of Steps 1 and 3 in more detail below. Figures 1 and 2 show an example of the extraction of two lexicalized rules for a parallel Arabic-English sentence: Arabic: h*A hW fndq +k English: this is your hotel Given two parse trees and a w"
W09-2310,N04-1021,0,0.0351906,"incorporate any linguistic analysis and work at the surface level of word forms. However, more recently MT systems are moving towards including additional linguistic and syntactic informative sources (for example, source- and/or targetside syntax) into word reordering process. In this paper we propose using a syntactic reordering system operating with fully, partially and non- lexicalized reordering patterns, which are applied on the step 78 Related work Many reordering algorithms have appeared over the past few years. Word class-based reordering was a part of Och’s Alignment Template system (Och et al., 2004); the main criticism of this approach is that it shows bad performance for the pair of languages with very distinct word order. The state-of-the-art SMT system Moses implements a distance-based reordering model (Koehn et al., 2003) and a distortion model, operating with rewrite patterns extracted from a phrase alignment table (Tillman, 2004). Many SMT models implement the brute force approach, introducing several constrains for the reordering search as described in Kanthak et al. (2005) and Crego et al. (2005). The main criticism of such systems is that the constraints are not lexicalized. Rec"
W09-2310,P02-1040,0,0.0778072,"Missing"
W09-2310,N04-4026,0,0.222367,"ng with fully, partially and non- lexicalized reordering patterns, which are applied on the step 78 Related work Many reordering algorithms have appeared over the past few years. Word class-based reordering was a part of Och’s Alignment Template system (Och et al., 2004); the main criticism of this approach is that it shows bad performance for the pair of languages with very distinct word order. The state-of-the-art SMT system Moses implements a distance-based reordering model (Koehn et al., 2003) and a distortion model, operating with rewrite patterns extracted from a phrase alignment table (Tillman, 2004). Many SMT models implement the brute force approach, introducing several constrains for the reordering search as described in Kanthak et al. (2005) and Crego et al. (2005). The main criticism of such systems is that the constraints are not lexicalized. Recently there has been interest in SMT exploiting non-monotonic decoding which allow for extension of the search space and linguistic information involvement. The variety of such models includes a constrained distance-based reordering (Costa-jussà et al., 2006); and a constrained version of distortion model where the reordering search problem"
W09-2310,D07-1077,0,0.0155249,"ird Workshop on Syntax and Structure in Statistical Translation, pages 78–86, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics A quite popular class of reordering algorithms is a monotonization of the source part of the parallel corpus prior to translation. The first work on this approach is described in Nießen and Ney (2004), where morpho-syntactic information was used to account for the reorderings needed. A representative set of similar systems includes: a set of hand-crafted reordering patterns for German-to-English (Collins et al., 2005) and Chinese-English (Wang et al., 2007) translations, emphasizing the distinction between German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-jussà and Fonollosa, 2006). Coupling of SMR algorithm and the search space extension via generating a set of weighted reordering hypotheses has demonstrated a significant improvement, as shown in Costa-jussà and Fonollosa (2008). The technique proposed in this study is most similar to the one propose"
W09-2310,C04-1073,0,0.0336371,"etween German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-jussà and Fonollosa, 2006). Coupling of SMR algorithm and the search space extension via generating a set of weighted reordering hypotheses has demonstrated a significant improvement, as shown in Costa-jussà and Fonollosa (2008). The technique proposed in this study is most similar to the one proposed for French-to-English translation task in Xia and McCord (2004), where the authors present a hybrid system for FrenchEnglish translation based on the principle of automatic rewrite patterns extraction using a parse tree and phrase alignments. We propose using a word distortion model not only to monotonize the source part of the corpus (using a different approach to rewrite rule organization from Xia and McCord), but also to extend the search space during decoding. 3 4 Syntax-based reordering coupled with word graph Our syntax-based reordering system requires access to source and target language parse trees and word alignments intersections. 4.1 Notation S"
W09-2310,2005.iwslt-1.18,0,0.173457,"e crossed links found in the word alignment and, conIn order to improve reordering power of the translation system, we implemented an additional reordering as described in Crego and Mariño (2006). Multiple word segmentations is encoded in a lattice, which is then passed to the input of the decoder, containing reordering alternatives consistent with the previously extracted rules. The decoder takes the n-best reordering of a source sentence coded in the form of a word lattice. This approach is in line with recent research tendencies in SMT, as described for example in (Hildebrand et al., 2008; Xu et al., 2005). Originally, word lattice algorithms do not involve syntax into reordering process, there(a) Monotonic search, plain text: >n +h mTEm *w tAryx Eryq >n S +h 1 mTEm 2 *w 3 tAryx 4 Eryq 5 L (b) Word lattice, plain text: >n +h mTEm *w tAryx Eryq mTEm +h mTEm Eryq tAryx *w >n S 2 1 4 3 >n 7 6 5 >n L 10 9 8 mTEm mTEm tAryx tAryx +h Eryq *w (c) Word lattice, reordered text: >n +h mTEm *w Eryq tAryx tAryx tAryx Eryq *w *w *w +h Eryq mTEm mTEm mTEm >n S tAryx 1 2 3 >n 4 5 6 7 10 9 8 11 12 >n 13 14 L *w mTEm +h *w *w Eryq Eryq Eryq Eryq Figure 5: Comparative example of a monotone search (a), word latti"
W09-2310,P01-1067,0,0.0647201,"tput functioning as an input to the next rule can lead to situations reverting the change of word order that the previously applied rule made. Therefore, the rules that can be ambiguous when applied sequentially during decoding are pruned according to the higher probability principle. For example, for the pair of patterns with the same lexicon (which is empty for a general rule leading to a recurring contradiction NP@0 VP@1 → VP@1 NP@0 p1, VP@0 NP@1 → NP@1 VP@0 p2 ), the less probable rule is removed. Finally, there are three resulting parameter tables analogous to the &quot;r-table&quot; as stated in (Yamada and Knight, 2001), consisting of POS- and constituentbased patterns allowing for reordering and monoInitial rule: Part. lexic. rules: General rule: tone distortion (examples can be found in Table 5). 4.4 Source-side monotonization Rule application is performed as a bottom-up parse tree traversal following two principles: (1) the longest possible rule is applied, i.e. among a set of nested rules, the rule with a longest left-side covering is selected. For example, in the case of the appearance of an NN JJ RB sequence and presence of the two reordering rules NN@0 JJ@1 → ... and NN@0 JJ@1 RB@2 → ... the latter pa"
W09-2508,N04-1041,0,0.078572,"Missing"
W09-2508,U07-1004,1,0.794554,"ery single entailment pair: there are entailment pairs that are recognized poorly by all the generic systems. Some approaches consequently propose a component-based model. In this framework, a generic system would have additional special components that take care of special subclasses of entailment pairs. Such a component is involved when a pair of its subclass is recognized. Vanderwende and Dolan (2005), and subsequently Vanderwende et al. (2006), divide all the entailment pairs according to whether categorization could be accurately predicted based solely on syntactic cues. Related to this, Akhmatova and Dras (2007) present an entailment type where the relationship expressed in the Hypothesis is encoded in a syntactic construction in the Text. 2 Related Work 2.1 Component-based RTE Vanderwende et al. (2006) use an approach based on logical forms, which they generate by the NLPwin parser. Nodes in the resulting syntactic dependency graphs for Text and Hypothesis are then heuristically aligned; then syntax-based heuristics 1 http://pascallin.ecs.soton.ac.uk/Challenges/RTE2, (BarHaim et al., 2006) 52 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 52–60, c Suntec, Singa"
W09-2508,C04-1111,0,0.0172137,"es a parent for the other members of the class. The first work that introduced co-occurrence methods to the field is that of Caraballo (1999). First she clusters nouns into groups based on conjunctive and appositive data collected from the Wall Street Journal. Nouns are grouped according to the similarity of being seen with other nouns in conjunctive and appositive relationships. In the second stage, using some knowledge about which conjuncts connect hypernyms reliably, a parent for a group of nouns is searched for in the same text corpora. Other co-occurrence methods can be found in works by Pantel et al. (2004) and Pantel and Ravichandran (2004). (1) Text: Japan’s Kyodo news agency said the US could be ready to set up a liaison office—the lowest level of diplomatic representation—in Pyongyang if it abandons its nuclear program. Hypothesis: Kyodo news agency is based in Japan. Asymmetric Association Measures In Asymmetric Association (see Dias et al. (2008)) hypernymy is derived through the measure of how much one word ‘attracts’ another one. When hearing “fruit”, more common fruits will be likely to come into mind such as “apple” or “banana”. In this case, there exists an oriented association betwee"
W09-2508,P06-1101,0,0.0983576,"Missing"
W09-2508,J90-2002,0,0.684181,"Missing"
W09-2508,P99-1016,0,0.0247135,"eric systems participating in an RTE Challenge. These are the entailment pairs where a specific syntactic construction in the Text encodes a semantic relationship between its elements that is explicitly shown in the Hypothesis, as in example (1): Cooccurence Approaches Co-occurrence approaches first cluster words into similarity classes and consider the elements of a class to be siblings of one parent. Therefore the search for a parent for some members from the class gives a parent for the other members of the class. The first work that introduced co-occurrence methods to the field is that of Caraballo (1999). First she clusters nouns into groups based on conjunctive and appositive data collected from the Wall Street Journal. Nouns are grouped according to the similarity of being seen with other nouns in conjunctive and appositive relationships. In the second stage, using some knowledge about which conjuncts connect hypernyms reliably, a parent for a group of nouns is searched for in the same text corpora. Other co-occurrence methods can be found in works by Pantel et al. (2004) and Pantel and Ravichandran (2004). (1) Text: Japan’s Kyodo news agency said the US could be ready to set up a liaison o"
W09-2508,U07-1008,0,0.0202702,"but as we show below, it is necessary to develop a more complex set of counts in order to apply this to our entailments type. In particular, we expect that the division of patterns into lexical and syntactic parts, in order to score them separately, is beneficial for entailment. Again, it is a result of scarcity of information: we have only one text sentence, not the whole text corpus to make the entailment decision. Examples (3) - (4) are taken from the RTE2 test corpus. 55 4 4.1 Experimental Setup from the pair itself. The counts are extracted with the help of the software get1t written by Hawker et al. (2007). We refer to this as our LEXICAL PATTERN TRAINING SET . Data Our goal is to build a classifier that will detect whether a given potential hypernymy entailment pair is true or false; we first need to construct sets of such pairs for training and testing. As our basic data source, we use 500 000 sentences from the Wikipedia XML corpus (Denoyer and Gallinari, 2006); this is the corpus used by Akhmatova and Dras (2007), and related to one used in one set of experiments by Snow et al. (2005). These sentences were parsed with the MINIPAR parser. We identified Known Hypernym pairs as did Snow et al."
W09-2508,C92-2082,0,0.497732,"ed Text, so the problem reduces to one of classification over the Text. 2.2 Pattern-based Methods Pattern-based methods are based on the observation that hypernyms tend to be connected in the sentences by specific words or patterns, and that some patterns can predict hypernymy with very high probability, like the X and other Y pattern. Generally, some amount of manual work on finding the seed patterns is done first. Automated algorithms use these patterns for discovering more patterns and for the subsequent hypernymy extraction. The fundamental work for the pattern-based approaches is that of Hearst (1992). More recently, Snow et al. (2005) and Snow et al. (2006) have described a method of hypernymy extraction using machine learning of Hypernymy Extraction The aim of work on hypernymy extraction is usually the enrichment of a lexical resource such as WordNet, or creation of specific hierarchical lexical data directly for the purpose of some appli53 an ancestor of the first sense of ni in the WordNet hypernym taxonomy (Fellbaum, 1998). A noun pair might have been assigned to the second set of Known Non-Hypernym pairs if both nouns are contained within WordNet, but neither noun is an ancestor of"
W09-2508,W07-1401,0,\N,Missing
W11-2828,H05-1042,0,0.0173431,"00 Player Jesse White Jarrad McVeigh Patrick Ryder Event H A G B B Score HAM 6 0 6 7 0 7 7 1 6 Table 1: Sample scoring events data Player Jude Bolton Adam Goodes Heath Grundy K 16 11 8 M 3 5 2 H 20 5 8 G 0 2 0 B 0 4 0 T 12 1 1 Table 2: Sample of in-game player statistics approach, working on American football data. Formulating the problem as one of energy minimisation allows them to find a globally optimal set of database rows, in contrast to the independent row selection of Duboue and McKeown (2003). The goal of both approaches was to extract and present items that occur in the tabular data; Barzilay and Lapata (2005) explicitly restrict themselves to selecting from this raw data. Kelly et al. (2009), applying Barzilay and Lapata’s approach to the domain of cricket, go beyond looking at raw data items to a limited ‘grouping’ of data, for example in pairing player data for batting partnerships. In contrast, we are interested in presenting not just raw data, but data over which some inference has been carried out (as in the selection of time series data by Yu et al. (2004)), and the feasibility of using a machine learning approach to achieve this. 3 Correlating data and texts Our data comes in the form of ta"
W11-2828,W03-1016,0,0.0778121,"Missing"
W11-2828,W09-0623,0,0.031242,"1 6 Table 1: Sample scoring events data Player Jude Bolton Adam Goodes Heath Grundy K 16 11 8 M 3 5 2 H 20 5 8 G 0 2 0 B 0 4 0 T 12 1 1 Table 2: Sample of in-game player statistics approach, working on American football data. Formulating the problem as one of energy minimisation allows them to find a globally optimal set of database rows, in contrast to the independent row selection of Duboue and McKeown (2003). The goal of both approaches was to extract and present items that occur in the tabular data; Barzilay and Lapata (2005) explicitly restrict themselves to selecting from this raw data. Kelly et al. (2009), applying Barzilay and Lapata’s approach to the domain of cricket, go beyond looking at raw data items to a limited ‘grouping’ of data, for example in pairing player data for batting partnerships. In contrast, we are interested in presenting not just raw data, but data over which some inference has been carried out (as in the selection of time series data by Yu et al. (2004)), and the feasibility of using a machine learning approach to achieve this. 3 Correlating data and texts Our data comes in the form of tables that focus on different aspects of the game. The most important for our current"
W11-2828,W08-1124,0,0.0672643,"Missing"
W11-2828,E03-1021,0,0.0340066,"this task that can be used as a baseline for future work, and examine its adequacy for content selection (§4). 2 Related work Time series Previous work has dealt with time series data and the particular problem of segmenting them meaningfully. Time series are typically continuous processes monitored at regular intervals; ours, in contrast, are irregular sequences of discrete events. The main difference is the number of data points: for example, a pressure sensor can produce thousands of readings in a day, but we only need to consider about 50 events in a game (see §3). The S UM T IME project (Sripada et al., 2003b; Yu Led by Brownlow medallist Adam Goodes and et al., 2004) aims to produce a generic time series veteran Jude Bolton, the Swans kicked seven goals from 16 entries inside their forward 50 to summary generator. It has been applied to weather open a 30-point advantage at the final change— forecasts (Sripada et al., 2002; Sripada et al., 2003a), to that point the largest lead of the match. neo-natal intensive care (Sripada et al., 2003c; Portet There is a corresponding database which contains et al., 2009), and gas turbine monitoring (Yu et al., quantitative and other data regarding the game: w"
W13-1716,P06-4020,0,0.0157839,"Docj → j Docj → Docj T opici T opici → W ords W ords → W ord W ords → W ords W ord W ord → w POS n-grams Most studies have found that POS tag n-grams are a very useful feature for NLI (Koppel et al., 2005; Bykh and Meurers, 2012, for example). The tagset provided by the Penn TreeBank is the most widely used in these experiments, with tagging performed by the Stanford Tagger (Toutanova et al., 2003). We investigate the effect of tagset granularity on classification accuracy by comparing the classification accuracy of texts tagged with the PTB tagset against those annotated by the RASP Tagger (Briscoe et al., 2006). The PTB POS tagset contains 36 unique tags, while the RASP system uses a subset of the CLAWS2 tagset, consisting of 150 tags. This is a significant size difference and we hypothesize that a larger tagset could provide richer levels of syntactically meaningful info which is more fine-grained in distinction between syntactic categories and contains more morpho-syntactic information such as gender, number, person, case and tense. For example, while the PTB tagset has four tags for pronouns (PRP, PRP$, WP, WP$), the CLAWS tagset provides over 20 pronoun tags (PPHO1, PPIS1, PPX2, PPY, etc.) disti"
W13-1716,C12-1025,0,0.317902,"e Identification (NLI) noted by the shared task organisers, there are two trends in recent work in particular that we considered in building our submission. The first is the proposal and use of new features that might have relevance to NLI: for example, Wong and Dras (2011), motivated by the Contrastive Analysis Hypothesis (Lado, 1957) from the field of Second Language Acquisition, introduced A second trend, most apparent in 2012, was the examination of other corpora besides the International Corpus of Learner English used in earlier work, and in particular the use of cross-corpus evaluation (Brooke and Hirst, 2012; Tetreault et al., 2012) to avoid topic bias in determining native language. Possible topic bias had been a reason for avoiding a full range of n-grams, in particular those containing content words (Koppel et al., 2009); the development of new corpora and the analysis of the effect of topic bias mitigated this. The consequent use of a full range of n-grams further reinforced the view that novel features were unlikely to be a major source of interesting results. We therefore concentrated on two areas: the use of classifier ensembles, and the choice of part-ofspeech tags. With classifier ensemb"
W13-1716,C12-1027,0,0.0298664,"e n-grams. Character n-grams Tsur and Rappoport (2007) demonstrated that character n-grams are a useful feature for NLI. These n-grams can be considered as a sub-word feature and their effectiveness is hypothesized to be a result of phoneme transfer from the writer’s L1. They can also capture orthographic conventions of a language. Accordingly, we limit our n-grams to a maximum size of 3 as longer sequences would correspond to short words and not phonemes or syllables. Word n-grams There has been a shift towards the use of word-based features in several recent studies (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Tetreault et al., 2012), with new corpora come into use for NLI and researchers exploring and addressing the issues relating to topic bias that previously prevented their use. Lexical choice is considered to be a prime feature for studying language transfer effects, and researchers have found word n-grams to be one of the strongest features for NLI. Tetreault et al. (2012) expanded on this by integrating 5-gram language models into their system. While we did not replicate this, we made use of word trigrams. 3.2 mars where each is associated with a different set of vocabulary: either pure POS"
W13-1716,P07-2009,0,0.0171866,"Missing"
W13-1716,de-marneffe-etal-2006-generating,0,0.00600023,"Missing"
W13-1716,E03-1068,0,0.0975297,"Missing"
W13-1716,guthrie-etal-2006-closer,0,0.113095,"auxiliary verbs. They have been widely used in studies of authorship attribution as well as NLI and established to be informative for these tasks. We use the list of 398 common English function words from Wong and Dras (2011). We also tested smaller sets, but observed that the larger sets achieve higher accuracy. Function Word n-grams We devised and tested a new feature that attempts to capture patterns of function word use at the sentence level. We define function word n-grams as a type of word n-gram where content words are skipped: they are thus a specific subtype of skip-gram discussed by Guthrie et al. (2006). For example, the sentence We should all start taking the bus would be reduced to we should all the, from which we would extract the n-grams. Character n-grams Tsur and Rappoport (2007) demonstrated that character n-grams are a useful feature for NLI. These n-grams can be considered as a sub-word feature and their effectiveness is hypothesized to be a result of phoneme transfer from the writer’s L1. They can also capture orthographic conventions of a language. Accordingly, we limit our n-grams to a maximum size of 3 as longer sequences would correspond to short words and not phonemes or sylla"
W13-1716,P10-1117,0,0.022804,"archers exploring and addressing the issues relating to topic bias that previously prevented their use. Lexical choice is considered to be a prime feature for studying language transfer effects, and researchers have found word n-grams to be one of the strongest features for NLI. Tetreault et al. (2012) expanded on this by integrating 5-gram language models into their system. While we did not replicate this, we made use of word trigrams. 3.2 mars where each is associated with a different set of vocabulary: either pure POS or the mixture of POS and function words. We use the grammar proposed by Johnson (2010) for capturing topical collocations as presented below: Sentence → Docj Docj → j Docj → Docj T opici T opici → W ords W ords → W ord W ords → W ords W ord W ord → w POS n-grams Most studies have found that POS tag n-grams are a very useful feature for NLI (Koppel et al., 2005; Bykh and Meurers, 2012, for example). The tagset provided by the Penn TreeBank is the most widely used in these experiments, with tagging performed by the Stanford Tagger (Toutanova et al., 2003). We investigate the effect of tagset granularity on classification accuracy by comparing the classification accuracy of texts"
W13-1716,P03-1054,0,0.00307486,"n Monte Carlo technique made available by Johnson (2010).3 Tree Subtitution Grammar fragments In relation to the context-free grammar (CFG) rules explored in the previous NLI work of Wong and Dras (2011), Tree Substitution Grammar (TSG) fragments have been proposed by Swanson and Charniak (2012) as another form of syntactic features for NLI classification tasks. Here, as an approximation to deploying the Bayesian approach to induce a TSG (Post and Gildea, 2009; Swanson and Charniak, 2012), we first parse each of the essays in the TOEFL training corpus with the Stanford Parser (version 2.0.4) (Klein and Manning, 2003) to obtain the parse trees. We then extract the TSG fragments from the parse trees using the TSG system made available by Post and Gildea (2009).4 Stanford dependencies In Tetreault et al. (2012), Stanford dependencies were investigated as yet another form of syntactic features. We follow a similar approach: for each essay in the training corpus, we extract all the basic (rather than 3 http://web.science.mq.edu.au/˜mjohnson/ Software.htm 4 https://github.com/mjpost/dptsg the collapsed) dependencies returned by the Stanford Parser (de Marneffe et al., 2006). Similarly, we generate all the varia"
W13-1716,P09-2012,0,0.0101138,"Wong et al. (2012)) given that the TOEFL corpus is larger than the ICLE corpus. The inference algorithm for the adaptor grammars are based on the Markov Chain Monte Carlo technique made available by Johnson (2010).3 Tree Subtitution Grammar fragments In relation to the context-free grammar (CFG) rules explored in the previous NLI work of Wong and Dras (2011), Tree Substitution Grammar (TSG) fragments have been proposed by Swanson and Charniak (2012) as another form of syntactic features for NLI classification tasks. Here, as an approximation to deploying the Bayesian approach to induce a TSG (Post and Gildea, 2009; Swanson and Charniak, 2012), we first parse each of the essays in the TOEFL training corpus with the Stanford Parser (version 2.0.4) (Klein and Manning, 2003) to obtain the parse trees. We then extract the TSG fragments from the parse trees using the TSG system made available by Post and Gildea (2009).4 Stanford dependencies In Tetreault et al. (2012), Stanford dependencies were investigated as yet another form of syntactic features. We follow a similar approach: for each essay in the training corpus, we extract all the basic (rather than 3 http://web.science.mq.edu.au/˜mjohnson/ Software.ht"
W13-1716,P12-2038,0,0.0508456,"t POS tags based on the Brown tagset and Vpos+f w is extended with 398 function words used in Wong and Dras (2011). The number of topics t is set to 50 (instead of 25 as per Wong et al. (2012)) given that the TOEFL corpus is larger than the ICLE corpus. The inference algorithm for the adaptor grammars are based on the Markov Chain Monte Carlo technique made available by Johnson (2010).3 Tree Subtitution Grammar fragments In relation to the context-free grammar (CFG) rules explored in the previous NLI work of Wong and Dras (2011), Tree Substitution Grammar (TSG) fragments have been proposed by Swanson and Charniak (2012) as another form of syntactic features for NLI classification tasks. Here, as an approximation to deploying the Bayesian approach to induce a TSG (Post and Gildea, 2009; Swanson and Charniak, 2012), we first parse each of the essays in the TOEFL training corpus with the Stanford Parser (version 2.0.4) (Klein and Manning, 2003) to obtain the parse trees. We then extract the TSG fragments from the parse trees using the TSG system made available by Post and Gildea (2009).4 Stanford dependencies In Tetreault et al. (2012), Stanford dependencies were investigated as yet another form of syntactic fe"
W13-1716,C12-1158,0,0.354586,"guage Technology Macquarie University Sydney, Australia {shervin.malmasi,sze.wong,mark.dras}@mq.edu.au Abstract syntactic structure as a feature; Swanson and Charniak (2012) introduced more complex Tree Substitution (TSG) structures, learned by Bayesian inference; and Bykh and Meurers (2012) used recurring n-grams, inspired by the variation n-gram approach to corpus error annotation detection (Dickinson and Meurers, 2003). Starting from the features introduced in these papers and others, then, other recent papers have compiled a comprehensive collection of features based on the earlier work — Tetreault et al. (2012) is an example, combining and analysing most of the features used in previous work. Given the timeframe of the shared task, there seemed to be not much mileage in trying new features that were likely to be more peripheral to the task. Our submission for this NLI shared task used for the most part standard features found in recent work. Our focus was instead on two other aspects of our system: at a high level, on possible ways of constructing ensembles of multiple classifiers; and at a low level, on the granularity of part-of-speech tags used as features. We found that the choice of ensemble co"
W13-1716,N03-1033,0,0.0244956,"associated with a different set of vocabulary: either pure POS or the mixture of POS and function words. We use the grammar proposed by Johnson (2010) for capturing topical collocations as presented below: Sentence → Docj Docj → j Docj → Docj T opici T opici → W ords W ords → W ord W ords → W ords W ord W ord → w POS n-grams Most studies have found that POS tag n-grams are a very useful feature for NLI (Koppel et al., 2005; Bykh and Meurers, 2012, for example). The tagset provided by the Penn TreeBank is the most widely used in these experiments, with tagging performed by the Stanford Tagger (Toutanova et al., 2003). We investigate the effect of tagset granularity on classification accuracy by comparing the classification accuracy of texts tagged with the PTB tagset against those annotated by the RASP Tagger (Briscoe et al., 2006). The PTB POS tagset contains 36 unique tags, while the RASP system uses a subset of the CLAWS2 tagset, consisting of 150 tags. This is a significant size difference and we hypothesize that a larger tagset could provide richer levels of syntactically meaningful info which is more fine-grained in distinction between syntactic categories and contains more morpho-syntactic informat"
W13-1716,W07-0602,0,0.0518239,"sh function words from Wong and Dras (2011). We also tested smaller sets, but observed that the larger sets achieve higher accuracy. Function Word n-grams We devised and tested a new feature that attempts to capture patterns of function word use at the sentence level. We define function word n-grams as a type of word n-gram where content words are skipped: they are thus a specific subtype of skip-gram discussed by Guthrie et al. (2006). For example, the sentence We should all start taking the bus would be reduced to we should all the, from which we would extract the n-grams. Character n-grams Tsur and Rappoport (2007) demonstrated that character n-grams are a useful feature for NLI. These n-grams can be considered as a sub-word feature and their effectiveness is hypothesized to be a result of phoneme transfer from the writer’s L1. They can also capture orthographic conventions of a language. Accordingly, we limit our n-grams to a maximum size of 3 as longer sequences would correspond to short words and not phonemes or syllables. Word n-grams There has been a shift towards the use of word-based features in several recent studies (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Tetreault et al., 2012), with"
W13-1716,D11-1148,1,0.789005,"tion approach, broadly speaking points are assigned to ranks, and these tallied for the overall weight. With the exception of the plurality vote, all of these can be weighted. In our ensembles we also experiment with weighting the output of each classifier using its individual accuracy on the training data as an indication of our degree of confidence in it. 2.4 Feature Representation Most NLI studies have used two types of feature representations: binary (presence or absence of a feature in a text) and normalized frequencies. Although binary feature values have been used in some studies (e.g. Wong and Dras (2011)), most have used frequency-based values. In the course of our experiments we have observed that the effect of the feature representation varies with the feature type, size of the feature space and the learning algorithm itself. In our current system, then, we generate two classifiers for each feature type, one trained with frequency-based values (raw counts scaled using the L2-norm) and the other with binary. Our experiments assess both their individual and joint performance. 2.5 Proficiency-level Based Classification To utilise the proficiency level information provided in the TOEFL11 corpus"
W13-1716,D12-1064,1,0.459335,"vide richer levels of syntactically meaningful info which is more fine-grained in distinction between syntactic categories and contains more morpho-syntactic information such as gender, number, person, case and tense. For example, while the PTB tagset has four tags for pronouns (PRP, PRP$, WP, WP$), the CLAWS tagset provides over 20 pronoun tags (PPHO1, PPIS1, PPX2, PPY, etc.) distinguishing between person, number and grammatical role. Consequently, these tags could help better capture error patterns to be used for classification. 3.3 Syntactic Features Adaptor grammar collocations Drawing on Wong et al. (2012), we also utilise an adaptor grammar to discover arbitrary lengths of n-gram collocations for the TOEFL11 corpus. We explore both the pure part-of-speech (POS) n-grams as well as the more promising mixtures of POS and function words. Following a similar experimental setup as per Wong et al. (2012), we derive two adaptor gram127 j ∈ 1, . . . , m j ∈ 1, . . . , m i ∈ 1, . . . , t; j ∈ 1, . . . , m i ∈ 1, . . . , t w ∈ Vpos ; w ∈ Vpos+f w As per Wong et al. (2012), Vpos contains 119 distinct POS tags based on the Brown tagset and Vpos+f w is extended with 398 function words used in Wong and Dras"
W13-4901,A00-2018,0,0.108553,"eriments by Charniak (1997) concluded that self-training is ineffective because mistakes made by the parser are magnified rather than smoothed during the self-training process. The selftraining experiments of Steedman et al. (2003) also yielded disappointing results. Reichart and Rappaport (2007) found, on the other hand, that selftraining could be effective if the seed training set was very small. McClosky et al. (2006) also report positive results from self-training, but the selftraining protocol that they use cannot be considered to be pure self-training as the first-stage Charniak parser (Charniak, 2000) is retrained on the output of the two-stage parser (Charniak and Johnson, 2005) They later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure (McClosky et al., 2008). Sagae (2010) reports positive self-training results even without the reranking phase in a domain adaptation scenario, as do Huang and Harper (2009) who employ self-training with a PCFG-LA parser. 5.1.2 Experimental Setup The labelled data available to us for this experiment comprises the 803 gold standard trees referred to in Section 3. This small treebank"
W13-4901,D09-1087,0,0.0164299,"g set was very small. McClosky et al. (2006) also report positive results from self-training, but the selftraining protocol that they use cannot be considered to be pure self-training as the first-stage Charniak parser (Charniak, 2000) is retrained on the output of the two-stage parser (Charniak and Johnson, 2005) They later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure (McClosky et al., 2008). Sagae (2010) reports positive self-training results even without the reranking phase in a domain adaptation scenario, as do Huang and Harper (2009) who employ self-training with a PCFG-LA parser. 5.1.2 Experimental Setup The labelled data available to us for this experiment comprises the 803 gold standard trees referred to in Section 3. This small treebank includes the 150-tree development set and 150-tree test set used in experiments by Lynn et al. (2012b). We use the same development and test sets for this study. As for the remaining 503 trees, we remove any trees that have more than 200 tokens. The motivation for this is two-fold: (i) we had difficulties training Mate parser with long sentences due to memory resource issues, and (ii)"
W13-4901,U12-1005,1,0.741841,"ey later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure (McClosky et al., 2008). Sagae (2010) reports positive self-training results even without the reranking phase in a domain adaptation scenario, as do Huang and Harper (2009) who employ self-training with a PCFG-LA parser. 5.1.2 Experimental Setup The labelled data available to us for this experiment comprises the 803 gold standard trees referred to in Section 3. This small treebank includes the 150-tree development set and 150-tree test set used in experiments by Lynn et al. (2012b). We use the same development and test sets for this study. As for the remaining 503 trees, we remove any trees that have more than 200 tokens. The motivation for this is two-fold: (i) we had difficulties training Mate parser with long sentences due to memory resource issues, and (ii) in keeping with the findings of Lynn et al. (2012b), the large trees were sentences from legislative text that were difficult to analyse for automatic parsers and human annotators. This leaves us with 500 gold-standard trees as our seed training data set. For our unlabelled data, we take the next 1945 sentences"
W13-4901,N06-1020,0,0.029392,"well known semi-supervised techniques. 5.1 Self-Training 5.1.1 Related Work Self-training, the process of training a system on its own output, has a long and chequered history in parsing. Early experiments by Charniak (1997) concluded that self-training is ineffective because mistakes made by the parser are magnified rather than smoothed during the self-training process. The selftraining experiments of Steedman et al. (2003) also yielded disappointing results. Reichart and Rappaport (2007) found, on the other hand, that selftraining could be effective if the seed training set was very small. McClosky et al. (2006) also report positive results from self-training, but the selftraining protocol that they use cannot be considered to be pure self-training as the first-stage Charniak parser (Charniak, 2000) is retrained on the output of the two-stage parser (Charniak and Johnson, 2005) They later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure (McClosky et al., 2008). Sagae (2010) reports positive self-training results even without the reranking phase in a domain adaptation scenario, as do Huang and Harper (2009) who employ self-trai"
W13-4901,C08-1071,0,0.0167963,"also yielded disappointing results. Reichart and Rappaport (2007) found, on the other hand, that selftraining could be effective if the seed training set was very small. McClosky et al. (2006) also report positive results from self-training, but the selftraining protocol that they use cannot be considered to be pure self-training as the first-stage Charniak parser (Charniak, 2000) is retrained on the output of the two-stage parser (Charniak and Johnson, 2005) They later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure (McClosky et al., 2008). Sagae (2010) reports positive self-training results even without the reranking phase in a domain adaptation scenario, as do Huang and Harper (2009) who employ self-training with a PCFG-LA parser. 5.1.2 Experimental Setup The labelled data available to us for this experiment comprises the 803 gold standard trees referred to in Section 3. This small treebank includes the 150-tree development set and 150-tree test set used in experiments by Lynn et al. (2012b). We use the same development and test sets for this study. As for the remaining 503 trees, we remove any trees that have more than 200 t"
W13-4901,D07-1013,0,0.0693299,"Missing"
W13-4901,nivre-etal-2006-maltparser,0,0.223232,"Missing"
W13-4901,D08-1093,0,0.0129817,"parsers (as approximated by parse probability). In Active Learning research, the Query By Committee selection method (Seung et al., 1992) is used to choose items for annotation – if a committee of two or more systems disagrees on an item, this is evidence that the item needs to be prioritised for manual correction (see for example Lynn et al. (2012b)). Steedman et al. (2003) discuss a sample selection approach based on differences between parsers – if parser A and parser B disagree on an analysis, parser A can be improved by being retrained on parser B’s analysis, and vice versa. In contrast, Ravi et al. (2008) show that parser agreement is a strong indicator of parse quality, and in parser domain adaptation, Sagae and Tsujii (2007) and Le Roux et al. (2012) use agreement between parsers to choose which automatically parsed target domain items to add to the training set. Sample selection can be used with both selftraining and co-training. We restrict our attention to co-training since our previous experiments have demonstrated that it has more potential than selftraining. In the following set of experiments, we explore the role of both parser agreement and parser disagreement in sample selection in"
W13-4901,W11-3808,0,0.0345938,"Missing"
W13-4901,P07-1078,0,0.016503,"ta sparsity issues brought about by our lack of training material, we experi4 ment with automatically expanding our training set using well known semi-supervised techniques. 5.1 Self-Training 5.1.1 Related Work Self-training, the process of training a system on its own output, has a long and chequered history in parsing. Early experiments by Charniak (1997) concluded that self-training is ineffective because mistakes made by the parser are magnified rather than smoothed during the self-training process. The selftraining experiments of Steedman et al. (2003) also yielded disappointing results. Reichart and Rappaport (2007) found, on the other hand, that selftraining could be effective if the seed training set was very small. McClosky et al. (2006) also report positive results from self-training, but the selftraining protocol that they use cannot be considered to be pure self-training as the first-stage Charniak parser (Charniak, 2000) is retrained on the output of the two-stage parser (Charniak and Johnson, 2005) They later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure (McClosky et al., 2008). Sagae (2010) reports positive self-traini"
W13-4901,D07-1111,0,0.0230024,"eung et al., 1992) is used to choose items for annotation – if a committee of two or more systems disagrees on an item, this is evidence that the item needs to be prioritised for manual correction (see for example Lynn et al. (2012b)). Steedman et al. (2003) discuss a sample selection approach based on differences between parsers – if parser A and parser B disagree on an analysis, parser A can be improved by being retrained on parser B’s analysis, and vice versa. In contrast, Ravi et al. (2008) show that parser agreement is a strong indicator of parse quality, and in parser domain adaptation, Sagae and Tsujii (2007) and Le Roux et al. (2012) use agreement between parsers to choose which automatically parsed target domain items to add to the training set. Sample selection can be used with both selftraining and co-training. We restrict our attention to co-training since our previous experiments have demonstrated that it has more potential than selftraining. In the following set of experiments, we explore the role of both parser agreement and parser disagreement in sample selection in co-training. 5.3.2 Agreement-Based Co-Training Experimental Setup The main algorithm for agreement-based co-training is give"
W13-4901,W10-2606,0,0.0175687,"ng results. Reichart and Rappaport (2007) found, on the other hand, that selftraining could be effective if the seed training set was very small. McClosky et al. (2006) also report positive results from self-training, but the selftraining protocol that they use cannot be considered to be pure self-training as the first-stage Charniak parser (Charniak, 2000) is retrained on the output of the two-stage parser (Charniak and Johnson, 2005) They later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure (McClosky et al., 2008). Sagae (2010) reports positive self-training results even without the reranking phase in a domain adaptation scenario, as do Huang and Harper (2009) who employ self-training with a PCFG-LA parser. 5.1.2 Experimental Setup The labelled data available to us for this experiment comprises the 803 gold standard trees referred to in Section 3. This small treebank includes the 150-tree development set and 150-tree test set used in experiments by Lynn et al. (2012b). We use the same development and test sets for this study. As for the remaining 503 trees, we remove any trees that have more than 200 tokens. The mot"
W13-4901,E03-1008,0,0.269735,"emi-Supervised Parsing Experiments In order to alleviate data sparsity issues brought about by our lack of training material, we experi4 ment with automatically expanding our training set using well known semi-supervised techniques. 5.1 Self-Training 5.1.1 Related Work Self-training, the process of training a system on its own output, has a long and chequered history in parsing. Early experiments by Charniak (1997) concluded that self-training is ineffective because mistakes made by the parser are magnified rather than smoothed during the self-training process. The selftraining experiments of Steedman et al. (2003) also yielded disappointing results. Reichart and Rappaport (2007) found, on the other hand, that selftraining could be effective if the seed training set was very small. McClosky et al. (2006) also report positive results from self-training, but the selftraining protocol that they use cannot be considered to be pure self-training as the first-stage Charniak parser (Charniak, 2000) is retrained on the output of the two-stage parser (Charniak and Johnson, 2005) They later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure"
W13-4901,C10-1011,0,\N,Missing
W13-4901,P05-1022,0,\N,Missing
W13-4901,ballesteros-nivre-2012-maltoptimizer-system,0,\N,Missing
W14-3625,abuhakema-etal-2008-annotating,0,0.100196,"we employ a supervised multi-class classification approach. The learner texts are organized into classes according on the author’s L1 and these documents are used for training and testing in our experiments. A diagram conceptualizing our NLI system is shown in Figure 1. Data Although the majority of currently available learner corpora are based on English L2 (Granger, 2012), data from learners of other languages such as Chinese have also attracted attention in the past several years. No Arabic learner corpora were available for a long time. This paucity of data has been noted by researchers (Abuhakema et al., 2008; Zaghouani et al., 2014) and is thought to be due to issues such as difficulties with non-Latin script and a lack of linguistic and NLP software to work with the data. More recently, the first version of the Arabic Learner Corpus1 (ALC) was released by Alfaifi and Atwell (2013). The corpus includes texts by Arabic learners studying in Saudi Arabia, mostly timed essays written in class. In total, 66 different L1 backgrounds are represented. While texts by native Arabic speakers studying to improve their writing are also included, we do not utilize these. We use the more recent second version o"
W14-3625,W13-1716,1,0.412699,"Missing"
W14-3625,E14-4033,0,0.083742,"Missing"
W14-3625,C12-1158,0,0.182398,"Missing"
W14-3625,W13-1706,0,0.0776445,"180–186, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2 Native Language Texts Chinese 76 Urdu 64 Malay 46 French 44 Fulani 36 English 35 Yoruba 28 Total 329 Background NLI has drawn the attention of many researchers in recent years. With the influx of new researchers, the most substantive work in this field has come in the last few years, leading to the organization of the inaugural NLI Shared Task in 2013 which was attended by 29 teams from the NLP and SLA areas. A detailed exposition of the shared task results and a review of prior NLI work can be found in Tetreault et al. (2013). While there exists a large body of literature produced in the last decade, almost all of this work has focused exclusively on L2 English. The most recent work in this field successfully presented the first application of NLI to a large non-English dataset (Malmasi and Dras, 2014a), evidencing the usefulness of syntactic features in distinguishing L2 Chinese texts. 3 Table 1: The L1 classes included in this experiment and the number of texts within each class. texts are provided with the corpus. Here we use text versions and strip the metadata information from the files, leaving only the auth"
W14-3625,E14-4019,1,0.799275,"x of new researchers, the most substantive work in this field has come in the last few years, leading to the organization of the inaugural NLI Shared Task in 2013 which was attended by 29 teams from the NLP and SLA areas. A detailed exposition of the shared task results and a review of prior NLI work can be found in Tetreault et al. (2013). While there exists a large body of literature produced in the last decade, almost all of this work has focused exclusively on L2 English. The most recent work in this field successfully presented the first application of NLI to a large non-English dataset (Malmasi and Dras, 2014a), evidencing the usefulness of syntactic features in distinguishing L2 Chinese texts. 3 Table 1: The L1 classes included in this experiment and the number of texts within each class. texts are provided with the corpus. Here we use text versions and strip the metadata information from the files, leaving only the author’s writings. 4 In this study we employ a supervised multi-class classification approach. The learner texts are organized into classes according on the author’s L1 and these documents are used for training and testing in our experiments. A diagram conceptualizing our NLI system i"
W14-3625,W14-3708,1,0.904561,"x of new researchers, the most substantive work in this field has come in the last few years, leading to the organization of the inaugural NLI Shared Task in 2013 which was attended by 29 teams from the NLP and SLA areas. A detailed exposition of the shared task results and a review of prior NLI work can be found in Tetreault et al. (2013). While there exists a large body of literature produced in the last decade, almost all of this work has focused exclusively on L2 English. The most recent work in this field successfully presented the first application of NLI to a large non-English dataset (Malmasi and Dras, 2014a), evidencing the usefulness of syntactic features in distinguishing L2 Chinese texts. 3 Table 1: The L1 classes included in this experiment and the number of texts within each class. texts are provided with the corpus. Here we use text versions and strip the metadata information from the files, leaving only the author’s writings. 4 In this study we employ a supervised multi-class classification approach. The learner texts are organized into classes according on the author’s L1 and these documents are used for training and testing in our experiments. A diagram conceptualizing our NLI system i"
W14-3625,U09-1008,1,0.825041,"Missing"
W14-3625,D14-1144,1,\N,Missing
W14-3625,zaghouani-etal-2014-large,0,\N,Missing
W14-3708,P06-4020,0,0.00970702,"ms (n = 1, 2, 3) as features in this work. Note that we use the terminology of Yannakoudakis et al. (2012) here: what had their origin as features in the essay classification task are still referred to as features in the visualisation tool, although the task carried out there is not a classification one. Similarly, we refer to our PoS n-grams as features, although we are not classifying errors using these features and so are not carrying out feature selection for the typical purpose of optimising classification performance. For this, as did Yannakoudakis et al. (2012), we use the RASP parser (Briscoe et al., 2006) for tagging; the tags are consequently from the CLAWS2 tagset,4 which are more fine-grained in terms of linguistic analysis than the more frequently used Penn Treebank tags. For our task, we then used the subset of the FCE corpus where the languages overlapped with the TOEFL11 corpus: we refer to this as FCE SUB. This gives 799 scripts over 8 languages, distributed as in Table 1; a positive byproduct is that the L1s are more similar in size than the full FCE corpus. 3.2 Association Measure We noted in §1 that SLA studies such as Di´ez-Bedmar and Papp (2008) use standard hypothesis testing tec"
W14-3708,E14-4033,0,0.371275,"; see for example the recent NLI shared task with 29 entrants (Tetreault et al., 2013).1 From within linguistics, there has been much interest in how data-driven approaches can contribute to SLA. Granger (2011) discusses a body of work based on the the methodology of carrying out corpus-based approaches to SLA with a focus on NLP tools; Jarvis and Crossley (2012) in an edited collection present recent work by linguists who extend the corpus-based setup by using a text classification approach, looking at what feature selection might say for SLA. From within NLP, Swanson and Charniak (2013) and Swanson and Charniak (2014) take a data-driven approach to SLA investigations much in the spirit of this work. One particular approach to finding aspects of texts characteristic of their L1s that has motivated the present work is described in Yannakoudakis et al. (2012), the goal of which is to develop visualisation tools for SLA researchers. They present graphs of the relationships between errors and their contexts, such that SLA researchers can navigate through the graphs to find contexts for particular errors that can lead to hypotheses like that of Di´ez-Bedmar and Papp (2008) above. In this paper, we look at approa"
W14-3708,W13-1706,0,0.581249,"arried out on relatively small datasets, and use fairly elementary tools. Sources such as Ellis (2008) and Ortega (2009) give good overviews of such studies and of SLA research in general. A goal of this paper is to investigate a particular way in which Natural Language Processing (NLP) can usefully contribute to SLA. In terms of existing work, the subfield of Native Language Identification (NLI) has been quite active recently, which looks at predicting the L1 of writers writing in a common L2 within a classification task framework; see for example the recent NLI shared task with 29 entrants (Tetreault et al., 2013).1 From within linguistics, there has been much interest in how data-driven approaches can contribute to SLA. Granger (2011) discusses a body of work based on the the methodology of carrying out corpus-based approaches to SLA with a focus on NLP tools; Jarvis and Crossley (2012) in an edited collection present recent work by linguists who extend the corpus-based setup by using a text classification approach, looking at what feature selection might say for SLA. From within NLP, Swanson and Charniak (2013) and Swanson and Charniak (2014) take a data-driven approach to SLA investigations much in"
W14-3708,D11-1148,1,0.83211,"nce to other tasks has varied. Below we discuss works which directly look at how features might be related to language-learning tasks or SLA research. The seminal work of Koppel et al. (2005) that presented NLI as a classification task included, in addition to standard lexical and PoS n-gram features, errors made by the writers; these errors were automatically identified using Microsoft Word grammar checker. Kochmar (2011) used the FCE corpus for NLI, including the manually annotated errors as features, and presented an analysis of usefulness of features (including errors) with respect to L1. Wong and Dras (2011) used syntactic features on the basis of SLA theory that posits that L1 constructions may be reflected in some form of characteristic errors or patterns in L2 constructions to some extent, or through overuse or avoidance of particular constructions in L2 (Lado, 1957; Ellis, 2008); they did note distributional differences of features related to L1. Wong et al. (2012) induced topic models over function words and PoS ngrams, where some of the topics appeared to reflect L1specific characteristics. These works, while interested in the nature of the features, do not evaluate them except via classifi"
W14-3708,D12-1064,1,0.839857,"ied using Microsoft Word grammar checker. Kochmar (2011) used the FCE corpus for NLI, including the manually annotated errors as features, and presented an analysis of usefulness of features (including errors) with respect to L1. Wong and Dras (2011) used syntactic features on the basis of SLA theory that posits that L1 constructions may be reflected in some form of characteristic errors or patterns in L2 constructions to some extent, or through overuse or avoidance of particular constructions in L2 (Lado, 1957; Ellis, 2008); they did note distributional differences of features related to L1. Wong et al. (2012) induced topic models over function words and PoS ngrams, where some of the topics appeared to reflect L1specific characteristics. These works, while interested in the nature of the features, do not evaluate them except via classification accuracy. Swanson and Charniak (2012) similarly explore using syntax, where they propose a richer representation 13 We assume that the multiple examples come from the larger CLC corpus. 62 for L1-specific constructions through Tree Substitution Grammar (TSG). Swanson and Charniak (2013) subsequently examine both relevancy and redundancy of features through a"
W14-3708,W14-3625,1,0.847854,"and redundancy of features through a number of metrics (including the χ2 -statistic used in this paper). They then extend a Bayesian induction model for TSG inference based on a supervised mixture of hierarchical grammars, in order to extract a filtered set of more linguistically informed features that could benefit both NLI and SLA research; an aim was to find relatively rare features that are nevertheless useful for L1 prediction. Swanson and Charniak (2014) continue on from this with a data-driven approach to inferring possible relationships between L1 and L2 structures, again using TSGs. Malmasi and Dras (2014c) also propose a method for identifying potential language transfer effects by using additional linguistic features such as adaptor grammars and grammatical dependencies to analyse differences in learner language. This body of work thus shares some similarities with the present paper, but our focus is on errors rather than on the distributional differences, and we look at error contexts that may not constitute a TSG tree or grammatical dependency. Coming from a linguistic perspective, the works in Jarvis and Crossley (2012) use Linear Discriminant Analysis for classification of texts by L1, a"
W14-3708,P11-1019,0,0.0125675,"ing Hypotheses: A Visualisation Tool The context of the Yannakoudakis et al. (2012) work is automated grading of English as a Second or Other Language (ESOL) exam scripts, as described in Briscoe et al. (2010). The automated grading takes a classification approach, using a binary discriminative learner, with useful features including lexical and part-of-speech (PoS) n-grams. The publicly available dataset on which the work was carried out consists of texts from the First Certificate in English (FCE) exam, aimed at upper-intermediate students of English across various L1s, and was presented in Yannakoudakis et al. (2011). This FCE corpus2 consists of a subset of 1244 texts of the Cambridge Learner Corpus,3 and is manually annotated with errors and their corrections, as well as a classification according to an error typology, as in Figure 1. Yannakoudakis et al. (2012) present their English Profile (EP) visualiser as a way to “visually analyse as well as perform a linguistic interpretation of discriminative features that characterise learner English”, using the features of this essay classification task. They define a measure of co-occurrence of features, among themselves and with errors, as a core part of the"
W14-3708,E14-4019,1,0.812532,"and redundancy of features through a number of metrics (including the χ2 -statistic used in this paper). They then extend a Bayesian induction model for TSG inference based on a supervised mixture of hierarchical grammars, in order to extract a filtered set of more linguistically informed features that could benefit both NLI and SLA research; an aim was to find relatively rare features that are nevertheless useful for L1 prediction. Swanson and Charniak (2014) continue on from this with a data-driven approach to inferring possible relationships between L1 and L2 structures, again using TSGs. Malmasi and Dras (2014c) also propose a method for identifying potential language transfer effects by using additional linguistic features such as adaptor grammars and grammatical dependencies to analyse differences in learner language. This body of work thus shares some similarities with the present paper, but our focus is on errors rather than on the distributional differences, and we look at error contexts that may not constitute a TSG tree or grammatical dependency. Coming from a linguistic perspective, the works in Jarvis and Crossley (2012) use Linear Discriminant Analysis for classification of texts by L1, a"
W14-3708,W12-0206,0,0.0613498,"n the the methodology of carrying out corpus-based approaches to SLA with a focus on NLP tools; Jarvis and Crossley (2012) in an edited collection present recent work by linguists who extend the corpus-based setup by using a text classification approach, looking at what feature selection might say for SLA. From within NLP, Swanson and Charniak (2013) and Swanson and Charniak (2014) take a data-driven approach to SLA investigations much in the spirit of this work. One particular approach to finding aspects of texts characteristic of their L1s that has motivated the present work is described in Yannakoudakis et al. (2012), the goal of which is to develop visualisation tools for SLA researchers. They present graphs of the relationships between errors and their contexts, such that SLA researchers can navigate through the graphs to find contexts for particular errors that can lead to hypotheses like that of Di´ez-Bedmar and Papp (2008) above. In this paper, we look at approaches to finding such hypothesis candidates automatically in the context of L1–L2 interaction by analysing the graphs used in the visualisations One research goal in Second Language Acquisition (SLA) is to formulate and test hypotheses about er"
W14-3708,D14-1144,1,0.814348,"and redundancy of features through a number of metrics (including the χ2 -statistic used in this paper). They then extend a Bayesian induction model for TSG inference based on a supervised mixture of hierarchical grammars, in order to extract a filtered set of more linguistically informed features that could benefit both NLI and SLA research; an aim was to find relatively rare features that are nevertheless useful for L1 prediction. Swanson and Charniak (2014) continue on from this with a data-driven approach to inferring possible relationships between L1 and L2 structures, again using TSGs. Malmasi and Dras (2014c) also propose a method for identifying potential language transfer effects by using additional linguistic features such as adaptor grammars and grammatical dependencies to analyse differences in learner language. This body of work thus shares some similarities with the present paper, but our focus is on errors rather than on the distributional differences, and we look at error contexts that may not constitute a TSG tree or grammatical dependency. Coming from a linguistic perspective, the works in Jarvis and Crossley (2012) use Linear Discriminant Analysis for classification of texts by L1, a"
W14-3708,W13-1716,1,0.779168,"ly find error context such that they behave differently with respect to the L1s according to the ANOVA F-statistic, but produces false positives. Overall, a recurring issue illustrated for all models by the examples is the proposal of error context far away from any likely relevance to SLA. 5 Related Work While Native Language Identification (NLI) as a subfield of NLP has seen much new work in the last few years — the papers from the shared task (Tetreault et al., 2013) provide a recent sample — the emphasis on optimising classification task results, for example by using classifier ensembles (Malmasi et al., 2013), versus analysing features for relevance to other tasks has varied. Below we discuss works which directly look at how features might be related to language-learning tasks or SLA research. The seminal work of Koppel et al. (2005) that presented NLI as a classification task included, in addition to standard lexical and PoS n-gram features, errors made by the writers; these errors were automatically identified using Microsoft Word grammar checker. Kochmar (2011) used the FCE corpus for NLI, including the manually annotated errors as features, and presented an analysis of usefulness of features ("
W14-3708,P12-2038,0,0.0156654,"eatures on the basis of SLA theory that posits that L1 constructions may be reflected in some form of characteristic errors or patterns in L2 constructions to some extent, or through overuse or avoidance of particular constructions in L2 (Lado, 1957; Ellis, 2008); they did note distributional differences of features related to L1. Wong et al. (2012) induced topic models over function words and PoS ngrams, where some of the topics appeared to reflect L1specific characteristics. These works, while interested in the nature of the features, do not evaluate them except via classification accuracy. Swanson and Charniak (2012) similarly explore using syntax, where they propose a richer representation 13 We assume that the multiple examples come from the larger CLC corpus. 62 for L1-specific constructions through Tree Substitution Grammar (TSG). Swanson and Charniak (2013) subsequently examine both relevancy and redundancy of features through a number of metrics (including the χ2 -statistic used in this paper). They then extend a Bayesian induction model for TSG inference based on a supervised mixture of hierarchical grammars, in order to extract a filtered set of more linguistically informed features that could ben"
W14-3708,N13-1009,0,0.155119,"a classification task framework; see for example the recent NLI shared task with 29 entrants (Tetreault et al., 2013).1 From within linguistics, there has been much interest in how data-driven approaches can contribute to SLA. Granger (2011) discusses a body of work based on the the methodology of carrying out corpus-based approaches to SLA with a focus on NLP tools; Jarvis and Crossley (2012) in an edited collection present recent work by linguists who extend the corpus-based setup by using a text classification approach, looking at what feature selection might say for SLA. From within NLP, Swanson and Charniak (2013) and Swanson and Charniak (2014) take a data-driven approach to SLA investigations much in the spirit of this work. One particular approach to finding aspects of texts characteristic of their L1s that has motivated the present work is described in Yannakoudakis et al. (2012), the goal of which is to develop visualisation tools for SLA researchers. They present graphs of the relationships between errors and their contexts, such that SLA researchers can navigate through the graphs to find contexts for particular errors that can lead to hypotheses like that of Di´ez-Bedmar and Papp (2008) above."
W14-4606,W08-1301,0,0.168621,"Missing"
W14-4606,lynn-etal-2012-irish,1,0.795987,"Missing"
W14-4606,U12-1005,1,0.805413,"languages from four language family groups to assess which languages are the most useful for cross-lingual parsing of Irish by using these treebanks to train delexicalised parsing models which are then applied to sentences from the Irish Dependency Treebank. The best results are achieved when using Indonesian, a language from the Austronesian language family. 1 Introduction Considerable efforts have been made over the past decade to develop natural language processing resources for the Irish language (U´ı Dhonnchadha et al., 2003; U´ı Dhonnchadha and van Genabith, 2006; U´ı Dhonnchadha, 2009; Lynn et al., 2012a; Lynn et al., 2012b; Lynn et al., 2013). One such resource is the Irish Dependency Treebank (Lynn et al., 2012a) which contains just over 1000 gold standard dependency parse trees. These trees are labelled with deep syntactic information, marking grammatical roles such as subject, object, modifier, and coordinator. While a valuable resource, the treebank does not compare in size to similar resources of other languages.1 The small size of the treebank affects the accuracy of any statistical parsing models learned from this treebank. Therefore, we would like to investigate whether training dat"
W14-4606,W13-4901,1,0.939414,"ps to assess which languages are the most useful for cross-lingual parsing of Irish by using these treebanks to train delexicalised parsing models which are then applied to sentences from the Irish Dependency Treebank. The best results are achieved when using Indonesian, a language from the Austronesian language family. 1 Introduction Considerable efforts have been made over the past decade to develop natural language processing resources for the Irish language (U´ı Dhonnchadha et al., 2003; U´ı Dhonnchadha and van Genabith, 2006; U´ı Dhonnchadha, 2009; Lynn et al., 2012a; Lynn et al., 2012b; Lynn et al., 2013). One such resource is the Irish Dependency Treebank (Lynn et al., 2012a) which contains just over 1000 gold standard dependency parse trees. These trees are labelled with deep syntactic information, marking grammatical roles such as subject, object, modifier, and coordinator. While a valuable resource, the treebank does not compare in size to similar resources of other languages.1 The small size of the treebank affects the accuracy of any statistical parsing models learned from this treebank. Therefore, we would like to investigate whether training data from other languages can be successfull"
W14-4606,J93-2004,0,0.0451671,", we select the first 4447 trees from each treebank – to match the number of trees in the smallest data set (Swedish). We delexicalise all treebanks and use the universal POS tags as both the coarse- and fine-grained values.5 We train a parser on all 10 source data sets outlined and use each induced parsing model to parse and test on a delexicalised version of the Universal Irish Dependency Treebank. Largest transfer source training data - Universal English Dependency Treebank English has the largest source training data set (sections 2-21 of the Wall Street Journal data in the Penn Treebank (Marcus et al., 1993) contains 39, 832 trees). As with the smaller transfer datasets, we delexicalise this dataset and use the universal POS tag values only. We experiment with this larger training set in order to establish whether more training data helps in a cross-lingual setting. 4 Version 2 data sets downloaded from https://code.google.com/p/uni-dep-tb/ Note that the downloaded treebanks had some fine-grained POS tags that were not used across all languages: e.g. VERBVPRT (Spanish), CD (English). 5 46 Parser and Evaluation Metrics We use a transition-based dependency parsing system, MaltParser (Nivre et al.,"
W14-4606,D11-1006,0,0.12918,"ees are labelled with deep syntactic information, marking grammatical roles such as subject, object, modifier, and coordinator. While a valuable resource, the treebank does not compare in size to similar resources of other languages.1 The small size of the treebank affects the accuracy of any statistical parsing models learned from this treebank. Therefore, we would like to investigate whether training data from other languages can be successfully utilised to improve Irish parsing. Cross-lingual transfer parsing involves training a parser on one language, and parsing data of another language. McDonald et al. (2011) describe two types of cross-lingual parsing, direct transfer parsing in which a delexicalised version of the source language treebank is used to train a parsing model which is then used to parse the target language, and a more complicated projected transfer approach in which the direct transfer approach is used to seed a parsing model which is then trained to obey source-target constraints learned from a parallel corpus. These experiments revealed that languages that were typologically similar were not necessarily the best source-target pairs, sometimes due to variations between their languag"
W14-4606,nivre-etal-2006-maltparser,0,0.0527279,"ological features such as inflected prepositions and initial mutations, for example. Compared to other EU-official languages, Irish language technology is under-resourced, as highlighted by a recent study (Judge et al., 2012). In the area of morpho-syntactic processing, recent years have seen the development of a part-of-speech tagger (U´ı Dhonnchadha and van Genabith, 2006), a morphological analyser (U´ı Dhonnchadha et al., 2003), a shallow chunker (U´ı Dhonnchadha, 2009), a dependency treebank (Lynn et al., 2012a; Lynn et al., 2012b) and statistical dependency parsing models for MaltParser (Nivre et al., 2006) and Mate parser (Bohnet, 2010) trained on this treebank (Lynn et al., 2013). The annotation scheme for the Irish Dependency Treebank (Lynn et al., 2012b) was inspired by Lexical Functional Grammar (Bresnan, 2001) and has its roots in the dependency annotation scheme described by C¸etino˘glu et al. (2010). It was extended and adapted to suit the linguistic characterisics of the Irish language. The final label set consists of 47 dependency labels, defining grammatical and functional relations between the words in a sentence. The label set is hierarchical in nature with labels such as vparticle"
W14-4606,petrov-etal-2012-universal,0,0.482334,"urce-target constraints learned from a parallel corpus. These experiments revealed that languages that were typologically similar were not necessarily the best source-target pairs, sometimes due to variations between their language-specific annotation schemes. In more recent work, however, McDonald et al. (2013) reported improved results on cross-lingual direct transfer parsing using a universal annotation scheme, to which six chosen treebanks are mapped for uniformity purposes. Underlying the experiments with this new annotation scheme is the universal part-of-speech (POS) tagset designed by Petrov et al. (2012). While their results confirm that parsers trained on data from languages in the same language group (e.g. Romance and Germanic) show the most accurate results, they also show that training data taken across language-groups also produces promising results. We attempt to apply the direct transfer approach with Irish as the target language. The Irish language belongs to the Celtic branch of the Indo-European language family. The natural first step in cross-lingual parsing for Irish would be to look to those languages of the Celtic language This work is licensed under a Creative Commons Attributi"
W14-4606,ui-dhonnchadha-van-genabith-2006-part,0,0.082773,"Missing"
W14-4606,C10-1011,0,\N,Missing
W14-4606,P13-2017,0,\N,Missing
W15-0620,C14-1185,0,0.408647,"Missing"
W15-0620,W13-1726,0,0.0970702,"Missing"
W15-0620,P05-1022,0,0.225282,"Missing"
W15-0620,W13-1727,0,0.253176,"Missing"
W15-0620,W13-1712,0,0.0938667,"Missing"
W15-0620,D14-1142,0,0.246652,"Missing"
W15-0620,W13-1714,0,0.336048,"Missing"
W15-0620,W15-0606,1,0.830691,"Missing"
W15-0620,D14-1144,1,0.807671,"Missing"
W15-0620,W13-1716,1,0.797299,"Missing"
W15-0620,D08-1027,0,0.0773695,"Missing"
W15-0620,C12-1158,1,0.820422,"Missing"
W15-0620,W13-1706,1,0.864163,"Missing"
W15-5407,W15-0606,1,0.885187,"Missing"
W15-5407,N15-1160,1,0.920145,"ning (1994). Automatic LID methods have since been widely used in NLP. Although LID can be extremely accurate in distinguishing languages that use distinct character sets (e.g. Chinese or Japanese) or are very dissimilar (e.g. Spanish and Swedish), performance is degraded when it is used for discriminating similar languages or dialects. This has led to researchers turning their attention to the sub-problem of discriminating between closely-related languages and varieties. This issue has been researched in the context of confusable languages, including MalayIndonesian (Bali, 2006), Farsi-Dari (Malmasi and Dras, 2015a), Croatian-Slovene-Serbian (Ljubesic et al., 2007), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013), and Chinese varieties (Huang and Lee, 2008). The task of Arabic Dialect Identification has also drawn attention in the Arabic NLP community (Malmasi et al., 2015a). This issue was also the focus of the first “Discriminating Similar Language” (DSL) shared task1 in 2014. The shared task used data from 13 different languages and varieties divided into 6 sub-groups and teams needed to build systems for distinguishing these classes. They were provided wit"
W15-5407,Y08-1042,0,0.370058,"Missing"
W15-5407,W13-1716,1,0.836051,",000 18,000 18,000 252,000 Dev 2,000 2,000 2,000 2,000 2,000 2,000 2,000 2,000 2,000 2,000 2,000 2,000 2,000 2,000 28,000 Test 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 14,000 Table 1: The languages included in the corpus and the number of sentences in each set. 3 In particular, we use the LIBLINEAR2 package (Fan et al., 2008) which has been shown to be efficient for text classification problems such as this. For example, it has been demonstrated to be a very effective classifier for the task of Native Language Identification (Malmasi and Dras, 2015b; Malmasi et al., 2013) which also relies on text classification methods. Data The data for the shared task comes from the DSL Corpus Collection (Tan et al., 2014). The task is performed at the sentence-level and the corpus consists of 294,000 sentences distributed evenly between 14 language classes. The corpus is subdivided into training, development and test sets. The languages and the number of sentences in each set are listed in Table 1. An interesting addition to this year’s data is the inclusion of an “other” class which contains data from various additional languages. The motivation here is to emulate a reali"
W15-5407,W15-0620,1,0.928553,"imilar languages or dialects. This has led to researchers turning their attention to the sub-problem of discriminating between closely-related languages and varieties. This issue has been researched in the context of confusable languages, including MalayIndonesian (Bali, 2006), Farsi-Dari (Malmasi and Dras, 2015a), Croatian-Slovene-Serbian (Ljubesic et al., 2007), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013), and Chinese varieties (Huang and Lee, 2008). The task of Arabic Dialect Identification has also drawn attention in the Arabic NLP community (Malmasi et al., 2015a). This issue was also the focus of the first “Discriminating Similar Language” (DSL) shared task1 in 2014. The shared task used data from 13 different languages and varieties divided into 6 sub-groups and teams needed to build systems for distinguishing these classes. They were provided with a training and development dataset comprised of 20,000 sentences from each language and an unlabelled test set of 1,000 sentences per language was used for evaluation. Most entries used surface features and many applied hierarchical classifiers, taking advantage of the structure provided by the language"
W15-5407,C12-1160,0,0.121099,"Missing"
W15-5407,W15-5401,0,0.0774697,"Missing"
W16-0314,W14-3207,0,0.0144592,"native language (Gebre et al., 2013; Malmasi and Dras, 2015a), age and gender (Nguyen et al., 2013), and even economic conditions such as income (Preot¸iuc-Pietro et al., 2015). These tasks are often considered to be a part of a broader natural language processing task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples of health and psychological conditions studied using texts and social media are: suicide risk (Thompson et al., 2014), depression (Schwartz et al., 2014), autism (Tanaka et al., 2014; Rouhizadeh et al., 2015), and schizophrenia (Mitchell et al., 2015). In this paper we propose an approach to predict the severity of posts in a mental health online forum. Posts were classified into for levels of severity (or urgency) represented by the labels green, amber, red, and crisis according to indication of risky or harmful behavior by users (e.g. self-harm, suicide, etc.). This kind of classi"
W16-0314,W13-1728,1,0.766386,"individual classifiers as well as an ensemble classifier. This meta-classifier was then extended to a Random Forest of meta-classifiers, yielding further improvements in classification accuracy. We achieved competitive results, ranking first among a total of 60 submitted entries in the competition. 1 Introduction Computational methods have been widely used to extract and/or predict a number of phenomena in text documents. It has been shown that algorithms are able to learn a wide range of information about the authors of texts as well. This includes, for example, the author’s native language (Gebre et al., 2013; Malmasi and Dras, 2015a), age and gender (Nguyen et al., 2013), and even economic conditions such as income (Preot¸iuc-Pietro et al., 2015). These tasks are often considered to be a part of a broader natural language processing task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples"
W16-0314,guthrie-etal-2006-closer,0,0.0183918,"prioritize addressing this post. • Amber: a moderator needs to look at this and assess if there are enough responses and support from others or if they should reply. • Red: a moderator needs to look at this as soon as possible and take action. • Crisis: the author (or someone they know) might hurt themselves or others (a red instance that is of urgent importance). Participating systems should be trained to predict these labels, with evaluation on the test set. • Word skip-grams: To capture the longer distance dependencies not covered by word ngrams we also used word skip-grams as described in Guthrie et al. (2006). We extract 1, 2 and 3-skip word bigrams. • Lemma n-grams: we used a lemmatized version of the texts and extract lemma n-grams of order 1–3. • Word Representations: To increase the generalizability of our models we used word representation features based on Brown clustering as a form of semi-supervised learning. This was done using the method described by Malmasi et al. (2015a). We used the clusters generated by Owoputi et al. (2013). They collected From 56 million English tweets (837 million tokens) and used it to generate 1,000 hierarchical clusters over 217 thousand words. 3.3 Feature Extr"
W16-0314,E14-4019,1,0.894839,"Missing"
W16-0314,W15-5407,1,0.746304,"rs as well as an ensemble classifier. This meta-classifier was then extended to a Random Forest of meta-classifiers, yielding further improvements in classification accuracy. We achieved competitive results, ranking first among a total of 60 submitted entries in the competition. 1 Introduction Computational methods have been widely used to extract and/or predict a number of phenomena in text documents. It has been shown that algorithms are able to learn a wide range of information about the authors of texts as well. This includes, for example, the author’s native language (Gebre et al., 2013; Malmasi and Dras, 2015a), age and gender (Nguyen et al., 2013), and even economic conditions such as income (Preot¸iuc-Pietro et al., 2015). These tasks are often considered to be a part of a broader natural language processing task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples of health and psycholog"
W16-0314,U15-1008,1,0.837443,"systems should be trained to predict these labels, with evaluation on the test set. • Word skip-grams: To capture the longer distance dependencies not covered by word ngrams we also used word skip-grams as described in Guthrie et al. (2006). We extract 1, 2 and 3-skip word bigrams. • Lemma n-grams: we used a lemmatized version of the texts and extract lemma n-grams of order 1–3. • Word Representations: To increase the generalizability of our models we used word representation features based on Brown clustering as a form of semi-supervised learning. This was done using the method described by Malmasi et al. (2015a). We used the clusters generated by Owoputi et al. (2013). They collected From 56 million English tweets (837 million tokens) and used it to generate 1,000 hierarchical clusters over 217 thousand words. 3.3 Feature Extraction We used three categories of features: lexical, syntactic, and metadata features. These features and our preprocessing method are outlined here. 3.1 Lexical Features • Character n-grams: we extracted n-grams of order 2–8. Table 1: CLPsych Corpus Divided by Data Set 3 3.2 Preprocessing The following preprocessing was performed on the texts: HTML removal was performed, wit"
W16-0314,S16-1154,1,0.836465,"asi and Dras, 2015b). Run Run 1 Run 2 Run 3 Run 4 Run 5 Official Accuracy F-score Accuracy Rank Score (NG vs. G) (NG vs. G) 0.37 0.80 0.83 0.89 11th 0.38 0.80 0.83 0.89 9th 0.42 0.83 0.87 0.91 1st 0.42 0.84 0.87 0.91 1st 0.40 0.82 0.85 0.90 6th Table 2: Official CLPsych scores. Best results in bold. Rankings are out of the 60 systems submitted. Classifiers ensembles have proven to be an efficient and robust alternative in other text classification tasks such as language identification (Malmasi and Dras, 2015a), grammatical error detection (Xiang et al., 2015), and complex word identification (Malmasi et al., 2016). 4.2 Meta-classifier For our meta-classifier, We experimented with three algorithms: Random Forests of decision trees, a linear SVM just like our base classifiers and a Radial Basis Function (RBF) kernel SVM. The inputs to the meta-classifier are the continuous outputs from each base SVM classifier in our ensemble, along with the original gold label. For the Random Forest classifiers, the final label is selected through a plurality voting process across all decision trees in the forest. All were found to perform well, but the linear SVM was was outperformed by its RBF-kernel counterpart. This"
W16-0314,W15-1202,0,0.0122244,"task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples of health and psychological conditions studied using texts and social media are: suicide risk (Thompson et al., 2014), depression (Schwartz et al., 2014), autism (Tanaka et al., 2014; Rouhizadeh et al., 2015), and schizophrenia (Mitchell et al., 2015). In this paper we propose an approach to predict the severity of posts in a mental health online forum. Posts were classified into for levels of severity (or urgency) represented by the labels green, amber, red, and crisis according to indication of risky or harmful behavior by users (e.g. self-harm, suicide, etc.). This kind of classification task serves to provide automatic triage of user posts in order to help moderators of forums and related online communities to respond to urgent posts. Our approach competed in the CLPsych 2016 shared task and achieved the highest accuracy among submitte"
W16-0314,N13-1039,0,0.0716559,"Missing"
W16-0314,W15-1214,0,0.0518374,"Missing"
W16-0314,W14-3214,0,0.158613,"Missing"
W16-0314,W14-3211,0,0.0226416,"considered to be a part of a broader natural language processing task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples of health and psychological conditions studied using texts and social media are: suicide risk (Thompson et al., 2014), depression (Schwartz et al., 2014), autism (Tanaka et al., 2014; Rouhizadeh et al., 2015), and schizophrenia (Mitchell et al., 2015). In this paper we propose an approach to predict the severity of posts in a mental health online forum. Posts were classified into for levels of severity (or urgency) represented by the labels green, amber, red, and crisis according to indication of risky or harmful behavior by users (e.g. self-harm, suicide, etc.). This kind of classification task serves to provide automatic triage of user posts in order to help moderators of forums and related online communities to respond to urgent posts. Our approach competed in the CLPs"
W16-0314,W14-3201,0,0.146759,"uch as income (Preot¸iuc-Pietro et al., 2015). These tasks are often considered to be a part of a broader natural language processing task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples of health and psychological conditions studied using texts and social media are: suicide risk (Thompson et al., 2014), depression (Schwartz et al., 2014), autism (Tanaka et al., 2014; Rouhizadeh et al., 2015), and schizophrenia (Mitchell et al., 2015). In this paper we propose an approach to predict the severity of posts in a mental health online forum. Posts were classified into for levels of severity (or urgency) represented by the labels green, amber, red, and crisis according to indication of risky or harmful behavior by users (e.g. self-harm, suicide, etc.). This kind of classification task serves to provide automatic triage of user posts in order to help moderators of forums and related online communit"
W16-0314,W15-4415,0,0.0452785,"s (Malmasi and Dras, 2014; Malmasi et al., 2015b; Malmasi and Dras, 2015b). Run Run 1 Run 2 Run 3 Run 4 Run 5 Official Accuracy F-score Accuracy Rank Score (NG vs. G) (NG vs. G) 0.37 0.80 0.83 0.89 11th 0.38 0.80 0.83 0.89 9th 0.42 0.83 0.87 0.91 1st 0.42 0.84 0.87 0.91 1st 0.40 0.82 0.85 0.90 6th Table 2: Official CLPsych scores. Best results in bold. Rankings are out of the 60 systems submitted. Classifiers ensembles have proven to be an efficient and robust alternative in other text classification tasks such as language identification (Malmasi and Dras, 2015a), grammatical error detection (Xiang et al., 2015), and complex word identification (Malmasi et al., 2016). 4.2 Meta-classifier For our meta-classifier, We experimented with three algorithms: Random Forests of decision trees, a linear SVM just like our base classifiers and a Radial Basis Function (RBF) kernel SVM. The inputs to the meta-classifier are the continuous outputs from each base SVM classifier in our ensemble, along with the original gold label. For the Random Forest classifiers, the final label is selected through a plurality voting process across all decision trees in the forest. All were found to perform well, but the linear SVM"
