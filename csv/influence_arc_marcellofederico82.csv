2004.iwslt-evaluation.1,2004.iwslt-evaluation.5,0,0.0434217,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.6,0,0.0792214,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.8,1,0.857983,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.11,0,0.00856795,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.13,0,0.00858588,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.14,0,0.0294958,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.7,0,0.0434735,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.10,0,0.0223145,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.12,0,0.0295568,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.15,0,0.0273582,"Missing"
2004.iwslt-evaluation.1,niessen-etal-2000-evaluation,0,\N,Missing
2004.iwslt-evaluation.1,2004.iwslt-evaluation.2,1,\N,Missing
2004.iwslt-evaluation.1,P02-1040,0,\N,Missing
2004.iwslt-evaluation.1,W05-0909,0,\N,Missing
2004.iwslt-evaluation.1,2005.iwslt-1.1,0,\N,Missing
2004.iwslt-evaluation.1,2005.iwslt-1.6,0,\N,Missing
2004.iwslt-evaluation.1,2004.iwslt-evaluation.4,0,\N,Missing
2004.iwslt-evaluation.1,zhang-etal-2004-interpreting,0,\N,Missing
2004.iwslt-evaluation.1,P03-1021,0,\N,Missing
2004.iwslt-evaluation.1,2004.iwslt-evaluation.9,0,\N,Missing
2004.iwslt-evaluation.8,P00-1056,0,0.237361,"ineseEnglish track. 2.1. Log-linear Model As originally proposed by [8], the most likely translation of a foreign source sentence f into English is obtained by searching for the sentence with the highest posterior probability: The ITC-irst SMT system implements an extension of the IBM Model 4 as a log-linear interpolation of statistical models, which estimate probabilities in terms of phrases. The use of phrases rather than words has recently emerged as a mean to cope with the limited context that Model 4 exploits to guess word translation (lexicon model) and word positions (distortion model) [1, 2, 3, 4, 5, 6, 7]. e∗ = arg max Pr(e |f ) e (1) Usually, the hidden variable a is introduced: While parameters of the models are estimated exploiting statistics of phrase pairs extracted from word alignments, the weights of the interpolation are optimized through a training procedure which directly aims at minimizing translation errors on a development set. e∗ = arg max e X Pr(e, a |f ) (2) a which represents an alignment from source to target positions. The framework of maximum entropy [9] provides a mean to directly estimate the posterior probability Pr(e, a |f ). It is determined through suitable real value"
2004.iwslt-evaluation.8,J03-1002,0,0.0116263,"own that if the process succeeds in generating a triple (φl0 , τ0l , π0l ), then there is exactly one corresponding pair (f , a), and viceversa. This property justifies the following decomposition of Model 4: pθ (f , a |e) = p(φl0 , τ0l , π0l |el0 ) = p(φ, τ , π |e) p(τi |φi , ei ) k=1 a a φi ) (10) i=1 with where λi ’s represent scaling factors of factors. In eq. (5), English strings e are ranked on the basis of the weighted product of the language model probability Pr(e), usually computed through an n-gram language model [13], and the marginal of the translation probability Pr(f , a |e). In [8, 14] six translation models (Model 1 to 6) of increasing complexity are introduced. These alignment models are usually estimated through the Expectation Maximization algorithm [15], or approximations of it, by exploiting a suitable parallel corpus of translation pairs. For computational reasons, the optimal translation of f is computed with the approximated search criterion: e l Y l X i=0 exploiting eq. (3), eq. (2) can be rewritten as: e∗ p(φi |ei ) p(φ0 | i=1 Unfortunately, a closed-form solution of (4) does not exist. An iterative procedure converging to the solution was proposed by [10]; an im"
2004.iwslt-evaluation.8,P01-1067,0,0.114159,"ineseEnglish track. 2.1. Log-linear Model As originally proposed by [8], the most likely translation of a foreign source sentence f into English is obtained by searching for the sentence with the highest posterior probability: The ITC-irst SMT system implements an extension of the IBM Model 4 as a log-linear interpolation of statistical models, which estimate probabilities in terms of phrases. The use of phrases rather than words has recently emerged as a mean to cope with the limited context that Model 4 exploits to guess word translation (lexicon model) and word positions (distortion model) [1, 2, 3, 4, 5, 6, 7]. e∗ = arg max Pr(e |f ) e (1) Usually, the hidden variable a is introduced: While parameters of the models are estimated exploiting statistics of phrase pairs extracted from word alignments, the weights of the interpolation are optimized through a training procedure which directly aims at minimizing translation errors on a development set. e∗ = arg max e X Pr(e, a |f ) (2) a which represents an alignment from source to target positions. The framework of maximum entropy [9] provides a mean to directly estimate the posterior probability Pr(e, a |f ). It is determined through suitable real value"
2004.iwslt-evaluation.8,W02-1018,0,0.0226266,"ineseEnglish track. 2.1. Log-linear Model As originally proposed by [8], the most likely translation of a foreign source sentence f into English is obtained by searching for the sentence with the highest posterior probability: The ITC-irst SMT system implements an extension of the IBM Model 4 as a log-linear interpolation of statistical models, which estimate probabilities in terms of phrases. The use of phrases rather than words has recently emerged as a mean to cope with the limited context that Model 4 exploits to guess word translation (lexicon model) and word positions (distortion model) [1, 2, 3, 4, 5, 6, 7]. e∗ = arg max Pr(e |f ) e (1) Usually, the hidden variable a is introduced: While parameters of the models are estimated exploiting statistics of phrase pairs extracted from word alignments, the weights of the interpolation are optimized through a training procedure which directly aims at minimizing translation errors on a development set. e∗ = arg max e X Pr(e, a |f ) (2) a which represents an alignment from source to target positions. The framework of maximum entropy [9] provides a mean to directly estimate the posterior probability Pr(e, a |f ). It is determined through suitable real value"
2004.iwslt-evaluation.8,2003.mtsummit-papers.53,0,0.0486164,"ineseEnglish track. 2.1. Log-linear Model As originally proposed by [8], the most likely translation of a foreign source sentence f into English is obtained by searching for the sentence with the highest posterior probability: The ITC-irst SMT system implements an extension of the IBM Model 4 as a log-linear interpolation of statistical models, which estimate probabilities in terms of phrases. The use of phrases rather than words has recently emerged as a mean to cope with the limited context that Model 4 exploits to guess word translation (lexicon model) and word positions (distortion model) [1, 2, 3, 4, 5, 6, 7]. e∗ = arg max Pr(e |f ) e (1) Usually, the hidden variable a is introduced: While parameters of the models are estimated exploiting statistics of phrase pairs extracted from word alignments, the weights of the interpolation are optimized through a training procedure which directly aims at minimizing translation errors on a development set. e∗ = arg max e X Pr(e, a |f ) (2) a which represents an alignment from source to target positions. The framework of maximum entropy [9] provides a mean to directly estimate the posterior probability Pr(e, a |f ). It is determined through suitable real value"
2004.iwslt-evaluation.8,N03-1017,0,0.0501189,"d by the many papers on the subject, which followed its first introduction. Of course, there have been also attempts to overcome some of its shortcomings, e.g. the use of limited context within the foreign string to guess word translations and word positions. Recently, several research labs have reported improvements in translation accuracy by shifting from word- to phrase-based SMT. In particular, statistical phrasebased translation models have recently emerged, which rely on statistics of phrase pairs. Phrase pairs statistics can be automatically extracted from word-aligned parallel corpora [5]. In the following subsections, we introduce the SMT framework and the Model 4. Then, we briefly describe a method for extracting phrase pairs. Finally, a novel phrase-based translation framework is presented which is tightly related to Model 4. Focus of this paper is the system for statistical machine translation developed at ITC-irst. It has been employed in the evaluation campaign of the International Workshop on Spoken Language Translation 2004 in all the three data set conditions of the Chinese-English track. Both the statistical model underlying the system and the system architecture are"
2004.iwslt-evaluation.8,2004.iwslt-papers.2,1,0.890764,"by means of the GIZA++ toolkit [1]. Phrase pairs are then extracted taking into account both direct and inverse alignments (see section 2.3), and the phrase-based distributions are estimated (section 2.4). In the second phase the scaling factors of the log-linear model are estimated by the so-called minimum error training procedure. This iterative method searches for a set of factors that minimizes a given error measure on a development corpus. The simplex method [17] is used to explore the space of scaling factors. A detailed description of the minimum error training approach is reported in [18]. BLEU NIST MWER MPER 0.3001 0.3509 0.3466 0.3460 0.4311 0.4574 7.0157 7.5099 7.4475 7.4427 8.5336 8.7890 50.8 47.2 47.6 47.1 42.0 39.7 41.5 38.1 38.3 38.3 33.3 30.5 baseline system, provided these data are close enough to the domain of the test set. However, an exhaustive exploration of corpora available for the IWSLT evaluation for finding the best combination for training the system is unfeasible. Hence, first we searched for the best monolingual resources consisting of the English part of parallel corpora. Successively, we tried the effectiveness of additional bilingual resources. Note tha"
2004.iwslt-evaluation.8,W03-1001,0,0.125553,"ineseEnglish track. 2.1. Log-linear Model As originally proposed by [8], the most likely translation of a foreign source sentence f into English is obtained by searching for the sentence with the highest posterior probability: The ITC-irst SMT system implements an extension of the IBM Model 4 as a log-linear interpolation of statistical models, which estimate probabilities in terms of phrases. The use of phrases rather than words has recently emerged as a mean to cope with the limited context that Model 4 exploits to guess word translation (lexicon model) and word positions (distortion model) [1, 2, 3, 4, 5, 6, 7]. e∗ = arg max Pr(e |f ) e (1) Usually, the hidden variable a is introduced: While parameters of the models are estimated exploiting statistics of phrase pairs extracted from word alignments, the weights of the interpolation are optimized through a training procedure which directly aims at minimizing translation errors on a development set. e∗ = arg max e X Pr(e, a |f ) (2) a which represents an alignment from source to target positions. The framework of maximum entropy [9] provides a mean to directly estimate the posterior probability Pr(e, a |f ). It is determined through suitable real value"
2004.iwslt-evaluation.8,takezawa-etal-2002-toward,0,0.115361,"Missing"
2004.iwslt-evaluation.8,P01-1050,0,0.0488447,"ineseEnglish track. 2.1. Log-linear Model As originally proposed by [8], the most likely translation of a foreign source sentence f into English is obtained by searching for the sentence with the highest posterior probability: The ITC-irst SMT system implements an extension of the IBM Model 4 as a log-linear interpolation of statistical models, which estimate probabilities in terms of phrases. The use of phrases rather than words has recently emerged as a mean to cope with the limited context that Model 4 exploits to guess word translation (lexicon model) and word positions (distortion model) [1, 2, 3, 4, 5, 6, 7]. e∗ = arg max Pr(e |f ) e (1) Usually, the hidden variable a is introduced: While parameters of the models are estimated exploiting statistics of phrase pairs extracted from word alignments, the weights of the interpolation are optimized through a training procedure which directly aims at minimizing translation errors on a development set. e∗ = arg max e X Pr(e, a |f ) (2) a which represents an alignment from source to target positions. The framework of maximum entropy [9] provides a mean to directly estimate the posterior probability Pr(e, a |f ). It is determined through suitable real value"
2004.iwslt-evaluation.8,P02-1038,0,0.215643,"These alignment models are usually estimated through the Expectation Maximization algorithm [15], or approximations of it, by exploiting a suitable parallel corpus of translation pairs. For computational reasons, the optimal translation of f is computed with the approximated search criterion: e l Y l X i=0 exploiting eq. (3), eq. (2) can be rewritten as: e∗ p(φi |ei ) p(φ0 | i=1 Unfortunately, a closed-form solution of (4) does not exist. An iterative procedure converging to the solution was proposed by [10]; an improved version is given in [11]. If the following feature functions are chosen [12]: h1 (e, f , a) h2 (e, f , a) l Y 2.3. Phrase-pair Extraction The here used method exploits the so called union alignments between sentence pairs of the training corpus [5]. Given strings f = f1 , . . . , fm and e = e1 , . . . , el , a direct alignment a (from f to e) and an inverted alignment b (from e to f ), the union alignment is defined as: c = {(j, i) : aj = i ∨ bi = j} (15) It is easy to verify that while a and b are many-to-one alignments, c is a many-to-many alignment. Moreover, the union 1 A cept is a target word (including e ) with positive fertility. A not-cept 0 word may only gene"
2004.iwslt-evaluation.8,J93-2003,0,\N,Missing
2004.iwslt-evaluation.8,J96-1002,0,\N,Missing
2004.iwslt-papers.2,P03-1021,0,0.144581,"the addition of four scaling factors: 2. Log-Linear Model for SMT a arg max Pr(e, a |f ) (e,f ,a)∈T An interesting log-linear model results if the following feature functions derived from Model 4 [6] are used: Log-linear interpolation models, which can be formally derived within the maximum entropy framework [1], have been only recently applied to statistical machine translation (SMT) [2]. In addition, and similarly to what proposed for speech recognition [3], optimization of interpolation parameters can directly address translation quality, rather than the usual maximum likelihood criterion [4]. This paper goes along the direction of [4], and proposes an alternative and more direct training procedure, but computationally more intensive. Moreover, a subtle relationship between the parameter optimization and the beam search algorithm is pointed out, which might have an important impact on the choice of optimal parameters. ≈ (2) The maximum entropy criterion suggests to compute values λi , which maximize the log-likelihood over a training sample T : X λ∗ = arg max log pλ (e, a |f ) (3) 1. Introduction e P exp{ i λi hi (e, f , a)} P e,a exp{ i λi hi (e, f , a)} (1) 103 is deployed, too."
2004.iwslt-papers.2,J96-1002,0,0.0776359,"mentally ex˜ ) of the source tends partial translation hypotheses (˜ e, a string, until an optimal complete translation is found. A translation is said partial if its corresponding alignment ˜ does not cover all positions in f . The complexity of a the search algorithm mainly depends on the number of possible translations and of target positions to be considered for each source word. To avoid exponential complexity, constraints on both factors are generally introduced. Moreover, the so-called pruning of hypotheses Given a source string f and a target string e, the framework of maximum entropy [5] provides a mean to directly address the posterior probability Pr(e |f ). By introducing the hidden alignment variable a, the usual SMT optimization criterion is expressed by: X e∗ = arg max Pr(e, a |f ) e,a = log Pr(e) = log Pr(φ |e) = log Pr(τ |e, φ) = log Pr(π |e, φ, τ ), which explain f and a for e in terms of fertilities φ, tablets τ and permutations π. In fact, after simple manipulations, the usual decoding criterion for Model 4 results, with the addition of four scaling factors: 2. Log-Linear Model for SMT a arg max Pr(e, a |f ) (e,f ,a)∈T An interesting log-linear model results if the"
2004.iwslt-papers.2,J93-2003,0,0.0211432,"r probability Pr(e |f ). By introducing the hidden alignment variable a, the usual SMT optimization criterion is expressed by: X e∗ = arg max Pr(e, a |f ) e,a = log Pr(e) = log Pr(φ |e) = log Pr(τ |e, φ) = log Pr(π |e, φ, τ ), which explain f and a for e in terms of fertilities φ, tablets τ and permutations π. In fact, after simple manipulations, the usual decoding criterion for Model 4 results, with the addition of four scaling factors: 2. Log-Linear Model for SMT a arg max Pr(e, a |f ) (e,f ,a)∈T An interesting log-linear model results if the following feature functions derived from Model 4 [6] are used: Log-linear interpolation models, which can be formally derived within the maximum entropy framework [1], have been only recently applied to statistical machine translation (SMT) [2]. In addition, and similarly to what proposed for speech recognition [3], optimization of interpolation parameters can directly address translation quality, rather than the usual maximum likelihood criterion [4]. This paper goes along the direction of [4], and proposes an alternative and more direct training procedure, but computationally more intensive. Moreover, a subtle relationship between the paramet"
2004.iwslt-papers.2,N04-1033,0,0.0381245,"and abbreviations, number extraction, case normalization, etc. In the following, we will refer to the baseline system when uniform parameters are assumed, which can be possibly scaled up or down to modify the beam-search pruning. (7) λ Unlike the log-likelihood criterion (3), the objective function ED (·) might have many local minima. Hence, finding an optimal solution can be very hard. In this work, we use the simplex method [7], an algorithm for multivariate function minimization which requires relatively few function evaluations. The same algorithm was already applied for the same task in [8] and for training log-linear language models in [1]. 3.1. Interaction with Beam Search The optimization process, besides tuning the parameters of the statistical model, may also interfere with the beam search. The reason is in the following property of the scoring function (4): ˜ ; αλ) = Q(˜ ˜ ; λ) Q(˜ e, a e, a α 4.2. Data We evaluated our approach on two Chinese-English translation tasks: the Nist 2003 MT evaluation task1 , large-data case-insensitive conditions, and the C-Star 2003 evaluation campaign 2 . The first task concerns with translation of new agencies, while the second task concer"
2004.iwslt-papers.2,2004.iwslt-evaluation.8,1,0.890938,"t among the top N best scoring ones are pruned. 3. Minimum Error Training 4. Experiments In place of the criterion (3), [4] recently proposed to estimate parameters by minimizing the number of translation errors. We assume that a function ED (λ) is available, which measures the translation errors made by running a model defined by parameter values λ on a development set D. Hence, parameters are searched by: λ∗ = arg min ED (λ) 4.1. Baseline System The core of the translation system is a statistical model, based on the IBM Model 4 and extended to deal with phrases rather than with single words [9]. The corresponding log-linear model is similar to that shown in eq. (5) with the addition of two terms, which explicitly scale the fertility and distortion probabilities of the null word. Search is performed by a decoder based on dynamic programming. Both in training and testing, sentences are pre-processed in order to reduce data sparseness. Pre-processing includes: Chinese word segmentation, separation of words from punctuation, handling of acronyms and abbreviations, number extraction, case normalization, etc. In the following, we will refer to the baseline system when uniform parameters a"
2004.iwslt-papers.2,takezawa-etal-2002-toward,0,0.150824,"e models in [1]. 3.1. Interaction with Beam Search The optimization process, besides tuning the parameters of the statistical model, may also interfere with the beam search. The reason is in the following property of the scoring function (4): ˜ ; αλ) = Q(˜ ˜ ; λ) Q(˜ e, a e, a α 4.2. Data We evaluated our approach on two Chinese-English translation tasks: the Nist 2003 MT evaluation task1 , large-data case-insensitive conditions, and the C-Star 2003 evaluation campaign 2 . The first task concerns with translation of new agencies, while the second task concerns with basic traveling expressions [10]. Test sentences are provided with 4 and 16 human translations, respectively. Tables 1 and 2 report detailed statistics about the used training and test data. For parameter optimization, the Nist 2002 MT evaluation data and 1,000 sentences extracted from the C-Star training data were used, respectively. It is worth noticing that the C-Star 2003 test set has been used as development set for the IWSLT-2004 evalu(8) for any positive real number α. As a consequence, the threshold criterion (6) is affected by any change of the parameter vector λ which corresponds to a scaling transformation. For in"
2004.iwslt-papers.2,2001.mtsummit-papers.68,0,0.070638,"Missing"
2004.iwslt-papers.2,P02-1040,0,\N,Missing
2004.iwslt-papers.2,P02-1038,0,\N,Missing
2005.iwslt-1.11,2004.iwslt-evaluation.8,1,\N,Missing
2005.iwslt-1.11,J93-2003,0,\N,Missing
2005.iwslt-1.11,J96-1002,0,\N,Missing
2005.iwslt-1.11,J00-2004,0,\N,Missing
2005.iwslt-1.11,J03-1005,0,\N,Missing
2005.iwslt-1.11,takezawa-etal-2002-toward,0,\N,Missing
2005.iwslt-1.11,P00-1056,0,\N,Missing
2005.iwslt-1.11,2004.iwslt-papers.2,1,\N,Missing
2006.eamt-1.10,2005.eamt-1.29,0,0.147193,"Missing"
2006.eamt-1.10,popovic-ney-2004-towards,0,0.203729,"Missing"
2006.eamt-1.10,P05-2012,0,0.0249537,"Missing"
2006.eamt-1.10,N03-1017,0,0.00535028,"Missing"
2006.eamt-1.10,W05-0835,0,0.03805,"string e maximizing the posterior probability Pr(e, a |f ) over all possible word alignments a. The conditional distribution is computed with the log-linear model: pλ (e, a |f ) ∝ exp{ R X λr hr (e, f , a)}, r=1 1 http://garraf.epsevg.upc.es/freeling http://www.snowball.tartarus.org/ 3 http://tc-star.itc.it. 2 where hr (e, f , a), r = 1 . . . R are real valued feature functions. The log-linear model is used to score translation hypotheses (e, a) built in terms of strings of phrases, which are simple n-grams of words. The phrase-based translation (Koehn et. al., 2003) process works as follows (Federico and Bertoldi, 2005). At each step, a target phrase is added to the translation whose corresponding source phrase within f is identified through three random quantities: the fertility which establishes its length; the permutation which sets its first position; the tablet which tells its word string. Notice that target phrases might have fertility equal to zero, hence they do not translate any source word. Moreover, untranslated words in f are also modelled through some random variables. The above process is modelled with eight feature functions (Cettolo et. al., 2004) whose parameters are either estimated from da"
2006.eamt-1.10,J03-1002,0,0.00247712,"from data (e.g. target language models, phrase-based lexicon models) or empirically fixed (e.g. permutation models). While feature functions exploit statistics extracted from monolingual or word-aligned texts from the training data, the scaling factors λ of the log-linear model are estimated on the development data by applying a minimum error training procedure (Och, 2004). The phrase-based lexicon model is computed from a parallel corpus provided with wordalignments in both directions, i.e. from source to target positions, and viceversa. Word alignments are computed with the GIZA++ toolkit (Och and Ney, 2003). Translation pairs of phrases are extracted in a way to preserve the original word alignments (Cettolo et. al., 2004). The target language model exploits a 3-gram language models estimated on 39.4 million words from the EPPS corpus. Finally, the search algorithm that computes the most probable translation is implemented with a beam-search algorithm explained in Federico and Bertoldi (2005). The following section addresses data sparseness issues and investigates the use of word transformations. 4 Reduction of Data Sparseness Spanish is a morphologically richer language than English. However, a"
2006.eamt-1.10,P03-1021,0,\N,Missing
2006.iwslt-evaluation.7,W06-3110,0,0.0903759,"ments. Briefly, the CLA computes an association score between all possible word pairs within the parallel corpus, and then applies a greedy algorithm to compute the best word-alignment for each sentence pair • question feature, i.e. a binary feature which triggers when text ends with a question mark and starts with one of the typical starting words of question sentences found in training data • frequency of its n-grams (n=1,2,3,4) within the N-best translations • ratio between the target and source length • 2,3,5-grams target LMs • n-gram posterior probabilities within the N-best translations [4] • sentence length posterior probabilities [4] • word/block reordering probabilities (Section 2.2) 2. System Description • ratio of the source length and the number of source phrases (Section 2.3) ITC-irst SMT system [1] implements a log-linear model and features a two-step decoding strategy. In the first pass, a dynamic programming beam search algorithm generates N-best translation hypotheses for each source sentence. In the secThe first six feature functions were used in our system in IWSLT-2005. The two posterior probabilities represent a 53 Table 1: Statistics of training, development and"
2006.iwslt-evaluation.7,2006.iwslt-papers.4,1,0.889045,". Preprocessing tokenization txt-to-digit lower-casing Chi-to-Eng Chinese English x x x x – x Jpn-to-Eng Japanese English x x – – – x refinement of the counts of n-grams in N-best lists we employed for the 2005 campaign. The last two features are new and are presented in the following. Ara-to-Eng Arabic English x x – – – x Ita-to-Eng Italian English x x x x x x hrules (˜ e, f , a) = K 1 X log Pr(ri ) K i=1 (1) where ri is a matching rule, Pr(ri ) its probability and K the number of the reordering patterns matching the given source/target pair. 2.2. Word/Block Reordering In the companion paper [5], the use of rules for modeling word reordering phenomena in phrase-based SMT is proposed. Reordering rules consist of two sides: the left-handside (lhs), which is a word-based pattern, and the right-handside (rhs), which corresponds to a possible reordering of that pattern. Different rules can share the lhs, because the same pattern can be reordered in more than one way. Rules are automatically extracted from word aligned training data and weighted according to observed statistics. Rules can reorder sequences of single words or a pair of blocks of words. A block is a sequence of source words"
2006.iwslt-evaluation.7,takezawa-etal-2002-toward,0,0.137694,"Missing"
2006.iwslt-evaluation.7,P00-1056,0,0.124776,"2 20.04 5.445 21.07 5.465 Ita-to-Eng BLEU NIST 35.24 7.779 35.59 7.859 – – – – 37.47 8.075 tuation, etc. For both the modules we use the disambig tool,1 as suggested in the instructions supplied by the evaluation organizers. The English training data have been employed to train the language models of the two modules. 3.3. Development: Baseline and Improvements The setup of baselines includes the use of phrases up to 8 words and monotonic search. We extracted phrases and estimated the phrase translation models from the intersection of direct and inverse IBM alignments, expanded as suggested in [7]. Improvements were carried out by introducing novelties in an incremental way, monitoring performance on development sets. Upgrades involve: (i) the addition of CLA wordalignments and (ii) of IBM union word-alignments [3], (iii) the execution of non-monotonic search, and (iv) the application of the rescoring module. Non-monotonic search uses constraints on word reordering defined by means of the maximum vacancy number (MVN) and maximum vacancy distance (MVD) parameters. The following setting was used for experiments: 3.1. Preprocessing The most important stage of preprocessing is tokenization"
2006.iwslt-evaluation.7,W05-0835,0,0.122059,"lations is generated for each source sentence by means of a beam-search decoder; in the second pass, N-best lists are rescored and reranked exploiting additional feature functions. Main updates brought to the 2005 system involve novel additional features which are here described. Results on development sets are analyzed and commented. 2.1. Rescoring Models The feature functions and search constraints adopted for decoding are quite standard: phrase and word translation models, 4-gram language model, fertility model, IBM reordering constraints, beam search. A detailed description is provided in [1, 2]. On the other hand, SMT systems often differ a lot in the models employed for rescoring N-best candidates. Here the list of those we apply: 1. Introduction • direct and inverse IBM model 1 and 3 lexicons, over all possible alignments In this paper, we report on the participation of ITC-irst to the evaluation campaign of the International Workshop on Spoken Language Translation 2006. We submitted runs under the Open Data conditions for all the language pairs: Arabic-to-English, Chinese-to-English, Japanese-to-English and Italian-to-English. For each language pair, we performed translations of"
2006.iwslt-evaluation.7,2005.iwslt-1.11,1,\N,Missing
2006.iwslt-evaluation.8,takezawa-etal-2002-toward,0,0.0497024,"Missing"
2006.iwslt-evaluation.8,P03-1020,0,0.10591,"Missing"
2006.iwslt-evaluation.8,koen-2004-pharaoh,0,\N,Missing
2006.iwslt-evaluation.8,J00-2004,0,\N,Missing
2006.iwslt-evaluation.8,2005.iwslt-1.11,1,\N,Missing
2006.iwslt-evaluation.8,2006.iwslt-evaluation.15,1,\N,Missing
2006.iwslt-papers.4,J99-4005,0,0.203932,"slation task. On other language pairs which differ a lot in the word order, the use of lexicalized rules allows to observe significant improvements as well. 1. Introduction In Machine Translation (MT), one of the main problems to handle is word reordering. Informally, a word is “reordered” when it and its translation occupy different positions within the corresponding sentences. In Statistical Machine Translation (SMT) [1], word reordering is faced from two points of view: constraints and modeling. If arbitrary word-reorderings are permitted, the exact decoding problem was shown to be NP-hard [2]; it can be made polynomial-time by introducing proper constraints, such as IBM constraints [3] and Inversion Transduction Grammars (ITG) constraints [4, 5]. It is worth to notice that both types of constraints are linguistically blind, i.e. they are unable to tune the number of allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausib"
2006.iwslt-papers.4,J97-3002,0,0.157164,"ll. 1. Introduction In Machine Translation (MT), one of the main problems to handle is word reordering. Informally, a word is “reordered” when it and its translation occupy different positions within the corresponding sentences. In Statistical Machine Translation (SMT) [1], word reordering is faced from two points of view: constraints and modeling. If arbitrary word-reorderings are permitted, the exact decoding problem was shown to be NP-hard [2]; it can be made polynomial-time by introducing proper constraints, such as IBM constraints [3] and Inversion Transduction Grammars (ITG) constraints [4, 5]. It is worth to notice that both types of constraints are linguistically blind, i.e. they are unable to tune the number of allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target"
2006.iwslt-papers.4,N04-1021,0,0.0472691,"allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. Some lexicalized block re-ordering models were presented in [6, 7, 8], where each block is associated with an orientation with respect to its predecessor. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. In [9] and [10], the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. Part-of-Speech (POS) rules are applied for preprocessing the source side both in translation model training and in decoding. In this work we present a novel method for extracting reordering rules from word-aligned training data. The units in the left-hand-s"
2006.iwslt-papers.4,N04-4026,0,0.0427816,"allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. Some lexicalized block re-ordering models were presented in [6, 7, 8], where each block is associated with an orientation with respect to its predecessor. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. In [9] and [10], the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. Part-of-Speech (POS) rules are applied for preprocessing the source side both in translation model training and in decoding. In this work we present a novel method for extracting reordering rules from word-aligned training data. The units in the left-hand-s"
2006.iwslt-papers.4,P05-1069,0,0.0683208,"allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. Some lexicalized block re-ordering models were presented in [6, 7, 8], where each block is associated with an orientation with respect to its predecessor. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. In [9] and [10], the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. Part-of-Speech (POS) rules are applied for preprocessing the source side both in translation model training and in decoding. In this work we present a novel method for extracting reordering rules from word-aligned training data. The units in the left-hand-s"
2006.iwslt-papers.4,2004.iwslt-evaluation.6,0,0.0148907,"s. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. Some lexicalized block re-ordering models were presented in [6, 7, 8], where each block is associated with an orientation with respect to its predecessor. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. In [9] and [10], the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. Part-of-Speech (POS) rules are applied for preprocessing the source side both in translation model training and in decoding. In this work we present a novel method for extracting reordering rules from word-aligned training data. The units in the left-hand-side of rules can be plain words or POS’s; moreover, rules can reorder sequences of single units or a pair of unit blocks. In a two-stage SMT system like our one, reordering rules could be exploited"
2006.iwslt-papers.4,P02-1038,0,0.332166,"we have worked with. Section 3 introduces the new reordering method. Sections 4 and 5 present the experimental results and future application, respectively. Some conclusions are drawn in Section 6. 2. The Phrase-based SMT System Given a string f in the source language, the goal of statistical machine translation is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrase-based translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model [11, 12] and by introducing the concept of word alignment [1], the optimal translation can be searched for with the criterion: ˜ e∗ = arg max max ˜ e 182 a R X r=1 λr hr (˜ e, f , a), source string decoder target string WG N−best Rescoring : extractor (from WG) a b c d b a b Figure 1: Architecture of the ITC-irst SMT system. The decoder produces a word-graph (WG) of translation hypotheses. In single-stage translation the most probable string is output. In two-stage decoding, N-best translations are extracted, re-scored, and re-ranked by applying additional feature functions. where ˜ e represents a str"
2006.iwslt-papers.4,takezawa-etal-2002-toward,0,0.0601658,"ORDER BlockREORDER BlockREORDER BlockREORDER BlockREORDER BlockREORDER BlockREORDER BlockREORDER /rr •/vmodal /rr /vmodal /rr /vmodal * * * * [/rr /vmodal] [/rr /vmodal] [/rr /vmodal] [/rr /vmodal] * * * * * * /v * * * * [/v] [/v] * * [/v] [/v] * where K is the number of the reordering patterns matching the given source/target pair. If for a matching POS pattern there is no reordering suggestion matching the actual translation, a small probability is used in the sum (2). 4. Experiments 4.1. Translation Tasks and Data Experiments were carried out on the Basic Traveling Expression Corpus (BTEC) [19]. BTEC is a multilingual speech corpus that contains translation pairs taken from phrase books for tourists. We conducted experiments on three language pairs: Chinese-to-English, Japanese-to-English and Arabic-to-English. On the Chinese-to-English direction, both POS and lexicalized rules were tested. On the contrary, for the other three language pairs, only lexicalized rules were experimented: lexicalized rules were extracted and applied in the same way as POS rules, with the only obvious difference that here the lhs is defined on words instead of POS’s. Detailed statistics on BTEC training d"
2006.iwslt-papers.4,W03-1709,0,0.0362555,"that here the lhs is defined on words instead of POS’s. Detailed statistics on BTEC training data are reported in Table 2. Data sets distributed for the CSTAR 2003 Evaluation Campaign were employed for development, while testing was performed on sets of IWSLT 2004 and IWSLT 2005 Evaluation Campaigns, and on one of the development sets (devset4) distributed for the IWSLT 2006 Evaluation Campaign. For each source sentence of those sets, 16 or 7 references are available. Detailed statistics are reported in Table 3. The Chinese word segmentation and POS tagging were performed by means of ICTCLAS [20] for the experiments involving POS rules. In the experiments using lexicalized rules, Chinese and Japanese were re-segmented with an inhouse word segmentation tool. We found that this permits to smooth inconsistencies of the manual segmentation. All texts were finally tokenized and put in lower case. In the following, translation performance are provided in terms of BLEU [21] and NIST1 scores, computed in the 1 http://www.nist.gov/speech/tests/mt/ /ng * * /ng /ng * * [/ng [/ng [/ng [/ng [/ng ¡/v * * /v /v * * /v] /v] /v] /v] /v] /wfullstop * * * * * * * * * * [/wfullstop] #21: #12: #12: #10: #"
2006.iwslt-papers.4,W05-0835,0,0.105966,"ed to model different aspects of the translation process. Figure 1 illustrates how the translation of an input string is performed by the ITC-irst SMT system. In the first stage, a beam search algorithm (decoder) computes a word graph of translation hypotheses. Hence, either the best translation hypothesis is directly extracted from the word graph and output, or an N-best list of translations is computed by means of an exact algorithm [13]. The N-best translations are then reranked by applying additional feature functions and the top ranking translation is finally output. The search algorithm [14] exploits dynamic programming, i.e. the optimal solution is computed by expanding and recombining previously computed partial theories. The target string is extended step by step by covering new source positions until all of them are covered. For each added target phrase, a source phrase within the source string is chosen, and the corresponding score is computed on the basis of its position and phrase-to-phrase translation probabilities. The fluency of the added target phrase with respect to its left context is evaluated by a 4-gram language model. Some exceptions are also managed: target phra"
2006.iwslt-papers.4,P03-1021,0,0.0875236,"e less promising and constraints are set to possible word re-ordering. Word re-ordering constraints are applied during translation each time a new source position is covered, by limiting the number of vacant positions on the left and the distance from the left most vacant position. The log-linear model on which both the search algorithm and the rescoring stage work embeds feature functions whose parameters are either estimated from data or empirically fixed. The scaling factors λ of the log-linear model are instead estimated on a development set, by applying a minimum error training procedure [15, 16]. The language model feature function is estimated on unsegmented monolingual texts. c a c d b Figure 2: Example illustrating the concept of block. The phrase-to-phrase probability feature is estimated from phrase-pair statistics extracted from word-aligned parallel texts. Alignments are computed with the GIZA++ software tool [17]. Phrase pairs are extracted from the segment pairs by means of the algorithm described in [18]. The distortion model feature function is a fixed negative exponential function computed on the distance between the current and the previously translated source phrases. 3"
2006.iwslt-papers.4,2004.iwslt-papers.2,1,0.770981,"e less promising and constraints are set to possible word re-ordering. Word re-ordering constraints are applied during translation each time a new source position is covered, by limiting the number of vacant positions on the left and the distance from the left most vacant position. The log-linear model on which both the search algorithm and the rescoring stage work embeds feature functions whose parameters are either estimated from data or empirically fixed. The scaling factors λ of the log-linear model are instead estimated on a development set, by applying a minimum error training procedure [15, 16]. The language model feature function is estimated on unsegmented monolingual texts. c a c d b Figure 2: Example illustrating the concept of block. The phrase-to-phrase probability feature is estimated from phrase-pair statistics extracted from word-aligned parallel texts. Alignments are computed with the GIZA++ software tool [17]. Phrase pairs are extracted from the segment pairs by means of the algorithm described in [18]. The distortion model feature function is a fixed negative exponential function computed on the distance between the current and the previously translated source phrases. 3"
2006.iwslt-papers.4,P00-1056,0,0.0950438,"hm and the rescoring stage work embeds feature functions whose parameters are either estimated from data or empirically fixed. The scaling factors λ of the log-linear model are instead estimated on a development set, by applying a minimum error training procedure [15, 16]. The language model feature function is estimated on unsegmented monolingual texts. c a c d b Figure 2: Example illustrating the concept of block. The phrase-to-phrase probability feature is estimated from phrase-pair statistics extracted from word-aligned parallel texts. Alignments are computed with the GIZA++ software tool [17]. Phrase pairs are extracted from the segment pairs by means of the algorithm described in [18]. The distortion model feature function is a fixed negative exponential function computed on the distance between the current and the previously translated source phrases. 3. Reordering Rules In order to overcome the limitations of our simple distortion model, we propose to enrich the translation process with reordering rules as defined in the following. Units on which rules work can be words (lexicalized rules) or POS’s (POS rules). Rules can suggest/constraint reorderings at the level of either sin"
2006.iwslt-papers.4,J93-2003,0,\N,Missing
2006.iwslt-papers.4,J96-1002,0,\N,Missing
2006.iwslt-papers.4,P02-1040,0,\N,Missing
2006.iwslt-papers.4,2005.iwslt-1.11,1,\N,Missing
2007.iwslt-1.11,P07-2045,1,0.0120612,"dels, and finally the use of multiple phrase-tables. By working on top of a state-of-the art baseline, experiments showed that the above methods accounted for significant BLEU score improvements. 1. Introduction This paper presents work carried out at FBK (formerly ITCirst) to develop speech translation systems for three translation tasks of the IWSLT 2007 Evaluation, namely Chineseto-English (CE), Japanese-to-English (JE), and Italian-toEnglish (IE). All three systems are based on different set-ups of the same translation engine, namely the Moses statistical machine translation (SMT) Toolkit [1].1 The FBK team joined the Moses open source project in 2006 and indeed discontinued the development of its own decoding software [2]. This paper is organized as follows. Section 2 gives a short introduction of the general architecture of the systems, which basically takes advantage of experience gained in the past IWSLT evaluations. Section 3 focuses on the novel aspects that were investigated specifically for IWSLT 2007, namely the management of punctuation and the use of additional language resources. Sections 4 to 6 discuss details related to the development and experimentation of each sin"
2007.iwslt-1.11,W07-0712,1,0.913023,"rrently available release of Moses features a multistack, phrase-based, beam-search decoder able to process a CN as well as plain text. It implements a log-linear translation model including as feature functions: direct and inverse phrase-based and word-based lexicons, multiple wordbased n-gram target language models (LMs), phrase and word penalties, and distance-based (possibly lexicalized) reordering model. Computational efficiency is obtained through prefetching and early recombining the translation alternatives of the source phrases. On-demand loading of lexicon, distortion models and LMs [6] and quantization of LMs [7] allow a big reduction of run-time memory usage. 2 Available from http://www.speech.sri.com/projects/srilm A detailed description of the CN decoder can be found in [8]. 2.3. Rescoring After running Moses, a second decoding step can possibly be applied, which rescores the 5000-best list of translation hypotheses with the following 7 additional features: • direct and inverse IBM Model 1 lexicon scores which should capture lexical co-occurrences in the source and target strings; • weighted sum of n-grams relative frequencies (n from 1 to 4) in N -best list, which favor"
2007.iwslt-1.11,W06-3113,1,0.833384,"Moses features a multistack, phrase-based, beam-search decoder able to process a CN as well as plain text. It implements a log-linear translation model including as feature functions: direct and inverse phrase-based and word-based lexicons, multiple wordbased n-gram target language models (LMs), phrase and word penalties, and distance-based (possibly lexicalized) reordering model. Computational efficiency is obtained through prefetching and early recombining the translation alternatives of the source phrases. On-demand loading of lexicon, distortion models and LMs [6] and quantization of LMs [7] allow a big reduction of run-time memory usage. 2 Available from http://www.speech.sri.com/projects/srilm A detailed description of the CN decoder can be found in [8]. 2.3. Rescoring After running Moses, a second decoding step can possibly be applied, which rescores the 5000-best list of translation hypotheses with the following 7 additional features: • direct and inverse IBM Model 1 lexicon scores which should capture lexical co-occurrences in the source and target strings; • weighted sum of n-grams relative frequencies (n from 1 to 4) in N -best list, which favors hypotheses containing popu"
2007.iwslt-1.11,W06-3110,0,0.0169392,"rescores the 5000-best list of translation hypotheses with the following 7 additional features: • direct and inverse IBM Model 1 lexicon scores which should capture lexical co-occurrences in the source and target strings; • weighted sum of n-grams relative frequencies (n from 1 to 4) in N -best list, which favors hypotheses containing popular n-grams of higher order; • the reciprocal of the rank (log), which prefers high-ranked hypotheses; • counts of hypothesis duplicates (log), which awards translations occurring several times; • n-gram posterior probabilities within the N-best translations [9]; • sentence length posterior probabilities [9]. 2.4. Capitalization In the IE task both human and automatic transcripts do not contain case information. We decided to perform translation with models trained on lower-cased texts, and restore capitalization as postprocessing step, by means of the disambig tool of SRILM toolkit, fed with a n-gram case sensitive target LM. Instead, in the CE and JE tasks this issue is not present because the source languages do not represent capitalization explicitly. Hence, case information is automatically introduced during translation by using case-sensitive m"
2007.iwslt-1.11,2005.iwslt-1.8,0,0.0285146,"Missing"
2007.iwslt-1.11,P03-1010,0,0.02494,"s, also in this case performance well compare with the best official results of the 2006 IWSLT evaluation campaign. 5. Japanese-to-English System FBK also developed a system for the JE classic translation task, that is the translation of read speech in the travel domain. We exploited most of the outcomes of the CE system development, with few adjustments as specified in the following. Statistics of corpora utilized here are given in Table 5. It can be noted the symmetry with respect to the resources used for the CE task, with the LDC parallel corpus replaced by the much smaller Reuters corpus [15], one of the shared resources.4 Performance of the various system configurations tested for the JE task are collected in Table 6. The first row refers to the baseline system (Section 2 and Table 2), that is the 1TM/1-LM system corresponding to the entry +inter. of Table 4. In this task, the attempt of adding the 5-gram web75nc LM did not give any improvement (second row). On the contrary, the use of a second TM trained on the Reuters corpus yielded to the best performing system (third row), which was also tested on the best automatic transcription of the Japanese speech (last row). Even for th"
2007.iwslt-1.11,J03-1002,0,\N,Missing
2007.mtsummit-papers.15,J96-1002,0,0.0149963,"organized as follow. Section 2 presents the phrase-based SMT system we work with. Section 3 introduces the new hypotheses generation algorithm. Section 4 describes experiments and analyzes results. A discussion and conclusions end the paper. 2. SMT Process Given a string f in the source language, the goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996; Och and Ney, 2002) and by introducing the concept of word alignment (Brown et al., 1993), the optimal translation can be searched for with the criterion: ˜ e∗ = arg max max ˜ e a R X λr hr (˜ e, f , a), r=1 where ˜ e represents a string of phrases in the target language, a an alignment from the words in f to the phrases in ˜ e, and hr (˜ e, f , a) r = 1, . . . , R are feature functions, designed to model different aspects of the translation process. We performed the “argmax” operation of the above equation by means of the decoder available in Moses,1 an open source toolkit for SMT. Besides t"
2007.mtsummit-papers.15,J93-2003,0,0.0168755,"3 introduces the new hypotheses generation algorithm. Section 4 describes experiments and analyzes results. A discussion and conclusions end the paper. 2. SMT Process Given a string f in the source language, the goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996; Och and Ney, 2002) and by introducing the concept of word alignment (Brown et al., 1993), the optimal translation can be searched for with the criterion: ˜ e∗ = arg max max ˜ e a R X λr hr (˜ e, f , a), r=1 where ˜ e represents a string of phrases in the target language, a an alignment from the words in f to the phrases in ˜ e, and hr (˜ e, f , a) r = 1, . . . , R are feature functions, designed to model different aspects of the translation process. We performed the “argmax” operation of the above equation by means of the decoder available in Moses,1 an open source toolkit for SMT. Besides the decoder, Moses provides tools for training translation and lexicalized reordering model"
2007.mtsummit-papers.15,E06-1032,0,0.0127946,"er’s constraints. For instance, it is easy to verify that a low-order LM (e.g. a bigram LM) permits long word movements and the creation of phrases which are not contained in the phrase-table. The algorithm we have proposed for expanding N-best lists is simple and intuitive, but nevertheless effective in improving the quality of a two-pass SMT system. In fact, small but consistent improvements were measured on various evaluation sets on the challenging Chinese-to-English NIST MT task over a state-of-the-art performing baseline. We are aware of the criticisms about the BLEU score expressed in (Callison-Burch et al., 2006), which could particularly apply to our technique. In their paper, Burch et. al showed that for a translation output there are many possible variants, based on word permutations, that would each receive a similar BLEU score. Some variations could even correspond to higher scores but not to any genuine improvement in translation quality. The same could indeed apply to the translations computed by the expansion step, which in practice generates new word arrangements from the N-best list. We cannot reject this claim, as we did not manually inspect all the translations. However, from one hand our"
2007.mtsummit-papers.15,2006.iwslt-papers.4,1,0.898932,"Missing"
2007.mtsummit-papers.15,W05-0835,0,0.0183761,"ist indeed among the N-best ones. In this paper we present a technique to expand existing N-best lists in order to increase their potential of containing better translations. New entries are generated by means of a word-based n-gram language model estimated on the N-best entries. Experimental results on the NIST Chinese-to-English task show that better N-best lists can be obtained which also result in systematic BLEU score improvements in the re-scoring step. 1. Introduction In Statistical Machine Translation (SMT), performance improvements are often reported by applying two processing steps (Federico and Bertoldi, 2005; Koehn et al., 2003). In the first step, a decoding algorithms is applied that generates an N-best list of translation hypotheses. In the second step, the final translation is computed by re-ranking the Nbest translations through additional scores, computed with more sophisticated feature functions. Clearly, a fundamental assumption of the two step approach is that the generated N-best list contains better translations than the best one found by the decoder. The aim of the additional feature functions is indeed to reward better translations found among the N-best entries of the decoder. The r"
2007.mtsummit-papers.15,N03-1017,0,0.00526933,"nes. In this paper we present a technique to expand existing N-best lists in order to increase their potential of containing better translations. New entries are generated by means of a word-based n-gram language model estimated on the N-best entries. Experimental results on the NIST Chinese-to-English task show that better N-best lists can be obtained which also result in systematic BLEU score improvements in the re-scoring step. 1. Introduction In Statistical Machine Translation (SMT), performance improvements are often reported by applying two processing steps (Federico and Bertoldi, 2005; Koehn et al., 2003). In the first step, a decoding algorithms is applied that generates an N-best list of translation hypotheses. In the second step, the final translation is computed by re-ranking the Nbest translations through additional scores, computed with more sophisticated feature functions. Clearly, a fundamental assumption of the two step approach is that the generated N-best list contains better translations than the best one found by the decoder. The aim of the additional feature functions is indeed to reward better translations found among the N-best entries of the decoder. The reason for applying tw"
2007.mtsummit-papers.15,2005.iwslt-1.8,0,0.0157904,"coding: besides “large” and “giga” LMs, a third 4-gram LM was trained on the English side of the development sets (“dev”). MT performance are provided in terms of case-insensitive BLEU and NIST scores, as computed with the NIST scoring tool. 4.2. Setup For the generation of N-best translation lists, we run Moses with the maximum reordering distance set to 6 and the following feature functions: • phrase translation model, with phrases including at most 7 words • 3 LMs, namely “dev”, “large” and “giga” • lexicalized distortion model, trained specifying the option “orientation-bidirectional-fe” (Koehn et al., 2005) • word and phrase penalties, for balancing the length of translations with that of input sentences. Once N-best lists are available, they are expanded through the algorithm described in Section 3 by setting n = 4. Then, original (1) and updated (2) lists are re-scored and then re-ranked by applying some of the following feature functions, as specified in brackets: • n-gram posterior probabilities within the expanded N+M-best translations (2) • sentence length posterior probabilities within the original N-best translations (Zens and Ney, 2006) (1, 2) • sentence length posterior probabilities w"
2007.mtsummit-papers.15,J00-2004,0,0.245198,"Missing"
2007.mtsummit-papers.15,P02-1038,0,0.0289396,"Section 2 presents the phrase-based SMT system we work with. Section 3 introduces the new hypotheses generation algorithm. Section 4 describes experiments and analyzes results. A discussion and conclusions end the paper. 2. SMT Process Given a string f in the source language, the goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996; Och and Ney, 2002) and by introducing the concept of word alignment (Brown et al., 1993), the optimal translation can be searched for with the criterion: ˜ e∗ = arg max max ˜ e a R X λr hr (˜ e, f , a), r=1 where ˜ e represents a string of phrases in the target language, a an alignment from the words in f to the phrases in ˜ e, and hr (˜ e, f , a) r = 1, . . . , R are feature functions, designed to model different aspects of the translation process. We performed the “argmax” operation of the above equation by means of the decoder available in Moses,1 an open source toolkit for SMT. Besides the decoder, Moses pr"
2007.mtsummit-papers.15,W06-3110,0,0.460989,"t contains better translations than the best one found by the decoder. The aim of the additional feature functions is indeed to reward better translations found among the N-best entries of the decoder. The reason for applying two steps instead of one is that not all available feature functions can be efficiently implemented into the decoder. In fact, not all of them can be decomposed into local scores that can be computed on partial translation hypotheses. Moreover, recently feature functions have been proposed that are estimated directly on the N-best list. In particular, (Chen et al., 2005; Zens and Ney, 2006) have recently reported performance improvements by computing posterior probabilities through n-gram language models (LMs) estimated on the N-best translations. This paper proposes an intermediate step in the chain. Before applying re-scoring, the N-best list is further expanded by applying a generative statistical n-gram LM, estimated on the N-best list itself. In particular, the LM is used to generate M new and different target strings, that do not occur in the N-best list. We applied this technique to a well performing baseline for Chinese-to-English translation, trained under the largedata"
2007.mtsummit-papers.28,P06-1067,0,0.0371199,"Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orientation is implemented through weighted finite state transducers. Unfortunately, that model cannot capture all possible phrase movements. Discriminative lexicalized reordering models are presented in (Zens and Ney, 2006). Several types of features are tested: word-based, word class-based, POS-based and based on local context. Also (Xiong et al., 2006) exploit a discriminative model to predict reordering of consecutive blocks. Two kinds of reorderings are considered: straight and inverted. Any block reordering is allowed, no matter whether it was obser"
2007.mtsummit-papers.28,J96-1002,0,0.0249367,"language point of view, namely English. Moreover, differently from what can happen in lexicalized models, our model does not suffer from data sparseness, since statistics are collected for POS classes instead of plain words. 4. The Phrase-based SMT System Given a string f in the source language, the goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996b; Och and Ney, 2002), the optimal translation can be searched for by exploiting a set of feature functions, designed to model different aspects of the translation process. Our translation system works in two steps. In the first stage, the beam search decoder available in Moses (Koehn et al., 2007),1 computes an N-best list of translations. Moses is an open source toolkit for statistical machine translation which includes, besides the decoder, tools for training translation and lexicalized reordering models, and a minimum error training procedure for estimating optimal interpolation weights. I"
2007.mtsummit-papers.28,P05-1033,0,0.0426713,"unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. A few years ago SMT moved from words to phrases as basic units of translation. Phrases are sequences of words, not necessarily with a syntactic meaning, that allow to model local reorderings, short idioms, insertions and deletions that are sensitive to local context. They are a simple mechanism but powerful enough to really improve performance (Koehn et al., 2003; Och and Ney, 2004). Nevertheless, they are able to capture only local phenomena. In (Chiang, 2005) an interesting extension toward hierarchical phrases was proposed, which allows one to predict long-span reordering phenomena, too. In this work we present a novel word reordering model. In particular, our goal is to model reorderings concerning three major part-of-speech (POS) classes, namely nouns, verbs and adjectives. Relevant statistics are collected from wordaligned parallel texts regarding the distance between target words and the distance between the corresponding source words. The model was applied as a set of additional feature functions for re-scoring N-best lists generated by a ph"
2007.mtsummit-papers.28,W05-0835,0,0.0174814,"roach should have over them. 3. Related Work One of the main research areas in SMT is word/phrase reordering models. Many reordering models have recently been proposed in the literature. The simplest but effective way to capture movements of target phrases is the use of a relative distortion probability distribution d(ai , bi−1 ), where ai denotes the start position of the source phrase that is translated into the i-th target phrase, while bi−1 denotes the end position of the source phrase translated into the i − 1-th target phrase. Systems described in (Och and Ney, 2004; Koehn et al., 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and"
2007.mtsummit-papers.28,J99-4005,0,0.294462,"Missing"
2007.mtsummit-papers.28,N03-1017,0,0.277583,"iding a measure of the plausibility of word movements. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. A few years ago SMT moved from words to phrases as basic units of translation. Phrases are sequences of words, not necessarily with a syntactic meaning, that allow to model local reorderings, short idioms, insertions and deletions that are sensitive to local context. They are a simple mechanism but powerful enough to really improve performance (Koehn et al., 2003; Och and Ney, 2004). Nevertheless, they are able to capture only local phenomena. In (Chiang, 2005) an interesting extension toward hierarchical phrases was proposed, which allows one to predict long-span reordering phenomena, too. In this work we present a novel word reordering model. In particular, our goal is to model reorderings concerning three major part-of-speech (POS) classes, namely nouns, verbs and adjectives. Relevant statistics are collected from wordaligned parallel texts regarding the distance between target words and the distance between the corresponding source words. The mode"
2007.mtsummit-papers.28,P07-2045,1,0.0139298,"he goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996b; Och and Ney, 2002), the optimal translation can be searched for by exploiting a set of feature functions, designed to model different aspects of the translation process. Our translation system works in two steps. In the first stage, the beam search decoder available in Moses (Koehn et al., 2007),1 computes an N-best list of translations. Moses is an open source toolkit for statistical machine translation which includes, besides the decoder, tools for training translation and lexicalized reordering models, and a minimum error training procedure for estimating optimal interpolation weights. In the second stage, the N-best translations are re-scored by applying additional feature functions and re-ranked: the top-ranked translation is finally output. The log-linear models used in both steps have interpolation parameters which are estimated from a development set by applying a minimum err"
2007.mtsummit-papers.28,2005.mtsummit-papers.11,0,0.027414,"Missing"
2007.mtsummit-papers.28,H05-1021,0,0.0173776,"t phrase. Systems described in (Och and Ney, 2004; Koehn et al., 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orientation is implemented through weighted finite state transducers. Unfortunately, that model cannot capture all possible phrase movements. Discriminative lexicalized reordering models are presented in (Zens and Ney, 2006). Several types of features are tested: word-based, word class-based, POS-based and based on local context. Also (Xiong et al., 2006) exploit a discriminative model to predict reordering of consecutive blocks. Two kinds of reorderings are consi"
2007.mtsummit-papers.28,2004.iwslt-evaluation.6,0,0.0171378,"dering. It predicts four types of reordering patterns: monotone adjacent, monotone gap, reverse adjacent and reverse gap. By collapsing into the same neutral class monotone gaps and reverse gaps, it models only three possible events similarly to local reordering models (Tillmann and Zhang, 2005). The distortion model proposed in (Al-Onaizan and Papineni, 2006) assigns a probability distribution over possible relative jumps conditioned on source words. It consists of three components: outbound, inbound and pair distortion. The model’s parameters are directly estimated from word alignments. In (Lee and Roukos, 2004) and (Lee, 2006), the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. POS rules are applied for preprocessing the source side both in translation model training and in decoding. All models referred to above were tested on different language pairs, including Arabic, Chinese, English, German and Japanese languages. Apart Chinese, which is typologically inconsistent (Newmeyer, 2004), each one of other languages has its own grammatical properties which are peculiar but nevertheless comparable. Hence, the reordering m"
2007.mtsummit-papers.28,P06-1090,0,0.0571521,", 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orientation is implemented through weighted finite state transducers. Unfortunately, that model cannot capture all possible phrase movements. Discriminative lexicalized reordering models are presented in (Zens and Ney, 2006). Several types of features are tested: word-based, word class-based, POS-based and based on local context. Also (Xiong et al., 2006) exploit a discriminative model to predict reordering of consecutive blocks. Two kinds of reorderings are considered: straight and inverted. Any block reordering is allowed"
2007.mtsummit-papers.28,P02-1038,0,0.0451674,"w, namely English. Moreover, differently from what can happen in lexicalized models, our model does not suffer from data sparseness, since statistics are collected for POS classes instead of plain words. 4. The Phrase-based SMT System Given a string f in the source language, the goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996b; Och and Ney, 2002), the optimal translation can be searched for by exploiting a set of feature functions, designed to model different aspects of the translation process. Our translation system works in two steps. In the first stage, the beam search decoder available in Moses (Koehn et al., 2007),1 computes an N-best list of translations. Moses is an open source toolkit for statistical machine translation which includes, besides the decoder, tools for training translation and lexicalized reordering models, and a minimum error training procedure for estimating optimal interpolation weights. In the second stage, t"
2007.mtsummit-papers.28,J04-4002,0,0.398776,"he plausibility of word movements. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. A few years ago SMT moved from words to phrases as basic units of translation. Phrases are sequences of words, not necessarily with a syntactic meaning, that allow to model local reorderings, short idioms, insertions and deletions that are sensitive to local context. They are a simple mechanism but powerful enough to really improve performance (Koehn et al., 2003; Och and Ney, 2004). Nevertheless, they are able to capture only local phenomena. In (Chiang, 2005) an interesting extension toward hierarchical phrases was proposed, which allows one to predict long-span reordering phenomena, too. In this work we present a novel word reordering model. In particular, our goal is to model reorderings concerning three major part-of-speech (POS) classes, namely nouns, verbs and adjectives. Relevant statistics are collected from wordaligned parallel texts regarding the distance between target words and the distance between the corresponding source words. The model was applied as a s"
2007.mtsummit-papers.28,N04-1021,0,0.0203889,"eas in SMT is word/phrase reordering models. Many reordering models have recently been proposed in the literature. The simplest but effective way to capture movements of target phrases is the use of a relative distortion probability distribution d(ai , bi−1 ), where ai denotes the start position of the source phrase that is translated into the i-th target phrase, while bi−1 denotes the end position of the source phrase translated into the i − 1-th target phrase. Systems described in (Och and Ney, 2004; Koehn et al., 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orient"
2007.mtsummit-papers.28,P03-1021,0,0.0164861,"ist of translations. Moses is an open source toolkit for statistical machine translation which includes, besides the decoder, tools for training translation and lexicalized reordering models, and a minimum error training procedure for estimating optimal interpolation weights. In the second stage, the N-best translations are re-scored by applying additional feature functions and re-ranked: the top-ranked translation is finally output. The log-linear models used in both steps have interpolation parameters which are estimated from a development set by applying a minimum error training procedure (Och, 2003). The reordering model presented in the following section is the only additional feature function applied for re-scoring the N-best lists. 5. The POS-based Reordering Model We assume that we have a parallel training corpus provided with inverted word alignments, that is alignments from target to source positions. Let (f , e) be a source-target sentence pair, and let a be an inverted alignment which maps target positions i into source positions ai = j. For any target position i, we look for its predecessor i∗ that is aligned to the rightmost source position. Our interest is indeed in the differ"
2007.mtsummit-papers.28,takezawa-etal-2002-toward,0,0.0773993,"Missing"
2007.mtsummit-papers.28,P05-1069,0,0.0861631,"ng models. Many reordering models have recently been proposed in the literature. The simplest but effective way to capture movements of target phrases is the use of a relative distortion probability distribution d(ai , bi−1 ), where ai denotes the start position of the source phrase that is translated into the i-th target phrase, while bi−1 denotes the end position of the source phrase translated into the i − 1-th target phrase. Systems described in (Och and Ney, 2004; Koehn et al., 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orientation is implemented through weighted finit"
2007.mtsummit-papers.28,N04-4026,0,0.0424606,"/phrase reordering models. Many reordering models have recently been proposed in the literature. The simplest but effective way to capture movements of target phrases is the use of a relative distortion probability distribution d(ai , bi−1 ), where ai denotes the start position of the source phrase that is translated into the i-th target phrase, while bi−1 denotes the end position of the source phrase translated into the i − 1-th target phrase. Systems described in (Och and Ney, 2004; Koehn et al., 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orientation is impleme"
2007.mtsummit-papers.28,J97-3002,0,0.0838103,"1. Introduction In machine translation (MT), one of the main problems to handle is word reordering. A word is “reordered” when it and its translation occupy different positions within the corresponding sentence. In Statistical MT (SMT) (Brown et al., 1993), word reordering is faced from two points of view: constraints and modeling. If arbitrary wordreorderings are permitted, the exact decoding problem is NP-hard (Knight, 1999); it can be made polynomialtime by introducing proper constraints, such as IBM constraints (Berger et al., 1996a) and Inversion Transduction Grammars (ITG) constraints (Wu, 1997). Among all the allowed word-reorderings, it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is that of providing a measure of the plausibility of word movements. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. A few years ago SMT moved from words to phrases as basic units of translation. Phrases are sequences of words, not necessarily with a syntactic meaning, that allow to model"
2007.mtsummit-papers.28,P06-1066,0,0.0475135,", 2004; Koehn et al., 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orientation is implemented through weighted finite state transducers. Unfortunately, that model cannot capture all possible phrase movements. Discriminative lexicalized reordering models are presented in (Zens and Ney, 2006). Several types of features are tested: word-based, word class-based, POS-based and based on local context. Also (Xiong et al., 2006) exploit a discriminative model to predict reordering of consecutive blocks. Two kinds of reorderings are considered: straight and inverted. Any block"
2007.mtsummit-papers.28,W06-3108,0,0.053813,"ibed in (Och and Ney, 2004; Koehn et al., 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orientation is implemented through weighted finite state transducers. Unfortunately, that model cannot capture all possible phrase movements. Discriminative lexicalized reordering models are presented in (Zens and Ney, 2006). Several types of features are tested: word-based, word class-based, POS-based and based on local context. Also (Xiong et al., 2006) exploit a discriminative model to predict reordering of consecutive blocks. Two kinds of reorderings are considered: straight and"
2007.mtsummit-papers.28,J93-2003,0,\N,Missing
2007.mtsummit-papers.28,2005.iwslt-1.8,0,\N,Missing
2008.iwslt-evaluation.4,2008.iwslt-papers.1,1,0.809097,"tistical machine translation (SMT) system relies on the availability of parallel corpus for the estimation of its models. The translation quality is affected by the size of such corpus and its closeness to the task domain. Unfortunately, for many relevant language pairs such parallel data are available only to a small extent, or they are out-of-domain. To circumvent the data bottleneck for this low-resourced language pairs, research on SMT has been recently investigated the use of so-called pivot or bridge languages. An overview of research on pivot translation is given in our companion paper [1]. The assumptions underlying the adoption of a pivot language are simple to state: (i) there is lack of parallel texts between F and E, while (ii) there exists a language G for which (abundant) parallel texts between F and G and between G and E are available. These assumptions are fully matched by the specifications of the Pivot task, because the English parts of the Chinese-English and English- 34 - Spanish corpora do not overlap. We analyzed the pivot translation task from a theoretical point of view providing a mathematically sound formulation of the various approaches presented in the lite"
2008.iwslt-evaluation.4,P07-2045,1,0.0311749,"the problem and experimentally compared them on the Pivot task. Two of them couple Chinese-English and English-Spanish MT systems, the third approach creates a new translation model starting from them, and the fourth approach synthesizes Chinese-Spanish training data translating the target part of the available ChineseEnglish corpus and creates a MT system on these data. These approaches are briefly introduced in the following Section, while a detailed description can be found in [1]. To perform a fair comparison between these approaches we relied on the well-known open source MT system Moses [2] in its standard configuration, and we did not apply any specific enhancement like lexicalized reordering models or rescoring. For each approach specific training of the models were performed on the provided BTEC data only, without using any additional training data. We also submitted runs for the Chinese-English and Chinese-Spanish BTEC tasks and for the Chinese-English Challenge task. This paper is organized as follows. In next Section we introduce the four approaches we have taken into account to address the pivot translation. Section 3 describes the data and the systems we employed to part"
2008.iwslt-evaluation.4,takezawa-etal-2002-toward,0,0.0393909,"training and development, and the employed preprocessing. Then, the baseline system is described, which is used both for the BTEC and Challenge tasks and as building blocks for the Pivot task. Later, the systems specific for the Pivot task are presented with some details. Finally, the performance of the developed systems on a blind test are reported. 3.1. Data Five monolingual corpora are employed for training our systems: namely two for Chinese (C1 and C2), two for English (E1 and E2) and one for Spanish (S1). All corpora are officially provided by the organizers, and are extracted from BTEC [4]; each of them contains 20K sentences. According to the evaluation specification, the parallel corpora CE1 and CS1 are exploited for the CE- and CS-btec tasks, respectively; CE1 for the CE-challenge task; CE2 and ES1 are used to train the systems for the CES-pivot task. We stress that the parallel corpus CE1 are not considered at all for this task. Six development sets are provided consisting of about 500 sentences each and a number of references ranging from 6 to 16 for the CE-btec task. Only one of them is available Proceedings of IWSLT 2008, Hawaii - U.S.A. desde que la nueva administracion"
2008.iwslt-evaluation.4,P03-1021,0,0.0160048,"sent CE-btec CS-btec CE-chal CE-pivot ES-pivot 54,021 28,068 55,743 28,095 19,972 source words dict 439K 8,847 229K 8,284 447K 8,864 217K 8,987 182K 8,385 target words dict 499K 10,765 250K 11,734 507K 11,051 248K 8,951 177K 11,019 3.2. Baseline System The baseline system Direct is built upon the open-source MT toolkit Moses [2]. The decoder features a statistical loglinear model including a phrase-based translation model, a language model, a distortion model and word and phrase penalties. The 8 weights of the log-linear combination are optimized by means of a minimum error training procedure [5]. The phrase-based translation model provides direct and inverted frequency-based and lexical-based probabilities for each phrase pair included in a given phrase table. Phrase pairs are extracted from symmetrized word alignments generated by GIZA++ [6]. This extraction method does not apply in the case of pivoting with constrained alignments (see Section 2.1.2): phrase pairs and their scores are obtained by the product of two existing phrase tables (from source to pivot and from pivot to target). A 5-gram word-based LM is estimated on the target side of the parallel corpora using the improved"
2008.iwslt-evaluation.4,J03-1002,0,0.00295554,"The baseline system Direct is built upon the open-source MT toolkit Moses [2]. The decoder features a statistical loglinear model including a phrase-based translation model, a language model, a distortion model and word and phrase penalties. The 8 weights of the log-linear combination are optimized by means of a minimum error training procedure [5]. The phrase-based translation model provides direct and inverted frequency-based and lexical-based probabilities for each phrase pair included in a given phrase table. Phrase pairs are extracted from symmetrized word alignments generated by GIZA++ [6]. This extraction method does not apply in the case of pivoting with constrained alignments (see Section 2.1.2): phrase pairs and their scores are obtained by the product of two existing phrase tables (from source to pivot and from pivot to target). A 5-gram word-based LM is estimated on the target side of the parallel corpora using the improved Kneser-Ney smoothing [7]. The distortion model is a standard negative-exponential model. The Direct systems have been used in the BTEC and Challenge tasks, and they have been exploited as constituents of the systems employed in the Pivot task. 3.3. Piv"
2008.iwslt-papers.1,C88-2125,0,0.601156,"e interpreted carefully. This paper presents a theoretical formulation of SMT with pivot languages, that embraces several approaches in the literature and a few original methods. Extensive experiments are reported that compare performance of each bridging when using dependent and independent parallel data. Experiments were conducted on the IWSLT 2008 benchmark, namely the translation of traveling domain expressions from Chinese to Spanish via English. 2. Previous Work The use of pivot or bridge languages has been advocated for different purposes, such as rule-based machine translation systems [1], translation lexicon induction [2, 3], word alignment [4, 5, 6], cross language information retrieval [7]. Concerning statistical machine translation, pivot language translation has been investigated for instance by [8] in order to extend an interlingua based speech translation system to a new language. In [9], Catalan-English translation is bridged through Spanish. The authors compared two coupling strategies: cascading of two translation systems versus training of system from parallel texts whose target part has been automatically translated from pivot to target. System cascading was recent"
2008.iwslt-papers.1,N01-1020,0,0.0857246,"presents a theoretical formulation of SMT with pivot languages, that embraces several approaches in the literature and a few original methods. Extensive experiments are reported that compare performance of each bridging when using dependent and independent parallel data. Experiments were conducted on the IWSLT 2008 benchmark, namely the translation of traveling domain expressions from Chinese to Spanish via English. 2. Previous Work The use of pivot or bridge languages has been advocated for different purposes, such as rule-based machine translation systems [1], translation lexicon induction [2, 3], word alignment [4, 5, 6], cross language information retrieval [7]. Concerning statistical machine translation, pivot language translation has been investigated for instance by [8] in order to extend an interlingua based speech translation system to a new language. In [9], Catalan-English translation is bridged through Spanish. The authors compared two coupling strategies: cascading of two translation systems versus training of system from parallel texts whose target part has been automatically translated from pivot to target. System cascading was recently investigated in [10], too. In [11]"
2008.iwslt-papers.1,W02-2026,0,0.0369528,"presents a theoretical formulation of SMT with pivot languages, that embraces several approaches in the literature and a few original methods. Extensive experiments are reported that compare performance of each bridging when using dependent and independent parallel data. Experiments were conducted on the IWSLT 2008 benchmark, namely the translation of traveling domain expressions from Chinese to Spanish via English. 2. Previous Work The use of pivot or bridge languages has been advocated for different purposes, such as rule-based machine translation systems [1], translation lexicon induction [2, 3], word alignment [4, 5, 6], cross language information retrieval [7]. Concerning statistical machine translation, pivot language translation has been investigated for instance by [8] in order to extend an interlingua based speech translation system to a new language. In [9], Catalan-English translation is bridged through Spanish. The authors compared two coupling strategies: cascading of two translation systems versus training of system from parallel texts whose target part has been automatically translated from pivot to target. System cascading was recently investigated in [10], too. In [11]"
2008.iwslt-papers.1,C00-1015,0,0.0328714,"formulation of SMT with pivot languages, that embraces several approaches in the literature and a few original methods. Extensive experiments are reported that compare performance of each bridging when using dependent and independent parallel data. Experiments were conducted on the IWSLT 2008 benchmark, namely the translation of traveling domain expressions from Chinese to Spanish via English. 2. Previous Work The use of pivot or bridge languages has been advocated for different purposes, such as rule-based machine translation systems [1], translation lexicon induction [2, 3], word alignment [4, 5, 6], cross language information retrieval [7]. Concerning statistical machine translation, pivot language translation has been investigated for instance by [8] in order to extend an interlingua based speech translation system to a new language. In [9], Catalan-English translation is bridged through Spanish. The authors compared two coupling strategies: cascading of two translation systems versus training of system from parallel texts whose target part has been automatically translated from pivot to target. System cascading was recently investigated in [10], too. In [11] word alignment systems are"
2008.iwslt-papers.1,P06-2112,0,0.0435046,"formulation of SMT with pivot languages, that embraces several approaches in the literature and a few original methods. Extensive experiments are reported that compare performance of each bridging when using dependent and independent parallel data. Experiments were conducted on the IWSLT 2008 benchmark, namely the translation of traveling domain expressions from Chinese to Spanish via English. 2. Previous Work The use of pivot or bridge languages has been advocated for different purposes, such as rule-based machine translation systems [1], translation lexicon induction [2, 3], word alignment [4, 5, 6], cross language information retrieval [7]. Concerning statistical machine translation, pivot language translation has been investigated for instance by [8] in order to extend an interlingua based speech translation system to a new language. In [9], Catalan-English translation is bridged through Spanish. The authors compared two coupling strategies: cascading of two translation systems versus training of system from parallel texts whose target part has been automatically translated from pivot to target. System cascading was recently investigated in [10], too. In [11] word alignment systems are"
2008.iwslt-papers.1,N07-1061,0,0.413231,"on induction [2, 3], word alignment [4, 5, 6], cross language information retrieval [7]. Concerning statistical machine translation, pivot language translation has been investigated for instance by [8] in order to extend an interlingua based speech translation system to a new language. In [9], Catalan-English translation is bridged through Spanish. The authors compared two coupling strategies: cascading of two translation systems versus training of system from parallel texts whose target part has been automatically translated from pivot to target. System cascading was recently investigated in [10], too. In [11] word alignment systems are combined from multiple bridge languages by multiplying posterior probability matrices. This technique requires the existence of parallel Proceedings of IWSLT 2008, Hawaii - U.S.A. data for several languages, like the proceedings of United Nations or European Parliament. An approach based on phrase table multiplication is discussed in [10, 12]. Scores of the new phrase table are computed by combining corresponding translation probabilities in the source-pivot and pivot-target phrase-tables. Finally, in [13] a similar approach is described, but for the s"
2008.iwslt-papers.1,D07-1005,0,0.0350209,"2, 3], word alignment [4, 5, 6], cross language information retrieval [7]. Concerning statistical machine translation, pivot language translation has been investigated for instance by [8] in order to extend an interlingua based speech translation system to a new language. In [9], Catalan-English translation is bridged through Spanish. The authors compared two coupling strategies: cascading of two translation systems versus training of system from parallel texts whose target part has been automatically translated from pivot to target. System cascading was recently investigated in [10], too. In [11] word alignment systems are combined from multiple bridge languages by multiplying posterior probability matrices. This technique requires the existence of parallel Proceedings of IWSLT 2008, Hawaii - U.S.A. data for several languages, like the proceedings of United Nations or European Parliament. An approach based on phrase table multiplication is discussed in [10, 12]. Scores of the new phrase table are computed by combining corresponding translation probabilities in the source-pivot and pivot-target phrase-tables. Finally, in [13] a similar approach is described, but for the sake of improvi"
2008.iwslt-papers.1,P07-1108,0,0.41977,"ng strategies: cascading of two translation systems versus training of system from parallel texts whose target part has been automatically translated from pivot to target. System cascading was recently investigated in [10], too. In [11] word alignment systems are combined from multiple bridge languages by multiplying posterior probability matrices. This technique requires the existence of parallel Proceedings of IWSLT 2008, Hawaii - U.S.A. data for several languages, like the proceedings of United Nations or European Parliament. An approach based on phrase table multiplication is discussed in [10, 12]. Scores of the new phrase table are computed by combining corresponding translation probabilities in the source-pivot and pivot-target phrase-tables. Finally, in [13] a similar approach is described, but for the sake of improving translation probabilities through triangulation with other languages. 3. SMT through pivot languages SMT with bridge languages is concerned about how to optimally perform translation from F to E, by taking advantage of the available language resources. We can device two general approaches to apply bridge languages in SMT, namely bridging at translation time or bridgi"
2008.iwslt-papers.1,P07-1092,0,0.510381,"System cascading was recently investigated in [10], too. In [11] word alignment systems are combined from multiple bridge languages by multiplying posterior probability matrices. This technique requires the existence of parallel Proceedings of IWSLT 2008, Hawaii - U.S.A. data for several languages, like the proceedings of United Nations or European Parliament. An approach based on phrase table multiplication is discussed in [10, 12]. Scores of the new phrase table are computed by combining corresponding translation probabilities in the source-pivot and pivot-target phrase-tables. Finally, in [13] a similar approach is described, but for the sake of improving translation probabilities through triangulation with other languages. 3. SMT through pivot languages SMT with bridge languages is concerned about how to optimally perform translation from F to E, by taking advantage of the available language resources. We can device two general approaches to apply bridge languages in SMT, namely bridging at translation time or bridging at training time, which we briefly overview now. 3.1. Bridging at Translation Time Under this framework, we try to integrate or couple two levels of translation wit"
2008.iwslt-papers.1,J93-2003,0,0.0338674,"-ordering for each considered translation direction. Figure 1 shows the two level alignments for a simple example involving translations from Chinese to Italian, through English. Horizontal segments show that the English string is segmented differently when it is generated from Chinese than when it is translated into Italian. - 144 - Another way to exploit parallel training corpora F-G and G-E is to use them to develop and train a translation system from F to E. 3.2.1. Bridging Alignment Models We will focus here on possible extension of the standard training criterion of IBM alignment models [15], which assumes a parallel corpus (F, E) = {(fi , ei )} and looks for Proceedings of IWSLT 2008, Hawaii - U.S.A. desde que la nueva administracion tomo posesion de su cargo este año desde que la nueva administracion tomo posesion de su cargo este año since the new administration took office this year this year new administration took office since the Pivot Translation direction Target Source Figure 1: Phrase-based translation from Chinese to Spanish, through English, with independent alignments (left) and constrained alignments (right). parameters maximizing θF∗ E = argmax Y θF E PθF E (fi |ei"
2008.iwslt-papers.1,takezawa-etal-2002-toward,0,0.0108447,"are reinforced during training as well as phrase-pairs using words of the most probable translations. This approach is indeed more sound than just taking the list of n-best, as experimental results will confirm in the following sections. 4. Task description The approaches introduced in the previous section were evaluated on a benchmark provided by the 2008 International Workshop on Spoken Language Translation1 . One of the proposed tasks consists in translating from Chinese to Spanish by pivoting through English. Training and evaluation data are from the Basic Travel Expression Corpus (BTEC) [16], a collection of parallel translations in the traveling domain. Five monolingual corpora were available: two for Chinese (C1 and C2), two for English (E1 and E2) and one for Spanish (S1). C1, E1, and S1 are also aligned at the sentence level, hence they provide a trilingual parallel corpus; C2 and E2 are aligned as well and form a bilingual parallel corpus. The official benchmark for the pivot task of IWSLT consists only in the two non overlapping bilingual parallel corpora CE2 and ES1, while the bilingual parallel corpus CS1 is for contrastive experiments. The benchmark also includes a devel"
2008.iwslt-papers.1,P07-2045,1,0.0137344,"Missing"
2008.iwslt-papers.1,P03-1021,0,0.0374955,"Missing"
2008.iwslt-papers.1,J03-1002,0,0.00862119,"ms developed with the Moses open-source toolkit [17]. The employed decoder features a statistical log-linear model including a phrase-based translation model, a language model, a distortion model, and word and phrase penalties. The resulting eight weights of the log-linear combination are optimized by means of a minimum error training procedure [18]. The phrase-based translation model provides direct and inverted frequency-based and lexical-based probabilities for each phrase pair included in a given phrase table. Phrase pairs are extracted from symmetrized word alignments generated by GIZA++ [19]. This extraction method does not apply in the case of pivoting with constrained alignments (see Section 3.1.2) as the phrase table is obtained by taking the product of two existing phrase tables. A 5-gram word-based LM is estimated on the target side of the parallel corpora using the improved Kneser-Ney smoothing [20]. The distortion model is a standard negativeexponential model. Table 2 shows the BLEU scores achieved by the baseline systems (Direct) on the dev and test sets. It is worth noticing that the system trained on CE1 outperforms the one trained on CE2. The reason is that both dev an"
2009.iwslt-evaluation.5,N06-2013,0,0.0238153,"e-Processing for Morphologically Rich Languages Indeed linguistic preprocessing plays a fundamental role in any NLP application involving morphologically rich languages, such as Arabic and Turkish. This is particularly true for SMT into English where differences in word granularity between languages reflects on much higher data sparseness on the source side and on the difficulty to properly model word-level alignments. We approached these problems through morphological segmentation of the source languages, referring partly to the work of [1] on an EnglishTurkish task, and partly to the one of [2] on an ArabicEnglish task. Secondly, as this was shown to have a positive effect on some Arabic-English SMT systems of previous IWSLT editions [3, 4], we developed two simple languagespecific techniques of lexical approximation, which consists in replacing the OOVs of the test set by words of the training that are morphologically close to them. 1. Introduction FBK submitted runs at the IWSLT 2009 Evaluation for the Arabic-English and Turkish-English BTEC tasks, and for the Challenge Task involving Chinese and English languages in both directions. This paper reports on efforts we made in the de"
2009.iwslt-evaluation.5,2009.iwslt-papers.1,1,0.811638,"eaning absence of possessive suffixes is also removed; • copula is split off; • person suffixes are split off from finite verb forms and from copula. The following example shows an analyzed Turkish word before and after segmentation. The number of tokens increases from 1 to 5 as the word is split into noun, possessive, instrumental case, copula and verbal person: 2 More precisely: score = match × 20 − diff × 2 − diff × 5, 1 2 where match, diff1 and diff2 are respectively the numbers of shared contiguous tags, different tags in the OOV word, different tags in the replacer candidate. 1 Refer to [8] for a more detailed and linguistically motivated description. - 38 - Proceedings of IWSLT 2009, Tokyo - Japan where pi ’s are target LMs built on clusters which the training data are split in. With the help of Figure 1, the basic adaptation procedure is described in the following. Let us assume that the parallel training data have been partitioned into a set of M bilingual clusters, according to some criterion. On each cluster, language specific LMs are estimated, which are then organized into two language specific mixture models. All operations described so far are performed off-line. Now le"
2009.iwslt-evaluation.5,P05-1071,0,0.015153,"o language specific mixture models. All operations described so far are performed off-line. Now let us consider a source text or sentence to be translated. Before translation, the input is used to estimate optimal weights of the source language mixture through Expectation-Maximization. The resulting weights are then transferred to the target language mixture, which is finally used as LM feature function by the SMT system. 2.2.1. Morphological Segmentation Several state-of-the-art tools are available to perform morphological segmentation of Arabic text. For the evaluation we have compared MADA [9] and AMIRA [10], two softwares that differ both on their decision strategy and on the segmentation scheme they apply to the words. While the first is a morphological disambiguator based on linguistic features produced by the Buckwalter analyzer [11], the second is a much lighter-weight SVM classifier based on a -5/+5 character context. As for the segmentation schemes (see Table 2) MADA (scheme “D2”) only splits proclitics – namely conjunctions (w+ ‘and’, f+ ‘then’), prepositions (b+ ‘by’, k+ ‘as’, l+ ‘to’) and the future tense (s+) – whereas AMIRA also separates enclitics, i.e. object and poss"
2009.iwslt-evaluation.5,N04-4038,0,0.0110197,"cific mixture models. All operations described so far are performed off-line. Now let us consider a source text or sentence to be translated. Before translation, the input is used to estimate optimal weights of the source language mixture through Expectation-Maximization. The resulting weights are then transferred to the target language mixture, which is finally used as LM feature function by the SMT system. 2.2.1. Morphological Segmentation Several state-of-the-art tools are available to perform morphological segmentation of Arabic text. For the evaluation we have compared MADA [9] and AMIRA [10], two softwares that differ both on their decision strategy and on the segmentation scheme they apply to the words. While the first is a morphological disambiguator based on linguistic features produced by the Buckwalter analyzer [11], the second is a much lighter-weight SVM classifier based on a -5/+5 character context. As for the segmentation schemes (see Table 2) MADA (scheme “D2”) only splits proclitics – namely conjunctions (w+ ‘and’, f+ ‘then’), prepositions (b+ ‘by’, k+ ‘as’, l+ ‘to’) and the future tense (s+) – whereas AMIRA also separates enclitics, i.e. object and possessive pronouns"
2009.iwslt-evaluation.5,P02-1038,0,0.0119197,"ng the possible data sparseness issue that can affect the sentence specific weight estimation. 4.2. Turkish-English System 4.2.1. Data For training our Turkish-English system we exclusively used the provided BTEC training corpus. Parameters were tuned on IWSLT09’s devset1 using the gold reference translation only. Evaluation during development was performed on devset2. 4. Evaluation results 4.1. Baseline System Given a string f in the source language, the goal of statistical machine translation [12] is to select the most probable string e in the target language. By assuming a log-linear model [13, 14], the optimal translation can be searched for with the criterion: e∗ = arg max max e a R X 4.2.2. Baseline Setup The baseline preprocessing consists in simple tokenization (IWSLT09’s released script) and lowercasing of the source side data. Due to the severe mismatch in word order between the languages, we set the distortion limit (DL) to 10. Moses option -drop-unknown was active in all submitted runs. λr hr (e, f , a), 4.2.3. Results r=1 where a represents a word- or phrase-based alignment between f and e, and hr (e, f , a) r = 1, . . . , R are feature functions, designed to model different a"
2009.iwslt-evaluation.5,P07-2045,1,0.0126036,"t of separating the training corpus into several subsets yields wi pi (e) i=1 3 AMIRA splits the same proclitics as MADA except for the future tense. 4 Available - 39 - from http://glaros.dtc.umn.edu/gkhome/views/cluto Proceedings of IWSLT 2009, Tokyo - Japan SRC TRAINING PARALLEL TEXTS SRC SRC TGT CLSTR 1 LM ESTIMATION CLSTR 2 TGT CLSTR M TGT LM 1 LM 1 LM 2 LM 2 LM M LM M CLUSTERING OPTIMIZATION of SRC LMs INTERPOLATION SRC wi OFF−LINE ON−LINE INTERPOLATION of TGT LMs TEXT SMT TRANSLATION Figure 1: Basic procedure for LM adaptation. Our systems are built upon the open-source MT toolkit Moses [15]. The decoder features a statistical log-linear model including a phrase-based translation model, a language model, a distortion model and word and phrase penalties. The weights λr of the log-linear combination are optimized by means of a minimum error training (MERT) procedure [16]. The phrase-based translation model provides direct and inverted frequency-based and lexical-based probabilities for each phrase pair included in a given phrase table. Phrase pairs are extracted from symmetrized word alignments generated by GIZA++ [17]. better results, since the EM procedure is allowed complete fre"
2009.iwslt-evaluation.5,P03-1021,0,0.0149284,"EXTS SRC SRC TGT CLSTR 1 LM ESTIMATION CLSTR 2 TGT CLSTR M TGT LM 1 LM 1 LM 2 LM 2 LM M LM M CLUSTERING OPTIMIZATION of SRC LMs INTERPOLATION SRC wi OFF−LINE ON−LINE INTERPOLATION of TGT LMs TEXT SMT TRANSLATION Figure 1: Basic procedure for LM adaptation. Our systems are built upon the open-source MT toolkit Moses [15]. The decoder features a statistical log-linear model including a phrase-based translation model, a language model, a distortion model and word and phrase penalties. The weights λr of the log-linear combination are optimized by means of a minimum error training (MERT) procedure [16]. The phrase-based translation model provides direct and inverted frequency-based and lexical-based probabilities for each phrase pair included in a given phrase table. Phrase pairs are extracted from symmetrized word alignments generated by GIZA++ [17]. better results, since the EM procedure is allowed complete freedom in assigning the LM weights. However, weights computed in such a manner may be less reliable, since the estimation is performed on few data (one single sentence). 3.3.3. Two-step weight estimation This approach merges the previous two in the attempt of keeping their advantages"
2009.iwslt-evaluation.5,J93-2003,0,\N,Missing
2009.iwslt-evaluation.5,J96-1002,0,\N,Missing
2009.iwslt-evaluation.5,J03-1002,0,\N,Missing
2009.iwslt-evaluation.5,W07-0704,0,\N,Missing
2009.iwslt-evaluation.5,2007.iwslt-1.27,0,\N,Missing
2009.iwslt-evaluation.5,2008.iwslt-evaluation.10,0,\N,Missing
2009.iwslt-papers.1,N06-2013,0,0.588381,"orms of the same suffix into one single form. much simpler than those for Turkish, given that the number of involved clitics and suffixes is typically smaller1 . As pointed by [2], Turkish employs about 30,000 root words and about 150 distinct suffixes. Altough not all possible suffix combinations are grammatical, the number of potential inflected/derived forms of a given root word is still extremely high. This implies that linguistic knowledge becomes crucial to guide the investigation of meaningful segmentation schemes among all possible rule combinations. Another difference with respect to [3] is that in our work we consider not only splitting but also removing suffixes. In previous editions of IWSLT, [4] and [5] tried to further decrease the out-of-vocabulary rate of the Arabic test set by a so-called lexical approximation approach. This idea consists in finding words of the training that are morphologically close to OOVs and introducing them into the translation process by various techniques – i.e. best replacer computation at run-time in [4] or phrase table expansion in [5]. This method was shown to have a positive effect on Arabic-English SMT systems. In this work, we developed"
2009.iwslt-papers.1,P07-2045,1,0.0174347,"on one side to correctly map the translation phrase-pair kız arkadas¸‘girlfriend’ (left), and on the other to capture the complex word re-ordering of the phrase bu yere literally meaning ‘this to-place’ (right). - 131 - Proceedings of IWSLT 2009, Tokyo - Japan Figure 1: Two examples of sentence alignments before (up) and after (bottom) morphological segmentation MS11. 5. Experiments source side of the parallel data. This can be seen as a further sign of the fact that the translation task is being better modeled. 5.1. Baseline The baseline system is built upon the open-source MT toolkit Moses [9]. Phrase pairs are extracted from symmetrized word alignments generated by GIZA++ [10]. The phrase-based translation model provides direct and inverted frequency-based and lexical-based probabilities for each phrase pair included in a given phrase table. The decoder features a statistical log-linear model including a phrase-based translation model, a 5-gram language model, a lexicalized distortion model and word and phrase penalties. Distortion limit is 6 by default. The weights of the log-linear combination are optimized by means of a minimum error training procedure [11] run on IWSLT09’s dev"
2009.iwslt-papers.1,J03-1002,0,0.00382807,"eft), and on the other to capture the complex word re-ordering of the phrase bu yere literally meaning ‘this to-place’ (right). - 131 - Proceedings of IWSLT 2009, Tokyo - Japan Figure 1: Two examples of sentence alignments before (up) and after (bottom) morphological segmentation MS11. 5. Experiments source side of the parallel data. This can be seen as a further sign of the fact that the translation task is being better modeled. 5.1. Baseline The baseline system is built upon the open-source MT toolkit Moses [9]. Phrase pairs are extracted from symmetrized word alignments generated by GIZA++ [10]. The phrase-based translation model provides direct and inverted frequency-based and lexical-based probabilities for each phrase pair included in a given phrase table. The decoder features a statistical log-linear model including a phrase-based translation model, a 5-gram language model, a lexicalized distortion model and word and phrase penalties. Distortion limit is 6 by default. The weights of the log-linear combination are optimized by means of a minimum error training procedure [11] run on IWSLT09’s devset 1 using only the gold reference translation. Evaluation is performed on devset 2."
2009.iwslt-papers.1,W07-0704,0,0.431869,")m kol + (I)m g¨oz + (I)m kafa + (I)m → → → → → sac¸ım elim kolum g¨ozum ¨ kafam ‘my hair’ ‘my hand’ ‘my arm’ ‘my eye’ ‘my head’ Table 2: Different surface forms of possessive suffix -(I)m. If we envisage treating suffixes as single tokens, we foresee an additional increase of data sparseness due to suffix allomorphy. To cope with this problem, we need to introduce an abstract notation that factorizes different surface forms of the same suffix into one single form. much simpler than those for Turkish, given that the number of involved clitics and suffixes is typically smaller1 . As pointed by [2], Turkish employs about 30,000 root words and about 150 distinct suffixes. Altough not all possible suffix combinations are grammatical, the number of potential inflected/derived forms of a given root word is still extremely high. This implies that linguistic knowledge becomes crucial to guide the investigation of meaningful segmentation schemes among all possible rule combinations. Another difference with respect to [3] is that in our work we consider not only splitting but also removing suffixes. In previous editions of IWSLT, [4] and [5] tried to further decrease the out-of-vocabulary rate"
2009.iwslt-papers.1,2007.iwslt-1.27,0,\N,Missing
2009.iwslt-papers.1,P03-1021,0,\N,Missing
2009.iwslt-papers.1,2008.iwslt-evaluation.10,0,\N,Missing
2009.iwslt-papers.5,C04-1059,0,\N,Missing
2009.iwslt-papers.5,J93-2003,0,\N,Missing
2009.iwslt-papers.5,J96-1002,0,\N,Missing
2009.iwslt-papers.5,P07-2045,1,\N,Missing
2009.iwslt-papers.5,W04-3225,0,\N,Missing
2009.iwslt-papers.5,D07-1036,0,\N,Missing
2009.iwslt-papers.5,W07-0733,0,\N,Missing
2009.iwslt-papers.5,N07-1064,0,\N,Missing
2009.iwslt-papers.5,P02-1038,0,\N,Missing
2009.iwslt-papers.5,J03-1002,0,\N,Missing
2009.iwslt-papers.5,W07-0722,0,\N,Missing
2009.iwslt-papers.5,2005.iwslt-1.8,0,\N,Missing
2009.iwslt-papers.5,P03-1021,0,\N,Missing
2010.iwslt-evaluation.1,2005.iwslt-1.19,0,0.0489962,"Missing"
2010.iwslt-evaluation.1,W07-0718,0,0.0381577,"tic evaluation scores were also calculated for case-insensitive (lower-case only) MT outputs with punctuation marks removed (no_case+no_punc). 2.4. Evaluation Specifications In this section, we summarize the subjective and automatic evaluation metrics used to assess the translation quality of the primary run submissions. 2.4.1. Subjective Evaluation Human assessments of translation quality were carried out using the Ranking metrics. For the Ranking evaluation, human graders were asked to “rank each whole sentence translation from Best to Worst relative to the other choices (ties are allowed)” [5]. The Ranking evaluation was carried out using a web-browser interface and graders had to order up to five system outputs by assigning a grade between 5 (best) and 1 (worse). This year’s evaluations were carried out by paid evaluation experts, i.e., three graders for each of the target languages. The Ranking scores were obtained as the average number of times that a system was judged better than any other system. In addition, normalized ranks (NormRank) on a per-judge basis using the method of [6] were calculated for each run submission. The Ranking metric was applied to all submitted primary"
2010.iwslt-evaluation.1,2006.amta-papers.25,0,0.0771934,"Missing"
2010.iwslt-evaluation.1,zhang-etal-2004-interpreting,0,0.0672413,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.2,0,0.0412904,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.4,0,0.0757522,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.20,0,0.0355769,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.21,0,0.0337253,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.23,0,0.0323627,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.11,0,0.0546484,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.26,0,0.0352774,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.28,0,0.039409,"Missing"
2010.iwslt-evaluation.5,N03-1017,0,0.00258058,"ish-English. The combination of several Turkish segmentation schemes into a lattice input led to an improvement wrt to last year. The use of additional training data was explored for Arabic-English, while on the English to French task improvement was achieved over a strong baseline by automatically selecting relevant and high quality data from the available training corpora. 1. BTEC task Turkish and Arabic are morphologically rich languages. When dealing with a small scale task such as the BTEC, this characteristic can have a particularly negative impact on phrase-based statistical MT methods [1]. Following last year’s findings [2] we decided to continue working on the problem of out-of-vocabulary words (OOVs) using different strategies. In the Arabic-English pair we tested the usefulness of additional resources by decoding with multiple phrase-tables. As for Turkish-English, we enriched our morphological segmentation rule set and combined several segmentation schemes inside a word lattice. In order to further improve the coverage of the models on the test, we then tried to refine the lexical approximation technique developed last year. 1.1. Arabic-English The experience of last year"
2010.iwslt-evaluation.5,P08-1115,0,0.0847327,"fixes replaced by tags as explained in [4]. The rules for suffix splitting or removal are then applied according to the selected segmentation scheme. It can be seen that in some cases the new rules allow for a better correspondence at the level of words between the Turkish sentence and its English translation. However, this doesn’t always corresponds to an improvement in translation quality (see Table 3). It was shown in [7] that the choice of the optimal segmentation scheme for Arabic-English SMT is not a trivial problem and may depend on several factors such as the training data size. Later [8] obtained considerable gains in translation quality by combining unsegmented and segmented Arabic test sentences into a lattice. Given these findings and given that the segmentation space of Turkish is even richer 1o ¨ ks¨ur¨uk: ‘cough’, P1sg: 1st person singular possessive, Acc: accusative, dur-: ‘stop’, Caus: causative, Able: ability, Neg: negation, Prog1: present progressive, A1sg: 1st person singular subject suffix. segmentation MS11 MS13 MS14 MS15 MS11+13+15 BLEU – NIST 60.30 – 9.367 58.98 – 9.357 57.76 – 9.373 60.32 – 9.575 60.41 – 9.650 1.3. Evaluation results and discussion All our sys"
2010.iwslt-evaluation.5,P07-2045,1,0.0268624,"Missing"
2010.iwslt-evaluation.5,P03-1021,0,0.0310921,"Missing"
2010.iwslt-evaluation.5,N04-4038,0,0.322573,"Missing"
2010.iwslt-evaluation.5,2009.iwslt-papers.1,1,0.928397,"ide) of newswire parallel text. Multiple phrase-table decoding was handled by the Moses decoder [9] in the ‘either’ mode, that is for each phrase the union of translation options coming from all the tables is considered. 1.2. Turkish-English Turkish morphology is agglutinative, which implies that the vocabulary is built by a wide range of basic suffix combinations. Thus it often occurs that a Turkish word is aligned with an English phrase, and sometimes even to a whole sentence as in the following example: oda odam odamda odamdayım ‘room’ ‘my room’ ‘in my room’ ‘I am in my room’ Previous work [4] has shown that selectively splitting and removing suffixes from the Turkish text used to train a phrase-base SMT system considerably boosts performances 53 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 in a small scale task like the BTEC. The best segmentation scheme reported in that paper (MS11) mainly includes rules for nominal case and possessive suffixes, plus a few rules on verbal suffixation, namely the splitting of the copula and of the person subject suffixes. In order to better address the rich verbal morphology we adde"
2010.iwslt-evaluation.5,N06-2013,0,0.0267942,"make my cough stop) Table 21 illustrates the segmentation process: the Turkish text is first morphologically analysed and disambiguated ([5], [6]) and the surface form of suffixes replaced by tags as explained in [4]. The rules for suffix splitting or removal are then applied according to the selected segmentation scheme. It can be seen that in some cases the new rules allow for a better correspondence at the level of words between the Turkish sentence and its English translation. However, this doesn’t always corresponds to an improvement in translation quality (see Table 3). It was shown in [7] that the choice of the optimal segmentation scheme for Arabic-English SMT is not a trivial problem and may depend on several factors such as the training data size. Later [8] obtained considerable gains in translation quality by combining unsegmented and segmented Arabic test sentences into a lattice. Given these findings and given that the segmentation space of Turkish is even richer 1o ¨ ks¨ur¨uk: ‘cough’, P1sg: 1st person singular possessive, Acc: accusative, dur-: ‘stop’, Caus: causative, Able: ability, Neg: negation, Prog1: present progressive, A1sg: 1st person singular subject suffix. s"
2010.iwslt-evaluation.5,2010.iwslt-papers.3,1,0.882252,"Missing"
2010.iwslt-evaluation.5,J03-1002,0,0.00410318,"Missing"
2010.iwslt-evaluation.5,P07-1019,0,0.0308228,"Missing"
2010.iwslt-papers.10,W10-1710,1,0.863634,"Missing"
2010.iwslt-papers.10,S10-1021,0,0.0227125,"n occur anywhere in the text, even in a different sentence. Since a given input word can be translated with different words in the target language and the pronoun must agree with the word that was actually chosen, correct pronoun choice depends on a translation decision taken earlier by the Machine Translation system. Our model attempts to face this challenge by explicitly identifying anaphoric links in the SMT input and measuring in the output how well the translation of an anaphoric pronoun matches the translation of its antecedent. We used the open-source coreference resolution system BART [8] to link pronouns to their antecedents in the text. The preliminary case study described in the preceding section was about German-English translation. In our practical experiments, we worked on the inverse translation direction, English-German, because we had ready access to an English coreference resolver. The coreference resolution system we used was trained on the ACE02-npaper corpus and uses separate models for pronouns and non-pronouns in order to increase pronounresolution performance. For each resolvable pronoun, the system finds a link to exactly one direct antecedent noun phrase. In"
2010.iwslt-papers.10,P07-2045,1,0.0209518,"sentence are satisfied, the sentence is put on the queue. The actual implementation is multi-threaded and feeds a number of parallel decoder processes. The decoder input queue is realised as a priority queue ordered by the number of incoming dependencies of the sentences in order to resolve as many dependencies as possible as early as possible and thus increase the throughput of the system. Since the sentences are not processed in order, a final ordering step restores the original document order. Coreference information was integrated into an SMT system based on the phrase-based Moses decoder [9] in the form of a new model which represents dependencies between pairs of target-language words produced by the MT system. The decoder driver encodes the links found by the coreference resolver in the input passed to the SMT decoder. Pronouns and their antecedents are marked as illustrated in the lower half of figure 2. Each token is annotated with a pair of elements. The first part numbers the antecedents to which there is a reference in the same sentence. The second part contains the number of the sentence-internal antecedent to which this word refers, or the word itself, if it occurred in"
2010.iwslt-papers.10,P02-1038,0,0.0123676,"using the word forms for our word dependency model, we map the antecedent words to a tag representing their gender and number; thus, in the example, the word hospital in the first sentence, which is translated by the system into the neuter singular word Krankenhaus (not shown), gets mapped to the tag neut_sg in the input for sentence 2. Gender and number of German words were annotated using the RFTagger [?]. The representation of the referent words, by contrast, is fully lexicalised. The word dependency module is integrated as an additional feature function in a standard log-linear SMT model [10]. It keeps track of pairs of source words (sant , sref ) participating as antecedent and referent in a coreference link. Usually, the antecedent sant will be processed first; however, it is also possible for the referent sref to be encountered first, either because of a cataphoric link in the source sentence or, more likely, because of word reordering during decoding. When the second element in an antecedent-referent pair is translated, the word dependency module adds a score of the following form: p(Tref |Tant ) = max (tref ,tant )∈Tref ×Tant p(tref |tant ), (1) where Tref is the set of targe"
2010.iwslt-papers.10,W01-1408,0,0.0133752,"he staph. The same hospital had had to contend with a similar infection early this year . It|*-&gt;neut_sg had discharged a patient admitted after a serious traffic accident . Shortly afterward , it|*-&gt;neut_sg had to re-admit the patient because of an MRSA infection , and doctors|1-* have been unable to perform surgery that would be vital to full recovery because they|*-1 have been unable to get rid of the staph . Figure 2: Coreference link annotation and decoder input is another search path that is superior under every possible continuation of the search. This is called hypothesis recombination [11]. Since our model introduces dependencies that can span large parts of the sentence, care must be taken not to recombine hypotheses that could be ranked differently after including the word dependency scores. We therefore extend the decoder search state to include, on the one hand, the set of antecedents already processed and, on the other hand, the set of referents encountered for which no antecedent has been seen yet. In either case, the translation chosen by the decoder is stored along with the item. Hypotheses can only be recombined if both of these sets match. 4.4. Word-Dependency Model:"
2010.iwslt-papers.10,J03-1002,0,0.0236175,"addresses a specific problem of the MT system, evaluation with a general-purpose score such as BLEU may not be fully adequate. Besides measuring overall translation quality, which is what general-purpose measures purport to do, we also want to know details about the impact of the new model on pronoun translation. We therefore propose a method to measure precision and recall of pronoun translations more directly. We use a test corpus with one reference translation, for which we construct word alignments by concatenating it with additional parallel training data, running the GIZA++ word aligner [13] and symmetrising the alignments as is usually done for SMT system training. We also produce word alignments between the source text and the candidate translation by considering the phrase-internal word alignments stored in the phrase table. The basic idea is to count the number of pronouns translated correctly. Doing so would require a 1 : 1 mapping from pronouns to their translations. However, word alignments can link a word to zero, one or more words, so we suggest using a measure based on precision and recall instead. For every pronoun occurring in the source text, we obtain the set of ali"
2010.iwslt-papers.10,P02-1040,0,0.080021,"the source text and the candidate translation by considering the phrase-internal word alignments stored in the phrase table. The basic idea is to count the number of pronouns translated correctly. Doing so would require a 1 : 1 mapping from pronouns to their translations. However, word alignments can link a word to zero, one or more words, so we suggest using a measure based on precision and recall instead. For every pronoun occurring in the source text, we obtain the set of aligned target words in the reference and the candidate translation, R and C, respectively. Inspired by the BLEU score [14], we define the clipped count of a particular candidate word w as the number of times it occurs in the candidate set, limited by the number of times it occurs in the reference set: cclip (w) = min (cC (w), cR (w)) (2) We then consider the match count to be the sum of the clipped counts over all words in the candidate translation aligned to pronouns in the source text, which allows us to define precision and recall in the usual way: Precision = ∑ cclip (w) w∈C |C| ; Recall = ∑ cclip (w) w∈C |R| (3) This measure can be applied both to obtain a comprehensive score for a particular system on a tes"
2010.iwslt-papers.10,W10-1737,0,0.579477,"Missing"
2010.iwslt-papers.10,J90-2002,0,\N,Missing
2010.iwslt-papers.10,W09-0401,0,\N,Missing
2010.iwslt-papers.3,W04-3208,0,0.167575,"Missing"
2010.iwslt-papers.3,J05-4003,0,0.17025,"al approaches have recently been proposed in the literature to extract parallel excerpts. Most of the techniques, if not all, share the stages of splitting documents into sentences and of pairing sentences across documents. Methods significantly differ in the successive filtering steps, that can be clustered into two main groups: procedures aiming at deciding if paired sentences are mutual translations or at extracting parallel sub-sentential fragments. In the following, we briefly describe a few works tightly related to our approach, which also report significant performance improvements. In [2] words in the source documents are translated through a bilingual lexicon; all possible sentence pairs of the two documents are passed first through a word-overlap filter and then classified as parallel or not by a maximum entropy classifier trained on (a small amount of) parallel sentences. In [3] a SMT system, trained on a small amount of parallel data, is used to directly translate the source side of the documents. Then, instead of the maximum entropy classifier, WER and TER scores are computed at the sentence level by comparing the translations with the target side; the amount of filtered"
2010.iwslt-papers.3,E09-1003,0,0.197294,"clustered into two main groups: procedures aiming at deciding if paired sentences are mutual translations or at extracting parallel sub-sentential fragments. In the following, we briefly describe a few works tightly related to our approach, which also report significant performance improvements. In [2] words in the source documents are translated through a bilingual lexicon; all possible sentence pairs of the two documents are passed first through a word-overlap filter and then classified as parallel or not by a maximum entropy classifier trained on (a small amount of) parallel sentences. In [3] a SMT system, trained on a small amount of parallel data, is used to directly translate the source side of the documents. Then, instead of the maximum entropy classifier, WER and TER scores are computed at the sentence level by comparing the translations with the target side; the amount of filtered pairs can be tuned by varying the acceptance threshold. The approach in [4] resembles [2] up to the definition of the set of candidate sentence pairs. Instead of deciding whether the two sentences are mutual translations, now they search for parallel fragments using an approach inspired by signal p"
2010.iwslt-papers.3,P06-1011,0,0.49035,"n; all possible sentence pairs of the two documents are passed first through a word-overlap filter and then classified as parallel or not by a maximum entropy classifier trained on (a small amount of) parallel sentences. In [3] a SMT system, trained on a small amount of parallel data, is used to directly translate the source side of the documents. Then, instead of the maximum entropy classifier, WER and TER scores are computed at the sentence level by comparing the translations with the target side; the amount of filtered pairs can be tuned by varying the acceptance threshold. The approach in [4] resembles [2] up to the definition of the set of candidate sentence pairs. Instead of deciding whether the two sentences are mutual translations, now they search for parallel fragments using an approach inspired by signal processing. Using a set of parameters derived from Log-Likelihood-Ratio statistics, each word is annotated with values in [−1, +1] indicating the likelihood that the word has some translations in the other sentence by performing a greedy alignment. This stream of values is then treated as a signal and passed through an averaging filter. Spans that have only positive signal v"
2010.iwslt-papers.3,2007.mtsummit-papers.50,0,0.391793,"ameters derived from Log-Likelihood-Ratio statistics, each word is annotated with values in [−1, +1] indicating the likelihood that the word has some translations in the other sentence by performing a greedy alignment. This stream of values is then treated as a signal and passed through an averaging filter. Spans that have only positive signal values and are longer than a threshold (3 words) are considered more likely to have a translation on the other side. The same process is repeated on the other translation direction, and the resulting fragment pair is assumed to be parallel. Quirk et al. [5] try to overcome some of the limitation of the approach described in [4] in the way parallel fragments are identified. In particular, they propose two generative models for generating noisy target sentences from the source sentences. One model is employed to align words in candidate sentence pairs; fragments are then extracted from alignments by applying simple heuristics. Another model tries to directly generate fragments; in the process, three generation options are competing: source-only fragment, targetonly fragment, or joint source-target fragment. The rational behind this model is that “"
2010.iwslt-papers.3,W04-3225,0,0.225101,"Missing"
2010.iwslt-papers.3,W07-0733,0,0.108535,"Missing"
2010.iwslt-papers.3,2009.mtsummit-posters.17,0,0.0274716,"Missing"
2010.iwslt-papers.3,D07-1103,0,0.0280528,"eam search, histogram pruning...) are omitted, but implemented. Some aspects of the algorithm deserve to be highlighted and commented. First of all, translation probabilities are not used at all. This allows the exploitation of any phrase pair repository, even lacking of probabilities (like multiwordnets), and prevents hypotheses built on low probability phrase pairs from cutting by the beam search. In our specific case, the SMT phrase table used as repository is likely to be noisy. In order to have the repository as clean as possible, the phrase table is pruned via the algorithm described in [10]. Secondly, the use of a phrase-based translation model allows us to cover phrases instead of single words, differently from what is done in similar approaches (e.g. [5] but also [4]). This way the job done in SMT training for discovering reliable phrase pairs is exploited: if such pairs occur in the bilingual input text, they represent a solid anchor for the fragment extraction. Finally, it is worth to noticing that the algorithm permits partial alignment: portions of either source or target texts can remain unaligned. This is achieved by (i) adding dummy translation options (i.e. a target ph"
2010.iwslt-papers.3,2005.mtsummit-papers.11,0,0.0449093,"ble 1: Statistics of the De-En/Ar-En translation/reordering models: size of training texts (running words), dictionary size, and number of phrase pairs in the baselines. from a corpus already cleaned at the sentence level. 5.1. Data Experiments were conducted on two different tasks and language pairs. In the first task, German news are translated into English (De-En) according to the setup established in the Workshop on Statistical Machine Translation of the ACL 2010.2 Parallel training data consist of a small in-domain corpus (News Commentary - NC) and a larger out-of-domain corpus (Europarl [11], version 5 - EP). News-test2008 has been used for development, while news-test2009 (TST09) and news-test2010 (TST10) for testing purposes. As comparable data, we used a set of 25,517 bilingual documents downloaded from the multilingual and pan-European television news channel EuroNews (EN),3 for a total of 4.3 million and 4.7 million German and English words, respectively. The second task involves the translation of news from Arabic into English (Ar-En) in the framework defined by the 2009 NIST evaluation campaign.4 In this case, for development and testing purposes the portions containing ne"
2010.iwslt-papers.3,P07-2045,1,0.010614,"and of both 2008 and 2009 evaluation sets have been employed. We used only one of the parallel resources allowed for the constrained training condition, namely the ISI Arabic-English Automatically Extracted Parallel Text (LDC2007T08). It consists of sentence pairs extracted automatically from the Arabic and English monolingual Gigaword corpora by means of the method described in [2]. For each sentence pair, a confidence score (between 0.5 and 1.0) is provided, which is indicative of its degree of parallelism. 5.2. Baselines The baseline systems are built upon the open-source MT toolkit Moses [12].5 The translation and the lexicalized reordering models have been trained on NC for De-En, and on the subset of ISI corpus containing sentences with confidence score larger than 0.993 (ISI-0.993), for Ar-En. Phrase tables are pruned according to [10]. In all experiments, 6gram LMs have been employed, smoothed with the improved Kneser-Ney technique [13] and computed with the IRSTLM 2 www.statmt.org/wmt10/ 3 www.euronews.net 4 www.itl.nist.gov/iad/mig/tests/mt/2009/ 5 www.statmt.org/moses/ 231 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3"
2010.iwslt-papers.3,P96-1041,0,0.144766,"Missing"
2011.eamt-1.34,E09-1003,0,0.0205869,"entences across documents. Methods significantly differ in the successive filtering steps, and can be clustered into two main groups: procedures aiming at deciding if paired sentences are mutual translations or at detecting and extracting parallel sub-sentential fragments. In (Munteanu and Marcu, 2005) words in the source documents are translated through a bilingual lexicon; all possible sentence pairs of the two documents are passed first through a word-overlap filter and then classified as parallel or not by a maximum entropy classifier trained on (a small amount of) parallel sentences. In (Abdul-Rauf and Schwenk, 2009) a SMT system, trained on a small amount of parallel data, is used to directly translate the source side of the documents. Then, instead of the maximum entropy classifier, WER and TER scores are computed at the sentence level by comparing the translations with the target side; the amount of filtered pairs can be tuned by varying the acceptance threshold. The approach in (Munteanu and Marcu, 2006) resembles (Munteanu and Marcu, 2005) up to the definition of the set of candidate sentence pairs. Instead of deciding whether the two sentences are mutual translations, now they search for parallel fr"
2011.eamt-1.34,C10-2010,0,0.0308503,"the estimation of SMT models. type web parallel sent. web parallel frag. total total clean web monol. sent. |W | ar it 1.4M 1.4M 1.8M 1.6M 3.0M 3.0M 2.8M 1.06G trained models LM TM RM LM Table 5: Training data for ArIt models. English-to-Italian (EnIt) Unlike in the case of the Arabic–Italian pair, many Web sites publish news in English and Italian which are (almost) parallel. We have been able to collect so far sentences for a total of about 24M words per side. Document pairs have been split into sentences on strong punctuation and sentence alignment has been performed by means of Gargantua (Braune and Fraser, 2010). On the contrary, from sites where documents are at most comparable, fragments (2.8M words) have been mined via the procedures described in Sections 4.1 and 4.2. In addition, Europarl8 and JRC-Acquis9 parallel corpora (70M words) have been employed. The LM of the ArIt system has been re-used. Table 6 provides some statistics on texts employed for the estimation of these specific SMT models. type web parallel sent. web parallel frag. total total clean ep5+acquis clean web monol. sent. |W | en it 24.2M 24.1M 2.7M 2.8M 27.0M 23.3M 23.5M 70.0M 70.0M 1.06G trained models LM TM RM TM RM LM Devsets"
2011.eamt-1.34,2010.iwslt-papers.3,1,0.845686,"te sentence pairs; fragments are then extracted from alignments by applying simple heuristics. Another model tries to directly generate fragments; in the process, three generation options are competing: source-only fragment, target-only fragment, or joint source-target fragment. The rational behind this model is that “the probability of generating source and target fragments jointly should be more likely than generating them independently if and only if they are parallel”. The latter model is definitely more complex than the former one, although their impact on SMT performance is similar. In (Cettolo et al., 2010), a method is defined for extracting fragments which was new in some aspects and that is summarized in Section 4.2. 3 Evaluation Sets Evaluation and comparison of MT systems are still open issues to which much attention and many efforts have been directed in the last years. One of the few certainties is the use of publicly available evaluation sets. Nowadays, several MT evaluation campaigns are held every year which release evaluation sets. Unfortunately, no evaluation set built on news texts has been made available so far for Arabic–Italian translation. To fill this gap, we decided to add two"
2011.eamt-1.34,P07-2045,1,0.0102695,"Missing"
2011.eamt-1.34,J05-4003,0,0.0790055,"t al., 2010). Once document pairs are available, the problem is the detection of actual parallel text inside them. Several approaches have recently been proposed in the literature to extract such parallel excerpts. Most of the techniques, if not all, share the stages of splitting documents into sentences and of pairing sentences across documents. Methods significantly differ in the successive filtering steps, and can be clustered into two main groups: procedures aiming at deciding if paired sentences are mutual translations or at detecting and extracting parallel sub-sentential fragments. In (Munteanu and Marcu, 2005) words in the source documents are translated through a bilingual lexicon; all possible sentence pairs of the two documents are passed first through a word-overlap filter and then classified as parallel or not by a maximum entropy classifier trained on (a small amount of) parallel sentences. In (Abdul-Rauf and Schwenk, 2009) a SMT system, trained on a small amount of parallel data, is used to directly translate the source side of the documents. Then, instead of the maximum entropy classifier, WER and TER scores are computed at the sentence level by comparing the translations with the target si"
2011.eamt-1.34,P06-1011,0,0.0293232,"rs of the two documents are passed first through a word-overlap filter and then classified as parallel or not by a maximum entropy classifier trained on (a small amount of) parallel sentences. In (Abdul-Rauf and Schwenk, 2009) a SMT system, trained on a small amount of parallel data, is used to directly translate the source side of the documents. Then, instead of the maximum entropy classifier, WER and TER scores are computed at the sentence level by comparing the translations with the target side; the amount of filtered pairs can be tuned by varying the acceptance threshold. The approach in (Munteanu and Marcu, 2006) resembles (Munteanu and Marcu, 2005) up to the definition of the set of candidate sentence pairs. Instead of deciding whether the two sentences are mutual translations, now they search for parallel fragments using an approach inspired by signal processing. Using a set of parameters derived from Log-Likelihood-Ratio statistics, each word is annotated with values in [−1, +1] indicating the likelihood that the word has some translations in the other sentence by performing a greedy alignment. This stream of values is then treated as a signal and passed through an averaging filter. Spans that have"
2011.eamt-1.34,2007.mtsummit-papers.50,0,0.0176098,"meters derived from Log-Likelihood-Ratio statistics, each word is annotated with values in [−1, +1] indicating the likelihood that the word has some translations in the other sentence by performing a greedy alignment. This stream of values is then treated as a signal and passed through an averaging filter. Spans that have only positive signal values and are longer than a threshold (3 words) are considered more likely to have a translation on the other side. The same process is repeated on the other translation direction, and the resulting fragment pair is assumed to be parallel. Quirk et al. (Quirk et al., 2007) try to overcome some of the limitations of the approach described in (Munteanu and Marcu, 2006) in the way paralset #sentences eval08-NW eval09-NW 813 586 Arabic |W | |V | 21.9k 7.8k 17.5k 6.4k English |W | |V | 29.1k 4.9k 23.1k 3.9k French |W | |V | 33.2k 4.9k 26.7k 4.4k Italian |W | |V | 32.0k 5.7k 25.1k 4.8k Table 1: Statistics of the dev/test sets for the pivoting task. Texts are tokenized. For the English side, 4 manual translations (references) are available: average values are reported. |W |stands for “running words”, |V |for “vocabulary size”. lel fragments are identified. In particul"
2011.eamt-1.34,C10-1124,0,0.0120521,"ng the same content in different languages. Although these documents are not parallel, it often happens that some portions of them are mutual translations to some extent. In the last years, much effort has been devoted by the research community to the effective exploitation of such data for SMT. The first problem to face is the alignment of multilingual documents reporting the same news. This problem has been rarely investigated systematically, as the usual pairing strategies aim to keep low the missing rate, that is to reward the recall. One of the most valuable methods is that presented in (Uszkoreit et al., 2010). There, all non-English documents are translated into English through a initial, even low-quality, translation system. Documents are then paired in two steps: the first generates a set of candidate pairs of documents sharing at least a certain number of rare features. This step is made linear in the number of input documents by setting a threshold defining such rare features. In the second step, a computationally more expensive and fine grained comparison is performed for deciding whether such document pairs are comparable or not. Pairing multilingual documents is also the goal of (Steinberge"
2011.eamt-1.34,P09-1018,0,0.145031,"el; on the target side, specific LMs were estimated as well; on the contrary, a single reordering model was built on such texts. Concerning the exploitation of monolingual data, again specific LMs were built on each of the 6 provided sets (English Gigaword 3rd Edition); also in this case, a single model has been defined by linearly interpolating the total of 11 LMs. Table 7 provides some statistics on texts actually employed for the estimation of SMT models. |W | ar/en 0.6M 0.9M 6.2M 24.3M 115.2M 142.2M 3.6G 5.2 The pivoting technique employed here is the composition, also called transfer in (Wu and Wang, 2009), consisting in the translation of the source language into the pivot language, and of this into the target language. Table 8 shows the performance obtained by composing the proper direct systems presented above, together with that by composing the two corresponding Google Translate systems. Table 6: Training data for EnIt models. corpus of-the-art SMT systems is possible (English-toItalian and Arabic-to-English directions); otherwise (Arabic-to-Italian) the need arises for alternative approaches to achieve acceptable quality. www.statmt.org/europarl wt.jrc.it/lt/Acquis/ 254 ArEn ⊗ EnIt ArEn-g"
2011.eamt-1.34,2010.iwslt-evaluation.1,1,\N,Missing
2011.iwslt-evaluation.1,P02-1040,0,0.111082,"4.2. In addition, for development purposes, ASR outputs for the IWSLT 2010 development and test sets were also made available to participants. 3.3. Evaluation Specifications The participants had to provide the result of the translation of the English audio in NIST XML format. The output had to be true-cased and had to contain punctuation. The participants could either use the audio files directly, or use the output— either first best hypotheses in CTM format or word lattices in SLF — of KIT, LIUM, and FBK from the ASR task. The quality of the translations was measured automatically with BLEU [1] by scoring against the human translations created by the TED open translation project, and by human subjective evaluation (paired comparison, Section 7). Since the reference translations from the TED website match the segmentation of the reference transcriptions of the talks, 12 automatic evaluation scores for the MT outputs could be directly computed. The evaluation specifications for the SLT task were defined as case-sensitive with punctuation marks (case+punc). Tokenization scripts were applied automatically to all run submissions prior to evaluation. Moreover, automatic evaluation scores"
2011.iwslt-evaluation.1,W07-0734,0,0.031722,"Missing"
2011.iwslt-evaluation.1,niessen-etal-2000-evaluation,0,0.0557796,"Missing"
2011.iwslt-evaluation.1,2006.amta-papers.25,0,0.148776,"Missing"
2011.iwslt-evaluation.1,2003.mtsummit-papers.51,0,0.142051,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.8,0,0.040341,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.7,0,0.030857,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.10,0,0.0773305,"Missing"
2011.iwslt-evaluation.1,2011.mtsummit-papers.59,1,0.858689,"xception of sentences with less than 5 words, which were excluded from the subjective evaluation. The IWSLT 2011 subjective evaluation focused solely on the Ranking task10 and a number of novelties were introduced with respect to the traditional system ranking evaluation carried out in previous campaigns. Firstly, this year’s evaluation was not carried out by hired expert graders but by relying on crowdsourced data. The feasibility of using crowdsourcing methodologies as an effective way to reduce the costs of MT evaluation without sacrificing quality was investigated in a previous experiment [23], where the ranking evaluation of the IWSLT 2010 Arabic-English BTEC task was replicated by hiring non-experts through Amazon’s Mechanical Turk. The analysis of the collected data showed that agreement rates for non-experts were comparable to those for experts, and that the crowd-based system ranking had a very strong correlation with expert-based ranking. Secondly, the cost reduction obtained by using crowdsourcing allowed us to focus on modifying and extending the ranking methodology in different respects, with the aim of maximizing the overall evaluation reliability. The goal of the Ranking"
2011.iwslt-evaluation.1,W07-0718,0,0.0308834,"uation is to produce a complete ordering of the systems participating in a given task. The ranking task requires human judges to decide whether one system output is better than another for a given source sentence. The judgments collected through these comparisons are used to obtain the ranking scores, which are calculated as the average number of times that a system was judged better than any other system. Traditionally, in the ranking task, the judge was presented with the output of five submissions for a given source sentence and was asked to rank them from best to worst (ties were allowed) [24]. Each evaluation block contained the implicit pairwise comparisons (i.e. each system against the other systems presented in the same block) which constituted the basis of the ranking scores. 10 Last year human evaluation was also carried out for the Fluency and Adequacy metrics. In the following sections we analyze the data that we collected by posting the ranking task on Amazon’s Mechanical Although ranking a number of translated sentences relative to each other is quite intuitive, a 5-fold ranking task is less reliable than a direct comparison between only two translated sentences due to th"
2011.iwslt-evaluation.1,D09-1030,0,0.0421625,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.6,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.3,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.9,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.13,0,\N,Missing
2011.iwslt-evaluation.1,zhang-etal-2004-interpreting,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.4,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.5,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.11,0,\N,Missing
2011.iwslt-evaluation.18,W08-0320,0,0.577452,"eavily depends on the quantity of the available training material. The strain to balance these contrasting needs has motivated a large body of work in domain adaptation. In this work, we aim at increasing the coverage of a small but precise in-domain model. To this end, we assume that all the information coming from our primary source should be preserved as is, and use the secondary sources only to ‘fill the gaps’. The idea of fill-up goes back to Besling and Meier [1], which addressed the problem of language model adaptation for speech recognition, and was recently introduced in SMT by Nakov [2]. The original method was conceived by [1] to train speaker-dependent dictation systems and proved to outperform classical linear interpolation. In that context, the primary source of information was, naturally, the set of sentences uttered by a given speaker, as opposed to all the others. The SMT scenario that we are addressing is of course different, but here we can use our prior knowledge of the task to make assumptions on the relevance of the available corpora. For a practical example, consider the TED1 talks translation task [3]. The training material provided for the IWSLT11 1 http://www"
2011.iwslt-evaluation.18,eck-etal-2004-language,0,0.0652307,"can thus obtain models that are less redundant and easier to tune. The rest of this paper is organized as follows. After a review of relevant work, we describe in detail the fill-up technique and present possible refinements and extensions. In the experimental section, we apply the fill-up technique to two TED translation tasks and compare it with the two most popular methods for phrase table combination: linear and loglinear interpolation. 2. Previous Work Previous work on domain adaptation for SMT has focused on techniques for selecting parallel or monolingual in-domain training data (e.g. [4]), as well as methods for combining models trained independently on in-domain and on outof-domain data. Given the scope of our work, we review here only approaches for combining in-domain and out-ofdomain (background) translation models. Existing approaches combine different sources either at the data level or at phrase-table level. Adaptation at phrasetable level is either done off-line, typically by a linear mixture of weights, or at decoding-time through a log-linear combination. In the former case, a generative model and maximum likelihood estimation are employed; in the latter case weight"
2011.iwslt-evaluation.18,W07-0717,0,0.893608,"review here only approaches for combining in-domain and out-ofdomain (background) translation models. Existing approaches combine different sources either at the data level or at phrase-table level. Adaptation at phrasetable level is either done off-line, typically by a linear mixture of weights, or at decoding-time through a log-linear combination. In the former case, a generative model and maximum likelihood estimation are employed; in the latter case weights of the log-linear interpolation are typically learned discriminatively by directly optimizing the performance of the SMT decoder. In [5] a mixture-model approach is proposed, with weights depending on some text distances between in136 domain data and the mixture components. The authors explored different choices: cross-domain and dynamic adaptation; linear and log-linear mixtures; different text distance metrics and methods to map them to linear mixture weights. For log-linear mixtures, weights were estimated globally with the other features of the phrase-based model, through minimum error rate training [6]. Notice that the employed system used a relatively small number of features: two probabilities for each phrase table, one"
2011.iwslt-evaluation.18,P03-1021,0,0.270727,"linear interpolation are typically learned discriminatively by directly optimizing the performance of the SMT decoder. In [5] a mixture-model approach is proposed, with weights depending on some text distances between in136 domain data and the mixture components. The authors explored different choices: cross-domain and dynamic adaptation; linear and log-linear mixtures; different text distance metrics and methods to map them to linear mixture weights. For log-linear mixtures, weights were estimated globally with the other features of the phrase-based model, through minimum error rate training [6]. Notice that the employed system used a relatively small number of features: two probabilities for each phrase table, one for each language model, a length penalty, and a distortion model. Reported results show improvements by the linear and log-linear mixtures over a baseline trained on the union of all training data. Remarkably, best results with the linear mixture were obtained using uniform weights. In [7] a phrase-based SMT system trained on Europarl data is adapted to the news domain by integrating it with language and translation models, explicitly trained on indomain data. In particul"
2011.iwslt-evaluation.18,W07-0733,0,0.848423,"ds to map them to linear mixture weights. For log-linear mixtures, weights were estimated globally with the other features of the phrase-based model, through minimum error rate training [6]. Notice that the employed system used a relatively small number of features: two probabilities for each phrase table, one for each language model, a length penalty, and a distortion model. Reported results show improvements by the linear and log-linear mixtures over a baseline trained on the union of all training data. Remarkably, best results with the linear mixture were obtained using uniform weights. In [7] a phrase-based SMT system trained on Europarl data is adapted to the news domain by integrating it with language and translation models, explicitly trained on indomain data. In particular, the in-domain phrase-table was added to the global log-linear model. As a difference with [5], phrase-pairs are here scored with four translation probabilities and four reordering probabilities, thus resulting in a significantly larger set of feature weights to be trained. In [8] in-domain and out-of-domain phrase-tables are also combined using a two-component linear mixture. Extensive experiments are repor"
2011.iwslt-evaluation.18,I08-2088,0,0.0632227,"ained on the union of all training data. Remarkably, best results with the linear mixture were obtained using uniform weights. In [7] a phrase-based SMT system trained on Europarl data is adapted to the news domain by integrating it with language and translation models, explicitly trained on indomain data. In particular, the in-domain phrase-table was added to the global log-linear model. As a difference with [5], phrase-pairs are here scored with four translation probabilities and four reordering probabilities, thus resulting in a significantly larger set of feature weights to be trained. In [8] in-domain and out-of-domain phrase-tables are also combined using a two-component linear mixture. Extensive experiments are reported with different data-selection criteria and empirical weight settings. The contribution of the mixture approach is relevant and quite stable within a large interval of weight values, centered around 0.5. Very recently [9] proposed novel data selection criteria to extract “pseudo in-domain” data from a large background parallel corpus which is then used either to train a domainspecific SMT system, or to adapt a generic SMT system via linear and log-linear mixtures"
2011.iwslt-evaluation.18,D11-1033,0,0.294499,"added to the global log-linear model. As a difference with [5], phrase-pairs are here scored with four translation probabilities and four reordering probabilities, thus resulting in a significantly larger set of feature weights to be trained. In [8] in-domain and out-of-domain phrase-tables are also combined using a two-component linear mixture. Extensive experiments are reported with different data-selection criteria and empirical weight settings. The contribution of the mixture approach is relevant and quite stable within a large interval of weight values, centered around 0.5. Very recently [9] proposed novel data selection criteria to extract “pseudo in-domain” data from a large background parallel corpus which is then used either to train a domainspecific SMT system, or to adapt a generic SMT system via linear and log-linear mixtures, similarly to [5] but with a feature set similar to that used in [7]. In the reported experiments, the log-linear method outperformed the linear mixture adaptation method and both methods outperformed the in-domain and generic baselines. In [10] a corpus identifier is introduced to distinguish parallel in-domain data from out-of-domain data in a facto"
2011.iwslt-evaluation.18,2010.eamt-1.29,0,0.0596402,"ch is relevant and quite stable within a large interval of weight values, centered around 0.5. Very recently [9] proposed novel data selection criteria to extract “pseudo in-domain” data from a large background parallel corpus which is then used either to train a domainspecific SMT system, or to adapt a generic SMT system via linear and log-linear mixtures, similarly to [5] but with a feature set similar to that used in [7]. In the reported experiments, the log-linear method outperformed the linear mixture adaptation method and both methods outperformed the in-domain and generic baselines. In [10] a corpus identifier is introduced to distinguish parallel in-domain data from out-of-domain data in a factored translation model. Each target word is assigned an id tag corresponding to the part of the corpus from which it belongs. Three additional translation model features are introduced to compute the probability of corpus id tags being generated given the source phrase, as well as the source and target phrase probabilities, given the corpus id tags. The incorporation of corpus id tags promotes the preference of phrase pairs from a specific domain. Finally, the system description paper [2]"
2011.iwslt-evaluation.18,N03-1017,0,0.0142772,"and background data. This implies word alignment2 , phrase extraction and phrase scoring. In standard adaptation scenarios, background data is augmented with in-domain data; however, in the fill-up case, the background table is merged with the in-domain table by adding only new phrase pairs that do not appear in the in-domain table. Formally, let T1 and T2 be the in-domain and the background phrase tables, respectively. The translation model assigns a feature vector to each phrase pair φ(f˜, e˜), where f˜ and e˜ are respectively the source and target phrases. Namely, in the model we are using [11], five features are defined for each phrase pair: φ(f˜, e˜) = (Pph (˜ e|f˜), Pph (f˜|˜ e), Plex (˜ e|f˜), Plex (f˜|˜ e), pp(f˜|˜ e)) where Pph refers to the phrase translation probability, Plex is the lexical weighting probability, and pp is a constant phrase penalty that serves to adjust the degree of phrase segmentation (typically pp = exp(1)). Then, the filled-up model TF is defined as follows: ∀(f˜, e˜) ∈ T1 ∪ T2 : 8 &gt; &gt; &lt; (φ1 (f˜, e˜), exp(0)) φF (f˜, e˜) = &gt; &gt; : (φ2 (f˜, e˜), exp(1)) if (f˜, e˜) ∈ T1 otherwise The entries of the filled-up model correspond to the union of the two phrase t"
2011.iwslt-evaluation.18,N04-4026,0,0.0610555,"word alignment, this first step can be performed on the concatenation of all corpora, provided that phrase extraction and scoring are carried out separately on each corpus. 3 We apply the exponential function to binary features to neutralize the log function that is applied to all features participating in the log-linear model. 137 ing a way to promote one set of phrase pairs with respect to the other. 3.1. Reordering table fill-up When combining multiple phrase tables, one has generally to deal with phrase reordering models as well. Our system includes a popular lexicalized reordering model [12, 13, 14] whose entries are those of the phrase table trained on the same corpus, and whose features are reordering probabilities with three possible values: monotonic if immediately following the last translated phrase, swap if immediately preceding it or else discontinuous. The phrase table fill-up technique can be seamlessly applied to this type of reordering model, with the only difference that no additional feature is introduced. 3.2. Pruning options We explored several pruning options to limit the new translation model size: • NewSourceMaxLength: set a maximum length for the source side of new (b"
2011.iwslt-evaluation.18,2005.iwslt-1.8,0,0.17682,"word alignment, this first step can be performed on the concatenation of all corpora, provided that phrase extraction and scoring are carried out separately on each corpus. 3 We apply the exponential function to binary features to neutralize the log function that is applied to all features participating in the log-linear model. 137 ing a way to promote one set of phrase pairs with respect to the other. 3.1. Reordering table fill-up When combining multiple phrase tables, one has generally to deal with phrase reordering models as well. Our system includes a popular lexicalized reordering model [12, 13, 14] whose entries are those of the phrase table trained on the same corpus, and whose features are reordering probabilities with three possible values: monotonic if immediately following the last translated phrase, swap if immediately preceding it or else discontinuous. The phrase table fill-up technique can be seamlessly applied to this type of reordering model, with the only difference that no additional feature is introduced. 3.2. Pruning options We explored several pruning options to limit the new translation model size: • NewSourceMaxLength: set a maximum length for the source side of new (b"
2011.iwslt-evaluation.18,D08-1089,0,0.503345,"word alignment, this first step can be performed on the concatenation of all corpora, provided that phrase extraction and scoring are carried out separately on each corpus. 3 We apply the exponential function to binary features to neutralize the log function that is applied to all features participating in the log-linear model. 137 ing a way to promote one set of phrase pairs with respect to the other. 3.1. Reordering table fill-up When combining multiple phrase tables, one has generally to deal with phrase reordering models as well. Our system includes a popular lexicalized reordering model [12, 13, 14] whose entries are those of the phrase table trained on the same corpus, and whose features are reordering probabilities with three possible values: monotonic if immediately following the last translated phrase, swap if immediately preceding it or else discontinuous. The phrase table fill-up technique can be seamlessly applied to this type of reordering model, with the only difference that no additional feature is introduced. 3.2. Pruning options We explored several pruning options to limit the new translation model size: • NewSourceMaxLength: set a maximum length for the source side of new (b"
2011.iwslt-evaluation.18,P07-2045,1,0.0236945,"English and English-to-French. Training and test data were provided by the organizers of the IWSLT11 evaluation, and are summarized in Table 15 . The tuning (dev2010) and test (test2010) sets have one reference translation. Concerning preprocessing we apply standard tokenization to the English and French data, while for Arabic we use our in-house tokenizer that also removes diacritics and normalizes special characters and digits. Arabic text is then segmented with AMIRA [16] according to the ATB scheme6 . For both language pairs, we set up a standard phrasebased system using the Moses toolkit [15]. The decoder features a statistical log-linear model including one or more 4 These observations refer to the Moses decoder [15], but we are not aware of other decoders having a different solution to this problem. 5 Europarl corpus was also available for English-to-French, but we did not use it in our experiments. 6 The Arabic Treebank tokenization scheme isolates conjunctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article Al+. 138 Table 1: IWSLT11 training and test data statistics: number of sentences |S|, number of tokens |W |and average senten"
2011.iwslt-evaluation.18,N04-4038,0,0.0135362,"d of 33. We evaluate fill-up, log-linear and linear interpolation on the TED task, in two different language pairs: Arabic-toEnglish and English-to-French. Training and test data were provided by the organizers of the IWSLT11 evaluation, and are summarized in Table 15 . The tuning (dev2010) and test (test2010) sets have one reference translation. Concerning preprocessing we apply standard tokenization to the English and French data, while for Arabic we use our in-house tokenizer that also removes diacritics and normalizes special characters and digits. Arabic text is then segmented with AMIRA [16] according to the ATB scheme6 . For both language pairs, we set up a standard phrasebased system using the Moses toolkit [15]. The decoder features a statistical log-linear model including one or more 4 These observations refer to the Moses decoder [15], but we are not aware of other decoders having a different solution to this problem. 5 Europarl corpus was also available for English-to-French, but we did not use it in our experiments. 6 The Arabic Treebank tokenization scheme isolates conjunctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article"
2011.iwslt-evaluation.18,D07-1103,0,0.0241895,"1.6 128.3 32.1 0.7 – 2.7 – 0.1 – 1.3 – 2.5 – 0.02 5.1. Arabic to English phrase translation models, target language models, a phrase reordering model [12, 13], distortion, word and phrase penalties. In the Arabic-English task, we use a hierarchical reordering model [14], while in the English-French task we use a default word-based bidirectional extraction model. For each target language, two 5-gram language models are trained independently on the monolingual TED and NEWS datasets, and log-linearly combined at decoding time. The distortion limit is set to the default value of 6. As proposed by [17], statistically improbable phrase pairs are removed by all our phrase tables (before merging). The Arabic-English systems use cased translation models, while the English-French systems use lowercased models and a standard recasing postprocess. Word alignments are computed by GIZA++ [18] on the concatenation of all data. Consequently, phrase extraction and scoring are carried out separately on each corpus. Table 2 provides summary statistics on the phrase overlaps of the NEWS and UN phrase tables with respect to the TED phrase table. Note that, in this work, we do not evaluate the contribution"
2011.iwslt-evaluation.18,J03-1002,0,0.0118468,"glish-French task we use a default word-based bidirectional extraction model. For each target language, two 5-gram language models are trained independently on the monolingual TED and NEWS datasets, and log-linearly combined at decoding time. The distortion limit is set to the default value of 6. As proposed by [17], statistically improbable phrase pairs are removed by all our phrase tables (before merging). The Arabic-English systems use cased translation models, while the English-French systems use lowercased models and a standard recasing postprocess. Word alignments are computed by GIZA++ [18] on the concatenation of all data. Consequently, phrase extraction and scoring are carried out separately on each corpus. Table 2 provides summary statistics on the phrase overlaps of the NEWS and UN phrase tables with respect to the TED phrase table. Note that, in this work, we do not evaluate the contribution of the reodering model in isolation. Thus, in each experiment, the same data combination technique is used to build both translation and reordering models. As suggested by [19], we use approximate randomization to test whether differences among system performances are statistically sign"
2011.iwslt-evaluation.18,W05-0908,0,0.0299325,"ench systems use lowercased models and a standard recasing postprocess. Word alignments are computed by GIZA++ [18] on the concatenation of all data. Consequently, phrase extraction and scoring are carried out separately on each corpus. Table 2 provides summary statistics on the phrase overlaps of the NEWS and UN phrase tables with respect to the TED phrase table. Note that, in this work, we do not evaluate the contribution of the reodering model in isolation. Thus, in each experiment, the same data combination technique is used to build both translation and reordering models. As suggested by [19], we use approximate randomization to test whether differences among system performances are statistically significant7 . 7 Significance tests were computed with the Multeval toolkit: https://github.com/jhclark/multeval We apply fill-up and plug the resulting phrase and reordering tables (5+1 and 6 features respectively) to the decoder. The global feature vector for each experimental setting is then optimized by minimum error rate training (MERT) [6]. Table 3 presents translation quality results in terms of BLEU and NIST scores, using different data combination techniques: concat stands for a"
2011.iwslt-evaluation.18,W05-0900,0,\N,Missing
2011.iwslt-evaluation.18,2011.iwslt-evaluation.1,1,\N,Missing
2011.iwslt-evaluation.24,2009.iwslt-papers.4,1,0.871245,"nslation. While most features can be summed over grammar rules that comprise a constituent, language models examine cross-constituent N -grams. A straightforward dynamic programming algorithm [1] accounts for these N grams by combining hypotheses only if their first N − 1 and last N − 1 words are the same. This algorithm takes O(V 2N −2 ) time and space per constituent, where V is the vocabulary size. That is too expensive, so practical decoders implement approximate search by estimating the probability of sentence fragments for purposes of pruning and prioritization. We focus on the decoders [2, 3, 4] that build translations bottom-up by recursively concatenating sentence fragments, estimating their score after each rule application. Cube pruning [5] is a commonly-implemented method to prioritize and prune grammar rule applications. Within a hypergraph node, each non-terminal has a set of possible values. Applying a rule consists of choosing a value for each non-terminal and scoring. Cube pruning estimates that the score under rule application will be the product (or sum in log space) of the scores of the rule itself and of each value. It then uses these estimates to prioritize rule applic"
2011.iwslt-evaluation.24,P10-4002,0,0.0502427,"nslation. While most features can be summed over grammar rules that comprise a constituent, language models examine cross-constituent N -grams. A straightforward dynamic programming algorithm [1] accounts for these N grams by combining hypotheses only if their first N − 1 and last N − 1 words are the same. This algorithm takes O(V 2N −2 ) time and space per constituent, where V is the vocabulary size. That is too expensive, so practical decoders implement approximate search by estimating the probability of sentence fragments for purposes of pruning and prioritization. We focus on the decoders [2, 3, 4] that build translations bottom-up by recursively concatenating sentence fragments, estimating their score after each rule application. Cube pruning [5] is a commonly-implemented method to prioritize and prune grammar rule applications. Within a hypergraph node, each non-terminal has a set of possible values. Applying a rule consists of choosing a value for each non-terminal and scoring. Cube pruning estimates that the score under rule application will be the product (or sum in log space) of the scores of the rule itself and of each value. It then uses these estimates to prioritize rule applic"
2011.iwslt-evaluation.24,W09-0424,0,0.0335352,"nslation. While most features can be summed over grammar rules that comprise a constituent, language models examine cross-constituent N -grams. A straightforward dynamic programming algorithm [1] accounts for these N grams by combining hypotheses only if their first N − 1 and last N − 1 words are the same. This algorithm takes O(V 2N −2 ) time and space per constituent, where V is the vocabulary size. That is too expensive, so practical decoders implement approximate search by estimating the probability of sentence fragments for purposes of pruning and prioritization. We focus on the decoders [2, 3, 4] that build translations bottom-up by recursively concatenating sentence fragments, estimating their score after each rule application. Cube pruning [5] is a commonly-implemented method to prioritize and prune grammar rule applications. Within a hypergraph node, each non-terminal has a set of possible values. Applying a rule consists of choosing a value for each non-terminal and scoring. Cube pruning estimates that the score under rule application will be the product (or sum in log space) of the scores of the rule itself and of each value. It then uses these estimates to prioritize rule applic"
2011.iwslt-evaluation.24,J07-2003,0,0.117216,"ard dynamic programming algorithm [1] accounts for these N grams by combining hypotheses only if their first N − 1 and last N − 1 words are the same. This algorithm takes O(V 2N −2 ) time and space per constituent, where V is the vocabulary size. That is too expensive, so practical decoders implement approximate search by estimating the probability of sentence fragments for purposes of pruning and prioritization. We focus on the decoders [2, 3, 4] that build translations bottom-up by recursively concatenating sentence fragments, estimating their score after each rule application. Cube pruning [5] is a commonly-implemented method to prioritize and prune grammar rule applications. Within a hypergraph node, each non-terminal has a set of possible values. Applying a rule consists of choosing a value for each non-terminal and scoring. Cube pruning estimates that the score under rule application will be the product (or sum in log space) of the scores of the rule itself and of each value. It then uses these estimates to prioritize rule applications, starting with the highest estimated score. This process continues until the pop limit is reached, which acts as a hard limit on the number of ru"
2011.iwslt-evaluation.24,P06-1098,0,0.251231,"tate, the decoder recombines them, thus efficiently reasoning over many sentence fragments via dynamic programming. To increase recombination, it is desirable to encode less than 2N –2 words where possible. In this paper, we make three improvements related to state and concatenation: 1. Minimizing the number of words encoded by left state, enabling more recombination. 2. Encoding left state using pointers into the language model’s data structure, making concatenation faster. 3. Avoiding queries that will not impact the estimated score, speeding concatenation. 183 2. Related Work Some decoders [6, 7] avoid left state entirely by building translations left-to-right. Hypotheses may therefore recombine, for purposes of language modeling, when their right states are equal. Typically, these decoders use beam search, where the beam consists of approximately comparable hypotheses, such as those of equal length. These decoders have the advantage that more recombinations do happen, although they risk repeating work because constituents are evaluated in multiple different contexts. The purpose of our work here is not to decide whether one search algorithm or approximation is better, but simply to i"
2011.iwslt-evaluation.24,D10-1027,0,0.256726,"tate, the decoder recombines them, thus efficiently reasoning over many sentence fragments via dynamic programming. To increase recombination, it is desirable to encode less than 2N –2 words where possible. In this paper, we make three improvements related to state and concatenation: 1. Minimizing the number of words encoded by left state, enabling more recombination. 2. Encoding left state using pointers into the language model’s data structure, making concatenation faster. 3. Avoiding queries that will not impact the estimated score, speeding concatenation. 183 2. Related Work Some decoders [6, 7] avoid left state entirely by building translations left-to-right. Hypotheses may therefore recombine, for purposes of language modeling, when their right states are equal. Typically, these decoders use beam search, where the beam consists of approximately comparable hypotheses, such as those of equal length. These decoders have the advantage that more recombinations do happen, although they risk repeating work because constituents are evaluated in multiple different contexts. The purpose of our work here is not to decide whether one search algorithm or approximation is better, but simply to i"
2011.iwslt-evaluation.24,P07-1019,0,0.165067,"s of language modeling, when their right states are equal. Typically, these decoders use beam search, where the beam consists of approximately comparable hypotheses, such as those of equal length. These decoders have the advantage that more recombinations do happen, although they risk repeating work because constituents are evaluated in multiple different contexts. The purpose of our work here is not to decide whether one search algorithm or approximation is better, but simply to improve the commonlyimplemented bottom-up strategy. A faster alternative to bottom-up cube pruning is cube growing [8] that lazily generates hypotheses for each constituent instead of generating a fixed number. Like cube pruning, cube growing generates sentence fragments, recombines them into hypotheses, and ranks hypotheses according to estimated language model probabilities. The improvements we discuss here are therefore complementary, since the effect of our work is to improve recombination and ranking within each constituent. Prior work [9] described and implemented algorithms to minimize left and right state in the context of a bottom-up chart decoder. For hypotheses shorter than N –1 words, they store t"
2011.iwslt-evaluation.24,W08-0402,0,0.491083,"algorithm or approximation is better, but simply to improve the commonlyimplemented bottom-up strategy. A faster alternative to bottom-up cube pruning is cube growing [8] that lazily generates hypotheses for each constituent instead of generating a fixed number. Like cube pruning, cube growing generates sentence fragments, recombines them into hypotheses, and ranks hypotheses according to estimated language model probabilities. The improvements we discuss here are therefore complementary, since the effect of our work is to improve recombination and ranking within each constituent. Prior work [9] described and implemented algorithms to minimize left and right state in the context of a bottom-up chart decoder. For hypotheses shorter than N –1 words, they store the entire hypothesis in state. In our work, we apply state minimization to all hypotheses, including those shorter than N –1 words. For hypotheses longer than N –1 words, we minimize state in the same way that [9] does. While [9] described an “inefficient implementation of the prefix- and suffix-lookup”, we store the additional information with each n-gram entry, incurring minimal overhead and reusing lookups already performed i"
2011.iwslt-evaluation.24,W11-2123,1,0.837556,"e n-gram that it matched with each query. Decoders can use this information to store at most n words in left or right state, depending on the position of the words being queried. However, this does not fully minimize state, as w1n may be matched by the model, but w1n v may not be in the model for any word v. In this case w1 may be safely omitted from right state but this information is hidden from the decoder. Similarly, were the toolkit to indicate that vw1n does not appear for any word v (i.e. w1n does not extend left), then wn could be omitted from left state. In this work, we extend KenLM [12] to store and expose the necessary information. It implements two data structures, probing and trie. The probing data structure is a hash table from n-grams to probability and backoff and is byte-aligned for speed. The trie data structure is a reverse trie similar to SRILM and IRSTLM but with bit-level packing (i.e. it uses 31 bits to store probability since the sign bit is always negative). Since entries that do not extend right have zero backoff, a special backoff value flags n-grams that do not extend right; this information is provided to the decoder, as is the length of n-gram matched. We"
2011.iwslt-evaluation.24,P02-1040,0,0.0855548,"Missing"
2011.iwslt-evaluation.24,2005.mtsummit-papers.11,1,0.0814087,"Missing"
2011.iwslt-evaluation.24,W11-2103,1,0.794863,"Missing"
2011.iwslt-evaluation.24,J03-4003,0,\N,Missing
2011.mtsummit-papers.1,W08-0304,0,0.0462071,"rload, as multiple translations of the input are needed. On the other side, the average system well competes with the system combination, without any computational additional cost. 2 Related Work Since its ﬁrst appearance, the MERT procedure (Och, 2003) has been recognized as unstable. The author analyzed the problem and proposed a way for smoothing the objective function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from origi"
2011.mtsummit-papers.1,2004.iwslt-papers.2,1,0.704303,"he system combination, without any computational additional cost. 2 Related Work Since its ﬁrst appearance, the MERT procedure (Och, 2003) has been recognized as unstable. The author analyzed the problem and proposed a way for smoothing the objective function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e"
2011.mtsummit-papers.1,D08-1024,0,0.0531715,"Och, 2003) has been recognized as unstable. The author analyzed the problem and proposed a way for smoothing the objective function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavi"
2011.mtsummit-papers.1,P11-2031,0,0.108123,"ion of models. Interpolation weights are typically computed by means of iterative procedures which aim at maximizing a given scoring function. Unfortunately, such a function is deﬁnitely non-convex; hence, only local optima can be reached. Moreover, it has been observed that the commonly used optimization procedure, the N-best minimum error rate training (simply MERT hereafter) (Och, 2003), is quite unstable. In the last years, many efforts have been devoted for making the procedure or its results more reliable. Recently, a deep investigation of the optimizer instability has been presented by Clark et al. (2011). In that work, experimental evidence of the instability problems affecting optimizers is shown; then, statistical tools 32 are selected for making possible both a quantitative evaluation of the optimization process of single systems and a fair comparison of two systems, somehow independent from the optimization process. The work ends with some recommendations: for instance, run optimization at least three times; use additional held-out test sets for manual analysis of translations in order to select the best optimization (better: “more reliable”) among those available. By taking that investig"
2011.mtsummit-papers.1,W02-1001,0,0.0979044,"MERT procedure (Och, 2003) has been recognized as unstable. The author analyzed the problem and proposed a way for smoothing the objective function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (200"
2011.mtsummit-papers.1,P08-2010,0,0.0192258,"ct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combined systems is the set of interpolation weights. BoostedMERT (Duh and Kirchhoff, 2008) uses one single set of models, with different sets of interpolation weights. Each log-linear model is treated as a “weak learner”, and boosting is used to combine such weak learners for N-best re-ranking. On the contrary, we either produce a new log-linear model"
2011.mtsummit-papers.1,W09-0439,0,0.0317607,"ive function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti"
2011.mtsummit-papers.1,W11-2130,0,0.0127143,"cognized as unstable. The author analyzed the problem and proposed a way for smoothing the objective function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et a"
2011.mtsummit-papers.1,D08-1011,0,0.015922,", Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combined systems is the set of interpolation weights. BoostedMERT (Duh and Kirchhoff, 2008) uses one single set"
2011.mtsummit-papers.1,W10-1744,0,0.0182485,"lso to high values of the test set score function. The centroid should then fall inside that region both for the development and for the test set, increasing the chance of stabilizing the performance on the test set. It is worth noticing that this approach requires additional computation effort only in tuning stage, as multiple optimization runs have to be performed. System combination (sysComb) System combination is the second solution we propose for smoothing the outputs of various optimizer samples. For combining the outputs of the available systems, we used the software1 developed at CMU (Heaﬁeld and Lavie, 2010). It works at the word level and smartly allows “the synthesis of new word orderings”. The scheme includes several stages. Hypotheses are aligned in pairs using the publicly available METEOR (Banerjee and Lavie, 2005) aligner. Then, on these alignments a search space is deﬁned, which is explored by a beam search decoding. Hypotheses are scored using a linear interpolation of features, including the LM score, the hypothesis length and the average n-gram length found in the LM. Interpolation weights are tuned using Z-MERT (Zaidan, 2009). 1 Available at http://kheaﬁeld.com/code/mt/ 34 This approa"
2011.mtsummit-papers.1,2008.amta-srw.3,0,0.0156974,"x, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained"
2011.mtsummit-papers.1,P05-3026,0,0.0182981,"iang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combine"
2011.mtsummit-papers.1,P07-2045,1,0.0123236,"se alignments a search space is deﬁned, which is explored by a beam search decoding. Hypotheses are scored using a linear interpolation of features, including the LM score, the hypothesis length and the average n-gram length found in the LM. Interpolation weights are tuned using Z-MERT (Zaidan, 2009). 1 Available at http://kheaﬁeld.com/code/mt/ 34 This approach requires the translation of the test set by each optimizer sample to be combined, besides the computational cost of the combination of translations itself. 4 Experiments The SMT systems are built upon the open-source MT toolkit Moses2 (Koehn et al., 2007). The translation and the lexicalized reordering models have been trained on parallel data. The LMs have been estimated via the IRSTLM toolkit (Federico et al., 2008) either on the monolingual data, when available, or on the target side of the parallel data; they are smoothed through the improved Kneser-Ney technique (Chen and Goodman, 1999). The weights of the log-linear interpolation model have been optimized on the development sets by means of the standard MERT procedure provided within the Moses toolkit: different realizations of tuned systems (optimizer samples) have been obtained by mult"
2011.mtsummit-papers.1,D07-1105,0,0.0173459,"evel combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combined systems is the set of interpolation weights. BoostedMERT (Duh and Kirchhoff, 2008) uses one single set of models, with different sets of interpolation weights. Each log-linear model is treated as a “weak learner”, and boosting is used to combine such weak learners for N-best re-ranking. On the contrary, we either"
2011.mtsummit-papers.1,E06-1005,0,0.0215326,"t al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combined systems is the set of"
2011.mtsummit-papers.1,C08-1074,0,0.0488716,"for smoothing the objective function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008),"
2011.mtsummit-papers.1,P03-1021,0,0.396757,"ffective in smoothing instability, but also that the average system well competes with the more expensive system combination. 1 Introduction Statistical machine translation (SMT) systems feature a log-linear interpolation of models. Interpolation weights are typically computed by means of iterative procedures which aim at maximizing a given scoring function. Unfortunately, such a function is deﬁnitely non-convex; hence, only local optima can be reached. Moreover, it has been observed that the commonly used optimization procedure, the N-best minimum error rate training (simply MERT hereafter) (Och, 2003), is quite unstable. In the last years, many efforts have been devoted for making the procedure or its results more reliable. Recently, a deep investigation of the optimizer instability has been presented by Clark et al. (2011). In that work, experimental evidence of the instability problems affecting optimizers is shown; then, statistical tools 32 are selected for making possible both a quantitative evaluation of the optimization process of single systems and a fair comparison of two systems, somehow independent from the optimization process. The work ends with some recommendations: for insta"
2011.mtsummit-papers.1,N07-1029,0,0.020768,"hoice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combined systems is the set of interpolation weigh"
2011.mtsummit-papers.1,P07-1040,0,0.0185003,"hoice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combined systems is the set of interpolation weigh"
2011.mtsummit-papers.1,W08-0329,0,0.0192641,"(2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combined systems is the set of interpolation weights. BoostedMERT (Duh and Kirchhoff, 2008) uses one single set of models, with diffe"
2011.mtsummit-papers.1,2009.iwslt-evaluation.12,0,0.0351409,"Missing"
2011.mtsummit-papers.1,D07-1055,0,0.0210922,"nt computational overload, as multiple translations of the input are needed. On the other side, the average system well competes with the system combination, without any computational additional cost. 2 Related Work Since its ﬁrst appearance, the MERT procedure (Och, 2003) has been recognized as unstable. The author analyzed the problem and proposed a way for smoothing the objective function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct s"
2011.mtsummit-papers.59,S10-1007,0,0.0268043,"tasks, including the creation of parallel corpora, the word-level alignment of parallel sentences, the creation of paraphrases of existing reference translations, and the creation of translation lexica for low resource languages (Callison-Burch and Dredze, 2010). Annotated data is also crucial for evaluation purposes. Very recently, crowdsourced data has started being ofﬁcially used in international evaluation campaigns. In the CLEF 2010 Web People Search Clustering Task (Artiles et al., 2010) and the SemEval2010 Task of Noun Compounds Interpretation Using Paraphrasing Verbs and Prepositions (Butnariu et al., 2010), for example, MTurk was used for the annotation of the training/test data sets. For MT evaluation, a number of studies have been carried out with the aim of understanding the feasibility of substituting expert data with non-expert data for different types of human evaluation tasks. In (Callison-Burch, 2009), it is shown that MTurk can be effectively used to collect relative rankings, to perform human-mediated translation edit rate (HTER), and to carry out evaluation through reading comprehension questions. In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessment"
2011.mtsummit-papers.59,W10-0701,0,0.0210162,"Vision, and Natural Language Processing tasks such as relation extraction, word sense disambiguation, textual entailment, named entity annotation, and natural language generation. Machine Translation is one of the ﬁelds where research on crowdsourcing is most active. The feasibility of collecting good quality crowdsourced data has been explored for many different MT tasks, including the creation of parallel corpora, the word-level alignment of parallel sentences, the creation of paraphrases of existing reference translations, and the creation of translation lexica for low resource languages (Callison-Burch and Dredze, 2010). Annotated data is also crucial for evaluation purposes. Very recently, crowdsourced data has started being ofﬁcially used in international evaluation campaigns. In the CLEF 2010 Web People Search Clustering Task (Artiles et al., 2010) and the SemEval2010 Task of Noun Compounds Interpretation Using Paraphrasing Verbs and Prepositions (Butnariu et al., 2010), for example, MTurk was used for the annotation of the training/test data sets. For MT evaluation, a number of studies have been carried out with the aim of understanding the feasibility of substituting expert data with non-expert data for"
2011.mtsummit-papers.59,W07-0718,0,0.230666,"a given translation depends on numerous factors like the intended use of the translation, the characteristics of the MT software, and the nature of the translation process. Early attempts tried to manually produce numerical judgements of MT quality with respect to a set of reference translations (White et al., 1994). Recently, human assessment of MT quality has been carried out by either assigning a single grade on a scale of 5 or 7 specifying the ﬂuency or adequacy of a given translation (Przybocki et al., 2008), or by relatively ranking to each other multiple translations of the same input (Callison-Burch et al., 2007). 521 Although human evaluation of MT output provides the most direct and reliable assessment, it is time consuming, costly and subjective, i.e., evaluation results might vary from person to person due to different backgrounds, bilingual experience, and inconsistent judgements caused by the high complexity of the multi-class grading task. These drawbacks to human assessment schemes have encouraged many researchers to seek reliable methods for estimating such measures automatically. Various automatic evaluation measures have been proposed to make the evaluation of MT outputs cheaper and faster."
2011.mtsummit-papers.59,D09-1030,0,0.0704963,"purposes. Very recently, crowdsourced data has started being ofﬁcially used in international evaluation campaigns. In the CLEF 2010 Web People Search Clustering Task (Artiles et al., 2010) and the SemEval2010 Task of Noun Compounds Interpretation Using Paraphrasing Verbs and Prepositions (Butnariu et al., 2010), for example, MTurk was used for the annotation of the training/test data sets. For MT evaluation, a number of studies have been carried out with the aim of understanding the feasibility of substituting expert data with non-expert data for different types of human evaluation tasks. In (Callison-Burch, 2009), it is shown that MTurk can be effectively used to collect relative rankings, to perform human-mediated translation edit rate (HTER), and to carry out evaluation through reading comprehension questions. In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessments. Finally, in (Callison-Burch et al., 2010) a subset of the ofﬁcial WMT10 relative ranking evaluation was reproduced with non-expert judges and various methods to improve the quality of the collected data are presented. All these MT evaluation experiments have been conducted using MTurk directly, as have mo"
2011.mtsummit-papers.59,W10-0709,0,0.112325,"ds Interpretation Using Paraphrasing Verbs and Prepositions (Butnariu et al., 2010), for example, MTurk was used for the annotation of the training/test data sets. For MT evaluation, a number of studies have been carried out with the aim of understanding the feasibility of substituting expert data with non-expert data for different types of human evaluation tasks. In (Callison-Burch, 2009), it is shown that MTurk can be effectively used to collect relative rankings, to perform human-mediated translation edit rate (HTER), and to carry out evaluation through reading comprehension questions. In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessments. Finally, in (Callison-Burch et al., 2010) a subset of the ofﬁcial WMT10 relative ranking evaluation was reproduced with non-expert judges and various methods to improve the quality of the collected data are presented. All these MT evaluation experiments have been conducted using MTurk directly, as have most of the available studies on the effectiveness of crowdsourcing. Up to now, only a few papers have reported on the use of CF as an interface to MTurk (Wang and Callison-Burch, 2010; Finin et al., 2010; Negri and Mehdad, 2010), none"
2011.mtsummit-papers.59,W10-0713,0,0.0166157,"ehension questions. In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessments. Finally, in (Callison-Burch et al., 2010) a subset of the ofﬁcial WMT10 relative ranking evaluation was reproduced with non-expert judges and various methods to improve the quality of the collected data are presented. All these MT evaluation experiments have been conducted using MTurk directly, as have most of the available studies on the effectiveness of crowdsourcing. Up to now, only a few papers have reported on the use of CF as an interface to MTurk (Wang and Callison-Burch, 2010; Finin et al., 2010; Negri and Mehdad, 2010), none of them addressing the task of MT evaluation. 3 IWSLT 2010 BTEC Task Evaluation The International Workshop on Spoken Language Translation (IWSLT) is a yearly, open evaluation campaign for spoken language translation. IWSLT’s evaluations are not competition-oriented; their goal is to foster cooperative work and scientiﬁc exchange. In this respect, IWSLT proposes challenging research tasks and an open experimental infrastructure for the scientiﬁc community working on spoken and written language translation. In the IWSLT 2010 campaign (Paul et al., 2010), three dif"
2011.mtsummit-papers.59,W10-0734,0,0.0133446,"In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessments. Finally, in (Callison-Burch et al., 2010) a subset of the ofﬁcial WMT10 relative ranking evaluation was reproduced with non-expert judges and various methods to improve the quality of the collected data are presented. All these MT evaluation experiments have been conducted using MTurk directly, as have most of the available studies on the effectiveness of crowdsourcing. Up to now, only a few papers have reported on the use of CF as an interface to MTurk (Wang and Callison-Burch, 2010; Finin et al., 2010; Negri and Mehdad, 2010), none of them addressing the task of MT evaluation. 3 IWSLT 2010 BTEC Task Evaluation The International Workshop on Spoken Language Translation (IWSLT) is a yearly, open evaluation campaign for spoken language translation. IWSLT’s evaluations are not competition-oriented; their goal is to foster cooperative work and scientiﬁc exchange. In this respect, IWSLT proposes challenging research tasks and an open experimental infrastructure for the scientiﬁc community working on spoken and written language translation. In the IWSLT 2010 campaign (Paul et al., 2010), three different shared tasks addre"
2011.mtsummit-papers.59,J82-2005,0,0.718625,"Missing"
2011.mtsummit-papers.59,W10-0725,0,0.0281807,"valuation through reading comprehension questions. In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessments. Finally, in (Callison-Burch et al., 2010) a subset of the ofﬁcial WMT10 relative ranking evaluation was reproduced with non-expert judges and various methods to improve the quality of the collected data are presented. All these MT evaluation experiments have been conducted using MTurk directly, as have most of the available studies on the effectiveness of crowdsourcing. Up to now, only a few papers have reported on the use of CF as an interface to MTurk (Wang and Callison-Burch, 2010; Finin et al., 2010; Negri and Mehdad, 2010), none of them addressing the task of MT evaluation. 3 IWSLT 2010 BTEC Task Evaluation The International Workshop on Spoken Language Translation (IWSLT) is a yearly, open evaluation campaign for spoken language translation. IWSLT’s evaluations are not competition-oriented; their goal is to foster cooperative work and scientiﬁc exchange. In this respect, IWSLT proposes challenging research tasks and an open experimental infrastructure for the scientiﬁc community working on spoken and written language translation. In the IWSLT 2010 campaign (Paul et a"
2011.mtsummit-papers.59,1994.amta-1.25,0,0.69757,"Missing"
2012.eamt-1.60,2003.mtsummit-papers.9,0,0.00847487,"Missing"
2012.eamt-1.60,N04-4038,0,0.00519852,"e site will also release unofficial benchmarks for many other language pairs. 4 4.1 4.1.1 265 Data Experiments were performed on data supplied by the organizers of the IWSLT 2011 evaluation campaign for the MT track,4 who asked participants to automatically translate talks from Arabic to English, from Chinese to English and from English to French. For developing the baselines, only texts from the TED domain were employed, i.e. no additional out-of-domain resources were used. Different preprocessings were performed depending to the language: Arabic and Chinese were segmented by means of AMIRA (Diab et al., 2004) and the Stanford Chinese Segmenter (Tseng et al., 2005), respectively; the tokenizer script released together with the Europarl corpus (Koehn, 2005) was applied to other languages. The same partitioning of the evaluation campaign in terms of parallel training data, development (dev2010, tst2010) and test (tst2011) sets has been adopted: Tables 4 and 5 report some statistics of such texts. Baselines In this section, we present results on some benchmarks that we obtained by training MT baseline systems on the available TED Talks data. The aim is to provide MT scientists with reference results t"
2012.eamt-1.60,eisele-chen-2010-multiun,0,0.0802012,"e the main source of information to infer parameter values of the employed mathematical model. In statistical machine translation (SMT), learning is performed on parallel texts, i.e. documents, sentences or even fragments of sentences with their translation(s). Large amounts of in-domain parallel data are usually required to properly train translation and reordering models. Unfortunately, parallel data are a scarce resource, which are freely available only for some language pairs and for few, very specific domains. c 2012 European Association for Machine Translation. 261 For example, MultiUN (Eisele and Chen, 2010) provides large parallel texts (300 million words) but for only 6 languages; Europarl (Koehn, 2005) consists of the translation into most European languages of the proceedings of the European Parliament (at most 50 million words); JRC-Acquis1 comprises the total body of European Union law applicable to the Member States, written in 22 European languages (35 million words); other smaller parallel corpora in specific domains are included in OPUS (Tiedemann, 2009) for various languages. On the other hand, it is unfeasible for research laboratories to cover all possible needs in terms of parallel"
2012.eamt-1.60,P07-2045,1,0.0608402,"Missing"
2012.eamt-1.60,2005.mtsummit-papers.11,0,0.528374,"machine translation (SMT), learning is performed on parallel texts, i.e. documents, sentences or even fragments of sentences with their translation(s). Large amounts of in-domain parallel data are usually required to properly train translation and reordering models. Unfortunately, parallel data are a scarce resource, which are freely available only for some language pairs and for few, very specific domains. c 2012 European Association for Machine Translation. 261 For example, MultiUN (Eisele and Chen, 2010) provides large parallel texts (300 million words) but for only 6 languages; Europarl (Koehn, 2005) consists of the translation into most European languages of the proceedings of the European Parliament (at most 50 million words); JRC-Acquis1 comprises the total body of European Union law applicable to the Member States, written in 22 European languages (35 million words); other smaller parallel corpora in specific domains are included in OPUS (Tiedemann, 2009) for various languages. On the other hand, it is unfeasible for research laboratories to cover all possible needs in terms of parallel texts by resorting to professional translators, given their high cost. The data available at the TE"
2012.eamt-1.60,P11-2031,0,\N,Missing
2012.eamt-1.60,O07-5005,0,\N,Missing
2012.eamt-1.60,I05-3027,0,\N,Missing
2012.eamt-1.60,2011.iwslt-evaluation.1,1,\N,Missing
2012.eamt-1.60,2010.iwslt-evaluation.1,1,\N,Missing
2012.iwslt-papers.14,N06-2037,0,0.0664379,"dence between the models by learning the hyperparameters of the variational Dirichlet posteriors in one LDA model and bootstrapping the second model by ﬁxing the hyperparameters. The bilingual LSA framework is also applied to adapt translation models. Other bilingual topic modeling approaches include Hidden Markov Bilingual Topic AdMixtures [6] and Polylingual Topic Models [7]. The literature focuses primarily on domain adaptation, using techniques such as information retrieval to select similar sentences in training corpora for adaptation, either through interpolation [8] or corpora ﬁltering [9], or mixture model adaptation approaches [10, 11]. An alternative to MDI adaptation is proposed by [12], which uses a log-linear combination of binary features fi (h, w) to scale LM probabilities P (w |h):    ˆ fi (h, w)λi P (w |h). P (w |h) = exp i Normalization is avoided by simply dividing Pˆ (w |h) by Pˆ (w |h) + 1. 3. MDI Adaptation MDI adaptation was originally presented in [1] as a means for domain adaptation on language models. MDI adaptation scales the probabilities of a background language model, PB (h, w), by a factor determined by a ratio between the unigram statistics observed"
2012.iwslt-papers.14,W07-0717,0,0.482947,"erparameters of the variational Dirichlet posteriors in one LDA model and bootstrapping the second model by ﬁxing the hyperparameters. The bilingual LSA framework is also applied to adapt translation models. Other bilingual topic modeling approaches include Hidden Markov Bilingual Topic AdMixtures [6] and Polylingual Topic Models [7]. The literature focuses primarily on domain adaptation, using techniques such as information retrieval to select similar sentences in training corpora for adaptation, either through interpolation [8] or corpora ﬁltering [9], or mixture model adaptation approaches [10, 11]. An alternative to MDI adaptation is proposed by [12], which uses a log-linear combination of binary features fi (h, w) to scale LM probabilities P (w |h):    ˆ fi (h, w)λi P (w |h). P (w |h) = exp i Normalization is avoided by simply dividing Pˆ (w |h) by Pˆ (w |h) + 1. 3. MDI Adaptation MDI adaptation was originally presented in [1] as a means for domain adaptation on language models. MDI adaptation scales the probabilities of a background language model, PB (h, w), by a factor determined by a ratio between the unigram statistics observed in an adaptation text A versus the same statistic"
2012.iwslt-papers.14,W07-0733,0,0.0534828,"erparameters of the variational Dirichlet posteriors in one LDA model and bootstrapping the second model by ﬁxing the hyperparameters. The bilingual LSA framework is also applied to adapt translation models. Other bilingual topic modeling approaches include Hidden Markov Bilingual Topic AdMixtures [6] and Polylingual Topic Models [7]. The literature focuses primarily on domain adaptation, using techniques such as information retrieval to select similar sentences in training corpora for adaptation, either through interpolation [8] or corpora ﬁltering [9], or mixture model adaptation approaches [10, 11]. An alternative to MDI adaptation is proposed by [12], which uses a log-linear combination of binary features fi (h, w) to scale LM probabilities P (w |h):    ˆ fi (h, w)λi P (w |h). P (w |h) = exp i Normalization is avoided by simply dividing Pˆ (w |h) by Pˆ (w |h) + 1. 3. MDI Adaptation MDI adaptation was originally presented in [1] as a means for domain adaptation on language models. MDI adaptation scales the probabilities of a background language model, PB (h, w), by a factor determined by a ratio between the unigram statistics observed in an adaptation text A versus the same statistic"
2012.iwslt-papers.14,W11-2133,1,0.786045,"esses the desired scalability features for real-time adaptation of large-order n-gram LMs. This paper is organized as follows: In Section 2, we discuss relevant previous work. In Section 3, we review MDI adaptation. In Section 4, we describe Lazy MDI adaptation for machine translation and review how unigram statistics of adaptation texts can be derived using bilingual topic modeling. In Section 5, we report adaptation experiments on TED talks1 from IWSLT 2010 and 2012, followed by our conclusions and suggestions for future work in Section 6. 2. Previous Work This paper is based on the work of [3], which combines MDI adaptation with bilingual topic modeling on small adaptation contexts for lecture translation. Adaptation texts are drawn from source language input and leveraged for language model adaptation. A bilingual Probabilistic Latent Semantic Analysis (PLSA) [4] model is constructed by combining parallel training texts, allowing for inference on monolingual source texts for MDI adaptation by removing source language unigram statistics. 1 http://www.ted.com/talks 244 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 A similar approach"
2012.iwslt-papers.14,D09-1092,0,0.0345793,"guage Translation Hong Kong, December 6th-7th, 2012 A similar approach is considered by [5] in domain adaptation by constructing two hierarchical LDA models from parallel document corpora and enforcing a one-to-one correspondence between the models by learning the hyperparameters of the variational Dirichlet posteriors in one LDA model and bootstrapping the second model by ﬁxing the hyperparameters. The bilingual LSA framework is also applied to adapt translation models. Other bilingual topic modeling approaches include Hidden Markov Bilingual Topic AdMixtures [6] and Polylingual Topic Models [7]. The literature focuses primarily on domain adaptation, using techniques such as information retrieval to select similar sentences in training corpora for adaptation, either through interpolation [8] or corpora ﬁltering [9], or mixture model adaptation approaches [10, 11]. An alternative to MDI adaptation is proposed by [12], which uses a log-linear combination of binary features fi (h, w) to scale LM probabilities P (w |h):    ˆ fi (h, w)λi P (w |h). P (w |h) = exp i Normalization is avoided by simply dividing Pˆ (w |h) by Pˆ (w |h) + 1. 3. MDI Adaptation MDI adaptation was originally pre"
2012.iwslt-papers.14,C04-1059,0,0.0607356,"g a one-to-one correspondence between the models by learning the hyperparameters of the variational Dirichlet posteriors in one LDA model and bootstrapping the second model by ﬁxing the hyperparameters. The bilingual LSA framework is also applied to adapt translation models. Other bilingual topic modeling approaches include Hidden Markov Bilingual Topic AdMixtures [6] and Polylingual Topic Models [7]. The literature focuses primarily on domain adaptation, using techniques such as information retrieval to select similar sentences in training corpora for adaptation, either through interpolation [8] or corpora ﬁltering [9], or mixture model adaptation approaches [10, 11]. An alternative to MDI adaptation is proposed by [12], which uses a log-linear combination of binary features fi (h, w) to scale LM probabilities P (w |h):    ˆ fi (h, w)λi P (w |h). P (w |h) = exp i Normalization is avoided by simply dividing Pˆ (w |h) by Pˆ (w |h) + 1. 3. MDI Adaptation MDI adaptation was originally presented in [1] as a means for domain adaptation on language models. MDI adaptation scales the probabilities of a background language model, PB (h, w), by a factor determined by a ratio between the unig"
2012.iwslt-papers.14,P07-2045,1,0.00936227,"ED website as it was on March 30, 2011 and split into training, dev and test sets according to indexes used for IWSLT 20103 evaluation. The data set is segmented at the clause level, rather than at the level of sentences. The TED training data consists of 329 parallel talk transcripts with approximately 84k sentences. The TED test data consists of transcriptions created via 1-best ASR outputs from the KIT Quaero Evaluation System. It consists of 2381 clauses and approximately 25,000 English and French words, respectively. Lowercased SMT systems are built upon the Moses open-source SMT toolkit [15]4 . The translation and lexicalized reordering models have been trained on parallel data. One 5-gram background LM was constructed with the IRSTLM toolkit [16] on the French side of the TED training data (740k words), and smoothed via the improved KneserNey technique [17]. The weights of the log-linear interpolation model were optimized via minimum error rate training (MERT) [18] on the TED development set, using 200 best translations at each tuning iteration. As in [3], online adaptation is simulated by splitting the training corpus into small non-overlapping contexts of 5 lines (41,847 “docu"
2012.iwslt-papers.14,P03-1021,0,0.0170721,"t ASR outputs from the KIT Quaero Evaluation System. It consists of 2381 clauses and approximately 25,000 English and French words, respectively. Lowercased SMT systems are built upon the Moses open-source SMT toolkit [15]4 . The translation and lexicalized reordering models have been trained on parallel data. One 5-gram background LM was constructed with the IRSTLM toolkit [16] on the French side of the TED training data (740k words), and smoothed via the improved KneserNey technique [17]. The weights of the log-linear interpolation model were optimized via minimum error rate training (MERT) [18] on the TED development set, using 200 best translations at each tuning iteration. As in [3], online adaptation is simulated by splitting the training corpus into small non-overlapping contexts of 5 lines (41,847 “documents” in total) and performing bilingual BLEU ↑ 2 To avoid overlapping types in the topic mdoel, we annotate the source and target vocabularies to track their provenance. 3 http://iwslt2010.fbk.eu/ 4 http://www.statmt.org/moses/ METEOR ↑ TER ↓ Length System Baseline MDI Lazy MDI (unsmoothed) Lazy MDI (smoothed) Baseline MDI Lazy MDI (unsmoothed) Lazy MDI (smoothed) Baseline MDI"
2012.iwslt-papers.14,P11-2031,0,0.0485139,"M for SMT decoding, preserving the same feature weight as the background LM. In the case of Lazy MDI, adaptation is integrated into the Moses decoder using the same context unigrams. MERT is performed on the development set with simultaneous adaptation for each context. We experiment with both adaptation via unsmoothed unigram ratios and smoothing via our transformed fast sigmoid function. Words not in the adaptation unigram LM are ﬁxed with a 1:1 ratio to prevent their effect on the global translation hypothesis score. We ran 3 MERT instances for each system and evaluated using MultiEval 0.3 [19]. Evaluation results in terms of BLEU, METEOR (French), TER, and segment length are listed in Table 1. We observe similar results between MDI 5.1. IWSLT 2010 Metric We replicate the experimental settings of [3] and provide a comparison of classic MDI against Lazy MDI, using the same data set of English-French translations of TED talks, downloaded from the TED website as it was on March 30, 2011 and split into training, dev and test sets according to indexes used for IWSLT 20103 evaluation. The data set is segmented at the clause level, rather than at the level of sentences. The TED training da"
2012.iwslt-papers.14,W12-3102,0,0.0371578,"ore or less the same. The baseline suggests that white pills are worse than blue pills – a subtle difference from the intent of the reference. The Lazy-adapted hypothesis corrects this difference, but makes common mistakes in 5 http://hltc.cs.ust.hk/iwslt/index.php/evaluation-campaign/ted-task translating “good” and “as”. Lazy MDI yields a shorter trans6 https://wit3.fbk.eu/mt.php?release=2012-03-test lation in ID #1055 that moves away from a literal translation 7 109 French-English data set provided by the WMT 2012 translation in the ﬁrst half of the sentence that closely matches the reftask [20]. 8 http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2009T28 erence. ID #1059 results in a very minor article change from 248 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 “the” to “our”. In this context, this subtle difference is important because the speaker is comparing the water at his ﬁsh farm to other farms. ID 364 1055 1059 Text But a white pill is not as good as a blue pill . Mais un comprim´e blanc n’ est pas aussi bon qu’ une comprim´e bleu Mais une pilule blanche est moins bonne qu’ une pilule bleue . Mais une pilule bl"
2012.iwslt-papers.14,P10-2041,0,0.0852678,"oidsmoothed Lazy MDI setting on a state-of-the-art SMT system submitted for the IWSLT 2012 TED English-French MT shared task5 . In this experiment, we build cased translation systems using Moses and evaluate the effects of Lazy MDI adaptation from lowercased unigram context statistics. Our baseline system consists of translation and reordering models trained from the in-domain TED6 corpus, as well as outof-domain Giga French-English7 and Europarl v7 [21] corpora. Each out-of-domain corpus was domain-adapted by aggressive ﬁltering using a cross-entropy difference scoring technique described by [22] on the French side and optimizing the perplexity against the (French) TED training data by incrementally adding sentences. The corresponding parallel English sentences were preserved to provide compact parallel corpora. A single phrase and reordering table were constructed using the ﬁll-up technique described in [23] in a cascaded fashion in the order of TED, Giga French-English, and Europarl. A domain-adapted 5-gram mixture language model was constructed with IRSTLM from the TED, Giga FrenchEnglish, Gigaword French v2 AFP8 , and WMT News Commentary v7 corpora. The same ﬁltering technique [22"
2012.iwslt-papers.14,2011.iwslt-evaluation.18,1,0.912702,"translation and reordering models trained from the in-domain TED6 corpus, as well as outof-domain Giga French-English7 and Europarl v7 [21] corpora. Each out-of-domain corpus was domain-adapted by aggressive ﬁltering using a cross-entropy difference scoring technique described by [22] on the French side and optimizing the perplexity against the (French) TED training data by incrementally adding sentences. The corresponding parallel English sentences were preserved to provide compact parallel corpora. A single phrase and reordering table were constructed using the ﬁll-up technique described in [23] in a cascaded fashion in the order of TED, Giga French-English, and Europarl. A domain-adapted 5-gram mixture language model was constructed with IRSTLM from the TED, Giga FrenchEnglish, Gigaword French v2 AFP8 , and WMT News Commentary v7 corpora. The same ﬁltering technique [22] was applied to the LM corpora. For Lazy MDI, we again use the bilingual PLSA model constructed from the IWSLT 2010 training data, with 250 topics and 20 EM iterations. MERT is again performed on the development set with simultaneous Lazy MDI adaptation for each context. Topic adaptation results against the domain-ad"
2013.iwslt-evaluation.1,2010.iwslt-evaluation.1,1,0.863584,"al of 217 primary runs submitted. All runs were evaluated with objective metrics on a current test set and two progress test sets, in order to compare the progresses against systems of the previous years. In addition, submissions of one of the official machine translation tracks were also evaluated with human post-editing. 1. Introduction This paper overviews the results of the evaluation campaign organized by the International Workshop of Spoken Language Translation. The IWSLT evaluation has been now running for a decade and has offered along these years a variety of speech translation tasks [1, 2, 3, 4, 5, 6, 7, 8, 9]. The 2013 IWSLT evaluation continued along the line set in 2010, by focusing on the translation of TED Talks, a collection of public speeches covering many different topics. As in the previous two years, the evaluation included tracks for all the core technologies involved in the spoken language translation task, namely: • Automatic speech recognition (ASR), i.e. the conversion of a speech signal into a transcript, • Machine translation (MT), i.e. the translation of a polished transcript into another language, • Spoken language translation (SLT), that addressed the conversion and translation"
2013.iwslt-evaluation.1,2012.eamt-1.60,1,0.897031,"on tracks, many other optional translation directions were also offered. Optional SLT directions were from English to Spanish, Portuguese (B), Italian, Chinese, Polish, Slovenian, Arabic, and Persian. Optional MT translation directions were: English from/to Arabic, Spanish, Portuguese (B), Italian, Chinese, Polish, Persian, Slovenian, Turkish, Dutch, Romanian, and Russian. For each official and optional translation direction, training and development data were supplied by the organizers through the workshop’s website. Major parallel collections made available to the participants were the WIT3 [10] corpus of TED talks, all data from the WMT 2013 workshop [11], the MULTIUN corpus [12], and the SETIMES parallel corpus [13]. A list of monolingual resources was provided too, that includes both freely available corpora and corpora available from the LDC. Test data were released at the begin of each test period, requiring participants to return one primary run and optional contrastive runs within one week. The schedule of the evaluation was organized as follows: June 8, release of training data; Sept 2-8, ASR test of period; Sept 9-15, SLT test period; Oct 7-13, MT test period; Oct 7-20, test"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.21,0,0.0238082,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.11,0,0.0818098,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.5,0,0.0447354,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.6,0,0.0540129,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.9,0,0.0348889,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.23,0,0.0935135,"Missing"
2013.iwslt-evaluation.1,2005.mtsummit-papers.11,0,0.125368,"y, the standard dev2010 and tst2010 development sets have been released as well. Tables 2 and 3 provide statistics on in-domain texts supplied for training, development and evaluation purposes for the official directions. Reference results from baseline MT systems on the development set tst2010 are provided via the WIT3 repository. This helps participants and MT scientists to assess their experimental outcomes. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [38] was applied to all languages, with the exception of Chinese and Arabic languages, which were preprocessed by, respectively: the Stanford Chinese Segmenter [39]; either AMIRA [40], in the Arabic-to-English direction, or the QCRI-normalizer,3 in the English-to-Arabic direction. The baselines were developed with the Moses toolkit [41]. Translation and lexicalized reordering models were trained on the parallel training data; 5-gram LMs with improved Kneser-Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [42]. The weights of the log-linear inte"
2013.iwslt-evaluation.1,N04-4038,0,0.0999494,"tion purposes for the official directions. Reference results from baseline MT systems on the development set tst2010 are provided via the WIT3 repository. This helps participants and MT scientists to assess their experimental outcomes. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [38] was applied to all languages, with the exception of Chinese and Arabic languages, which were preprocessed by, respectively: the Stanford Chinese Segmenter [39]; either AMIRA [40], in the Arabic-to-English direction, or the QCRI-normalizer,3 in the English-to-Arabic direction. The baselines were developed with the Moses toolkit [41]. Translation and lexicalized reordering models were trained on the parallel training data; 5-gram LMs with improved Kneser-Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [42]. The weights of the log-linear interpolation model were 3 Specifically developed for IWSLT 2013 by P. Nakov and F. Al-Obaidli at Qatar Computing Research Institute. Table 3: Bilingual resources for official languag"
2013.iwslt-evaluation.1,P07-2045,1,0.010973,"This helps participants and MT scientists to assess their experimental outcomes. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [38] was applied to all languages, with the exception of Chinese and Arabic languages, which were preprocessed by, respectively: the Stanford Chinese Segmenter [39]; either AMIRA [40], in the Arabic-to-English direction, or the QCRI-normalizer,3 in the English-to-Arabic direction. The baselines were developed with the Moses toolkit [41]. Translation and lexicalized reordering models were trained on the parallel training data; 5-gram LMs with improved Kneser-Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [42]. The weights of the log-linear interpolation model were 3 Specifically developed for IWSLT 2013 by P. Nakov and F. Al-Obaidli at Qatar Computing Research Institute. Table 3: Bilingual resources for official language pairs task MTEnF r MTDeEn MTEnDe data set train dev2010 tst2010 tst2011 tst2012 tst2013 train dev2010 tst2010 dev2012 tst2013 train dev2010 tst2010 tst20"
2013.iwslt-evaluation.1,2012.amta-papers.22,1,0.822673,"Missing"
2013.iwslt-evaluation.1,2006.amta-papers.25,0,0.207086,"Missing"
2013.iwslt-evaluation.1,J93-3001,0,0.516669,"Missing"
2013.iwslt-evaluation.1,W05-0908,0,0.0754408,"Missing"
2013.iwslt-evaluation.1,1993.eamt-1.1,0,0.4595,"Missing"
2013.iwslt-evaluation.16,2012.eamt-1.60,1,0.8976,"neous speech and heterogeneous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been v"
2013.iwslt-evaluation.16,2005.mtsummit-papers.11,1,0.078384,"nd heterogeneous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been very successful"
2013.iwslt-evaluation.16,eisele-chen-2010-multiun,0,0.0452925,"ous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been very successful in contribut"
2013.iwslt-evaluation.16,E06-1005,1,0.921596,"e-scale evaluation campaigns like IWSLT and WMT in recent years, thereby demonstrating their ability to continuously enhance their systems and promoting progress in machine translation. Machine translation research within EU-BRIDGE has a strong focus on translation of spoken language. The IWSLT TED talks task constitutes an interesting framework for empirical testing of some of the systems for spoken language translation which are developed as part of the project. The work described here is an attempt to attain translation quality beyond strong single system performance via system combination [11]. Similar cooperative approaches based on system combination have proven to be valuable for machine translation in other projects, e.g. in the Quaero programme [12, 13]. Within EU-BRIDGE, we built combined system setups for text translation of talks from English to French as well as from German to English. We found that the combined translation engines of RWTH, UEDIN, KIT, and FBK systems are very effective. In the rest of the paper we will give some insight into the technology behind the combined engines which have been used to produce the joint EU-BRIDGE submission to the IWSLT 2013 MT track"
2013.iwslt-evaluation.16,P02-1040,0,0.0892795,"-BRIDGE submission to the IWSLT 2013 MT track. The remainder of the paper is structured as follows: We first describe the individual English→French and German→English systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), Karlsruhe Institute of Technology (Section 4), and Fondazione Bruno Kessler (Section 5), respectively. We then present the techniques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SR"
2013.iwslt-evaluation.16,2006.amta-papers.25,0,0.15922,"sion to the IWSLT 2013 MT track. The remainder of the paper is structured as follows: We first describe the individual English→French and German→English systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), Karlsruhe Institute of Technology (Section 4), and Fondazione Bruno Kessler (Section 5), respectively. We then present the techniques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [2"
2013.iwslt-evaluation.16,W10-1738,1,0.880309,"iques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [21]. All RWTH systems include the standard set of models provided by Jane. For English→French, the final setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment w"
2013.iwslt-evaluation.16,popovic-ney-2006-pos,1,0.929216,"ll available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective, for which we define B LEU on the sentence level with smoothed 3-gram"
2013.iwslt-evaluation.16,P03-1021,0,0.129032,"ns from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [21]. All RWTH systems include the standard set of models provided by Jane. For English→French, the final setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment was created with fast align [22]. A language model was trained on the target side of all avai"
2013.iwslt-evaluation.16,P12-1031,0,0.10415,"For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective, for which we define B LEU on the sentence level with smoothed 3-gram and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discri"
2013.iwslt-evaluation.16,P10-2041,0,0.0805135,"setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment was created with fast align [22]. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 41 of the French Gigaword Second Edition corpus. The monolingual data selection for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a se"
2013.iwslt-evaluation.16,D08-1089,0,0.117877,"orpus. The monolingual data selection for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of"
2013.iwslt-evaluation.16,P07-2045,1,0.0125349,"EU on the sentence level with smoothed 3-gram and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a"
2013.iwslt-evaluation.16,W13-2212,1,0.868834,"m and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation"
2013.iwslt-evaluation.16,W11-2123,0,0.0545914,"tion by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequen"
2013.iwslt-evaluation.16,P11-1105,1,0.916011,"d on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev"
2013.iwslt-evaluation.16,D09-1022,1,0.892821,"n for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resu"
2013.iwslt-evaluation.16,2012.iwslt-papers.17,1,0.860668,"em [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate resu"
2013.iwslt-evaluation.16,D13-1138,1,0.815085,"based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running w"
2013.iwslt-evaluation.16,N04-1022,0,0.487773,"atistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence m"
2013.iwslt-evaluation.16,W12-2702,0,0.051709,"cribed in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RW"
2013.iwslt-evaluation.16,P07-1019,0,0.222647,"ranslation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence models over Brown wo"
2013.iwslt-evaluation.16,E03-1076,1,0.900834,"data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective"
2013.iwslt-evaluation.16,N12-1047,0,0.148125,"penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence models over Brown word clusters, these setups were not finished in time for the contribution to the EU-BRIDGE system combination. language models trained on WIT3 , Europarl, News Commentary, 109 , and Common Crawl by minimizing the perplexity on the development data. For the class-based language model, KIT utilized in-domain WIT3 data with 4grams and 50 clusters. In addition, a 9-gram POS-based language model derived fr"
2013.iwslt-evaluation.16,2011.iwslt-evaluation.9,1,0.925869,"ge model derived from LIA POS tags [55] on all monolingual data was applied. KIT optimized the log-linear combination of all these models on the provided development data using Minimum Error Rate Training [20]. 4. Karlsruhe Institute of Technology The KIT translations have been generated by an in-house phrase-based translations system [41]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, K"
2013.iwslt-evaluation.16,2007.tmi-papers.21,0,0.422618,"e-based translations system [41]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED"
2013.iwslt-evaluation.16,W09-0435,1,0.918776,"Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection"
2013.iwslt-evaluation.16,W13-0805,1,0.889592,"ra for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase tabl"
2013.iwslt-evaluation.16,W08-1006,0,0.169177,"SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a"
2013.iwslt-evaluation.16,2005.iwslt-1.8,1,0.888473,"t. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context feat"
2013.iwslt-evaluation.16,W08-0303,1,0.796238,"word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to bet"
2013.iwslt-evaluation.16,2012.amta-papers.19,1,0.890564,"and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-doma"
2013.iwslt-evaluation.16,W11-2124,1,0.885306,"ated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-"
2013.iwslt-evaluation.16,W13-2264,1,0.887808,"se trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, K"
2013.iwslt-evaluation.16,2012.iwslt-papers.3,1,0.886049,"criminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, KIT used an RBM-based language model [53] trained on the WIT3 corpus. Finally, KIT also used a classbased language model, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a"
2013.iwslt-evaluation.16,E99-1010,0,0.0594124,"ed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, KIT used an RBM-based language model [53] trained on the WIT3 corpus. Finally, KIT also used a classbased language model, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a statistical log-linear model including a filled-up phrase translation model [56] and lexicalized reordering models (RMs), two F"
2013.iwslt-evaluation.16,2011.iwslt-evaluation.18,1,0.928081,"el, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a statistical log-linear model including a filled-up phrase translation model [56] and lexicalized reordering models (RMs), two French language models (LMs), as well as distortion, word, and phrase penalties. In order to focus it on TED specific domain and genre, and to reduce the size of the system, data selection by means of IRSTLM toolkit [57] was performed on the whole parallel English→French corpus, using the WIT3 training data as in-domain data. Different amount of data are selected from each available corpora but the WIT3 data, for a total of 66 M English running words. Two TMs and two RMs were trained on WIT3 and selected data, separately, and combined using the fil"
2013.iwslt-evaluation.16,W05-0909,0,0.0593782,"es which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we applied a system combination implementation that has been developed at RWTH Aachen University. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. [60]. This approach includes an enhanced alignment and reordering framework. Alignments between the system outputs are learned using METEOR [61]. A confusion network is then built using one of the hypotheses as “primary” hypothesis. We do not make a hard decision on which of the hypotheses to use for that, but instead combine all possible confusion networks into a single lattice. Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models, e.g. a special n-gram language model which is learned on the input hypotheses. Scaling factors of the models are optimized using the Minimum Error Rate Training algorithm. The translation with the best total score within the"
2013.iwslt-evaluation.16,W12-3140,1,\N,Missing
2013.iwslt-evaluation.16,J03-1002,1,\N,Missing
2013.iwslt-evaluation.16,C12-3061,1,\N,Missing
2013.iwslt-evaluation.16,federico-etal-2012-iwslt,1,\N,Missing
2013.iwslt-evaluation.16,2011.iwslt-evaluation.1,1,\N,Missing
2013.iwslt-evaluation.16,W13-2223,1,\N,Missing
2013.iwslt-evaluation.16,N13-1073,0,\N,Missing
2013.iwslt-evaluation.20,P07-2045,1,0.0131661,"models, and language models from multiple corpora, respectively. In Section 3, we describe several experiments in the English-French translation task. In Section 4, we describe our first efforts at translated to and from English and Persian, a language pair with few parallel resources available. We introduce our efforts to collect and preprocess Perian corpora to improve the quality of Persian translation and show significant improvements over the state of the art. In Section 5 we summarize our findings. For all language pairs, we set up a standard phrase-based system using the Moses toolkit [2]. We construct a statistical 1 http://www.ted.com/talks 2 University of Trento ICT Doctoral School Via Sommarive, 5 38123 Trento, TN, Italy log-linear models including domain-adapted phrase translation and hierarchical reordering models [3, 4, 5], one or more target language models (LM), as well as distortion, word, and phrase penalties. 2. Domain adaptation techniques In this section, we summarize several well-known techniques for domain adaptation we applied to build high-performance models for our SMT submissions. 2.1. Data selection The idea of data selection is to find the subset of sente"
2013.iwslt-evaluation.20,N04-4026,0,0.0482934,"n, a language pair with few parallel resources available. We introduce our efforts to collect and preprocess Perian corpora to improve the quality of Persian translation and show significant improvements over the state of the art. In Section 5 we summarize our findings. For all language pairs, we set up a standard phrase-based system using the Moses toolkit [2]. We construct a statistical 1 http://www.ted.com/talks 2 University of Trento ICT Doctoral School Via Sommarive, 5 38123 Trento, TN, Italy log-linear models including domain-adapted phrase translation and hierarchical reordering models [3, 4, 5], one or more target language models (LM), as well as distortion, word, and phrase penalties. 2. Domain adaptation techniques In this section, we summarize several well-known techniques for domain adaptation we applied to build high-performance models for our SMT submissions. 2.1. Data selection The idea of data selection is to find the subset of sentences within an out-of-domain corpus that better fits with a given in-domain corpus. To this purpose, we follow the procedure described in [6], which adapts the cross-entropy difference scoring technique introduced by [7] toward bitext data select"
2013.iwslt-evaluation.20,2005.iwslt-1.8,0,0.0549463,"n, a language pair with few parallel resources available. We introduce our efforts to collect and preprocess Perian corpora to improve the quality of Persian translation and show significant improvements over the state of the art. In Section 5 we summarize our findings. For all language pairs, we set up a standard phrase-based system using the Moses toolkit [2]. We construct a statistical 1 http://www.ted.com/talks 2 University of Trento ICT Doctoral School Via Sommarive, 5 38123 Trento, TN, Italy log-linear models including domain-adapted phrase translation and hierarchical reordering models [3, 4, 5], one or more target language models (LM), as well as distortion, word, and phrase penalties. 2. Domain adaptation techniques In this section, we summarize several well-known techniques for domain adaptation we applied to build high-performance models for our SMT submissions. 2.1. Data selection The idea of data selection is to find the subset of sentences within an out-of-domain corpus that better fits with a given in-domain corpus. To this purpose, we follow the procedure described in [6], which adapts the cross-entropy difference scoring technique introduced by [7] toward bitext data select"
2013.iwslt-evaluation.20,D08-1089,0,0.0582814,"n, a language pair with few parallel resources available. We introduce our efforts to collect and preprocess Perian corpora to improve the quality of Persian translation and show significant improvements over the state of the art. In Section 5 we summarize our findings. For all language pairs, we set up a standard phrase-based system using the Moses toolkit [2]. We construct a statistical 1 http://www.ted.com/talks 2 University of Trento ICT Doctoral School Via Sommarive, 5 38123 Trento, TN, Italy log-linear models including domain-adapted phrase translation and hierarchical reordering models [3, 4, 5], one or more target language models (LM), as well as distortion, word, and phrase penalties. 2. Domain adaptation techniques In this section, we summarize several well-known techniques for domain adaptation we applied to build high-performance models for our SMT submissions. 2.1. Data selection The idea of data selection is to find the subset of sentences within an out-of-domain corpus that better fits with a given in-domain corpus. To this purpose, we follow the procedure described in [6], which adapts the cross-entropy difference scoring technique introduced by [7] toward bitext data select"
2013.iwslt-evaluation.20,P10-2041,0,0.060814,"l reordering models [3, 4, 5], one or more target language models (LM), as well as distortion, word, and phrase penalties. 2. Domain adaptation techniques In this section, we summarize several well-known techniques for domain adaptation we applied to build high-performance models for our SMT submissions. 2.1. Data selection The idea of data selection is to find the subset of sentences within an out-of-domain corpus that better fits with a given in-domain corpus. To this purpose, we follow the procedure described in [6], which adapts the cross-entropy difference scoring technique introduced by [7] toward bitext data selection. First, all sentence pairs of the out-of-domain corpus are associated with a source- and target-side scores, each computed as the basic technique proposes for the corresponding monolingual scenarios, using the in-domain (TED) data as a seed and LMs of order 3. Then, the sentences are sorted according to the sum of these two scores. Finally, the optimal split between useful and useless sentences is found by minimizing the source-side perplexity of a development set on growing percentages of the sorted corpus. In our experiments, dev2010 and tst2010 are concatenated"
2013.iwslt-evaluation.20,2011.iwslt-evaluation.18,1,0.880119,"onolingual scenarios, using the in-domain (TED) data as a seed and LMs of order 3. Then, the sentences are sorted according to the sum of these two scores. Finally, the optimal split between useful and useless sentences is found by minimizing the source-side perplexity of a development set on growing percentages of the sorted corpus. In our experiments, dev2010 and tst2010 are concatenated and used as the filtering development set. 2.2. Translation model combination Three methods are applied in our submissions to combine the TM built on the available parallel training corpora: namely, fill-up [8, 9], back-off, and interpolation. 2.2.1. Fill-up and Back-off In the fill-up approach, out-of-domain phrase pairs that do not appear in an in-domain (TED) phrase table are added, along with their scores – effectively filling the in-domain table with additional phrase translation options. The fill-up process is performed in a cascaded order, first filling in missing phrases from the corpora that are closest in domain to TED. Moreover, out-of-domain phrase pairs with more than four source tokens are pruned. Following [8, 9] the fill-up approach adds k-1 provenance binary features to weight the impo"
2013.iwslt-evaluation.20,E12-1055,0,0.0256716,"an four source tokens are pruned. Following [8, 9] the fill-up approach adds k-1 provenance binary features to weight the importance of out-of-domain data, where k is the number of phrase tables to combine. A similar back-off approach performs the fill-up technique, but does not add any provenance binary features. 2.2.2. Linear interpolation A common approach for building multi-model is through the linear interpolation of component models. Various approaches have been suggested for computing the coefficients of the interpolated model, the most recent being perplexity minimization described in [10] where the perplexity of each component translation model is minimized on the parallel development set. However, the mixing coefficients can be separately computed by several other techniques. In this paper, instead of calculating translation model perplexity we calculate language model perplexity on target side development set. After minimizing perplexity we get the interpolation weights which we then use as mixing coefficients for component translation models. 2.3. Reordering model combination All techniques available for combining the TMs can be applied straightfowardly to combine the RMs."
2013.iwslt-evaluation.20,2012.eamt-1.60,1,0.817797,"based system using the Moses toolkit [2], exploiting a huge amount of English-French bitexts and monolingual French training data. Each system features a statistical loglinear model including one phrase translation model [9] and one lexicalized reordering model, multiple French language models (LMs), as well as distortion, word, and phrase penalties. The training data are composed from some of the corpora allowed by the IWSLT Evaluation Campaign organizers. As parallel data the following corpora were taken into account: Web Inventory of Transcribed and Translated Talks (version 2013-01) (TED) [13], 109 -French-English (version 2) (Giga), English-French Europarl (version 7) (EP), Common Crawl (CC), MultiUN (UN), and the News Commentary (News) corpus as distributed by the organizers of the Workshop of Machine Translation (WMT). As monolingual data we use the entire monolingual news corpora (Full) distributed by WMT organizers for language model training. All texts were processed according to the language specific tokenization provided by Moses toolkit and kept casesensitive. Statistics of the training corpora are reported in Table 1. Corpus TED Giga UN CC EP News Full unselected En Fr Se"
2013.iwslt-evaluation.20,W11-2123,0,0.0253156,"omponent models. Details for these systems are provided in Section 3.1. Most system parameters are kept fixed to allow a better comparison among the systems. Word alignments are computed by means of MGIZA++ on case-insensitive parallel texts to reduce data sparseness; casing information is re-introduced in order to estimate case-sensitive models, unless otherwise specified in the particular experiment. In all systems the maximum phrase length is set to 7 and the distortion limit is set to the default value of 6. We train 5-gram LMs with IRSTLM toolkit [12] in most cases; in other cases, KenLM [14] is used. Each language model is smoothed via the improved Kneser-Ney technique. Singleton n-grams of order three or higher are pruned. The weights of the log-linear combination are optimized either via minimum error rate training (MERT) [15] or the Margin Infused Relaxed Algorithm (MIRA) [16, 17] on dev2010. 3.1. English-French submissions As described in Section 3, we submit five systems which differ in the exploitation of the training data for the creation of TM, RM and LMs. We evaluate the performance of each system in Table 2 and use the results on tst2010 to select our primary submission"
2013.iwslt-evaluation.20,P03-1021,0,0.0543223,"to reduce data sparseness; casing information is re-introduced in order to estimate case-sensitive models, unless otherwise specified in the particular experiment. In all systems the maximum phrase length is set to 7 and the distortion limit is set to the default value of 6. We train 5-gram LMs with IRSTLM toolkit [12] in most cases; in other cases, KenLM [14] is used. Each language model is smoothed via the improved Kneser-Ney technique. Singleton n-grams of order three or higher are pruned. The weights of the log-linear combination are optimized either via minimum error rate training (MERT) [15] or the Margin Infused Relaxed Algorithm (MIRA) [16, 17] on dev2010. 3.1. English-French submissions As described in Section 3, we submit five systems which differ in the exploitation of the training data for the creation of TM, RM and LMs. We evaluate the performance of each system in Table 2 and use the results on tst2010 to select our primary submission. In our Primary, Contrastive 1, and Contrastive 2 systems, the dev2010 and tst2010 data are added to the TED training data after optimizing each system’s feature weights, before evaluating their performances on the 2011, 2012, and 2013 test"
2013.iwslt-evaluation.20,D07-1080,0,0.0550361,"introduced in order to estimate case-sensitive models, unless otherwise specified in the particular experiment. In all systems the maximum phrase length is set to 7 and the distortion limit is set to the default value of 6. We train 5-gram LMs with IRSTLM toolkit [12] in most cases; in other cases, KenLM [14] is used. Each language model is smoothed via the improved Kneser-Ney technique. Singleton n-grams of order three or higher are pruned. The weights of the log-linear combination are optimized either via minimum error rate training (MERT) [15] or the Margin Infused Relaxed Algorithm (MIRA) [16, 17] on dev2010. 3.1. English-French submissions As described in Section 3, we submit five systems which differ in the exploitation of the training data for the creation of TM, RM and LMs. We evaluate the performance of each system in Table 2 and use the results on tst2010 to select our primary submission. In our Primary, Contrastive 1, and Contrastive 2 systems, the dev2010 and tst2010 data are added to the TED training data after optimizing each system’s feature weights, before evaluating their performances on the 2011, 2012, and 2013 test sets. 3.1.1. Primary A backed-off TM is created combinin"
2013.iwslt-evaluation.20,N04-1022,0,0.0529711,"weights, before evaluating their performances on the 2011, 2012, and 2013 test sets. 3.1.1. Primary A backed-off TM is created combining a primary TM trained on TED training data (TED-TM) and a background TM trained on the selected training data (Slct-TM). The RM is constructed in a similar manner. A log-linear combination of two LMs is employed. The first LM is a mixture estimated from the in-domain TED training data (TED-LM) and the out-of-domain data-selected training data (Slct-LM). Additionally, a second Full-LM is estimated from the entire French monolingual corpora. Minimum Bayes Risk [18] (MBR) decoding technique, provided by Moses, is also exploited. Feature weights are averaged over three MERT optimizations. 3.1.3. Contrastive 2 This system aims at enhancing the primary system by further focusing its models to each specific talk that comprises the test set. Using the same optimized feature weights, we construct talk-specific translation, reordering, and language models and insert them with highest priority in their respective back-off and mixture models. Given a talk to translate, we perform the data selection procedure described in Section 2.1, using the source text of the"
2013.mtsummit-papers.2,W12-3155,1,0.930194,"cal consistency through online learning. Cesa-Bianchi et al. (2008) are the first to apply online discriminative re-ranking to a CAT scenario. Incremental adaptations of the generative components of SMT have been presented for a related scenario, interactive machine translation, where an MT component produces hypotheses based on partial translations of a sentence (Nepveu et al., 2004; Ortiz-Mart´ınez et al., 2010). Our online learning protocol is similar, but operating on the sentence instead of word or phrase level. Incremental adaptations have also been presented for larger batches of data (Bertoldi et al., 2012). In terms of granularity, our scenario is most similar to the work by Hardt and Elming (2010), where the Moses training procedure is employed to update the phrase table immediately after a reference becomes available. Our work, however, focuses on adapting both language and translation model with techniques where the global model remains unchanged. This is important in a CAT scenario, where several users might use the same global model but individual local models. 3 Online Adaptation in SMT Cesa-Bianchi and Lugosi (2006) presented a protocol for online learning with expert advice. This protoc"
2013.mtsummit-papers.2,2011.iwslt-evaluation.18,1,0.82902,"D. or streaming scenarios for incremental adaptation of the core components of SMT (Levenberg et al., 2010). However, the online learning protocol is applied in these approaches to training data only, i.e., parameters are updated on a per-example basis on the training set, while testing is done by retranslating the full test set using the final model. Further related work can be found in the application of incremental learning to domain adaptation in SMT. Here a local and a global model have to be combined, either in a log-linear combination (Koehn and Schroeder, 2007), with a fill-up method (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Carpuat and Simard (2012) show that increased translation consistency does not correlate with better translation quality, however, translation errors are indicated by inconsistencies. Our approach can be seen as a successful approach to improve translation quality by enforcing local consistency through online learning. Cesa-Bianchi et al. (2008) are the first to apply online discriminative re-ranking to a CAT scenario. Incremental adaptations of the generative components of SMT have been presented for a related scenario, interactive machi"
2013.mtsummit-papers.2,W12-3156,0,0.0256807,"f SMT (Levenberg et al., 2010). However, the online learning protocol is applied in these approaches to training data only, i.e., parameters are updated on a per-example basis on the training set, while testing is done by retranslating the full test set using the final model. Further related work can be found in the application of incremental learning to domain adaptation in SMT. Here a local and a global model have to be combined, either in a log-linear combination (Koehn and Schroeder, 2007), with a fill-up method (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Carpuat and Simard (2012) show that increased translation consistency does not correlate with better translation quality, however, translation errors are indicated by inconsistencies. Our approach can be seen as a successful approach to improve translation quality by enforcing local consistency through online learning. Cesa-Bianchi et al. (2008) are the first to apply online discriminative re-ranking to a CAT scenario. Incremental adaptations of the generative components of SMT have been presented for a related scenario, interactive machine translation, where an MT component produces hypotheses based on partial transl"
2013.mtsummit-papers.2,2010.iwslt-papers.3,1,0.879186,"itive. The log-linear interpolation weights are optimized using the standard MERT procedure provided with the Moses toolkit. The baseline system also provides a list of k-best translations. In the online discriminative re-ranking approach, this k-best list is rescored according to lexicalized sparse features including phrase pairs and target-side n-grams. 3.2 Constrained Search for Feedback Exploitation In order to extract information for system refinement from user feedback, source and user translation need to be aligned at the phrase-level. We use a constrained search technique described in Cettolo et al. (2010) to achieve this, which optimizes the coverage of both source and target sentences given a set of translation options. The search produces exactly one phrase segmentation and alignment, and allows gaps such that some source and target words may be uncovered. Unambiguous gaps (i.e. one on the source and one on the target side) can then be aligned. It differs in this respect from forced decoding which produces an alignment only when the target is fully reachable with the given models. From the phrase alignment, three types of phrase pairs can be collected: (i) new phrase pairs by aligning unambi"
2013.mtsummit-papers.2,D08-1024,0,0.0689582,"this is the first comparison of generative and discriminative online adaptation methods in a CAT scenario. The discriminative approach allows to perform feature development and training independently of the underlying SMT system. In the generative approach, the model is simple, however, updates have to be communicated to the decoder. In sum, the gains of both approaches add up to average BLEU improvements of 4 points over a baseline non-adapted model. 2 Previous Work Online learning methods in SMT are found in the context of stochastic methods for discriminative training (Liang et al., 2006; Chiang et al., 2008), Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 11–18. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. or streaming scenarios for incremental adaptation of the core components of SMT (Levenberg et al., 2010). However, the online learning protocol is applied in these approaches to training data only, i.e., parameters are updated on a per-example basis on the training set, while testing is done by retranslating"
2013.mtsummit-papers.2,W02-1001,0,0.263479,"und in the cache receives a score of 0, i.e. no reward. n-grams crossing over contiguous translation options are not taken into account. Note that the proposed feature is simply a stateless function which rewards approved translation options, which are expected to be of high quality. To control the influence of the local language model, the additional weight is optimized with the Simplex algorithm; weights of the baseline system tuned with MERT are taken as fixed. 3.5 Online Discriminative Re-Ranking The learner used in our online discriminative re-ranking approach is a structured perceptron (Collins, 2002). We use lexicalized sparse features defined by two feature templates: First, all phrase pairs found by the decoder (for system translations) or by the constrained search (for the user translation) are used as features. Second, we use features defined by target-side n-grams from n = 1, . . . , 4 in the user translation. Our features are not indicator functions, but use the number of source words (for the first type of features) and the number of words in target-side n-grams (for the second type of features) as values. Given a feature representation f (x, y) for a source-target pair (x, y), and"
2013.mtsummit-papers.2,2010.amta-papers.21,0,0.646236,"line discriminative re-ranking to a CAT scenario. Incremental adaptations of the generative components of SMT have been presented for a related scenario, interactive machine translation, where an MT component produces hypotheses based on partial translations of a sentence (Nepveu et al., 2004; Ortiz-Mart´ınez et al., 2010). Our online learning protocol is similar, but operating on the sentence instead of word or phrase level. Incremental adaptations have also been presented for larger batches of data (Bertoldi et al., 2012). In terms of granularity, our scenario is most similar to the work by Hardt and Elming (2010), where the Moses training procedure is employed to update the phrase table immediately after a reference becomes available. Our work, however, focuses on adapting both language and translation model with techniques where the global model remains unchanged. This is important in a CAT scenario, where several users might use the same global model but individual local models. 3 Online Adaptation in SMT Cesa-Bianchi and Lugosi (2006) presented a protocol for online learning with expert advice. This protocol can be adapted to our scenario of online adaptation in SMT as follows: Train global model M"
2013.mtsummit-papers.2,W07-0733,0,0.0548648,"licence, no derivative works, attribution, CC-BY-ND. or streaming scenarios for incremental adaptation of the core components of SMT (Levenberg et al., 2010). However, the online learning protocol is applied in these approaches to training data only, i.e., parameters are updated on a per-example basis on the training set, while testing is done by retranslating the full test set using the final model. Further related work can be found in the application of incremental learning to domain adaptation in SMT. Here a local and a global model have to be combined, either in a log-linear combination (Koehn and Schroeder, 2007), with a fill-up method (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Carpuat and Simard (2012) show that increased translation consistency does not correlate with better translation quality, however, translation errors are indicated by inconsistencies. Our approach can be seen as a successful approach to improve translation quality by enforcing local consistency through online learning. Cesa-Bianchi et al. (2008) are the first to apply online discriminative re-ranking to a CAT scenario. Incremental adaptations of the generative components of SMT have been prese"
2013.mtsummit-papers.2,P07-2045,1,0.0112703,"erative components of translation model (TM) (Section 3.3) and language model (LM) (Section 3.4) and adaptation via discriminative reranking (Section 3.5). Different refinements result in different modes of combination of global and local models (step 0). Both generative and discriminative adaptation modes deploy a constrained search technique (Section 3.2) to extract information relevant for system refinement from the received user feedback (step 3). Translation (step 2) employs a standard phrase-based SMT engine. 3.1 Baseline System The MT engine is built upon the open source toolkit Moses (Koehn et al., 2007). The global translation and the lexicalized reordering models are estimated on parallel training data with default 12 Annex Allegato to the all’ 3.3 Technical Offer Offerta Tecnica Figure 1: Phrase segmentation and alignment. setting. The global 5-gram LM smoothed through the improved Kneser-Ney technique is estimated on the target monolingual side of the parallel training data using the IRSTLM toolkit (Federico et al., 2008). Models are case-sensitive. The log-linear interpolation weights are optimized using the standard MERT procedure provided with the Moses toolkit. The baseline system als"
2013.mtsummit-papers.2,N10-1062,0,0.179294,"BLEU improvements of 4 points over a baseline non-adapted model. 2 Previous Work Online learning methods in SMT are found in the context of stochastic methods for discriminative training (Liang et al., 2006; Chiang et al., 2008), Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 11–18. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. or streaming scenarios for incremental adaptation of the core components of SMT (Levenberg et al., 2010). However, the online learning protocol is applied in these approaches to training data only, i.e., parameters are updated on a per-example basis on the training set, while testing is done by retranslating the full test set using the final model. Further related work can be found in the application of incremental learning to domain adaptation in SMT. Here a local and a global model have to be combined, either in a log-linear combination (Koehn and Schroeder, 2007), with a fill-up method (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Carpuat and Simard (2012) show"
2013.mtsummit-papers.2,P06-1096,0,0.193911,"Missing"
2013.mtsummit-papers.2,D12-1037,0,0.045275,"e core components of SMT (Levenberg et al., 2010). However, the online learning protocol is applied in these approaches to training data only, i.e., parameters are updated on a per-example basis on the training set, while testing is done by retranslating the full test set using the final model. Further related work can be found in the application of incremental learning to domain adaptation in SMT. Here a local and a global model have to be combined, either in a log-linear combination (Koehn and Schroeder, 2007), with a fill-up method (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Carpuat and Simard (2012) show that increased translation consistency does not correlate with better translation quality, however, translation errors are indicated by inconsistencies. Our approach can be seen as a successful approach to improve translation quality by enforcing local consistency through online learning. Cesa-Bianchi et al. (2008) are the first to apply online discriminative re-ranking to a CAT scenario. Incremental adaptations of the generative components of SMT have been presented for a related scenario, interactive machine translation, where an MT component produces hypothe"
2013.mtsummit-papers.2,W04-3225,0,0.303316,"ranslation consistency does not correlate with better translation quality, however, translation errors are indicated by inconsistencies. Our approach can be seen as a successful approach to improve translation quality by enforcing local consistency through online learning. Cesa-Bianchi et al. (2008) are the first to apply online discriminative re-ranking to a CAT scenario. Incremental adaptations of the generative components of SMT have been presented for a related scenario, interactive machine translation, where an MT component produces hypotheses based on partial translations of a sentence (Nepveu et al., 2004; Ortiz-Mart´ınez et al., 2010). Our online learning protocol is similar, but operating on the sentence instead of word or phrase level. Incremental adaptations have also been presented for larger batches of data (Bertoldi et al., 2012). In terms of granularity, our scenario is most similar to the work by Hardt and Elming (2010), where the Moses training procedure is employed to update the phrase table immediately after a reference becomes available. Our work, however, focuses on adapting both language and translation model with techniques where the global model remains unchanged. This is impo"
2013.mtsummit-papers.2,N10-1079,0,0.347766,"Missing"
2013.mtsummit-papers.2,P02-1040,0,0.0869298,"ange of millions of sentence pairs. Then for each document d, consisting of a few hundred up to a thousand sentences, a local model Md is created. For each example, first the static global model Mg and the current local model Md are combined into a model Mg+d . Then the input xt is translated into yˆt using the model Mg+d . Finally the local model Md is refined on feedback yt that is received immediately after producing yˆt . Evaluations reported in this paper take the local predictions yˆt and compare them to the user translations yt for each document, e.g., using |d| BLEU {(ˆ yt , yt )}t=1 (Papineni et al., 2002). Note that this setup differs from the more standard scenario where the whole test set is re-translated using the learned model. However, the evaluation in our online learning scenario is still fair since only feedback from previous test set examples is used to update the current model. We present three techniques for refinements of local SMT models (step 4), namely adaptations of the generative components of translation model (TM) (Section 3.3) and language model (LM) (Section 3.4) and adaptation via discriminative reranking (Section 3.5). Different refinements result in different modes of c"
2013.mtsummit-papers.4,P11-2031,0,0.0525206,"Missing"
2013.mtsummit-papers.4,D10-1044,0,0.0660693,"Missing"
2013.mtsummit-papers.4,P07-2045,1,0.00719976,"er; in fact: – there is a significant mismatch between IT-docs and the TM – the customer specific project TM-prjct matches the TM, not generic IT-docs • the IT-docs/TM mismatch can be reduced by properly selecting a portion of the TM. 4 Lab Test Results Lab tests have been performed on data sets described in Section 3. Performance are given in terms of BLEU and TER, computed by means of the MultEval script (Clark et al., 2011) that also provides the standard deviation σ, and of GTM.5 4.1 Baseline SMT for the IT domain An IT baseline system has been built upon the open-source MT toolkit Moses (Koehn et al., 2007). The translation and the lexicalized reordering models are trained on the parallel training data available (Table 1); a 6-gram LM smoothed through the improved Kneser-Ney technique (Chen and Goodman, 1999) is estimated on the target side via the IRSTLM toolkit (Federico et al., 2008). The weights of the log-linear interpolation model are optimized by means of the standard MERT procedure provided within the Moses toolkit. For the experiments, the set of IT-docs has been split into three equally sized blocks: the first is used for data selection (employed for adaptation, not for baselines), the"
2013.mtsummit-papers.4,D09-1074,0,0.0594647,"Missing"
2013.mtsummit-papers.4,P10-2041,0,0.0437632,"Missing"
2013.mtsummit-papers.4,W08-0320,0,0.0588141,"Missing"
2013.mtsummit-papers.4,2012.amta-papers.19,0,0.0312502,"nt sizes and content is rather typical. One way to face this combination issue is the fill-up technique, initially proposed by Nakov (2008) and then refined by Bisazza et al. (2011). Fill-up effectively exploits background knowledge to improve translation and distortion models coverage, while preserving the more reliable information coming from the foreground corpus. In practice, the background phrase table is merged with the foreground phrase table by adding only phrase pairs that do not appear in the foreground table. While performing at least as good as other popular adaptation techniques (Niehues and Waibel, 2012), fill-up approach builds models that are more compact and easier to tune by means of the minimum error rate training procedure. 2.3 Mixture LMs As concerns the LM adaptation, we employed the mixture of LMs since it is a well-established and good-performing method. The mixture model can be used to combine one or more background LMs with a foreground LM representing new features of the language we want to include (Federico and Bertoldi, 2004). The technique consists of the convex combination of the LMs; the mixture weights are estimated on the training data of the foreground LM by applying a cr"
2013.mtsummit-papers.4,steinberger-etal-2006-jrc,0,0.0700722,"n of the LMs; the mixture weights are estimated on the training data of the foreground LM by applying a cross-validation scheme that simulates the occurrence of new n-grams. The method is available in the IRSTLM toolkit (Federico et al., 2008). 28 3 #seg Data for Development For our experiments we relied on existing language resources, including parallel corpora and translation memories. For the IT domain, in addition to small publicly available corpora, proprietary data sets were employed (software documentation in general). For the Legal domain, the publicly available JRC-Acquis collection (Steinberger et al., 2006) was used, which mostly includes EU legislative texts translated into 22 languages. More details are provided in the next sections. 3.1 #tgt wrds TM+OPUS all entries (wd) 5.5M 63.8M 66.6M no duplicates (wod) 1.9M 27.8M 29.0M IT-docs 4.1k 56.0k 60.5k data-sel 1.4k 18.0k 19.3k dev 1.4k 21.1k 22.9k test 1.4k 16.9k 18.3k TM-prjct 1.8k 18.0k 18.7k dev 800 5.1k 5.4k test 989 12.9k 13.3k IT domain Most of text corpora for this domain were provided by the industrial partner of the MateCat project. In particular, we employed the following resources: • Translation Memory (TM): a large collection of para"
2013.mtsummit-papers.4,D11-1033,0,0.0376481,"Missing"
2013.mtsummit-papers.4,2011.iwslt-evaluation.18,1,0.885137,"of the background corpus. This score is the difference between the cross-entropy calculated with the foreground LM and the crossentropy calculated with the background LM. The background sentences are then ordered according to this score. The selection of useful sentences Fill-up for phrase-based SMT adaptation Given the scarcity of parallel linguistic resources, in SMT training, the need of combining data of parallel corpora of different sizes and content is rather typical. One way to face this combination issue is the fill-up technique, initially proposed by Nakov (2008) and then refined by Bisazza et al. (2011). Fill-up effectively exploits background knowledge to improve translation and distortion models coverage, while preserving the more reliable information coming from the foreground corpus. In practice, the background phrase table is merged with the foreground phrase table by adding only phrase pairs that do not appear in the foreground table. While performing at least as good as other popular adaptation techniques (Niehues and Waibel, 2012), fill-up approach builds models that are more compact and easier to tune by means of the minimum error rate training procedure. 2.3 Mixture LMs As concerns"
2013.mtsummit-papers.4,I08-2088,0,0.0718824,"Missing"
2013.mtsummit-papers.5,W12-3155,1,0.837488,"eover, we propose a further enhancement of the basic caching mechanism for rewarding cached items related to the current sentence to translate. Our work also deals with MT adaptation in general, and online learning more specifically. A thorough overview of the literature on these topics can be found in the companion paper (W¨aschle et al., 2013). Here, we just list some of the most representative papers: (Levenberg et al., 2010; Levenberg et al., 2011) for online learning methods in streaming scenarios; (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Bisazza et al., 2011; Liu et al., 2012; Bertoldi et al., 2012) for incremental adaptation. 3 Online Adaptation in SMT In the well established CAT work flow, source documents are split into chunks, typically corresponding to sentences and called segments, that are in general translated sequentially. When the translator opens a segment, the CAT tool tries to propose possible translation suggestions, originating from the translation memory and/or from a machine translation engine. Depending on the quality of the suggestions, the translator decides whether to post-edit one of them or to translate the source segment from scratch. Completed segments represent"
2013.mtsummit-papers.5,2011.iwslt-evaluation.18,1,0.249691,"e matches the sentence to translate. Moreover, we propose a further enhancement of the basic caching mechanism for rewarding cached items related to the current sentence to translate. Our work also deals with MT adaptation in general, and online learning more specifically. A thorough overview of the literature on these topics can be found in the companion paper (W¨aschle et al., 2013). Here, we just list some of the most representative papers: (Levenberg et al., 2010; Levenberg et al., 2011) for online learning methods in streaming scenarios; (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Bisazza et al., 2011; Liu et al., 2012; Bertoldi et al., 2012) for incremental adaptation. 3 Online Adaptation in SMT In the well established CAT work flow, source documents are split into chunks, typically corresponding to sentences and called segments, that are in general translated sequentially. When the translator opens a segment, the CAT tool tries to propose possible translation suggestions, originating from the translation memory and/or from a machine translation engine. Depending on the quality of the suggestions, the translator decides whether to post-edit one of them or to translate the source segment f"
2013.mtsummit-papers.5,2010.iwslt-papers.3,1,0.832754,"-pairs can be inserted, and (ii) scores of all entries are modified when new pairs are added. All entries are associated with an age, corresponding to the time they were actually 37 inserted, and scored accordingly. Each new insertion causes the ageing of the existing phrase pairs and hence their rescoring; in case of re-insertion of a phrase pair, the old value is overwritten. Phrase pairs are scored based on a negative exponential decaying function. For each segment pair, a set of phrase pairs are extracted from the partial alignment provided by the constrained search algorithm described by Cettolo et al. (2010). The procedure, detailed in (W¨aschle et al., 2013), extracts both already “known” and “new” pairs; the latter can provide translation options for OOVs and phrases including OOVs. All the extracted phrase pairs are simultaneously added to the local translation model by feeding the decoder with an XML-tag like: &lt;dlt cbtm=“The crude face of domination . Le visage rustre de la domination . |crude rustre |· · · |domination la domination |face visage”/&gt; The pair (x, y) consisting of the whole segments is also added to mimic the behaviour of a translation memory. During decoding, translation altern"
2013.mtsummit-papers.5,2012.eamt-1.60,1,0.823386,"(Federico et al., 2012) were considered, i.e. the translation of talks from Arabic to English and from English to French. 4.1.1 Training Data The training data for the IT domain mostly consist of data extracted from a translation memory built during the execution of translation projects commissioned by several commercial companies. In addition, parallel texts from the OPUS corpus (Tiedemann, 2012) were are also included. Training data of the Legal domain come from Version 3.0 of the JRC-Acquis collection (Steinberger et al., 2006), while TED training data are those released through the WIT3 (Cettolo et al., 2012) website for the IWSLT 2012 evaluation campaign (Federico et al., 2012). The reader may again refer to Table 1 for statistics on the actual corpora employed for training. Evaluations were performed on both proprietary and publicly available data, involving the translation of documents from three domains, namely Information Technology (IT), Legal (LGL) and TED talks,3 and for different language pairs. Table 1 summarizes the experimental framework in terms of data type. Commercial data were provided by the industrial partner of the MateCat project and were collected during the real use of CAT to"
2013.mtsummit-papers.5,2013.mtsummit-papers.4,1,0.805,"nguage pairs. Table 1 summarizes the experimental framework in terms of data type. Commercial data were provided by the industrial partner of the MateCat project and were collected during the real use of CAT tools by professional translators. Public data, which allow to 4.1.2 Evaluation Data Evaluation on IT and Legal domains has been performed on data collected in a two-day field test. During the first day (D0), MT suggestions came from baseline SMT systems (Section 4.4); during the second day (D1), they came from static SMT systems adapted to the post-edits of the first day as described in (Cettolo et al., 2013). For the IT domain, the text to be translated was taken from a software user manual. Concerning the Legal domain, the text was taken from a recent motion for a European Parliament resolution published on the EUR-Lex platform. Statistics on the test documents translated during the field test are reported in Table 2 (rows D0 and D1); they refer to tokenized texts. Figures on the source side (English) refer to the texts the users are requested to translate; figures on the target side (Italian) refer to the references, whatever they are actual post-edited texts or new translations. The additional"
2013.mtsummit-papers.5,P11-2031,0,0.0254423,"systems were developed with the Moses toolkit (Koehn et al., 2007). Translation and lexicalized reordering models were trained on the parallel training data; 6-gram (for IT and Legal systems) and 5-gram (for TED systems) LMs with improved Kneser-Ney smoothing (Chen and Goodman, 1999) were estimated on the target side of the training parallel data with the IRSTLM toolkit (Federico et al., 2008). The weights of the log-linear interpolation model were optimized via the MERT procedure provided with Moses. Performance are provided in terms of BLEU and TER, computed by means of the MultEval script (Clark et al., 2011) that also provides the standard deviation σ, and of GTM8 Here some specific features of systems developed for each task: IT/Legal baselines: different weights were used in the two days of the field test, 8 nlp.cs.nyu.edu/GTM 40 task IT LGL system baseline +cache +context baseline +cache +context BLEU (σ) 44.69 (1.67) 47.25 (1.81) 47.89 (1.82) 47.66 (2.13) 47.69 (2.05) 46.58 (2.05) D0 TER (σ) 36.34 (1.30) 34.83 (1.34) 34.61 (1.44) 34.69 (1.71) 33.94 (1.63) 34.74 (1.65) GTM 74.59 76.36 76.99 73.88 75.04 74.49 BLEU (σ) 41.06 (1.57) 44.61 (1.72) 48.04 (1.93) 47.42 (2.11) 47.57 (2.09) 48.07 (2.16)"
2013.mtsummit-papers.5,W07-0717,0,0.0370258,"ptions whose source side matches the sentence to translate. Moreover, we propose a further enhancement of the basic caching mechanism for rewarding cached items related to the current sentence to translate. Our work also deals with MT adaptation in general, and online learning more specifically. A thorough overview of the literature on these topics can be found in the companion paper (W¨aschle et al., 2013). Here, we just list some of the most representative papers: (Levenberg et al., 2010; Levenberg et al., 2011) for online learning methods in streaming scenarios; (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Bisazza et al., 2011; Liu et al., 2012; Bertoldi et al., 2012) for incremental adaptation. 3 Online Adaptation in SMT In the well established CAT work flow, source documents are split into chunks, typically corresponding to sentences and called segments, that are in general translated sequentially. When the translator opens a segment, the CAT tool tries to propose possible translation suggestions, originating from the translation memory and/or from a machine translation engine. Depending on the quality of the suggestions, the translator decides whether to post-edit one of them or to translat"
2013.mtsummit-papers.5,W07-0733,0,0.0223156,"h, fed by all translation options whose source side matches the sentence to translate. Moreover, we propose a further enhancement of the basic caching mechanism for rewarding cached items related to the current sentence to translate. Our work also deals with MT adaptation in general, and online learning more specifically. A thorough overview of the literature on these topics can be found in the companion paper (W¨aschle et al., 2013). Here, we just list some of the most representative papers: (Levenberg et al., 2010; Levenberg et al., 2011) for online learning methods in streaming scenarios; (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Bisazza et al., 2011; Liu et al., 2012; Bertoldi et al., 2012) for incremental adaptation. 3 Online Adaptation in SMT In the well established CAT work flow, source documents are split into chunks, typically corresponding to sentences and called segments, that are in general translated sequentially. When the translator opens a segment, the CAT tool tries to propose possible translation suggestions, originating from the translation memory and/or from a machine translation engine. Depending on the quality of the suggestions, the translator decides whether to post-edit one"
2013.mtsummit-papers.5,P07-2045,1,0.0105799,"ce segment xt is received . . . [optional step] . . . a translation yˆt is computed with Mt a post-edited translation yt is received a new system Mt+1 is created by adapting Mt with features extracted from (xt , yt ) Step 5 is implemented via a caching mechanism that allows us to define and dynamically adapt local (with respect to the document) models that are combined during decoding with the global SMT models estimated on the training data. In the following, we present how the local cache-based models are defined and adapted under the wellknown phrase-based SMT setting of the Moses toolkit (Koehn et al., 2007). They will be included in a future release of the toolkit. The optional step 2 is additional with respect to the basic online learning procedure and comprises the updating of Mt with context features, as described in more detail in Section 3.3. 3.1 TM Adaptation The pair (x, y) composed of a source segment and its post-edited translation, is exploited to update a local translation model. This model is intended for integrating new translation alternatives suggested by the user and for rewarding those approved, with the ultimate goal of translating the successive segments more consistently with"
2013.mtsummit-papers.5,N10-1062,0,0.161242,"-edition; explicit, possibly partial, phrase-alignment is obtained via an efficient constrained search, fed by all translation options whose source side matches the sentence to translate. Moreover, we propose a further enhancement of the basic caching mechanism for rewarding cached items related to the current sentence to translate. Our work also deals with MT adaptation in general, and online learning more specifically. A thorough overview of the literature on these topics can be found in the companion paper (W¨aschle et al., 2013). Here, we just list some of the most representative papers: (Levenberg et al., 2010; Levenberg et al., 2011) for online learning methods in streaming scenarios; (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Bisazza et al., 2011; Liu et al., 2012; Bertoldi et al., 2012) for incremental adaptation. 3 Online Adaptation in SMT In the well established CAT work flow, source documents are split into chunks, typically corresponding to sentences and called segments, that are in general translated sequentially. When the translator opens a segment, the CAT tool tries to propose possible translation suggestions, originating from the translation memory and/or from a machine translat"
2013.mtsummit-papers.5,W11-2122,0,0.0186151,"ibly partial, phrase-alignment is obtained via an efficient constrained search, fed by all translation options whose source side matches the sentence to translate. Moreover, we propose a further enhancement of the basic caching mechanism for rewarding cached items related to the current sentence to translate. Our work also deals with MT adaptation in general, and online learning more specifically. A thorough overview of the literature on these topics can be found in the companion paper (W¨aschle et al., 2013). Here, we just list some of the most representative papers: (Levenberg et al., 2010; Levenberg et al., 2011) for online learning methods in streaming scenarios; (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Bisazza et al., 2011; Liu et al., 2012; Bertoldi et al., 2012) for incremental adaptation. 3 Online Adaptation in SMT In the well established CAT work flow, source documents are split into chunks, typically corresponding to sentences and called segments, that are in general translated sequentially. When the translator opens a segment, the CAT tool tries to propose possible translation suggestions, originating from the translation memory and/or from a machine translation engine. Depending on"
2013.mtsummit-papers.5,D12-1037,0,0.0337503,"to translate. Moreover, we propose a further enhancement of the basic caching mechanism for rewarding cached items related to the current sentence to translate. Our work also deals with MT adaptation in general, and online learning more specifically. A thorough overview of the literature on these topics can be found in the companion paper (W¨aschle et al., 2013). Here, we just list some of the most representative papers: (Levenberg et al., 2010; Levenberg et al., 2011) for online learning methods in streaming scenarios; (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Bisazza et al., 2011; Liu et al., 2012; Bertoldi et al., 2012) for incremental adaptation. 3 Online Adaptation in SMT In the well established CAT work flow, source documents are split into chunks, typically corresponding to sentences and called segments, that are in general translated sequentially. When the translator opens a segment, the CAT tool tries to propose possible translation suggestions, originating from the translation memory and/or from a machine translation engine. Depending on the quality of the suggestions, the translator decides whether to post-edit one of them or to translate the source segment from scratch. Compl"
2013.mtsummit-papers.5,W04-3225,0,0.675866,"and CasmaCat2 European projects, which are jointly developing a new generation CAT tool integrating novel interaction modalities and MT functions. In this paper we deal with SMT models which dynamically learn from the user feedback by means of a caching mechanism. The main idea behind cache-based models is to mix a large global (static) model with a small local (dynamic) model estimated from recent items observed in the history of the input stream. In (Kuhn and De Mori, 1990), the caching mechanism was applied to language models, and only very later its use was extended to translation models (Nepveu et al., 2004). In lab tests, cache-based language and translation models have already proven to be effective in interactive SMT (Nepveu et al., 2004); in adapting generic SMT models, (Tiedemann, 2010) obtained interesting but not definitive gains, the “main obstacle” being “the invalid assumption that initial translations are correct”. In a real CAT framework like ours, the cached items are correct by definition. The road map of our work is defined through a list of research questions addressed in this paper. Clearly, the first questions we are interested in is: Q1 - Is the effectiveness of cache-based ada"
2013.mtsummit-papers.5,steinberger-etal-2006-jrc,0,0.032325,"ency of observed feature. To mimic the way translation memory works, we also decide to reward target ngram and phrase-pair features observed in similar and already translated segments of the document. This implies adding the following lazy learning step to the previous on-line learning algorithm: 2. Mt is updated with features extracted from the pair (x, y) in {(x1 , y1 ), . . . , (xt−1 , yt−1 )} such that x is most similar to xt . replicate and cross-assess our outcomes, were chosen to cover a wide range of linguistic conditions. For the Legal domain, we worked with the JRCAcquis collection (Steinberger et al., 2006), on the English to Italian direction. The same direction is also investigated for the IT domain. As concerns TED, the two official MT tasks of the IWSLT 2012 evaluation campaign (Federico et al., 2012) were considered, i.e. the translation of talks from Arabic to English and from English to French. 4.1.1 Training Data The training data for the IT domain mostly consist of data extracted from a translation memory built during the execution of translation projects commissioned by several commercial companies. In addition, parallel texts from the OPUS corpus (Tiedemann, 2012) were are also includ"
2013.mtsummit-papers.5,W10-2602,0,0.0475392,"dynamically learn from the user feedback by means of a caching mechanism. The main idea behind cache-based models is to mix a large global (static) model with a small local (dynamic) model estimated from recent items observed in the history of the input stream. In (Kuhn and De Mori, 1990), the caching mechanism was applied to language models, and only very later its use was extended to translation models (Nepveu et al., 2004). In lab tests, cache-based language and translation models have already proven to be effective in interactive SMT (Nepveu et al., 2004); in adapting generic SMT models, (Tiedemann, 2010) obtained interesting but not definitive gains, the “main obstacle” being “the invalid assumption that initial translations are correct”. In a real CAT framework like ours, the cached items are correct by definition. The road map of our work is defined through a list of research questions addressed in this paper. Clearly, the first questions we are interested in is: Q1 - Is the effectiveness of cache-based adaptation confirmed in a real CAT environment? As mentioned in (Tiedemann, 2010), there are two types of important properties in natural language and translation that are often ignored in s"
2013.mtsummit-papers.5,tiedemann-2012-parallel,0,0.0187782,"collection (Steinberger et al., 2006), on the English to Italian direction. The same direction is also investigated for the IT domain. As concerns TED, the two official MT tasks of the IWSLT 2012 evaluation campaign (Federico et al., 2012) were considered, i.e. the translation of talks from Arabic to English and from English to French. 4.1.1 Training Data The training data for the IT domain mostly consist of data extracted from a translation memory built during the execution of translation projects commissioned by several commercial companies. In addition, parallel texts from the OPUS corpus (Tiedemann, 2012) were are also included. Training data of the Legal domain come from Version 3.0 of the JRC-Acquis collection (Steinberger et al., 2006), while TED training data are those released through the WIT3 (Cettolo et al., 2012) website for the IWSLT 2012 evaluation campaign (Federico et al., 2012). The reader may again refer to Table 1 for statistics on the actual corpora employed for training. Evaluations were performed on both proprietary and publicly available data, involving the translation of documents from three domains, namely Information Technology (IT), Legal (LGL) and TED talks,3 and for di"
2013.mtsummit-papers.5,2013.mtsummit-papers.2,1,0.847457,"Missing"
2013.mtsummit-papers.5,federico-etal-2012-iwslt,1,\N,Missing
2013.mtsummit-wptp.13,D11-1033,0,0.0228773,"o adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram LM. However, this is in general true only if"
2013.mtsummit-wptp.13,N09-2038,0,0.0242009,"the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is performed on the MT outputs. On standard machine translation tasks, Niehues and Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in impr"
2013.mtsummit-wptp.13,W12-3155,1,0.851259,"ently homogeneous, the language is sufficiently complex, and there is sufficient multilingual data available to train and tune MT models. The paper is organized as follows. Section 2 lists some of the related works. Section 3 introduces methods used for project adaptation. Section 4 briefly describes the conduct of the field test. Section 5 and Section 6, respectively, introduce the set-up and results of experiments. Section 7 concludes the paper with a discussion on the overall results. 2 Related Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (e"
2013.mtsummit-wptp.13,2011.iwslt-evaluation.18,1,0.827144,"elated Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is pe"
2013.mtsummit-wptp.13,P11-2031,0,0.0146555,"Dntgt , n=0,1) or the concatenation of the source side of both the development and test set (D01src ); we name FGtgt and FGsrc the selected corpus and the models trained on it in the two cases. The table also provides the percentage of data selected, computed with respect to the target side. The optimal splitting was performed by minimizing the perplexity of the target side of the development set. 6 Experiments Lab test experiments have been performed on data sets described in Section 5. Performance are provided in terms of BLEU and TER, computed by means of the MultEval script implemented by Clark et al. (2011), and of GTM.4 For statistical significance, p-values were calculated via approximate randomization for adapted systems with respect to the baselines and are reported in Tables 5 and 6 whenever not larger than 0.10. The SMT systems have been built upon the open-source MT toolkit Moses (Koehn et al., 2007). The translation and the lexicalized reordering models are trained on the available parallel training data (Table 1); 5-gram LMs smoothed through the improved Kneser-Ney tech4 http://nlp.cs.nyu.edu/GTM nique (Chen and Goodman, 1999) are estimated on the target side via the IRSTLM toolkit (Fed"
2013.mtsummit-wptp.13,2012.amta-papers.22,1,0.534373,"MT suggestions for the second half of the document came from a system adapted to the text of the first day by means of one of the adaptation methods tested in our experiments (Section 6). Translators post-edited machine-generated translations for correcting mistakes and making them stylistically appropriate. The document was selected such that the size of its halves corresponds approximately to the daily productivity of professional translators, that is three to five thousand words. A report on the field test including an analysis of the productivity of translators has already been published (Federico et al., 2012). Moreover, we performed a preliminary measure of the performance of MT outputs versus the post-edition of each translator. In both cases, pretty large inter-translator differences were observed. Since the limited number of subjects would have led to scores with large variances, we decided to choose segments en→de up, called backoff, in which the indicator feature is discarded. Again, the backoff method proposed by Niehues and Waibel (2012) differs slightly in the way the scores of the phrase pairs stemming from the background phrase table are computed. Language model: As concerns the LM adapt"
2013.mtsummit-wptp.13,P02-1023,0,0.0361356,"Missing"
2013.mtsummit-wptp.13,W07-0733,0,0.0352986,"t-up and results of experiments. Section 7 concludes the paper with a discussion on the overall results. 2 Related Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performan"
2013.mtsummit-wptp.13,P07-2045,1,0.00690725,"plitting was performed by minimizing the perplexity of the target side of the development set. 6 Experiments Lab test experiments have been performed on data sets described in Section 5. Performance are provided in terms of BLEU and TER, computed by means of the MultEval script implemented by Clark et al. (2011), and of GTM.4 For statistical significance, p-values were calculated via approximate randomization for adapted systems with respect to the baselines and are reported in Tables 5 and 6 whenever not larger than 0.10. The SMT systems have been built upon the open-source MT toolkit Moses (Koehn et al., 2007). The translation and the lexicalized reordering models are trained on the available parallel training data (Table 1); 5-gram LMs smoothed through the improved Kneser-Ney tech4 http://nlp.cs.nyu.edu/GTM nique (Chen and Goodman, 1999) are estimated on the target side via the IRSTLM toolkit (Federico et al., 2008). The weights of the log-linear interpolation model have been optimized by means of the Margin Infused Relaxed Algorithm (MIRA) process (Hasler et al., 2011) provided within the Moses toolkit. Various models have been built by means of the methods described in Section 3. Here the list o"
2013.mtsummit-wptp.13,D12-1037,0,0.0226802,"and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is performed on the MT outputs. On standard machine transla"
2013.mtsummit-wptp.13,D09-1074,0,0.0233102,"Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram"
2013.mtsummit-wptp.13,P10-2041,0,0.0161319,"of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram LM. However, this is in general true only if the new data is enough relevant to the task at hand, a condition"
2013.mtsummit-wptp.13,W08-0320,0,0.0387515,"Missing"
2013.mtsummit-wptp.13,2012.amta-papers.19,0,0.127366,"rien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is performed on the MT outputs. On standard machine translation tasks, Niehues and Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et a"
2013.mtsummit-wptp.13,W12-3147,1,0.900227,"Missing"
2013.mtsummit-wptp.13,steinberger-etal-2006-jrc,0,0.0170217,"Missing"
2013.mtsummit-wptp.13,I08-2088,0,0.0276276,"on tasks, Niehues and Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistic"
2013.mtsummit-wptp.13,W07-0717,0,0.0387197,"a discussion on the overall results. 2 Related Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the M"
2013.mtsummit-wptp.13,D10-1044,0,0.0217311,"ifferent approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram LM. However, this is"
2014.amta-researchers.12,2013.mtsummit-papers.5,1,0.785139,"e the multi-task scenario. Legal domain data has been release as a part of JRC-acquis corpus (Steinberger et al., 2006). The dataset contains translation of legal documents from English to Italian. This dataset was also a part of the field test carried out under the same MateCat project, so essentially we have post-edited data from 4 different translators on a test set of 90 sentences. Since our methods regard the adaptation of MT models, the potential impact strictly depends on how much the considered text is repetitive. For measuring that text feature, we use the repetition rate proposed by Bertoldi et al. (2013). Equation 9 shows the formula for calculating the repetition rate of a document, where dict(n) represents the total number of different 2 In this paper we use the terms tasks and translators interchangeably as the tasks are translators in the CAT scenario. 3 To keep the notation light we again drop the dependency of h from x. 4 http://www.matecat.com Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 155 n-grams and nr is the number of different n-grams occurring exactly r times. Statistics of the parallel sets on source and target sides al"
2014.amta-researchers.12,C14-1040,1,0.88094,"Missing"
2014.amta-researchers.12,J93-3001,0,0.0669408,"Missing"
2014.amta-researchers.12,P13-1004,0,0.0276447,"e use a matrix-based regularization approach on the core features for online adaptation of all the translation models. Multi-Task learning has also been used in re-ranking the N-best list by Duh et al. (2010). Each N-Best list is considered as a different task and the weights are jointly learnt over a large set of sparse features. Simianer et al. (2011) trained a discriminative model using multi-task learning over a set of k documents belonging to different topics but with strong commonalities. Recent application of multi-task learning has been in quality estimation for machine translation by Cohn and Specia (2013) where the authors model annotator bias using multi-task Gaussian processes. Their model outperforms the annotator specific model and thus boosting the use of Multi-Task learning in NLP applications. Another application of MTL has been in supervised domain adaptation for quality estimation (C. de Souza et al., 2014). In this work the authors leverage all available training labels from different domains in order to learn a robust model for a target domain with very little labeled data. The approach proposed outperforms independent models trained separetely on each domain. 7 Conclusion We addres"
2014.amta-researchers.12,E14-1042,0,0.101531,"Missing"
2014.amta-researchers.12,W10-1757,0,0.0236818,"explored in SMT in the context of tuning the sparse log linear weights by Simianer et al. (2012) where they split the training set in random shards and perform a joint feature selection over these shards using `1 /`2 regularization. In this way after each epoch, the size of feature vector decreases and only the important features are taken into account. In our paper instead of `1 /`2 regularization we have use a matrix-based regularization approach on the core features for online adaptation of all the translation models. Multi-Task learning has also been used in re-ranking the N-best list by Duh et al. (2010). Each N-Best list is considered as a different task and the weights are jointly learnt over a large set of sparse features. Simianer et al. (2011) trained a discriminative model using multi-task learning over a set of k documents belonging to different topics but with strong commonalities. Recent application of multi-task learning has been in quality estimation for machine translation by Cohn and Specia (2013) where the authors model annotator bias using multi-task Gaussian processes. Their model outperforms the annotator specific model and thus boosting the use of Multi-Task learning in NLP"
2014.amta-researchers.12,1993.eamt-1.1,0,0.313734,"Missing"
2014.amta-researchers.12,2012.amta-papers.22,1,0.685493,"nd on a popular benchmark with multiple references, respectively. 1 Introduction In a professional localization environment, a document is post-edited by several professional translators with assistance of tools such as translation memory, dictionary, spell checkers etc. To speed up the process, lately localization companies have started using computer assisted translation (CAT) tools with statistical machine translation (SMT) systems in the backend. The role played by the SMT engine is to provide a translation hypothesis that the translator can post edit to produce high quality translations (Federico et al., 2012). In recent works on online adaptation by Mathur et al. (2013) and Denkowski et al. (2014), the SMT is fed with the post edited sentence, allowing the models to adapt to the corrections made by the translators. These kind of systems works well if the document is being post edited by a single translator because models can adapt to the style of that translator. Problems arise when a document is being post edited by a group of translators which is usually the case with big size documents. In fact, if the SMT system adapts to the corrections of all translators together, it will likely mix or overl"
2014.amta-researchers.12,P07-2045,1,0.0108768,"target side is 22.18, as reported in Table 1; this could be due to the fact that translators tend to be not consistent among themselves, yielding less repetitions in each post-edited test set than in the shuffled test set. #Sentence 1 2 3 4 Sentence Input Date # 0 Input Date # Data di input # 0 Evaluates conditionally # 1 Evaluates conditionally # Valuta in modo condizionale # 1 OnlineLearning Not Activated Activated Not Activated Activated Translator ID 0 0 1 1 Table 2: Excerpt from IT development set tagged with meta data. 5.2 Experiments The SMT systems were built using the Moses toolkit (Koehn et al., 2007). Domain specific training data was used to create translation and lexical reordering models. 5-gram language models for each task were estimated by means of IRSTLM toolkit (Federico et al., 2008), with improved Kneser-Ney smoothing (Chen and Goodman, 1998), on the target side of the training parallel corpora. After the training of MT models, the log linear weights were optimized using Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 156 MERT (Och, 2003) implementation provided in the Moses toolkit. Performance is computed not with corpus"
2014.amta-researchers.12,C04-1072,0,0.0308242,"learning algorithm which can be applied in CAT scenario. Experiments and results are shown in Section 5. We conclude the paper with a preview of interesting related works and few words about the future work. 2 Background: Online Large Margin Training Previous work by Mathur et al. (2013) applies an online large margin algorithm (MIRA), that updates the weights w of a phrase-based SMT model according to the loss that is occurred due to an incorrect translation. The margin is coupled with the following loss function based on the complement of the sentence level BLEU (BLEU+1, henceforth sBLEU) (Lin and Och, 2004; Nakov et al., 2012): lj = sBLEU (y ∗ ) − sBLEU (yj ) (1) where y ∗ is the oracle (closest translation to the reference) and yj is the j-th candidate being processed inside an N -best list. According to (Watanabe et al., 2007), weights are updated so that the loss is not larger than the difference between the scores given by the model: lj ≤ wT ∆hj (2) where ∆hj is the difference between the feature vectors of the oracle and the candidate, and w is the weight vector. Hence, the size of the weight update is: arg min ||w − w0 ||+ C w X ξj j subject to T w ∆hj + ξj ≥ lj ξj ≥ 0 ∀j ∈ {1 . . . N } A"
2014.amta-researchers.12,W13-2237,1,0.915445,". 1 Introduction In a professional localization environment, a document is post-edited by several professional translators with assistance of tools such as translation memory, dictionary, spell checkers etc. To speed up the process, lately localization companies have started using computer assisted translation (CAT) tools with statistical machine translation (SMT) systems in the backend. The role played by the SMT engine is to provide a translation hypothesis that the translator can post edit to produce high quality translations (Federico et al., 2012). In recent works on online adaptation by Mathur et al. (2013) and Denkowski et al. (2014), the SMT is fed with the post edited sentence, allowing the models to adapt to the corrections made by the translators. These kind of systems works well if the document is being post edited by a single translator because models can adapt to the style of that translator. Problems arise when a document is being post edited by a group of translators which is usually the case with big size documents. In fact, if the SMT system adapts to the corrections of all translators together, it will likely mix or overlap stylistic features of the post-editors and thus not learn t"
2014.amta-researchers.12,C12-1121,0,0.0234231,"Missing"
2014.amta-researchers.12,J03-1002,0,0.00383573,"di produzione nel volume di backup . you create a backup and after it completes # 1 possibile creare una copia di riserva e dopo il completamento creare un backup e dopo il completamento possibile creare un backup e , al suo completamento Table 8: Example from the IT test set. Here Multi-Task refers to MTL-pearson system and K-ind is K-independent system. 6 Related Works Despite several online adaptation strategies have been proposed in the past, only a few deal with adaptation of post-edited/evaluation data while most works are on adaptation over development data during tuning of parameters (Och and Ney, 2003). Cesa-Bianchi et al. (2008) proposed an online learning approach during decoding. They construct a layer of online weights over the regular feature weights and update these weights at sentence level using margin infused relaxed algorithm (Crammer and Singer, 2003); to our knowledge, this is the first work on online adaptation during decoding. Mart´ınez-G´omez et al. (2011, 2012) presented a comparison of online adaptation techniques in post editing scenario. They compared different adaptation strategies on feature weights and features itself. Multi-Task learning has been explored in SMT in th"
2014.amta-researchers.12,P03-1021,0,0.0421863,"eta data. 5.2 Experiments The SMT systems were built using the Moses toolkit (Koehn et al., 2007). Domain specific training data was used to create translation and lexical reordering models. 5-gram language models for each task were estimated by means of IRSTLM toolkit (Federico et al., 2008), with improved Kneser-Ney smoothing (Chen and Goodman, 1998), on the target side of the training parallel corpora. After the training of MT models, the log linear weights were optimized using Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 156 MERT (Och, 2003) implementation provided in the Moses toolkit. Performance is computed not with corpus level metrics but with sentence level metrics. We decided to do this to avoid a metric mismatch between the evaluation and actual optimization where the margin is calculated by the sentence level BLEU scores (refer to Section 2). Therefore, we computed sBLEU scores and sentence level TER (Snover et al., 2006) scores and reported their average over the whole documents. We call them avg-sBLEU and avg-sTER. Calculating A−1 matrix: Interaction matrix can be computed in different ways. It basically conveys the re"
2014.amta-researchers.12,W05-0908,0,0.0636609,"Missing"
2014.amta-researchers.12,P12-1002,0,0.0191377,"ing approach during decoding. They construct a layer of online weights over the regular feature weights and update these weights at sentence level using margin infused relaxed algorithm (Crammer and Singer, 2003); to our knowledge, this is the first work on online adaptation during decoding. Mart´ınez-G´omez et al. (2011, 2012) presented a comparison of online adaptation techniques in post editing scenario. They compared different adaptation strategies on feature weights and features itself. Multi-Task learning has been explored in SMT in the context of tuning the sparse log linear weights by Simianer et al. (2012) where they split the training set in random shards and perform a joint feature selection over these shards using `1 /`2 regularization. In this way after each epoch, the size of feature vector decreases and only the important features are taken into account. In our paper instead of `1 /`2 regularization we have use a matrix-based regularization approach on the core features for online adaptation of all the translation models. Multi-Task learning has also been used in re-ranking the N-best list by Duh et al. (2010). Each N-Best list is considered as a different task and the weights are jointly"
2014.amta-researchers.12,2006.amta-papers.25,0,0.0694535,"ing parallel corpora. After the training of MT models, the log linear weights were optimized using Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 156 MERT (Och, 2003) implementation provided in the Moses toolkit. Performance is computed not with corpus level metrics but with sentence level metrics. We decided to do this to avoid a metric mismatch between the evaluation and actual optimization where the margin is calculated by the sentence level BLEU scores (refer to Section 2). Therefore, we computed sBLEU scores and sentence level TER (Snover et al., 2006) scores and reported their average over the whole documents. We call them avg-sBLEU and avg-sTER. Calculating A−1 matrix: Interaction matrix can be computed in different ways. It basically conveys the relatedness/correlation between the translators who are post-editing a particular document. Usually a localization company keeps a ranking of the hired translators with them; either we can use the ranking to exploit the relatedness between the translators or we can calculate their correlation based on a known previous post-edited data set. Here, we assume that the relatedness between the translat"
2014.amta-researchers.12,steinberger-etal-2006-jrc,0,0.102397,"Missing"
2014.amta-researchers.12,D07-1080,0,0.0241727,"Online Large Margin Training Previous work by Mathur et al. (2013) applies an online large margin algorithm (MIRA), that updates the weights w of a phrase-based SMT model according to the loss that is occurred due to an incorrect translation. The margin is coupled with the following loss function based on the complement of the sentence level BLEU (BLEU+1, henceforth sBLEU) (Lin and Och, 2004; Nakov et al., 2012): lj = sBLEU (y ∗ ) − sBLEU (yj ) (1) where y ∗ is the oracle (closest translation to the reference) and yj is the j-th candidate being processed inside an N -best list. According to (Watanabe et al., 2007), weights are updated so that the loss is not larger than the difference between the scores given by the model: lj ≤ wT ∆hj (2) where ∆hj is the difference between the feature vectors of the oracle and the candidate, and w is the weight vector. Hence, the size of the weight update is: arg min ||w − w0 ||+ C w X ξj j subject to T w ∆hj + ξj ≥ lj ξj ≥ 0 ∀j ∈ {1 . . . N } Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC (3) © The Authors 153 C is an aggressiveness parameter which controls the size of the update and ξ are slack variables. Following (Watanab"
2014.amta-researchers.13,D11-1033,0,0.0121256,"d, which mostly includes EU legislative texts translated into 22 languages. Table 2 provides detailed statistics on the actual bitexts used for training purposes. In particular, the train entries refer to the whole generic training texts, while development set entries to additional data on which the parameters of the phrase-based MT model were optimized. The domain selection entry of the IT en→fr task refers to data selected from out-ofdomain texts (Giga English-French, United Nation, and Common Crawl corpora6 (Bojar et al., 2013)) by using the in-domain text as seed in the method proposed by Axelrod et al. (2011) and available within the XenC toolkit (Rousseau, 2013); this was done to augment the amount of training data, since the size of in-domain text available for that language pair (15.4/17.9 million words) is about four times smaller than for the other tasks. domain pair en→it IT en→fr en→it Legal en→fr en→es corpus segments train development set train domain selection development set train development set train development set train development set 5.4 M 2,156 1.1 M 1.2 M 4,755 2.7 M 181 2.8 M 600 2.3 M 700 tokens source target 57.2M 59.9M 26,080 28,137 15.4M 17.9M 20.0M 22.2M 26,747 30,100 61.4"
2014.amta-researchers.13,2013.mtsummit-papers.5,1,0.943654,"ed for capturing the repetitiveness of text. Indeed, in the case of MT related tasks, like the quality estimation of MT output, in addition to features computed on the source text, features have been proposed which involve the translated/target text or even the MT models: although they can be really effective, we focus our investigation to the source side only, since we are interested in deciding what kind of MT system is most suitable for translating a given text before having any MT engine at disposal. In this paper we experimentally assess the repetition rate, that we recently proposed in (Bertoldi et al., 2013) where no support to its effectiveness was provided, as a single light measure to characterize a full document to be translated. Roughly, the repetition rate computes the rate of event types (single words and n-grams) that occur more than once in a text; for making this statistics independent from the size of the document, it is computed on a fixed-size sliding window. We measured the prediction power of the repetition rate on several MT adaptation tasks and compared it against other text features that were proposed for very related NLP tasks. The comparison was carried out through a regressio"
2014.amta-researchers.13,I13-1185,0,0.045335,"Missing"
2014.amta-researchers.13,J10-4005,0,0.0180421,"analysis between feature values and MT performance gains by dynamically adapted versus non-adapted MT engines, on five different translation tasks. The main outcome of experiments is that the repetition rate correlates better than any other considered feature with the MT gains yielded by the online adaptation, although using all features jointly results in better predictions than with any single feature. 1 Introduction Language and content repetitiveness1 is a key factor for the successful deployment of translation memories (TMs) (Somers, 2003) as well as statistical machine translation (MT) (Koehn, 2010). The capability of a TM to provide useful translation suggestions for a text segment relies on the chance of finding segments with very similar content – i.e. with a significant percentage of overlapping words – inside a large repository of already translated texts. On the other hand, statistical MT also relies on the assumption that the segment to be translated shares with the training data a significant amount of patterns, from single words to groups of words. Advances on the integration of human post-editing into MT have recently revealed the potential of incremental and online MT adaptati"
2014.amta-researchers.13,P07-2045,1,0.0161214,"Missing"
2014.amta-researchers.13,P11-1132,0,0.0267725,"oportion of simple/complex sentences, ambiguity as the average of senses per word, word length as the proportion of syllables per word, lexical richness, and information load as the proportion of lexical words to tokens in (Ilisei et al., 2010); most frequent words and 3 Obfuscation is the strategy adopted by real plagiarists to rewrite their source passages in order to make detection more difficult. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 168 a list of some hundred function words are instead used in (Islam and Hoenen, 2013) and (Koppel and Ordan, 2011), respectively. 3 Repetition Rate We recently introduced the repetition rate (Bertoldi et al., 2013) as a way to measure the repetitiveness inside a text, by looking at the rate of non-singleton n-gram types (n=1. . .4) it contains. As shown there, this rate decays exponentially with n. For combining values with exponential decay, a reasonable scheme is to average their logarithms, or equivalently to compute their geometric mean. Furthermore, in order to make the measure comparable across different sized documents, statistics are collected on a sliding window of one thousand words, and properl"
2014.amta-researchers.13,P02-1040,0,0.0900417,"s iterates over all sentences of the document to be translated. In our systems, the local model is implemented by a caching mechanism. The caching regards both translation and language models: phrase pairs extracted from the alignment of the source and post-edit are extracted and inserted into the cache-based translation model, while n-grams of the post-edit fill the cache-based language model. More details are provided in (Bertoldi et al., 2013). Note that in our experiments the post-editing is simulated by using human references. 5.5 Results and comments First of all, Table 3 provides BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and GTM (Turian et al., 2003) scores computed on the evaluation documents with respect to human references for each of the five considered translation tasks. In both domains, but especially for IT, the improvements over the static systems yielded by the dynamic adaptation technique are remarkable. Focusing on the BLEU score, in the IT domain it improves by more than 7 absolute points for English-to-Italian (57.5 to 64.6), and almost 15 absolute points for English-to-French (41.4 to 56.3); in the LGL domain, the gain is quite limited in English-to-Italian (1.5 absolu"
2014.amta-researchers.13,2006.amta-papers.25,0,0.0546353,"of the document to be translated. In our systems, the local model is implemented by a caching mechanism. The caching regards both translation and language models: phrase pairs extracted from the alignment of the source and post-edit are extracted and inserted into the cache-based translation model, while n-grams of the post-edit fill the cache-based language model. More details are provided in (Bertoldi et al., 2013). Note that in our experiments the post-editing is simulated by using human references. 5.5 Results and comments First of all, Table 3 provides BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and GTM (Turian et al., 2003) scores computed on the evaluation documents with respect to human references for each of the five considered translation tasks. In both domains, but especially for IT, the improvements over the static systems yielded by the dynamic adaptation technique are remarkable. Focusing on the BLEU score, in the IT domain it improves by more than 7 absolute points for English-to-Italian (57.5 to 64.6), and almost 15 absolute points for English-to-French (41.4 to 56.3); in the LGL domain, the gain is quite limited in English-to-Italian (1.5 absolute points), but definitely"
2014.amta-researchers.13,steinberger-etal-2006-jrc,0,0.0939907,"Missing"
2014.amta-researchers.13,tiedemann-2012-parallel,0,0.0298076,"Missing"
2014.amta-researchers.13,2003.mtsummit-papers.51,0,0.043002,"ted. In our systems, the local model is implemented by a caching mechanism. The caching regards both translation and language models: phrase pairs extracted from the alignment of the source and post-edit are extracted and inserted into the cache-based translation model, while n-grams of the post-edit fill the cache-based language model. More details are provided in (Bertoldi et al., 2013). Note that in our experiments the post-editing is simulated by using human references. 5.5 Results and comments First of all, Table 3 provides BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and GTM (Turian et al., 2003) scores computed on the evaluation documents with respect to human references for each of the five considered translation tasks. In both domains, but especially for IT, the improvements over the static systems yielded by the dynamic adaptation technique are remarkable. Focusing on the BLEU score, in the IT domain it improves by more than 7 absolute points for English-to-Italian (57.5 to 64.6), and almost 15 absolute points for English-to-French (41.4 to 56.3); in the LGL domain, the gain is quite limited in English-to-Italian (1.5 absolute points), but definitely notable – almost 5 points – fo"
2014.amta-researchers.13,W12-2019,0,0.129424,"ts over the use of single tokens, especially by introducing not too long n-grams (F¨urnkranz, 1998), outcome that we exploit by defining the repetition rate over 1- to 4-grams. Readability assessment is a form of text classification aiming at retrieving texts that suit a particular target reading level. In a school setting, it can help teachers to find texts appropriate to their students; other real-life contexts where it can play an important role are those involving people with intellectual disabilities, dyslexics, immigrant populations, and second or foreign language learners. Commendably, Vajjala and Meurers (2012) present dozens of features used in previous research on text readability and complexity and group them into three broad categories: lexical, syntactic and traditional features. Examples of features from the first group are the type-token ratio (see Section 4) and the lexical density, defined as the ratio of the number of lexical word tokens (nouns, adjectives, verbs, adverbs) and the number of all tokens (total number of words) in the analysed text. Syntactic features include mean length of clauses and sentences, and co-ordinate phrases and complex nominals per clause. The average sentence le"
2014.amta-researchers.13,W13-2201,0,\N,Missing
2014.amta-researchers.20,2013.iwslt-papers.17,0,0.0347895,"fects on ASR quality are dependent on position and context. For example, they show that while disfluencies account for most ASR errors, only fragments, non-final repetitions, and words preceding fragments have a significant impact. We use a similar experimental setup to measure the influence of different ASR error types, expressed as continuous fixed effects, on the increase in machine translation errors over the translations of perfectly recognized utterances. A number of works have been proposed to mitigate the contextual effects of ASR errors on MT quality by adapting the SMT phrase table. Ananthakrishnan et al. (2013) use attention-shift decoding for ASR (Kumaran et al., 2007) to identify reliable islands and unreliable gaps in an ASR hypothesis. The SMT decoder penalizes phrase translation pairs whose source phrases span across island-gap boundaries. Likewise, the language model penalizes target language n-grams that cover the island-gap boundary in the source phrase. Tsvetkov et al. (2014) augment phrase-based MT translation models with synthetic phrases by identifying word contexts in ASR outputs that contain acoustically confusable phoneme sequences. The source phrase for each bilingual phrase pair is"
2014.amta-researchers.20,W05-0909,0,0.107879,"er describe how ASR errors violate the structural assumptions used to train standard SMT systems. For example, let us assume that we have two utterances with the same reference length and WER. If the errors in one utterance are concentrated at the beginning or end of the utterance, would its TER be greater than an utterance whose errors are uniformly distributed across the entire segment? Finally, while our experiments have focused on MT references generated by human posteditions, we propose to perform this analysis on automatic references with metrics such as NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), and HMEANT (Lo and Wu, 2011). Ultimately, we believe that the analysis of ASR errors on SLT can result in deriving an error metric that better correlates with MT quality in the speech translation pipeline. Acknowledgments This work was partially supported by the MateCAT (grant agreement 287688) and EU-BRIDGE projects (grant agreement 287658), which are funded by the EC under the 7th Framework Programme. The authors also thank Matteo Negri and Marco Turchi for their useful suggestions. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 272"
2014.amta-researchers.20,2013.iwslt-evaluation.20,1,0.70738,"tion, oracle word compounding (COMP), or compounding with word form normalization (NORM). Translated ASR outputs are tokenized and evaluated against the reference translation (Auto) and a combination of the human post-edited sentences from the MT task (Post-edit). 3.2 MT data processing Since we are evaluating the impact of ASR errors on translation quality, we use a fixed SMT system trained on TED talk transcripts, which correspond to the reference transcripts in the ASR track, with the addition of punctuation. We use FBK’s primary phrase-based SMT system used in the English-French MT track (Bertoldi et al., 2013). The normalized ASR hypotheses are post-processed by inserting punctuation and applying recasing. We insert the punctuation as closely as possible to the position dictated in the reference in order to control the impact of punctuation on translation output. This is done by computing the Levenshtein alignments between the unpunctuated TED transcripts and each ASR hypothesis, using SCLITE1 . We train and apply a recaser model using the standard Moses tools (Koehn et al., 2007) with IWSLT 2013’s TED training data to all of the newly-punctuated ASR outputs. After introducing punctuation and recas"
2014.amta-researchers.20,2013.iwslt-evaluation.1,1,0.813139,"Missing"
2014.amta-researchers.20,P12-1031,0,0.0130643,"significant findings that demonstrate differences in the contributions of specific ASR error types toward speech translation quality and suggest further error types that may contribute to translation difficulty. 1 Introduction Spoken language translation (SLT) systems are composed with an automatic speech recognition (ASR) system that transcribes source language utterances and a machine translation (MT) system that translates the transcripts into a target language. While there is growing interest in constructing tightly-coupled ASR and MT systems that leverage joint training and optimization (He and Deng, 2012, 2013), the dominant approach is to construct a pipeline consisting of a MT system that decodes one or more ASR hypotheses (Ney, 1999; Matusov et al., 2006; Bertoldi et al., 2007; Casacuberta et al., 2008). The individual SLT components are trained and evaluated independently against local optimization metrics that fit each model to its local task, but they do not generalize to overall SLT quality. In particular, the de-facto automatic evaluation metric for speech recognition is Word Error Rate (WER), which classifies ASR errors into three categories corresponding to Levenshtein distance alig"
2014.amta-researchers.20,P07-2045,1,0.00840183,"Missing"
2014.amta-researchers.20,W11-1002,0,0.0347899,"Missing"
2014.amta-researchers.20,J93-2004,0,0.0534362,"Missing"
2014.amta-researchers.20,2014.eamt-1.39,1,0.815333,"Missing"
2014.amta-researchers.20,2006.amta-papers.25,0,0.0546235,"es on the tst2012 test set and take the subset of the ASR hypotheses that correspond to the reference set for the English-French MT track. A subset of the MT outputs of each system in the MT track was manually post-edited by professionals and served as multiple human references for automatic evaluation. Using these postedited translations, we construct 3-way data consisting of eight English ASR hypotheses for 580 utterances, a single unpunctuated reference transcript from the ASR track, and the human post-edited translations from the English-French MT track. We will use Translation Edit Rate (Snover et al., 2006) as a sentence-level MT quality metric, as it models the original post-editing scenario of the evaluation campaign by estimating the amount of effort required to correct machine translation output. In order to analyze the impact of ASR errors on MT quality, we construct experiments to address the following questions: • Does ASR’s WER correlate with SMT’s automatic quality metrics (e.g. TER)? • Do higher WER scores cause a degradation in MT quality with respect to translations on perfectly recognized utterances (∆TER)? • Which types of ASR errors have the strongest impact on translation quality"
2014.amta-researchers.20,E14-1065,0,0.101248,"translation errors over the translations of perfectly recognized utterances. A number of works have been proposed to mitigate the contextual effects of ASR errors on MT quality by adapting the SMT phrase table. Ananthakrishnan et al. (2013) use attention-shift decoding for ASR (Kumaran et al., 2007) to identify reliable islands and unreliable gaps in an ASR hypothesis. The SMT decoder penalizes phrase translation pairs whose source phrases span across island-gap boundaries. Likewise, the language model penalizes target language n-grams that cover the island-gap boundary in the source phrase. Tsvetkov et al. (2014) augment phrase-based MT translation models with synthetic phrases by identifying word contexts in ASR outputs that contain acoustically confusable phoneme sequences. The source phrase for each bilingual phrase pair is checked for alternative word sequences that have acoustically similar phoneme sequences. Source-side matches are added to the phrase table with the same target language phrase and new phrase table features are added to measure their fluency. 3 Research methodology Our goal is to analyze the impact of ASR errors on machine translation quality. Using WER as a metric for ASR qualit"
2014.amta-researchers.20,W13-2231,1,0.756943,"Missing"
2014.amta-researchers.20,vilar-etal-2006-error,0,0.37376,"Missing"
2014.eamt-1.39,W09-0434,0,0.061097,"Missing"
2014.eamt-1.39,Q13-1027,1,0.893867,"Missing"
2014.eamt-1.39,P07-2045,1,0.00908066,"Missing"
2014.eamt-1.39,J90-2002,0,0.902293,"Missing"
2014.eamt-1.39,D07-1007,0,0.22297,"anguage features that indicate the capacity of a statistical system to generate fluent translations. We attempt to focus on complexity issues that are irrespective of a particular text, speaker, or language pair and focus on issues that are relevant to the MT task. We can categorize these issues into three general areas: the lexicon, syntax, and semantics. When considering the lexicon, we can observe effects of vocabulary size, morphological variations, and both lexical and translation ambiguity as key impacts affecting the ability of the statistical models to cover the words in the language (Carpuat and Wu, 2007). On the syntax level, sentence length, structure complexity, and structural dependencies affect the decoding search space. On the semantic level, phenomena such as idiomatic expressions, figures of speech, anaphora, and elliptical expressions define intrinsic limitations of syntactic models. While we can observe nearly all of these language features on the monolingual level, many of these issues have a greater impact when transferring linguistic information in the process of translation. Between distant language pairs, the effects of these linguistic features cause a cumulative increase in th"
2014.eamt-1.39,W09-2404,0,0.0191082,"level, phenomena such as idiomatic expressions, figures of speech, anaphora, and elliptical expressions define intrinsic limitations of syntactic models. While we can observe nearly all of these language features on the monolingual level, many of these issues have a greater impact when transferring linguistic information in the process of translation. Between distant language pairs, the effects of these linguistic features cause a cumulative increase in the difficulty of MT. Although discourse-based machine translation takes into account intersentential factors affecting translation quality (Carpuat, 2009; Foster et al., 2010), the majority of SMT systems treat each sentence independently, ruling out additional context. 4 Research methodology In this paper, we compare two sources of spoken and written language: TED talk transcripts1 and News Commentary texts2 . Both types of texts cover a variety of topics whose content is produced by several authors. Although these types of texts correspond to different genres, they are popular representatives of spoken and written language investigated in MT, while belonging to similar domains. Both genres consist of speakers or authors with similar communic"
2014.eamt-1.39,2013.iwslt-evaluation.1,1,0.868626,"Missing"
2014.eamt-1.39,2010.amta-papers.24,0,0.0320259,"na such as idiomatic expressions, figures of speech, anaphora, and elliptical expressions define intrinsic limitations of syntactic models. While we can observe nearly all of these language features on the monolingual level, many of these issues have a greater impact when transferring linguistic information in the process of translation. Between distant language pairs, the effects of these linguistic features cause a cumulative increase in the difficulty of MT. Although discourse-based machine translation takes into account intersentential factors affecting translation quality (Carpuat, 2009; Foster et al., 2010), the majority of SMT systems treat each sentence independently, ruling out additional context. 4 Research methodology In this paper, we compare two sources of spoken and written language: TED talk transcripts1 and News Commentary texts2 . Both types of texts cover a variety of topics whose content is produced by several authors. Although these types of texts correspond to different genres, they are popular representatives of spoken and written language investigated in MT, while belonging to similar domains. Both genres consist of speakers or authors with similar communication goals: namely, t"
2014.eamt-1.39,2010.iwslt-papers.10,1,0.870148,"Missing"
2014.eamt-1.39,J93-2004,0,0.0454002,"cases. In our experiments, we sample approximately two million words from both the English TED and WMT News Commentary corpora, as well as the German translations of their sentences. Rather than randomly sampling sentences from the corpora, we sequentially read the sample to allow us preserve the underlying discourse. Sentences containing more than 80 words are excluded. We additionally subdivide the sampled corpora into blocks of 100,000 words to measure statistics on vocabulary growth rate. We use TreeTagger (Schmid, 1994) to lemmatize and assign part-of-speech tags using the Penn Treebank (Marcus et al., 1993) and STTS (Schiller et al., 1995) tagsets for English and German, respectively. Some simple corpora statistics are provided in Table 1. 1800 1600 1400 1200 1000 800 600 400 200 TED-EN WMT-EN TED-DE WMT-DE 0.1M 0.4M 0.7M 1.0M 1.3M 1.6M 1.9M Word Count Figure 2: Perplexity change as corpus size increases for English and German. 6.1 Polysemy As an upper-bound measure of word ambiguity, we measure the number of senses each English word in the corpus can express, as reported by WordNet. While not every sense may be observed in our corpora, this measure estimates how ambiguous a corpus is for a stat"
2014.eamt-1.39,P10-1021,0,0.0288187,"restatements of unfamiliar concepts, and syntactic structures appropriate for the reading level of the intended audience. Graesser et al. (1994) introduce a coherence assumption, which claims that readers routinely attempt to construct coherent meanings and connections among text constituents unless the quality of the text is too poor. This concept forms one the core hypotheses in the constructivist theory of discourse comprehension. As a result, many complexity analysis tools attempt to detect coherence and cohesion through syntax, semantic, and discourse connectives (Graesser et al., 2004; Mitchell et al., 2010; Newbold and Gillam, 2010). Biber (1988) and follow-up work by researchers investigate the variation in cohesion across text and speech corpora. Louwerse et al. (2004) perform a multi-dimensional analysis to identify a number of linguistic features that divide the corpora along several registers. Their results show variance between speech and writing corpora on a variety of factors, including type frequency, polysemy, pronoun density, abstract noun usage, type-token ratios for nouns, and stem overlap. These features divide the written and spoken genres into subdomains posing unique challenges"
2014.eamt-1.39,W10-0409,0,0.0125819,"liar concepts, and syntactic structures appropriate for the reading level of the intended audience. Graesser et al. (1994) introduce a coherence assumption, which claims that readers routinely attempt to construct coherent meanings and connections among text constituents unless the quality of the text is too poor. This concept forms one the core hypotheses in the constructivist theory of discourse comprehension. As a result, many complexity analysis tools attempt to detect coherence and cohesion through syntax, semantic, and discourse connectives (Graesser et al., 2004; Mitchell et al., 2010; Newbold and Gillam, 2010). Biber (1988) and follow-up work by researchers investigate the variation in cohesion across text and speech corpora. Louwerse et al. (2004) perform a multi-dimensional analysis to identify a number of linguistic features that divide the corpora along several registers. Their results show variance between speech and writing corpora on a variety of factors, including type frequency, polysemy, pronoun density, abstract noun usage, type-token ratios for nouns, and stem overlap. These features divide the written and spoken genres into subdomains posing unique challenges in comprehension (e.g. pre"
2014.eamt-1.39,2006.amta-papers.25,0,0.323794,"Missing"
2014.eamt-1.39,2011.mtsummit-papers.58,0,0.0383014,"erform a multi-dimensional analysis to identify a number of linguistic features that divide the corpora along several registers. Their results show variance between speech and writing corpora on a variety of factors, including type frequency, polysemy, pronoun density, abstract noun usage, type-token ratios for nouns, and stem overlap. These features divide the written and spoken genres into subdomains posing unique challenges in comprehension (e.g. prepared speeches versus conversational speech; news broadcasts versus legal documents). 3 Language Complexity in Statistical Machine Translation Specia et al. (2011) outline three categories for features used in the task of MT quality estimation: 174 confidence indicators derived from SMT models, complexity indicators that measure the difficulty of translating the source text, and fluency indicators that measure the grammaticality of a translation. Likewise, the difficulty of a translation task can be estimated by analyzing source complexity and target language features that indicate the capacity of a statistical system to generate fluent translations. We attempt to focus on complexity issues that are irrespective of a particular text, speaker, or languag"
2014.iwslt-evaluation.1,2010.iwslt-evaluation.1,1,0.792952,"pants were also asked to submit runs on the 2013 test set (progress test set), in order to measure the progress of systems with respect to the previous year. All runs were evaluated with objective metrics, and submissions for two of the official text translation tracks were also evaluated with human post-editing. 1. Introduction This paper overviews the results of the 2014 evaluation campaign organized by the International Workshop of Spoken Language Translation. The IWSLT evaluation has been running now for over a decade and has offered along these years a variety of speech translation tasks [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. The 2014 IWSLT evaluation continued along the line set in 2010, by focusing on the translation of TED Talks, a collection of public speeches covering many different topics. As in the previous two years, the evaluation included tracks for all the core technologies involved in the spoken language translation task, namely: • Automatic speech recognition (ASR), i.e. the conversion of a speech signal into a transcript, • Spoken language translation (SLT), that addressed the conversion and translation of a speech signal into a transcript in another language, • Machine translation (MT), i.e. the tr"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.15,0,0.0679155,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.14,0,0.0614042,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.7,1,0.877515,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.13,0,0.0364543,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.5,1,0.82235,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.12,0,0.0879094,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.21,0,0.047922,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.10,0,0.0535615,"Missing"
2014.iwslt-evaluation.1,1993.eamt-1.1,0,0.523227,"Missing"
2014.iwslt-evaluation.1,2005.mtsummit-papers.11,0,0.0265085,"time that Italian is involved in ASR/SLT tracks, therefore no evaluation set is available for assessing progress. A single TEDx based development set was released for each pair, together with standard TED based development sets dev2010, tst2010, tst2011 and tst2012 sets. Tables 2 and 3 provides statistics on in-domain texts supplied for training, development and evaluation purposes for the official directions. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [33] was applied to all languages, with the exception of Chinese and Arabic languages, which were sent 179k 887 1,664 818 1,124 1,026 1,305 172k 887 1,565 1,433 1,700 993 1,305 1,165 1,363 1,414 182k 887 1,529 1,433 1,704 1,402 1,183 1,056 883 tokens talks En Fr 3.63M 3.88M 1415 20,1k 20,2k 8 32,0k 33,9k 11 14,5k 15,6k 8 21,5k 23,5k 11 21,7k 23,3k 16 24,8k 27,5k 15 En De 3.46M 3.24M 1361 20,1k 19,1k 8 32,0k 30,3k 11 26,9k 26,3k 16 30,7k 29,2k 15 20,9k 19,7k 16 24,8k 23,8k 15 21,6k 20,8k 7 23,3k 22,4k 9 28,1k 27,6k 10 En It 3.68M 3.44M 1434 20,1k 17,9k 8 31,0k 28,7k 10 26,9k 24,5k 16 30,7k 28,2k 15"
2014.iwslt-evaluation.1,2012.amta-papers.22,1,0.779032,"amely the MT English-German (EnDe) track and MT English-French (EnF r) track. Following the methodology introduced last year, human evaluation was based on PostEditing, and HTER (Human-mediated Translation Edit Rate) was adopted as the official evaluation metric to rank the systems. Post-Editing, i.e. the manual correction of machine translation output, has long been investigated by the translation industry as a form of machine assistance to reduce the costs of human translation. Nowadays, Computer-aided translation (CAT) tools incorporate post-editing functionalities, and a number of studies [35, 36] demonstrate the usefulness of MT to increase professional translators’ productivity. The MT TED task offered in IWSLT can be seen as an interesting application scenario to test the utility of MT systems in a real subtitling task. 5.4. Results Table 4: BLEU and TER scores of baseline SMT systems on all tst2014 sets. († ) TEDx test set. (⋆ ) Char-level scores. pair En Fr De It Ar Es Fa He Nl Pl Pt Ro Ru Sl Tr Zh direction BLEU 32.07 18.33 27.15 11.13 31.31 11.31 15.91 22.77 9.63 31.25 18.05 11.74 8.46 7.75 ⋆ 16.49 → TER 48.62 62.11 53.19 73.01 48.29 71.20 65.62 58.38 82.81 47.25 65.25 71.99 73."
2014.iwslt-evaluation.1,2006.amta-papers.25,0,0.397152,"paign, our goal was to adopt a human evaluation framework able to maximize the benefit to the research community, both in terms of information about MT systems and data and resources to be reused. With respect to other types of human assessment, such as judgments of translation quality (i.e. adequacy/fluency and ranking tasks), the post-editing task has the double advantage of producing (i) a set of edits pointing to specific translation errors, and (ii) a set of additional reference translations. Both these byproducts are very useful for MT system development and evaluation. Furthermore, HTER[37] - which consists of measuring the minimum edit distance between the machine translation and its manually post-edited version - has been shown to correlate quite well with human judgments of MT quality. First of all, for reference purposes Table 4 shows BLEU and TER scores on the tst2014 evaluation sets of the baseline systems we developed as described in Section 5.1. The results on the official test set for each participant are shown in Appendix A.1. For most languages, we show the case-sensitive and case-insensitive BLEU and TER scores. The human evaluation setup and the collection of posted"
2014.iwslt-evaluation.1,J93-3001,0,0.560866,"Missing"
2014.iwslt-evaluation.5,P07-2045,1,0.0126412,"f introduction to the baseline MT system in Section 2 employed for all tasks, in Section 3 we overview the data selection techniques used to extract TED-related data from the available huge and generic monolingual and bilingual corpora. Then, in Section 4 we describe the methods applied to combine translation models, reordering models, and language models trained on multiple corpora. Sections 5-7 give details about the actual MT and SLT systems built for evaluation task. 2. Baseline SMT system All our task-specific systems rely on the well-known and state-of-the-art phrase-based Moses toolkit [1]; and exploit the huge amount of parallel and monolingual training data 1 http://www.ted.com/talks University of Trento ICT Doctoral School Trento, Italy 2 provided by the organizers. Our common baseline system features a statistical log-linear model including a phrasebased translation model (TM), a lexicalized phrase-based reordering models (RM), one or more language models (LMs), as well as distortion, word and phrase penalties. Tuning of the baseline system is performed on tst2010 by optimizing BLEU using Minimum Error Rate Training [2]. However, all available development data sets, namely"
2014.iwslt-evaluation.5,P03-1021,0,0.0306464,"-known and state-of-the-art phrase-based Moses toolkit [1]; and exploit the huge amount of parallel and monolingual training data 1 http://www.ted.com/talks University of Trento ICT Doctoral School Trento, Italy 2 provided by the organizers. Our common baseline system features a statistical log-linear model including a phrasebased translation model (TM), a lexicalized phrase-based reordering models (RM), one or more language models (LMs), as well as distortion, word and phrase penalties. Tuning of the baseline system is performed on tst2010 by optimizing BLEU using Minimum Error Rate Training [2]. However, all available development data sets, namely dev2010 and tst2010-2012, are included in the in-domain training data to build the systems actually employed for the 2014 evaluation campaign. The task-specific systems differ in the way training data are processed and filtered, and how the models are trained and combined. 3. Data Filtering The idea of data selection is to find the subset of sentences within an out-of-domain corpus that better fits with a given in-domain corpus. To this purpose, we follow the procedure described in [3], implementing the bilingual cross-entropy difference ["
2014.iwslt-evaluation.5,D11-1033,0,0.055327,"]. However, all available development data sets, namely dev2010 and tst2010-2012, are included in the in-domain training data to build the systems actually employed for the 2014 evaluation campaign. The task-specific systems differ in the way training data are processed and filtered, and how the models are trained and combined. 3. Data Filtering The idea of data selection is to find the subset of sentences within an out-of-domain corpus that better fits with a given in-domain corpus. To this purpose, we follow the procedure described in [3], implementing the bilingual cross-entropy difference [4], i.e. an adaptation of the cross-entropy difference scoring technique introduced by [5] toward bitext data selection, by means of XenC toolkit [6]. First, all sentence pairs of the out-of-domain corpus are associated with source- and target-side scores, each of which are computed as the basic technique proposes for the corresponding monolingual scenarios. We use the in-domain (TED) data as a seed and LMs of order 2.2 Then, the sentences are sorted according to the sum of these two scores. Finally, the optimal split between useful and useless sentences is found by minimizing the source-side pe"
2014.iwslt-evaluation.5,P10-2041,0,0.0976877,"cluded in the in-domain training data to build the systems actually employed for the 2014 evaluation campaign. The task-specific systems differ in the way training data are processed and filtered, and how the models are trained and combined. 3. Data Filtering The idea of data selection is to find the subset of sentences within an out-of-domain corpus that better fits with a given in-domain corpus. To this purpose, we follow the procedure described in [3], implementing the bilingual cross-entropy difference [4], i.e. an adaptation of the cross-entropy difference scoring technique introduced by [5] toward bitext data selection, by means of XenC toolkit [6]. First, all sentence pairs of the out-of-domain corpus are associated with source- and target-side scores, each of which are computed as the basic technique proposes for the corresponding monolingual scenarios. We use the in-domain (TED) data as a seed and LMs of order 2.2 Then, the sentences are sorted according to the sum of these two scores. Finally, the optimal split between useful and useless sentences is found by minimizing the source-side perplexity of a development set on growing percentages of the sorted corpus. In our experi"
2014.iwslt-evaluation.5,2011.iwslt-evaluation.18,1,0.892258,"velopment set. 4. Domain Adaptation In this section, we summarize several well-known techniques for domain adaptation we applied to build high-performance models for our SMT submissions. 2 This small LM order permits a very fast computation of the scores, without losing performance. 42 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 4.1. Translation model combination 4.3. Language model combination Three methods are applied in our submissions to combine the TM built on the available parallel training corpora: namely, fill-up [7, 8], back-off, and interpolation. Language models are built from the monolingual training data, as well as the target language of the parallel data. As the corpora available in the IWSLT evaluation come from a number of sources, we apply several methods to combine the LMs built on the available target language training corpora, rather than concatenating the data. 4.1.1. Fill-up In the fill-up approach, out-of-domain phrase pairs that do not appear in an in-domain (TED) phrase table are added, along with their scores – effectively filling the in-domain table with additional phrase translation opti"
2014.iwslt-evaluation.5,W07-0717,0,0.0911291,"The mixture LM type can be loaded by Moses as any other LM type. 4.3.2. Log-linear interpolation This technique, provided directly within the Moses toolkit, consists in the log-linear interpolation of the n-gram probabilities from all component LMs. The weight optimization is performed during the tuning of all Moses features. 4.4. Factored Trigger Models 4.1.3. Linear interpolation Linear interpolation of component models is a widely used approach for building a domain adapted multi-model. Approaches such as using monolingual data or pairwise ranking optimization to set interpolation weights [9, 10], perplexity minimization [11], and combining lemmatized and non-lemmatized models [12] have been used in the past for improved domain adaptation. In this paper, we leverage a recent work of [13] which exploits the use of source-side of the parallel in-domain corpus for domain adaptation. This approach calculates a similarity score (known as BLEU-PT) for each of the out-domain translation models on the source in-domain data. We use these similarity scores and further normalize them by the number of phrases seen in each of the corresponding out-domain phrase tables. These normalized scores are"
2014.iwslt-evaluation.5,N13-1035,0,0.0274451,"The mixture LM type can be loaded by Moses as any other LM type. 4.3.2. Log-linear interpolation This technique, provided directly within the Moses toolkit, consists in the log-linear interpolation of the n-gram probabilities from all component LMs. The weight optimization is performed during the tuning of all Moses features. 4.4. Factored Trigger Models 4.1.3. Linear interpolation Linear interpolation of component models is a widely used approach for building a domain adapted multi-model. Approaches such as using monolingual data or pairwise ranking optimization to set interpolation weights [9, 10], perplexity minimization [11], and combining lemmatized and non-lemmatized models [12] have been used in the past for improved domain adaptation. In this paper, we leverage a recent work of [13] which exploits the use of source-side of the parallel in-domain corpus for domain adaptation. This approach calculates a similarity score (known as BLEU-PT) for each of the out-domain translation models on the source in-domain data. We use these similarity scores and further normalize them by the number of phrases seen in each of the corresponding out-domain phrase tables. These normalized scores are"
2014.iwslt-evaluation.5,E12-1055,0,0.0263646,"d by Moses as any other LM type. 4.3.2. Log-linear interpolation This technique, provided directly within the Moses toolkit, consists in the log-linear interpolation of the n-gram probabilities from all component LMs. The weight optimization is performed during the tuning of all Moses features. 4.4. Factored Trigger Models 4.1.3. Linear interpolation Linear interpolation of component models is a widely used approach for building a domain adapted multi-model. Approaches such as using monolingual data or pairwise ranking optimization to set interpolation weights [9, 10], perplexity minimization [11], and combining lemmatized and non-lemmatized models [12] have been used in the past for improved domain adaptation. In this paper, we leverage a recent work of [13] which exploits the use of source-side of the parallel in-domain corpus for domain adaptation. This approach calculates a similarity score (known as BLEU-PT) for each of the out-domain translation models on the source in-domain data. We use these similarity scores and further normalize them by the number of phrases seen in each of the corresponding out-domain phrase tables. These normalized scores are then used as linear interpolat"
2014.iwslt-evaluation.5,P07-2046,0,0.0647212,"olation This technique, provided directly within the Moses toolkit, consists in the log-linear interpolation of the n-gram probabilities from all component LMs. The weight optimization is performed during the tuning of all Moses features. 4.4. Factored Trigger Models 4.1.3. Linear interpolation Linear interpolation of component models is a widely used approach for building a domain adapted multi-model. Approaches such as using monolingual data or pairwise ranking optimization to set interpolation weights [9, 10], perplexity minimization [11], and combining lemmatized and non-lemmatized models [12] have been used in the past for improved domain adaptation. In this paper, we leverage a recent work of [13] which exploits the use of source-side of the parallel in-domain corpus for domain adaptation. This approach calculates a similarity score (known as BLEU-PT) for each of the out-domain translation models on the source in-domain data. We use these similarity scores and further normalize them by the number of phrases seen in each of the corresponding out-domain phrase tables. These normalized scores are then used as linear interpolation coefficients. In this paper, we perform linear interp"
2014.iwslt-evaluation.5,C14-1105,0,0.0191618,"of the n-gram probabilities from all component LMs. The weight optimization is performed during the tuning of all Moses features. 4.4. Factored Trigger Models 4.1.3. Linear interpolation Linear interpolation of component models is a widely used approach for building a domain adapted multi-model. Approaches such as using monolingual data or pairwise ranking optimization to set interpolation weights [9, 10], perplexity minimization [11], and combining lemmatized and non-lemmatized models [12] have been used in the past for improved domain adaptation. In this paper, we leverage a recent work of [13] which exploits the use of source-side of the parallel in-domain corpus for domain adaptation. This approach calculates a similarity score (known as BLEU-PT) for each of the out-domain translation models on the source in-domain data. We use these similarity scores and further normalize them by the number of phrases seen in each of the corresponding out-domain phrase tables. These normalized scores are then used as linear interpolation coefficients. In this paper, we perform linear interpolation of out-ofdomain models which results in one translation model. The in-domain translation model is th"
2014.iwslt-evaluation.5,2012.eamt-1.60,1,0.798668,"about the EU-Bridge system are available in a companion paper [23]. 6. German-English MT task Our German-English systems are built on top of the baseline system (see Section 2. Each system contains one translation model, reordering model, language model, the factored trigger model and operation sequence model; these models are then combined in a standard log-linear fashion. The training data is composed of several publicly available corpora provided in the IWSLT MT and the WMT 2014 translation tasks. As parallel data the following corpora were taken into account: WIT3 (version 2014-01) (TED) [18], German-English Europarl (version 7) (EP), Common Crawl (CC), MultiUN (UN), and the News Commentary (NC) corpus as distributed by the organizers of the WMT 2014. We used all the available monolingual corpora provided by the WMT 2014 translation task. The target side of the parallel corpora is also used to train our LMs. Corpus TED CC EP UN NC unselected De En Segm Words Words 171K 3.3M 3.46M 2.4M 56M 58M 1.9M 52M 53M 162K 5.8M 5.66M 200K 5.25M 5.0M Segm 171K 462K 188K 45K 59K selected De En Words Words 3.3M 3.46M 10.5M 10.7M 3.58M 3.64M 1.59M 1.52M 1.4M 1.3M Table 2: Statistics of the paralle"
2014.iwslt-evaluation.5,W08-0509,0,0.0224987,"Missing"
2014.iwslt-evaluation.5,N04-1022,0,0.129273,"Missing"
2014.iwslt-evaluation.5,2014.iwslt-evaluation.7,1,0.801513,"cn MT De-En tst2013 BLEU TER 38.20 44.83 38.13 44.83 37.88 45.05 36.27 47.48 38.16 44.90 38.04 44.93 37.95 45.08 36.73 46.44 37.89 44.98 25.45 55.59 25.76 55.80 tst2014 BLEU TER 34.24 46.75 34.18 46.61 33.79 47.02 32.07 50.02 33.98 47.03 34.02 46.87 33.67 47.24 32.49 48.81 34.03 46.86 20.52 63.54 20.37 63.37 Table 1: Case-sensitive BLEU and TER results for FBK’s submissions to the English-French and German-English MT tasks. The contrastive run 5, was also applied into the joint submission by the EU-Bridge project5 partners; details about the EU-Bridge system are available in a companion paper [23]. 6. German-English MT task Our German-English systems are built on top of the baseline system (see Section 2. Each system contains one translation model, reordering model, language model, the factored trigger model and operation sequence model; these models are then combined in a standard log-linear fashion. The training data is composed of several publicly available corpora provided in the IWSLT MT and the WMT 2014 translation tasks. As parallel data the following corpora were taken into account: WIT3 (version 2014-01) (TED) [18], German-English Europarl (version 7) (EP), Common Crawl (CC),"
2014.iwslt-evaluation.5,P13-2071,0,0.0418665,"Missing"
2014.iwslt-evaluation.5,2011.iwslt-papers.7,0,0.0471624,"nally, we describe our experimental results. 7.1. Preprocessing Prior to translating ASR outputs, we perform the following normalization steps to make them compatible with our phrase-based SMT system. Similar to the MT track, we tokenize ASR outputs using the scripts provided by Moses. After tokenization, we recase the outputs. The recaser system is trained using the Moses scripts and a 3-gram LM. The recaser model and language models are trained on a concatenation of TED and WMT News Commentary data. Finally, we insert punctuation via monotonic machine translation, similar to the approach of [29]. 7.2. Phoneme-motivated Text Normalization A SMT system trained only on transcripts and other text data results yields a search space that is inaccessible by ASR outputs that may contain errors and text normalization issues. In an ideal scenario, we would train our spoken language translation system on a combination of text corpora and speech recognition outputs with reference translations; however, a sufficiently large amount of such speech corpora is not readily available. In order to make our machine translation system more tolerant of potential ASR errors, we use a similar phoneme-motivat"
2014.iwslt-evaluation.5,2013.iwslt-evaluation.4,0,0.0340555,"scripts and other text data results yields a search space that is inaccessible by ASR outputs that may contain errors and text normalization issues. In an ideal scenario, we would train our spoken language translation system on a combination of text corpora and speech recognition outputs with reference translations; however, a sufficiently large amount of such speech corpora is not readily available. In order to make our machine translation system more tolerant of potential ASR errors, we use a similar phoneme-motivated text normalization approach as outlined in our previous year’s submission [30] to generate additional bilingual training data from the text corpora provided in the evaluation. We adapt the MT training data into ASR-like output to anticipate ASR errors and text normalization issues during SMT model training. We do this by leveraging several components from a target ASR system. In our experiments, we use the FBK’s Kaldi English ASR system, which was used in our ASR submission [31]. Similar to [32], we transform the text corpora into synthetic ASR outputs by first converting the text corpora into phonemes and then “translating” each phoneme sequence back into words that mo"
2014.iwslt-evaluation.5,E03-1076,0,0.134504,"Missing"
2014.iwslt-evaluation.5,D08-1089,0,0.0878675,"Missing"
2014.iwslt-evaluation.7,D14-1003,1,0.921764,"slation of spoken language. The IWSLT TED talks task constitutes an interesting framework for empirical testing of some of the systems for spoken language translation which are developed as part of the project. In this work, we describe the EU-BRIDGE submissions to the 2014 IWSLT translation task. This year, we combined several single systems of RWTH, UEDIN, KIT, and FBK for the German→English SLT, German→English MT, English→German MT, and English→French MT tasks. Additionally to the standard system combination pipeline presented in [1, 2], we applied a recurrent neural network rescoring step [3] for the English→French MT task. Similar cooperative approaches based on system combination have proven to be valuable for machine translation in previous joint submissions, e.g. [4, 5]. 2. RWTH Aachen University RWTH applied the identical training pipeline and models on both language pairs: The state-of-the-art phrase-based baseline systems were augmented with a hierarchical reordering model, several additional language models (LMs) and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translat"
2014.iwslt-evaluation.7,W10-1738,1,0.885248,"and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-"
2014.iwslt-evaluation.7,P03-1021,0,0.488353,"employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing."
2014.iwslt-evaluation.7,popovic-ney-2006-pos,1,0.798687,"th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selectio"
2014.iwslt-evaluation.7,P13-2121,1,0.819366,"mplemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWT"
2014.iwslt-evaluation.7,P10-2041,0,0.0916594,"rdering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU ob"
2014.iwslt-evaluation.7,E99-1010,0,0.0737032,"them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for"
2014.iwslt-evaluation.7,D13-1138,1,0.85854,"RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumv"
2014.iwslt-evaluation.7,P12-1031,0,0.0125863,"lection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and"
2014.iwslt-evaluation.7,P10-1049,1,0.833909,"the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LST"
2014.iwslt-evaluation.7,D14-1132,0,0.157332,"M. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficienc"
2014.iwslt-evaluation.7,2011.iwslt-papers.7,1,0.944851,"ort-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficiency as described in [20]. All neural networks were trained on the TED portion of the data with 2000 word classes. In addition to the recurrent language model (RNN-LM), RWTH applied the deep bidirectional word-based translation model (RNN-BTM) described in [3], which is capable of taking the full source context into account for each translation decision. Spoken Language Translation For the SLT task, RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the f"
2014.iwslt-evaluation.7,2014.iwslt-papers.17,1,0.734908,"RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided"
2014.iwslt-evaluation.7,P07-2045,1,0.0190208,"n hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26]"
2014.iwslt-evaluation.7,N04-1035,0,0.0565459,"equences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [2"
2014.iwslt-evaluation.7,W08-0509,0,0.192359,"[24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six"
2014.iwslt-evaluation.7,N13-1073,0,0.0453396,"e syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sp"
2014.iwslt-evaluation.7,C14-1041,1,0.839592,"ndividual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable"
2014.iwslt-evaluation.7,N12-1047,0,0.0681194,"them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is"
2014.iwslt-evaluation.7,P02-1040,0,0.0918061,"d to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by trai"
2014.iwslt-evaluation.7,2006.iwslt-papers.1,1,0.862433,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,2012.iwslt-papers.15,1,0.927241,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,P05-1066,1,0.733044,"ferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the"
2014.iwslt-evaluation.7,E03-1076,1,0.858704,"xt before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE sy"
2014.iwslt-evaluation.7,2012.amta-papers.9,1,0.84942,"arallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE system combination. Both comprise Brown clusters with 200 classes as additional factors on source and target"
2014.iwslt-evaluation.7,D08-1089,0,0.176922,"ign [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation a"
2014.iwslt-evaluation.7,W14-3324,1,0.784121,"ical tag. UEDIN-A was trained with all corpora, whereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house ph"
2014.iwslt-evaluation.7,2012.iwslt-papers.17,1,0.881764,"ain 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic diff"
2014.iwslt-evaluation.7,C04-1024,0,0.0400394,"ereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models wer"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.18,1,0.873679,"ined on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standa"
2014.iwslt-evaluation.7,W14-3362,1,0.610881,"N-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for"
2014.iwslt-evaluation.7,W14-4018,1,0.774295,"ptimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In t"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.9,1,0.861968,"m with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were"
2014.iwslt-evaluation.7,2007.tmi-papers.21,0,0.0614729,"ta. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabiliti"
2014.iwslt-evaluation.7,W09-0413,1,0.842557,"pora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the tra"
2014.iwslt-evaluation.7,W13-0805,1,0.85195,"ifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual langu"
2014.iwslt-evaluation.7,W08-1006,0,0.0150981,"k, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the ta"
2014.iwslt-evaluation.7,2012.amta-papers.19,1,0.839901,"e rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WI"
2014.iwslt-evaluation.7,W11-2124,1,0.902739,"for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cl"
2014.iwslt-evaluation.7,W13-2264,1,0.835602,"ed by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cluster-based 4-gram LM was trained on 500 clusters. For English→German"
2014.iwslt-evaluation.7,2012.eamt-1.60,1,0.892622,"Missing"
2014.iwslt-evaluation.7,D11-1033,0,0.167316,"Missing"
2014.iwslt-evaluation.7,W05-0909,0,0.085167,"m multiple hypotheses which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we applied a system combination implementation that has been developed at RWTH Aachen University [1]. In Fig. 1 an overview is illustrated. We first address the generation of a confusion network (CN) from I input translations. For that we need a pairwise alignment between all input hypotheses. This alignment is calculated via METEOR [60]. The hypotheses are then reordered to match the word order of a selected skeleton hypothesis. Instead of using only one of the input hypothesis as skeleton, we generate I different CNs, each having one of the input systems as skeleton. The final lattice is the union of all I previous generated CNs. In Fig. 2 an example confusion network of I = 4 input translations with one skeleton translation is illustrated. Between two adjacent nodes, we always have a choice between the I different system output words. The confusion network decoding step involves determining the shortest path through the ne"
2014.iwslt-evaluation.7,2006.amta-papers.25,0,0.0356913,"andard set of models is a word penalty, a 3-gram language model trained on the input hypotheses, and for each system one binary voting feature. During decoding the binary voting feature for system i (1 ≤ i ≤ I) is 1 iff the word is from system i, otherwise 0. The M different model weights λm are trained with MERT [8]. the red cab the a a red blue green train car car Figure 2: System A: the red cab ; System B: the red train ; System C: a blue car ; System D: a green car ; Reference: the blue car . 7. Results In this section, we present our experimental results. All reported B LEU [34] and T ER [61] scores are case-sensitive with one reference. All system combination results have been generated with RWTH’s open source system combination implementation Jane [1]. German→English SLT For the German→English SLT task, we combined three different individual systems generated by UEDIN, KIT, and RWTH. Experimental results are given in Table 1. The final system combination yields improvements of 1.5 points in B LEU and 1.2 points in T ER compared to the best single system (KIT). All single systems as well as the system combination parameters were tuned on dev2012. For this year’s IWSLT SLT track,"
2014.iwslt-evaluation.7,E06-1005,1,\N,Missing
2014.iwslt-evaluation.7,P11-1105,1,\N,Missing
2014.iwslt-evaluation.7,W10-1711,1,\N,Missing
2014.iwslt-evaluation.7,2010.iwslt-evaluation.22,1,\N,Missing
2014.iwslt-evaluation.7,E14-2008,1,\N,Missing
2014.iwslt-evaluation.7,2014.iwslt-evaluation.6,1,\N,Missing
2014.iwslt-evaluation.7,J03-1002,1,\N,Missing
2014.iwslt-evaluation.7,C12-3061,1,\N,Missing
2014.iwslt-evaluation.7,2013.iwslt-evaluation.16,1,\N,Missing
2014.iwslt-evaluation.7,W14-3310,1,\N,Missing
2015.iwslt-evaluation.1,2005.iwslt-1.19,0,\N,Missing
2015.iwslt-evaluation.1,2007.iwslt-1.1,0,\N,Missing
2015.iwslt-evaluation.1,2005.iwslt-1.1,0,\N,Missing
2015.iwslt-evaluation.1,2004.iwslt-evaluation.1,1,\N,Missing
2015.iwslt-evaluation.1,J93-3001,0,\N,Missing
2015.iwslt-evaluation.1,W05-0908,0,\N,Missing
2015.iwslt-evaluation.1,2005.mtsummit-papers.11,0,\N,Missing
2015.iwslt-evaluation.1,2015.iwslt-evaluation.16,0,\N,Missing
2015.iwslt-evaluation.1,2006.iwslt-evaluation.1,0,\N,Missing
2015.iwslt-evaluation.1,2008.iwslt-evaluation.1,0,\N,Missing
2015.iwslt-evaluation.1,2011.iwslt-evaluation.1,1,\N,Missing
2015.iwslt-evaluation.1,2009.iwslt-evaluation.1,0,\N,Missing
2015.iwslt-evaluation.1,2012.eamt-1.60,1,\N,Missing
2015.iwslt-evaluation.1,2010.iwslt-evaluation.1,1,\N,Missing
2015.mtsummit-papers.21,2011.iwslt-evaluation.18,1,0.809528,"ation models along with operation sequence models (Durrani et al., 2011) were trained using the standard pipeline of Moses. Due to the nature of the item titles, we did not use any lexicalized reordering model in the MT system. The distortion limit was set to 6. On the target side, we built a trigram LM, using KenLM (Heafield, 2011) trained with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Domain Adapted System The domain adapted system was built on top of the baseline system. An additional translation model was built using the in-domain data and then the fill-up adaptation method (Bisazza et al., 2011) was applied to combine the in-domain and out-domain phrase tables. Fill-up simply adds a provenance feature in the phrase table with a score of 1 if the phrase pair is present in in-domain phrase table and 0 if it is from out-domain phrase table. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 277 Topic Adapted Systems To evaluate the performance of the topic adapted systems using the features functions presented in Section 4.2 we followed a component analysis approach. Each basic sparse feature was added to the domain adapted system separately, short"
2015.mtsummit-papers.21,P96-1041,0,0.180353,"stical significance tests were conducted using approximate randomization tests (Clark et al., 2011). Baseline System In-domain and out-domain parallel data were taken in 10:1 ratio for training the word alignments. Translation models along with operation sequence models (Durrani et al., 2011) were trained using the standard pipeline of Moses. Due to the nature of the item titles, we did not use any lexicalized reordering model in the MT system. The distortion limit was set to 6. On the target side, we built a trigram LM, using KenLM (Heafield, 2011) trained with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Domain Adapted System The domain adapted system was built on top of the baseline system. An additional translation model was built using the in-domain data and then the fill-up adaptation method (Bisazza et al., 2011) was applied to combine the in-domain and out-domain phrase tables. Fill-up simply adds a provenance feature in the phrase table with a score of 1 if the phrase pair is present in in-domain phrase table and 0 if it is from out-domain phrase table. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 277 Topic Adapted Systems To evaluate the p"
2015.mtsummit-papers.21,N12-1047,0,0.0210062,". Statistics on the amount of parallel data for each category are given in Table 2. Segments Tokens EN Tokens PT Train (Out-Domain) 5.28M 69M 70M Train (In-Domain) 336K 2M 2M Dev 1631 27K 31K Test 1000 10K 11.6K Table 2: Statistics of English-Portuguese parallel data. 5.2 MT Systems This section describes the topic adapted MT systems and the two baseline MT systems developed for comparison purposes. All MT systems are built using the Moses toolkit (Koehn et al., 2007) and the linear weights for all systems are optimized using the k-best batch MIRA implementation provided in the Moses toolkit (Cherry and Foster, 2012). Performance of all systems are reported in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Statistical significance tests were conducted using approximate randomization tests (Clark et al., 2011). Baseline System In-domain and out-domain parallel data were taken in 10:1 ratio for training the word alignments. Translation models along with operation sequence models (Durrani et al., 2011) were trained using the standard pipeline of Moses. Due to the nature of the item titles, we did not use any lexicalized reordering model in the MT system. The distortion limit was"
2015.mtsummit-papers.21,P11-2031,0,0.0127632,"s of English-Portuguese parallel data. 5.2 MT Systems This section describes the topic adapted MT systems and the two baseline MT systems developed for comparison purposes. All MT systems are built using the Moses toolkit (Koehn et al., 2007) and the linear weights for all systems are optimized using the k-best batch MIRA implementation provided in the Moses toolkit (Cherry and Foster, 2012). Performance of all systems are reported in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Statistical significance tests were conducted using approximate randomization tests (Clark et al., 2011). Baseline System In-domain and out-domain parallel data were taken in 10:1 ratio for training the word alignments. Translation models along with operation sequence models (Durrani et al., 2011) were trained using the standard pipeline of Moses. Due to the nature of the item titles, we did not use any lexicalized reordering model in the MT system. The distortion limit was set to 6. On the target side, we built a trigram LM, using KenLM (Heafield, 2011) trained with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Domain Adapted System The domain adapted system was built on top of the ba"
2015.mtsummit-papers.21,P11-1105,0,0.0192749,"built using the Moses toolkit (Koehn et al., 2007) and the linear weights for all systems are optimized using the k-best batch MIRA implementation provided in the Moses toolkit (Cherry and Foster, 2012). Performance of all systems are reported in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Statistical significance tests were conducted using approximate randomization tests (Clark et al., 2011). Baseline System In-domain and out-domain parallel data were taken in 10:1 ratio for training the word alignments. Translation models along with operation sequence models (Durrani et al., 2011) were trained using the standard pipeline of Moses. Due to the nature of the item titles, we did not use any lexicalized reordering model in the MT system. The distortion limit was set to 6. On the target side, we built a trigram LM, using KenLM (Heafield, 2011) trained with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Domain Adapted System The domain adapted system was built on top of the baseline system. An additional translation model was built using the in-domain data and then the fill-up adaptation method (Bisazza et al., 2011) was applied to combine the in-domain and out-domai"
2015.mtsummit-papers.21,P12-2023,0,0.356498,"rived from the training and testing data. Previous works have approached this issue under different perspectives, also providing different levels of integration of topic information. For instance, in (Gong et al., 2011) and (Ruiz and Federico, 2011) authors devised and exploited cross-lingual topic models to generate topic relevant target words for an entire document or sentence. This differs from other approaches and our one, which instead exploit monolingual topic models in the source language to directly enhance the selection process of single phrase translations pairs during decoding. In (Eidelman et al., 2012), topic dependent lexical probabilities are directly integrated in a hierarchical phrase-based system. The authors start with topic distributions inferred at the document level on the source side of the parallel data. After extraction of the translation rules, topic-conditional translation probabilities are inferred by computing the expectation over the topic vectors observed in all the sentences where each translation rule was extracted from. At decoding time, these probabilities are weighted by the topic prior inferred on the test document. This results in a set of sparse features, one per t"
2015.mtsummit-papers.21,D11-1084,0,0.0248318,"Miami, Oct 30 - Nov 3, 2015 |p. 272 sentences in the source side of all the available parallel data. As a result, each sentence of the parallel training data is associated with a topic vector or distribution. 3 Related Work Topic models have been investigated in the statistical MT literature to enhance both linear or hierarchical phrase-based models with additional context-topical information derived from the training and testing data. Previous works have approached this issue under different perspectives, also providing different levels of integration of topic information. For instance, in (Gong et al., 2011) and (Ruiz and Federico, 2011) authors devised and exploited cross-lingual topic models to generate topic relevant target words for an entire document or sentence. This differs from other approaches and our one, which instead exploit monolingual topic models in the source language to directly enhance the selection process of single phrase translations pairs during decoding. In (Eidelman et al., 2012), topic dependent lexical probabilities are directly integrated in a hierarchical phrase-based system. The authors start with topic distributions inferred at the document level on the source side o"
2015.mtsummit-papers.21,2014.amta-users.3,0,0.231032,"input can be leveraged to bias the system towards training data matching the same labels of the input. Potential advantages of topic adaptation over domain adaptation is that fuzzy clustering can better cope with data sparseness than hard clustering, and that automatic labels do not require any manual intervention. On the other side, however, domain adaptation becomes difficult to beat when training data can be effectively and naturally partitioned into in-domain and out-of-domain data. In this paper we discuss the application of domain and topic adaptation to an e-commerce online MT system (Guha and Heger, 2014), whose target is the translation of user queries and all item titles, descriptions, and specifics shown in the search result pages. In particular, our investigation focuses on the translation of item titles, which consists of concise user-generated texts describing each item put on sale. Item titles differ in several ways from text genres typically considered in machine translation research. Titles are usually short texts, of maximum ∗Most of the work was carried out during a visiting period of the first two authors at eBay Inc. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami,"
2015.mtsummit-papers.21,E14-1035,0,0.508447,"ic vector of the conversation is incrementally updated at each turn. During decoding, with a Moses-like phrase-based system, each candidate phrase-pair activates a feature function measuring the highest similarity between the current topic vector and all topic vectors associated to the occurrences of the phrase-pair in the training corpus. Similarity is computed by taking the complement of the Jensen-Shannon divergence. In our work, we also deploy this divergence measure to measure the match between the topic vector of the input and the topic vector of each candidate phrase-pair. Finally, in (Hasler et al., 2014a) the authors combine and compare domain adaptation and Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 273 topic adaptation in phrase-based statistical MT for the translation of texts from three different domains. Concerning topic adaptation, the standard Moses phrase-based feature functions associated to the phrase-table are augmented three sets of dense feature functions: (i) two translation probabilities, (ii) one language model score and (iii) three topic distribution similarity feature functions. The first set of features introduce source-to-tar"
2015.mtsummit-papers.21,W11-2123,0,0.010529,"ni et al., 2002) and TER (Snover et al., 2006) scores. Statistical significance tests were conducted using approximate randomization tests (Clark et al., 2011). Baseline System In-domain and out-domain parallel data were taken in 10:1 ratio for training the word alignments. Translation models along with operation sequence models (Durrani et al., 2011) were trained using the standard pipeline of Moses. Due to the nature of the item titles, we did not use any lexicalized reordering model in the MT system. The distortion limit was set to 6. On the target side, we built a trigram LM, using KenLM (Heafield, 2011) trained with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Domain Adapted System The domain adapted system was built on top of the baseline system. An additional translation model was built using the in-domain data and then the fill-up adaptation method (Bisazza et al., 2011) was applied to combine the in-domain and out-domain phrase tables. Fill-up simply adds a provenance feature in the phrase table with a score of 1 if the phrase pair is present in in-domain phrase table and 0 if it is from out-domain phrase table. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami,"
2015.mtsummit-papers.21,P13-2122,0,0.0187486,"re associated to hiero rules. Differently from (Eidelman et al., 2012), during decoding topic posterior distribution on phrase-pairs are matched against the topic distribution of the test sentence. Matching is performed with the Hellinger divergence. In addition, a feature measuring topic sensitivity of each phrase-pair is included based on the entropy function. In our work, we integrate and compare several dense features that compute the match between the topic distributions of the input and of each phrase-pair candidate. Among them we also include the two features proposed by this work. In (Hewavitharana et al., 2013) topic adaptation is performed in context of machine translation of task-driven conversations. Topics vectors are inferred via LDA on the source side of in-domain parallel training data. At test time, the topic vector of the conversation is incrementally updated at each turn. During decoding, with a Moses-like phrase-based system, each candidate phrase-pair activates a feature function measuring the highest similarity between the current topic vector and all topic vectors associated to the occurrences of the phrase-pair in the training corpus. Similarity is computed by taking the complement of"
2015.mtsummit-papers.21,P07-2045,1,0.00910484,"out of domain data. For development and testing purposes we use manually translated item titles for which two reference translations are available. Statistics on the amount of parallel data for each category are given in Table 2. Segments Tokens EN Tokens PT Train (Out-Domain) 5.28M 69M 70M Train (In-Domain) 336K 2M 2M Dev 1631 27K 31K Test 1000 10K 11.6K Table 2: Statistics of English-Portuguese parallel data. 5.2 MT Systems This section describes the topic adapted MT systems and the two baseline MT systems developed for comparison purposes. All MT systems are built using the Moses toolkit (Koehn et al., 2007) and the linear weights for all systems are optimized using the k-best batch MIRA implementation provided in the Moses toolkit (Cherry and Foster, 2012). Performance of all systems are reported in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Statistical significance tests were conducted using approximate randomization tests (Clark et al., 2011). Baseline System In-domain and out-domain parallel data were taken in 10:1 ratio for training the word alignments. Translation models along with operation sequence models (Durrani et al., 2011) were trained using the stand"
2015.mtsummit-papers.21,P02-1040,0,0.0918923,"Segments Tokens EN Tokens PT Train (Out-Domain) 5.28M 69M 70M Train (In-Domain) 336K 2M 2M Dev 1631 27K 31K Test 1000 10K 11.6K Table 2: Statistics of English-Portuguese parallel data. 5.2 MT Systems This section describes the topic adapted MT systems and the two baseline MT systems developed for comparison purposes. All MT systems are built using the Moses toolkit (Koehn et al., 2007) and the linear weights for all systems are optimized using the k-best batch MIRA implementation provided in the Moses toolkit (Cherry and Foster, 2012). Performance of all systems are reported in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Statistical significance tests were conducted using approximate randomization tests (Clark et al., 2011). Baseline System In-domain and out-domain parallel data were taken in 10:1 ratio for training the word alignments. Translation models along with operation sequence models (Durrani et al., 2011) were trained using the standard pipeline of Moses. Due to the nature of the item titles, we did not use any lexicalized reordering model in the MT system. The distortion limit was set to 6. On the target side, we built a trigram LM, using KenLM (Heafield, 2011)"
2015.mtsummit-papers.21,2014.amta-users.6,0,0.0415668,"ur types of texts need to be translated: queries, item titles, descriptions, and item specifics. This work focuses on the translation of item titles, which are concise and usually very informative descriptions of the items put on sale. For instance, the item title: new men’s white jekyll & hyde jeans winston designer regular fit shirt size s-xxl specifies, in order, the condition, target gender, color, brand, designer, fit, item type, and size of the product. Common challenges in the translation of eBay’s user generated content in general, and of titles (Sanchez and Badeka, 2014) and queries (Picinini, 2014) in particular, are the proper rendering of proper names and the translation of words which can have multiple senses, depending on the context in which they appear. For example, the word “age” might have different meanings if context is “baby”, “wine” or “collectibles”. Similarly, the word “j-hook” might have different meanings if context is “motors” or “garden”. The core idea of this work, hence is to apply topic modeling to efficiently represent the context of single words or expressions in order to improve the accuracy of their translation by a phrase-based statistical MT system. In the fol"
2015.mtsummit-papers.21,W11-2133,1,0.837642,"2015 |p. 272 sentences in the source side of all the available parallel data. As a result, each sentence of the parallel training data is associated with a topic vector or distribution. 3 Related Work Topic models have been investigated in the statistical MT literature to enhance both linear or hierarchical phrase-based models with additional context-topical information derived from the training and testing data. Previous works have approached this issue under different perspectives, also providing different levels of integration of topic information. For instance, in (Gong et al., 2011) and (Ruiz and Federico, 2011) authors devised and exploited cross-lingual topic models to generate topic relevant target words for an entire document or sentence. This differs from other approaches and our one, which instead exploit monolingual topic models in the source language to directly enhance the selection process of single phrase translations pairs during decoding. In (Eidelman et al., 2012), topic dependent lexical probabilities are directly integrated in a hierarchical phrase-based system. The authors start with topic distributions inferred at the document level on the source side of the parallel data. After ext"
2015.mtsummit-papers.21,2014.amta-users.1,0,0.142388,"tem titles, which consists of concise user-generated texts describing each item put on sale. Item titles differ in several ways from text genres typically considered in machine translation research. Titles are usually short texts, of maximum ∗Most of the work was carried out during a visiting period of the first two authors at eBay Inc. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 270 100 characters, with a simple syntactic structure, and containing brand names, feature values, as well as specific jargon. Their translation poses several challenges (Sanchez and Badeka, 2014), such as the correct rendering of proper names, which can often be confused with common names, and the correct translation of product features, which often depends on the context. From a statistical learning perspective, MT of item titles is also hard because of the large variety of content present in eBay’s inventory, which are very unevenly populated but also significantly overlapping in terms of linguistic content. The idea that we follow in this work is to employ topic modeling to better translate English item titles to a foreign language. Since we have a relatively small amount of biling"
2015.mtsummit-papers.21,2006.amta-papers.25,0,0.0501239,"in (Out-Domain) 5.28M 69M 70M Train (In-Domain) 336K 2M 2M Dev 1631 27K 31K Test 1000 10K 11.6K Table 2: Statistics of English-Portuguese parallel data. 5.2 MT Systems This section describes the topic adapted MT systems and the two baseline MT systems developed for comparison purposes. All MT systems are built using the Moses toolkit (Koehn et al., 2007) and the linear weights for all systems are optimized using the k-best batch MIRA implementation provided in the Moses toolkit (Cherry and Foster, 2012). Performance of all systems are reported in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Statistical significance tests were conducted using approximate randomization tests (Clark et al., 2011). Baseline System In-domain and out-domain parallel data were taken in 10:1 ratio for training the word alignments. Translation models along with operation sequence models (Durrani et al., 2011) were trained using the standard pipeline of Moses. Due to the nature of the item titles, we did not use any lexicalized reordering model in the MT system. The distortion limit was set to 6. On the target side, we built a trigram LM, using KenLM (Heafield, 2011) trained with modified Kneser-N"
2015.mtsummit-papers.21,P12-1048,0,0.0166831,"erred at the document level on the source side of the parallel data. After extraction of the translation rules, topic-conditional translation probabilities are inferred by computing the expectation over the topic vectors observed in all the sentences where each translation rule was extracted from. At decoding time, these probabilities are weighted by the topic prior inferred on the test document. This results in a set of sparse features, one per topic, which are tuned on a development set. In this work, we implement and evaluate a similar set of sparse features for a phrase-based decoder. In (Su et al., 2012) topic models are used to off-line adapt a phrase table trained on outof-domain parallel data by using in-domain monolingual data. Topic distributions are inferred through hidden topic Markov models trained on monolingual sentences. Two distinct topic models are trained, one in-domain and one out-of-domain, which are mapped via a mixture model. Finally, similar to (Eidelman et al., 2012) topic-conditioned phrase translation probabilities trained on out of domain data are weighted with the in-domain topic prior probabilities. Contrary in our case, topic information is integrated just in the tra"
2015.mtsummit-papers.21,P12-1079,0,0.09643,"erred through hidden topic Markov models trained on monolingual sentences. Two distinct topic models are trained, one in-domain and one out-of-domain, which are mapped via a mixture model. Finally, similar to (Eidelman et al., 2012) topic-conditioned phrase translation probabilities trained on out of domain data are weighted with the in-domain topic prior probabilities. Contrary in our case, topic information is integrated just in the training phase to bias the translation model toward the in-domain data. Our purpose, instead, is to dynamically adapt the translation model at testing time. In (Xiao et al., 2012) monolingual topic models are trained on the source side with LDA and topic vectors are associated to hiero rules. Differently from (Eidelman et al., 2012), during decoding topic posterior distribution on phrase-pairs are matched against the topic distribution of the test sentence. Matching is performed with the Hellinger divergence. In addition, a feature measuring topic sensitivity of each phrase-pair is included based on the entropy function. In our work, we integrate and compare several dense features that compute the match between the topic distributions of the input and of each phrase-pa"
2015.mtsummit-users.5,W14-3340,1,0.89891,"Missing"
2015.mtsummit-users.5,C14-1040,1,0.889922,"Missing"
2015.mtsummit-users.5,2014.amta-users.3,0,0.0356899,"teristics of these data and evaluate our models against post-edits produced by humans. 2 Background eBay is a marketplace platform in which sellers can advertise items and buyers can search for items, electronically bid and eventually buy them. To enable trade between buyers and sellers with different languages, at least four types of texts need to be translated: queries, item titles, descriptions, and item specifics. Machine translation has been recently introduced in eBay’s platform with the objective of fostering cross-border trade between sellers and buyers that speak different languages (Guha and Heger, 2014). In this work we predict the quality of translation of item titles, which are concise and usually very informative descriptions of items put on sale. One item title example is given below: Universal 12000mAh Backup External Battery USB Power Bank Charger for Cell Phone It specifies several characteristics of a product ranging from more generic information (i.e. ”Backup External Battery”) to more specific characteristics (i.e. 12000 mAh). Common challenges in the translation of eBay’s user generated content in general, and of titles (Sanchez and Badeka, 2014) are the correct rendering of prope"
2015.mtsummit-users.5,2011.mtsummit-papers.27,0,0.0351075,"014). Likewise, all the features designed in previous work assume that source sentences are grammatical and that the MT systems were trained over parallel data with fluent and well-formed segments. To the best of our knowledge, the first MT QE approach to consider user-generated data was presented by Rubino et al. (2013a). In this work, the authors present regression and classification models trained and evaluated on two different language pairs and two different domains. In particular, they developed a QE classification model for English-French information technology forums data described in Roturier and Bensadoun (2011). The approach explores features based on topic models that focus on the adequacy aspect of the translations (i.e. check whether the meaning of the source sentence is present in its translation). 1 The translation error rate is calculated as the number of edits (word insertions, deletions, substitutions, and shifts) divided by the number of words in the reference. Lower HTER values indicate better translations. Proceedings of MT Summit XV, vol. 2: MT Users&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 21 The same English-French dataset is used by Rubino et al. (2013b) to develop QE classification mod"
2015.mtsummit-users.5,2013.mtsummit-posters.13,1,0.901766,"Missing"
2015.mtsummit-users.5,2014.amta-users.1,0,0.0425036,"buyers that speak different languages (Guha and Heger, 2014). In this work we predict the quality of translation of item titles, which are concise and usually very informative descriptions of items put on sale. One item title example is given below: Universal 12000mAh Backup External Battery USB Power Bank Charger for Cell Phone It specifies several characteristics of a product ranging from more generic information (i.e. ”Backup External Battery”) to more specific characteristics (i.e. 12000 mAh). Common challenges in the translation of eBay’s user generated content in general, and of titles (Sanchez and Badeka, 2014) are the correct rendering of proper names and the translation of words which can have multiple senses, depending on the context in which they appear. Furthermore, words can appear in a relative free-order in the title without damaging its meaning. This presents a challenge for MT QE models because they assume that source sentences are well-formed and grammatical in the source language. 3 Related Work Most of the work for MT QE has been developed using well-formed and grammatical sentences belonging to different domains such as legal (transcription of political speeches), or news-wire texts co"
2015.mtsummit-users.5,W09-0441,0,0.0284293,"ed using supervised learning techniques. The different forms of supervision used to train the models imply different ways of perceiving the quality of a translation. The choice of the supervision label depends on the envisaged application scenario. For example, for regression and ranking, previous work employed either the time required to post-edit the translations or the minimum number of modifications required to make the translation acceptable as measured by the human Proceedings of MT Summit XV, vol. 2: MT Users&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 20 translation error rate (HTER1 , see Snover et al. (2009)). Another required information that must be defined a priori is the kind of linguistic cues that are going to be used to predict quality. Such indicators are extracted from the source and the translated sentence and aim to serve as a proxy for the complexity of translating the source sentence, the fluency of the translated sentence and the adequacy of the translation in function of the source. In this work we present the first approach to MT QE geared towards e-commerce usergenerated data. Our challenge is two-fold: (i) the data have been generated by many users and therefore are not necessar"
2015.mtsummit-users.5,2009.eamt-1.5,0,0.0680889,"ituations in which a quality score about the translation is required but no references translations are available. In MT QE, automatically translated sentences have their quality estimated without using references. Such scenarios include supporting the work of translators in a CAT scenario (Turchi et al., 2015), informing readers of the translation whether the translation is reliable or not (Turchi et al., 2012), selection of the best translation generated by a pool of MT systems (Specia et al., 2010), or filtering out low-quality translation suggestions that should be rewritten from scratch (Specia et al., 2009). QE is usually cast as a classification, regression or ranking problem that is modelled using supervised learning techniques. The different forms of supervision used to train the models imply different ways of perceiving the quality of a translation. The choice of the supervision label depends on the envisaged application scenario. For example, for regression and ranking, previous work employed either the time required to post-edit the translations or the minimum number of modifications required to make the translation acceptable as measured by the human Proceedings of MT Summit XV, vol. 2: M"
2015.mtsummit-users.5,P13-4014,1,0.902832,"Missing"
2015.mtsummit-users.5,P15-2087,1,0.832389,"ance. In order to evaluate the efficiency of our approach, we evaluate the quality scores assigned by the QE system (predicted TER) against the human posteditions (real TER) using the Pearson correlation coefficient. 1 Introduction Approaches to machine translation (MT) quality estimation (QE) are used in situations in which a quality score about the translation is required but no references translations are available. In MT QE, automatically translated sentences have their quality estimated without using references. Such scenarios include supporting the work of translators in a CAT scenario (Turchi et al., 2015), informing readers of the translation whether the translation is reliable or not (Turchi et al., 2012), selection of the best translation generated by a pool of MT systems (Specia et al., 2010), or filtering out low-quality translation suggestions that should be rewritten from scratch (Specia et al., 2009). QE is usually cast as a classification, regression or ranking problem that is modelled using supervised learning techniques. The different forms of supervision used to train the models imply different ways of perceiving the quality of a translation. The choice of the supervision label depe"
2015.mtsummit-users.5,2012.eamt-1.39,0,0.0160792,"he QE system (predicted TER) against the human posteditions (real TER) using the Pearson correlation coefficient. 1 Introduction Approaches to machine translation (MT) quality estimation (QE) are used in situations in which a quality score about the translation is required but no references translations are available. In MT QE, automatically translated sentences have their quality estimated without using references. Such scenarios include supporting the work of translators in a CAT scenario (Turchi et al., 2015), informing readers of the translation whether the translation is reliable or not (Turchi et al., 2012), selection of the best translation generated by a pool of MT systems (Specia et al., 2010), or filtering out low-quality translation suggestions that should be rewritten from scratch (Specia et al., 2009). QE is usually cast as a classification, regression or ranking problem that is modelled using supervised learning techniques. The different forms of supervision used to train the models imply different ways of perceiving the quality of a translation. The choice of the supervision label depends on the envisaged application scenario. For example, for regression and ranking, previous work emplo"
2020.iwslt-1.1,P18-4020,0,0.0279555,"Missing"
2020.iwslt-1.1,2005.iwslt-1.19,0,0.174539,"Missing"
2020.iwslt-1.1,2020.iwslt-1.11,0,0.0306756,"Missing"
2020.iwslt-1.1,W18-6319,0,0.0215087,"Missing"
2020.iwslt-1.1,2013.iwslt-papers.14,0,0.069836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.9,0,0.053316,"Missing"
2020.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0549836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.22,0,0.0613503,"Missing"
2020.iwslt-1.31,N19-1202,0,0.13224,"Missing"
2020.iwslt-1.31,W11-2123,0,0.0125396,"r, fj+1 )1/3 + Pr(fj , fj+1 )1/2 The rational of our model is that we want to favor split points were also TTS was trained to produce pauses. TTS was in fact trained on read speech that generally introduces pauses in correspondence of punctuation marks such as period, comma, semicolon, colon, etc. Notice that our interest, at the moment, is to produce fluent TTS speech, not to closely match the speaking style of the original speaker. In our implementation, we use a larger text window (last and first two words), we replace words with parts-of speech, and estimate the language model with KenLM (Heafield, 2011) on the training portion of the MUST-C corpus tagged with parts-of-speech using an online service6 . j By assuming a Markovian dependency on j, i.e.: k Pr(j ∣ i, e, f ) = ∑ log Pr(jt ∣ jt−1 ; t, i, e, f ) (2) 5 t=1 and omitting from the notation the constant terms i, e, f , we can derive the following recurrent quantity: Text To Speech Our neural TTS system consists of two modules: a Context Generation module, which generates a context sequence from the input text, and a Neural Vocoder module, which converts the context sequence into a speech waveform. The first one is an attention-based seque"
2020.iwslt-1.31,N19-2026,1,0.787172,"of the MUST-C corpus tagged with parts-of-speech using an online service6 . j By assuming a Markovian dependency on j, i.e.: k Pr(j ∣ i, e, f ) = ∑ log Pr(jt ∣ jt−1 ; t, i, e, f ) (2) 5 t=1 and omitting from the notation the constant terms i, e, f , we can derive the following recurrent quantity: Text To Speech Our neural TTS system consists of two modules: a Context Generation module, which generates a context sequence from the input text, and a Neural Vocoder module, which converts the context sequence into a speech waveform. The first one is an attention-based sequence-to-sequence network (Prateek et al., 2019; Latorre et al., 2019) that predicts a Mel-spectrogram given an input text. A grapheme-to-phoneme module converts the sequence of words into a sequence of phonemes Q(j, t) = max log Pr(j ∣ j ′ ; t) + Q(j ′ , t − 1) (3) ′ j <j where Q(j, t) denotes the log-probability of the optimal segmentation of f up to position j with t break points. It is easy to show that the solution of (1) corresponds to Q(m, k) and that optimal segmentation can be efficiently computed via 3 5 We approximate the duration d(⋅) of a segment with the sum of the lengths of its words. We plan to use better approx¨ imations"
2020.iwslt-1.31,lavie-etal-2002-nespole,0,0.140528,"n, which measures the perceived naturalness of automatic dubbing and the relative importance of each proposed enhancement. 1 Introduction Automatic dubbing can be regarded as an extension of the speech-to-speech translation (STST) task (Wahlster, 2013), which is generally seen as the combination of three sub-tasks: (i) transcribing speech to text in a source language (ASR), (ii) translating text from a source to a target language (MT) and (iii) generating speech from text in a target language (TTS). Independently from the implementation approach (Weiss et al., 2017; Waibel, 1996; Vidal, 1997; Metze et al., 2002; Nakamura et al., 2006; Casacuberta et al., 2008), the main goal of STST is producing an output that reflects the linguistic content of the original sentence. On the other hand, automatic dubbing aims to replace all speech contained in a video document with speech in a different language, so that the result sounds and looks as natural as the original. Hence, in addition to conveying the same content of the original utterance, dubbing should also match the ∗ 1 Actually, there is still a divide between countries/languages where either subtitling or dubbing are the preferred translation modes (K"
2020.iwslt-1.31,Q17-1024,0,\N,Missing
2020.iwslt-1.32,E17-3017,0,0.0385322,"Missing"
2020.iwslt-1.32,2005.mtsummit-papers.11,0,0.0180579,"the model reaches high precision in conversion only with data two orders of magnitude larger. Both 3.2 Joint MT and unit conversion In a second set of experiments we investigate if the transformer model is able to perform both the translation and the unit conversion tasks and learns to adequately switch from one to the other in context. We use the same architecture as in the previous section, with minor modifications: we use subword embeddings with a shared vocabulary of size 32000 and a maximum number of epochs of 30. Data As standard MT parallel data we use a collection containing Europarl (Koehn, 2005) and news commentary data from WMT En→De shared task 2019 totalling 2.2 million sentences.2 Standard translation test sets do not have, however, enough examples of unit conversions and in fact corpora such as CommonCrawl show inconsistent treatment of units. For this reason, we create a unit conversion (Localization) data set. We extract sentences containing Fahrenheit/Celsius and miles/km from a mix of open source data sets namely, ParaCrawl, DGT (Translation Memories), Wikipedia and OpenSubtitles, TED talks from OPUS (Tiedemann, 2012). Regular expressions are used to extract the sentences co"
2020.iwslt-1.32,N16-1136,0,0.0475449,"Missing"
2020.iwslt-1.32,Q16-1037,0,0.0132697,"es that Transformer networks suffer limitations in modeling regular periodic languages (such as an bn ) as well as hierarchical (context-free) structures, unless their depth or self-attention heads increase with the input length. On the other hand, Merrill (2019) proves that LSTM networks can recognize a subset of periodic languages. Also experimental papers analyzed the capability of LSTMs to recognize these two language classes (Weiss et al., 2018; Suzgun et al., 2019; Sennhauser and Berwick, 2018; Skachkova et al., 2018; Bernardy, 2018), as well as natural language hierarchical structures (Linzen et al., 2016; Gulordava et al., 2018). It is worth noticing, however, that differently from formal language recognition tasks, state of the art machine translation systems (Barrault et al., 2019; Niehues et al., 2019) are still based on the Transformer architecture . Other related work addresses specialized neural architectures capable to process and reason with numerical expressions for binary addition, evaluating arithmetic expressions or other number manipulation tasks (Joulin and Mikolov, 2015; Saxton et al., 2019; Trask et al., 2018; Chen et al., 2018). While this line of work is very relevant, we fo"
2020.iwslt-1.32,W19-3901,0,0.0159045,"al problems, or any of the benchmarks above, in an end-to-end fashion. Related work Several theoretical and empirical works have addressed the computational capabilities end expressiveness of deep learning models. Theoretical studies on language modeling have mostly targeted simple grammars from the Chomsky hierarchy. In particular, Hahn (2019) proves that Transformer networks suffer limitations in modeling regular periodic languages (such as an bn ) as well as hierarchical (context-free) structures, unless their depth or self-attention heads increase with the input length. On the other hand, Merrill (2019) proves that LSTM networks can recognize a subset of periodic languages. Also experimental papers analyzed the capability of LSTMs to recognize these two language classes (Weiss et al., 2018; Suzgun et al., 2019; Sennhauser and Berwick, 2018; Skachkova et al., 2018; Bernardy, 2018), as well as natural language hierarchical structures (Linzen et al., 2016; Gulordava et al., 2018). It is worth noticing, however, that differently from formal language recognition tasks, state of the art machine translation systems (Barrault et al., 2019; Niehues et al., 2019) are still based on the Transformer arc"
2020.iwslt-1.32,P16-1202,0,0.0224051,"es specialized neural architectures capable to process and reason with numerical expressions for binary addition, evaluating arithmetic expressions or other number manipulation tasks (Joulin and Mikolov, 2015; Saxton et al., 2019; Trask et al., 2018; Chen et al., 2018). While this line of work is very relevant, we focus on the natural intersection of formal and everyday language. The types of generalization that these studies address, such as testing with numbers orders of magnitude larger than those in seen in training, are less relevant to our task. The task of solving verbal math problems (Mitra and Baral, 2016; Wang et al., 2017; KoncelKedziorski et al., 2016; Saxton et al., 2019) specifically addresses natural language mixed with formal language. Similarly, (Ravichander et al., 2019) introduces a benchmark for evaluating quantitative 3 Unit conversion in MT localization The goal of localization is to enhance plain content translation so that the final result looks and feels as being created for a specific target audience. Parallel corpora in general include localization of formats numeric expressions (e.g. from 1,000,000.00 (en-us) to 1.000.000,00 (de-de)). Format conversions in most of the cases"
2020.iwslt-1.32,N18-1202,0,0.02766,"e phenomena: e.g. a neural network may learn to invoke a simple conversion rule for dates, if enough examples are seen training. However, at the other end of the spectrum, correctly converting distance units, which itself is a simple algorithm, requires knowledge of numbers, basic arithmetic and the specific conversion function to apply. It is unrealistic to assume a model could learn such conversions from limited amounts of parallel running text alone. Furthermore, this is an unrealistic task even for distributional, unsupervised pre-training (Turney and Pantel, 2010; Baroni and Lenci, 2010; Peters et al., 2018), despite the success of such methods in capturing other nonlinguistic phenomena such as world knowledge or cultural biases (Bolukbasi et al., 2016; Vanmassenhove et al., 2018).1 While the second approach is currently the preferred one in translation technology, such decoupling methods do not bring us closer to end-to-end solutions and they ignore the often tight interplay of the two types of language: taking unit conversion as an example, approximately 500 miles, should be translated into ungef¨ahr 800 km (approx. 800km) and not ungef¨ahr 804 km (approx. 804km). In this paper we highlight sev"
2020.iwslt-1.32,K19-1033,0,0.022023,"Missing"
2020.iwslt-1.32,W18-5414,0,0.0140865,"tudies on language modeling have mostly targeted simple grammars from the Chomsky hierarchy. In particular, Hahn (2019) proves that Transformer networks suffer limitations in modeling regular periodic languages (such as an bn ) as well as hierarchical (context-free) structures, unless their depth or self-attention heads increase with the input length. On the other hand, Merrill (2019) proves that LSTM networks can recognize a subset of periodic languages. Also experimental papers analyzed the capability of LSTMs to recognize these two language classes (Weiss et al., 2018; Suzgun et al., 2019; Sennhauser and Berwick, 2018; Skachkova et al., 2018; Bernardy, 2018), as well as natural language hierarchical structures (Linzen et al., 2016; Gulordava et al., 2018). It is worth noticing, however, that differently from formal language recognition tasks, state of the art machine translation systems (Barrault et al., 2019; Niehues et al., 2019) are still based on the Transformer architecture . Other related work addresses specialized neural architectures capable to process and reason with numerical expressions for binary addition, evaluating arithmetic expressions or other number manipulation tasks (Joulin and Mikolov,"
2020.iwslt-1.32,W16-2209,0,0.036683,"Missing"
2020.iwslt-1.32,W18-5425,0,0.0147413,"ve mostly targeted simple grammars from the Chomsky hierarchy. In particular, Hahn (2019) proves that Transformer networks suffer limitations in modeling regular periodic languages (such as an bn ) as well as hierarchical (context-free) structures, unless their depth or self-attention heads increase with the input length. On the other hand, Merrill (2019) proves that LSTM networks can recognize a subset of periodic languages. Also experimental papers analyzed the capability of LSTMs to recognize these two language classes (Weiss et al., 2018; Suzgun et al., 2019; Sennhauser and Berwick, 2018; Skachkova et al., 2018; Bernardy, 2018), as well as natural language hierarchical structures (Linzen et al., 2016; Gulordava et al., 2018). It is worth noticing, however, that differently from formal language recognition tasks, state of the art machine translation systems (Barrault et al., 2019; Niehues et al., 2019) are still based on the Transformer architecture . Other related work addresses specialized neural architectures capable to process and reason with numerical expressions for binary addition, evaluating arithmetic expressions or other number manipulation tasks (Joulin and Mikolov, 2015; Saxton et al., 20"
2020.iwslt-1.32,P19-1355,0,0.0248146,"Missing"
2020.iwslt-1.32,W19-0128,0,0.0232201,"Missing"
2020.iwslt-1.32,tiedemann-2012-parallel,0,0.0445256,"ard MT parallel data we use a collection containing Europarl (Koehn, 2005) and news commentary data from WMT En→De shared task 2019 totalling 2.2 million sentences.2 Standard translation test sets do not have, however, enough examples of unit conversions and in fact corpora such as CommonCrawl show inconsistent treatment of units. For this reason, we create a unit conversion (Localization) data set. We extract sentences containing Fahrenheit/Celsius and miles/km from a mix of open source data sets namely, ParaCrawl, DGT (Translation Memories), Wikipedia and OpenSubtitles, TED talks from OPUS (Tiedemann, 2012). Regular expressions are used to extract the sentences containing the units and modify the source or the reference by 267 2 We opt for a smaller experiment in order to speed up computations and to prioritize efficiency in our experiments (Strubell et al., 2019). We have no reason to assume any dependency on the data size. Conv MT Loc. Example 5 2 1 miles We do not know what is happening. The venue is within 3 . 8 miles from the city center 8 3 9 km Wir wissen nicht, was passiert. Die Unterkunft ist 6 km vom Stadtzentrum entfernt Table 1: The three types of data used in training the joint mode"
2020.iwslt-1.32,D18-1334,0,0.0142777,"m, correctly converting distance units, which itself is a simple algorithm, requires knowledge of numbers, basic arithmetic and the specific conversion function to apply. It is unrealistic to assume a model could learn such conversions from limited amounts of parallel running text alone. Furthermore, this is an unrealistic task even for distributional, unsupervised pre-training (Turney and Pantel, 2010; Baroni and Lenci, 2010; Peters et al., 2018), despite the success of such methods in capturing other nonlinguistic phenomena such as world knowledge or cultural biases (Bolukbasi et al., 2016; Vanmassenhove et al., 2018).1 While the second approach is currently the preferred one in translation technology, such decoupling methods do not bring us closer to end-to-end solutions and they ignore the often tight interplay of the two types of language: taking unit conversion as an example, approximately 500 miles, should be translated into ungef¨ahr 800 km (approx. 800km) and not ungef¨ahr 804 km (approx. 804km). In this paper we highlight several of such language mixing phenomena related to the task of localization for translation and focus on two distance (miles to kilometers) and temperature (Fahrenheit to Celsiu"
2020.iwslt-1.32,D19-1534,0,0.0150114,"e preferred one in translation technology, such decoupling methods do not bring us closer to end-to-end solutions and they ignore the often tight interplay of the two types of language: taking unit conversion as an example, approximately 500 miles, should be translated into ungef¨ahr 800 km (approx. 800km) and not ungef¨ahr 804 km (approx. 804km). In this paper we highlight several of such language mixing phenomena related to the task of localization for translation and focus on two distance (miles to kilometers) and temperature (Fahrenheit to Celsius) conversion tasks. Specifically, we per1 (Wallace et al., 2019) show that numeracy is encoded in pre-trained embeddings. While promising, this does not show that more complex and varied manipulation of numerical expressions can be learned in a solely unsupervised fashion. 265 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 265–271 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 form experiments using the popular MT transformer architecture and show that the model is successful at learning these functions from symbolically represented examples. Furthermore, we"
2020.iwslt-1.32,D17-1088,0,0.0429085,"Missing"
2020.iwslt-1.32,P18-2117,0,0.0137245,"ss of deep learning models. Theoretical studies on language modeling have mostly targeted simple grammars from the Chomsky hierarchy. In particular, Hahn (2019) proves that Transformer networks suffer limitations in modeling regular periodic languages (such as an bn ) as well as hierarchical (context-free) structures, unless their depth or self-attention heads increase with the input length. On the other hand, Merrill (2019) proves that LSTM networks can recognize a subset of periodic languages. Also experimental papers analyzed the capability of LSTMs to recognize these two language classes (Weiss et al., 2018; Suzgun et al., 2019; Sennhauser and Berwick, 2018; Skachkova et al., 2018; Bernardy, 2018), as well as natural language hierarchical structures (Linzen et al., 2016; Gulordava et al., 2018). It is worth noticing, however, that differently from formal language recognition tasks, state of the art machine translation systems (Barrault et al., 2019; Niehues et al., 2019) are still based on the Transformer architecture . Other related work addresses specialized neural architectures capable to process and reason with numerical expressions for binary addition, evaluating arithmetic expressions or o"
2020.iwslt-1.32,J10-4006,0,\N,Missing
2020.iwslt-1.32,2018.lilt-16.1,0,\N,Missing
2020.iwslt-1.32,N19-1246,0,\N,Missing
2021.iwslt-1.1,2020.lrec-1.520,0,0.0759372,"Missing"
2021.iwslt-1.1,2020.iwslt-1.3,0,0.0592828,"Missing"
2021.iwslt-1.1,2021.iwslt-1.5,0,0.0628415,"Missing"
2021.iwslt-1.1,N19-1202,1,0.897776,"Missing"
2021.iwslt-1.1,2021.acl-long.224,1,0.769761,"sh-German section of the MuST-C V2 corpus7 and include training, dev, and test (Test Common), in the same structure of the MuST-C V1 corpus (Cattoni et al., 2021) used last year. Since the 2021 test set was processed using the same pipeline applied to create MuST-C V2, the use of the new training resource was strongly recommended. The main differences with respect to MuST-C v1 are: gap with respect to the traditional cascade approach (integrating ASR and MT components in a pipelined architecture). In light of last year’s IWSLT results (Ansari et al., 2020) and of the findings of recent works (Bentivogli et al., 2021) attesting that the gap between the two paradigms has substantially closed, also this year a key element of the evaluation was to set up a shared framework for their comparison. For this reason, and to reliably measure progress with respect to the past rounds, the general evaluation setting was kept unchanged. This stability mainly concerns two aspects: the allowed architectures and the test set provision. On the architecture side, participation was allowed both with cascade and end-to-end (also known as direct) systems. In the latter case, valid submissions had to be obtained by models that:"
2021.iwslt-1.1,2021.iwslt-1.22,0,0.082468,"Missing"
2021.iwslt-1.1,2021.iwslt-1.27,1,0.721453,"to use the same training and development data as in the Offline Speech Translation track. More details are available in §3.2. For the English-Japanese text-to-text track, participants could use the parallel data and monolingual data available for the English-Japanese WMT20 news task (Barrault et al., 2020). For development, participants could use the IWSLT 2017 development sets,2 the IWSLT 2021 development set3 and the simultaneous interpretation transcripts for the IWSLT 2021 development set.4 The simultaneous interpretation was recorded as a part of NAIST Simultaneous Interpretation Corpus (Doi et al., 2021). Systems were evaluated with respect to quality and latency. Quality was evaluated with the standard BLEU metric (Papineni et al., 2002a). Latency was evaluated with metrics developed for simultaneous machine translation, including average proportion (AP), average lagging (AL) and differentiable average lagging (DAL, Cherry and Foster 2019), and later extended to the task of simultaneous speech translation (Ma et al., 2020b). The evaluation was run with the S IMUL E VAL toolkit (Ma et al., 2020a). For the latency measurement of speech input systems, we contrasted computation-aware and non com"
2021.iwslt-1.1,2012.eamt-1.60,1,0.698226,"rently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun,"
2021.iwslt-1.1,2020.iwslt-1.8,1,0.838546,"Missing"
2021.iwslt-1.1,2021.iwslt-1.12,0,0.0248123,"a et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two S"
2021.iwslt-1.1,L18-1001,0,0.0652124,"Missing"
2021.iwslt-1.1,2021.acl-long.68,1,0.787992,"Missing"
2021.iwslt-1.1,L18-1275,0,0.0286278,"ng two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guidelines.19 This makes them less literal compared to standard, unconstrained translations; • OpenSubtitles 2018 (Lison et al., 2018); • Augmented LibriSpeech et al., 2018)16 (Kocabiyikoglu • Mozilla Common Voice17 ; • Unconstrained translations. These references were created from scratch20 by adhering to the usual translation guidelines. They are hence exact (more literal) translations, without paraphrasing and with proper punctuation. • LibriSpeech ASR corpus (Panayotov et al., 2015). The list of allowed development data includes the dev set from IWSLT 2010, as well as the test sets used for the 2010, 2013, 2014, 2015 and 2018 IWSLT campaigns. Using other training/development resources was allowed but, in this case, parti"
2021.iwslt-1.1,2020.iwslt-1.9,0,0.0594925,"Missing"
2021.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0742394,"Missing"
2021.iwslt-1.1,2021.iwslt-1.4,0,0.134226,"essler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and acces"
2021.iwslt-1.1,N18-2074,0,0.0499123,"Missing"
2021.iwslt-1.1,2006.amta-papers.25,0,0.370434,"Missing"
2021.iwslt-1.1,W14-3354,0,0.0745687,"Missing"
2021.iwslt-1.1,W18-6319,0,0.0144137,"cipants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guideli"
2021.iwslt-1.1,2021.iwslt-1.19,0,0.0280723,", 2021) Fondazione Bruno Kessler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; comm"
2021.iwslt-1.1,2020.findings-emnlp.230,0,0.0691772,"Missing"
2021.iwslt-1.1,2021.iwslt-1.7,0,0.0794497,"Missing"
2021.iwslt-1.1,2021.iwslt-1.16,0,0.0332503,"nyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two Swahili varieties (Congolese and Co"
2021.iwslt-1.1,2020.lrec-1.517,1,0.846095,"s were finally converted from two (stereo) to one (mono) channel and downsampled from 48 to 16 kHz, using FFmpeg.10 Upon inspection of the spectrograms of the same talks in the two versions of MuST-C, it clearly emerges that the upper limit band in the audios used in MuST-C V1 is 5 kHz, while it is at 8 kHz in the latest version, coherently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 thi"
2021.iwslt-1.1,2021.iwslt-1.6,0,0.156379,"2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource spe"
2021.iwslt-1.31,W17-4706,0,0.134285,"es are available. Sennrich et al. (2016) took this algorithm as a starting point, considering characters instead of bytes, and joining them using the same criterion to produce sub-word units (more details can be found in Section 3). One potential problem with this approach is that the objective of the original BPE algorithm differs from the goals for which it is being used for translation, as detailed above. While it is certainly effective for the first objective (reducing the vocabulary size), it is arguable whether it is appropriate for the goal of generating new words (Ataman et al., 2017; Huck et al., 2017; Banerjee and Bhattacharyya, 2018). Intuitively, in order to generate new words, we would expect the sub-word units to have some linguistic meaning, so that a new word can be created 263 Proceedings of the 18th International Conference on Spoken Language Translation, pages 263–275 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics beklagen ↓ bek@@ lagen founded optimization criterion also allows us to define a data driven stopping criterion. Our proposed criterion allows to select a nearly optimal number of units using only an intrinsic measure on th"
2021.iwslt-1.31,W04-3250,0,0.607871,"Missing"
2021.iwslt-1.31,P18-1007,0,0.08563,"n TED Talks in five directions, all from/to English, show systematic improvements on Arabic, Turkish, Czech, but not on Italian and German. Banerjee and Bhattacharyya (2018) also use unsupervised morphological units generated by Morfessor (Virpioja et al., 2013) as input for a neural machine translation system and report improvements for low-resource conditions. Macháˇcek et al. (2018) follow a similar approach for translation into Czech on WMT data, but were not able to obtains improvements over the standard BPE approach. An alternative model to BPE which is also widely used was presented by Kudo (2018), which can be considered as an extension of (Schuster and Nakajima, 2012). They show that using a purely statistical approach, they are able to achieve subword units that are better linguistically motivated. Similar to our approach, a probability distribution over the sub-word units is defined with the goal of improving the likelihood over the training data. The strategy for defining the sub-word units differ 264 in his approach and ours. While we start with single characters and expand the units, Kudo (2018) starts with a large set of sub-word units and prunes iteratively until reducing the"
2021.iwslt-1.31,W16-2322,0,0.0431447,"Missing"
2021.iwslt-1.31,P16-1162,0,0.613236,"oal is twofold: On the one hand, sub-word splitting reduces the size of the input and output vocabularies. This is specially important when using neural models, as the size of the input layer is fixed and thus the vocabulary size cannot be dynamically adjusted. On the other hand, it tries to increase the generalization capabilities of the translation model, enabling the system to accept and/or generate new words at translation time by combining previously seen units. The most widespread method used for sub-word splitting in neural machine translation is Byte Pair Encoding (BPE), introduced by Sennrich et al. (2016). Since then, BPE has become a default preprocessing step for many NLP tasks. The BPE extraction algorithm is an adaptation of the algorithm introduced by Gage (1994) for data compression. The main idea of this algorithm is to replace the most frequent pair of bytes found in the input data with a new, unseen byte. The process is repeated until no more byte pairs are repeated or until no free bytes are available. Sennrich et al. (2016) took this algorithm as a starting point, considering characters instead of bytes, and joining them using the same criterion to produce sub-word units (more detai"
2021.iwslt-1.31,P19-1021,0,0.0206795,"n does not specify a criterion for stopping the creation of new symbols. If the algorithm runs for an unlimited time, it will merge all sub-words into the original input vocabulary, which is clearly undesired. In practice, one specifies a fixed number of merges to be carried out, or a threshold frequency and when the considered symbols fall below this value the algorithm is stopped. It is however not clear how to set these hyperparameters, although they can have a drastic effect on translation quality depending on the translation direction, task and amount of data (Denkowski and Neubig, 2017; Sennrich and Zhang, 2019). Furthermore, these hyperparameters are rarely optimized, as evaluating them constitutes a full training-evaluation cycle, which is notoriously costly. In this paper we introduce a new criterion for defining sub-word units that tries to address these shortcomings. We introduce a probability distribution over the units which in turn induces a likelihood function over the corpus which we can optimize. We will show how this statistical approach can guide the extraction process towards more linguistically satisfying units, while still remaining a purely data driven approach. Having a well Related"
2021.naacl-main.94,D19-1166,0,0.0153336,"ut style and not 1193 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1193–1199 June 6–11, 2021. ©2021 Association for Computational Linguistics translation quality. Finally, we show that the translator information has more impact on NMT than the speaker information, which was investigated by Michel and Neubig (2018). 2 Related Work Style itself is a broad concept (Kang and Hovy, 2019). It includes both simple high-level stylistic aspects of language such as verbosity (Marchisio et al., 2019; Agrawal and Carpuat, 2019; Lakew et al., 2019), formality (Niu et al., 2017; Xu et al., 2019), politeness (Mirkin et al., 2015) and complex aspects such as demography (Vanmassenhove et al., 2018; Moryossef et al., 2019; Hovy et al., 2020) and personal traits (Mirkin and Meunier, 2015; Rabinovich et al., 2017; Michel and Neubig, 2018). Our study focuses on capturing the personal style of translators. The closest work to our study is thus the work of Michel and Neubig (2018), where they study instead the effects of using the speaker information in NMT. In our results, we show that the translator information has indeed m"
2021.naacl-main.94,2020.ngt-1.21,0,0.0185514,", 2020) and personal traits (Mirkin and Meunier, 2015; Rabinovich et al., 2017; Michel and Neubig, 2018). Our study focuses on capturing the personal style of translators. The closest work to our study is thus the work of Michel and Neubig (2018), where they study instead the effects of using the speaker information in NMT. In our results, we show that the translator information has indeed more impact to NMT than the speaker information. Finally, another distantly related research line tries to improve the diversity in the top rank translations of an input (Li et al., 2016; Shen et al., 2019; Agrawal and Carpuat, 2020). In fact, adding the translator information to NMT also provides means to generate translations with significantly different stylistic variations. 3 NMT with Translator Information NMT reads an input sequence x = x1 , ..., xn in the source language with an encoder and then produces an output sequence y = y1 , ..., ym in the target language. The generation process is performed in a token-by-token Qmmanner and its probability can be factored as j=1 P (yj |y&lt;j , x), where y&lt;j denotes the previous sub-sequence before j-th token. The prediction for each token over the vocabulary V is based on a so"
2021.naacl-main.94,2012.eamt-1.60,1,0.716123,"an be thought of as a translator-token embedding with dimension |V |rather than d. We also explore another variant, named FACT-B IAS, as in Michel and Neubig (2018). This variant instead learns the translator bias through the factorization: bT = WsT , (6) with parameters W ∈ R|V|×k and sT ∈ Rk×1 where k &lt;&lt; |V|. Note that while the above methods digest the translator token at an earlier stage, this one consumes translator signals in a late fusion manner. 4 4.1 Experiments Dataset and Models We run experiments with the WIT3 public dataset Here, oj ∈ is an output vector with size d of TED talks (Cettolo et al., 2012), with four (e.g. 512 or 1024), encoding both the context from language pairs: English-German (en-de), Englishthe encoder and the state of the decoder at time j. French (en-fr), English-Italian (en-it) and EnglishMeanwhile, WV ∈ R|V|×d and bV ∈ R|V |are a Spanish (en-es). The dataset contains both speaker trainable projection matrix and bias vector. and translator information for each talk and translaWe adjust NMT in different ways as below to let it tion, thus allowing to measure the effects of transmimic and control the translator’s style. lators and speakers. 1194 Rd 100 300 90 302 80 60 20"
2021.naacl-main.94,D14-1179,0,0.00921597,"Missing"
2021.naacl-main.94,2020.acl-main.154,0,0.0164375,"Computational Linguistics translation quality. Finally, we show that the translator information has more impact on NMT than the speaker information, which was investigated by Michel and Neubig (2018). 2 Related Work Style itself is a broad concept (Kang and Hovy, 2019). It includes both simple high-level stylistic aspects of language such as verbosity (Marchisio et al., 2019; Agrawal and Carpuat, 2019; Lakew et al., 2019), formality (Niu et al., 2017; Xu et al., 2019), politeness (Mirkin et al., 2015) and complex aspects such as demography (Vanmassenhove et al., 2018; Moryossef et al., 2019; Hovy et al., 2020) and personal traits (Mirkin and Meunier, 2015; Rabinovich et al., 2017; Michel and Neubig, 2018). Our study focuses on capturing the personal style of translators. The closest work to our study is thus the work of Michel and Neubig (2018), where they study instead the effects of using the speaker information in NMT. In our results, we show that the translator information has indeed more impact to NMT than the speaker information. Finally, another distantly related research line tries to improve the diversity in the top rank translations of an input (Li et al., 2016; Shen et al., 2019; Agrawal"
2021.naacl-main.94,P07-2045,1,0.0244986,"Missing"
2021.naacl-main.94,W19-6619,0,0.0129172,"differences are all about style and not 1193 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1193–1199 June 6–11, 2021. ©2021 Association for Computational Linguistics translation quality. Finally, we show that the translator information has more impact on NMT than the speaker information, which was investigated by Michel and Neubig (2018). 2 Related Work Style itself is a broad concept (Kang and Hovy, 2019). It includes both simple high-level stylistic aspects of language such as verbosity (Marchisio et al., 2019; Agrawal and Carpuat, 2019; Lakew et al., 2019), formality (Niu et al., 2017; Xu et al., 2019), politeness (Mirkin et al., 2015) and complex aspects such as demography (Vanmassenhove et al., 2018; Moryossef et al., 2019; Hovy et al., 2020) and personal traits (Mirkin and Meunier, 2015; Rabinovich et al., 2017; Michel and Neubig, 2018). Our study focuses on capturing the personal style of translators. The closest work to our study is thus the work of Michel and Neubig (2018), where they study instead the effects of using the speaker information in NMT. In our results, we show that the translat"
2021.naacl-main.94,P18-2050,0,0.296762,"to model and control new data in different and controllable styles. translator-related stylistic variations in translation. In this work, we investigate methods to augNote that using a discrete token is a common apment the state-of-the-art Transformer model proach to model and control not only specific traits with translator information that is available in in translation such as verbosity, politeness and part of the training data. We show that our style-augmented translation models are able to speaker-related variances (Sennrich et al., 2016a; capture the style variations of translators and Michel and Neubig, 2018)) but also other aspects to generate translations with different styles on in NMT such as language ids (Johnson et al., 2017; new data. Indeed, the generated variations difFan et al., 2020). However, our study is the first to fer significantly, up to +4.5 BLEU score differuse such a discrete token to model the style of transence. Despite that, human evaluation confirms lators. It also provides several insights regarding that the translations are of the same quality. translation style modeling as follows. 1 Introduction First, we show that the state-of-the-art Transformer model implicitly learn"
2021.naacl-main.94,D15-1238,0,0.021725,"uality. Finally, we show that the translator information has more impact on NMT than the speaker information, which was investigated by Michel and Neubig (2018). 2 Related Work Style itself is a broad concept (Kang and Hovy, 2019). It includes both simple high-level stylistic aspects of language such as verbosity (Marchisio et al., 2019; Agrawal and Carpuat, 2019; Lakew et al., 2019), formality (Niu et al., 2017; Xu et al., 2019), politeness (Mirkin et al., 2015) and complex aspects such as demography (Vanmassenhove et al., 2018; Moryossef et al., 2019; Hovy et al., 2020) and personal traits (Mirkin and Meunier, 2015; Rabinovich et al., 2017; Michel and Neubig, 2018). Our study focuses on capturing the personal style of translators. The closest work to our study is thus the work of Michel and Neubig (2018), where they study instead the effects of using the speaker information in NMT. In our results, we show that the translator information has indeed more impact to NMT than the speaker information. Finally, another distantly related research line tries to improve the diversity in the top rank translations of an input (Li et al., 2016; Shen et al., 2019; Agrawal and Carpuat, 2020). In fact, adding the trans"
2021.naacl-main.94,D15-1130,0,0.0202095,"or Computational Linguistics: Human Language Technologies, pages 1193–1199 June 6–11, 2021. ©2021 Association for Computational Linguistics translation quality. Finally, we show that the translator information has more impact on NMT than the speaker information, which was investigated by Michel and Neubig (2018). 2 Related Work Style itself is a broad concept (Kang and Hovy, 2019). It includes both simple high-level stylistic aspects of language such as verbosity (Marchisio et al., 2019; Agrawal and Carpuat, 2019; Lakew et al., 2019), formality (Niu et al., 2017; Xu et al., 2019), politeness (Mirkin et al., 2015) and complex aspects such as demography (Vanmassenhove et al., 2018; Moryossef et al., 2019; Hovy et al., 2020) and personal traits (Mirkin and Meunier, 2015; Rabinovich et al., 2017; Michel and Neubig, 2018). Our study focuses on capturing the personal style of translators. The closest work to our study is thus the work of Michel and Neubig (2018), where they study instead the effects of using the speaker information in NMT. In our results, we show that the translator information has indeed more impact to NMT than the speaker information. Finally, another distantly related research line tries"
2021.naacl-main.94,W19-3807,0,0.0119101,"1. ©2021 Association for Computational Linguistics translation quality. Finally, we show that the translator information has more impact on NMT than the speaker information, which was investigated by Michel and Neubig (2018). 2 Related Work Style itself is a broad concept (Kang and Hovy, 2019). It includes both simple high-level stylistic aspects of language such as verbosity (Marchisio et al., 2019; Agrawal and Carpuat, 2019; Lakew et al., 2019), formality (Niu et al., 2017; Xu et al., 2019), politeness (Mirkin et al., 2015) and complex aspects such as demography (Vanmassenhove et al., 2018; Moryossef et al., 2019; Hovy et al., 2020) and personal traits (Mirkin and Meunier, 2015; Rabinovich et al., 2017; Michel and Neubig, 2018). Our study focuses on capturing the personal style of translators. The closest work to our study is thus the work of Michel and Neubig (2018), where they study instead the effects of using the speaker information in NMT. In our results, we show that the translator information has indeed more impact to NMT than the speaker information. Finally, another distantly related research line tries to improve the diversity in the top rank translations of an input (Li et al., 2016; Shen e"
2021.naacl-main.94,D17-1299,0,0.0208028,"the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1193–1199 June 6–11, 2021. ©2021 Association for Computational Linguistics translation quality. Finally, we show that the translator information has more impact on NMT than the speaker information, which was investigated by Michel and Neubig (2018). 2 Related Work Style itself is a broad concept (Kang and Hovy, 2019). It includes both simple high-level stylistic aspects of language such as verbosity (Marchisio et al., 2019; Agrawal and Carpuat, 2019; Lakew et al., 2019), formality (Niu et al., 2017; Xu et al., 2019), politeness (Mirkin et al., 2015) and complex aspects such as demography (Vanmassenhove et al., 2018; Moryossef et al., 2019; Hovy et al., 2020) and personal traits (Mirkin and Meunier, 2015; Rabinovich et al., 2017; Michel and Neubig, 2018). Our study focuses on capturing the personal style of translators. The closest work to our study is thus the work of Michel and Neubig (2018), where they study instead the effects of using the speaker information in NMT. In our results, we show that the translator information has indeed more impact to NMT than the speaker information. Fi"
2021.naacl-main.94,N19-4009,0,0.0383895,"Missing"
2021.naacl-main.94,W18-6301,0,0.0167898,"ention hidden state of size 1024 and a feedforward hidden state of size 4096. We employ Adam optimizer (Kingma and Ba, 2015) to update model parameters. We warm up the model by linearly increasing the learning rate from 1 × 10−7 to 5 × 10−4 for 4000 updates and then decay it with an inverse square root of the rest training steps by a rate of 1 × 10−4 . We apply a Dropout of 0.3 for en-de and 0.1 for both en-fr and en-it. For all MT systems, we load weights from pretrained models to set up a better model initialization. Specifically, we employ models pretrained on WMT data for en-de and en-fr (Ott et al., 2018), and pretrain models for en-it and en-es using our large in-house out-of-domain data, as there are no previous pretrained models for these pairs. We finetune models on TED talk data for 10 epochs4 and select the best model based on the validation loss. During inference, we employ beam search with a beam size of 4 and add a length penalty of 0.4. We use the BLEU score (Papineni et al., 2002) to evaluate translation accuracy. 4.2 4.2.1 Table 1: Data statistics for four language pairs. We construct training, validation and test sets for each translation direction as follows. We first extract all"
2021.naacl-main.94,P02-1040,0,0.109818,"Missing"
2021.naacl-main.94,D18-1334,0,0.0135555,"ges 1193–1199 June 6–11, 2021. ©2021 Association for Computational Linguistics translation quality. Finally, we show that the translator information has more impact on NMT than the speaker information, which was investigated by Michel and Neubig (2018). 2 Related Work Style itself is a broad concept (Kang and Hovy, 2019). It includes both simple high-level stylistic aspects of language such as verbosity (Marchisio et al., 2019; Agrawal and Carpuat, 2019; Lakew et al., 2019), formality (Niu et al., 2017; Xu et al., 2019), politeness (Mirkin et al., 2015) and complex aspects such as demography (Vanmassenhove et al., 2018; Moryossef et al., 2019; Hovy et al., 2020) and personal traits (Mirkin and Meunier, 2015; Rabinovich et al., 2017; Michel and Neubig, 2018). Our study focuses on capturing the personal style of translators. The closest work to our study is thus the work of Michel and Neubig (2018), where they study instead the effects of using the speaker information in NMT. In our results, we show that the translator information has indeed more impact to NMT than the speaker information. Finally, another distantly related research line tries to improve the diversity in the top rank translations of an input"
2021.naacl-main.94,E17-1101,0,0.0154703,"hat the translator information has more impact on NMT than the speaker information, which was investigated by Michel and Neubig (2018). 2 Related Work Style itself is a broad concept (Kang and Hovy, 2019). It includes both simple high-level stylistic aspects of language such as verbosity (Marchisio et al., 2019; Agrawal and Carpuat, 2019; Lakew et al., 2019), formality (Niu et al., 2017; Xu et al., 2019), politeness (Mirkin et al., 2015) and complex aspects such as demography (Vanmassenhove et al., 2018; Moryossef et al., 2019; Hovy et al., 2020) and personal traits (Mirkin and Meunier, 2015; Rabinovich et al., 2017; Michel and Neubig, 2018). Our study focuses on capturing the personal style of translators. The closest work to our study is thus the work of Michel and Neubig (2018), where they study instead the effects of using the speaker information in NMT. In our results, we show that the translator information has indeed more impact to NMT than the speaker information. Finally, another distantly related research line tries to improve the diversity in the top rank translations of an input (Li et al., 2016; Shen et al., 2019; Agrawal and Carpuat, 2020). In fact, adding the translator information to NMT"
2021.naacl-main.94,N16-1005,0,0.0221467,"on styles present in the data nor translates a discrete translator token to model and control new data in different and controllable styles. translator-related stylistic variations in translation. In this work, we investigate methods to augNote that using a discrete token is a common apment the state-of-the-art Transformer model proach to model and control not only specific traits with translator information that is available in in translation such as verbosity, politeness and part of the training data. We show that our style-augmented translation models are able to speaker-related variances (Sennrich et al., 2016a; capture the style variations of translators and Michel and Neubig, 2018)) but also other aspects to generate translations with different styles on in NMT such as language ids (Johnson et al., 2017; new data. Indeed, the generated variations difFan et al., 2020). However, our study is the first to fer significantly, up to +4.5 BLEU score differuse such a discrete token to model the style of transence. Despite that, human evaluation confirms lators. It also provides several insights regarding that the translations are of the same quality. translation style modeling as follows. 1 Introduction"
2021.naacl-main.94,P16-1162,0,0.0548548,"on styles present in the data nor translates a discrete translator token to model and control new data in different and controllable styles. translator-related stylistic variations in translation. In this work, we investigate methods to augNote that using a discrete token is a common apment the state-of-the-art Transformer model proach to model and control not only specific traits with translator information that is available in in translation such as verbosity, politeness and part of the training data. We show that our style-augmented translation models are able to speaker-related variances (Sennrich et al., 2016a; capture the style variations of translators and Michel and Neubig, 2018)) but also other aspects to generate translations with different styles on in NMT such as language ids (Johnson et al., 2017; new data. Indeed, the generated variations difFan et al., 2020). However, our study is the first to fer significantly, up to +4.5 BLEU score differuse such a discrete token to model the style of transence. Despite that, human evaluation confirms lators. It also provides several insights regarding that the translations are of the same quality. translation style modeling as follows. 1 Introduction"
C14-2026,aziz-etal-2012-pet,0,0.105813,"Missing"
C14-2026,P11-4010,0,0.0202014,"ly fulfill the requirements illustrated above. Over the years, various annotation tools with different characteristics have been made available for the assessment tasks offered by our toolkit. However, none of them incorporates all the features of MTEQuAl: either the integration in a multi-task platform, or a web-based interface, or the implementation of the error annotation task which is the most needed to support the upcoming research. The most comparable tools to MT-EQuAl are PET (Aziz et al., 2012), COSTA (Chatzitheodorou and Chatzistamatis, 2013), TAUS DQF framework,1 translate5,2 Blast (Stymne, 2011), and Appraise (Federmann, 2012), since they all implement translation error annotation. These tools were created for different purposes and differ in various ways among each other and with respect to MT-EQuAl. All of them except Appraise do not support multiple MT outputs, and PET, COSTA, and Blast are stand-alone tools. From the error analysis point of view, their interfaces show different levels of flexibility. PET and COSTA permit only sentence-level annotation, which is not the suitable granularity for that kind of information. Appraise offers word-level annotation but displays the MT out"
C14-2026,vilar-etal-2006-error,0,0.191191,"Missing"
C14-2026,2012.eamt-1.31,0,\N,Missing
C14-2026,2012.tc-1.5,0,\N,Missing
C14-2028,2013.mtsummit-papers.5,1,0.880457,"Missing"
C14-2028,2013.mtsummit-wptp.13,1,0.900099,"Missing"
C14-2028,2012.amta-papers.22,1,0.865102,"Missing"
C14-2028,2013.mtsummit-wptp.10,0,0.135035,"Missing"
C14-2028,W13-2231,1,0.678754,"Missing"
C14-2028,P14-1067,1,0.806908,"Missing"
C14-2028,2013.mtsummit-wptp.7,1,\N,Missing
C18-1054,D16-1025,1,0.920078,"valuates different implementations of the attention mechanism, the comprehension of what a model can learn and the errors it makes has been drawing much attention of the research community, as This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 641 Proceedings of the 27th International Conference on Computational Linguistics, pages 641–652 Santa Fe, New Mexico, USA, August 20-26, 2018. evidenced by the number of recent publications aiming at comparing the behavior of neural vs. phrasebased systems (Bentivogli et al., 2016; Toral and S´anchez-Cartagena, 2017; Bentivogli et al., 2018). However, understanding the capability of multilingual NMT models in general and zero-shot translation, in particular, has not been thoroughly analyzed yet. By taking the bilingual model as the reference, this work quantitatively analyzes the translation outputs of multilingual and zero-shot models, aiming at answering the following research questions: • How do bilingual, multilingual, and zero-shot systems compare in terms of general translation quality? Is there any translation aspect better modeled by each specific system? • How"
C18-1054,P11-2031,0,0.0243927,"eferences of the official IWSLT test sets. In addition, two TER-based scores are reported, namely the multiple-reference TER (mTER) and a lemma-based TER (lmmTER), which are instead computed on the nine post-edits of the IWSLT 2017 human evaluation set. In mTER, TER is computed by counting, for each segment of the MT output, the minimum number of edits across all the references and dividing by the average length of references. lmmTER is computed similarly to mTER but looking for matches at the lemma level instead of surface forms. Significance tests for all scores are reported using Multeval (Clark et al., 2011) tool. Systems are also compared in terms of three well known and widely used error categories, that is lexical, morphological, and word order errors, exploiting TER and post-edits as follows. First, the MT outputs 2 3 https://github.com/OpenNMT/OpenNMT-py A script from the Moses SMT toolkithttp://www.statmt.org/moses 645 Direction Recurrent System Transformer BLEU TER mTER lmmTER Nl→De NMT M-NMT ZST 18.05 17.79 17.06 64.61 66.18 65.73 23.70 21.75 26.35 20.60 18.28 22.29 Ro→It NMT M-NMT ZST 22.16 21.69 18.72 59.35 59.50 62.08 22.99 21.12 29.66 20.39 18.46 26.15 BLEU TER mTER lmmTER 18.37 19.95"
C18-1054,W17-3203,0,0.0199033,"reprocessing The experimental setting comprises seven languages; for each language pair, we use the ≈200,000 parallel sentences made publicly available by the IWSLT 2017 evaluation campaign (Cettolo et al., 2017), partitioned in training, development, and test sets. In the preprocessing pipeline, the raw data is first tokenized and cleaned by removing empty lines. Then, a shared byte pair encoding (BPE) model (Sennrich et al., 2015) is trained using the union of the source and target sides of the training data. The number of BPE segmentation rules is set to 8, 000, following the suggestion of Denkowski and Neubig (2017) for experiments in small training data condition. For the case of Transformer training, the internal sub-word segmentation (Wu et al., 2016) provided by the Tensor2Tensor library1 is used. Note that prepending the “language-flag” on the source side of the corpus is specific to the multilingual and zero-shot models. 4.2 Evaluation data For our investigation, we exploit the nine post-edits available from the IWSLT 2017 evaluation campaign. Post-editing regarded the bilingual, multilingual, and zero-shot runs of three different participants to the two tasks Dutch (Nl)→German (De) and Romanian (R"
C18-1054,P15-1166,0,0.0797078,"lo et al., 2017), WMT (Bojar et al., 2017)), in the past few years several model variants and training procedures have been proposed and tested in neural machine translation (NMT). NMT models were mostly employed in conventional single language-pair settings, where the training process exploits a parallel corpus from a source language to a target language, and the inference involves only those two languages in the same direction. However, there have also been attempts to incorporate multiple languages in the source (Luong et al., 2015a; Zoph and Knight, 2016; Lee et al., 2016), in the target (Dong et al., 2015), or in both sides like Firat et al. (2016) which combines a shared attention mechanism and multiple encoder-decoder layers. Regardless, the simple approach proposed in Johnson et al. (2016) and Ha et al. (2016) remains outstandingly effective: it relies on single “universal” encoder, decoder and attention modules, and manages multilinguality by introducing an artificial token at the beginning of the input sentence to specify the requested target language. The current NMT state-of-the-art includes the use of recurrent neural networks, initially introduced in Sutskever et al. (2014; Cho et al."
C18-1054,N16-1101,0,0.0607578,"), in the past few years several model variants and training procedures have been proposed and tested in neural machine translation (NMT). NMT models were mostly employed in conventional single language-pair settings, where the training process exploits a parallel corpus from a source language to a target language, and the inference involves only those two languages in the same direction. However, there have also been attempts to incorporate multiple languages in the source (Luong et al., 2015a; Zoph and Knight, 2016; Lee et al., 2016), in the target (Dong et al., 2015), or in both sides like Firat et al. (2016) which combines a shared attention mechanism and multiple encoder-decoder layers. Regardless, the simple approach proposed in Johnson et al. (2016) and Ha et al. (2016) remains outstandingly effective: it relies on single “universal” encoder, decoder and attention modules, and manages multilinguality by introducing an artificial token at the beginning of the input sentence to specify the requested target language. The current NMT state-of-the-art includes the use of recurrent neural networks, initially introduced in Sutskever et al. (2014; Cho et al. (2014), convolutional neural networks, prop"
C18-1054,P17-4012,0,0.0416632,"ing also De↔Ro and Nl↔It data Zero-shot, trained as ZST A but adding En↔Fr/Es data Table 2: The training setting of 4*bilingual, 1*multilingual, and 3*zero-shot systems. 4.3 Training setting Each of the three system types, namely bilingual, multilingual and zero-shot, is trained using both Recurrent and Transformer architectures, with the proper training data provided in the IWSLT 2017 evaluation campaign. Meta training parameters were set in a preliminary stage with the aim of maximizing the quality of each approach. Recurrent NMT experiments are carried out using the open source OpenNMTpy2 (Klein et al., 2017), whereas the Transformer models are trained using the Tensor2Tensor toolkit. Hence, we took the precaution of selecting the optimal training and inference parameters for both approaches and toolkits. For instance, for our low-resource setting characterized by a high data sparsity, the dropout (Srivastava et al., 2014) is set to 0.3 (Gal and Ghahramani, 2016) in Recurrent models and to 0.2 in Transformer models to prevent over-fitting. Similarly, Adam (Kingma and Ba, 2014) optimizer with an initial learning rate of either 0.001 (RNN) or 0.2 (Transformer) is used. If the perplexity does not dec"
C18-1054,W17-3204,0,0.0351809,"setting (§4.3), models (§4.4) and the evaluation methods (§4.5). In Section (§5), we analyze the overall translation quality for related and unrelated language directions. Before the summary and conclusion, we will focus on lexical, morphological and word-order error types for the fine-grained analysis (§6). 2 Related Work Recent trends in NMT evaluation show that post-editing helps to identify and address the weakness of systems (Bentivogli et al., 2018). Furthermore, the use of multiple post-edits in addition to the manual reference is gaining more and more ground (Bentivogli et al., 2016; Koehn and Knowles, 2017; Toral and S´anchez-Cartagena, 2017; Bentivogli et al., 2018). For our investigation, we follow the error analysis approach defined in Bentivogli et al. (2018), where multiple post-edits are exploited in order to quantify morphological, lexical, and word order errors, a simplified error classification with respect to that proposed in Vilar et al. (2006), which settles two additional classes, namely missing and extra words. The first work that compares bilingual, multilingual, and zero-shot systems comes from the IWSLT 2017 evaluation campaign (Cettolo et al., 2017). The authors analyze the ou"
C18-1054,P16-1160,0,0.023201,"evaluation campaigns (IWSLT (Cettolo et al., 2017), WMT (Bojar et al., 2017)), in the past few years several model variants and training procedures have been proposed and tested in neural machine translation (NMT). NMT models were mostly employed in conventional single language-pair settings, where the training process exploits a parallel corpus from a source language to a target language, and the inference involves only those two languages in the same direction. However, there have also been attempts to incorporate multiple languages in the source (Luong et al., 2015a; Zoph and Knight, 2016; Lee et al., 2016), in the target (Dong et al., 2015), or in both sides like Firat et al. (2016) which combines a shared attention mechanism and multiple encoder-decoder layers. Regardless, the simple approach proposed in Johnson et al. (2016) and Ha et al. (2016) remains outstandingly effective: it relies on single “universal” encoder, decoder and attention modules, and manages multilinguality by introducing an artificial token at the beginning of the input sentence to specify the requested target language. The current NMT state-of-the-art includes the use of recurrent neural networks, initially introduced in"
C18-1054,D15-1166,0,0.53415,"As witnessed by recent machine translation evaluation campaigns (IWSLT (Cettolo et al., 2017), WMT (Bojar et al., 2017)), in the past few years several model variants and training procedures have been proposed and tested in neural machine translation (NMT). NMT models were mostly employed in conventional single language-pair settings, where the training process exploits a parallel corpus from a source language to a target language, and the inference involves only those two languages in the same direction. However, there have also been attempts to incorporate multiple languages in the source (Luong et al., 2015a; Zoph and Knight, 2016; Lee et al., 2016), in the target (Dong et al., 2015), or in both sides like Firat et al. (2016) which combines a shared attention mechanism and multiple encoder-decoder layers. Regardless, the simple approach proposed in Johnson et al. (2016) and Ha et al. (2016) remains outstandingly effective: it relies on single “universal” encoder, decoder and attention modules, and manages multilinguality by introducing an artificial token at the beginning of the input sentence to specify the requested target language. The current NMT state-of-the-art includes the use of recurren"
C18-1054,P02-1040,0,0.101105,"T) trained in all directions in the set {En,De,Nl,It,Ro}. Then, we test zero-shot translation (ZST) between related languages, namely Nl→De and Ro→It, by training a multilingual NMT without any data for these language pairs. We also test zero-shot translation between unrelated languages (ZST A), namely Ro→De and Nl→It, by excluding parallel data between these languages. Finally, for the same unrelated zero-shot directions we also train multi-lingual systems (ZST B) that include data related to Romanian and Italian, namely En↔Fr/Es. 4.5 Evaluation methods Systems are compared in terms of BLEU (Papineni et al., 2002) (as implemented in multi-bleu.perl 3 ) and TER (Snover et al., 2006) scores, on the single references of the official IWSLT test sets. In addition, two TER-based scores are reported, namely the multiple-reference TER (mTER) and a lemma-based TER (lmmTER), which are instead computed on the nine post-edits of the IWSLT 2017 human evaluation set. In mTER, TER is computed by counting, for each segment of the MT output, the minimum number of edits across all the references and dividing by the average length of references. lmmTER is computed similarly to mTER but looking for matches at the lemma le"
C18-1054,R13-1079,0,0.0131769,"18.46 26.15 BLEU TER mTER lmmTER 18.37 19.95 ↑ 19.13 63.74 61.90 62.69 27.95 23.62 25.19 23.86 20.05 21.53 22.48 22.12 ↑ 21.29 57.34 57.51 59.08 26.60 25.05 26.93 23.36 21.57 23.33 ↑ ↑ Table 3: Automatic scores on tasks involving related languages. BLEU and TER are computed on test2017, while mTER and lmmTER are reported for human evaluation sets. Best scores of the Transformer model against the Recurrent are highlighted in bold, whereas arrow ↑ indicates statistically significant differences (p &lt; 0.05). and the corresponding post-edits are lemmatized and POS-tagged; for that, we used ParZu (Sennrich et al., 2013) for German and TreeTagger (Schmid, 1994) for Italian. Then, the lemmatized outputs are evaluated against the corresponding post-edits via a variant of the tercom implementation4 of TER: in addition to computing TER, the tool provides complete information about matching lemmas, as well as shift (matches after displacements), insertion, deletion, and substitution operations. Since for each lemma the tool keeps track of the corresponding original word form and POS tag, we are able to measure the number of errors falling in the three error categories, following the scheme described in detail in B"
C18-1054,N18-2074,0,0.0191071,"at are found in the previous approach.In other words, the attention mechanism is repurposed to compute the latent space representation of both the encoder and the decoder sides. However, with the absence of recurrence, positional-encoding is added to the input and output embeddings. Similarly, as the time-step in a recurrent network, the positional information provides the Transformer network with the order of input and output sequences. In our work, we use the absolute positional encoding, but very recently the use of the relative positional information has been shown to improve performance (Shaw et al., 2018). The model is organized as a stack of encoder-decoder networks that works in an auto-regressive way, using the previously generated symbol as input for the next prediction. Both the decoder and encoder can be composed of uniform layers, each built of two sublayers, i.e., a multi-head self-attention layer and a position wise feed-forward network (FFN) layer. The multi-head sub-layer enables the use of multiple attention functions with a similar cost of utilizing attention, while the FFN sub-layer is a fully connected network used to process the attention sublayers; as such, FFN applies two lin"
C18-1054,2006.amta-papers.25,0,0.109971,"zero-shot translation (ZST) between related languages, namely Nl→De and Ro→It, by training a multilingual NMT without any data for these language pairs. We also test zero-shot translation between unrelated languages (ZST A), namely Ro→De and Nl→It, by excluding parallel data between these languages. Finally, for the same unrelated zero-shot directions we also train multi-lingual systems (ZST B) that include data related to Romanian and Italian, namely En↔Fr/Es. 4.5 Evaluation methods Systems are compared in terms of BLEU (Papineni et al., 2002) (as implemented in multi-bleu.perl 3 ) and TER (Snover et al., 2006) scores, on the single references of the official IWSLT test sets. In addition, two TER-based scores are reported, namely the multiple-reference TER (mTER) and a lemma-based TER (lmmTER), which are instead computed on the nine post-edits of the IWSLT 2017 human evaluation set. In mTER, TER is computed by counting, for each segment of the MT output, the minimum number of edits across all the references and dividing by the average length of references. lmmTER is computed similarly to mTER but looking for matches at the lemma level instead of surface forms. Significance tests for all scores are r"
C18-1054,E17-1100,0,0.0382274,"Missing"
C18-1054,vilar-etal-2006-error,0,0.0926287,"ow that post-editing helps to identify and address the weakness of systems (Bentivogli et al., 2018). Furthermore, the use of multiple post-edits in addition to the manual reference is gaining more and more ground (Bentivogli et al., 2016; Koehn and Knowles, 2017; Toral and S´anchez-Cartagena, 2017; Bentivogli et al., 2018). For our investigation, we follow the error analysis approach defined in Bentivogli et al. (2018), where multiple post-edits are exploited in order to quantify morphological, lexical, and word order errors, a simplified error classification with respect to that proposed in Vilar et al. (2006), which settles two additional classes, namely missing and extra words. The first work that compares bilingual, multilingual, and zero-shot systems comes from the IWSLT 2017 evaluation campaign (Cettolo et al., 2017). The authors analyze the outputs of several systems through two human evaluation methods: direct assessment which focuses on the generic assessment of overall translation quality, and post-editing which directly measures the utility of a given MT output to translators. Post-edits are also exploited to run a fine-grained analysis of errors made by the systems. The main findings are"
C18-1054,N16-1004,0,0.0449802,"nt machine translation evaluation campaigns (IWSLT (Cettolo et al., 2017), WMT (Bojar et al., 2017)), in the past few years several model variants and training procedures have been proposed and tested in neural machine translation (NMT). NMT models were mostly employed in conventional single language-pair settings, where the training process exploits a parallel corpus from a source language to a target language, and the inference involves only those two languages in the same direction. However, there have also been attempts to incorporate multiple languages in the source (Luong et al., 2015a; Zoph and Knight, 2016; Lee et al., 2016), in the target (Dong et al., 2015), or in both sides like Firat et al. (2016) which combines a shared attention mechanism and multiple encoder-decoder layers. Regardless, the simple approach proposed in Johnson et al. (2016) and Ha et al. (2016) remains outstandingly effective: it relies on single “universal” encoder, decoder and attention modules, and manages multilinguality by introducing an artificial token at the beginning of the input sentence to specify the requested target language. The current NMT state-of-the-art includes the use of recurrent neural networks, initi"
C18-1054,Q17-1026,0,\N,Missing
C18-1054,W17-4717,0,\N,Missing
C18-1054,P16-1162,0,\N,Missing
D14-1172,W05-0909,0,0.0397333,". Our experiments are carried out on different language pairs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reason is that, as a development and ev"
D14-1172,2011.mtsummit-papers.17,0,0.0134317,"ems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011). The impact of edit operations on post-editors’ productivity, which implicitly connects the severity of different errors to human activity, has also been studied (Temnikova, 2010; O’Brien, 2011; Blain et al., 2011), but few attempts have been made to explicitly model how fine-grained errors impact on human quality judgements and automatic metrics. Recently, the relation between different error types, their frequency, and human quality judgements has been investigated from a descriptive standpoint in (Lommel et al., 2014; Popovi´c et al., 2014). In both works, however, the underlying assumption that the most frequent error has also the largest impact on quality perception is not verified (in general and, least of all, across language pairs, domains, MT systems and post-editors). Another limitation of the"
D14-1172,2010.eamt-1.12,0,0.183772,"Missing"
D14-1172,C14-2026,1,0.831976,"e, Arabic, and Russian. An international organization provided us a set of English sentences together with their translation produced by two anonymous MT systems. For each evaluation item (source sentence and two MT outputs) three experts were asked to assign quality scores to the MT outputs, and a fourth expert was asked to annotate translation errors. The four experts, who were all professional translators native in the examined target languages, were carefully trained to get acquainted with the evaluation guidelines and the annotation tool specifically developed for these evaluation tasks (Girardi et al., 2014). The annotation process was carried out in parallel by all annotators over one week, resulting in a final dataset composed of 312 evaluation items for the ENZH direction, 393 for ENAR, and 437 for ENRU. 4.1 Quality Judgements Quality judgements were collected by asking the three experts to rate each automatic translation according to a 1-5 Likert scale, where 1 means “incomprehensible translation” and 5 means “perfect translation”. The distribution of the collected annotations with respect to each quality score is shown in Figure 1. As we can see, this distribution reflects different levels o"
D14-1172,N09-1057,0,0.0141677,"total error frequencies and automatic scores (Popovi´c and Ney, 2011; Farr´us et al., 2012). Using two different error taxonomies, both works show that the sum of the errors has a high correlation with BLEU and TER scores. Similar to the aforementioned works addressing the impact of MT errors on human perception, these studies disregard error interactions, and their possible impact on automatic scores. To overcome these issues, we propose a robust statistic analysis framework based on mixedeffects models, which have been successfully applied to several NLP problems such as sentiment analysis (Greene and Resnik, 2009), automatic speech recognition (Goldwater et al., 2010), and spoken language translation (Ruiz and Federico, 2014). Despite their effectiveness, the use of mixed-effects models in the MT field is rather recent and limited to the analysis of human posteditions (Green et al., 2013; L¨aubli et al., 2013). In both studies, the goal was to evaluate the impact of post-editing on the quality and productivity of human translation assuming an ANOVA mixed model for a between-subject design, in which human translators either post-edited or translated the same texts. Our scenario is rather different as we"
D14-1172,2013.mtsummit-wptp.10,0,0.0451853,"Missing"
D14-1172,C04-1072,0,0.0133562,"n the plot. The total number of errors amounts to 16,320 characters for ENZH, 4,926 words for ENAR, and 5,965 words for ENRU. This distribution highlights some differences between languages directions. For example, translations into Arabic and Russian present several morphology errors, while word reordering is the most frequent issue for translations into Chinese. As we will see in §5.1, error frequency does not give a direct indication of their impact on traslation quality judgements. 4.3 Automatic Metrics In our investigation we consider three popular automatic metrics: sentence-level BLEU (Lin and Och, 2004), TER (Snover et al., 2006), and GTM (Turian et al., 2003). We compute all automatic scores by relying on a single reference and by 1647 4000 3500 3000 2500 2000 1500 1000 500 0 LEX MISS MORPH REO ENZH ENAR ENRU Figure 3: Distribution of error types. means of standard packages. In particular, automatic scores on Chinese are computed at the character level. Moreover, as we use metrics as response variables for our regression models, we compute all metrics at the sentence level. The overall mean scores for all systems and languages are reported in Table 2. Differences in systems’ performance can"
D14-1172,2014.eamt-1.38,0,0.54091,"Missing"
D14-1172,2001.mtsummit-papers.68,0,0.0130816,"empirical observations are drawn. Our experiments are carried out on different language pairs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reas"
D14-1172,J11-4002,0,0.0770266,"Missing"
D14-1172,2013.mtsummit-posters.5,0,0.0997061,"few simpler methods proposed so far. Overall, our study has clear practical implications for MT systems’ development and evaluation. Indeed, the proposed statistical analysis framework represents an ideal instrument to: i) identify translation issues having the highest impact on human perception of quality and ii) choose the most appropriate evaluation metric to measure progress towards their solution. 2 Related Work Error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, is gaining increasing interest in the MT community (Popovi´c and Ney, 2011; Popovic et al., 2013). Along this direction, the initial efforts to develop error taxonomies covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014) have been recently complemented by investigations on how to exploit error annotations for diagnostic purposes. Error annotations of sentences produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic"
D14-1172,temnikova-2010-cognitive,0,0.0145572,"es produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011). The impact of edit operations on post-editors’ productivity, which implicitly connects the severity of different errors to human activity, has also been studied (Temnikova, 2010; O’Brien, 2011; Blain et al., 2011), but few attempts have been made to explicitly model how fine-grained errors impact on human quality judgements and automatic metrics. Recently, the relation between different error types, their frequency, and human quality judgements has been investigated from a descriptive standpoint in (Lommel et al., 2014; Popovi´c et al., 2014). In both works, however, the underlying assumption that the most frequent error has also the largest impact on quality perception is not verified (in general and, least of all, across language pairs, domains, MT systems and post"
D14-1172,2003.mtsummit-papers.51,0,0.431724,"rs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reason is that, as a development and evaluation criterion, automatic metrics provide a holist"
D14-1172,vilar-etal-2006-error,0,0.851647,"as a development and evaluation criterion, automatic metrics provide a holistic view of systems’ behaviour without identifying the specific issues of a translation. Indeed, the global scores returned by MT evaluation metrics depend on comparisons between translation hypotheses and reference translations, where the causes and the nature of the differences between them are not identified. To cope with these issues and define system improvement priorities, the focus of MT evaluation research is gradually shifting towards profiling systems’ behaviour with respect to various typologies of errors (Vilar et al., 2006; Popovi´c and Ney, 2011; Farr´us et al., 2012, inter alia). This shift has enriched the traditional MT evaluation framework with a new element, that is the actual errors done by a system. Until now, most of the research has focused on the relationship (i.e. the correlation) between two elements of the framework: humans and automatic evaluation metrics. As a new element of the framework, which becomes a sort of “evaluation triangle”, the analysis of error annotations opens interesting research problems related to the relationships between: i) error types and human perception of MT quality and"
D14-1172,W12-3106,0,0.019253,"rror taxonomies covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014) have been recently complemented by investigations on how to exploit error annotations for diagnostic purposes. Error annotations of sentences produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011). The impact of edit operations on post-editors’ productivity, which implicitly connects the severity of different errors to human activity, has also been studied (Temnikova, 2010; O’Brien, 2011; Blain et al., 2011), but few attempts have been made to explicitly model how fine-grained errors impact on human quality judgements and automatic metrics. Recently, the relation between different error types, their frequency, and human quality judgements has been inv"
D14-1172,2014.amta-researchers.20,1,0.775141,"rror taxonomies, both works show that the sum of the errors has a high correlation with BLEU and TER scores. Similar to the aforementioned works addressing the impact of MT errors on human perception, these studies disregard error interactions, and their possible impact on automatic scores. To overcome these issues, we propose a robust statistic analysis framework based on mixedeffects models, which have been successfully applied to several NLP problems such as sentiment analysis (Greene and Resnik, 2009), automatic speech recognition (Goldwater et al., 2010), and spoken language translation (Ruiz and Federico, 2014). Despite their effectiveness, the use of mixed-effects models in the MT field is rather recent and limited to the analysis of human posteditions (Green et al., 2013; L¨aubli et al., 2013). In both studies, the goal was to evaluate the impact of post-editing on the quality and productivity of human translation assuming an ANOVA mixed model for a between-subject design, in which human translators either post-edited or translated the same texts. Our scenario is rather different as we employ mixed models to measure the influence of different MT error types - expressed as continuous fixed effects"
D14-1172,2006.amta-papers.25,0,0.280249,"t on different language pairs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reason is that, as a development and evaluation criterion, automat"
D14-1172,stymne-ahrenberg-2012-practice,0,0.266291,"al instrument to: i) identify translation issues having the highest impact on human perception of quality and ii) choose the most appropriate evaluation metric to measure progress towards their solution. 2 Related Work Error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, is gaining increasing interest in the MT community (Popovi´c and Ney, 2011; Popovic et al., 2013). Along this direction, the initial efforts to develop error taxonomies covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014) have been recently complemented by investigations on how to exploit error annotations for diagnostic purposes. Error annotations of sentences produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011)."
D14-1172,P02-1040,0,\N,Missing
D14-1172,W13-2201,0,\N,Missing
D16-1025,W13-2257,1,0.870008,"Missing"
D16-1025,P11-1059,0,0.0237569,"Missing"
D16-1025,2012.eamt-1.60,1,0.116875,"not applicable to NMT, which does not rely on a fixed inventory of translation units extracted from the parallel data. Previous error analyses based on manually postedited translations were presented in (Bojar, 2011; Koponen, 2012; Popovi´c et al., 2013). We are the first to conduct this kind of study on the output of a neural MT system. 3 Experimental Setting We perform a number of analyses on data and results of the IWSLT 2015 MT En-De task, which consists in translating manual transcripts of English TED talks into German. Evaluation data are publicly available through the WIT3 repository (Cettolo et al., 2012).3 3.1 Task Data TED Talks4 are a collection of rather short speeches (max 18 minutes each, roughly equivalent to 2,500 words) covering a wide variety of topics. All talks have captions, which are translated into many languages by volunteers worldwide. Besides representing a popular benchmark for spoken language technology, TED Talks embed interesting research challenges. Translating TED Talks implies dealing with spoken rather than written language, which is hence expected to be structurally less complex, formal and fluent (Ruiz and Federico, 2014). Moreover, as human translations of the talk"
D16-1025,W14-4012,0,0.0937737,"Missing"
D16-1025,D14-1179,0,0.0584623,"Missing"
D16-1025,daems-etal-2014-origin,0,0.0125712,"aspects are better modeled by different MT frameworks. To this end, a detailed and systematic error analysis of NMT vs. PBMT output is required. Translation error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, has received a fair amount of attention in the MT community. In this work we opt for the automatic detection and classification of translation errors based on manual post-edits of the MT output. We believe this choice provides an optimal trade-off between fully manual error analysis (Farr´us Cabeceran et al., 2010; Popovi´c et al., 2013; Daems et al., 2014; Federico et al., 2014; Neubig et al., 2015), which is very costly and complex, and fully automatic error analysis (Popovi´c and Ney, 2011; Irvine et al., 2013), which is noisy and biased towards one or few arbitrary reference translations. Existing tools for translation error detection are either based on Word Error Rate (WER) and Position-independent word Error Rate (PER) (Popovi´c, 2011) or on output-reference alignment (Zeman et al., 2011). Regarding error classification, Hjerson (Popovi´c, 2011) detects five main types of word-level errors as defined in (Vilar et al., 2006): morphologica"
D16-1025,N13-1073,0,0.0838252,"Missing"
D16-1025,D14-1172,1,0.84939,"modeled by different MT frameworks. To this end, a detailed and systematic error analysis of NMT vs. PBMT output is required. Translation error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, has received a fair amount of attention in the MT community. In this work we opt for the automatic detection and classification of translation errors based on manual post-edits of the MT output. We believe this choice provides an optimal trade-off between fully manual error analysis (Farr´us Cabeceran et al., 2010; Popovi´c et al., 2013; Daems et al., 2014; Federico et al., 2014; Neubig et al., 2015), which is very costly and complex, and fully automatic error analysis (Popovi´c and Ney, 2011; Irvine et al., 2013), which is noisy and biased towards one or few arbitrary reference translations. Existing tools for translation error detection are either based on Word Error Rate (WER) and Position-independent word Error Rate (PER) (Popovi´c, 2011) or on output-reference alignment (Zeman et al., 2011). Regarding error classification, Hjerson (Popovi´c, 2011) detects five main types of word-level errors as defined in (Vilar et al., 2006): morphological, reordering, missing"
D16-1025,fishel-etal-2012-terra,0,0.038989,"Missing"
D16-1025,1994.amta-1.9,0,0.196451,"s vocabulary and the variety of subject matter in a text. For the first two features we did not find any correlation; on the contrary, we found a moderate Pearson correlation (R=0.7332) between TTR and the mTER gains of NMT over its closest competitor in each talk. This result suggests that NMT is able to cope with lexical diversity better than any other considered approach. 5 Analysis of Translation Errors We now turn to analyze which types of linguistic errors characterize NMT vs. PBMT. In the literature, various error taxonomies covering different levels of granularity have been developed (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014). We focus on three error categories, namely (i) morphology errors, (ii) lexical errors, and (iii) word order errors. As for lexical errors, a number of existing taxonomies further distinguish among translation errors due to missing words, extra words, or incorrect lexical choice. However, given the proven difficulty of disambiguating between these three subclasses (Popovi´c and Ney, 2011; Fishel et al., 2012), we prefer to rely on a more coarse-grained linguistic error classification where le"
D16-1025,D12-1043,0,0.0167871,"Missing"
D16-1025,2015.iwslt-evaluation.9,0,0.093503,"Missing"
D16-1025,2015.iwslt-evaluation.4,0,0.120054,"rephrasing and reordering is expected than in the translation of written documents. As regards the English-German language pair, the two languages are interesting since, while belonging to the same language family, they have marked differences in levels of inflection, morphological variation, and word order, especially long-range reordering of verbs. 3.2 Evaluation Data Five systems participated in the MT En-De task and were manually evaluated on a representative subset of the official 2015 test set. The Human Evaluation (HE) set includes the first half of each of the 12 test 3 4 System PBSY (Huck and Birch, 2015) HPB (Jehl et al., 2015) SPB (Ha et al., 2015) NMT (Luong & Manning, 2015) Data 175M/ 3.1B 166M/ 854M 117M/ 2.4B 120M/ – Table 1: MT systems’ overview. Data column: size of parallel/monolingual training data for each system in terms of English and German tokens. talks, for a total of 600 sentences and around 10K words. Five professional translators were asked to post-edit the MT output by applying the minimal edits required to transform it into a fluent sentence with the same meaning as the source sentence. Data were prepared so that all translators equally post-edited the five MT outputs, i.e"
D16-1025,Q13-1035,0,0.0418296,"Missing"
D16-1025,P15-1001,0,0.0782404,"tationally costly and resource demanding to compete with state-of-the-art Phrase-Based MT (PBMT)1 , the situation changed in 2015. For the first time, in the latest edition of IWSLT2 (Cettolo et 1 We use the generic term phrase-based MT to cover standard phrase-based, hierarchical and syntax-based SMT approaches. 2 International Workshop on Spoken Language Translation (http://workshop2015.iwslt.org/) This impressive improvement follows the distance reduction previously observed in the WMT 2015 shared translation task (Bojar et al., 2015). Just few months earlier, the NMT systems described in (Jean et al., 2015b) ranked on par with the best phrase-based models on a couple of language pairs. Such rapid progress stems from the improvement of the recurrent neural network encoderdecoder model, originally proposed in (Sutskever et al., 2014; Cho et al., 2014b), with the use of the attention mechanism (Bahdanau et al., 2015). This evolution has several implications. On one side, NMT represents a simplification with respect to previous paradigms. From a management point of view, similar to PBMT, it allows for a more efficient use of human and data resources with respect to rulebased MT. From the architectu"
D16-1025,W15-3014,0,0.0509369,"tationally costly and resource demanding to compete with state-of-the-art Phrase-Based MT (PBMT)1 , the situation changed in 2015. For the first time, in the latest edition of IWSLT2 (Cettolo et 1 We use the generic term phrase-based MT to cover standard phrase-based, hierarchical and syntax-based SMT approaches. 2 International Workshop on Spoken Language Translation (http://workshop2015.iwslt.org/) This impressive improvement follows the distance reduction previously observed in the WMT 2015 shared translation task (Bojar et al., 2015). Just few months earlier, the NMT systems described in (Jean et al., 2015b) ranked on par with the best phrase-based models on a couple of language pairs. Such rapid progress stems from the improvement of the recurrent neural network encoderdecoder model, originally proposed in (Sutskever et al., 2014; Cho et al., 2014b), with the use of the attention mechanism (Bahdanau et al., 2015). This evolution has several implications. On one side, NMT represents a simplification with respect to previous paradigms. From a management point of view, similar to PBMT, it allows for a more efficient use of human and data resources with respect to rulebased MT. From the architectu"
D16-1025,2015.iwslt-evaluation.6,0,0.129399,"s expected than in the translation of written documents. As regards the English-German language pair, the two languages are interesting since, while belonging to the same language family, they have marked differences in levels of inflection, morphological variation, and word order, especially long-range reordering of verbs. 3.2 Evaluation Data Five systems participated in the MT En-De task and were manually evaluated on a representative subset of the official 2015 test set. The Human Evaluation (HE) set includes the first half of each of the 12 test 3 4 System PBSY (Huck and Birch, 2015) HPB (Jehl et al., 2015) SPB (Ha et al., 2015) NMT (Luong & Manning, 2015) Data 175M/ 3.1B 166M/ 854M 117M/ 2.4B 120M/ – Table 1: MT systems’ overview. Data column: size of parallel/monolingual training data for each system in terms of English and German tokens. talks, for a total of 600 sentences and around 10K words. Five professional translators were asked to post-edit the MT output by applying the minimal edits required to transform it into a fluent sentence with the same meaning as the source sentence. Data were prepared so that all translators equally post-edited the five MT outputs, i.e. 120 sentences for each"
D16-1025,W12-3123,0,0.0131782,"(see also Section 3.4). Irvine et al. (2013) propose another word-level error analysis technique specifically focused on lexical choice and aimed at understanding the effects of domain differences on MT. Their error classification is strictly related to model coverage and insensitive to word order differences. The technique requires access to the system’s phrase table and is thus not applicable to NMT, which does not rely on a fixed inventory of translation units extracted from the parallel data. Previous error analyses based on manually postedited translations were presented in (Bojar, 2011; Koponen, 2012; Popovi´c et al., 2013). We are the first to conduct this kind of study on the output of a neural MT system. 3 Experimental Setting We perform a number of analyses on data and results of the IWSLT 2015 MT En-De task, which consists in translating manual transcripts of English TED talks into German. Evaluation data are publicly available through the WIT3 repository (Cettolo et al., 2012).3 3.1 Task Data TED Talks4 are a collection of rather short speeches (max 18 minutes each, roughly equivalent to 2,500 words) covering a wide variety of topics. All talks have captions, which are translated in"
D16-1025,2014.eamt-1.38,0,0.0817661,"Missing"
D16-1025,2015.iwslt-evaluation.11,0,0.0570707,"e. 120 sentences for each evaluated system. The resulting evaluation data consist of five new reference translations for each of the sentences in the HE set. Each one of these references represents the targeted translation of the system output from which it was derived, but the other four additional translations can also be used to evaluate each MT system. We will see in the next sections how we exploited the available post-edits in the more suitable way depending on the kind of analysis carried out. 3.3 MT Systems Our analysis focuses on the first four top-ranking systems, which include NMT (Luong and Manning, 2015) and three different phrase-based approaches: standard phrase-based (Ha et al., 2015), hierarchical (Jehl et al., 2015) and a combination of phrasebased and syntax-based (Huck and Birch, 2015). Table 1 presents an overview of each system, as well as figures about the training data used.5 The phrase+syntax-based (PBSY) system combines the outputs of a string-to-tree decoder, trained with the GHKM algorithm, with those of two stan5 wit3.fbk.eu http://www.ted.com/ Approach Combination: Phrase+Syntax-based GHKM string-to-tree; hierarchical + sparse lexicalized reordering models Hierarchical Phrase"
D16-1025,D15-1166,0,0.088644,"nvestigate how MT systems’ quality varies with specific characteristics of the input, i.e. sentence length and type of content of each talk (Section 4). Then, we focus on differences among MT systems with respect to morphology, lexical, and word order errors (Section 5). Finally, based on the finding that word reordering is the strongest aspect of NMT compared to the other systems, we carry out a finegrained analysis of word order errors (Section 6). 2 Previous Work To date, NMT systems have only been evaluated by BLEU in single-reference setups (Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015a; G¨ulc¸ehre et al., 2015). Ad258 ditionally, the Montreal NMT system submitted to WMT 2015 (Jean et al., 2015b) was part of a manual evaluation experiment where a large number of non-professional annotators were asked to rank the outputs of multiple MT systems (Bojar et al., 2015). Results for the Montreal system were very positive – ranked first in English-German, third in GermanEnglish, English-Czech and Czech-English – which confirmed and strengthened the BLEU results published so far. Unfortunately neither BLEU nor manual ranking judgements tell us which translation as"
D16-1025,W15-5003,0,0.016732,"frameworks. To this end, a detailed and systematic error analysis of NMT vs. PBMT output is required. Translation error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, has received a fair amount of attention in the MT community. In this work we opt for the automatic detection and classification of translation errors based on manual post-edits of the MT output. We believe this choice provides an optimal trade-off between fully manual error analysis (Farr´us Cabeceran et al., 2010; Popovi´c et al., 2013; Daems et al., 2014; Federico et al., 2014; Neubig et al., 2015), which is very costly and complex, and fully automatic error analysis (Popovi´c and Ney, 2011; Irvine et al., 2013), which is noisy and biased towards one or few arbitrary reference translations. Existing tools for translation error detection are either based on Word Error Rate (WER) and Position-independent word Error Rate (PER) (Popovi´c, 2011) or on output-reference alignment (Zeman et al., 2011). Regarding error classification, Hjerson (Popovi´c, 2011) detects five main types of word-level errors as defined in (Vilar et al., 2006): morphological, reordering, missing words, extra words, an"
D16-1025,J11-4002,0,0.200619,"Missing"
D16-1025,2013.mtsummit-posters.5,0,0.0498368,"Missing"
D16-1025,W14-4009,0,0.0473704,"Missing"
D16-1025,2014.eamt-1.39,1,0.718619,"publicly available through the WIT3 repository (Cettolo et al., 2012).3 3.1 Task Data TED Talks4 are a collection of rather short speeches (max 18 minutes each, roughly equivalent to 2,500 words) covering a wide variety of topics. All talks have captions, which are translated into many languages by volunteers worldwide. Besides representing a popular benchmark for spoken language technology, TED Talks embed interesting research challenges. Translating TED Talks implies dealing with spoken rather than written language, which is hence expected to be structurally less complex, formal and fluent (Ruiz and Federico, 2014). Moreover, as human translations of the talks are required to follow the structure and rhythm of the English captions, a lower amount of rephrasing and reordering is expected than in the translation of written documents. As regards the English-German language pair, the two languages are interesting since, while belonging to the same language family, they have marked differences in levels of inflection, morphological variation, and word order, especially long-range reordering of verbs. 3.2 Evaluation Data Five systems participated in the MT En-De task and were manually evaluated on a represent"
D16-1025,stymne-ahrenberg-2012-practice,0,0.0488173,"rst two features we did not find any correlation; on the contrary, we found a moderate Pearson correlation (R=0.7332) between TTR and the mTER gains of NMT over its closest competitor in each talk. This result suggests that NMT is able to cope with lexical diversity better than any other considered approach. 5 Analysis of Translation Errors We now turn to analyze which types of linguistic errors characterize NMT vs. PBMT. In the literature, various error taxonomies covering different levels of granularity have been developed (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014). We focus on three error categories, namely (i) morphology errors, (ii) lexical errors, and (iii) word order errors. As for lexical errors, a number of existing taxonomies further distinguish among translation errors due to missing words, extra words, or incorrect lexical choice. However, given the proven difficulty of disambiguating between these three subclasses (Popovi´c and Ney, 2011; Fishel et al., 2012), we prefer to rely on a more coarse-grained linguistic error classification where lexical errors include all of them (Farr´us Cabeceran et al., 2010). 6 The type-to"
D16-1025,vilar-etal-2006-error,0,0.431417,"Missing"
D16-1025,2010.iwslt-evaluation.11,0,\N,Missing
D16-1025,W15-3001,0,\N,Missing
D16-1025,R13-1079,0,\N,Missing
D16-1025,2015.iwslt-evaluation.1,1,\N,Missing
D19-5619,P17-4012,0,0.0255823,"aracter RNN to continue the beam search. When the beam search is complete, the most likely character sequence is generated as the best hypothesis. We evaluate decoding architectures using different levels of granularity in the vocabulary units and the attention mechanism, including the standard decoding architecture implemented either with subword (Sennrich et al., 2016) or fully character-level (Cherry et al., 2018) units, which constitute the baseline approaches, and the hierarchical decoding architecture, by implementing all in Pytorch (Paszke et al., 2017) within the OpenNMT-py framework (Klein et al., 2017). In order to evaluate how each generative method performs in languages with different morphological typology, we model the machine translation task from English into five languages from different language families and exhibiting distinct morphological typology: Arabic (templatic), Czech (mostly fusional, partially agglutinative), German (fusional), Italian (fusional) and Turkish (agglutinative). We use the TED Talks corpora (Cettolo, 2012) for training the NMT models, which range from 110K to 240K sentences, and the official development and test sets from IWSLT1 (Cettolo et al., 2017). The lo"
D19-5619,W17-4710,1,0.844901,"t al., 2018) evaluation sets. All models are implemented using gated recurrent units (GRU) (Cho et al., 2014) with the same number of parameters. The hierarchical decoding model implements a 3-layer GRU architecture, which is compared with a character-level decoder which also uses a 3-layer stacked GRU architecture. The subword-level decoder has a 2-layer stacked GRU architecture, to account also for the larger number of embedding parameters. The models using the standard architecture have the attention mechanism after the first GRU layer, and have residual connections after the second layer (Barone et al., 2017). The hierarchical decoder implements the attention mechanism after the second layer and has a residual connection between the first and second layers. The source sides of the data used for training characterlevel NMT models are segmented using BPE with 16,000 merge rules on the IWSLT data, and 32,000 on WMT. For subword-based models we learn shared merging rules for BPE for 16,000 (in IWSLT) and 32,000 (in WMT) units. The models use an embedding and hidden unit size of 512 under low-resource (IWSLT) and 1024 under high-resource (WMT) settings, and are trained using the Adam (Kinga and Ba, 201"
D19-5619,P16-1100,0,0.0429342,"Missing"
D19-5619,D15-1166,0,0.0569219,"ctional recurrent neural network (RNN) predicts the most likely output token yi in the target sequence using an approximate search algorithm based on the previous target token yi´1 , represented with the embedding of the previous token in the target sequence, the previous decoder hidden state, representing the sequence history, and the current attention context in the source sequence, represented by the context vector ct . The latter is a linear combination of the encoder hidden states, whose weights are dynamically computed by a dot product based similarity metric called the attention model (Luong et al., 2015). The probability of generating each target word yi is estimated via a softmax function T ezj ppyi “ zj |x; θq “ řK k“1 3 In this paper, we explore the benefit of integrating a notion of hierarchy into the decoding architecture which could increase the computational efficiency in character-level NMT, following the work of (Luong and Manning, 2016). In this architecture, the input embedding layer of the decoder is augmented with a character-level bi-RNN, which estimates a composition function over the embeddings of the characters in each word in order to compute the distributed representations"
D19-5619,P02-1040,0,0.103618,"egmented using BPE with 16,000 merge rules on the IWSLT data, and 32,000 on WMT. For subword-based models we learn shared merging rules for BPE for 16,000 (in IWSLT) and 32,000 (in WMT) units. The models use an embedding and hidden unit size of 512 under low-resource (IWSLT) and 1024 under high-resource (WMT) settings, and are trained using the Adam (Kinga and Ba, 2015) optimizer with a learning rate of 0.0003 and decay of 0.5, batch size of 100 and a dropout of 0.2. Decoding in all models is performed with a beam size of 5. The accuracy of each output is measured in terms of the BLEU metric (Papineni et al., 2002) and the significance of the improvements are measured using bootstrap hypothesis testing (Clark et al., 2011). 1 The International Workshop on Spoken Language Translation. 2 The Conference on Machine Translation, with shared task organized for news translation. 5 Experiments 190 Variation Paradigm contrast features Positive vs. comparative adjective Present vs. future tense Negation Singular vs. plural noun Present vs. past tense Compound generation Indicative vs. conditional mode Average Agreement features Pronoun vs. Nouns (gender) Pronoun vs. Nouns (number) Pronoun (plural) Pronoun (relati"
D19-5619,W18-6433,0,0.0568282,"Missing"
D19-5619,2012.eamt-1.60,1,0.62811,"baseline approaches, and the hierarchical decoding architecture, by implementing all in Pytorch (Paszke et al., 2017) within the OpenNMT-py framework (Klein et al., 2017). In order to evaluate how each generative method performs in languages with different morphological typology, we model the machine translation task from English into five languages from different language families and exhibiting distinct morphological typology: Arabic (templatic), Czech (mostly fusional, partially agglutinative), German (fusional), Italian (fusional) and Turkish (agglutinative). We use the TED Talks corpora (Cettolo, 2012) for training the NMT models, which range from 110K to 240K sentences, and the official development and test sets from IWSLT1 (Cettolo et al., 2017). The low-resource settings for the training data allows us to examine the quality of the internal representations learned by each decoder under high data sparseness. In order to evaluate how the performance of each method scales with increasing data size, we evaluate the models also by training with a multidomain training data using the public data sets from WMT2 (Bojar et al., 2016) in the English-to-German direction, followed by an analysis on e"
D19-5619,P16-1162,1,0.481276,"Missing"
D19-5619,D18-1461,1,0.843197,"cal distribution are often discarded during translation since they are not found in the vocabulary. The prominent approach to overcome this limitation is to segment words into subword units (Sennrich et al., 2016) and perform translation based on a vocabulary composed of these units. However, subword segmentation methods generally rely on statistical heuristics that lack any linguistic notion. Moreover, they are typically deployed as a pre-processing step before training the NMT model, hence, the predicted set of subword units are essentially not optimized for the translation task. Recently, (Cherry et al., 2018) extended the approach of NMT based on subword units to implement the translation model directly at the level of characters, which could reach comparable performance to the subword-based model, although this would require much larger networks which may be more difficult to train. The major reason to this requirement may lie behind the fact that treating the characters as individual tokens at the same level and processing the input sequences in linear time increases the difficulty of the learning task, where translation would then be modeled as a mapping between the characters in two languages."
D19-5619,W14-4012,0,0.12813,"Missing"
D19-5619,P11-2031,0,0.0325467,"n shared merging rules for BPE for 16,000 (in IWSLT) and 32,000 (in WMT) units. The models use an embedding and hidden unit size of 512 under low-resource (IWSLT) and 1024 under high-resource (WMT) settings, and are trained using the Adam (Kinga and Ba, 2015) optimizer with a learning rate of 0.0003 and decay of 0.5, batch size of 100 and a dropout of 0.2. Decoding in all models is performed with a beam size of 5. The accuracy of each output is measured in terms of the BLEU metric (Papineni et al., 2002) and the significance of the improvements are measured using bootstrap hypothesis testing (Clark et al., 2011). 1 The International Workshop on Spoken Language Translation. 2 The Conference on Machine Translation, with shared task organized for news translation. 5 Experiments 190 Variation Paradigm contrast features Positive vs. comparative adjective Present vs. future tense Negation Singular vs. plural noun Present vs. past tense Compound generation Indicative vs. conditional mode Average Agreement features Pronoun vs. Nouns (gender) Pronoun vs. Nouns (number) Pronoun (plural) Pronoun (relative-gender) Pronoun (relative-number) Positive vs. superlative adjective Simple vs. coordinated verbs (number)"
E06-2002,2005.mtsummit-posters.19,0,0.0175623,"del-4. Starting from the parallel training corpus, provided with direct and inverted alignments, the socalled union alignment (Och and Ney, 2003) is computed. Phrase-pairs are extracted from each sentence pair which correspond to sub-intervals of the source and target positions, J and I, such that the union alignment links all positions of J into I and all positions of I into J. In general, phrases are extracted with maximum length in the source and target defined by the parameters Jmax and Imax . All such phrase-pairs are efficiently computed by an 2 algorithm with complexity O(lImax Jmax ) (Cettolo et al., 2005). Given all phrase-pairs extracted from the training corpus, lexicon probabilities and fertility probabilities are estimated. Target language models (LMs) used by the decoder and rescoring modules are, respectively, estimated from 3-gram and 4-gram statistics by applying the modified Kneser-Ney smoothing method (Goodman and Chen, 1998). LMs are estimated with an in-house software toolkit which also provides a compact binary representation of the LM which is used by the decoder. 3 Demo Architecture Figure 1 shows the two-layer architecture of the demo. At the bottom lie the programs that provid"
E06-2002,W05-0835,0,0.0137666,"interface of the demo is described. 1 http://www.nist.gov/speech/tests/mt/ http://www.slt.atr.jp/IWSLT2004/ 3 http://www.is.cs.cmu.edu/iwslt2005/ 2 Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f ) can be determined through suitable real valued functions (called features) hr (e, f , a), r = 1 . . . R, and takes the parametric form: pλ (e, a |f ) ∝ exp{ R X λr hr (e, f , a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases e˜1 . . . e˜l . For each target phrase e˜ the corresponding source phrase within the source string is identified through three random quantities: the fertility φ, which establishes its length; the permutation πi , which sets its first position; the tablet f˜, which tells its word string. Notice that target phrases might have fertility equal to zero, hence they do not translate any 91 source word. Moreover, uncovered source positions are associated to a special target word (null) according to specific fertility and permutatio"
E06-2002,N03-1017,0,0.00726484,"ion 5 the Web-based interface of the demo is described. 1 http://www.nist.gov/speech/tests/mt/ http://www.slt.atr.jp/IWSLT2004/ 3 http://www.is.cs.cmu.edu/iwslt2005/ 2 Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f ) can be determined through suitable real valued functions (called features) hr (e, f , a), r = 1 . . . R, and takes the parametric form: pλ (e, a |f ) ∝ exp{ R X λr hr (e, f , a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases e˜1 . . . e˜l . For each target phrase e˜ the corresponding source phrase within the source string is identified through three random quantities: the fertility φ, which establishes its length; the permutation πi , which sets its first position; the tablet f˜, which tells its word string. Notice that target phrases might have fertility equal to zero, hence they do not translate any 91 source word. Moreover, uncovered source positions are associated to a special target word (null) according to spe"
E06-2002,J03-1002,0,0.00280669,"of generated theories some approximations are introduced during the search: less promising theories are pruned off (beam search) and a new source position is selected by limiting the number of vacant positions on the left-hand and the distance from the left most vacant position (re-ordering constraints). 2.3 Phrase extraction and model training Training of the phrase-based translation model requires a parallel corpus provided with wordalignments in both directions, i.e. from source to target positions, and viceversa. This preprocessing step can be accomplished by applying the GIZA++ toolkit (Och and Ney, 2003) that provides Viterbi alignments based on IBM Model-4. Starting from the parallel training corpus, provided with direct and inverted alignments, the socalled union alignment (Och and Ney, 2003) is computed. Phrase-pairs are extracted from each sentence pair which correspond to sub-intervals of the source and target positions, J and I, such that the union alignment links all positions of J into I and all positions of I into J. In general, phrases are extracted with maximum length in the source and target defined by the parameters Jmax and Imax . All such phrase-pairs are efficiently computed b"
E06-2002,takezawa-etal-2002-toward,0,0.0141419,"concerned, all the languages are encoded in UTF8: this allows to manage the processing phase in an uniform way and to render graphically different character sets. 4 The supported language-pairs Although there is no theoretical limit to the number of supported language-pairs, the current version of the demo provides translations to English from three source languages: Arabic, Chinese and The Arabic-to-English system has been trained with the data provided by the International Workshop on Spoken Language Translation 2005 The context is that of the Basic Traveling Expression Corpus (BTEC) task (Takezawa et al., 2002). BTEC is a multilingual speech corpus which contains sentences coming from phrase books for tourists. Training set includes 20k sentences containing 159K Arabic and 182K English running words; vocabulary size is 18K for Arabic, 7K for English. Chinese-to-English (Newswire) The Chinese-to-English system has been trained with the data provided by the NIST MT Evaluation Campaign 2005 , large-data condition. In this case parallel data are mainly news-wires provided by news agencies. Training set includes 71M Chinese and 77M English running words; vocabulary size is 157K for Chinese, 214K for Engl"
E06-2002,J96-1002,0,0.0194882,"se-based Statistical Machine Translation system which can be accessed by means of a Web page. Section 2 presents the general log-linear framework to SMT and gives an overview of our phrase-based SMT system. In section 3 the software architecture of the demo is outlined. Section 4 focuses on the currently supported language-pairs: Arabic-to-English, Chinese-toEnglish and Spanish-to-English. In section 5 the Web-based interface of the demo is described. 1 http://www.nist.gov/speech/tests/mt/ http://www.slt.atr.jp/IWSLT2004/ 3 http://www.is.cs.cmu.edu/iwslt2005/ 2 Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f ) can be determined through suitable real valued functions (called features) hr (e, f , a), r = 1 . . . R, and takes the parametric form: pλ (e, a |f ) ∝ exp{ R X λr hr (e, f , a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases e˜1 . . . e˜l . For each target phrase e˜ the corresponding source phrase within the source string is"
E06-2002,J93-2003,0,0.00522389,"and Spanish-to-English. In section 5 the Web-based interface of the demo is described. 1 http://www.nist.gov/speech/tests/mt/ http://www.slt.atr.jp/IWSLT2004/ 3 http://www.is.cs.cmu.edu/iwslt2005/ 2 Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f ) can be determined through suitable real valued functions (called features) hr (e, f , a), r = 1 . . . R, and takes the parametric form: pλ (e, a |f ) ∝ exp{ R X λr hr (e, f , a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases e˜1 . . . e˜l . For each target phrase e˜ the corresponding source phrase within the source string is identified through three random quantities: the fertility φ, which establishes its length; the permutation πi , which sets its first position; the tablet f˜, which tells its word string. Notice that target phrases might have fertility equal to zero, hence they do not translate any 91 source word. Moreover, uncovered source positions are associated to a special targ"
E06-2002,2005.iwslt-1.11,1,\N,Missing
E06-2002,P03-1021,0,\N,Missing
E12-1045,W07-0702,0,0.0465663,"T system weights. For each target language, two standard 5-gram LMs are trained separately on the monolingual TED and NEWS datasets, and log-linearly combined at decoding time. In the Arabic-English task, we use a hierarchical reordering model (Galley and Manning, 2008; Hardmeier et al., 2011), while in the English-French task we use a default word-based bidirectional model. The distortion limit is set to the default value of 6. Note that the use of large n-gram LMs and of lexicalized reordering models was shown to wipe out the improvement achievable by POS-level LM (Kirchhoff and Yang, 2005; Birch et al., 2007). Concerning data preprocessing we apply standard tokenization to the English and French text, while for Arabic we use an in-house tokenizer that removes diacritics and normalizes special characters and digits. Arabic text is then segmented with AMIRA (Diab et al., 2004) according to the ATB scheme7 . The Arabic-English system uses cased 7 The Arabic Treebank tokenization scheme isolates conjunctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article Al+. translation models, while the English-French system uses lowercased models and a standard recasi"
E12-1045,2011.iwslt-evaluation.18,1,0.853422,"2010 tst2010 |S| 90K 7.9M 124K 30.7M 934 1664 |W | 1.7M 220M 2.4M 782M 19K 30K ` 18.9 27.8 19.5 25.4 20.0 18.1 TED UN NEWS TED NEWS dev2010 tst2010 105K 11M 111K 107K 11.6M 934 1664 2.0M 291M 3.1M 2.2M 291M 20K 32K 19.5 26.5 27.6 20.6 25.2 21.5 19.1 Table 6: IWSLT11 training and test data statistics: number of sentences |S|, number of tokens |W |and average sentence length `. Token numbers are computed on the target language, except for the test sets. lel corpora: namely TED and NEWS for ArabicEnglish; TED, NEWS and UN for EnglishFrench. To this end we applied the fill-up method (Nakov, 2008; Bisazza et al., 2011) in which outof-domain phrase tables are merged with the indomain table by adding only new phrase pairs. Out-of-domain phrases are marked with a binary feature whose weight is tuned together with the SMT system weights. For each target language, two standard 5-gram LMs are trained separately on the monolingual TED and NEWS datasets, and log-linearly combined at decoding time. In the Arabic-English task, we use a hierarchical reordering model (Galley and Manning, 2008; Hardmeier et al., 2011), while in the English-French task we use a default word-based bidirectional model. The distortion limit"
E12-1045,J92-4003,0,0.289015,"its to better tailor the LM to the task syntax, by complementing word-based n-grams with a part-of-speech (POS) LM , that can be estimated even on a limited amount of task-specific data. Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011). Our work makes another step in this direction by investigating hybrid LMs that try to explicitly represent the speaking style of the talk genre. As a difference from standard class-based LMs (Brown et al., 1992) or the more recent local LMs (Monz, 2011), which are used to predict sequences of classes or wordclass pairs, our hybrid LM is devised to predict sequences of classes interleaved by words. While we do not claim any technical novelty in the model itself, to our knowledge a deep investigation of hybrid LMs for the sake of style adaptation is definitely new. Finally, the term hybrid LM was inspired by Yazgan and Sarac¸lar (2004), which called with this name a LM predicting sequences of words and sub-words units, devised to let a speech recognizer detect out-of-vocabularywords. 4 Hybrid Language"
E12-1045,2011.mtsummit-papers.1,1,0.747648,"ormalizes special characters and digits. Arabic text is then segmented with AMIRA (Diab et al., 2004) according to the ATB scheme7 . The Arabic-English system uses cased 7 The Arabic Treebank tokenization scheme isolates conjunctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article Al+. translation models, while the English-French system uses lowercased models and a standard recasing post-process. Feature weights are tuned on dev2010 by means of a minimum error training procedure (MERT) (Och, 2003). Following suggestions by Clark et al. (2011) and Cettolo et al. (2011) on controlling optimizer instability, we run MERT four times on the same configuration and use the average of the resulting weights to evaluate translation performance. 5.3 Hybrid LM integration As previously stated, hybrid LMs are trained only on in-domain data and are added to the log-linear decoder as an additional target LM. To this end, we use the class-based LM implementation provided in Moses and IRSTLM, which applies the word-to-class mapping to translation hypotheses before LM querying8 . The order of the additional LM is set to 10 in the Arabic-English evaluation and 7 in the Englis"
E12-1045,P11-2031,0,0.100977,"removes diacritics and normalizes special characters and digits. Arabic text is then segmented with AMIRA (Diab et al., 2004) according to the ATB scheme7 . The Arabic-English system uses cased 7 The Arabic Treebank tokenization scheme isolates conjunctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article Al+. translation models, while the English-French system uses lowercased models and a standard recasing post-process. Feature weights are tuned on dev2010 by means of a minimum error training procedure (MERT) (Och, 2003). Following suggestions by Clark et al. (2011) and Cettolo et al. (2011) on controlling optimizer instability, we run MERT four times on the same configuration and use the average of the resulting weights to evaluate translation performance. 5.3 Hybrid LM integration As previously stated, hybrid LMs are trained only on in-domain data and are added to the log-linear decoder as an additional target LM. To this end, we use the class-based LM implementation provided in Moses and IRSTLM, which applies the word-to-class mapping to translation hypotheses before LM querying8 . The order of the additional LM is set to 10 in the Arabic-English eval"
E12-1045,N04-4038,0,0.01621,"ardmeier et al., 2011), while in the English-French task we use a default word-based bidirectional model. The distortion limit is set to the default value of 6. Note that the use of large n-gram LMs and of lexicalized reordering models was shown to wipe out the improvement achievable by POS-level LM (Kirchhoff and Yang, 2005; Birch et al., 2007). Concerning data preprocessing we apply standard tokenization to the English and French text, while for Arabic we use an in-house tokenizer that removes diacritics and normalizes special characters and digits. Arabic text is then segmented with AMIRA (Diab et al., 2004) according to the ATB scheme7 . The Arabic-English system uses cased 7 The Arabic Treebank tokenization scheme isolates conjunctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article Al+. translation models, while the English-French system uses lowercased models and a standard recasing post-process. Feature weights are tuned on dev2010 by means of a minimum error training procedure (MERT) (Och, 2003). Following suggestions by Clark et al. (2011) and Cettolo et al. (2011) on controlling optimizer instability, we run MERT four times on the same config"
E12-1045,W07-0717,0,0.230189,"nce of one single LM. The progressive adoption of the log-linear modeling framework in many NLP tasks has recently introduced the use of multiple LM components (features), which permit to naturally factor out and integrate different aspects of language into one model. In SMT, the factored model (Koehn and Hoang, 2007), for instance, permits to better tailor the LM to the task syntax, by complementing word-based n-grams with a part-of-speech (POS) LM , that can be estimated even on a limited amount of task-specific data. Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011). Our work makes another step in this direction by investigating hybrid LMs that try to explicitly represent the speaking style of the talk genre. As a difference from standard class-based LMs (Brown et al., 1992) or the more recent local LMs (Monz, 2011), which are used to predict sequences of classes or wordclass pairs, our hybrid LM is devised to predict sequences of classes interleaved by words. While we do not claim any technical novelty in the model itself, to our kn"
E12-1045,D08-1089,0,0.0807084,"namely TED and NEWS for ArabicEnglish; TED, NEWS and UN for EnglishFrench. To this end we applied the fill-up method (Nakov, 2008; Bisazza et al., 2011) in which outof-domain phrase tables are merged with the indomain table by adding only new phrase pairs. Out-of-domain phrases are marked with a binary feature whose weight is tuned together with the SMT system weights. For each target language, two standard 5-gram LMs are trained separately on the monolingual TED and NEWS datasets, and log-linearly combined at decoding time. In the Arabic-English task, we use a hierarchical reordering model (Galley and Manning, 2008; Hardmeier et al., 2011), while in the English-French task we use a default word-based bidirectional model. The distortion limit is set to the default value of 6. Note that the use of large n-gram LMs and of lexicalized reordering models was shown to wipe out the improvement achievable by POS-level LM (Kirchhoff and Yang, 2005; Birch et al., 2007). Concerning data preprocessing we apply standard tokenization to the English and French text, while for Arabic we use an in-house tokenizer that removes diacritics and normalizes special characters and digits. Arabic text is then segmented with AMIR"
E12-1045,W11-2144,1,0.852296,"itics and normalizes special characters and digits. Arabic text is then segmented with AMIRA (Diab et al., 2004) according to the ATB scheme7 . The Arabic-English system uses cased 7 The Arabic Treebank tokenization scheme isolates conjunctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article Al+. translation models, while the English-French system uses lowercased models and a standard recasing post-process. Feature weights are tuned on dev2010 by means of a minimum error training procedure (MERT) (Och, 2003). Following suggestions by Clark et al. (2011) and Cettolo et al. (2011) on controlling optimizer instability, we run MERT four times on the same configuration and use the average of the resulting weights to evaluate translation performance. 5.3 Hybrid LM integration As previously stated, hybrid LMs are trained only on in-domain data and are added to the log-linear decoder as an additional target LM. To this end, we use the class-based LM implementation provided in Moses and IRSTLM, which applies the word-to-class mapping to translation hypotheses before LM querying8 . The order of the additional LM is set to 10 in the Arabic-English eval"
E12-1045,W05-0821,0,0.0197691,"tuned together with the SMT system weights. For each target language, two standard 5-gram LMs are trained separately on the monolingual TED and NEWS datasets, and log-linearly combined at decoding time. In the Arabic-English task, we use a hierarchical reordering model (Galley and Manning, 2008; Hardmeier et al., 2011), while in the English-French task we use a default word-based bidirectional model. The distortion limit is set to the default value of 6. Note that the use of large n-gram LMs and of lexicalized reordering models was shown to wipe out the improvement achievable by POS-level LM (Kirchhoff and Yang, 2005; Birch et al., 2007). Concerning data preprocessing we apply standard tokenization to the English and French text, while for Arabic we use an in-house tokenizer that removes diacritics and normalizes special characters and digits. Arabic text is then segmented with AMIRA (Diab et al., 2004) according to the ATB scheme7 . The Arabic-English system uses cased 7 The Arabic Treebank tokenization scheme isolates conjunctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article Al+. translation models, while the English-French system uses lowercased models"
E12-1045,D07-1091,0,0.0222642,"ontent, and style. Most of the known LM adaption techniques (Bellegarda, 2004), however, address all these variations in a holistic way. A possible reason for this is that LM adaptation methods were originally developed under the automatic speech recognition framework, which typically assumes the presence of one single LM. The progressive adoption of the log-linear modeling framework in many NLP tasks has recently introduced the use of multiple LM components (features), which permit to naturally factor out and integrate different aspects of language into one model. In SMT, the factored model (Koehn and Hoang, 2007), for instance, permits to better tailor the LM to the task syntax, by complementing word-based n-grams with a part-of-speech (POS) LM , that can be estimated even on a limited amount of task-specific data. Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011). Our work makes another step in this direction by investigating hybrid LMs that try to explicitly represent the speaking style of the talk genre. As a difference from standa"
E12-1045,2005.iwslt-1.8,0,0.0609508,"line Our SMT experiments address the translation of TED talks from Arabic to English and from English to French. The training and test datasets were provided by the organizers of the IWSLT11 evaluation, and are summarized in Table 6. Marked in bold are the corpora used for hybrid LM training. Dev and test sets have a single reference translation. For both language pairs, we set up competitive phrase-based systems6 using the Moses toolkit (Koehn et al., 2007). The decoder features a statistical log-linear model including a phrase translation model and a phrase reordering model (Tillmann, 2004; Koehn et al., 2005), two word-based language models, distortion, word and phrase penalties. The translation and reordering models are obtained by combining models independently trained on the available paral6 The SMT systems used in this paper are thoroughly described in (Ruiz et al., 2011). 443 Corpus AR-EN EN AR test EN-FR FR EN test TED UN TED NEWS dev2010 tst2010 |S| 90K 7.9M 124K 30.7M 934 1664 |W | 1.7M 220M 2.4M 782M 19K 30K ` 18.9 27.8 19.5 25.4 20.0 18.1 TED UN NEWS TED NEWS dev2010 tst2010 105K 11M 111K 107K 11.6M 934 1664 2.0M 291M 3.1M 2.2M 291M 20K 32K 19.5 26.5 27.6 20.6 25.2 21.5 19.1 Table 6: IWS"
E12-1045,P07-2045,1,0.0182773,"Missing"
E12-1045,D11-1080,0,0.0936842,"complementing word-based n-grams with a part-of-speech (POS) LM , that can be estimated even on a limited amount of task-specific data. Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011). Our work makes another step in this direction by investigating hybrid LMs that try to explicitly represent the speaking style of the talk genre. As a difference from standard class-based LMs (Brown et al., 1992) or the more recent local LMs (Monz, 2011), which are used to predict sequences of classes or wordclass pairs, our hybrid LM is devised to predict sequences of classes interleaved by words. While we do not claim any technical novelty in the model itself, to our knowledge a deep investigation of hybrid LMs for the sake of style adaptation is definitely new. Finally, the term hybrid LM was inspired by Yazgan and Sarac¸lar (2004), which called with this name a LM predicting sequences of words and sub-words units, devised to let a speech recognizer detect out-of-vocabularywords. 4 Hybrid Language Model Hybrid LMs are n-gram models trained"
E12-1045,E99-1010,0,0.134571,"ect to the style, than its English counterpart. In fact, while the English corpus include transcripts of talks given by English speakers, the French one is mostly a collection of (human) translations. Typical features of the speech style may have been lost in this process. Comparison with baseline. In Table 8 the best performing hybrid LM is compared against the baseline that only includes the standard LMs described in Section 5.2. To complete our evaluation, we also report the effect of an in-domain LM trained on 50 word classes induced from the corpus by maximum-likelihood based clustering (Och, 1999). In the two language pairs, both types of LM result in consistent improvements over the baseline. However, the gains achieved by the hybrid approach are larger and all statistically significant. The hybrid approach is significantly better than the unsupervised one by TER in ArabicEnglish and by BLEU and METEOR in EnglishFrench (these siginificances are not reported in (a) Arabic to English, IWSLT–tst2010 Added InDomain 10g LM none (baseline) unsup. classes hybrid BLEU↑ MET ↑ TER ↓ 26.0 30.4 55.6 26.4◦ 30.8• 55.1◦ • • 26.5 (+.5) 30.8 (+.4) 54.6• (-1.0) (b) English to French, IWSLT–tst2010 Adde"
E12-1045,P03-1021,0,0.0120362,"ic we use an in-house tokenizer that removes diacritics and normalizes special characters and digits. Arabic text is then segmented with AMIRA (Diab et al., 2004) according to the ATB scheme7 . The Arabic-English system uses cased 7 The Arabic Treebank tokenization scheme isolates conjunctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article Al+. translation models, while the English-French system uses lowercased models and a standard recasing post-process. Feature weights are tuned on dev2010 by means of a minimum error training procedure (MERT) (Och, 2003). Following suggestions by Clark et al. (2011) and Cettolo et al. (2011) on controlling optimizer instability, we run MERT four times on the same configuration and use the average of the resulting weights to evaluate translation performance. 5.3 Hybrid LM integration As previously stated, hybrid LMs are trained only on in-domain data and are added to the log-linear decoder as an additional target LM. To this end, we use the class-based LM implementation provided in Moses and IRSTLM, which applies the word-to-class mapping to translation hypotheses before LM querying8 . The order of the additio"
E12-1045,P02-1040,0,0.0851667,"resulting weights to evaluate translation performance. 5.3 Hybrid LM integration As previously stated, hybrid LMs are trained only on in-domain data and are added to the log-linear decoder as an additional target LM. To this end, we use the class-based LM implementation provided in Moses and IRSTLM, which applies the word-to-class mapping to translation hypotheses before LM querying8 . The order of the additional LM is set to 10 in the Arabic-English evaluation and 7 in the English-French, as these appeared to be the best settings in preliminary tests. Translation quality is measured by BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006)9 . To test whether differences among systems are statistically significant we use approximate randomization as done in (Riezler and Maxwell, 2005)10 . Model variants. The effect on MT quality of various hybrid LM variants is shown in Table 7. Note that allPOS and allLemmas refer to deterministically assigned POS tags and lemmas, respectively. Concerning the ratio of POS-mapped tokens, the best performing values are WP =.25 in Arabic-English and WP =.50 in English-French. These hybrid mappings outperform all the uniform represent"
E12-1045,W05-0908,0,0.167854,"rget LM. To this end, we use the class-based LM implementation provided in Moses and IRSTLM, which applies the word-to-class mapping to translation hypotheses before LM querying8 . The order of the additional LM is set to 10 in the Arabic-English evaluation and 7 in the English-French, as these appeared to be the best settings in preliminary tests. Translation quality is measured by BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006)9 . To test whether differences among systems are statistically significant we use approximate randomization as done in (Riezler and Maxwell, 2005)10 . Model variants. The effect on MT quality of various hybrid LM variants is shown in Table 7. Note that allPOS and allLemmas refer to deterministically assigned POS tags and lemmas, respectively. Concerning the ratio of POS-mapped tokens, the best performing values are WP =.25 in Arabic-English and WP =.50 in English-French. These hybrid mappings outperform all the uniform representations (words, lemmas and POS) with statistically significant BLEU and METEOR improvements. The fdf experiment involves the use of document frequency for the selection of common words. Its performance is very clo"
E12-1045,W11-2133,1,0.849634,"uced the use of multiple LM components (features), which permit to naturally factor out and integrate different aspects of language into one model. In SMT, the factored model (Koehn and Hoang, 2007), for instance, permits to better tailor the LM to the task syntax, by complementing word-based n-grams with a part-of-speech (POS) LM , that can be estimated even on a limited amount of task-specific data. Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011). Our work makes another step in this direction by investigating hybrid LMs that try to explicitly represent the speaking style of the talk genre. As a difference from standard class-based LMs (Brown et al., 1992) or the more recent local LMs (Monz, 2011), which are used to predict sequences of classes or wordclass pairs, our hybrid LM is devised to predict sequences of classes interleaved by words. While we do not claim any technical novelty in the model itself, to our knowledge a deep investigation of hybrid LMs for the sake of style adaptation is definitely new. Finally, the term hybrid LM"
E12-1045,2006.amta-papers.25,0,0.0236666,"M integration As previously stated, hybrid LMs are trained only on in-domain data and are added to the log-linear decoder as an additional target LM. To this end, we use the class-based LM implementation provided in Moses and IRSTLM, which applies the word-to-class mapping to translation hypotheses before LM querying8 . The order of the additional LM is set to 10 in the Arabic-English evaluation and 7 in the English-French, as these appeared to be the best settings in preliminary tests. Translation quality is measured by BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006)9 . To test whether differences among systems are statistically significant we use approximate randomization as done in (Riezler and Maxwell, 2005)10 . Model variants. The effect on MT quality of various hybrid LM variants is shown in Table 7. Note that allPOS and allLemmas refer to deterministically assigned POS tags and lemmas, respectively. Concerning the ratio of POS-mapped tokens, the best performing values are WP =.25 in Arabic-English and WP =.50 in English-French. These hybrid mappings outperform all the uniform representations (words, lemmas and POS) with statistically significant BLE"
E12-1045,N04-4026,0,0.109614,"Missing"
E12-1045,W08-0320,0,\N,Missing
E12-1045,W05-0909,0,\N,Missing
E12-1045,2011.iwslt-evaluation.1,1,\N,Missing
E12-1045,2011.iwslt-evaluation.11,0,\N,Missing
E17-2045,Q17-1024,0,0.0617671,"Missing"
E17-2045,W14-3363,0,0.0202907,"-Domain Machine Translation Multi-domain machine translation is very wellstudied in the field of statistical phrase-based MT. The approaches proposed for this issue vary from learning a single model from pooled training data, 280 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 280–284, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics to more complicated (log-)linear interpolations of multiple models using mixture models (Foster and Kuhn, 2007) and linear mixture models (Carpuat et al., 2014). However, being a very new field of research, to the best of our knowledge, there is no work on developing multi-domain NMT systems. However, to the best of our knowledge, there is still no work on developing multi-domain systems (i.e. generic/multi-purpose systems trained with all the data available at a given time) within the stateof-the-art NMT framework. Indeed, though interesting and well motivated from an applicationoriented perspective (e.g. think about a translation company looking for a generic MT backbone usable for jobs coming from any domain), this issue is still unexplored. The c"
E17-2045,N12-1047,0,0.0332362,"ra, after pre-processing. Experimental Setup 3.1 Segments 147.7K 238.4K 689.2K 163.2K 34.5K 38.4K 9.0K 40.3K 2.6M 1.7M 3.2 Phrase-based SMT The experiments of the phrase-based SMT systems are carried out using the open source Moses All these corpora are available in http://opus.lingfil.uu.se 281 4 toolkit (Koehn et al., 2007). The word alignment models are trained using fast-align (Dyer et al., 2013). In our experiments we used 5-gram language models trained with modified Kneser-Ney smoothing using KenLM toolkit (Heafield et al., 2013). The weights of the parameters are tuned with batch MIRA (Cherry and Foster, 2012) to maximize BLEU on the development set. Development set is a combination of all the development corpora of all the domains. 3.3 Table 3 presents the results of the generic systems (PBMT gen. and NMT gen.) and the NMT system adapted to the concatenation of all the eight specific domains (NMT-adp.jnt), as well as the NMT systems which are specifically adapted to each domain separately (NMT-adp.sep). In the case of NMT-adp.jnt and NMT-adp.sep we used the best model of the NMT gen. and adapted it to their corresponding training corpora by continuing the training for several epochs, using the tra"
E17-2045,P07-2045,1,0.014309,"ice (OOffice), PHP, Ubuntu, and translated UN documents (UNTM).1 Since the size of these corpora are relatively small for training robust data-driven MT systems, 1 Tokens 3.1M 1.7M 10.8M 1.0M 389.0K 259.0K 47.7K 913.8K 57.8M 39.6M Table 1: Statistics of the English side of the original corpora, after pre-processing. Experimental Setup 3.1 Segments 147.7K 238.4K 689.2K 163.2K 34.5K 38.4K 9.0K 40.3K 2.6M 1.7M 3.2 Phrase-based SMT The experiments of the phrase-based SMT systems are carried out using the open source Moses All these corpora are available in http://opus.lingfil.uu.se 281 4 toolkit (Koehn et al., 2007). The word alignment models are trained using fast-align (Dyer et al., 2013). In our experiments we used 5-gram language models trained with modified Kneser-Ney smoothing using KenLM toolkit (Heafield et al., 2013). The weights of the parameters are tuned with batch MIRA (Cherry and Foster, 2012) to maximize BLEU on the development set. Development set is a combination of all the development corpora of all the domains. 3.3 Table 3 presents the results of the generic systems (PBMT gen. and NMT gen.) and the NMT system adapted to the concatenation of all the eight specific domains (NMT-adp.jnt),"
E17-2045,N13-1073,0,0.0200347,"ze of these corpora are relatively small for training robust data-driven MT systems, 1 Tokens 3.1M 1.7M 10.8M 1.0M 389.0K 259.0K 47.7K 913.8K 57.8M 39.6M Table 1: Statistics of the English side of the original corpora, after pre-processing. Experimental Setup 3.1 Segments 147.7K 238.4K 689.2K 163.2K 34.5K 38.4K 9.0K 40.3K 2.6M 1.7M 3.2 Phrase-based SMT The experiments of the phrase-based SMT systems are carried out using the open source Moses All these corpora are available in http://opus.lingfil.uu.se 281 4 toolkit (Koehn et al., 2007). The word alignment models are trained using fast-align (Dyer et al., 2013). In our experiments we used 5-gram language models trained with modified Kneser-Ney smoothing using KenLM toolkit (Heafield et al., 2013). The weights of the parameters are tuned with batch MIRA (Cherry and Foster, 2012) to maximize BLEU on the development set. Development set is a combination of all the development corpora of all the domains. 3.3 Table 3 presents the results of the generic systems (PBMT gen. and NMT gen.) and the NMT system adapted to the concatenation of all the eight specific domains (NMT-adp.jnt), as well as the NMT systems which are specifically adapted to each domain se"
E17-2045,2015.iwslt-evaluation.11,0,0.698635,"g them on a generic parallel corpus composed of data from different domains. Our results on multi-domain English-French data show that, in these realistic conditions, PBMT outperforms its neural counterpart. This raises the question: is NMT ready for deployment as a generic/multi-purpose MT backbone in real-world settings? 1 Introduction Neural machine translation systems have recently outperformed their conventional statistical counterparts in the translation tasks in several domains such as news (Sennrich et al., 2016a), UN documents (Junczys-Dowmunt et al., 2016), and spoken language data (Luong and Manning, 2015). One common pattern in all these cases is that the target domain is always predefined, hence it is feasible to perform domain adaptation techniques in order to boost system performance for that particular application. However, in real-world applications it is very hard, if not impossible, to develop and maintain several specific MT systems for multiple domains. This is mostly due to the fact that usually: i) the target domain is not known in advance, and users might query different sen2 Multi-Domain Machine Translation Multi-domain machine translation is very wellstudied in the field of stati"
E17-2045,P16-5005,0,0.0509751,"Missing"
E17-2045,P02-1040,0,0.0980109,"to increase the consistency in segmenting the source and target text, the source and target side of the training set are combined and number of merge rules is set to 89,500, resulting in vocabularies of size 78K and 86K tokens for English and French languages, respectively. We use mini-batches of size 100, word embeddings of size 500, and hidden layers of size 1024. The maximum sentence length is set to 50 in our experiments. The models are trained using Adagrad (Duchi et al., 2011), reshuffling the training corpora for each epoch. The models are evaluated every 10,000 mini-batches via BLEU (Papineni et al., 2002). It is worth mentioning that with the same set-up we recently achieved state-of-theart performance in the International Workshop on Spoken Language Translation evaluation (Farajian et al., 2016). 2 Analysis and Discussion NMT vs. PBMT in Multi-domain scenario As the results show, the generic PBMT system outperforms its NMT counterpart in all the domains by a very large margin; and as the NMT system becomes more specific by observing more domain-specific data, the gap between the performances reduces until the NMT outperforms; which confirms the results of the previous works in this field (Luo"
E17-2045,N16-1101,0,0.0237093,"for jobs coming from any domain), this issue is still unexplored. The current state-of-theart research in NMT explored the effectiveness of domain adaptation, and the approaches for how to adapt existing NMT systems to a new domain (Luong and Manning, 2015). The assumption of these works, however, is that the new target domains are either known in advance or presented together after some sample data have been made available to fine-tune the system. There exist an active field of research that is trying to solve a quite different issue that has a similar motivation, which is multi-lingual NMT (Firat et al., 2016a; Firat et al., 2016b; Johnson et al., 2016). The motivations behind these works are very similar to the ones described in Section 1, which is mostly simplifying the deployment of MT engines in the production lines. So, the final goal is to reduce the number of final systems, trained with pooled multi-domain data sets, without degrading the final performance. As we will see in the remainder of this paper, this issue is still open, especially when we embrace the state-of-the-art NMT paradigm. 3 ECB Gnome JRC KDE4 OOffice PHP Ubuntu UN-TM CommonCrawl Europarl ECB Gnome JRC KDE4 OOffice PHP Ubun"
E17-2045,D16-1026,0,0.0380055,"Missing"
E17-2045,P16-1162,0,0.877463,"f a generic NMT system and phrase-based statistical machine translation (PBMT) system by training them on a generic parallel corpus composed of data from different domains. Our results on multi-domain English-French data show that, in these realistic conditions, PBMT outperforms its neural counterpart. This raises the question: is NMT ready for deployment as a generic/multi-purpose MT backbone in real-world settings? 1 Introduction Neural machine translation systems have recently outperformed their conventional statistical counterparts in the translation tasks in several domains such as news (Sennrich et al., 2016a), UN documents (Junczys-Dowmunt et al., 2016), and spoken language data (Luong and Manning, 2015). One common pattern in all these cases is that the target domain is always predefined, hence it is feasible to perform domain adaptation techniques in order to boost system performance for that particular application. However, in real-world applications it is very hard, if not impossible, to develop and maintain several specific MT systems for multiple domains. This is mostly due to the fact that usually: i) the target domain is not known in advance, and users might query different sen2 Multi-Do"
E17-2045,W07-0717,0,0.0623347,"dvance, and users might query different sen2 Multi-Domain Machine Translation Multi-domain machine translation is very wellstudied in the field of statistical phrase-based MT. The approaches proposed for this issue vary from learning a single model from pooled training data, 280 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 280–284, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics to more complicated (log-)linear interpolations of multiple models using mixture models (Foster and Kuhn, 2007) and linear mixture models (Carpuat et al., 2014). However, being a very new field of research, to the best of our knowledge, there is no work on developing multi-domain NMT systems. However, to the best of our knowledge, there is still no work on developing multi-domain systems (i.e. generic/multi-purpose systems trained with all the data available at a given time) within the stateof-the-art NMT framework. Indeed, though interesting and well motivated from an applicationoriented perspective (e.g. think about a translation company looking for a generic MT backbone usable for jobs coming from a"
federico-etal-2012-iwslt,niessen-etal-2000-evaluation,0,\N,Missing
federico-etal-2012-iwslt,N04-4038,0,\N,Missing
federico-etal-2012-iwslt,P02-1040,0,\N,Missing
federico-etal-2012-iwslt,W07-0734,0,\N,Missing
federico-etal-2012-iwslt,2005.mtsummit-papers.11,0,\N,Missing
federico-etal-2012-iwslt,O07-5005,0,\N,Missing
federico-etal-2012-iwslt,2011.iwslt-evaluation.10,0,\N,Missing
federico-etal-2012-iwslt,I05-3027,0,\N,Missing
federico-etal-2012-iwslt,2011.iwslt-evaluation.1,1,\N,Missing
federico-etal-2012-iwslt,2010.iwslt-evaluation.5,1,\N,Missing
federico-etal-2012-iwslt,2010.iwslt-evaluation.1,1,\N,Missing
J11-2009,1998.amta-tutorials.5,0,0.100052,"Missing"
J16-2001,P06-1067,0,0.21262,"ure 2, the phrase pair [Almlk mHmd AlsAds]-[King Mohamed VI] has monotone orientation whereas [jdd]-[renewed] has discontinuous left orientation with respect to the previously translated phrase. Because of their simple reordering step classification, POM can be conditioned on very finegrained information, such as the whole phrase pair, without suffering too much from data sparseness. However, because POM ignore the distance between consecutively translated phrases, they cannot properly handle long-range reordering phenomena and are typically used with a low distortion limit. Jump models (JM) (Al-Onaizan and Papineni 2006; Green, Galley, and Manning 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Because of data sparseness, JM work best when trained in a discriminative fashion using a variety of binary features (such as the last translated word, its POS tag, and relative position in the sentence) and when length bins are used instead of the exact jump length (Green, Galley, and Manning 2010). A drawback of JM is that they typically over-penalize long jumps because they are more rarely"
J16-2001,W11-2127,0,0.0258221,"Missing"
J16-2001,D14-1132,0,0.031863,"Missing"
J16-2001,J04-4004,0,0.0455168,"Missing"
J16-2001,W09-0434,0,0.0605206,"Missing"
J16-2001,P11-1103,0,0.0647627,"Missing"
J16-2001,D08-1078,0,0.0765434,"Missing"
J16-2001,W10-1735,1,0.842988,"s similarly or better than HSMT for the Arabic-to-English language pair. However, HSMT was shown to better cope with the reordering of VSO sentences (Bisazza 2013). Pre-ordering of Arabic VSO sentences for translation into English has proved to 1 7 Pre-verbal negation can be omitted in colloquial French. 190 Bisazza and Federico A Survey of Word Reordering in Statistical Machine Translation be a particularly difficult task (Green, Sathi, and Manning 2009; Carpuat, Marton, and Habash 2010) and has inspired work on hybrid pre-ordering where multiple verb pre-orderings are fed to a PSMT decoder (Bisazza and Federico 2010; Andreas, Habash, and Rambow 2011); see also Section 2.4. English and Turkish [ Main order: different; CDiff: 5.5; PDiff: 1.5 ] Turkish is a good example of head-final language, except for the fact that it can use both clause-final and clause-initial subordinators.18 As a result, almost all clause-level features are discordant in this pair. At the phrase level, Turkish mainly differs from English for the use of postpositions instead of prepositions. Among our language pairs, this is one of the most difficult to reorder for an SMT system. The complex nature of its reordering phenomena suggests"
J16-2001,P12-1050,1,0.842953,"is adopted by Bisazza and Federico (2010) and to-HFE translation and 2) HFE-to-English postAndreas, Habash, and Rambow (2011): Rules are used to generate multiple likely preordering. butWe achieved significant orderings, only for specific language improvements phenomena that are responsible for difficult (long-range) reordering patterns. The sparse reordering lattices produced by these techfrom baseline (phrase-based, hierarchical phraseniques are then translated by a decoder performing additional based, and string-to-tree) translation methods by phrase-based reordering. In a follow-up work, Bisazza and Federico (2012) introduce another way to encode 1.56, 0.76, and 2.77 points in BLEU, respectively, multiple pre-orderings of the input: Instead of generating a word lattice, pre-computed in the experiment of patent by translation. permutations are represented a modified distortion matrix so that lower distortion costs or “shortcuts” are permitted selectedaspairs The remainder of this paperbetween is organized fol-of input positions. Pre-ordering methods can also be classified by the kind of pre-ordering rules that lows. Section 2 briefly reviews related studies on the they apply: that is, manually written ba"
J16-2001,2012.eamt-1.42,0,0.044209,"Missing"
J16-2001,J90-2002,0,0.21784,"Missing"
J16-2001,J93-2003,0,0.105778,"Missing"
J16-2001,2010.jeptalnrecital-long.30,0,0.0817498,"Missing"
J16-2001,J04-2004,0,0.0240884,"ing model by relative frequency or maximum entropy and using its score as one dense feature function, Cherry (2013) introduces sparse phrase orientation features that are directly added to the model score during decoding (cf. Equation (1)) and optimized jointly with all other SMT feature weights. Effective sparse reordering features can be obtained by simply coupling a phrase pair’s orientation with the first or last word (or word class) of its source and target side (Cherry 2013), or even with the whole phrase pair identity (Auli, Galley, and Gao 2014). 2.2 n-gram Based SMT n-gram based SMT (Casacuberta and Vidal 2004; Mariño et al. 2006) is a string-based alternative to PSMT. In this framework, smoothed n-gram models are learned over sequences of minimal translation units (called tuples), which, like phrase pairs, are pairs of word sequences extracted from word-aligned parallel sentences. Tuples, however, are typically shorter than phrase pairs and are extracted from a unique, monotonic segmentation of the sentence pair. Thus, the problem of spurious phrase segmentation is avoided but non-local reordering becomes an issue. For instance, in Figure 2, a monotonic phrase segmentation could be achieved only b"
J16-2001,2014.iwslt-evaluation.1,1,0.641125,"el, German predominantly places the genitive after the noun, while English displays both orders. Thus, despite belonging to the same family branch (Indo-European/Germanic), this pair displays complex reordering patterns. Indeed, German–English reordering has been widely studied in SMT and is still an open topic. At the Workshop on Statistical Machine Translation 2014 (Bojar et al. 2014), a syntaxbased string-to-tree SMT approach (Williams et al. 2014) won in both language directions (official results excluding online systems). At the International Workshop on Spoken Language Translation 2014 (Cettolo et al. 2014), the best submission was a combination of PSMT with POS- and syntax-based preordering (Slawik et al. 2014), string-to-tree syntax-based SMT, and factored PSMT (Birch et al. 2014). English and French [ Main order: same; CDiff: 0.5; PDiff: 1.5 ] Most clause-level features have the same values in French as in English, except for the negation, which is typically expressed by two words in French: one preceding and one following the verb.17 At the phrase level, differences are found in the location of genitives and adjectives. Thus, English and French have very similar clause-level orders, but reor"
J16-2001,P07-1002,0,0.0223343,"et al. 2009) are added to the treebank data and used to re-train the baseline parser. 2.4.5 Post-ordering. A somewhat smaller line of research has instead treated reordering as post-processing. In Bangalore and Riccardi (2000) and Sudoh et al. (2011), target words are reordered after a monotonic translation process. Other work has focused on rescoring a set of n-best translation candidates produced by a regular PSMT decoder— for instance, by means of POS-based reordering templates (Chen, Cettolo, and Federico 2006) or word-class specific distortion models (Gupta, Cettolo, and Federico 2007). Chang and Toutanova (2007) use a dependency tree reordering model to generate n alternative orders for each 1-best sentence produced by the SMT system. Each set of n sentence reorderings is then reranked using a discriminative model trained on word bigram features and standard word reordering features (i.e., distance or orientation between consecutively translated input words). Focusing on Japanese-to-English translation, Sudoh et al. (2011, 2013) proposed to “translate” foreign-order English into correct-order English using a monolingual phrase-based (Sudoh et al. 2011) or syntax-based (Sudoh et al. 2013) SMT system t"
J16-2001,2006.iwslt-papers.4,1,0.832764,"Missing"
J16-2001,P08-1009,0,0.0145689,"o generate inconsistent constraints leading to decoding dead-ends. Bisazza and Federico (2013a) further develop this idea so that only long reorderings predicted by a specific reordering model are explored by the decoder. This form of early reordering pruning enables the PSMT system to capture long-range reordering without hurting efficiency and is not affected by the constraint inconsistency problem. When available, a parse tree of the input may also be used to constrain PSMT reordering, following the principle of syntactic cohesion (Fox 2002). Concretely, the dependency cohesion constraint (Cherry 2008) states that when part of a source subtree is translated, all words under the same subtree must be covered before moving to words outside of it. Integrated in phrase-based decoding as soft constraints (i. e., by using the number of violations as a feature function), dependency cohesion and its variants (Cherry 2008; Bach, Vogel, and Cherry 2009) were shown to significantly improve translation quality. In related work, Feng, Sun, and Ney (2012) derive similar cohesion constraints from the semantic role labeling structure of the input sentence. The divide-and-translate approach of Sudoh et al. ("
J16-2001,N13-1003,0,0.0606426,"in a generative (gener.) or discriminative (discr.) way. All examples refer to the sentence pair shown in Figure 2. Reordering models References Model type Reordering step classification Features Phrase orientation models (POM): Example: P(orient=discontinuous-left |next-phrase-pair=[jdd]-[renewed]) lexicalized (hierarchical) phrase orientation model Tillmann 2004; Koehn et al. 2005; Nagata et al. 2006; Galley & Manning 2008 gener. monotonic, swap, source/target phrases discontinuous (left or right) phrase orientation maxent classifier Zens & Ney 2006 discr. sparse phrase orientation features Cherry 2013 discr. source/target words or word clusters Jump models (JM): Example: P(jump=−5 |from=AlsAds, to=jdd ) inbound/outbound/pairwise Al-Onaizan & Papineni lexicalized distortion 2006 gener. jump length inbound/outbound length-bin classifier discr. Green et al. 2010 source words jump length based source words, POS, (9 length bins) position; sent. length Source decoding sequence models (SDSM): Example: P(next-word=jdd |prev-translated-words=AlEahil Almlk mHmd AlsAds) reordered source n-gram Feng et al. 2010a gener. source word-after-word Bisazza & Federico 2013; discr. Goto et al. 2013 — source wo"
J16-2001,W12-3125,0,0.0266987,"Missing"
J16-2001,P05-1033,0,0.0495033,"ng treelet pairs and order templates are combined to construct lexicalized translation rules for that sentence and, finally, decoding is performed with a chart parsing algorithm. We will now discuss SMT frameworks that model translation as a process of parallel parsing of the source and target language via a synchronous grammar. 2.3.2 Tree-Based SMT Without Syntax. The idea of extracting bilingual translation (i.e., synchronous) grammars directly from word-aligned parallel data originates in early work on ITG by Wu (1996, 1997). In a more mature approach, hierarchical phrase-based SMT (HSMT) (Chiang 2005), the translation model is a probabilistic synchronous context-free grammar (SCFG) whose rules can correspond to arbitrary (i.e., nonsyntactically motivated) phrases labeled by only two generic non-terminal symbols (X or S). As shown in Figure 5, HSMT translation rules can either include a mix of terminals and non-terminals capturing reordering patterns and discontinuities (rules 1–4), or only terminals (rules 7–10) basically corresponding to phrase pairs in string-based PSMT. Finally, the so-called glue rules (5–6) Hiero& are always added to the grammar to combine translated blocks in a monot"
J16-2001,D08-1024,0,0.0171774,"Missing"
J16-2001,P05-1066,0,0.0630099,"Missing"
J16-2001,W06-1609,0,0.437444,"ntext. Figure 7 (Sudoh et al. 2011) illustrates the workflows of pre- and post-ordering approaches as opposed to standard SMT. 2.4.1 Main Pre-ordering Strategies. A large number of pre-ordering strategies have been proposed. As a first classification, we divide them into deterministic, non-deterministic, and hybrid. Deterministic pre-ordering aims at finding a single optimal permutation of the input sentence, which is then translated monotonically or with a low distortion limit (Nießen and Ney 2001; Xia and McCord 2004; Collins, Koehn, and Kucerova 2005; Popovi´c and Ney 2006; Costa-jussà and Fonollosa 2006; Wang, Collins, and Koehn 2007; 177 d order ng synlies anng techble, we n which s. for this e name ntences ces and et landifferirection nguage, e posthe preologies. st idene postng diford orrdering ed sena SMT and tarmodels ly genes. We ces and rain the “correct ach has op new od pret it can ave pre. ation in g transdirec10) prong rules Computational Linguistics [Source language] (c1) monotone translation in source word order [Source-ordered target] Words : source Word order : source (b1) pre-ordering Volume 42, Number 2 Words : target Word order : source (a) standard translation (c2) post-or"
J16-2001,N04-4038,0,0.00961171,"Missing"
J16-2001,P05-1067,0,0.00680197,"specific categories. For instance, the swap of Alr}ys Alfrnsy may only be dictated by a rule applying to noun phrases composed of a noun and an adjective. On the other hand, string-to-tree methods (Yamada and Knight 2002; Galley et al. 2004; Marcu et al. 2006; Shen, Xu, and Weischedel 2010) use syntax as a way to restrict translation hypotheses to well-formed target language sentences—ruling out, for instance, a translation that fails to reorder the translated verb renewed with respect to its subject. Using syntax on both source and target sides (treeto-tree) (Imamura, Okuma, and Sumita 2005; Ding and Palmer 2005; Smith and Eisner 2006; Watanabe, Tsukada, and Isozaki 2006; Zhang et al. 2008) has proven rather difficult in practice due to the complexity of aligning potentially very different tree topologies and to the large size of the resulting translation grammars. Moreover, the need for highquality parsers in both language sides seriously limits the applicability of this approach. Syntax-based SMT approaches also differ in the formalism they use to represent the trees. Those based on phrase structure (constituency) grammars typically comply with the principle that each translation/reordering rule sh"
J16-2001,2010.amta-papers.22,0,0.209269,"led IBM constraints. A different kind of reordering constraint can be derived from the Inversion Transduction Grammars (ITGs) (Wu 1995, 1997). ITG constraints only admit permutations 167 Computational Linguistics Volume 42, Number 2 that are generated by recursively swapping pairs of adjacent blocks of words.4 In particular, ITG constraints disallow reorderings that generalize the patterns (3 1 4 2) and (2 4 1 3), which are rarely attested in natural languages (Wu 1997).5 Enforcing ITG constraints in left-to-right PSMT decoding requires the use of a shift-reduce permutation parser (Zens 2008; Feng et al. 2010). Alternatively, a relaxed version of the ITG constraints (i.e., Baxter permutations) may be enforced by simply inspecting the set of covered source positions, as proposed by Zens et al. (2004) and Zens (2008). Interestingly, Cherry, Moore, and Quirk (2012) found no consistent benefit from applying either exact or approximate ITG-constraints to a PSMT system that already included a hierarchical phrase orientation model6 (Galley and Manning 2008). The reordering constraints presented so far are not sensitive to the words being translated nor to their context. This results in a very coarse defin"
J16-2001,P13-1032,0,0.0478629,"Missing"
J16-2001,C12-1053,0,0.0507379,"Missing"
J16-2001,C10-2033,0,0.131191,"led IBM constraints. A different kind of reordering constraint can be derived from the Inversion Transduction Grammars (ITGs) (Wu 1995, 1997). ITG constraints only admit permutations 167 Computational Linguistics Volume 42, Number 2 that are generated by recursively swapping pairs of adjacent blocks of words.4 In particular, ITG constraints disallow reorderings that generalize the patterns (3 1 4 2) and (2 4 1 3), which are rarely attested in natural languages (Wu 1997).5 Enforcing ITG constraints in left-to-right PSMT decoding requires the use of a shift-reduce permutation parser (Zens 2008; Feng et al. 2010). Alternatively, a relaxed version of the ITG constraints (i.e., Baxter permutations) may be enforced by simply inspecting the set of covered source positions, as proposed by Zens et al. (2004) and Zens (2008). Interestingly, Cherry, Moore, and Quirk (2012) found no consistent benefit from applying either exact or approximate ITG-constraints to a PSMT system that already included a hierarchical phrase orientation model6 (Galley and Manning 2008). The reordering constraints presented so far are not sensitive to the words being translated nor to their context. This results in a very coarse defin"
J16-2001,D09-1117,0,0.0316319,"Missing"
J16-2001,W02-1039,0,0.619559,"L after that position. Unfortunately, this method appears to generate inconsistent constraints leading to decoding dead-ends. Bisazza and Federico (2013a) further develop this idea so that only long reorderings predicted by a specific reordering model are explored by the decoder. This form of early reordering pruning enables the PSMT system to capture long-range reordering without hurting efficiency and is not affected by the constraint inconsistency problem. When available, a parse tree of the input may also be used to constrain PSMT reordering, following the principle of syntactic cohesion (Fox 2002). Concretely, the dependency cohesion constraint (Cherry 2008) states that when part of a source subtree is translated, all words under the same subtree must be covered before moving to words outside of it. Integrated in phrase-based decoding as soft constraints (i. e., by using the number of violations as a feature function), dependency cohesion and its variants (Cherry 2008; Bach, Vogel, and Cherry 2009) were shown to significantly improve translation quality. In related work, Feng, Sun, and Ney (2012) derive similar cohesion constraints from the semantic role labeling structure of the input"
J16-2001,N04-1035,0,0.0564172,"ing methods (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006) use a given input sentence parse tree to restrict the application of translation/reordering rules to word 8 More pre-ordering techniques will be discussed in Section 2.4. 172 Bisazza and Federico A Survey of Word Reordering in Statistical Machine Translation spans that coincide with syntactic constituents of specific categories. For instance, the swap of Alr}ys Alfrnsy may only be dictated by a rule applying to noun phrases composed of a noun and an adjective. On the other hand, string-to-tree methods (Yamada and Knight 2002; Galley et al. 2004; Marcu et al. 2006; Shen, Xu, and Weischedel 2010) use syntax as a way to restrict translation hypotheses to well-formed target language sentences—ruling out, for instance, a translation that fails to reorder the translated verb renewed with respect to its subject. Using syntax on both source and target sides (treeto-tree) (Imamura, Okuma, and Sumita 2005; Ding and Palmer 2005; Smith and Eisner 2006; Watanabe, Tsukada, and Isozaki 2006; Zhang et al. 2008) has proven rather difficult in practice due to the complexity of aligning potentially very different tree topologies and to the large size"
J16-2001,D08-1089,0,0.215595,"d in natural languages (Wu 1997).5 Enforcing ITG constraints in left-to-right PSMT decoding requires the use of a shift-reduce permutation parser (Zens 2008; Feng et al. 2010). Alternatively, a relaxed version of the ITG constraints (i.e., Baxter permutations) may be enforced by simply inspecting the set of covered source positions, as proposed by Zens et al. (2004) and Zens (2008). Interestingly, Cherry, Moore, and Quirk (2012) found no consistent benefit from applying either exact or approximate ITG-constraints to a PSMT system that already included a hierarchical phrase orientation model6 (Galley and Manning 2008). The reordering constraints presented so far are not sensitive to the words being translated nor to their context. This results in a very coarse definition of the reordering search space, which is problematic in language pairs with different syntactic structures. To address this problem, Yahyaei and Monz (2009) propose decoupling local and global reordering by segmenting the input sentence into chunks that can be permuted arbitrarily, but each of which is translated monotonically. In related work, Yahyaei and Monz (2010) present a technique to dynamically set the DL during decoding: They trai"
J16-2001,D11-1079,0,0.0375711,"Missing"
J16-2001,C10-1043,0,0.0440401,"rds : source Word order : source (b1) pre-ordering Volume 42, Number 2 Words : target Word order : source (a) standard translation (c2) post-ordering [Target language] [Target-ordered source] Words : source (b2) Word order : target monotone translation in target word order Words : target Word order : target Figure 7 Figure 1: A typical work flow in a standard translation, Typical workflows of standard, pre-ordering, and post-ordering approaches to SMT. Taken from pre-ordering and post-ordering approach. Sudoh et al. (2011). Habash 2007; Li et al. 2007; Tromble and Eisner 2009; Xu et al. 2009; Genzel 2010; the opposite direction by placing Japanese syntac-and Fonollosa 2011; Khalilov Isozaki et al. 2010b; Yeniterzi and Oflazer 2010; Khalilov and Sima’an 2011; Visweswariah et al. 2011; Gojun and Fraser 2012; Yang et al. 2012; tic heads in the middle is not a trivial problem. Lerner and Petrov 2013; Jehl et al. 2014).10 Non-deterministic pre-ordering encodes We utilize the Head-Finalization rules to genermultiple alternative reorderings into a word lattice and lets a monotonic (usually ate intermediate head-finalized English sentencesto its models (Zens, Och, and n-gram–based) decoder choose the"
J16-2001,E12-1074,0,0.0220821,"lly and practically challenging problem in SMT. In the early period 10 Li et al. (2007) experiment with a small number of n-best pre-orderings given as alternative inputs to the of SMT SMT studies, reordering is modeled by distancesystem. based constraints in translation model (Brown et al., 1993; Koehn et al., 2003). This reordering model 178 is easy to compute and also works well in relatively similar language pair like French-to-English. Bisazza and Federico A Survey of Word Reordering in Statistical Machine Translation are applied to constituency parse trees. Following a similar approach, Gojun and Fraser (2012) develop a set of rules for the opposite translation direction (English-to-German). Xu et al. (2009) instead propose a simple set of dependency-based rules to pre-order English for translation into subject-object-verb (SOV) languages, which is shown to be effective for Korean, Japanese, Hindi, Urdu, and Turkish. Isozaki et al. (2010b) obtain even better results in an English-to-Japanese task using only one pre-ordering rule (i.e., head finalization) with a parser annotating syntactic heads. 2.4.3 Data-Driven Pre-ordering. This kind of model is learned from sets of pairs (f, f0 ) where f is a s"
J16-2001,P12-2061,0,0.0303757,"Missing"
J16-2001,N10-1129,0,0.0158044,"ous-left |next-phrase-pair=[jdd]-[renewed]) lexicalized (hierarchical) phrase orientation model Tillmann 2004; Koehn et al. 2005; Nagata et al. 2006; Galley & Manning 2008 gener. monotonic, swap, source/target phrases discontinuous (left or right) phrase orientation maxent classifier Zens & Ney 2006 discr. sparse phrase orientation features Cherry 2013 discr. source/target words or word clusters Jump models (JM): Example: P(jump=−5 |from=AlsAds, to=jdd ) inbound/outbound/pairwise Al-Onaizan & Papineni lexicalized distortion 2006 gener. jump length inbound/outbound length-bin classifier discr. Green et al. 2010 source words jump length based source words, POS, (9 length bins) position; sent. length Source decoding sequence models (SDSM): Example: P(next-word=jdd |prev-translated-words=AlEahil Almlk mHmd AlsAds) reordered source n-gram Feng et al. 2010a gener. source word-after-word Bisazza & Federico 2013; discr. Goto et al. 2013 — source words (9-gram context) — source words, POS; source context’s words and POS Operation sequence models (OSM): Example: P(next − operation = generate[jdd,renewed] |prev-operations=generate[AlsAds,VI] jumpBack[1]) translation/reordering operation n-gram Durrani et al."
J16-2001,2009.mtsummit-caasl.4,0,0.0190353,"Missing"
J16-2001,2007.mtsummit-papers.28,1,0.833171,"Missing"
J16-2001,2007.mtsummit-papers.29,0,0.437369,") monotone translation in source word order [Source-ordered target] Words : source Word order : source (b1) pre-ordering Volume 42, Number 2 Words : target Word order : source (a) standard translation (c2) post-ordering [Target language] [Target-ordered source] Words : source (b2) Word order : target monotone translation in target word order Words : target Word order : target Figure 7 Figure 1: A typical work flow in a standard translation, Typical workflows of standard, pre-ordering, and post-ordering approaches to SMT. Taken from pre-ordering and post-ordering approach. Sudoh et al. (2011). Habash 2007; Li et al. 2007; Tromble and Eisner 2009; Xu et al. 2009; Genzel 2010; the opposite direction by placing Japanese syntac-and Fonollosa 2011; Khalilov Isozaki et al. 2010b; Yeniterzi and Oflazer 2010; Khalilov and Sima’an 2011; Visweswariah et al. 2011; Gojun and Fraser 2012; Yang et al. 2012; tic heads in the middle is not a trivial problem. Lerner and Petrov 2013; Jehl et al. 2014).10 Non-deterministic pre-ordering encodes We utilize the Head-Finalization rules to genermultiple alternative reorderings into a word lattice and lets a monotonic (usually ate intermediate head-finalized English s"
J16-2001,W10-1710,1,0.88774,"Missing"
J16-2001,hasan-ney-2008-multi,0,0.0143221,"rch, Blunsom, and Osborne 2009). It is worth noting that translation between Chinese and English has been the main motivation and test bed for the development of HSMT. French and Arabic [ Main order: different; CDiff: 1.5; PDiff: 1 ] At the clause level, this pair differs in main word order (SVO versus VSO or SVO) like the English–Arabic pair, but also in the order of negation and verb. On the other hand, phrase-level order is notably more similar, with only one discordant feature of minor importance (adjective and degree word). Less research was published on this language pair. Nevertheless, Hasan and Ney (2008) and Schwenk and Senellart (2009) chose a PSMT approach to experiment with an Arabic-to-French task. Figure 8 illustrates the reordering characteristics of three language pairs by means of sentence examples that were automatically word-aligned with GIZA++ (Och and Ney 2003) (intersection of direct and inverse alignments). In the first row, we see two English–German sentence pairs; in both cases, most of the points lie close to the diagonal representing an overall monotonic translation, whereas few isolated points denote the very long-range reordering of verbs. Similarly, in the two English–Ara"
J16-2001,D13-1139,0,0.0212233,"The underlying motivation is that, while English-to-Japanese is well handled by pre-ordering with the aforementioned head-finalization rule (Isozaki et al. 2010b), it is much harder to predict the English-like order of Japanese constituents for Japanese-to-English translation. Post-ordering addresses this issue by generating head-final English (HFE) sentences that are used to create a HFE-to-English parallel corpus. Goto, Utiyama, and Sumita (2012, 2013) solve post-ordering by parsing the HFE sentences into binary trees annotated with both syntactic labels and ITG-style monotone/swap labels. Hayashi et al. (2013) improve upon this work with a shiftreduce parser that efficiently integrates non-local features like n-grams of the postordered string. Also related to post-ordering is the work on right-to-left or reverse decoding by Watanabe and Sumita (2002), Finch and Sumita (2009), and Freitag et al. (2013). Here, the target sentence is built up from the last word to the first, thereby altering language model context and reordering search space. Finch and Sumita obtain best results on a wide range of language pairs by combining the outputs of standard and reverse decoding systems. 3. Evaluating Word Reor"
J16-2001,P11-2067,0,0.0289348,"Missing"
J16-2001,2006.amta-papers.8,0,0.0999394,"Missing"
J16-2001,D13-1053,0,0.0262152,"Missing"
J16-2001,W13-2258,0,0.0124887,"on on the whole phrase pair, the classifier uses features extracted from it, such as first and last word (or POS tag) of the source and target side. A similar model was first developed by Xiong, Liu, and Lin (2006) for simpler phrase translation models (i.e., without discontinuities) based on ITG. Li, Liu, and Sun (2013) use recursive autoencoders (Socher et al. 2011) to assign vector representations to the neighboring phrases given as input to the ITG classifier, thereby avoiding manual feature engineering but affecting hypothesis recombination and decoding speed. Nguyen and Vogel (2013) and Huck et al. (2013) successfully integrate the distortion cost feature function and phrase orientation models initially designed for string-based PSMT into a chart-based HSMT decoder. Finally, Setiawan, Kan, and Li (2007) observe that, in languages like Chinese and English, function words provide important clues on the grammatical relationships among phrases. Consequently, they introduce a SCFG where function words (approximated by high-frequency words) are the only lexicalized non-terminals guiding phrase reordering. Based on the same intuition, Setiawan et al. (2009) augment a HSMT system with a function-word"
J16-2001,2005.mtsummit-papers.35,0,0.0266991,"Missing"
J16-2001,D10-1092,0,0.00858649,"Missing"
J16-2001,W10-1736,0,0.498296,"rder : source (a) standard translation (c2) post-ordering [Target language] [Target-ordered source] Words : source (b2) Word order : target monotone translation in target word order Words : target Word order : target Figure 7 Figure 1: A typical work flow in a standard translation, Typical workflows of standard, pre-ordering, and post-ordering approaches to SMT. Taken from pre-ordering and post-ordering approach. Sudoh et al. (2011). Habash 2007; Li et al. 2007; Tromble and Eisner 2009; Xu et al. 2009; Genzel 2010; the opposite direction by placing Japanese syntac-and Fonollosa 2011; Khalilov Isozaki et al. 2010b; Yeniterzi and Oflazer 2010; Khalilov and Sima’an 2011; Visweswariah et al. 2011; Gojun and Fraser 2012; Yang et al. 2012; tic heads in the middle is not a trivial problem. Lerner and Petrov 2013; Jehl et al. 2014).10 Non-deterministic pre-ordering encodes We utilize the Head-Finalization rules to genermultiple alternative reorderings into a word lattice and lets a monotonic (usually ate intermediate head-finalized English sentencesto its models (Zens, Och, and n-gram–based) decoder choose the best path according called Head-Final andMariño decompose Ney 2002; Kanthak etEnglish al. 2005; (HF"
J16-2001,E14-1026,0,0.138313,"Missing"
J16-2001,W05-0831,0,0.0991336,"Missing"
J16-2001,D11-1017,0,0.0134501,"ng model is trained to maximize the likelihood of source trees that can generate such reorderings. Finally, a pre-ordering model is trained to permute each node in the tree. Evaluated on the English–Japanese language pair, this method almost equals the performance of a pre-ordering method based on a supervised parser. Neubig, Watanabe, and Mori (2012) follow a similar approach but build a single ITG-style pre-ordering model treating the parse tree as a 180 Bisazza and Federico A Survey of Word Reordering in Statistical Machine Translation latent variable. In the target self-training method of Katz-Brown et al. (2011), a baseline treebank-trained parser is used to produce n-best parses of a parallel corpus’s source side. Then, the parses resulting in the most accurate pre-ordering after application of a dependency-based pre-ordering rule set (Xu et al. 2009) are added to the treebank data and used to re-train the baseline parser. 2.4.5 Post-ordering. A somewhat smaller line of research has instead treated reordering as post-processing. In Bangalore and Riccardi (2000) and Sudoh et al. (2011), target words are reordered after a monotonic translation process. Other work has focused on rescoring a set of n-be"
J16-2001,I11-1005,0,0.047333,"Missing"
J16-2001,J99-4005,0,0.363405,"e division of reordering patterns into long range, or global, and short range, or local. However, other language pairs display more complex, hierarchical patterns. Word reordering phenomena are naturally handled by human translators1 but are a major source of complexity for SMT. In very general terms, the task of SMT consists of breaking the input sentence into smaller units, selecting an optimal translation for each unit, and placing them in the correct order. Searching for the overall best translation throughout the space of all possible reorderings is, however, computationally intractable (Knight 1999). This crucial fact has motivated an impressive amount of research around two inter-related questions: namely, how to effectively restrict the set of allowed word permutations and how to detect the best permutation among them. Existing solutions to these problems range from heuristic constraints, based on word-to-word distances and completely agnostic about the sentence content, to linguistically motivated SMT frameworks where the entire translation process is guided by syntactic structure. The research in word reordering has advanced together with core SMT research and has sometimes directed"
J16-2001,2005.iwslt-1.8,0,0.065623,"t yet to be incurred (Moore and Quirk 2007). State-of-the-art systems use the distortion cost in combination with more sophisticated reordering models that take into account the identity of the reordered phrases and, optionally, various kinds of contextual information. A representative selection of such models is summarized in Table 1. To ease the presentation, we have divided the models into four groups according to their problem formulation: phrase orientation models, jump models, source decoding sequence models, and operation sequence models. Phrase orientation models (POM) (Tillmann 2004; Koehn et al. 2005; Nagata et al. 2006; Zens and Ney 2006; Li et al. 2014), simply known as lexicalized reordering models, predict whether the next translated source span should be immediately to the right (monotone), immediately to the left (swap), or anywhere else (discontinuous) relatively to the last translated one.7 For example, in Figure 2, the phrase pair [Almlk mHmd AlsAds]-[King Mohamed VI] has monotone orientation whereas [jdd]-[renewed] has discontinuous left orientation with respect to the previously translated phrase. Because of their simple reordering step classification, POM can be conditioned on"
J16-2001,N03-1017,0,0.0194014,"early example of syntax-based pre-ordering, Collins, Koehn, and Kucerova (2005) 2 Related propose a set ofWork six rules aimed at arranging German sentences in English-like order. The rules address the position of verbs, verb particles, and negation particles, and they Reordering is a both theoretically and practically challenging problem in SMT. In the early period 10 Li et al. (2007) experiment with a small number of n-best pre-orderings given as alternative inputs to the of SMT SMT studies, reordering is modeled by distancesystem. based constraints in translation model (Brown et al., 1993; Koehn et al., 2003). This reordering model 178 is easy to compute and also works well in relatively similar language pair like French-to-English. Bisazza and Federico A Survey of Word Reordering in Statistical Machine Translation are applied to constituency parse trees. Following a similar approach, Gojun and Fraser (2012) develop a set of rules for the opposite translation direction (English-to-German). Xu et al. (2009) instead propose a simple set of dependency-based rules to pre-order English for translation into subject-object-verb (SOV) languages, which is shown to be effective for Korean, Japanese, Hindi,"
J16-2001,D13-1049,0,0.156843,"et Word order : target Figure 7 Figure 1: A typical work flow in a standard translation, Typical workflows of standard, pre-ordering, and post-ordering approaches to SMT. Taken from pre-ordering and post-ordering approach. Sudoh et al. (2011). Habash 2007; Li et al. 2007; Tromble and Eisner 2009; Xu et al. 2009; Genzel 2010; the opposite direction by placing Japanese syntac-and Fonollosa 2011; Khalilov Isozaki et al. 2010b; Yeniterzi and Oflazer 2010; Khalilov and Sima’an 2011; Visweswariah et al. 2011; Gojun and Fraser 2012; Yang et al. 2012; tic heads in the middle is not a trivial problem. Lerner and Petrov 2013; Jehl et al. 2014).10 Non-deterministic pre-ordering encodes We utilize the Head-Finalization rules to genermultiple alternative reorderings into a word lattice and lets a monotonic (usually ate intermediate head-finalized English sentencesto its models (Zens, Och, and n-gram–based) decoder choose the best path according called Head-Final andMariño decompose Ney 2002; Kanthak etEnglish al. 2005; (HFE) Crego and 2006; Zhang, Zens, and Ney 2007; Rottmann and Vogel 2007; Crego and Habash 2008; Elming and Habash 2009; Niehues Japanese-to-English translation into 1) Japaneseand Kolss 2009). A hybr"
J16-2001,P07-1091,0,0.426577,"anslation in source word order [Source-ordered target] Words : source Word order : source (b1) pre-ordering Volume 42, Number 2 Words : target Word order : source (a) standard translation (c2) post-ordering [Target language] [Target-ordered source] Words : source (b2) Word order : target monotone translation in target word order Words : target Word order : target Figure 7 Figure 1: A typical work flow in a standard translation, Typical workflows of standard, pre-ordering, and post-ordering approaches to SMT. Taken from pre-ordering and post-ordering approach. Sudoh et al. (2011). Habash 2007; Li et al. 2007; Tromble and Eisner 2009; Xu et al. 2009; Genzel 2010; the opposite direction by placing Japanese syntac-and Fonollosa 2011; Khalilov Isozaki et al. 2010b; Yeniterzi and Oflazer 2010; Khalilov and Sima’an 2011; Visweswariah et al. 2011; Gojun and Fraser 2012; Yang et al. 2012; tic heads in the middle is not a trivial problem. Lerner and Petrov 2013; Jehl et al. 2014).10 Non-deterministic pre-ordering encodes We utilize the Head-Finalization rules to genermultiple alternative reorderings into a word lattice and lets a monotonic (usually ate intermediate head-finalized English sentencesto its m"
J16-2001,W12-3128,0,0.0292487,"Missing"
J16-2001,D13-1054,0,0.00598118,"Missing"
J16-2001,C14-1179,0,0.0394367,"art systems use the distortion cost in combination with more sophisticated reordering models that take into account the identity of the reordered phrases and, optionally, various kinds of contextual information. A representative selection of such models is summarized in Table 1. To ease the presentation, we have divided the models into four groups according to their problem formulation: phrase orientation models, jump models, source decoding sequence models, and operation sequence models. Phrase orientation models (POM) (Tillmann 2004; Koehn et al. 2005; Nagata et al. 2006; Zens and Ney 2006; Li et al. 2014), simply known as lexicalized reordering models, predict whether the next translated source span should be immediately to the right (monotone), immediately to the left (swap), or anywhere else (discontinuous) relatively to the last translated one.7 For example, in Figure 2, the phrase pair [Almlk mHmd AlsAds]-[King Mohamed VI] has monotone orientation whereas [jdd]-[renewed] has discontinuous left orientation with respect to the previously translated phrase. Because of their simple reordering step classification, POM can be conditioned on very finegrained information, such as the whole phrase"
J16-2001,P06-1077,0,0.0297579,"Missing"
J16-2001,W14-4002,0,0.0217398,"Missing"
J16-2001,W06-1606,0,0.0317476,"Knight, and Joshi 2006; Liu, Liu, and Lin 2006) use a given input sentence parse tree to restrict the application of translation/reordering rules to word 8 More pre-ordering techniques will be discussed in Section 2.4. 172 Bisazza and Federico A Survey of Word Reordering in Statistical Machine Translation spans that coincide with syntactic constituents of specific categories. For instance, the swap of Alr}ys Alfrnsy may only be dictated by a rule applying to noun phrases composed of a noun and an adjective. On the other hand, string-to-tree methods (Yamada and Knight 2002; Galley et al. 2004; Marcu et al. 2006; Shen, Xu, and Weischedel 2010) use syntax as a way to restrict translation hypotheses to well-formed target language sentences—ruling out, for instance, a translation that fails to reorder the translated verb renewed with respect to its subject. Using syntax on both source and target sides (treeto-tree) (Imamura, Okuma, and Sumita 2005; Ding and Palmer 2005; Smith and Eisner 2006; Watanabe, Tsukada, and Isozaki 2006; Zhang et al. 2008) has proven rather difficult in practice due to the complexity of aligning potentially very different tree topologies and to the large size of the resulting tr"
J16-2001,J06-4004,0,0.0856055,"Missing"
J16-2001,P08-1114,0,0.0093755,"he translation model is fully based on the syntactic parse tree of the source or target sentence (Section 2.3.1) or where syntax is not used at all (Section 2.3.2). A third line of work bridges between these two by exploiting syntactic information in the form of soft constraints while operating with a synchronous translation grammar extracted from non-parsed parallel data. Chiang (2005) first experimented with a feature function rewarding translation rules applied to full syntactic constituents (constituent feature). Although this initial attempt did not appear to improve translation quality, Marton and Resnik (2008) further elaborated the idea and proposed a series of finer-grained features distinguishing among 9 Two other models utilizing function words as the anchors of global reordering decisions are proposed in Setiawan et al. (2013) and Setiawan, Zhou, and Xiang (2013). Although integrated in a syntax-based system (Shen, Xu, and Weischedel 2010), these models are in principle applicable to other SMT frameworks such as HSMT. 176 Bisazza and Federico A Survey of Word Reordering in Statistical Machine Translation constituent types (VP, NP, etc.), eventually leading to better performance. Gao, Koehn, an"
J16-2001,W07-0701,0,0.0161184,"t pair (3) determines the swapping of jdd and AlEAhl but does not specify the ordering of dEm, which is also a child of jdd. Hence, during decoding, all possible reorderings of the unmatched children are considered and scored by a separate discriminative model, predicting the position of a child node (or modifier m) relative to its head h, given lexical, POS, and positional features of m and h. Reordering modeling is thus largely decoupled from lexical selection, which makes the model very flexible but results in a very large search space and high risk of search errors. To address this issue, Menezes and Quirk (2007) introduce another mechanism to complement treelet reordering: namely, dependency order templates. An order template is an unlexicalized rule specifying the reordering of a node and all its children based on their POS tags. For instance, in Figure 4, treelet pair (3) may be combined with template (a) to specify the order of the child dEm. For each new test sentence, matching treelet pairs and order templates are combined to construct lexicalized translation rules for that sentence and, finally, decoding is performed with a chart parsing algorithm. We will now discuss SMT frameworks that model"
J16-2001,2007.mtsummit-papers.43,0,0.0137911,"word or phrase that was previously translated. The simplest example of reordering feature function is the distortion cost or distortion penalty jump(Ji−1 , Ji ), which by convention assigns zero cost to hypotheses that preserve the order of the source phrases (monotonic translations). During decoding, the basic implementation of distortion cost penalizes long jumps only when they are performed, leading to the proliferation of hypotheses with gaps (i.e., uncovered input positions). This issue can be addressed by incorporating into the distortion cost an estimate of the cost yet to be incurred (Moore and Quirk 2007). State-of-the-art systems use the distortion cost in combination with more sophisticated reordering models that take into account the identity of the reordered phrases and, optionally, various kinds of contextual information. A representative selection of such models is summarized in Table 1. To ease the presentation, we have divided the models into four groups according to their problem formulation: phrase orientation models, jump models, source decoding sequence models, and operation sequence models. Phrase orientation models (POM) (Tillmann 2004; Koehn et al. 2005; Nagata et al. 2006; Zens"
J16-2001,W10-2915,0,0.0569807,"Missing"
J16-2001,P11-1065,0,0.0412683,"Missing"
J16-2001,P06-1090,0,0.217647,"d (Moore and Quirk 2007). State-of-the-art systems use the distortion cost in combination with more sophisticated reordering models that take into account the identity of the reordered phrases and, optionally, various kinds of contextual information. A representative selection of such models is summarized in Table 1. To ease the presentation, we have divided the models into four groups according to their problem formulation: phrase orientation models, jump models, source decoding sequence models, and operation sequence models. Phrase orientation models (POM) (Tillmann 2004; Koehn et al. 2005; Nagata et al. 2006; Zens and Ney 2006; Li et al. 2014), simply known as lexicalized reordering models, predict whether the next translated source span should be immediately to the right (monotone), immediately to the left (swap), or anywhere else (discontinuous) relatively to the last translated one.7 For example, in Figure 2, the phrase pair [Almlk mHmd AlsAds]-[King Mohamed VI] has monotone orientation whereas [jdd]-[renewed] has discontinuous left orientation with respect to the previously translated phrase. Because of their simple reordering step classification, POM can be conditioned on very finegrained in"
J16-2001,D12-1077,0,0.030601,"Missing"
J16-2001,P13-1156,0,0.0126456,"than conditioning the decision on the whole phrase pair, the classifier uses features extracted from it, such as first and last word (or POS tag) of the source and target side. A similar model was first developed by Xiong, Liu, and Lin (2006) for simpler phrase translation models (i.e., without discontinuities) based on ITG. Li, Liu, and Sun (2013) use recursive autoencoders (Socher et al. 2011) to assign vector representations to the neighboring phrases given as input to the ITG classifier, thereby avoiding manual feature engineering but affecting hypothesis recombination and decoding speed. Nguyen and Vogel (2013) and Huck et al. (2013) successfully integrate the distortion cost feature function and phrase orientation models initially designed for string-based PSMT into a chart-based HSMT decoder. Finally, Setiawan, Kan, and Li (2007) observe that, in languages like Chinese and English, function words provide important clues on the grammatical relationships among phrases. Consequently, they introduce a SCFG where function words (approximated by high-frequency words) are the only lexicalized non-terminals guiding phrase reordering. Based on the same intuition, Setiawan et al. (2009) augment a HSMT syste"
J16-2001,W09-0435,0,0.0240666,"rce sentence and f0 is its reference permutation (pre-ordering) inferred from a reference translation e via a word-level alignment.11 These approaches typically require some form of linguistic annotation of the source language, such as syntactic parse trees (Xia and McCord 2004; Habash 2007; Li et al. 2007; Elming and Habash 2009; Genzel 2010; Khalilov and Fonollosa 2011; Khalilov and Sima’an 2011; Yang et al. 2012; Lerner and Petrov 2013; Jehl et al. 2014), shallow syntax chunks (Zhang, Zens, and Ney 2007; Crego and Habash 2008), or POS labels (Crego and Mariño 2006; Rottmann and Vogel 2007; Niehues and Kolss 2009; Tromble and Eisner 2009; Visweswariah et al. 2011). Among the first examples of data-driven tree-based pre-ordering, Xia and McCord (2004) propose a method to automatically learn reordering patterns from a dependencyparsed French–English bitext, using a number of heuristics. While source-side parses are required by their method, target-side parses are optionally used to provide additional constraints during rule extraction. Habash (2007) extracts pre-ordering rules from an Arabic–English parallel corpus dependency-parsed on the source side. In both these works, pre-ordering rules are applied"
J16-2001,2001.mtsummit-papers.45,0,0.100136,"i.e., pre-ordering or post-ordering) in a monolingual fashion and with unconstrained access to the whole sentence context. Figure 7 (Sudoh et al. 2011) illustrates the workflows of pre- and post-ordering approaches as opposed to standard SMT. 2.4.1 Main Pre-ordering Strategies. A large number of pre-ordering strategies have been proposed. As a first classification, we divide them into deterministic, non-deterministic, and hybrid. Deterministic pre-ordering aims at finding a single optimal permutation of the input sentence, which is then translated monotonically or with a low distortion limit (Nießen and Ney 2001; Xia and McCord 2004; Collins, Koehn, and Kucerova 2005; Popovi´c and Ney 2006; Costa-jussà and Fonollosa 2006; Wang, Collins, and Koehn 2007; 177 d order ng synlies anng techble, we n which s. for this e name ntences ces and et landifferirection nguage, e posthe preologies. st idene postng diford orrdering ed sena SMT and tarmodels ly genes. We ces and rain the “correct ach has op new od pret it can ave pre. ation in g transdirec10) prong rules Computational Linguistics [Source language] (c1) monotone translation in source word order [Source-ordered target] Words : source Word order : source"
J16-2001,nivre-etal-2006-maltparser,0,0.0158681,"Missing"
J16-2001,E99-1010,0,0.135134,"med to be locally decomposable to allow for efficient decoding via dynamic programming. Feature weights are tuned discriminatively by directly optimizing translation quality3 on a development set, using parameter tuning techniques such as MERT (Och 2003), MIRA (Chiang, Marton, and Resnik 2008), or PRO (Hopkins and May 2011). 2.1 Phrase-Based SMT Phrase-based SMT (PSMT) is the currently dominant approach in string-based SMT. PSMT ruled out the early word-based SMT framework (Brown et al. 1990, 1993; Berger et al. 1996) thanks to two important novelties: the use of multi-word translation units (Och 1999; Zens, Och, and Ney 2002; Koehn, Och, and Marcu 2003), and the move from a generative to a discriminative modeling framework (Och and Ney 2002). The search process (1) in PSMT is guided by the target string e, built from left to right, and the alignment variable b that embeds both segmentation and reordering of the source phrases. This is defined as b = bI1 = ((J1 , K1 ), (J2 , K2 ), . . . , (JI , KI )) (2) such that K1 , . . . , KI are consecutive intervals partitioning the target word positions, and J1 , . . . , JI are corresponding but not necessarily consecutive intervals partitioning the"
J16-2001,P03-1021,0,0.0651104,"cal mapping (alignment) between f and e, hr (e, f, b) are R arbitrary feature functions and λr the corresponding feature weights. Feature functions try to capture relevant translation adequacy and word reordering aspects from aligned parallel data, as well as translation fluency aspects from monolingual target texts. Moreover, feature functions are assumed to be locally decomposable to allow for efficient decoding via dynamic programming. Feature weights are tuned discriminatively by directly optimizing translation quality3 on a development set, using parameter tuning techniques such as MERT (Och 2003), MIRA (Chiang, Marton, and Resnik 2008), or PRO (Hopkins and May 2011). 2.1 Phrase-Based SMT Phrase-based SMT (PSMT) is the currently dominant approach in string-based SMT. PSMT ruled out the early word-based SMT framework (Brown et al. 1990, 1993; Berger et al. 1996) thanks to two important novelties: the use of multi-word translation units (Och 1999; Zens, Och, and Ney 2002; Koehn, Och, and Marcu 2003), and the move from a generative to a discriminative modeling framework (Och and Ney 2002). The search process (1) in PSMT is guided by the target string e, built from left to right, and the a"
J16-2001,P02-1038,0,0.0466997,"irectly optimizing translation quality3 on a development set, using parameter tuning techniques such as MERT (Och 2003), MIRA (Chiang, Marton, and Resnik 2008), or PRO (Hopkins and May 2011). 2.1 Phrase-Based SMT Phrase-based SMT (PSMT) is the currently dominant approach in string-based SMT. PSMT ruled out the early word-based SMT framework (Brown et al. 1990, 1993; Berger et al. 1996) thanks to two important novelties: the use of multi-word translation units (Och 1999; Zens, Och, and Ney 2002; Koehn, Och, and Marcu 2003), and the move from a generative to a discriminative modeling framework (Och and Ney 2002). The search process (1) in PSMT is guided by the target string e, built from left to right, and the alignment variable b that embeds both segmentation and reordering of the source phrases. This is defined as b = bI1 = ((J1 , K1 ), (J2 , K2 ), . . . , (JI , KI )) (2) such that K1 , . . . , KI are consecutive intervals partitioning the target word positions, and J1 , . . . , JI are corresponding but not necessarily consecutive intervals partitioning the source word positions. A phrase segmentation for our running example is shown in Figure 2. The use of phrases mainly results in a better handli"
J16-2001,J03-1002,0,0.0111105,"ain word order (SVO versus VSO or SVO) like the English–Arabic pair, but also in the order of negation and verb. On the other hand, phrase-level order is notably more similar, with only one discordant feature of minor importance (adjective and degree word). Less research was published on this language pair. Nevertheless, Hasan and Ney (2008) and Schwenk and Senellart (2009) chose a PSMT approach to experiment with an Arabic-to-French task. Figure 8 illustrates the reordering characteristics of three language pairs by means of sentence examples that were automatically word-aligned with GIZA++ (Och and Ney 2003) (intersection of direct and inverse alignments). In the first row, we see two English–German sentence pairs; in both cases, most of the points lie close to the diagonal representing an overall monotonic translation, whereas few isolated points denote the very long-range reordering of verbs. Similarly, in the two English–Arabic sentence pairs, we mostly observe local reorderings, with the exception of few isolated points corresponding to the Arabic clause-initial verbs. Finally, the two Turkish–English examples display global reordering, due to the high number of clause-level order differences"
J16-2001,2001.mtsummit-papers.68,0,0.024586,"ta-jussà and Fonollosa (2006), except that here the monolingual SMT process is applied to the target language after a monotonic translation phase. 181 Computational Linguistics Volume 42, Number 2 feature weights on a development corpus—for instance, by means of minimum error rate training procedures (Och 2003). The design of MT evaluation metrics correlating with human judgments is an active research area. Here we briefly survey two widely used general-purpose metrics, BLEU and METEOR, and then describe in more detail a number of reordering-specific metrics. 3.1 General-Purpose Metrics BLEU (Papineni et al. 2001) is a lexical match–based score that represents the de facto standard for SMT evaluation. Here, proximity between candidate and reference translations is measured in terms of overlapping word n-grams, with n typically ranging from 1 to 4. For each order n a modified precision score (see Papineni et al. [2001] for details) is computed on the whole test set and combined in a geometric mean. The resulting score is then multiplied by a brevity penalty that accounts for length mismatches between reference and candidate translations. Al-Onaizan and Papineni (2006) use BLEU to measure word order simi"
J16-2001,popovic-ney-2006-pos,0,0.0958148,"Missing"
J16-2001,P05-1034,0,0.070858,"Missing"
J16-2001,2007.tmi-papers.21,0,0.380885,"ma’an 2011; Visweswariah et al. 2011; Gojun and Fraser 2012; Yang et al. 2012; tic heads in the middle is not a trivial problem. Lerner and Petrov 2013; Jehl et al. 2014).10 Non-deterministic pre-ordering encodes We utilize the Head-Finalization rules to genermultiple alternative reorderings into a word lattice and lets a monotonic (usually ate intermediate head-finalized English sentencesto its models (Zens, Och, and n-gram–based) decoder choose the best path according called Head-Final andMariño decompose Ney 2002; Kanthak etEnglish al. 2005; (HFE) Crego and 2006; Zhang, Zens, and Ney 2007; Rottmann and Vogel 2007; Crego and Habash 2008; Elming and Habash 2009; Niehues Japanese-to-English translation into 1) Japaneseand Kolss 2009). A hybrid approach is adopted by Bisazza and Federico (2010) and to-HFE translation and 2) HFE-to-English postAndreas, Habash, and Rambow (2011): Rules are used to generate multiple likely preordering. butWe achieved significant orderings, only for specific language improvements phenomena that are responsible for difficult (long-range) reordering patterns. The sparse reordering lattices produced by these techfrom baseline (phrase-based, hierarchical phraseniques are then tra"
J16-2001,2012.iwslt-evaluation.6,0,0.0495299,"Missing"
J16-2001,2009.mtsummit-posters.17,0,0.0151225,"2009). It is worth noting that translation between Chinese and English has been the main motivation and test bed for the development of HSMT. French and Arabic [ Main order: different; CDiff: 1.5; PDiff: 1 ] At the clause level, this pair differs in main word order (SVO versus VSO or SVO) like the English–Arabic pair, but also in the order of negation and verb. On the other hand, phrase-level order is notably more similar, with only one discordant feature of minor importance (adjective and degree word). Less research was published on this language pair. Nevertheless, Hasan and Ney (2008) and Schwenk and Senellart (2009) chose a PSMT approach to experiment with an Arabic-to-French task. Figure 8 illustrates the reordering characteristics of three language pairs by means of sentence examples that were automatically word-aligned with GIZA++ (Och and Ney 2003) (intersection of direct and inverse alignments). In the first row, we see two English–German sentence pairs; in both cases, most of the points lie close to the diagonal representing an overall monotonic translation, whereas few isolated points denote the very long-range reordering of verbs. Similarly, in the two English–Arabic sentence pairs, we mostly obs"
J16-2001,P07-1090,0,0.0551832,"Missing"
J16-2001,P09-1037,0,0.0242489,"and decoding speed. Nguyen and Vogel (2013) and Huck et al. (2013) successfully integrate the distortion cost feature function and phrase orientation models initially designed for string-based PSMT into a chart-based HSMT decoder. Finally, Setiawan, Kan, and Li (2007) observe that, in languages like Chinese and English, function words provide important clues on the grammatical relationships among phrases. Consequently, they introduce a SCFG where function words (approximated by high-frequency words) are the only lexicalized non-terminals guiding phrase reordering. Based on the same intuition, Setiawan et al. (2009) augment a HSMT system with a function-word ordering model that predicts, for any pair of translation rules, which one should dominate the other in the hierarchical structure, based on the function words that they contain.9 2.3.3 Tree-Based SMT with Soft Syntactic Constraints. We have discussed SMT frameworks where the translation model is fully based on the syntactic parse tree of the source or target sentence (Section 2.3.1) or where syntax is not used at all (Section 2.3.2). A third line of work bridges between these two by exploiting syntactic information in the form of soft constraints wh"
J16-2001,D13-1048,0,0.0175651,"ntactic information in the form of soft constraints while operating with a synchronous translation grammar extracted from non-parsed parallel data. Chiang (2005) first experimented with a feature function rewarding translation rules applied to full syntactic constituents (constituent feature). Although this initial attempt did not appear to improve translation quality, Marton and Resnik (2008) further elaborated the idea and proposed a series of finer-grained features distinguishing among 9 Two other models utilizing function words as the anchors of global reordering decisions are proposed in Setiawan et al. (2013) and Setiawan, Zhou, and Xiang (2013). Although integrated in a syntax-based system (Shen, Xu, and Weischedel 2010), these models are in principle applicable to other SMT frameworks such as HSMT. 176 Bisazza and Federico A Survey of Word Reordering in Statistical Machine Translation constituent types (VP, NP, etc.), eventually leading to better performance. Gao, Koehn, and Birch (2011) extract two reordering-related feature functions from source dependency parse trees: (i) The dependency orientation model predicts whether the relative order of a source word and its head should be reversed duri"
J16-2001,P13-1124,0,0.01341,"ntactic information in the form of soft constraints while operating with a synchronous translation grammar extracted from non-parsed parallel data. Chiang (2005) first experimented with a feature function rewarding translation rules applied to full syntactic constituents (constituent feature). Although this initial attempt did not appear to improve translation quality, Marton and Resnik (2008) further elaborated the idea and proposed a series of finer-grained features distinguishing among 9 Two other models utilizing function words as the anchors of global reordering decisions are proposed in Setiawan et al. (2013) and Setiawan, Zhou, and Xiang (2013). Although integrated in a syntax-based system (Shen, Xu, and Weischedel 2010), these models are in principle applicable to other SMT frameworks such as HSMT. 176 Bisazza and Federico A Survey of Word Reordering in Statistical Machine Translation constituent types (VP, NP, etc.), eventually leading to better performance. Gao, Koehn, and Birch (2011) extract two reordering-related feature functions from source dependency parse trees: (i) The dependency orientation model predicts whether the relative order of a source word and its head should be reversed duri"
J16-2001,J10-4005,0,0.0360101,"Missing"
J16-2001,2014.iwslt-evaluation.17,0,0.0745524,"Missing"
J16-2001,W06-3104,0,0.00704584,"or instance, the swap of Alr}ys Alfrnsy may only be dictated by a rule applying to noun phrases composed of a noun and an adjective. On the other hand, string-to-tree methods (Yamada and Knight 2002; Galley et al. 2004; Marcu et al. 2006; Shen, Xu, and Weischedel 2010) use syntax as a way to restrict translation hypotheses to well-formed target language sentences—ruling out, for instance, a translation that fails to reorder the translated verb renewed with respect to its subject. Using syntax on both source and target sides (treeto-tree) (Imamura, Okuma, and Sumita 2005; Ding and Palmer 2005; Smith and Eisner 2006; Watanabe, Tsukada, and Isozaki 2006; Zhang et al. 2008) has proven rather difficult in practice due to the complexity of aligning potentially very different tree topologies and to the large size of the resulting translation grammars. Moreover, the need for highquality parsers in both language sides seriously limits the applicability of this approach. Syntax-based SMT approaches also differ in the formalism they use to represent the trees. Those based on phrase structure (constituency) grammars typically comply with the principle that each translation/reordering rule should match a complete c"
J16-2001,D11-1014,0,0.00498533,"Missing"
J16-2001,W14-4017,0,0.0509408,"Missing"
J16-2001,D14-1025,0,0.0165578,"Missing"
J16-2001,W10-1762,0,0.0158828,"(Cherry 2008) states that when part of a source subtree is translated, all words under the same subtree must be covered before moving to words outside of it. Integrated in phrase-based decoding as soft constraints (i. e., by using the number of violations as a feature function), dependency cohesion and its variants (Cherry 2008; Bach, Vogel, and Cherry 2009) were shown to significantly improve translation quality. In related work, Feng, Sun, and Ney (2012) derive similar cohesion constraints from the semantic role labeling structure of the input sentence. The divide-and-translate approach of Sudoh et al. (2010) uses source-side parse trees to segment complex sentences into simple clauses which are replaced by specific symbols and translated independently. Then, the target sentence is reconstructed using the placeholders, with the aim of simplifying long-range clause-level reordering. 2.1.2 PSMT Reordering Feature Functions. Target language modeling is the primary way to reward promising reorderings during translation. This happens indirectly, through the scoring of target word n-grams, which are generated by translating the source positions in different orders. However, the fixed-size context of lan"
J16-2001,P13-2060,0,0.121098,"07). Chang and Toutanova (2007) use a dependency tree reordering model to generate n alternative orders for each 1-best sentence produced by the SMT system. Each set of n sentence reorderings is then reranked using a discriminative model trained on word bigram features and standard word reordering features (i.e., distance or orientation between consecutively translated input words). Focusing on Japanese-to-English translation, Sudoh et al. (2011, 2013) proposed to “translate” foreign-order English into correct-order English using a monolingual phrase-based (Sudoh et al. 2011) or syntax-based (Sudoh et al. 2013) SMT system trained for this specific subtask.12 The underlying motivation is that, while English-to-Japanese is well handled by pre-ordering with the aforementioned head-finalization rule (Isozaki et al. 2010b), it is much harder to predict the English-like order of Japanese constituents for Japanese-to-English translation. Post-ordering addresses this issue by generating head-final English (HFE) sentences that are used to create a HFE-to-English parallel corpus. Goto, Utiyama, and Sumita (2012, 2013) solve post-ordering by parsing the HFE sentences into binary trees annotated with both synta"
J16-2001,W11-2102,0,0.0137081,"lled RIBES, Isozaki et al. (2010a) propose directly measuring the reordering occurring between the words of the hypothesis and those of the reference translation, thereby eliminating the need to word-align input and output sentence. A limitation of this approach is that only identical words contribute to the score. As a solution, the permutation distance is multiplied by a word precision score that penalizes hypotheses containing few reference words. Nevertheless, the resulting metric assigns different scores to hypotheses that differ in their lexical choice, but not in their word reordering. Talbot et al. (2011) introduce yet another reordering-specific metric, called fuzzy reordering score (FRS) which, like the KRS, is independent from lexical choice and measures the similarity between a sentence’s reference reordering and the reordering produced by an SMT system (or by a pre-ordering technique). However, whereas Birch, Osborne, and Blunsom (2010) used Kendall’s tau between the two sentence permutations, Talbot et al. count the smallest number of chunks that the hypothesis permutation must be divided into to align with the reference permutation. This corresponds precisely to the fragmentation penalt"
J16-2001,N04-4026,0,0.301097,"mate of the cost yet to be incurred (Moore and Quirk 2007). State-of-the-art systems use the distortion cost in combination with more sophisticated reordering models that take into account the identity of the reordered phrases and, optionally, various kinds of contextual information. A representative selection of such models is summarized in Table 1. To ease the presentation, we have divided the models into four groups according to their problem formulation: phrase orientation models, jump models, source decoding sequence models, and operation sequence models. Phrase orientation models (POM) (Tillmann 2004; Koehn et al. 2005; Nagata et al. 2006; Zens and Ney 2006; Li et al. 2014), simply known as lexicalized reordering models, predict whether the next translated source span should be immediately to the right (monotone), immediately to the left (swap), or anywhere else (discontinuous) relatively to the last translated one.7 For example, in Figure 2, the phrase pair [Almlk mHmd AlsAds]-[King Mohamed VI] has monotone orientation whereas [jdd]-[renewed] has discontinuous left orientation with respect to the previously translated phrase. Because of their simple reordering step classification, POM ca"
J16-2001,D09-1105,0,0.062238,"Missing"
J16-2001,D11-1045,0,0.21337,"Target-ordered source] Words : source (b2) Word order : target monotone translation in target word order Words : target Word order : target Figure 7 Figure 1: A typical work flow in a standard translation, Typical workflows of standard, pre-ordering, and post-ordering approaches to SMT. Taken from pre-ordering and post-ordering approach. Sudoh et al. (2011). Habash 2007; Li et al. 2007; Tromble and Eisner 2009; Xu et al. 2009; Genzel 2010; the opposite direction by placing Japanese syntac-and Fonollosa 2011; Khalilov Isozaki et al. 2010b; Yeniterzi and Oflazer 2010; Khalilov and Sima’an 2011; Visweswariah et al. 2011; Gojun and Fraser 2012; Yang et al. 2012; tic heads in the middle is not a trivial problem. Lerner and Petrov 2013; Jehl et al. 2014).10 Non-deterministic pre-ordering encodes We utilize the Head-Finalization rules to genermultiple alternative reorderings into a word lattice and lets a monotonic (usually ate intermediate head-finalized English sentencesto its models (Zens, Och, and n-gram–based) decoder choose the best path according called Head-Final andMariño decompose Ney 2002; Kanthak etEnglish al. 2005; (HFE) Crego and 2006; Zhang, Zens, and Ney 2007; Rottmann and Vogel 2007; Crego and H"
J16-2001,D07-1077,0,0.0432283,"Missing"
J16-2001,C02-1050,0,0.0175591,"Missing"
J16-2001,P06-1098,0,0.0199238,"Missing"
J16-2001,P06-1123,0,0.0955611,"Missing"
J16-2001,W14-3324,0,0.0127617,"n also differs from English with respect to the position of oblique phrases and that of the negation: Both are fixed in English but mixed in German. At the phrase level, German predominantly places the genitive after the noun, while English displays both orders. Thus, despite belonging to the same family branch (Indo-European/Germanic), this pair displays complex reordering patterns. Indeed, German–English reordering has been widely studied in SMT and is still an open topic. At the Workshop on Statistical Machine Translation 2014 (Bojar et al. 2014), a syntaxbased string-to-tree SMT approach (Williams et al. 2014) won in both language directions (official results excluding online systems). At the International Workshop on Spoken Language Translation 2014 (Cettolo et al. 2014), the best submission was a combination of PSMT with POS- and syntax-based preordering (Slawik et al. 2014), string-to-tree syntax-based SMT, and factored PSMT (Birch et al. 2014). English and French [ Main order: same; CDiff: 0.5; PDiff: 1.5 ] Most clause-level features have the same values in French as in English, except for the negation, which is typically expressed by two words in French: one preceding and one following the ver"
J16-2001,P96-1021,0,0.0730453,"late (a) to specify the order of the child dEm. For each new test sentence, matching treelet pairs and order templates are combined to construct lexicalized translation rules for that sentence and, finally, decoding is performed with a chart parsing algorithm. We will now discuss SMT frameworks that model translation as a process of parallel parsing of the source and target language via a synchronous grammar. 2.3.2 Tree-Based SMT Without Syntax. The idea of extracting bilingual translation (i.e., synchronous) grammars directly from word-aligned parallel data originates in early work on ITG by Wu (1996, 1997). In a more mature approach, hierarchical phrase-based SMT (HSMT) (Chiang 2005), the translation model is a probabilistic synchronous context-free grammar (SCFG) whose rules can correspond to arbitrary (i.e., nonsyntactically motivated) phrases labeled by only two generic non-terminal symbols (X or S). As shown in Figure 5, HSMT translation rules can either include a mix of terminals and non-terminals capturing reordering patterns and discontinuities (rules 1–4), or only terminals (rules 7–10) basically corresponding to phrase pairs in string-based PSMT. Finally, the so-called glue rule"
J16-2001,J97-3002,0,0.526747,"stems, the first constraining paradigms were formulated earlier for word-based SMT (Berger et al. 1996; Zens and Ney 2003) and are called IBM constraints. A different kind of reordering constraint can be derived from the Inversion Transduction Grammars (ITGs) (Wu 1995, 1997). ITG constraints only admit permutations 167 Computational Linguistics Volume 42, Number 2 that are generated by recursively swapping pairs of adjacent blocks of words.4 In particular, ITG constraints disallow reorderings that generalize the patterns (3 1 4 2) and (2 4 1 3), which are rarely attested in natural languages (Wu 1997).5 Enforcing ITG constraints in left-to-right PSMT decoding requires the use of a shift-reduce permutation parser (Zens 2008; Feng et al. 2010). Alternatively, a relaxed version of the ITG constraints (i.e., Baxter permutations) may be enforced by simply inspecting the set of covered source positions, as proposed by Zens et al. (2004) and Zens (2008). Interestingly, Cherry, Moore, and Quirk (2012) found no consistent benefit from applying either exact or approximate ITG-constraints to a PSMT system that already included a hierarchical phrase orientation model6 (Galley and Manning 2008). The re"
J16-2001,C04-1073,0,0.409974,"Missing"
J16-2001,P06-1066,0,0.225701,"Missing"
J16-2001,N09-1028,0,0.249285,"dered target] Words : source Word order : source (b1) pre-ordering Volume 42, Number 2 Words : target Word order : source (a) standard translation (c2) post-ordering [Target language] [Target-ordered source] Words : source (b2) Word order : target monotone translation in target word order Words : target Word order : target Figure 7 Figure 1: A typical work flow in a standard translation, Typical workflows of standard, pre-ordering, and post-ordering approaches to SMT. Taken from pre-ordering and post-ordering approach. Sudoh et al. (2011). Habash 2007; Li et al. 2007; Tromble and Eisner 2009; Xu et al. 2009; Genzel 2010; the opposite direction by placing Japanese syntac-and Fonollosa 2011; Khalilov Isozaki et al. 2010b; Yeniterzi and Oflazer 2010; Khalilov and Sima’an 2011; Visweswariah et al. 2011; Gojun and Fraser 2012; Yang et al. 2012; tic heads in the middle is not a trivial problem. Lerner and Petrov 2013; Jehl et al. 2014).10 Non-deterministic pre-ordering encodes We utilize the Head-Finalization rules to genermultiple alternative reorderings into a word lattice and lets a monotonic (usually ate intermediate head-finalized English sentencesto its models (Zens, Och, and n-gram–based) decod"
J16-2001,2009.mtsummit-papers.19,0,0.0170227,"ed source positions, as proposed by Zens et al. (2004) and Zens (2008). Interestingly, Cherry, Moore, and Quirk (2012) found no consistent benefit from applying either exact or approximate ITG-constraints to a PSMT system that already included a hierarchical phrase orientation model6 (Galley and Manning 2008). The reordering constraints presented so far are not sensitive to the words being translated nor to their context. This results in a very coarse definition of the reordering search space, which is problematic in language pairs with different syntactic structures. To address this problem, Yahyaei and Monz (2009) propose decoupling local and global reordering by segmenting the input sentence into chunks that can be permuted arbitrarily, but each of which is translated monotonically. In related work, Yahyaei and Monz (2010) present a technique to dynamically set the DL during decoding: They train a discriminative classifier to predict the most probable jump length after each input word, and use the predicted value as the DL after that position. Unfortunately, this method appears to generate inconsistent constraints leading to decoding dead-ends. Bisazza and Federico (2013a) further develop this idea so"
J16-2001,2010.iwslt-papers.19,0,0.0216811,"system that already included a hierarchical phrase orientation model6 (Galley and Manning 2008). The reordering constraints presented so far are not sensitive to the words being translated nor to their context. This results in a very coarse definition of the reordering search space, which is problematic in language pairs with different syntactic structures. To address this problem, Yahyaei and Monz (2009) propose decoupling local and global reordering by segmenting the input sentence into chunks that can be permuted arbitrarily, but each of which is translated monotonically. In related work, Yahyaei and Monz (2010) present a technique to dynamically set the DL during decoding: They train a discriminative classifier to predict the most probable jump length after each input word, and use the predicted value as the DL after that position. Unfortunately, this method appears to generate inconsistent constraints leading to decoding dead-ends. Bisazza and Federico (2013a) further develop this idea so that only long reorderings predicted by a specific reordering model are explored by the decoder. This form of early reordering pruning enables the PSMT system to capture long-range reordering without hurting effic"
J16-2001,P02-1039,0,0.0609456,"h. So-called tree-to-string methods (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006) use a given input sentence parse tree to restrict the application of translation/reordering rules to word 8 More pre-ordering techniques will be discussed in Section 2.4. 172 Bisazza and Federico A Survey of Word Reordering in Statistical Machine Translation spans that coincide with syntactic constituents of specific categories. For instance, the swap of Alr}ys Alfrnsy may only be dictated by a rule applying to noun phrases composed of a noun and an adjective. On the other hand, string-to-tree methods (Yamada and Knight 2002; Galley et al. 2004; Marcu et al. 2006; Shen, Xu, and Weischedel 2010) use syntax as a way to restrict translation hypotheses to well-formed target language sentences—ruling out, for instance, a translation that fails to reorder the translated verb renewed with respect to its subject. Using syntax on both source and target sides (treeto-tree) (Imamura, Okuma, and Sumita 2005; Ding and Palmer 2005; Smith and Eisner 2006; Watanabe, Tsukada, and Isozaki 2006; Zhang et al. 2008) has proven rather difficult in practice due to the complexity of aligning potentially very different tree topologies an"
J16-2001,P12-1096,0,0.261056,"rder : target monotone translation in target word order Words : target Word order : target Figure 7 Figure 1: A typical work flow in a standard translation, Typical workflows of standard, pre-ordering, and post-ordering approaches to SMT. Taken from pre-ordering and post-ordering approach. Sudoh et al. (2011). Habash 2007; Li et al. 2007; Tromble and Eisner 2009; Xu et al. 2009; Genzel 2010; the opposite direction by placing Japanese syntac-and Fonollosa 2011; Khalilov Isozaki et al. 2010b; Yeniterzi and Oflazer 2010; Khalilov and Sima’an 2011; Visweswariah et al. 2011; Gojun and Fraser 2012; Yang et al. 2012; tic heads in the middle is not a trivial problem. Lerner and Petrov 2013; Jehl et al. 2014).10 Non-deterministic pre-ordering encodes We utilize the Head-Finalization rules to genermultiple alternative reorderings into a word lattice and lets a monotonic (usually ate intermediate head-finalized English sentencesto its models (Zens, Och, and n-gram–based) decoder choose the best path according called Head-Final andMariño decompose Ney 2002; Kanthak etEnglish al. 2005; (HFE) Crego and 2006; Zhang, Zens, and Ney 2007; Rottmann and Vogel 2007; Crego and Habash 2008; Elming and Habash 2009; Niehu"
J16-2001,P10-1047,0,0.0159843,"difficult to reorder for an SMT system. The complex nature of its reordering phenomena suggests a good fit for tree-based SMT approaches, and indeed, HSMT was shown to significantly outperform PSMT between Turkish and English in both language directions (Ruiz et al. 2012; Yılmaz et al. 2013). However, state-of-the-art SMT quality in this language pair is still very low, mostly because of the agglutinative nature of Turkish, which makes it difficult to tear apart word reordering issues from rich morphology issues. Attempting to address both issues in an English-to-Turkish factored PSMT system, Yeniterzi and Oflazer (2010) pre-process the parsed English side with a number of syntax-to-morphology mapping rules and constituent pre-ordering rules dealing with local and global reordering phenomena, respectively. Only the former, though, resulted in better translation quality. English and Japanese [ Main order: different; CDiff: 6; PDiff: 1.5 ] Japanese is the prototypical example of head-final language. In this pair all clause-level features are discordant, whereas at the phrase level, Japanese differs from English for the use of postpositions and the strictly head-final genitive construction. This pair, like the p"
J16-2001,I08-1068,0,0.0412101,"and are called IBM constraints. A different kind of reordering constraint can be derived from the Inversion Transduction Grammars (ITGs) (Wu 1995, 1997). ITG constraints only admit permutations 167 Computational Linguistics Volume 42, Number 2 that are generated by recursively swapping pairs of adjacent blocks of words.4 In particular, ITG constraints disallow reorderings that generalize the patterns (3 1 4 2) and (2 4 1 3), which are rarely attested in natural languages (Wu 1997).5 Enforcing ITG constraints in left-to-right PSMT decoding requires the use of a shift-reduce permutation parser (Zens 2008; Feng et al. 2010). Alternatively, a relaxed version of the ITG constraints (i.e., Baxter permutations) may be enforced by simply inspecting the set of covered source positions, as proposed by Zens et al. (2004) and Zens (2008). Interestingly, Cherry, Moore, and Quirk (2012) found no consistent benefit from applying either exact or approximate ITG-constraints to a PSMT system that already included a hierarchical phrase orientation model6 (Galley and Manning 2008). The reordering constraints presented so far are not sensitive to the words being translated nor to their context. This results in"
J16-2001,P03-1019,0,0.26372,"rtant for translation quality because the existing SMT models are typically not discriminative enough to guide the search over very large sets of reordering hypotheses. However, reordering constraints have also several drawbacks. For instance, the verb reordering in Figure 2 may not be captured by a PSMT system that applies a DL of k = 5 or less, because jumping back from AlsAds to jdd corresponds to a skip of six positions. While the distortion limit is a de facto standard in modern PSMT systems, the first constraining paradigms were formulated earlier for word-based SMT (Berger et al. 1996; Zens and Ney 2003) and are called IBM constraints. A different kind of reordering constraint can be derived from the Inversion Transduction Grammars (ITGs) (Wu 1995, 1997). ITG constraints only admit permutations 167 Computational Linguistics Volume 42, Number 2 that are generated by recursively swapping pairs of adjacent blocks of words.4 In particular, ITG constraints disallow reorderings that generalize the patterns (3 1 4 2) and (2 4 1 3), which are rarely attested in natural languages (Wu 1997).5 Enforcing ITG constraints in left-to-right PSMT decoding requires the use of a shift-reduce permutation parser"
J16-2001,W06-3108,0,0.0415396,"007). State-of-the-art systems use the distortion cost in combination with more sophisticated reordering models that take into account the identity of the reordered phrases and, optionally, various kinds of contextual information. A representative selection of such models is summarized in Table 1. To ease the presentation, we have divided the models into four groups according to their problem formulation: phrase orientation models, jump models, source decoding sequence models, and operation sequence models. Phrase orientation models (POM) (Tillmann 2004; Koehn et al. 2005; Nagata et al. 2006; Zens and Ney 2006; Li et al. 2014), simply known as lexicalized reordering models, predict whether the next translated source span should be immediately to the right (monotone), immediately to the left (swap), or anywhere else (discontinuous) relatively to the last translated one.7 For example, in Figure 2, the phrase pair [Almlk mHmd AlsAds]-[King Mohamed VI] has monotone orientation whereas [jdd]-[renewed] has discontinuous left orientation with respect to the previously translated phrase. Because of their simple reordering step classification, POM can be conditioned on very finegrained information, such as"
J16-2001,C04-1030,0,0.0580088,"tational Linguistics Volume 42, Number 2 that are generated by recursively swapping pairs of adjacent blocks of words.4 In particular, ITG constraints disallow reorderings that generalize the patterns (3 1 4 2) and (2 4 1 3), which are rarely attested in natural languages (Wu 1997).5 Enforcing ITG constraints in left-to-right PSMT decoding requires the use of a shift-reduce permutation parser (Zens 2008; Feng et al. 2010). Alternatively, a relaxed version of the ITG constraints (i.e., Baxter permutations) may be enforced by simply inspecting the set of covered source positions, as proposed by Zens et al. (2004) and Zens (2008). Interestingly, Cherry, Moore, and Quirk (2012) found no consistent benefit from applying either exact or approximate ITG-constraints to a PSMT system that already included a hierarchical phrase orientation model6 (Galley and Manning 2008). The reordering constraints presented so far are not sensitive to the words being translated nor to their context. This results in a very coarse definition of the reordering search space, which is problematic in language pairs with different syntactic structures. To address this problem, Yahyaei and Monz (2009) propose decoupling local and g"
J16-2001,2002.tmi-tutorials.2,0,0.0206069,"Missing"
J16-2001,W07-0404,0,0.0564113,"Missing"
J16-2001,P08-1064,0,0.0279742,"by a rule applying to noun phrases composed of a noun and an adjective. On the other hand, string-to-tree methods (Yamada and Knight 2002; Galley et al. 2004; Marcu et al. 2006; Shen, Xu, and Weischedel 2010) use syntax as a way to restrict translation hypotheses to well-formed target language sentences—ruling out, for instance, a translation that fails to reorder the translated verb renewed with respect to its subject. Using syntax on both source and target sides (treeto-tree) (Imamura, Okuma, and Sumita 2005; Ding and Palmer 2005; Smith and Eisner 2006; Watanabe, Tsukada, and Isozaki 2006; Zhang et al. 2008) has proven rather difficult in practice due to the complexity of aligning potentially very different tree topologies and to the large size of the resulting translation grammars. Moreover, the need for highquality parsers in both language sides seriously limits the applicability of this approach. Syntax-based SMT approaches also differ in the formalism they use to represent the trees. Those based on phrase structure (constituency) grammars typically comply with the principle that each translation/reordering rule should match a complete constituent, whereas those based on dependency grammars op"
J16-2001,W07-0401,0,0.0624539,"Missing"
J16-2001,W06-3119,0,0.00859258,"Devlin, and Zbib (2013) worked instead with constituency parses and trained a classifier to predict whether the order of any two sibling constituents in the input tree should be reversed or maintained during translation. The classifier is trained by maximum entropy, using a number of syntactic features and used during decoding at the word level: that is, each pair of input words inherit the orientation probabilities of the constituents that cover them, respectively. Syntactic annotation has also been used to refine non-terminal SCFG labels, potentially leading to better reordering choices. In Zollmann and Venugopal (2006) and Mylonakis and Sima’an (2011), labels indicate whether a phrase corresponds to a syntactic constituent or to part of it, as well as the constituent type, relatively to a target or source parse tree, respectively. Moreover, Mylonakis and Sima’an treat the phrasepair category as a latent variable and let their system learn reordering distributions over multiple labels per span (generic X or source-syntax based like NP, VBZ + DT, etc.). Li et al. (2012) use source dependency annotation to refine non-terminal symbols with syntactic head information. More specifically, given a hierarchical phra"
J16-2001,C08-1144,0,0.202405,"as observed in the French-to-English track. English and Arabic [ Main order: different; CDiff: 0.5; PDiff: 2.5 ] The dominant Arabic order is VSO, followed by SVO (cf. Section 4.2). Apart from this important difference, all other clause-level features agree between Arabic and English. At the phrase level, differences are found in genitives, adjectives, and degree words. As a result, reordering is overwhelmingly local but few crucial long-range reorderings also regularly occur. Thus, this pair is challenging for PSMT but, at the same time, not well suited for a tree-based approach. As shown by Zollmann et al. (2008) and Birch, Blunsom, and Osborne (2009), PSMT performs similarly or better than HSMT for the Arabic-to-English language pair. However, HSMT was shown to better cope with the reordering of VSO sentences (Bisazza 2013). Pre-ordering of Arabic VSO sentences for translation into English has proved to 1 7 Pre-verbal negation can be omitted in colloquial French. 190 Bisazza and Federico A Survey of Word Reordering in Statistical Machine Translation be a particularly difficult task (Green, Sathi, and Manning 2009; Carpuat, Marton, and Habash 2010) and has inspired work on hybrid pre-ordering where mu"
J16-2001,N10-1140,0,\N,Missing
J16-2001,D11-1018,0,\N,Missing
J16-2001,N10-1128,0,\N,Missing
J16-2001,N09-2001,0,\N,Missing
J16-2001,W06-3601,0,\N,Missing
J16-2001,P02-1040,0,\N,Missing
J16-2001,H05-1098,0,\N,Missing
J16-2001,P13-2071,0,\N,Missing
J16-2001,P13-1016,0,\N,Missing
J16-2001,Q13-1027,1,\N,Missing
J16-2001,J10-3008,0,\N,Missing
J16-2001,2010.iwslt-keynotes.2,0,\N,Missing
J16-2001,P11-1105,0,\N,Missing
J16-2001,J04-4002,0,\N,Missing
J16-2001,W05-0909,0,\N,Missing
J16-2001,P07-2045,1,\N,Missing
J16-2001,W09-0809,0,\N,Missing
J16-2001,P10-2033,0,\N,Missing
J16-2001,W13-2257,1,\N,Missing
J16-2001,E09-1061,0,\N,Missing
J16-2001,2014.iwslt-evaluation.6,0,\N,Missing
J16-2001,J03-1005,0,\N,Missing
J16-2001,W08-0307,0,\N,Missing
J16-2001,P08-1066,0,\N,Missing
J16-2001,2010.iwslt-evaluation.11,0,\N,Missing
J16-2001,C14-1041,0,\N,Missing
J16-2001,W14-3302,0,\N,Missing
J16-2001,D11-1125,0,\N,Missing
L16-1562,J90-2002,0,0.264106,"Missing"
L16-1562,P02-1033,0,0.10194,"Missing"
L16-1562,N13-1073,0,0.251905,"rucial aspect for WA and – more generally – machine translation. WAGS is publicly released under a Creative Commons Attribution license (CC BY 4.0) and is available at: http://hlt-mt.fbk.eu/technologies/wags In addition to the gold standard data, the release includes the annotation guidelines and an evaluation package that allows to compute Alignment Error Rate (AER) on customizable subsets of WAGS links, for example those aligning only OOV words. In the following, we describe the characteristics of WAGS 3535 and provide results from relevant WA state-of-the-art technology, namely fast align (Dyer et al., 2013) – a variant of IBM model 2 – and IBM model 4 as implemented in mgiza++ (Gao and Vogel, 2008). 2. Dataset Description To create WAGS, we used the publicly available Europarl parallel corpus1 (Koehn, 2005), which contains the proceedings of the European Parliament in the various official languages. A Common Test set, made of the texts from the 4th quarter of year 2000, was defined to be used for machine translation evaluation (Koehn et al., 2003). Table 1 shows some statistics about the Italian and English portions of Europarl v7 release. WAGS is a selection from the Common Test set, realized a"
L16-1562,W14-0313,1,0.899664,"ligned. Regarding this latter issue, two main approaches were followed in previous works: full text alignment, where all words in the text are manually aligned, and sample word alignment, where a set of test words are selected and only those words are manually aligned (V´eronis and Langlais, 2000; Merkel, 1999; Ahrenberg et al., 2002). One of the most challenging issues for current state-ofthe-art word aligners is that they show poor generalization capability and are prone to errors when infrequent or unknown words (with respect to the training data) occur in new sentence pairs to be aligned (Farajian et al., 2014). Thus, WA research would highly benefit from gold standard data specifically tailored to assess WA systems on this issue. However, to our knowledge, none of the available WA benchmarks specifically focuses on the problem of out-ofvocabulary (OOV) and rare words. The main contribution of our work is to provide the research community with WAGS (Word Alignment Gold Standard), a novel benchmark which allows extensive evaluation of WA tools on OOV and rare words. WAGS is a subset of the Common Test section of the Europarl English-Italian parallel corpus (Koehn et al., 2003; Koehn, 2005), and is sp"
L16-1562,J07-3002,0,0.0845348,"Missing"
L16-1562,W08-0509,0,0.188606,"under a Creative Commons Attribution license (CC BY 4.0) and is available at: http://hlt-mt.fbk.eu/technologies/wags In addition to the gold standard data, the release includes the annotation guidelines and an evaluation package that allows to compute Alignment Error Rate (AER) on customizable subsets of WAGS links, for example those aligning only OOV words. In the following, we describe the characteristics of WAGS 3535 and provide results from relevant WA state-of-the-art technology, namely fast align (Dyer et al., 2013) – a variant of IBM model 2 – and IBM model 4 as implemented in mgiza++ (Gao and Vogel, 2008). 2. Dataset Description To create WAGS, we used the publicly available Europarl parallel corpus1 (Koehn, 2005), which contains the proceedings of the European Parliament in the various official languages. A Common Test set, made of the texts from the 4th quarter of year 2000, was defined to be used for machine translation evaluation (Koehn et al., 2003). Table 1 shows some statistics about the Italian and English portions of Europarl v7 release. WAGS is a selection from the Common Test set, realized as described below. Training Common Test #seg 1,908,966 42,753 #Ita tokens 54,848,640 1,224,17"
L16-1562,C14-2026,1,0.822664,"adopted in other available word alignment benchmarks to cope with alignment ambiguity, namely all links in disagreement were included in the final reference alignment as P-links. In Example 4, both annotators linked “orfana” (“orphan” in English) and “deprived”, but one labeled it as S-link while the other as P-link. Since the adjudicator did not prefer one solution to the other, that link was labeled as Possible in the gold standard. Example 4: • Ita: ...questa Unione e` stata un po’ orfana. • Eng: ...the Union was somewhat deprived. Annotations were accomplished using the MT-EQuAl toolkit5 (Girardi et al., 2014). In addition to the traditional matrix-based alignment, MT-EQuAl allows a more user friendly text-based alignment procedure, where mouse clicks on words are used directly to establish alignment links. This alignment method was particularly useful in our task, since annotators were presented with long sentences and only few words were to be annotated. A screenshot of the alignment interface is presented in Figure 3, which shows the alignment of Example 1 above. In the figure, the P-link between “you” and “domanderete” has already been created: the words are underlined in the text (light blue)"
L16-1562,graca-etal-2008-building,0,0.0494955,"Missing"
L16-1562,W11-4615,0,0.0489247,"Missing"
L16-1562,N03-1017,0,0.0140844,"ds. In the following, we describe the characteristics of WAGS 3535 and provide results from relevant WA state-of-the-art technology, namely fast align (Dyer et al., 2013) – a variant of IBM model 2 – and IBM model 4 as implemented in mgiza++ (Gao and Vogel, 2008). 2. Dataset Description To create WAGS, we used the publicly available Europarl parallel corpus1 (Koehn, 2005), which contains the proceedings of the European Parliament in the various official languages. A Common Test set, made of the texts from the 4th quarter of year 2000, was defined to be used for machine translation evaluation (Koehn et al., 2003). Table 1 shows some statistics about the Italian and English portions of Europarl v7 release. WAGS is a selection from the Common Test set, realized as described below. Training Common Test #seg 1,908,966 42,753 #Ita tokens 54,848,640 1,224,178 #Eng tokens 55,141,541 1,266, 968 Table 1: Europarl v7 statistics: number of segments and Italian/English tokens in Training and Common Test sets. 2.1. Data selection The length of segments in the Europarl Common Test set ranges from one (single word segments) to more than two hundreds. It is a matter of fact that the automatic WA of either too short o"
L16-1562,2005.mtsummit-papers.11,0,0.0472688,"addition to the gold standard data, the release includes the annotation guidelines and an evaluation package that allows to compute Alignment Error Rate (AER) on customizable subsets of WAGS links, for example those aligning only OOV words. In the following, we describe the characteristics of WAGS 3535 and provide results from relevant WA state-of-the-art technology, namely fast align (Dyer et al., 2013) – a variant of IBM model 2 – and IBM model 4 as implemented in mgiza++ (Gao and Vogel, 2008). 2. Dataset Description To create WAGS, we used the publicly available Europarl parallel corpus1 (Koehn, 2005), which contains the proceedings of the European Parliament in the various official languages. A Common Test set, made of the texts from the 4th quarter of year 2000, was defined to be used for machine translation evaluation (Koehn et al., 2003). Table 1 shows some statistics about the Italian and English portions of Europarl v7 release. WAGS is a selection from the Common Test set, realized as described below. Training Common Test #seg 1,908,966 42,753 #Ita tokens 54,848,640 1,224,178 #Eng tokens 55,141,541 1,266, 968 Table 1: Europarl v7 statistics: number of segments and Italian/English tok"
L16-1562,kruijff-korbayova-etal-2006-annotation,0,0.0671595,"Missing"
L16-1562,P04-1060,0,0.0831318,"Missing"
L16-1562,macken-2010-annotation,0,0.0586513,"Missing"
L16-1562,W05-0809,0,0.0461213,"l Machine Translation (Och and Ney, 2004; Fraser and Marcu, 2007), but also other applications rely on WA, such as extraction of bilingual lexica (Smadja et al., 1996), word sense disambiguation (Diab and Resnik, 2002), projection of linguistic information between languages (Yarowsky and Ngai, 2001; Kuhn, 2004; Bentivogli and Pianta, 2005). WA gold standards represent a crucial resource to evaluate and analyse WA systems’ performance, and nowadays various benchmarks for different language pairs are available (Melamed, 1998; Och and Ney, 2000; Mihalcea and Pedersen, 2003; Lambert et al., 2005; Martin et al., 2005; Kruijff-Korbayov´a et al., 2006; Grac¸a et al., 2008; Macken, 2010; Holmqvist and Ahrenberg, 2011). Besides the languages addressed, existing benchmarks differ in various respects – also depending on the final application to be evaluated – such as the parallel data used, the annotation scheme adopted (and related guidelines), the selection of words to be manually aligned. Regarding this latter issue, two main approaches were followed in previous works: full text alignment, where all words in the text are manually aligned, and sample word alignment, where a set of test words are selected and"
L16-1562,W03-0301,0,0.129144,"Missing"
L16-1562,P00-1056,0,0.752067,"tence pair (Brown et al., 1990). WA is a basic component of Statistical Machine Translation (Och and Ney, 2004; Fraser and Marcu, 2007), but also other applications rely on WA, such as extraction of bilingual lexica (Smadja et al., 1996), word sense disambiguation (Diab and Resnik, 2002), projection of linguistic information between languages (Yarowsky and Ngai, 2001; Kuhn, 2004; Bentivogli and Pianta, 2005). WA gold standards represent a crucial resource to evaluate and analyse WA systems’ performance, and nowadays various benchmarks for different language pairs are available (Melamed, 1998; Och and Ney, 2000; Mihalcea and Pedersen, 2003; Lambert et al., 2005; Martin et al., 2005; Kruijff-Korbayov´a et al., 2006; Grac¸a et al., 2008; Macken, 2010; Holmqvist and Ahrenberg, 2011). Besides the languages addressed, existing benchmarks differ in various respects – also depending on the final application to be evaluated – such as the parallel data used, the annotation scheme adopted (and related guidelines), the selection of words to be manually aligned. Regarding this latter issue, two main approaches were followed in previous works: full text alignment, where all words in the text are manually aligned"
L16-1562,J04-4002,0,0.0713423,"Missing"
L16-1562,J96-1001,0,0.58811,"Missing"
L16-1562,steinberger-etal-2006-jrc,0,0.0981094,"Missing"
L16-1562,N01-1026,0,0.22907,"Missing"
L16-1562,ahrenberg-etal-2002-system,0,\N,Missing
N10-1045,P06-1114,0,0.0515693,"ed as a generic framework for modeling language variability. Given two texts T and H, the task consists in deciding if the meaning of H can be inferred from the meaning of T. So far, TE has been only applied in a monolingual setting, where both texts are assumed to be written in the same language. In this work, we propose and investigate a cross-lingual extension of TE, where we assume that T and H are written in different languages. The great potential of integrating (monolingual) TE recognition components into NLP architectures has been reported in several works, such as question answering (Harabagiu and Hickl, 2006), information retrieval (Clinchant et al., 2006), information extraction (Romano et al., 2006), and document summarization (Lloret et al., 2008). To the best of our knowledge, mainly due to the absence of cross-lingual TE (CLTE) recognition components, similar improvements have not been achieved yet in any cross-lingual application. As a matter of fact, despite the great deal of attention that TE has received in recent years (also witnessed by five editions of the Recognizing Textual Entailment Challenge1 ), interest for cross-lingual extensions has not been in the mainstream of TE research, w"
N10-1045,P07-2045,1,0.00784892,"Missing"
N10-1045,P09-2073,1,0.304023,"tance Textual Entailment Suite). This system is an open source software package based on edit distance algorithms, which computes the T-H distance as the cost of the edit operations (i.e. insertion, deletion and substitution) that are necessary to transform T into H. By defining the edit distance algorithm and a cost scheme (i.e. which defines the costs of each edit operation), this package is able to learn a distance model over a set of training pairs, which is used to decide if an entailment relation holds over each test pair. In order to obtain a monolingual TE model, we trained and tuned (Mehdad, 2009) our model on the RTE3 test set, to reduce the overfitting bias, since 4 http://translate.google.com http://www.statmt.org/moses/ 6 http://www.statmt.org/europarl/ 7 http://www.ldc.upenn.edu 8 http://edits.fbk.eu/ our original data was created over the RTE3 development set. Moreover, we used a set of lexical entailment rules extracted from Wikipedia and WordNet, as described in (Mehdad et al., 2009). To begin with, we used this model to classify the created cross-lingual entailment corpus in three different settings: 1) hypotheses translated by Google, 2) hypotheses translated by Moses (1st be"
N10-1045,W09-0404,0,0.0583924,"Missing"
N10-1045,E06-1052,0,0.0313124,"sts in deciding if the meaning of H can be inferred from the meaning of T. So far, TE has been only applied in a monolingual setting, where both texts are assumed to be written in the same language. In this work, we propose and investigate a cross-lingual extension of TE, where we assume that T and H are written in different languages. The great potential of integrating (monolingual) TE recognition components into NLP architectures has been reported in several works, such as question answering (Harabagiu and Hickl, 2006), information retrieval (Clinchant et al., 2006), information extraction (Romano et al., 2006), and document summarization (Lloret et al., 2008). To the best of our knowledge, mainly due to the absence of cross-lingual TE (CLTE) recognition components, similar improvements have not been achieved yet in any cross-lingual application. As a matter of fact, despite the great deal of attention that TE has received in recent years (also witnessed by five editions of the Recognizing Textual Entailment Challenge1 ), interest for cross-lingual extensions has not been in the mainstream of TE research, which until now has been mainly focused on the English language. Nevertheless, the strong inter"
N10-1064,P00-1037,0,0.264804,"ht affect text mining is in (Subramaniam et al., 2009), while an analysis of how noise phenomena, commonly occurring in blogs, affect an opinion mining application is in (Dey and Haque, 2009). Concerning spelling correction literature, many works apply the noisy channel model which consists of two components: a source model (prior of word probabilities) and a channel (error) model, that accounts for spelling transformations on letter sequences. Several approaches have been proposed under this framework, that mainly differ in the employed error model; see for example: (Church and Gale, 1991), (Brill and Moore, 2000) and (Toutanova and Moore, 2002). Comprehensive surveys on methods to model and recover spelling errors can be found in (Kukich, 1992) and (Pedler, 2007); in particular, the latter work is specifically centered on methods for correcting so-called real-word errors (cf. Section 3). The detection of errors and the suggestion of corrections typically rely on the availability of text corpora or human-made lexical resources. Search for correct alternatives can be based on word similarity measures, such as the edit distance (Mitton, 1995), anagram hashing (Reynaert, 2006), and semantic distance based"
N10-1064,E95-1010,0,0.187162,"Missing"
N10-1064,fossati-di-eugenio-2008-saw,0,0.0471505,"Missing"
N10-1064,P07-2045,1,0.011029,"Missing"
N10-1064,reynaert-2006-corpus,0,0.0152725,"rch and Gale, 1991), (Brill and Moore, 2000) and (Toutanova and Moore, 2002). Comprehensive surveys on methods to model and recover spelling errors can be found in (Kukich, 1992) and (Pedler, 2007); in particular, the latter work is specifically centered on methods for correcting so-called real-word errors (cf. Section 3). The detection of errors and the suggestion of corrections typically rely on the availability of text corpora or human-made lexical resources. Search for correct alternatives can be based on word similarity measures, such as the edit distance (Mitton, 1995), anagram hashing (Reynaert, 2006), and semantic distance based on WordNet (Hirst and Budanitsky, 2005). More sophisticated approaches have been proposed by (Fossati and Di Eugenio, 2008), that mixes surface and Part-Of-Speech Information, and (Schaback and Li, 2007), which combines similarity measures at the character, phonetic, word, syntax, and semantic levels into one global feature-based framework. 413 a) *W *w had just come in from Australia [Australia] b) good service we *staid one week. [Tahiti] c) The room was *exellent but the hallway was *filty . [NJ] d) is a good place to stay, if you are looking for a hotel *arrou"
N10-1064,P02-1019,0,0.149611,"(Subramaniam et al., 2009), while an analysis of how noise phenomena, commonly occurring in blogs, affect an opinion mining application is in (Dey and Haque, 2009). Concerning spelling correction literature, many works apply the noisy channel model which consists of two components: a source model (prior of word probabilities) and a channel (error) model, that accounts for spelling transformations on letter sequences. Several approaches have been proposed under this framework, that mainly differ in the employed error model; see for example: (Church and Gale, 1991), (Brill and Moore, 2000) and (Toutanova and Moore, 2002). Comprehensive surveys on methods to model and recover spelling errors can be found in (Kukich, 1992) and (Pedler, 2007); in particular, the latter work is specifically centered on methods for correcting so-called real-word errors (cf. Section 3). The detection of errors and the suggestion of corrections typically rely on the availability of text corpora or human-made lexical resources. Search for correct alternatives can be based on word similarity measures, such as the edit distance (Mitton, 1995), anagram hashing (Reynaert, 2006), and semantic distance based on WordNet (Hirst and Budanitsk"
N10-1064,E03-1050,0,\N,Missing
P07-2045,N03-2002,0,0.152204,"nfusion networks. This input type has been used successfully for speech to text translation (Shen et al. 2006). Every factor on the target language can have its own language model. Since many factors, like lemmas and POS tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors. This may encourage more syntactically correct output. In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas. Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006). 4 Confusion Network Decoding Machine translation input currently takes the form of simple sequences of words. However, there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP/speech processing tools (such as named entity recognizers, speech recognizers, morphological analyzers, etc.). These upstream processes tend to generate multiple, erroneous hypotheses with varying confidence. Current MT systems are designed to process only one input hypothesis, making them vulnerable to errors in the input."
P07-2045,koen-2004-pharaoh,0,0.148177,"to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is comparable to the best available systems. Moses has shown that it achieves results comparable to the most competitive and widely used statistical machine translation systems in translation quality and run-time (Shen et al. 2006). It features all the capabilities of the closed sourced Pharaoh decoder (Koehn 2004). Apart from providing an open-source toolkit for SMT, a further motivation for Moses is to extend phrase-based translation with factors and confusion network decoding. The current phrase-based approach to statistical machine translation is limited to the mapping of small text chunks without any explicit use of linguistic information, be it morphological, syntactic, or semantic. These additional sources of information have been shown to be valuable when integrated into pre-processing or post-processing steps. Moses also integrates confusion network decoding, which allows the translation of amb"
P07-2045,D07-1091,1,0.158367,"Missing"
P07-2045,N03-1017,1,0.161374,"informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 07aec_2@williams.edu. 8 evh4@cornell.edu 2 Abstract We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks. 1 Motivation Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research. However, until now, most work in this field has been carried out on proprietary and in-house research systems. This lack of openness has created a high barrier to entry for researchers as many of the components required have had to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is co"
P07-2045,P03-1021,0,0.176468,"ent data structures in Moses for the memory-intensive translation model and language model allow the exploitation of much larger data resources with limited hardware. 177 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customiz"
P07-2045,J03-1002,0,0.164868,"L 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Moses has an active research community and has reached over 1000 downloads as of 1st March 2007. The main online pre"
P07-2045,P02-1040,0,0.148118,"d language model allow the exploitation of much larger data resources with limited hardware. 177 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Mo"
P07-2045,N07-1062,1,0.152186,"up gigabytes of disk space, but for the translation of a single sentence only a tiny fraction of this table is needed. Moses implements an efficient representation of the phrase translation table. Its key properties are a prefix tree structure for source words and on demand loading, i.e. only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder. For the Chinese-English NIST task, the memory requirement of the phrase table is reduced from 1.7 gigabytes to less than 20 mega bytes, with no loss in translation quality and speed (Zens and Ney 2007). The other large data resource for statistical machine translation is the language model. Almost unlimited text resources can be collected from the Internet and used as training data for language modeling. This results in language models that are too large to easily fit into memory. The Moses system implements a data structure for language models that is more efficient than the canonical SRILM (Stolcke 2002) implementation used in most systems. The language model on disk is also converted into this binary format, resulting in a minimal loading time during start-up of the decoder. An even more"
P07-2045,D08-1076,0,\N,Missing
P07-2045,2006.iwslt-evaluation.8,1,\N,Missing
P11-1134,P98-1013,0,0.0179973,"g relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis. Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009). These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia (Mehdad et al., 2010; Kouylekov et al., 2009). DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts a"
P11-1134,P05-1074,0,0.237919,"een texts and hypotheses. Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores. Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways. As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others. Some previous works (Bannard and Callison-Burch, 2005; Zhao et al., 2009; Kouylekov et al., 2009) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words. Addressing CLTE we have to face additional and more problematic issues related to: i) the stronger need of lexical knowledge, and ii) the limited availability of multilingual lexical resources. As regards the first issue, it’s worth noting that in the monolingual scenario simple “bag of words” (or “bag of ngrams”) approaches are per se sufficient to achieve"
P11-1134,W04-3205,0,0.0289879,"chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis. Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009). These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia (Mehdad et al., 2010; Kouylekov et al., 2009). DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the s"
P11-1134,N10-1031,0,0.0124482,"hrase table, and extract their equivalents in l1 ; 3. use the paraphrase table in l1 to find paraphrases of the extracted fragments in l1 ; phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase. 4. map such paraphrases to phrases in T. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases. In our work we used available2 paraphrase databases for English and Spanish which have been extracted using t"
P11-1134,E09-1025,0,0.0260215,"alents in l1 ; 3. use the paraphrase table in l1 to find paraphrases of the extracted fragments in l1 ; phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase. 4. map such paraphrases to phrases in T. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases. In our work we used available2 paraphrase databases for English and Spanish which have been extracted using the method previously outlined."
P11-1134,N03-1017,0,0.0166869,"es. In particular, “YES” and “NO” judgements are assigned considering the proportion of words in the hypothesis that are found also in the text. This way to approximate entailment reflects the intuition that, as a directional relation between the text and the hypothesis, the full content of H has to be found in T. 3.1 Extracting Phrase and Paraphrase Tables Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities. They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003). There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT101 . We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses"
P11-1134,P07-2045,1,0.00976982,"re several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT101 . We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1 3.2 In order to maximize the usage of lexical knowledge, our entailment decision criterion is based on similarity s"
P11-1134,kouylekov-etal-2010-mining,1,0.859106,"Missing"
P11-1134,N10-1146,1,0.754403,"ages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge. 1 Introduction Cross-lingual Textual Entailment (CLTE) has been proposed by (Mehdad et al., 2010) as an extension of Textual Entailment (Dagan and Glickman, 2004) that consists in deciding, given two texts T and H in different languages, if the meaning of H can be inferred from the meaning of T. The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level. For instance, the reliance of current monolingual TE systems on lexical resources 1336 Marcello Federico FBK - irst Povo (Trento), Italy federico@fbk.eu (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic"
P11-1134,N10-1045,1,0.567905,"ages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge. 1 Introduction Cross-lingual Textual Entailment (CLTE) has been proposed by (Mehdad et al., 2010) as an extension of Textual Entailment (Dagan and Glickman, 2004) that consists in deciding, given two texts T and H in different languages, if the meaning of H can be inferred from the meaning of T. The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level. For instance, the reliance of current monolingual TE systems on lexical resources 1336 Marcello Federico FBK - irst Povo (Trento), Italy federico@fbk.eu (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic"
P11-1134,C02-1167,0,0.0845279,"Missing"
P11-1134,W10-0734,1,0.473862,"s obtained at each n-gram level, and optimize their relative weights, we trained a Support Vector Machine classifier, SVMlight (Joachims, 1999), using each score as a feature. Scoren = 4 Experiments on CLTE To address the first two questions outlined in Section 1, we experimented with the phrase matching method previously described, contrasting the effectiveness of lexical information extracted from parallel corpora with the knowledge provided by other resources used in the same way. 4.1 Dataset the CrowdFlower3 channel to Amazon Mechanical Turk4 (MTurk), adopting the methodology proposed by (Negri and Mehdad, 2010). The method relies on translation-validation cycles, defined as separate jobs routed to MTurk’s workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference. At each cycle, the translated hypothesis accepted by the majority of trustful validators5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job. Although the quality of the results is enhanced by the possibility to automatically weed out untru"
P11-1134,J03-1002,0,0.00279614,"ciation probabilities. They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003). There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT101 . We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities"
P11-1134,H05-1047,0,0.00878721,"available knowledge sources. Sections 3 and 4 address the first three questions, giving motivations for the use of bilingual parallel corpora in CLTE, and showing the results of our experiments. Section 5 addresses the last question, reporting on our experiments with paraphrase tables extracted from phrase tables on the monolingual RTE datasets. Section 6 concludes the paper, and outlines the directions of our future research. 2 Lexical resources for TE and CLTE All current approaches to monolingual TE, either syntactically oriented (Rus et al., 2005), or applying logical inference (Tatu and Moldovan, 2005), or adopting transformation-based techniques (Kouleykov and Magnini, 2005; Bar-Haim et al., 2008), incorporate different types of lexical knowledge to support textual inference. Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores. WordNet, the most widely used resource in TE, provides all the three types of information. Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each"
P11-1134,W09-0441,0,0.00557519,"n a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases. In our work we used available2 paraphrase databases for English and Spanish which have been extracted using the method previously outlined. Moreover, in order to experiment with different paraphrase sets providing different degrees of coverage and precision, we pruned the main paraphrase table based on the probabilities, associated to its entries, of 0.1, 0.2 and 0.3. The number of phrase pairs extracted varies from 6 million to about 80000, with an average of 3.2 words per phrase. With the second method, phrasal matches b"
P11-1134,D09-1082,0,0.0358948,"Missing"
P11-1134,P01-1067,0,0.0367702,"scenario. Contrasting results with those obtained with the most widely used resources in TE, we demonstrated the effectiveness of paraphrase tables as a mean to overcome the bias towards single words featured by the existing resources. Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE. On one side, we plan to explore alternative ways to build phrase and paraphrase tables. One possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by (Yamada and Knight, 2001). Another interesting direction is to investigate the potential of paraphrase patterns (i.e. patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009). On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge. As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment. Acknowledgments This work has been partially supported by the ECfunded project CoSyne (FP7-I"
P11-1134,C98-1013,0,\N,Missing
P12-1050,P06-1067,0,0.732893,"lated and described by a handful of linguistic rules. Notable examples are the family-unrelated Arabic-English and the related German-English language pairs. Interestingly, on these pairs, PSMT generally prevails over tree-based SMT1 , producing overall highquality outputs and isolated but critical reordering errors that undermine the global sentence meaning. Previous works on this type of language pairs have mostly focused on source reordering prior to translation (Xia and McCord, 2004; Collins et al., 2005), or on sophisticated reordering models integrated into decoding (Koehn et al., 2005; Al-Onaizan and Papineni, 2006), achieving mixed results. To merge the best of both approaches – namely, access to rich context in the former and natural coupling of reordering and translation decisions in the latter – we introduce modified distortion matrices: a novel method to seamlessly provide to the decoder a set of likely long reorderings pre-computed for a given input sentence. Added to the usual space of local permutations defined by a low distortion limit (DL), this results in a linguistically informed definition of the search space that simplifies the task of the in-decoder reordering model, besides decreasing its"
P12-1050,W11-2127,0,0.774583,"reordering for each input sentence, which is then translated monotonically (Xia and McCord, 2004) or with a low DL (Collins et al., 2005; Habash, 2007); non-deterministic reordering encodes multiple alternative reorderings into a word lattice and lets a monotonic decoder find the best path according to its models (Zhang et al., 2007; Crego and Habash, 2008; Elming and Habash, 2009; Niehues and Kolss, 2009). The latter approaches are ideally conceived as alternative to in-decoding reordering, and therefore require an exhaustive reordering rule set. Two recent works (Bisazza and Federico, 2010; Andreas et al., 2011) opt instead for a hybrid way: rules are used to generate multiple likely reorderings, but only for a specific phenomenon – namely verb-initial clauses in Arabic. This yields sparse reordering lattices that can be translated with a regular decoder performing additional reordering. Reordering rules for pre-processing are either manually written (Collins et al., 2005) or automatically learned from syntactic parses (Xia and McCord, 2004; Habash, 2007; Elming and Habash, 2009), shallow syntax chunks (Zhang et al., 2007; Crego and Habash, 2008) or part-of-speech labels (Niehues and Kolss, 2009). Si"
P12-1050,W05-0909,0,0.166472,"its authors to outperform the default word-based on an Arabic-English task. Finally, for German, we enable the Moses option monotone-atpunctuation which forbids reordering across punctuation marks. The DL is initially set to 5 words for Arabic-English and to 10 for German-English. According to our experience, these are the optimal settings for the evaluated tasks. Feature weights are optimized by minimum error training (Och, 2003) on the development sets (dev06-NW and test08). 7.2 Translation quality and efficiency results We evaluate translations with BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). As these scores are only indirectly sensitive to word order, we also compute KRS or Kendall Reordering Score (Birch et al., 2010; Bisazza et al., 2011) which is a positive score based on the Kendall’s Tau distance between the source-output and sourcereference permutations. To isolate the impact of our techniques on problematic reordering, we extract from each test set the sentences that got permuted by “oracle reordering” (see Sect. 5). These constitute about a half of the Arabic sentences, and about a third of the German. We refer to the KRS computed on these test subsets as KRS(R). Statist"
P12-1050,W10-1735,1,0.93138,"at finding a single optimal reordering for each input sentence, which is then translated monotonically (Xia and McCord, 2004) or with a low DL (Collins et al., 2005; Habash, 2007); non-deterministic reordering encodes multiple alternative reorderings into a word lattice and lets a monotonic decoder find the best path according to its models (Zhang et al., 2007; Crego and Habash, 2008; Elming and Habash, 2009; Niehues and Kolss, 2009). The latter approaches are ideally conceived as alternative to in-decoding reordering, and therefore require an exhaustive reordering rule set. Two recent works (Bisazza and Federico, 2010; Andreas et al., 2011) opt instead for a hybrid way: rules are used to generate multiple likely reorderings, but only for a specific phenomenon – namely verb-initial clauses in Arabic. This yields sparse reordering lattices that can be translated with a regular decoder performing additional reordering. Reordering rules for pre-processing are either manually written (Collins et al., 2005) or automatically learned from syntactic parses (Xia and McCord, 2004; Habash, 2007; Elming and Habash, 2009), shallow syntax chunks (Zhang et al., 2007; Crego and Habash, 2008) or part-of-speech labels (Niehu"
P12-1050,2011.eamt-1.3,0,0.0467422,"g is triggered by any verb chunk. 5 Reordering selection The number of chunk-based reorderings per sentence varies according to the rule set, to the size of chunks and to the context. A high degree of fuzziness can complicate the decoding process, leaving too much work to the in-decoding reordering model. A solution to this problem is using an external model to score the rule-generated reorderings and discard the less probable ones. In such a way, a further part of reordering complexity is taken out of decoding. At this end, instead of using a Support Vector Machine classifier as was done in (Bisazza et al., 2011), we apply reordered n-gram models that are lighterweight and more suitable for a ranking task. Differently from Feng et al. (2010), we train our models on partially reordered data and at the level of chunks. Chunks can be represented simply by their type label (such as VC or NC), but also by a combination of the type and head word, to obtain finer lexicalized distributions. LMs trained on different chunk representations can also be applied jointly, by log-linear combination. We perform reordering selection as follows: 1. Chunk-based reordering rules are applied deterministically to the source"
P12-1050,P05-1033,0,0.0490524,"well-known SMT benchmarks. 1 Introduction Despite the large research effort devoted to the modeling of word reordering, this remains one of the main obstacles to the development of accurate SMT systems for many language pairs. On one hand, the phrase-based approach (PSMT) (Och, 2002; Zens et al., 2002; Koehn et al., 2003), with its shallow and loose modeling of linguistic equivalences, appears as the most competitive choice for closely related language pairs with similar clause structures, both in terms of quality and of efficiency. On the other, tree-based approaches (Wu, 1997; Yamada, 2002; Chiang, 2005) gain advantage, at the cost of higher complexity and isomorphism assumptions, on language pairs with radically different word orders. Lying between these two extremes are language pairs where most of the reordering happens locally, and where long reorderings can be isolated and described by a handful of linguistic rules. Notable examples are the family-unrelated Arabic-English and the related German-English language pairs. Interestingly, on these pairs, PSMT generally prevails over tree-based SMT1 , producing overall highquality outputs and isolated but critical reordering errors that undermi"
P12-1050,P11-2031,0,0.0232596,"ermuted by “oracle reordering” (see Sect. 5). These constitute about a half of the Arabic sentences, and about a third of the German. We refer to the KRS computed on these test subsets as KRS(R). Statistically significant differences are assessed by approximate randomization as in (Riezler and Maxwell, 2005)13 . Tab. 1 reports results obtained by varying the DL junctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article Al+. 12 http://www2.lingsoft.fi/cgi-bin/gertwol 13 Translation scores and significance tests are computed with the tools multeval (Clark et al., 2011) and sigf (Pad´o, 2006). 484 and modifying the distortion function. To evaluate the reordering selection technique, we also compare the encoding of all rule-generated reorderings against only the 3 best per rule-matching sequence, as ranked by our best performing reordered LM (see end of Sect. 5). We mark the DL with a ‘+’ to denote that some longer jumps are being allowed by modified distortion. Run times refer to the translation of the first 100 sentences of eval08-NW and test09 by a 4-core processor. Arabic-English. As anticipated, raising the DL does not improve, but rather worsen performa"
P12-1050,P05-1066,0,0.585597,"emes are language pairs where most of the reordering happens locally, and where long reorderings can be isolated and described by a handful of linguistic rules. Notable examples are the family-unrelated Arabic-English and the related German-English language pairs. Interestingly, on these pairs, PSMT generally prevails over tree-based SMT1 , producing overall highquality outputs and isolated but critical reordering errors that undermine the global sentence meaning. Previous works on this type of language pairs have mostly focused on source reordering prior to translation (Xia and McCord, 2004; Collins et al., 2005), or on sophisticated reordering models integrated into decoding (Koehn et al., 2005; Al-Onaizan and Papineni, 2006), achieving mixed results. To merge the best of both approaches – namely, access to rich context in the former and natural coupling of reordering and translation decisions in the latter – we introduce modified distortion matrices: a novel method to seamlessly provide to the decoder a set of likely long reorderings pre-computed for a given input sentence. Added to the usual space of local permutations defined by a low distortion limit (DL), this results in a linguistically informe"
P12-1050,W08-0307,0,0.120168,"ate the proposed techniques on two popular MT tasks. 2 Previous work Pre-processing approaches to word reordering aim at permuting input words in a way that minimizes the reordering needed for translation: deterministic reordering aims at finding a single optimal reordering for each input sentence, which is then translated monotonically (Xia and McCord, 2004) or with a low DL (Collins et al., 2005; Habash, 2007); non-deterministic reordering encodes multiple alternative reorderings into a word lattice and lets a monotonic decoder find the best path according to its models (Zhang et al., 2007; Crego and Habash, 2008; Elming and Habash, 2009; Niehues and Kolss, 2009). The latter approaches are ideally conceived as alternative to in-decoding reordering, and therefore require an exhaustive reordering rule set. Two recent works (Bisazza and Federico, 2010; Andreas et al., 2011) opt instead for a hybrid way: rules are used to generate multiple likely reorderings, but only for a specific phenomenon – namely verb-initial clauses in Arabic. This yields sparse reordering lattices that can be translated with a regular decoder performing additional reordering. Reordering rules for pre-processing are either manually"
P12-1050,N04-4038,0,0.355651,"Missing"
P12-1050,W09-0809,0,0.0887519,"ues on two popular MT tasks. 2 Previous work Pre-processing approaches to word reordering aim at permuting input words in a way that minimizes the reordering needed for translation: deterministic reordering aims at finding a single optimal reordering for each input sentence, which is then translated monotonically (Xia and McCord, 2004) or with a low DL (Collins et al., 2005; Habash, 2007); non-deterministic reordering encodes multiple alternative reorderings into a word lattice and lets a monotonic decoder find the best path according to its models (Zhang et al., 2007; Crego and Habash, 2008; Elming and Habash, 2009; Niehues and Kolss, 2009). The latter approaches are ideally conceived as alternative to in-decoding reordering, and therefore require an exhaustive reordering rule set. Two recent works (Bisazza and Federico, 2010; Andreas et al., 2011) opt instead for a hybrid way: rules are used to generate multiple likely reorderings, but only for a specific phenomenon – namely verb-initial clauses in Arabic. This yields sparse reordering lattices that can be translated with a regular decoder performing additional reordering. Reordering rules for pre-processing are either manually written (Collins et al.,"
P12-1050,2010.amta-papers.22,0,0.519305,"MT community. However, they are unsuitable to model long reordering as they classify as “discontinuous” every phrase that does not immediately follow or precede the last translated one. Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al., 2010). These models are conceived to deal with long reordering, but can easily suffer from data sparseness, especially for longer jumps occurring less frequently. Following a typical sequence modeling approach, Feng et al. (2010) train n-gram language models on source data previously reordered in accordance to the target language translation. This method does not directly model reordering decisions, but rather word sequences produced by them. Despite their high perplexities, reordered LMs yield some improvements when integrated to a PSMT baseline that already includes a discriminative phrase orientation model (Zens and Ney, 2006). In this work we use similar models to rank sets of chunk permutations. Attempting to improve the reordering space definition, Yahyaei and Monz (2010) train a classifier to guess the most lik"
P12-1050,D08-1089,0,0.620207,"r distortion to more complex models that are conditioned on the words being translated. 479 The linear distortion model (Koehn et al., 2003) encourages monotonic translations by penalizing source position jumps proportionally to their length. If used alone, this model is inadequate for language pairs with different word orders. Green et al. (2010) tried to improve it with a future distortion cost estimate. Thus they were able to preserve baseline performance at a very high DL, but not to improve it. Lexicalized phrase orientation models (Tillmann, 2004; Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) predict the orientation of a phrase with respect to the last translated one. These models are known to well handle local reordering and are widely adopted by the PSMT community. However, they are unsuitable to model long reordering as they classify as “discontinuous” every phrase that does not immediately follow or precede the last translated one. Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al., 2010). These models are conceived to deal with"
P12-1050,N10-1129,0,0.724158,"in all the systems used to evaluate our methods. Another large body of work is devoted to the modeling of reordering decisions inside decoding, based on a decomposition of the problem into a sequence of basic reordering steps. Existing approaches range from basic linear distortion to more complex models that are conditioned on the words being translated. 479 The linear distortion model (Koehn et al., 2003) encourages monotonic translations by penalizing source position jumps proportionally to their length. If used alone, this model is inadequate for language pairs with different word orders. Green et al. (2010) tried to improve it with a future distortion cost estimate. Thus they were able to preserve baseline performance at a very high DL, but not to improve it. Lexicalized phrase orientation models (Tillmann, 2004; Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) predict the orientation of a phrase with respect to the last translated one. These models are known to well handle local reordering and are widely adopted by the PSMT community. However, they are unsuitable to model long reordering as they classify as “discontinuous” every phrase that does not immediately follow or preced"
P12-1050,2007.mtsummit-papers.29,0,0.128389,"f Korea, 8-14 July 2012. 2012 Association for Computational Linguistics reordered n-gram LMs and, finally, explain the notion of modified distortion matrices. In the last part of the paper, we evaluate the proposed techniques on two popular MT tasks. 2 Previous work Pre-processing approaches to word reordering aim at permuting input words in a way that minimizes the reordering needed for translation: deterministic reordering aims at finding a single optimal reordering for each input sentence, which is then translated monotonically (Xia and McCord, 2004) or with a low DL (Collins et al., 2005; Habash, 2007); non-deterministic reordering encodes multiple alternative reorderings into a word lattice and lets a monotonic decoder find the best path according to its models (Zhang et al., 2007; Crego and Habash, 2008; Elming and Habash, 2009; Niehues and Kolss, 2009). The latter approaches are ideally conceived as alternative to in-decoding reordering, and therefore require an exhaustive reordering rule set. Two recent works (Bisazza and Federico, 2010; Andreas et al., 2011) opt instead for a hybrid way: rules are used to generate multiple likely reorderings, but only for a specific phenomenon – namely"
P12-1050,W10-1710,1,0.928179,"Missing"
P12-1050,N03-1017,0,0.206016,"ode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks. 1 Introduction Despite the large research effort devoted to the modeling of word reordering, this remains one of the main obstacles to the development of accurate SMT systems for many language pairs. On one hand, the phrase-based approach (PSMT) (Och, 2002; Zens et al., 2002; Koehn et al., 2003), with its shallow and loose modeling of linguistic equivalences, appears as the most competitive choice for closely related language pairs with similar clause structures, both in terms of quality and of efficiency. On the other, tree-based approaches (Wu, 1997; Yamada, 2002; Chiang, 2005) gain advantage, at the cost of higher complexity and isomorphism assumptions, on language pairs with radically different word orders. Lying between these two extremes are language pairs where most of the reordering happens locally, and where long reorderings can be isolated and described by a handful of ling"
P12-1050,2005.iwslt-1.8,0,0.686546,"orderings can be isolated and described by a handful of linguistic rules. Notable examples are the family-unrelated Arabic-English and the related German-English language pairs. Interestingly, on these pairs, PSMT generally prevails over tree-based SMT1 , producing overall highquality outputs and isolated but critical reordering errors that undermine the global sentence meaning. Previous works on this type of language pairs have mostly focused on source reordering prior to translation (Xia and McCord, 2004; Collins et al., 2005), or on sophisticated reordering models integrated into decoding (Koehn et al., 2005; Al-Onaizan and Papineni, 2006), achieving mixed results. To merge the best of both approaches – namely, access to rich context in the former and natural coupling of reordering and translation decisions in the latter – we introduce modified distortion matrices: a novel method to seamlessly provide to the decoder a set of likely long reorderings pre-computed for a given input sentence. Added to the usual space of local permutations defined by a low distortion limit (DL), this results in a linguistically informed definition of the search space that simplifies the task of the in-decoder reorderi"
P12-1050,P07-2045,1,0.012963,"e sentence. If compared to the word reordering lattices used by Bisazza and Federico (2010) and Andreas et al. (2011), modified distortion matrices provide a more compact, implicit way to encode likely reorderings in a sentence-specific fashion. Matrix representation does not require multiplication of nodes for the same 8 In L×F mode, instead, each chunk-to-chunk jump would yield exactly one word shortcut, for a total of three. 483 7 Evaluation In this section we evaluate the impact of modified distortion matrices on two news translation tasks. Matrices were integrated into the Moses toolkit (Koehn et al., 2007) using a sentencelevel XML markup. The list of word shortcuts for each sentence is provided as an XML tag that is parsed by the decoder to modify the distortion matrix just before starting the search. As usual, the distortion matrix is queried by the distortion penalty generator and by the hypothesis expander9 . 7.1 Experimental setup For Arabic-English, we use the union of all indomain parallel corpora provided for the NIST- MT 09 evaluation10 for a total of 986K sentences, 31M English words. The target LM is trained on the English side of all available NIST- MT 09 parallel data, UN included"
P12-1050,N06-1014,0,0.065959,"tion class of a new hypothesis, thus they are not affected by changes in the matrix. 10 That is everything except the small GALE corpus and the UN corpus. As reported by Green et al. (2010) the removal of UN data does not affect baseline performances on news test. 11 The Arabic Treebank tokenization scheme isolates conand compound splitting is performed with Tree Tagger (Schmid, 1994) and the Gertwol morphological analyser (Koskenniemi and Haapalainen, 1994)12 . Using Moses we build competitive baselines on the training data described above. Word alignment is produced by the Berkeley Aligner (Liang et al., 2006). The decoder is based on the log-linear combination of a phrase translation model, a lexicalized reordering model, a 6-gram target language model, distortion cost, word and phrase penalties. The reordering model is a hierarchical phrase orientation model (Tillmann, 2004; Koehn et al., 2005; Galley and Manning, 2008) trained on all the available parallel data. We choose the hierarchical variant, as it was shown by its authors to outperform the default word-based on an Arabic-English task. Finally, for German, we enable the Moses option monotone-atpunctuation which forbids reordering across pun"
P12-1050,W09-0435,0,0.041795,"ks. 2 Previous work Pre-processing approaches to word reordering aim at permuting input words in a way that minimizes the reordering needed for translation: deterministic reordering aims at finding a single optimal reordering for each input sentence, which is then translated monotonically (Xia and McCord, 2004) or with a low DL (Collins et al., 2005; Habash, 2007); non-deterministic reordering encodes multiple alternative reorderings into a word lattice and lets a monotonic decoder find the best path according to its models (Zhang et al., 2007; Crego and Habash, 2008; Elming and Habash, 2009; Niehues and Kolss, 2009). The latter approaches are ideally conceived as alternative to in-decoding reordering, and therefore require an exhaustive reordering rule set. Two recent works (Bisazza and Federico, 2010; Andreas et al., 2011) opt instead for a hybrid way: rules are used to generate multiple likely reorderings, but only for a specific phenomenon – namely verb-initial clauses in Arabic. This yields sparse reordering lattices that can be translated with a regular decoder performing additional reordering. Reordering rules for pre-processing are either manually written (Collins et al., 2005) or automatically le"
P12-1050,P03-1021,0,0.0194585,"ntation model (Tillmann, 2004; Koehn et al., 2005; Galley and Manning, 2008) trained on all the available parallel data. We choose the hierarchical variant, as it was shown by its authors to outperform the default word-based on an Arabic-English task. Finally, for German, we enable the Moses option monotone-atpunctuation which forbids reordering across punctuation marks. The DL is initially set to 5 words for Arabic-English and to 10 for German-English. According to our experience, these are the optimal settings for the evaluated tasks. Feature weights are optimized by minimum error training (Och, 2003) on the development sets (dev06-NW and test08). 7.2 Translation quality and efficiency results We evaluate translations with BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). As these scores are only indirectly sensitive to word order, we also compute KRS or Kendall Reordering Score (Birch et al., 2010; Bisazza et al., 2011) which is a positive score based on the Kendall’s Tau distance between the source-output and sourcereference permutations. To isolate the impact of our techniques on problematic reordering, we extract from each test set the sentences that got permuted by “"
P12-1050,P02-1040,0,0.0880848,"chical variant, as it was shown by its authors to outperform the default word-based on an Arabic-English task. Finally, for German, we enable the Moses option monotone-atpunctuation which forbids reordering across punctuation marks. The DL is initially set to 5 words for Arabic-English and to 10 for German-English. According to our experience, these are the optimal settings for the evaluated tasks. Feature weights are optimized by minimum error training (Och, 2003) on the development sets (dev06-NW and test08). 7.2 Translation quality and efficiency results We evaluate translations with BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). As these scores are only indirectly sensitive to word order, we also compute KRS or Kendall Reordering Score (Birch et al., 2010; Bisazza et al., 2011) which is a positive score based on the Kendall’s Tau distance between the source-output and sourcereference permutations. To isolate the impact of our techniques on problematic reordering, we extract from each test set the sentences that got permuted by “oracle reordering” (see Sect. 5). These constitute about a half of the Arabic sentences, and about a third of the German. We refer to the KRS computed on"
P12-1050,W05-0908,0,0.124297,"RS or Kendall Reordering Score (Birch et al., 2010; Bisazza et al., 2011) which is a positive score based on the Kendall’s Tau distance between the source-output and sourcereference permutations. To isolate the impact of our techniques on problematic reordering, we extract from each test set the sentences that got permuted by “oracle reordering” (see Sect. 5). These constitute about a half of the Arabic sentences, and about a third of the German. We refer to the KRS computed on these test subsets as KRS(R). Statistically significant differences are assessed by approximate randomization as in (Riezler and Maxwell, 2005)13 . Tab. 1 reports results obtained by varying the DL junctions w+ and f+, prepositions l+, k+, b+, future marker s+, pronominal suffixes, but not the article Al+. 12 http://www2.lingsoft.fi/cgi-bin/gertwol 13 Translation scores and significance tests are computed with the tools multeval (Clark et al., 2011) and sigf (Pad´o, 2006). 484 and modifying the distortion function. To evaluate the reordering selection technique, we also compare the encoding of all rule-generated reorderings against only the 3 best per rule-matching sequence, as ranked by our best performing reordered LM (see end of S"
P12-1050,N04-4026,0,0.812663,"dering steps. Existing approaches range from basic linear distortion to more complex models that are conditioned on the words being translated. 479 The linear distortion model (Koehn et al., 2003) encourages monotonic translations by penalizing source position jumps proportionally to their length. If used alone, this model is inadequate for language pairs with different word orders. Green et al. (2010) tried to improve it with a future distortion cost estimate. Thus they were able to preserve baseline performance at a very high DL, but not to improve it. Lexicalized phrase orientation models (Tillmann, 2004; Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) predict the orientation of a phrase with respect to the last translated one. These models are known to well handle local reordering and are widely adopted by the PSMT community. However, they are unsuitable to model long reordering as they classify as “discontinuous” every phrase that does not immediately follow or precede the last translated one. Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of le"
P12-1050,J97-3002,0,0.373619,"nd German-English using well-known SMT benchmarks. 1 Introduction Despite the large research effort devoted to the modeling of word reordering, this remains one of the main obstacles to the development of accurate SMT systems for many language pairs. On one hand, the phrase-based approach (PSMT) (Och, 2002; Zens et al., 2002; Koehn et al., 2003), with its shallow and loose modeling of linguistic equivalences, appears as the most competitive choice for closely related language pairs with similar clause structures, both in terms of quality and of efficiency. On the other, tree-based approaches (Wu, 1997; Yamada, 2002; Chiang, 2005) gain advantage, at the cost of higher complexity and isomorphism assumptions, on language pairs with radically different word orders. Lying between these two extremes are language pairs where most of the reordering happens locally, and where long reorderings can be isolated and described by a handful of linguistic rules. Notable examples are the family-unrelated Arabic-English and the related German-English language pairs. Interestingly, on these pairs, PSMT generally prevails over tree-based SMT1 , producing overall highquality outputs and isolated but critical r"
P12-1050,C04-1073,0,0.401205,"between these two extremes are language pairs where most of the reordering happens locally, and where long reorderings can be isolated and described by a handful of linguistic rules. Notable examples are the family-unrelated Arabic-English and the related German-English language pairs. Interestingly, on these pairs, PSMT generally prevails over tree-based SMT1 , producing overall highquality outputs and isolated but critical reordering errors that undermine the global sentence meaning. Previous works on this type of language pairs have mostly focused on source reordering prior to translation (Xia and McCord, 2004; Collins et al., 2005), or on sophisticated reordering models integrated into decoding (Koehn et al., 2005; Al-Onaizan and Papineni, 2006), achieving mixed results. To merge the best of both approaches – namely, access to rich context in the former and natural coupling of reordering and translation decisions in the latter – we introduce modified distortion matrices: a novel method to seamlessly provide to the decoder a set of likely long reorderings pre-computed for a given input sentence. Added to the usual space of local permutations defined by a low distortion limit (DL), this results in a"
P12-1050,2010.iwslt-papers.19,0,0.422847,"llowing a typical sequence modeling approach, Feng et al. (2010) train n-gram language models on source data previously reordered in accordance to the target language translation. This method does not directly model reordering decisions, but rather word sequences produced by them. Despite their high perplexities, reordered LMs yield some improvements when integrated to a PSMT baseline that already includes a discriminative phrase orientation model (Zens and Ney, 2006). In this work we use similar models to rank sets of chunk permutations. Attempting to improve the reordering space definition, Yahyaei and Monz (2010) train a classifier to guess the most likely jump length at each source position, then use its predictions to dynamically set the DL. Translation improvements are obtained on a simple task with mostly short sentences (BTEC). Modifying the distortion function, as proposed in this paper, makes it possible to expand the pemutation search space by a much finer degree than varying the DL does. 3 Long reordering patterns Our study focuses on Arabic-English and GermanEnglish: two language pairs characterized by uneven distributions of word-reordering phenomena, with long-range movements concentrating"
P12-1050,W06-3108,0,0.361092,"Missing"
P12-1050,2002.tmi-tutorials.2,0,0.0700091,"ion. Finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks. 1 Introduction Despite the large research effort devoted to the modeling of word reordering, this remains one of the main obstacles to the development of accurate SMT systems for many language pairs. On one hand, the phrase-based approach (PSMT) (Och, 2002; Zens et al., 2002; Koehn et al., 2003), with its shallow and loose modeling of linguistic equivalences, appears as the most competitive choice for closely related language pairs with similar clause structures, both in terms of quality and of efficiency. On the other, tree-based approaches (Wu, 1997; Yamada, 2002; Chiang, 2005) gain advantage, at the cost of higher complexity and isomorphism assumptions, on language pairs with radically different word orders. Lying between these two extremes are language pairs where most of the reordering happens locally, and where long reorderings can be isolated and described"
P12-1050,W07-0401,0,0.116683,"Missing"
P12-1050,C08-1144,0,0.0529759,"results in a linguistically informed definition of the search space that simplifies the task of the in-decoder reordering model, besides decreasing its complexity. The paper is organized as follows. After reviewing a selection of relevant works, we analyze salient reordering patterns in Arabic-English and GermanEnglish, and describe the corresponding chunkbased reordering rule sets. In the following sections we present a reordering selection technique based on 1 A good comparison of phrase-based and tree-based approaches across language pairs with different reordering levels can be found in (Zollmann et al., 2008). 478 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 478–487, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics reordered n-gram LMs and, finally, explain the notion of modified distortion matrices. In the last part of the paper, we evaluate the proposed techniques on two popular MT tasks. 2 Previous work Pre-processing approaches to word reordering aim at permuting input words in a way that minimizes the reordering needed for translation: deterministic reordering aims at finding a single optimal reordering"
P12-2024,carreras-etal-2004-freeling,0,0.169167,"Missing"
P12-2024,de-marneffe-etal-2006-generating,0,0.0595464,"Missing"
P12-2024,P05-1045,0,0.0381264,"Missing"
P12-2024,P07-2045,1,0.00822022,"s, SPTs are extracted from parallel corpora. As a first step we annotate the parallel corpora with named-entity taggers for the source and target languages, replacing named entities with general semantic labels chosen from a coarse-grained taxonomy (person, location, organization, date and numeric expression). Then, we combine the sequences of unique labels into one single token of the same label, and we run Giza++ (Och and Ney, 2000) to align the resulting semantically augmented corpora. Finally, we extract the semantic phrase table from the augmented aligned corpora using the Moses toolkit (Koehn et al., 2007). For the matching phase, we first annotate T and H in the same way we labeled our parallel corpora. Then, for each n-gram order (n=1 to 5) we use the SPT to calculate a matching score as the number of n-grams in H that match with phrases in T divided by the number of n-grams in H.1 Dependency Relation (DR) matching targets the increase of CLTE precision. Adding syntactic constraints to the matching process, DR features aim to reduce the amount of wrong matches often occurring with bag-of-words methods (both at the lexical level and with recall-oriented SPTs). For instance, the contradiction b"
P12-2024,P10-4008,1,0.231125,"Missing"
P12-2024,W11-2404,1,0.892041,"Missing"
P12-2024,N10-1045,1,0.424311,"pairs) transformed into their cross-lingual counterpart by translating the hypotheses into other languages (Negri and Mehdad, 2010), and ii) machine translation (MT) evaluation datasets (Mehdad et al., 2012). Instead, we experiment with the only corpus representative of the multilingual content synchronization scenario, and the richer inventory of phenomena arising from it (multi-directional entailment relations). (b) Improvement of current CLTE methods. The CLTE methods proposed so far adopt either a “pivoting approach” based on the translation of the two input texts into the same language (Mehdad et al., 2010), or an “integrated solution” that exploits bilingual phrase tables to capture lexical relations and contextual information (Mehdad et al., 2011). The promising results achieved with the integrated approach, however, still rely on phrasal matching techniques that disregard relevant semantic aspects of the problem. By filling this gap integrating linguistically motivated features, we propose a novel approach that improves the state-of-the-art in CLTE. 2 CLTE-based content synchronization CLTE has been proposed by (Mehdad et al., 2010) as an extension of textual entailment which consists of deci"
P12-2024,P11-1134,1,0.505816,"hine translation (MT) evaluation datasets (Mehdad et al., 2012). Instead, we experiment with the only corpus representative of the multilingual content synchronization scenario, and the richer inventory of phenomena arising from it (multi-directional entailment relations). (b) Improvement of current CLTE methods. The CLTE methods proposed so far adopt either a “pivoting approach” based on the translation of the two input texts into the same language (Mehdad et al., 2010), or an “integrated solution” that exploits bilingual phrase tables to capture lexical relations and contextual information (Mehdad et al., 2011). The promising results achieved with the integrated approach, however, still rely on phrasal matching techniques that disregard relevant semantic aspects of the problem. By filling this gap integrating linguistically motivated features, we propose a novel approach that improves the state-of-the-art in CLTE. 2 CLTE-based content synchronization CLTE has been proposed by (Mehdad et al., 2010) as an extension of textual entailment which consists of deciding, given a text T and an hypothesis H in different languages, if the meaning of H can be inferred from the meaning of T. The adoption of entai"
P12-2024,W12-3122,1,0.51127,"et such problem as an application-oriented, crosslingual variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). Along this direction, we make two main contributions: (a) Experiments with multi-directional crosslingual textual entailment. So far, cross-lingual textual entailment (CLTE) has been only applied to: i) available TE datasets (uni-directional relations between monolingual pairs) transformed into their cross-lingual counterpart by translating the hypotheses into other languages (Negri and Mehdad, 2010), and ii) machine translation (MT) evaluation datasets (Mehdad et al., 2012). Instead, we experiment with the only corpus representative of the multilingual content synchronization scenario, and the richer inventory of phenomena arising from it (multi-directional entailment relations). (b) Improvement of current CLTE methods. The CLTE methods proposed so far adopt either a “pivoting approach” based on the translation of the two input texts into the same language (Mehdad et al., 2010), or an “integrated solution” that exploits bilingual phrase tables to capture lexical relations and contextual information (Mehdad et al., 2011). The promising results achieved with the i"
P12-2024,W10-0734,1,0.400732,"e informative with respect to the content of the other page. In this paper we set such problem as an application-oriented, crosslingual variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). Along this direction, we make two main contributions: (a) Experiments with multi-directional crosslingual textual entailment. So far, cross-lingual textual entailment (CLTE) has been only applied to: i) available TE datasets (uni-directional relations between monolingual pairs) transformed into their cross-lingual counterpart by translating the hypotheses into other languages (Negri and Mehdad, 2010), and ii) machine translation (MT) evaluation datasets (Mehdad et al., 2012). Instead, we experiment with the only corpus representative of the multilingual content synchronization scenario, and the richer inventory of phenomena arising from it (multi-directional entailment relations). (b) Improvement of current CLTE methods. The CLTE methods proposed so far adopt either a “pivoting approach” based on the translation of the two input texts into the same language (Mehdad et al., 2010), or an “integrated solution” that exploits bilingual phrase tables to capture lexical relations and contextual"
P12-2024,D11-1062,1,0.47246,"ed words can be either the same, or semantically equivalent terms in the two languages (e.g. according to a bilingual dictionary). Given the dependency tree representations of T and H, for each grammatical relation (r) we calculate a DR matching score as the number of matching occurrences of r in T and H, divided by the number of occurrences of r in H. Separate DR matching scores are calculated for each relation r appearing both in T and H. 4 Experiments and results 4.1 Content synchronization scenario In our first experiment we used the English-German portion of the CLTE corpus described in (Negri et al., 2011), consisting of 500 multi-directional entailment pairs which we equally divided into training and test sets. Each pair in the dataset is annotated with “Bidirectional”, “Forward”, or “Backward” entailment judgements. Although highly relevant for the content synchronization task, “Contradiction” and “Unknown” cases (i.e. “NO” entailment in both directions) are not present in the annotation. However, this is the only available dataset suitable to gather insights about the viability of our approach to multi-directional CLTE recognition.2 We chose the ENG-GER portion of the dataset since for such"
P12-2024,S12-1053,1,0.350796,"taset since for such language pair MT systems performance is often lower, making the adoption of simpler solutions based on pivoting more vulnerable. To build the English-German phrase tables we combined the Europarl, News Commentary and “denews”3 parallel corpora. After tokenization, Giza++ and Moses were respectively used to align the corpora and extract a lexical phrase table (PT). Similarly, the semantic phrase table (SPT) has been ex2 Recently, a new dataset including “Unknown” pairs has been used in the “Cross-Lingual Textual Entailment for Content Synchronization” task at SemEval-2012 (Negri et al., 2012). 3 http://homepages.inf.ed.ac.uk/pkoehn/ 122 tracted from the same corpora annotated with the Stanford NE tagger (Faruqui and Pad´o, 2010; Finkel et al., 2005). Dependency relations (DR) have been extracted running the Stanford parser (Rafferty and Manning, 2008; De Marneffe et al., 2006). The dictionary created during the alignment of the parallel corpora provided the lexical knowledge to perform matches when the connected words are different, but semantically equivalent in the two languages. To combine and weight features at different levels we used SVMlight (Joachims, 1999) with default pa"
P12-2024,P00-1056,0,0.0178874,"place of “out of vocabulary” terms (e.g. unseen person names) is an effective way to improve CLTE performance, even at the cost of some loss in precision. Like lexical phrase tables, SPTs are extracted from parallel corpora. As a first step we annotate the parallel corpora with named-entity taggers for the source and target languages, replacing named entities with general semantic labels chosen from a coarse-grained taxonomy (person, location, organization, date and numeric expression). Then, we combine the sequences of unique labels into one single token of the same label, and we run Giza++ (Och and Ney, 2000) to align the resulting semantically augmented corpora. Finally, we extract the semantic phrase table from the augmented aligned corpora using the Moses toolkit (Koehn et al., 2007). For the matching phase, we first annotate T and H in the same way we labeled our parallel corpora. Then, for each n-gram order (n=1 to 5) we use the SPT to calculate a matching score as the number of n-grams in H that match with phrases in T divided by the number of n-grams in H.1 Dependency Relation (DR) matching targets the increase of CLTE precision. Adding syntactic constraints to the matching process, DR feat"
P12-2024,W08-1006,0,0.0178413,"Missing"
P15-2087,P15-1022,1,0.749626,"Missing"
P15-2087,C14-2028,1,0.898342,"Missing"
P15-2087,P14-1081,0,0.0138533,"impact of QE predictions on translators’ productivity is analysed by measuring the number of words that can be post-edited in a fixed amount of time. The evaluation, however, only concentrates on the use of QE to rank MT outputs, and the gains in translation speed are measured against the contrastive condition in which no QE-based ranking mechanism is used. In this artificial scenario, the analysis disregards the relation The usefulness of translation quality estimation (QE) to increase productivity in a computer-assisted translation (CAT) framework is a widely held assumption (Specia, 2011; Huang et al., 2014). So far, however, the validity of this assumption has not been yet demonstrated through sound evaluations in realistic settings. To this aim, we report on an evaluation involving professional translators operating with a CAT tool in controlled but natural conditions. Contrastive experiments are carried out by measuring post-editing time differences when: i) translation suggestions are presented together with binary quality estimates, and ii) the same suggestions are presented without quality indicators. Translators’ productivity in the two conditions is analysed in a principled way, accountin"
P15-2087,W12-3122,1,0.85332,"counting for the main factors (e.g. differences in translators’ behaviour, quality of the suggestions) that directly impact on time measurements. While the general assumption about the usefulness of QE is verified, significance testing results reveal that real productivity gains can be observed only under specific conditions. 1 Introduction Machine translation (MT) quality estimation aims to automatically predict the expected time (e.g. in seconds) or effort (e.g. number of editing operations) required to correct machine-translated sentences into publishable translations (Specia et al., 2009; Mehdad et al., 2012; Turchi et al., 2014a; C. de Souza et al., 2015). In principle, the task has a number of practical applications. An intuitive one is speeding-up the work of human translators operating with a CAT tool, a software de1 Notice that the same sentence cannot be post-edited twice (e.g. with/without quality labels) by the same translator without introducing a bias in the time measurements. 530 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 530–535, c Beijing, China"
P15-2087,shah-etal-2014-efficient,1,0.808839,"parate binary QE classifier on the labeled samples. For this purpose we use the Scikit-learn implementation of support vector machines (Pedregosa et al., 2011), training our models with the 17 baseline features proposed by Specia et al. (2009). This feature set mainly takes into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the target translation (e.g. language model probabilities). The features are extracted from the data available at prediction time (source text and raw MT output) by using an adapted version (Shah et al., 2014) of the open-source QuEst software (Specia et al., 2013). The SVM parameters are optimized by cross-validation on the training set. With these classifiers, we finally assign quality flags to the raw segment translations in the test • Does QE really help in the CAT scenario? • If yes, under what conditions? 2 Experimental Setup One of the key questions in utilising QE in the CAT scenario is how to relay QE information to the user. In our experiments, we evaluate a way of visualising MT quality estimates that is based on a color-coded binary classification (green vs. red) as an alternative to re"
P15-2087,2006.amta-papers.25,0,0.122715,"up for a between-subject comparison on a single long document as follows. First, the document is split in two parts. The first part serves as the training portion for a binary quality estimator; the second part is reserved for evaluation. The training portion is machine-translated with a state-of-the-art, phrasebased Moses system (Koehn et al., 2007)2 and post-edited under standard conditions (i.e. without visualising QE information) by the same users involved in the testing phase. Based on their postedits, the raw MT output samples are then labeled as ‘good’ or ‘bad’ by considering the HTER (Snover et al., 2006) calculated between raw MT output and its post-edited version.3 Our labeling criterion follows the empirical findings of (Turchi et al., 2013; Turchi et al., 2014b), which indicate an HTER value of 0.4 as boundary between posteditable (HTER ≤ 0.4) and useless suggestions (HTER> 0.4). Then, to model the subjective concept of quality of different subjects, for of each translator we train a separate binary QE classifier on the labeled samples. For this purpose we use the Scikit-learn implementation of support vector machines (Pedregosa et al., 2011), training our models with the 17 baseline featu"
P15-2087,2009.eamt-1.5,1,0.923217,"a principled way, accounting for the main factors (e.g. differences in translators’ behaviour, quality of the suggestions) that directly impact on time measurements. While the general assumption about the usefulness of QE is verified, significance testing results reveal that real productivity gains can be observed only under specific conditions. 1 Introduction Machine translation (MT) quality estimation aims to automatically predict the expected time (e.g. in seconds) or effort (e.g. number of editing operations) required to correct machine-translated sentences into publishable translations (Specia et al., 2009; Mehdad et al., 2012; Turchi et al., 2014a; C. de Souza et al., 2015). In principle, the task has a number of practical applications. An intuitive one is speeding-up the work of human translators operating with a CAT tool, a software de1 Notice that the same sentence cannot be post-edited twice (e.g. with/without quality labels) by the same translator without introducing a bias in the time measurements. 530 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 530–"
P15-2087,P13-4014,0,0.0522816,"Missing"
P15-2087,2011.eamt-1.12,0,0.0408152,"al., 2013; Bojar et al., 2014). On-field evaluation is indeed a complex task, as it requires: i) the availability of a CAT tool capable to integrate MT QE functionalities, ii) professional translators used to MT post-editing, iii) a sound evaluation protocol to perform betweensubject comparisons,1 and iv) robust analysis techniques to measure statistical significance under variable conditions (e.g. differences in users’ postediting behavior). To bypass these issues, the works more closely related to our investigation resort to controlled and simplified evaluation protocols. For instance, in (Specia, 2011) the impact of QE predictions on translators’ productivity is analysed by measuring the number of words that can be post-edited in a fixed amount of time. The evaluation, however, only concentrates on the use of QE to rank MT outputs, and the gains in translation speed are measured against the contrastive condition in which no QE-based ranking mechanism is used. In this artificial scenario, the analysis disregards the relation The usefulness of translation quality estimation (QE) to increase productivity in a computer-assisted translation (CAT) framework is a widely held assumption (Specia, 20"
P15-2087,W13-2231,1,0.887438,"the training portion for a binary quality estimator; the second part is reserved for evaluation. The training portion is machine-translated with a state-of-the-art, phrasebased Moses system (Koehn et al., 2007)2 and post-edited under standard conditions (i.e. without visualising QE information) by the same users involved in the testing phase. Based on their postedits, the raw MT output samples are then labeled as ‘good’ or ‘bad’ by considering the HTER (Snover et al., 2006) calculated between raw MT output and its post-edited version.3 Our labeling criterion follows the empirical findings of (Turchi et al., 2013; Turchi et al., 2014b), which indicate an HTER value of 0.4 as boundary between posteditable (HTER ≤ 0.4) and useless suggestions (HTER> 0.4). Then, to model the subjective concept of quality of different subjects, for of each translator we train a separate binary QE classifier on the labeled samples. For this purpose we use the Scikit-learn implementation of support vector machines (Pedregosa et al., 2011), training our models with the 17 baseline features proposed by Specia et al. (2009). This feature set mainly takes into account the complexity of the source sentence (e.g. number of tokens"
P15-2087,P14-1067,1,0.894194,"Missing"
P15-2087,W12-3102,0,\N,Missing
P15-2087,P07-2045,1,\N,Missing
P15-2087,W05-0908,0,\N,Missing
P15-2087,W13-2201,0,\N,Missing
P16-4009,W13-2243,1,0.90459,"Missing"
P16-4009,W14-3340,1,0.905877,"Missing"
P16-4009,W15-5204,0,0.0456436,"ies: translation memories (TM - a high-precision mechanism for storing and retrieving previously translated segments) and machine translation (MT - a high-recall technology for translating unseen 49 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 49–54, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tions by the user. For this reason, most prior works on TM technology focused on improving this aspect (Gupta et al., 2014; Bloodgood and Strauss, 2014; Vanallemeersch and Vandeghinste, 2015; Chatzitheodoroou, 2015; Gupta et al., 2015). The other relevant factor, TU quality, relates to the reliability of the target translations. Indeed, a perfectly matching source text associated to a wrong translation would make the corresponding suggestion useless or, even worse, an obstacle to productivity. On this aspect, prior research is limited to the work proposed in (Barbu, 2015), which so far represents the only attempt to automatically spot false translations in the bi-segments of a TM. However, casting the problem as a supervised binary classification task, this approach highly depends on the availability of"
P16-4009,W08-0509,0,0.102406,"d deviation values for a given indicator (e.g. sentence length ratio, proportion of aligned words), quantiles or std counts in case of normal value distributions will be used as decision boundaries. Then, in the decision step, each filter uses the gathered information to decide about each TU. At the end of this process, for each 3 The tool has been recently used also in the unsupervised approach by Jalili Sabet et al. (2016). 51 models can be trained on the whole TM with one of the many existing word aligners. For instance, the results of WE filters reported in §4 were obtained using MGIZA++ (Gao and Vogel, 2008). TU the policy manager collects all the decisions taken by the filters and applies the policy set by the user in the configuration file to assign an accept or reject judgment. The final labels, the TUs and the filters outputs are saved in different files. 3.2 Word embedding filters (5). Cross-lingual word embeddings provide a common vector representation for words in different languages and allow looking at the source and target segments at the same time. In TMop, they are computed using the method proposed in (Søgaard et al., 2015) but, instead of considering bilingual documents as atomic co"
P16-4009,2014.tc-1.10,0,0.0168299,"des. Advanced CAT tools currently integrate the strengths of two complementary technologies: translation memories (TM - a high-precision mechanism for storing and retrieving previously translated segments) and machine translation (MT - a high-recall technology for translating unseen 49 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 49–54, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tions by the user. For this reason, most prior works on TM technology focused on improving this aspect (Gupta et al., 2014; Bloodgood and Strauss, 2014; Vanallemeersch and Vandeghinste, 2015; Chatzitheodoroou, 2015; Gupta et al., 2015). The other relevant factor, TU quality, relates to the reliability of the target translations. Indeed, a perfectly matching source text associated to a wrong translation would make the corresponding suggestion useless or, even worse, an obstacle to productivity. On this aspect, prior research is limited to the work proposed in (Barbu, 2015), which so far represents the only attempt to automatically spot false translations in the bi-segments of a TM. However, casting the problem as"
P16-4009,W15-4905,0,0.0648637,"Missing"
P16-4009,P16-2047,1,0.847076,"the learning step, each filter i iterates over the TM or a subset of it to gather the basic statistics needed to define its accept/reject criteria. For instance, by computing mean and standard deviation values for a given indicator (e.g. sentence length ratio, proportion of aligned words), quantiles or std counts in case of normal value distributions will be used as decision boundaries. Then, in the decision step, each filter uses the gathered information to decide about each TU. At the end of this process, for each 3 The tool has been recently used also in the unsupervised approach by Jalili Sabet et al. (2016). 51 models can be trained on the whole TM with one of the many existing word aligners. For instance, the results of WE filters reported in §4 were obtained using MGIZA++ (Gao and Vogel, 2008). TU the policy manager collects all the decisions taken by the filters and applies the policy set by the user in the configuration file to assign an accept or reject judgment. The final labels, the TUs and the filters outputs are saved in different files. 3.2 Word embedding filters (5). Cross-lingual word embeddings provide a common vector representation for words in different languages and allow looking"
P16-4009,P12-3005,0,0.0385588,"offered by commercial TM cleaning tools. They capture translation quality by looking at surface aspects, such as the possible mismatches in the number of dates, numbers, URLs, XML tags, ref and image tags present in the source and target segments. Other filters model the similarity between source and target by computing the direct and inverse ratio between the number of characters and words, as well as the average word length in the two segments. Finally, two filters look for uncommon character or word repetitions. Language identification filter (1). This filter (LI) exploits the Langid tool (Lui and Baldwin, 2012) to verify the consistency between the source and target languages of a TU and those indicated in the TM. Though simple, it is quite effective since often the two languages are inverted or even completely different from the expected ones. 3.3 Policies Decision policies allow TMop combining the output of the active filters into a final decision for each TU. Simple decision-making strategies can consider the number of accept and reject judgments, but more complex methods can be easily implemented by the user (both filters and policy managers can be easily modified and extended by exploiting well"
P16-4009,P12-3000,0,0.28566,"Missing"
P16-4009,W15-5202,0,0.349279,"ion for Computational Linguistics tions by the user. For this reason, most prior works on TM technology focused on improving this aspect (Gupta et al., 2014; Bloodgood and Strauss, 2014; Vanallemeersch and Vandeghinste, 2015; Chatzitheodoroou, 2015; Gupta et al., 2015). The other relevant factor, TU quality, relates to the reliability of the target translations. Indeed, a perfectly matching source text associated to a wrong translation would make the corresponding suggestion useless or, even worse, an obstacle to productivity. On this aspect, prior research is limited to the work proposed in (Barbu, 2015), which so far represents the only attempt to automatically spot false translations in the bi-segments of a TM. However, casting the problem as a supervised binary classification task, this approach highly depends on the availability of labelled training data. Our work goes beyond the initial effort of Barbu (2015) in two ways. First, we propose a configurable and extensible open source framework for TM cleaning. In this way, we address the demand of easy-to-use TM management tools whose development is out of the reach of individual translators and translation companies. Such demand is not onl"
P16-4009,W12-3122,1,0.840455,"ning methods (e.g. Apsic X-Bench1 ) only implement very simple syntactic checks (e.g. repetitions, opening/closing tags consistency). These are insufficient to capture the variety of errors that can be encountered in a TM (especially in the public ones). Second, our approach to TM cleaning is fully unsupervised. This is to cope with the lack of labelled training data which, due to the high acquisition costs, represents a bottleneck rendering supervised solutions unpractical. It is worth remarking that also current approaches to tasks closely related to TM cleaning (e.g. MT quality estimation (Mehdad et al., 2012; C. de Souza et al., 2014)) suffer from the same problem. Besides not being customised for the specificities of the TM cleaning scenario (their usefulness for the task should be demonstrated), their dependence on labelled 1 training data is a strong requirement from the TM cleaning application perspective. 2 The TM cleaning task The identification of “bad” TUs is a multifaceted problem. First, it deals with the recognition of a variety of errors. These include: • Surface errors, such as opening/closing tags inconsistencies and empty or suspiciously long/short translations; • Language inconsis"
P16-4009,E14-1022,0,0.0268135,"ols currently integrate the strengths of two complementary technologies: translation memories (TM - a high-precision mechanism for storing and retrieving previously translated segments) and machine translation (MT - a high-recall technology for translating unseen 49 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 49–54, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tions by the user. For this reason, most prior works on TM technology focused on improving this aspect (Gupta et al., 2014; Bloodgood and Strauss, 2014; Vanallemeersch and Vandeghinste, 2015; Chatzitheodoroou, 2015; Gupta et al., 2015). The other relevant factor, TU quality, relates to the reliability of the target translations. Indeed, a perfectly matching source text associated to a wrong translation would make the corresponding suggestion useless or, even worse, an obstacle to productivity. On this aspect, prior research is limited to the work proposed in (Barbu, 2015), which so far represents the only attempt to automatically spot false translations in the bi-segments of a TM. However, casting the problem as a supervised binary classific"
P16-4009,P15-1165,0,0.122512,"Missing"
P16-4009,W15-4920,0,0.0347765,"trengths of two complementary technologies: translation memories (TM - a high-precision mechanism for storing and retrieving previously translated segments) and machine translation (MT - a high-recall technology for translating unseen 49 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 49–54, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tions by the user. For this reason, most prior works on TM technology focused on improving this aspect (Gupta et al., 2014; Bloodgood and Strauss, 2014; Vanallemeersch and Vandeghinste, 2015; Chatzitheodoroou, 2015; Gupta et al., 2015). The other relevant factor, TU quality, relates to the reliability of the target translations. Indeed, a perfectly matching source text associated to a wrong translation would make the corresponding suggestion useless or, even worse, an obstacle to productivity. On this aspect, prior research is limited to the work proposed in (Barbu, 2015), which so far represents the only attempt to automatically spot false translations in the bi-segments of a TM. However, casting the problem as a supervised binary classification task, this approach highly depend"
P16-4009,2015.eamt-1.6,0,\N,Missing
P18-2049,W18-1810,1,0.748554,"U (Papineni et al., 2002) metric and the Multeval (Clark et al., 2011) significance test. The performance of NMT models in translating each language using different vocabulary units and encoder input representations can be seen in Table 4. With the simple model, LMVR based units achieve the best accuracy in translating all languages, with improvements over BPE by 0.85 to 1.09 BLEU points in languages with high morphological complexity (Arabic, Czech and Turkish) and 0.32 to 0.53 BLEU points in languages with low to medium complexity (Italian and German). This confirms our previous results in (Ataman and Federico, 2018). Moreover, simple models using character trigrams as vocabulary units reach much higher translation accuracy compared to models using characters, indicating their superior performance in handling contextual ambiguity. In the Italian to English translation direction, the performance of simple models using character trigrams and BPE sub-word units as input representations are almost comparable, showing that character trigrams can even be sufficient as the standalone vocabulary units in languages with low lexical sparseness. These findings suggest that each type of sub-word unit used in the simp"
P18-2049,W10-2211,0,0.0681825,"Cs), German (De), Italian (It) and Turk1 The International Workshop on Spoken Language Translation with shared tasks organized between 2003-2017. 307 5 The compositional model, on the other hand, performs NMT with input representations composed from sub-lexical vocabulary units. In our study, we evaluate representations composed from character trigrams, BPE, and LMVR units. In order to choose the segmentation method to apply on the English side (the output of NMT decoder), we compare BPE and LMVR sub-word units by carrying out an evaluation on the official data sets of Morpho Challenge 20102 (Kurimo et al., 2010). The results of this evaluation, as given in Table 3, suggest that LMVR seems to provide a segmentation that is more consistent with morpheme boundaries, which motivates us to use sub-word tokens generated by LMVR for the target side. This choice aids us in evaluating the morphological knowledge contained in input representations in terms of the translation accuracy in NMT. The compositional bi-RNN layer is implemented in Theano (Team et al., 2016) and integrated into the Nematus NMT toolkit (Sennrich et al., 2017). In our experiments, we use a compositional bi-RNN with 256 hidden units, an N"
P18-2049,Q17-1026,0,0.0602562,"Missing"
P18-2049,D15-1176,0,0.308554,"dden state, and the context vector. The latter is a linear combination of the encoder hidden states, whose weights are dynamically computed by a feed-forward neural network called attention model (Bahdanau et al., 2014). The probability of generating each target word yj is normalized via a softmax function. Both the source and target vocabulary sizes play an important role in terms of defining the complex3 Learning Compositional Input Representations via bi-RNNs In this paper, we propose to perform NMT from input representations learned by composing smaller symbols, such as character n-grams (Ling et al., 2015a), that can easily fit in the model vocabulary. This composition is essentially a function which can establish a mapping between combinations of ortographic units and lexical meaning, that is learned using the bilingual context so that it can produce representations that are optimized for machine translation. In our model (Figure 1), the one-hot vectors, after being fed into the embedding layer, are processed by an additional composition layer, which computes the final input representations passed to the encoder to generate translations. For learning the composition function, we employ a biRN"
P18-2049,2012.eamt-1.60,1,0.717937,"of input symbols required to cope with contextual ambiguities, we opt to use intersecting sequences of character trigrams, as recently suggested by Vania and Lopez (2017). Given a bi-RNN with a forward (f ) and backward (b) layer, the input representation w of a token of t characters is computed from the hidden states hft and h0b , i.e. the final outputs of the forward and backward RNNs, as follows: ish (TR). The characteristics of each language are given in Table 1, whereas Table 2 presents the statistical properties of the training data. We train our NMT models using the TED Talks corpora (Cettolo et al., 2012) and test them on the official data sets of IWSLT1 (Mauro et al., 2017). w = Wf htf + Wb h0b + b Table 1: The languages evaluated in our study and their morphological characteristics. Language Turkish Arabic Czech German Italian (2) where Wf and Wb are weight matrices associated to each RNN and b is a bias vector (Ling et al., 2015a). These parameters are jointly learned together with the internal parameters of the GRUs and the input token embedding matrix while training the NMT model. For an input of m tokens, our implementation increases the computational complexity of the network by O(Ktmax"
P18-2049,W14-4012,0,0.137646,"Missing"
P18-2049,P16-1100,0,0.0804433,"Missing"
P18-2049,P11-2031,0,0.0124513,"h source and target languages, and train the segmentation models (BPE and LMVR) to generate sub-word vocabularies of the same size. We train the NMT models using the Adagrad (Duchi et al., 2011) optimizer with a mini-batch size of 50, a learning rate of 0.01, and a dropout rate of 0.1 (in all layers and embeddings). In order to prevent over-fitting, we stop training if the perplexity on the validation does not decrease for 5 epochs, and use the best model to translate the test set. The model outputs are evaluated using the (case-sensitive) BLEU (Papineni et al., 2002) metric and the Multeval (Clark et al., 2011) significance test. The performance of NMT models in translating each language using different vocabulary units and encoder input representations can be seen in Table 4. With the simple model, LMVR based units achieve the best accuracy in translating all languages, with improvements over BPE by 0.85 to 1.09 BLEU points in languages with high morphological complexity (Arabic, Czech and Turkish) and 0.32 to 0.53 BLEU points in languages with low to medium complexity (Italian and German). This confirms our previous results in (Ataman and Federico, 2018). Moreover, simple models using character tr"
P18-2049,2004.iwslt-evaluation.1,1,0.473007,"Missing"
P18-2049,P16-2058,0,0.0676255,"Missing"
P18-2049,C16-1172,0,0.0606921,"Missing"
P18-2049,P02-1040,0,0.102122,"hly restricted dictionary size of 30,000 for both source and target languages, and train the segmentation models (BPE and LMVR) to generate sub-word vocabularies of the same size. We train the NMT models using the Adagrad (Duchi et al., 2011) optimizer with a mini-batch size of 50, a learning rate of 0.01, and a dropout rate of 0.1 (in all layers and embeddings). In order to prevent over-fitting, we stop training if the perplexity on the validation does not decrease for 5 epochs, and use the best model to translate the test set. The model outputs are evaluated using the (case-sensitive) BLEU (Papineni et al., 2002) metric and the Multeval (Clark et al., 2011) significance test. The performance of NMT models in translating each language using different vocabulary units and encoder input representations can be seen in Table 4. With the simple model, LMVR based units achieve the best accuracy in translating all languages, with improvements over BPE by 0.85 to 1.09 BLEU points in languages with high morphological complexity (Arabic, Czech and Turkish) and 0.32 to 0.53 BLEU points in languages with low to medium complexity (Italian and German). This confirms our previous results in (Ataman and Federico, 2018"
P18-2049,P16-1162,0,0.323188,"ne Translation Duygu Ataman FBK, Trento, Italy University of Trento, Italy ataman@fbk.eu Marcello Federico MMT Srl, Trento, Italy FBK, Trento, Italy federico@fbk.eu Abstract resource and/or morphologically-rich languages, due to their high lexical sparseness. To cope with this well-known problem, several approaches have been proposed redefining the model vocabulary in terms of interior orthographic units compounding the words, ranging from character ngrams (Ling et al., 2015b; Costa-juss`a and Fonollosa, 2016; Lee et al., 2017; Luong and Manning, 2016) to statistically-learned sub-word units (Sennrich et al., 2016; Wu et al., 2016; Ataman et al., 2017). While the former provide an ideal open vocabulary solution, they mostly failed to achieve competitive results. This might be related to the semantic ambiguity caused by solely relying on input representations based on character n-grams which are generally learned by disregarding any morphological information. In fact, the second approach is now prominent and has established a pre-processing step for constructing a vocabulary of sub-word units before training the NMT model. However, several studies have shown that segmenting words into sub-word units wit"
P18-2049,W17-4704,0,0.149648,"Missing"
P18-2049,P17-1184,0,0.277696,"can lead to loss of semantic and syntactic information and, thus, inaccurate translations (Niehues et al., 2016; Ataman et al., 2017; Pinnis et al., 2017; Huck et al., 2017; Tamchyna et al., 2017). In this paper, we propose to improve the quality of input (source language) representations of rare words in NMT by augmenting its embedding layer with a bi-directional recurrent neural network (biRNN), which can learn compositional input representations at different levels of granularity. Compositional word embeddings have recently been applied in language modeling and obtained successful results (Vania and Lopez, 2017). The apparent advantage of our approach is that by feeding NMT with simple character n-grams, our biRNN can potentially learn the morphology necessary to create word-level representations of the inNeural machine translation (NMT) models are typically trained with fixed-size input and output vocabularies, which creates an important bottleneck on their accuracy and generalization capability. As a solution, various studies proposed segmenting words into sub-word units and performing translation at the sub-lexical level. However, statistical word segmentation methods have recently shown to be pro"
P18-2049,E17-3017,0,\N,Missing
P18-2049,W17-4706,0,\N,Missing
P19-1294,W17-4716,1,0.829187,"Jaws is a scary movie should be translated as Lo Squalo e` un film pauroso. While translation memories can be seen as ready-to-use training data for NMT domain adaptation, terminology databases (in short term bases) are more difficult to handle and there has been significant work on proposing methods to integrate domain terminology into NMT at run time. Constrained decoding is the main approach to this problem. In short, it uses the target side of terminology entries whose source side match the input as decoding-time constraints. Constrained decoding and various improvements were addressed in Chatterjee et al. (2017), Hasler et al. (2018), Hokamp and Liu (2017) among others. Hokamp and Liu (2017) recently introduced the grid beam search (GBS) algorithm which uses a separate beam for each supplied lexical constraint. This solution however increases the run time complexity of the decoding process exponentially in the number of constraints. Post and Vilar (2018) recently suggested using a dynamic beam allocation (DBA) technique that reduces the computational overhead to a constant factor, independent from the number of constraints. In practice, results reported in Post and Vilar (2018) show that constrained"
P19-1294,W18-1820,0,0.0537751,"Missing"
P19-1294,P17-1141,0,0.0644995,"Squalo e` un film pauroso. While translation memories can be seen as ready-to-use training data for NMT domain adaptation, terminology databases (in short term bases) are more difficult to handle and there has been significant work on proposing methods to integrate domain terminology into NMT at run time. Constrained decoding is the main approach to this problem. In short, it uses the target side of terminology entries whose source side match the input as decoding-time constraints. Constrained decoding and various improvements were addressed in Chatterjee et al. (2017), Hasler et al. (2018), Hokamp and Liu (2017) among others. Hokamp and Liu (2017) recently introduced the grid beam search (GBS) algorithm which uses a separate beam for each supplied lexical constraint. This solution however increases the run time complexity of the decoding process exponentially in the number of constraints. Post and Vilar (2018) recently suggested using a dynamic beam allocation (DBA) technique that reduces the computational overhead to a constant factor, independent from the number of constraints. In practice, results reported in Post and Vilar (2018) show that constrained decoding with DBA is effective but still caus"
P19-1294,P07-2045,1,0.0196287,"Missing"
P19-1294,D15-1166,0,0.0861685,"ed decoding, we proposed a black-box approach in which a generic neural MT architecture is directly trained to learn how to use an external terminology that is provided at run-time. We performed experiments in a zero-shot setting, showing that the copy behaviour is triggered at test time with terms that were never seen in training. In contrast to constrained decoding, we have also observed that the method exhibits flexible use of terminology as in some cases the terms are used in their provided form while other times inflection is performed. 6 To our knowledge there is no existing work that 6 Luong et al. (2015) and SYSTRANs Pure NMT system (Crego et al., 2016) are an exception to the constrained decoding approach as they replace entities with special tags that remain unchanged during translation and are replaced in a postprocessing step. However this method also lacks flexibility, as the model will always replace the placeholder with the same phrase irrespective of grammatical context. We leave comparison to their approach to future work. 3066 has a better speed vs performance trade-off than our method in the space of constrained decoding algorithms for neural MT, which we believe makes it particula"
P19-1294,W18-2712,0,0.105272,"Missing"
P19-1294,N18-1119,0,0.357412,"me. Constrained decoding is the main approach to this problem. In short, it uses the target side of terminology entries whose source side match the input as decoding-time constraints. Constrained decoding and various improvements were addressed in Chatterjee et al. (2017), Hasler et al. (2018), Hokamp and Liu (2017) among others. Hokamp and Liu (2017) recently introduced the grid beam search (GBS) algorithm which uses a separate beam for each supplied lexical constraint. This solution however increases the run time complexity of the decoding process exponentially in the number of constraints. Post and Vilar (2018) recently suggested using a dynamic beam allocation (DBA) technique that reduces the computational overhead to a constant factor, independent from the number of constraints. In practice, results reported in Post and Vilar (2018) show that constrained decoding with DBA is effective but still causes a 3-fold increase in translation time when used with a beam size of 5. In this paper we address the problem of constrained decoding as that of learning a copy behaviour of terminology at training time. By modifying the training procedure of neural MT we are completely eliminating any computational ov"
P19-1294,P16-1162,0,0.396162,"Missing"
P19-1294,N18-2081,0,0.0738431,"ld be translated as Lo Squalo e` un film pauroso. While translation memories can be seen as ready-to-use training data for NMT domain adaptation, terminology databases (in short term bases) are more difficult to handle and there has been significant work on proposing methods to integrate domain terminology into NMT at run time. Constrained decoding is the main approach to this problem. In short, it uses the target side of terminology entries whose source side match the input as decoding-time constraints. Constrained decoding and various improvements were addressed in Chatterjee et al. (2017), Hasler et al. (2018), Hokamp and Liu (2017) among others. Hokamp and Liu (2017) recently introduced the grid beam search (GBS) algorithm which uses a separate beam for each supplied lexical constraint. This solution however increases the run time complexity of the decoding process exponentially in the number of constraints. Post and Vilar (2018) recently suggested using a dynamic beam allocation (DBA) technique that reduces the computational overhead to a constant factor, independent from the number of constraints. In practice, results reported in Post and Vilar (2018) show that constrained decoding with DBA is e"
Q13-1027,D07-1103,0,\N,Missing
Q13-1027,W11-2127,0,\N,Missing
Q13-1027,N04-4026,0,\N,Missing
Q13-1027,N04-4038,0,\N,Missing
Q13-1027,D08-1089,0,\N,Missing
Q13-1027,P06-1067,0,\N,Missing
Q13-1027,P12-1050,1,\N,Missing
Q13-1027,P02-1040,0,\N,Missing
Q13-1027,H05-1066,0,\N,Missing
Q13-1027,W05-0909,0,\N,Missing
Q13-1027,P07-2045,1,\N,Missing
Q13-1027,P10-2033,0,\N,Missing
Q13-1027,N03-1017,0,\N,Missing
Q13-1027,D11-1045,0,\N,Missing
Q13-1027,P02-1038,0,\N,Missing
Q13-1027,W09-0434,0,\N,Missing
Q13-1027,W06-3108,0,\N,Missing
Q13-1027,J97-3002,0,\N,Missing
Q13-1027,W05-0908,0,\N,Missing
Q13-1027,2005.iwslt-1.8,0,\N,Missing
Q13-1027,N10-1129,0,\N,Missing
Q13-1027,P03-1021,0,\N,Missing
Q13-1027,N06-1014,0,\N,Missing
Q13-1027,2010.iwslt-papers.19,0,\N,Missing
W02-1038,H94-1050,0,\N,Missing
W06-2601,W00-0729,0,0.0196059,"theory and its related training algorithm (Darroch and Ratcliff, 1972) do not set restrictions on the range of feature functions 1 , popular NLP text books (Manning and Schutze, 1999) and research papers (Berger et al., 1996) seem to limit them to binary features. In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). This paper focuses on ME models for two texttagging tasks: Named Entity Recognition (NER) and Text Chuncking (TC). By taking inspiration from the literature (Bender et al., 2003; Borthwick, 1999; Koeling, 2000), a set of standard binary features is introduced. Hence, for each feature type, a corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data. A direct comparison of ME models based on binary, realvalued, and mixed features is presented. Besides, performance on the tagging tasks, complexity and training time by each model are reported. ME estimation with real-valued features is accomplished by combining GIS with the leave-one-out method (Manning and Schutze, 1999). 1 Introduction The Maximum Entropy (ME) statistical framework (D"
W06-2601,P98-2140,0,0.024674,"he current class, i.e. ct = c, the identity and offset of the word in the context, i.e. wt+d = w. Formally, the feature is computed by: Lex c,w,d (x, y) = ˆ δ(ct = c) · δ(wt+d = w). For example, the lexical feature for word Verona, at position t with tag loc (location) is: Lexloc,Verona,0 (x, y) = δ(ct = loc) · ·δ(wt = Verona). The n feature functions fi (x, y) represent any kind of information about the event (x, y) which can be useful for the classification task. Typically, binary features are employed which model the verification of simple events within the target class and the context. In Mikheev (1998), binary features for text tagging are classified into two broad classes: atomic and complex. Atomic features tell information about the current tag and one single item (word or tag) of the context. Complex features result as a combination of two or more atomic features. In this way, if the grouped events are not independent, complex features should capture higher correlations or dependencies, possibly useful to discriminate. Lexical features might introduce data sparseness in the model, given that in real texts an important fraction of words occur only once. In other words, many words in the"
W06-2601,P02-1038,0,0.0600425,"into the standard ME training algorithm. Experimental results on two tagging tasks show statistically significant performance gains after augmenting standard binaryfeature models with real-valued features. Despite ME theory and its related training algorithm (Darroch and Ratcliff, 1972) do not set restrictions on the range of feature functions 1 , popular NLP text books (Manning and Schutze, 1999) and research papers (Berger et al., 1996) seem to limit them to binary features. In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). This paper focuses on ME models for two texttagging tasks: Named Entity Recognition (NER) and Text Chuncking (TC). By taking inspiration from the literature (Bender et al., 2003; Borthwick, 1999; Koeling, 2000), a set of standard binary features is introduced. Hence, for each feature type, a corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data. A direct comparison of ME models based on binary, realvalued, and mixed features is presented. Besides, performance on the tagging tasks, complexity and training time by each mode"
W06-2601,W03-0420,0,0.0949571,"with real-valued features. Despite ME theory and its related training algorithm (Darroch and Ratcliff, 1972) do not set restrictions on the range of feature functions 1 , popular NLP text books (Manning and Schutze, 1999) and research papers (Berger et al., 1996) seem to limit them to binary features. In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). This paper focuses on ME models for two texttagging tasks: Named Entity Recognition (NER) and Text Chuncking (TC). By taking inspiration from the literature (Bender et al., 2003; Borthwick, 1999; Koeling, 2000), a set of standard binary features is introduced. Hence, for each feature type, a corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data. A direct comparison of ME models based on binary, realvalued, and mixed features is presented. Besides, performance on the tagging tasks, complexity and training time by each model are reported. ME estimation with real-valued features is accomplished by combining GIS with the leave-one-out method (Manning and Schutze, 1999). 1 Introduction The Maximum Entr"
W06-2601,J96-1002,0,0.0519929,"lting ME models have orders of magnitude fewer parameters. Effective use of training data to estimate features and parameters is achieved by integrating a leaving-one-out method into the standard ME training algorithm. Experimental results on two tagging tasks show statistically significant performance gains after augmenting standard binaryfeature models with real-valued features. Despite ME theory and its related training algorithm (Darroch and Ratcliff, 1972) do not set restrictions on the range of feature functions 1 , popular NLP text books (Manning and Schutze, 1999) and research papers (Berger et al., 1996) seem to limit them to binary features. In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). This paper focuses on ME models for two texttagging tasks: Named Entity Recognition (NER) and Text Chuncking (TC). By taking inspiration from the literature (Bender et al., 2003; Borthwick, 1999; Koeling, 2000), a set of standard binary features is introduced. Hence, for each feature type, a corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data. A d"
W06-2601,C98-2135,0,\N,Missing
W06-3101,P04-1079,0,0.0149782,"sibilities for improvements. 1 Patrik Lambert† Rafael Banchs† 2 Introduction The evaluation of the generated output is an important issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al."
W06-3101,W05-0909,0,0.0366343,"lation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nie"
W06-3101,N04-4015,0,0.00900173,"has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic information in combination with the automatic evaluation measures WER and PER in order to get more details about the tran"
W06-3101,2005.iwslt-1.19,1,0.729357,"s. 1 Patrik Lambert† Rafael Banchs† 2 Introduction The evaluation of the generated output is an important issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed"
W06-3101,C00-2162,1,0.81769,"Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of"
W06-3101,2001.mtsummit-papers.45,1,0.834325,"05). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic infor"
W06-3101,W01-1407,1,0.840883,"05). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic infor"
W06-3101,niessen-etal-2000-evaluation,1,0.397729,"nt issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our kno"
W06-3101,P02-1040,0,0.115841,"t`ecnica de Catalunya (UPC), Barcelona, Spain ⊥ ITC-irst, Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy {popovic,ney}@informatik.rwth-aachen.de {gupta,federico}@itc.it {agispert,canton}@gps.tsc.upc.es {lambert,banchs}@gps.tsc.upc.es Abstract A variety of automatic evaluation measures have been proposed and studied over the last years, some of them are shown to be a very useful tool for comparing different systems as well as for evaluating improvements within one system. The most widely used are Word Error Rate (WER), Position Independent Word Error Rate (PER), the BLEU score (Papineni et al., 2002) and the NIST score (Doddington, 2002). However, none of these measures give any details about the nature of translation errors. A relationship between these error measures and the actual errors in the translation outputs is not easy to find. Therefore some analysis of the translation errors is necessary in order to define the main problems and to focus the research efforts. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006), but like human evaluation, this is also a time consuming task. The goal of this work is to present a framework for au"
W06-3101,popovic-ney-2004-towards,1,0.340303,"Missing"
W06-3101,popovic-ney-2006-pos,1,0.738548,"Missing"
W06-3101,2005.mtsummit-papers.34,1,0.721067,"173 0.15 0.09 2.7 1.7 840 1094 22774 26917 4081 3958 0.14 0.25 2.8 2.6 Table 1: Corpus statistics for the Spanish-English EPPS task (running words include punctuation marks) corpus). The statistics of the corpora can be seen in Table 1. The statistical machine translation system used in this work is based on a log-linear combination of seven different models. The most important ones are phrase based models in both directions, additionally IBM1 models at the phrase level in both directions as well as phrase and length penalty are used. A more detailed description of the system can be found in (Vilar et al., 2005; Zens et al., 2005). 4.3 Experiments The translation experiments have been done in both translation directions on both sizes of the corpus. In order to examine improvements of the baseline system, a new system with POS-based word reorderings of nouns and adjectives as proposed in (Popovi´c and Ney, 2006) is also analysed. Adjectives in the Spanish language are usually placed after the corresponding noun, whereas for English it is the other way round. Therefore, local reorderings of nouns and ad3 WER 34.5 33.5 41.8 38.9 PER 25.5 25.2 30.7 29.5 BLEU 54.7 56.4 43.2 48.5 English→Spanish full base"
W06-3101,vilar-etal-2006-error,1,0.542481,"Missing"
W06-3101,2005.iwslt-1.20,1,0.479674,"7 840 1094 22774 26917 4081 3958 0.14 0.25 2.8 2.6 Table 1: Corpus statistics for the Spanish-English EPPS task (running words include punctuation marks) corpus). The statistics of the corpora can be seen in Table 1. The statistical machine translation system used in this work is based on a log-linear combination of seven different models. The most important ones are phrase based models in both directions, additionally IBM1 models at the phrase level in both directions as well as phrase and length penalty are used. A more detailed description of the system can be found in (Vilar et al., 2005; Zens et al., 2005). 4.3 Experiments The translation experiments have been done in both translation directions on both sizes of the corpus. In order to examine improvements of the baseline system, a new system with POS-based word reorderings of nouns and adjectives as proposed in (Popovi´c and Ney, 2006) is also analysed. Adjectives in the Spanish language are usually placed after the corresponding noun, whereas for English it is the other way round. Therefore, local reorderings of nouns and ad3 WER 34.5 33.5 41.8 38.9 PER 25.5 25.2 30.7 29.5 BLEU 54.7 56.4 43.2 48.5 English→Spanish full baseline reorder 13k bas"
W06-3101,H05-1085,0,\N,Missing
W06-3113,2005.mtsummit-posters.19,0,0.0955322,"corresponds to the mean value of all points falling into it. If Ni is the number of points of the i-th bin, and xi the smallest point in the i-th bin, a partition [xi , xi+1 ] results such that Ni is constant for each i = 0, . . . , k − 1, where xk = 1 by default. The following map is thus defined: q(x) = ci if xi <= x < xi+1 . Our implementation uses the following greedy strategy: bins are build by uniformly partition all different points of the data set. Translation Model 4 Phrase-based Translation System Given a string f in the source language, our SMT system (Federico and Bertoldi, 2005; Cettolo et al., 2005), looks for the target string e maximizing the posterior probability Pr(e, a |f ) over all possible word alignments a. The conditional distribution is computed with the log-linear model: pλ (e, a |f ) ∝ exp ( R X ) λr hr (e, f , a) , r=1 where hr (e, f , a), r = 1 . . . R are real valued feature functions. The log-linear model is used to score translation hypotheses (e, a) built in terms of strings of phrases, which are simple sequences of words. The translation process works as follows. At each step, a target phrase is added to the translation whose corresponding source phrase within f is ide"
W06-3113,W05-0835,0,0.0928604,"tical machine translation (SMT) is based on parametric models incorporating a large number of observations and probabilities estimated from monolingual and parallel texts. The current state of the art is represented by the so-called phrase-based translation approach (Och and Ney, 2004; Koehn et • Which is the optimal trade-off between data compression and translation performance? • How do quantized models perform under different data-sparseness conditions? • Is the impact of quantization consistent across different translation tasks? Experiments were performed with our phrasebased SMT system (Federico and Bertoldi, 2005) on two large-vocabulary tasks: the translation of European Parliament Plenary Sessions from Spanish to 94 Proceedings of the Workshop on Statistical Machine Translation, pages 94–101, c New York City, June 2006. 2006 Association for Computational Linguistics English, and the translation of news agencies from Chinese to English, according to the set up defined by the 2005 NIST MT Evaluation Workshop. The paper is organized as follows. Section 2 reviews previous work addressing efficiency in speech recognition and information retrieval. Section 3 introduces the two quantization methods consider"
W06-3113,P02-1023,0,0.0656746,"Missing"
W06-3113,N03-1017,0,0.0130885,"Missing"
W06-3113,J04-4002,0,0.0338356,"y. • How does probability quantization impact on the components of the translation system, namely the language model and the translation model? 1 Introduction In several natural language processing tasks, such as automatic speech recognition and machine translation, state-of-the-art systems rely on the statistical approach. Statistical machine translation (SMT) is based on parametric models incorporating a large number of observations and probabilities estimated from monolingual and parallel texts. The current state of the art is represented by the so-called phrase-based translation approach (Och and Ney, 2004; Koehn et • Which is the optimal trade-off between data compression and translation performance? • How do quantized models perform under different data-sparseness conditions? • Is the impact of quantization consistent across different translation tasks? Experiments were performed with our phrasebased SMT system (Federico and Bertoldi, 2005) on two large-vocabulary tasks: the translation of European Parliament Plenary Sessions from Spanish to 94 Proceedings of the Workshop on Statistical Machine Translation, pages 94–101, c New York City, June 2006. 2006 Association for Computational Linguisti"
W06-3113,2005.mtsummit-papers.34,0,0.0189467,"of quantization, two separate codebooks are generated for each of the first three levels, and one codebook is generated for the last level. Hence, a total of 7 codebooks are generated. In all discussed quantized LMs, unigram probabilities are always encoded with 8 bits. The reason is that unigram probabilities have indeed the largest variability and do not contribute significantly to the total number of parameters. 5 Experiments Data and Experimental Framework We performed experiments on two large vocabulary translation tasks: the translation of European Parliamentary Plenary Sessions (EPPS) (Vilar et al., 2005) from Spanish to English, and the translation of documents from Chinese to English as proposed by the NIST MT Evaluation Workshops3 . Translation of EPPS is performed on the so-called final text editions, which are prepared by the translation office of the European Parliament. Both the training and testing data were collected by the TCSTAR4 project and were made freely available to participants in the 2006 TC-STAR Evaluation Campaign. In order to perform experiments under different data sparseness conditions, four subsamples of the training data with different sizes were generated, too. Traini"
W07-0712,W06-3113,1,0.766207,"opted by the CMU-Cambridge LM Toolkit (Clarkson and Rosenfeld, 1997) and well analyzed in (Whittaker and Raj, 2001). Briefly, n-grams are stored in a data structure which privileges memory saving rather than access time. In particular, single components of each n-gram are searched, via binary search, into blocks of successors stored contiguously (Figure 2). Further improvements in memory savings are obtained by quantizing both back-off weights and probabilities. 2.4 LM Quantization Quantization provides an effective way of reducing the number of bits needed to store floating point variables. (Federico and Bertoldi, 2006) showed that best results were achieved with the so-called binning method. This method partitions data points into uniformly populated intervals or bins. Bins are filled in in a greedy manner, starting from the lowest value. The center of each bin corresponds to the mean value 90 1-gr 2-gr 3-gr 3 1 w |pr 3 4 1 1 w |bo |pr |idx Figure 2: Static data structure for LMs. Number of bytes are shown used to encode single words (w), quantized back-off weights (bo) and probabilities (pr), and start index of successors (idx). of all its points. Quantization is applied separately at each n-gram level and"
W09-0432,W07-0717,0,0.524961,"ed only recently. In (Eck et al., 2004) adaptation is limited to the target language model (LM). The background LM is combined with one estimated on documents retrieved from the WEB by using the input sentence as query and applying crosslanguage information retrieval techniques. Refinements of this approach are described in (Zhao et al., 2004). In (Hildebrand et al., 2005) information retrieval techniques are applied to retrieve sentence pairs from the training corpus that are relevant to the test sentences. Both the language and the translation models are retrained on the extracted data. In (Foster and Kuhn, 2007) two basic settings are investigated: cross-domain adaptation, in which a small sample of parallel in-domain text is assumed, and dynamic adaptation, in which only the current input source text is considered. Adaptation relies on mixture models estimated on the training data through some unsupervised clustering method. Given available adaptation data, mixture weights are re-estimated ad-hoc. A variation of this approach was also recently proposed in (Finch and Sumita, 2008). In (Civera and Juan, 2007) mixture models are instead employed to adapt a word alignment model to in-domain parallel dat"
W09-0432,2005.eamt-1.19,0,0.474349,"with respect to previous work we also investigate the behavior of the minimum error training procedure to optimize the combination of feature functions on a small in-domain bilingual sample. Previous Work Domain adaptation in SMT has been investigated only recently. In (Eck et al., 2004) adaptation is limited to the target language model (LM). The background LM is combined with one estimated on documents retrieved from the WEB by using the input sentence as query and applying crosslanguage information retrieval techniques. Refinements of this approach are described in (Zhao et al., 2004). In (Hildebrand et al., 2005) information retrieval techniques are applied to retrieve sentence pairs from the training corpus that are relevant to the test sentences. Both the language and the translation models are retrained on the extracted data. In (Foster and Kuhn, 2007) two basic settings are investigated: cross-domain adaptation, in which a small sample of parallel in-domain text is assumed, and dynamic adaptation, in which only the current input source text is considered. Adaptation relies on mixture models estimated on the training data through some unsupervised clustering method. Given available adaptation data,"
W09-0432,W07-0733,0,0.694628,"umption that only monolingual texts are available, either in the source language or in the target language. The paper is organized as follows. Section 2 presents previous work on the problem of adaptation in SMT; Section 3 introduces the exemplar task and research questions we addressed; Section 4 describes the SMT system and the adaptation techniques that were investigated; Section 5 presents and discusses experimental results; and Section 6 provides conclusions. 2 tation methods relying on additional bilingual data synthesized from the development or test set. Our work is mostly related to (Koehn and Schroeder, 2007) but explores different assumptions about available adaptation data: i.e. only monolingual in-domain texts are available. The adaptation of the translation and re-ordering models is performed by generating synthetic bilingual data from monolingual texts, similarly to what proposed in (Schwenk, 2008). Interpolation of multiple phrase tables is applied in a more principled way than in (Koehn and Schroeder, 2007): all entries are merged into one single table, corresponding feature functions are concatenated and smoothing is applied when observations are missing. The approach proposed in this pape"
W09-0432,P07-2045,1,0.0220186,"Missing"
W09-0432,J93-2003,0,0.0141496,"Missing"
W09-0432,D08-1076,0,0.0247701,"nce. Figure 1 reports incremental tuning time and translation performance on the test set at each iteration. Notice that the four tuning configurations are ranked in order of complexity. Table 3 summaries the final performance of each tuning process, after convergence was reached. Notice that decoding time is not included in this plot, as Moses allows to perform this step in parallel on a computer cluster. Hence, to our view the real bottleneck of the tuning process is actually related to the strictly serial part of the mert implementation of Moses. As already observed in previous literature (Macherey et al., 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository. Configurations exploiting the small development set (c,d) show a slower and more unstable convergence; however, their final performance in Table 3 result only slightly lower than that obtained with the standard dev sets (a, b). Due to the larger number of iterations they needed, both configurations are indeed more time consuming than the intermediate configuration (b), which seems the best one. In conclusion, we fo"
W09-0432,W07-0722,0,0.548308,"tences. Both the language and the translation models are retrained on the extracted data. In (Foster and Kuhn, 2007) two basic settings are investigated: cross-domain adaptation, in which a small sample of parallel in-domain text is assumed, and dynamic adaptation, in which only the current input source text is considered. Adaptation relies on mixture models estimated on the training data through some unsupervised clustering method. Given available adaptation data, mixture weights are re-estimated ad-hoc. A variation of this approach was also recently proposed in (Finch and Sumita, 2008). In (Civera and Juan, 2007) mixture models are instead employed to adapt a word alignment model to in-domain parallel data. In (Koehn and Schroeder, 2007) cross-domain adaptation techniques were applied on a phrasebased SMT trained on the Europarl task, in order to translate news commentaries, from French to English. In particular, a small portion of indomain bilingual data was exploited to adapt the Europarl language model and translation models by means of linear interpolation techniques. Ueffing et al. (2007) proposed several elaborate adap3 Task description This paper addresses the issue of adapting an already devel"
W09-0432,J03-1002,0,0.0465772,"earned from training data? • How can interpolation of models be effectively learned from small amounts of indomain parallel data? 4 System description The investigation presented in this paper was carried out with the Moses toolkit (Koehn et al., 2007), a state-of-the-art open-source phrase-based SMT system. We trained Moses in a standard configuration, including a 4-feature translation model, a 7-feature lexicalized re-ordering model, one LM, word and phrase penalties. The translation and the re-ordering model relied on “grow-diag-final” symmetrized word-toword alignments built using GIZA++ (Och and Ney, 2003) and the training script of Moses. A 5-gram language model was trained on the target side of the training parallel corpus using the IRSTLM toolkit (Federico et al., 2008), exploiting Modified Kneser-Ney smoothing, and quantizing both probabilities and backoff weights. Decoding was performed applying cube-pruning with a poplimit of 6000 hypotheses. Log-linear interpolations of feature functions were estimated with the parallel version of minimum error rate training procedure distributed with Moses. 4.1 4.2 Model combination Once monolingual adaptation data is automatically translated, we can us"
W09-0432,eck-etal-2004-language,0,0.0892838,"all entries are merged into one single table, corresponding feature functions are concatenated and smoothing is applied when observations are missing. The approach proposed in this paper has many similarities with the simplest technique in (Ueffing et al., 2007), but it is applied to a much larger monolingual corpus. Finally, with respect to previous work we also investigate the behavior of the minimum error training procedure to optimize the combination of feature functions on a small in-domain bilingual sample. Previous Work Domain adaptation in SMT has been investigated only recently. In (Eck et al., 2004) adaptation is limited to the target language model (LM). The background LM is combined with one estimated on documents retrieved from the WEB by using the input sentence as query and applying crosslanguage information retrieval techniques. Refinements of this approach are described in (Zhao et al., 2004). In (Hildebrand et al., 2005) information retrieval techniques are applied to retrieve sentence pairs from the training corpus that are relevant to the test sentences. Both the language and the translation models are retrained on the extracted data. In (Foster and Kuhn, 2007) two basic settin"
W09-0432,P02-1040,0,0.0911631,"garithms. Henceforth, a phrase pair belonging to all original sets is penalized with respect to phrase pairs belonging to few of them only. To address this drawback, we proposed a new method3 to compute a more reliable and smoothed score in the undefined case, based on the IBM model 1 (Brown et al., 1993). If (f˜ = f1 , . . . , fl , e˜ = e1 , . . . , el ) ∈ SU  Sj for any j the 3 m X l Y  φ(ek |fh ) (l + 1)m k=1 h=0 4 Distributed by the Linguistic Data Consortium, catalogue # LDC94T4A. 5 http://www.statmt.org/wmt08 Authors are not aware of any work addressing this issue. 185 the BLEU score (Papineni et al., 2002), and tested on test2008. (Notice that one reference translation is available for both sets.) Table 1 reports statistics of original and synthetic parallel corpora, as well of the employed development and evaluation data sets. All the texts were just tokenized and mixed case was kept. Hence, all systems were developed to produce case-sensitive translations. corpus sent 2.5M 1.3M 1.3M 1.3M Spanish word dict 50.5M 253K 36.4M 164K 36.4M 164K 36.2M 120K English word dict 45.2M 224K 35.0M 109K 35.4M 133K 35.0M 109K UN EP ¯ SE-EP ¯SE-EP dev test 2,000 2,000 60,438 8,173 61,756 8,331 58,653 6,548 60,"
W09-0432,2008.iwslt-papers.6,0,0.127278,"SMT system and the adaptation techniques that were investigated; Section 5 presents and discusses experimental results; and Section 6 provides conclusions. 2 tation methods relying on additional bilingual data synthesized from the development or test set. Our work is mostly related to (Koehn and Schroeder, 2007) but explores different assumptions about available adaptation data: i.e. only monolingual in-domain texts are available. The adaptation of the translation and re-ordering models is performed by generating synthetic bilingual data from monolingual texts, similarly to what proposed in (Schwenk, 2008). Interpolation of multiple phrase tables is applied in a more principled way than in (Koehn and Schroeder, 2007): all entries are merged into one single table, corresponding feature functions are concatenated and smoothing is applied when observations are missing. The approach proposed in this paper has many similarities with the simplest technique in (Ueffing et al., 2007), but it is applied to a much larger monolingual corpus. Finally, with respect to previous work we also investigate the behavior of the minimum error training procedure to optimize the combination of feature functions on a"
W09-0432,C04-1059,0,0.813972,"Missing"
W09-0432,W08-0334,0,0.032125,"are relevant to the test sentences. Both the language and the translation models are retrained on the extracted data. In (Foster and Kuhn, 2007) two basic settings are investigated: cross-domain adaptation, in which a small sample of parallel in-domain text is assumed, and dynamic adaptation, in which only the current input source text is considered. Adaptation relies on mixture models estimated on the training data through some unsupervised clustering method. Given available adaptation data, mixture weights are re-estimated ad-hoc. A variation of this approach was also recently proposed in (Finch and Sumita, 2008). In (Civera and Juan, 2007) mixture models are instead employed to adapt a word alignment model to in-domain parallel data. In (Koehn and Schroeder, 2007) cross-domain adaptation techniques were applied on a phrasebased SMT trained on the Europarl task, in order to translate news commentaries, from French to English. In particular, a small portion of indomain bilingual data was exploited to adapt the Europarl language model and translation models by means of linear interpolation techniques. Ueffing et al. (2007) proposed several elaborate adap3 Task description This paper addresses the issue"
W10-1710,N04-1022,0,0.0353991,"phological Reduction and Chunk-based Reordering Christian Hardmeier, Arianna Bisazza and Marcello Federico Fondazione Bruno Kessler Human Language Technologies Trento, Italy {hardmeier,bisazza,federico}@fbk.eu Abstract a recasing step, we retained the data in document case throughout our system, except for the morphologically normalised word forms described in section 3. Our phrase tables were trained with the standard Moses training script, then filtered based on statistical significance according to the method described by Johnson et al. (2007). Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al., 2002). FBK participated in the WMT 2010 Machine Translation shared task with phrase-based Statistical Machine Translation systems based on the Moses decoder for English-German and German-English translation. Our work concentrates on exploiting the available language modelling resources by using linear mixtures of large 6-gram language models and on addressing linguistic differences between English and German with methods based on word lattices. In particular, we use lattices to integrate a morphological analyser for German into our system, and we pres"
W10-1710,W09-0435,0,0.190543,"Missing"
W10-1710,P03-1021,0,0.0227324,"Missing"
W10-1710,P02-1040,0,0.0782298,"Christian Hardmeier, Arianna Bisazza and Marcello Federico Fondazione Bruno Kessler Human Language Technologies Trento, Italy {hardmeier,bisazza,federico}@fbk.eu Abstract a recasing step, we retained the data in document case throughout our system, except for the morphologically normalised word forms described in section 3. Our phrase tables were trained with the standard Moses training script, then filtered based on statistical significance according to the method described by Johnson et al. (2007). Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al., 2002). FBK participated in the WMT 2010 Machine Translation shared task with phrase-based Statistical Machine Translation systems based on the Moses decoder for English-German and German-English translation. Our work concentrates on exploiting the available language modelling resources by using linear mixtures of large 6-gram language models and on addressing linguistic differences between English and German with methods based on word lattices. In particular, we use lattices to integrate a morphological analyser for German into our system, and we present some initial work on rule-based word reorder"
W10-1710,W10-1735,1,0.823142,"s a working solution that results in a significant improvement in translation quality. It is an alternative to the popular statistical compound splitting methods, such as the one by Koehn and Knight (2003), incorporating a greater amount of linguistic knowledge and offering morphological reduction even of simplex words to their base form in addition. It would be interesting to compare the relative performance of the two approaches systematically. Word reordering between German and English is a complex problem. Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish (Bisazza and Federico, 2010), we tried to adapt the same approach to the German-English language pair. It turned out that there is a larger variety of long reordering patterns in this case. Nevertheless, some experiments performed after 20.6 21.1 note: only News LM, case-sensitive evaluation Table 5: Results with morphological reduction and chunk reordering on newstest 2009/2010 ual inspection of a data sample, we then identified a few recurrent patterns of long reorderings involving the verbs. In particular, we focused on clause-final verbs in German SOV clauses, which we move to the left in order to approximate the Eng"
W10-1710,P08-1115,0,0.293714,"Missing"
W10-1710,D07-1103,0,0.0373687,"Missing"
W10-1710,E03-1076,0,0.286632,"n our parallel computing environment. We are working on methods to reduce and distribute disk accesses to large language models, which will be implemented in the IRSTLM language modelling toolkit (Federico et al., 2008). By doing so, we hope to overcome the current limitations and exploit the power of language model mixtures more fully. The Gertwol-based morphological reduction and decompounding component we used is a working solution that results in a significant improvement in translation quality. It is an alternative to the popular statistical compound splitting methods, such as the one by Koehn and Knight (2003), incorporating a greater amount of linguistic knowledge and offering morphological reduction even of simplex words to their base form in addition. It would be interesting to compare the relative performance of the two approaches systematically. Word reordering between German and English is a complex problem. Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish (Bisazza and Federico, 2010), we tried to adapt the same approach to the German-English language pair. It turned out that there is a larger variety of long reordering patterns in this case. Nevertheless, so"
W10-1710,P07-2045,1,0.0183052,"some more effort into the inverse translation direction to make better use of the abundance of language modelling data available for English and to address the richness of German morphology, which makes it hard for a Statistical Machine Translation (SMT) system to achieve good vocabulary coverage. In the remainder of this section, an overview of the common features of our systems will be given. The next two sections provide a more detailed description of our approaches to language modelling, morphological preprocessing and word reordering. Both of our systems were based on the Moses decoder (Koehn et al., 2007). They were similar to the WMT 2010 Moses baseline system. Instead of lowercasing the training data and adding 1. For a linear mixture of the complete set of 24 language models, we estimated a set of 88 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 88–92, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Corpus Europarl v5 News News commentary 10 Gigaword v3: 6 models Gigaword 2007/08: 6 models 109 fr-en UNDOC fr-en CzEng: 7 models Total: 24 models n-grams 115,702,157 1,437,562,740 10,381,511 7,990,828,834 1,418"
W10-1735,W09-0809,0,0.185667,"Missing"
W10-1735,D08-1089,0,0.0691096,"hese, however, appear to be mostly local, therefore more likely to be modeled through phrase internal alignments, or to be captured by the reordering capabilities of the decoder. In general there is a quite uneven distribution of word-reordering phenomena in Arabic-English, and long-range movements concentrate on few patterns. Reordering in PSMT is typically performed by (i) constraining the maximum allowed word movement and exponentially penalizing long reorderings (distortion limit and penalty), and (ii) through so-called lexicalized orientation models (Och et al., 2004; Koehn et al., 2007; Galley and Manning, 2008). While the former is mainly aimed at reducing the computational complexity of the decoding algorithm, the latter assigns at each decoding step a score to the next source phrase to cover, according to its orientation with respect to the last translated phrase. In fact, neither method discriminates among different reordering distances for a specific word or syntactic class. To our view, this could be a reason for their inadequacy to properly deal with the reordering peculiarities of the Arabic-English language pair. In In Arabic-to-English phrase-based statistical machine translation, a large n"
W10-1735,E06-1032,0,0.0214445,"Missing"
W10-1735,P01-1030,0,0.0607618,"Missing"
W10-1735,W08-0307,0,0.319807,"Missing"
W10-1735,2009.mtsummit-caasl.4,0,0.370195,"Missing"
W10-1735,N04-4038,0,0.128355,"e a correct translation. In order to restrict the set of possible movements of a verb and to abstract from the usual token-based movement length measure, we decided to use shallow syntax chunking of the source language. Full syntactic parsing is another option which we have not tried so far mainly because popular parsers that are available for Arabic do not mark grammatical relations such as the ones we are interested in. We assume that Arabic verb reordering only occurs between shallow syntax chunks, and not within them. For this purpose we annotated our Arabic data with the AMIRA chunker by Diab et al. (2004)2 . The resulting chunks are generally short (1.6 words on average). We then consider a specific type of reordering by defining a production rule of the kind: “move a chunk of type T along with its L left neighbours and R right neighbours by a shift of S chunks”. A basic set of rules 3 Analysis of Verb Reordering We applied the above technique to two parallel corpora3 provided by the organizers of the NISTMT09 Evaluation. The first corpus (Gale-NW) contains human-made alignments. As these refer to non-segmented text, they were adjusted to 2 This tool implies morphological segmentation of the A"
W10-1735,2007.mtsummit-papers.29,0,0.696769,"77 sentence pairs. 236 Figure 2: Percentage of verb reorderings by maximum shift (0 stands for no movement). Figure 3: Distortion reduction in the GALE-NW corpus: jump occurrences grouped by length range (in nb. of words). agree with AMIRA-style segmentation. For the second corpus (Eval08-NW), we filtered out sentences longer than 80 tokens in order to make word alignment feasible with GIZA++ (Och and Ney, 2003). We then used the Intersection of the direct and inverse alignments, as computed by Moses. The choice of such a high-precision, lowrecall alignment set is supported by the findings of Habash (2007) on syntactic rule extraction from parallel corpora. 3.2 3.1 Impact on Corpus Global Distortion We tried to measure the impact of chunk-based verb reordering on the total word distortion found in parallel data. For the sake of reliability, this investigation was carried out on the manually aligned corpus (Gale-NW) only. Fig. 3 shows the positive effect of verb reordering on the total distortion, which is measured as the number of words that have to be jumped on the source side in order to cover the sentence in the target order (that is |ai − (ai−1 + 1)|). Jumps have been grouped by length and"
W10-1735,P08-1115,0,0.25069,"act of VSO sentences on Arabic-English translation performance, we now propose a method to improve the handling of this phenomenon at decoding time. As in real working conditions word alignments of the input text are not available, we explore a reordering lattice approach. 5.1 Lattice Construction Firstly conceived to optimally encode multiple transcription hypothesis produced by a speech recognizer, word lattices have later been used to represent various forms of input ambiguity, mainly at the level of token boundaries (e.g. word segmentation, morphological decomposition, word decompounding (Dyer et al., 2008)). A main problem when dealing with permutaTo be consistent with the reordering applied to the training data, we use a set of rules that move each verb phrase alone or with its following chunk by 1 to 6 chunks to the right. With this settings, 239 Figure 6: Structure of a chunk-based reordering lattice for verb reordering, before word expansion. Edges in boldface represent the verbal chunk. our lattice generation algorithm computes a compact lattice (Fig. 6) that introduces at most 5 × ∆S chunk edges for each verb chunk, where ∆S is the permitted movement range (6 in this case). Before transla"
W10-1735,P07-2045,1,0.023212,"tructions (Idafa). These, however, appear to be mostly local, therefore more likely to be modeled through phrase internal alignments, or to be captured by the reordering capabilities of the decoder. In general there is a quite uneven distribution of word-reordering phenomena in Arabic-English, and long-range movements concentrate on few patterns. Reordering in PSMT is typically performed by (i) constraining the maximum allowed word movement and exponentially penalizing long reorderings (distortion limit and penalty), and (ii) through so-called lexicalized orientation models (Och et al., 2004; Koehn et al., 2007; Galley and Manning, 2008). While the former is mainly aimed at reducing the computational complexity of the decoding algorithm, the latter assigns at each decoding step a score to the next source phrase to cover, according to its orientation with respect to the last translated phrase. In fact, neither method discriminates among different reordering distances for a specific word or syntactic class. To our view, this could be a reason for their inadequacy to properly deal with the reordering peculiarities of the Arabic-English language pair. In In Arabic-to-English phrase-based statistical mac"
W10-1735,2006.amta-papers.11,0,0.0874189,"Missing"
W10-1735,W09-0435,0,0.25433,"Missing"
W10-1735,J03-1002,0,0.00536733,"This tool implies morphological segmentation of the Arabic text. All word statistics in this paper refer to AMIRAsegmented text. 3 Newswire sections of LDC2006E93 and LDC2009E08, respectively 4337 and 777 sentence pairs. 236 Figure 2: Percentage of verb reorderings by maximum shift (0 stands for no movement). Figure 3: Distortion reduction in the GALE-NW corpus: jump occurrences grouped by length range (in nb. of words). agree with AMIRA-style segmentation. For the second corpus (Eval08-NW), we filtered out sentences longer than 80 tokens in order to make word alignment feasible with GIZA++ (Och and Ney, 2003). We then used the Intersection of the direct and inverse alignments, as computed by Moses. The choice of such a high-precision, lowrecall alignment set is supported by the findings of Habash (2007) on syntactic rule extraction from parallel corpora. 3.2 3.1 Impact on Corpus Global Distortion We tried to measure the impact of chunk-based verb reordering on the total word distortion found in parallel data. For the sake of reliability, this investigation was carried out on the manually aligned corpus (Gale-NW) only. Fig. 3 shows the positive effect of verb reordering on the total distortion, whi"
W10-1735,N04-1021,0,0.0309736,"tial genitive constructions (Idafa). These, however, appear to be mostly local, therefore more likely to be modeled through phrase internal alignments, or to be captured by the reordering capabilities of the decoder. In general there is a quite uneven distribution of word-reordering phenomena in Arabic-English, and long-range movements concentrate on few patterns. Reordering in PSMT is typically performed by (i) constraining the maximum allowed word movement and exponentially penalizing long reorderings (distortion limit and penalty), and (ii) through so-called lexicalized orientation models (Och et al., 2004; Koehn et al., 2007; Galley and Manning, 2008). While the former is mainly aimed at reducing the computational complexity of the decoding algorithm, the latter assigns at each decoding step a score to the next source phrase to cover, according to its orientation with respect to the last translated phrase. In fact, neither method discriminates among different reordering distances for a specific word or syntactic class. To our view, this could be a reason for their inadequacy to properly deal with the reordering peculiarities of the Arabic-English language pair. In In Arabic-to-English phrase-b"
W10-1735,P03-1021,0,0.0159675,"verb reordering procedure on training only. The latter will provide an estimate of the maximum improvement we can expect from the application to the test of an optimal verb reordering prediction technique. Given our experimental setting, one could argue that our BLEU score is biased because one of the references was also used to generate the verb reordering. However, in a series of experiments not reported here, we evaluated the same systems using only the remaining three references and observed similar trends as when all four references are used. Feature weights were optimized through MERT (Och, 2003) on the newswire section of the NISTMT06 evaluation set (Dev06-NW), in the original version for the baseline system, in the verbreordered version for the reordered system. 4.2 Discussion The first observation is that the reordered system always performs better (0.5∼0.6 points) than the baseline on the plain test, despite the mismatch between training and test ordering. This may be due to the fact that automatic word alignments are more accurate when less reordering is present in the data, although previous work (Lopez and Resnik, 2006) showed that even large gains in alignment accuracy seldom"
W10-1735,W10-1710,1,\N,Missing
W10-1735,W09-0434,0,\N,Missing
W11-2133,W07-0717,0,0.150939,"round language model and use it to retrieve relevant documents from large monolin294 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 294–302, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics gual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model. In Sethy et al. (2006), domain-specific language models are obtained by including only the sentences that are similar to the ones in the target domain via a relative entropy based criterion. Researchers such as Foster and Kuhn (2007) and Koehn and Schroeder (2007) have investigated mixture model approaches to adaptation. Foster and Kuhn (2007) use a mixture model approach that involves splitting a training corpus into different components, training separate models on each component, and applying mixture weights as a function of the distances of each component to the source text. Koehn and Schroeder (2007) learn mixture weights for language models trained with in-domain and outof-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models. Although the application of mix"
W11-2133,D09-1092,0,0.0929081,"anslation model. Their work is extended in Tam and Schultz (2009) by constructing parallel document clusters formed by monolingual documents using M parallel seed documents. Additionally, Gong et al. (2010) propose translation model adaptation via a monolingual LDA training. A monolingual LDA model is trained from either the source or target side of the training corpus and each phrase pair is assigned a phrase-topic distribution based on: wj · M j ˆ Mij = Pkm i j , k=1 wk (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al. (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles). Documents are grouped into tuples w = (w1 , ..., wL ) for each language l = 1, ..., L. Each document wl in tuple w is assumed to have the same topic distribution, drawn from an asymmetric Dirichlet prior. Tuple-specific topic distributions are learned using LDA with distinct topic-word concentration parameters β l . Mimno et al. (2009) show that PLTM sufficiently aligns topics in parallel corpora. 3 Topic Modeling 3.1"
W11-2133,W06-1644,0,0.0163139,"models applied. Most works focus on monolingual language model adaptation in the context of automatic speech recognition. Federico (2002) combines Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) for topic modeling with the minimum discrimination information (MDI) estimation criterion for speech recognition and notes an improvement in terms of perplexity and word error rate (WER). Latent Dirichlet Allocation (LDA) techniques have been proposed as an alternative to PLSA to construct purely generative models. LDA techniques include variational Bayes (Blei et al., 2003) and HMM-LDA (Hsu and Glass, 2006). Recently, bilingual approaches to topic modeling have also been proposed. A Hidden Markov Bilingual Topic AdMixture (HM-BiTAM) model is proposed by Zhao and Xing (2008), which constructs a generative model in which words from a target language are sampled from a mixture of topics drawn from a Dirichlet distribution. Foreign words are sampled via alignment links from a first-order Markov process and a topic specific translation lexicon. While HM-BiTAM has been used for bilingual topic extraction and topic-specific lexicon mapping in the context of SMT, Zhao and Xing (2008) note 295 that HM-Bi"
W11-2133,P07-2045,1,0.0121304,"chology and thus do not adhere to a single genre. All talks were given in English and were manually transcribed and translated into French. The TED training data consists of 329 parallel talk transcripts with approximately 84k sentences. The TED test data consists of transcriptions created via 1-best ASR outputs from the KIT Quaero Evaluation System. It consists of 758 sentences and 27,432 and 27,307 English and French words, respectively. The TED talk data is segmented at the clause level, rather than at the level of sentences. Our SMT systems are built upon the Moses opensource SMT toolkit (Koehn et al., 2007)3 . The translation and lexicalized reordering models have been trained on parallel data. One 5-gram background LM was constructed from the French side of the TED training data (740k words), smoothed with the improved Kneser-Ney technique (Chen and Goodman, 1999) and computed with the IRSTLM toolkit (Federico et al., 2008). The weights of the log-linear interpolation model were optimized via minimum error rate training (MERT) (Och, 2003) on the TED development set, using 200 best translations at each tuning iteration. This paper investigates the effects of language model adaptation via bilingu"
W11-2133,N06-2037,0,0.352795,"Missing"
W11-2133,C04-1059,0,0.0742222,"e language words from the topic-word distributions and construct a unigram language model which is used to adapt our background LM via Minimum Discrimination Information (MDI) estimation (Federico, 1999, 2002; Kneser et al., 1997). We organize the paper as follows: In Section 2, we discuss relevant previous work. In Section 3, we review topic modeling. In Section 4, we review MDI adaptation. In Section 5, we describe our new bilingual topic modeling based adaptation technique. In Section 6, we report adaptation experiments, followed by conclusions and future work in Section 7. 2 Previous work Zhao et al. (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolin294 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 294–302, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics gual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model. In Sethy et al. (2006), domain-specific language models are obtained by including only the sentences that are similar to the ones in the target domain via a re"
W11-2133,J93-2003,0,\N,Missing
W11-2133,P02-1040,0,\N,Missing
W11-2133,P06-2124,0,\N,Missing
W11-2133,J04-4002,0,\N,Missing
W11-2133,W07-0733,0,\N,Missing
W11-2133,J03-1005,0,\N,Missing
W11-2133,H94-1028,0,\N,Missing
W11-2133,P03-1021,0,\N,Missing
W11-2133,2010.iwslt-evaluation.1,1,\N,Missing
W11-2144,S10-1021,0,0.0238005,"ds of texts. Anaphora can be local to a sentence, or it can cross sentence boundaries. Standard SMT methods do not handle this phenomenon in a satisfactory way at present: For sentence-internal anaphora, they depend on the n-gram language model with its limited history, while cross-sentence anaphora is left to chance. We therefore added a word-dependency model (Hardmeier and Federico, 2010) to our system to handle anaphora explicitly. Our processing of anaphoric pronouns follows the procedure outlined by Hardmeier and Federico (2010). We use the open-source coreference resolution system BART (Broscheit et al., 2010) to link pronouns to their antecedents in the text. Coreference links are handled differently depending on whether or not they cross sentence boundaries. If a coreference link points to a previous sentence, we process the sentence containing the antecedent with the SMT system and look up the translation of the antecedent in the translated output. If the coreference link is sentence-internal, the translation lookup is done dynamically by the decoder during search. In either case, the word-dependency model adds a feature function to the decoder score representing the probability of a particular"
W11-2144,de-marneffe-etal-2006-generating,0,0.0128364,"Missing"
W11-2144,D08-1089,0,0.301274,"ench Our submission to the English-French task was a phrase-based Statistical Machine Translation based on the Moses decoder (Koehn et al., 2007). Phrase tables were separately trained on Europarl, news commentary and UN data and then linearly interpolated with uniform weights. For language modelling, we used 5-gram models trained with the IRSTLM toolkit (Federico et al., 2008) on the monolingual News corpus and parts of the English-French 109 corpus. More unusual features of our system included a special component to handle pronominal anaphora and the hierarchical lexical reordering model by Galley and Manning (2008). Selected features of our system will be discussed in depth in the following sections. 1.1 Handling pronominal anaphora Pronominal anaphora is the use of pronominal expressions to refer to “something previously mentioned in the discourse” (Strube, 2006). It is a very common phenomenon found in almost all kinds of texts. Anaphora can be local to a sentence, or it can cross sentence boundaries. Standard SMT methods do not handle this phenomenon in a satisfactory way at present: For sentence-internal anaphora, they depend on the n-gram language model with its limited history, while cross-sentenc"
W11-2144,P07-2053,0,0.0307365,"Missing"
W11-2144,2010.iwslt-papers.10,1,0.918745,"ing sections. 1.1 Handling pronominal anaphora Pronominal anaphora is the use of pronominal expressions to refer to “something previously mentioned in the discourse” (Strube, 2006). It is a very common phenomenon found in almost all kinds of texts. Anaphora can be local to a sentence, or it can cross sentence boundaries. Standard SMT methods do not handle this phenomenon in a satisfactory way at present: For sentence-internal anaphora, they depend on the n-gram language model with its limited history, while cross-sentence anaphora is left to chance. We therefore added a word-dependency model (Hardmeier and Federico, 2010) to our system to handle anaphora explicitly. Our processing of anaphoric pronouns follows the procedure outlined by Hardmeier and Federico (2010). We use the open-source coreference resolution system BART (Broscheit et al., 2010) to link pronouns to their antecedents in the text. Coreference links are handled differently depending on whether or not they cross sentence boundaries. If a coreference link points to a previous sentence, we process the sentence containing the antecedent with the SMT system and look up the translation of the antecedent in the translated output. If the coreference li"
W11-2144,P06-1063,0,0.0313791,"Missing"
W11-2144,2005.iwslt-1.8,0,0.214486,"Missing"
W11-2144,P07-2045,1,0.0116502,"at WMT 2011. We created two largely independent systems for English-to-French and Haitian Creole-toEnglish translation to evaluate different features and components from our ongoing research on these language pairs. Key features of our systems include anaphora resolution, hierarchical lexical reordering, data selection for language modelling, linear transduction grammars for word alignment and syntaxbased decoding with monolingual dependency information. 1 English to French Our submission to the English-French task was a phrase-based Statistical Machine Translation based on the Moses decoder (Koehn et al., 2007). Phrase tables were separately trained on Europarl, news commentary and UN data and then linearly interpolated with uniform weights. For language modelling, we used 5-gram models trained with the IRSTLM toolkit (Federico et al., 2008) on the monolingual News corpus and parts of the English-French 109 corpus. More unusual features of our system included a special component to handle pronominal anaphora and the hierarchical lexical reordering model by Galley and Manning (2008). Selected features of our system will be discussed in depth in the following sections. 1.1 Handling pronominal anaphora"
W11-2144,J93-2004,0,0.0368938,"he overall best scores in both translation directions. The fact that both alignments lead to complementary information can be seen in the size of the phrase tables extracted (see table 3). 2.2 Syntax-based SMT We used Moses and its syntax-mode for our experiments with hierarchical phrase-based and syntaxaugmented models. Our main interest was to investigate the influence of monolingual parsing on the translation performance. In particular, we tried to integrate English dependency parses created by MaltParser (Nivre et al., 2007) trained on the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) extended with about 4000 questions 3 We actually swapped the development set and the test set by mistake. But, of course, we never mixed development and test data in any result reported. null from the Question Bank (Judge et al., 2006). The conversion to dependency trees was done using the Stanford Parser (de Marneffe et al., 2006). Again, we ran both translation directions to test our settings in more than just one task. Interesting here is also the question whether there are significant differences when integrating monolingual parses on the source or on the target side. The motivation for a"
W11-2144,J03-1002,0,0.00544101,"MT 2011 to build a large scale-background language model. The English data from the Haitian Creole task were used as a separate domain-specific language model. For the other translation direction we only used the in-domain data provided. We used standard 5-gram models with Witten-Bell discounting and backoff interpolation for all language models. For the translation model we applied standard techniques and settings for phrase extraction and score estimations. However, we applied two different systems for word alignment: One is the standard GIZA++ toolbox implementing the IBM alignment models (Och and Ney, 2003) and extensions and the other is based on transduction grammars which will briefly be introduced in the next section. 2.1.1 Alignment with PLITGs By making the assumption that the parallel corpus constitutes a linear transduction (Saers, 2011)2 we can induce a grammar that is the most likely to have generated the observed corpus. The grammar induced will generate a parse forest for each sentence pair in the corpus, and each parse tree in that forest will correspond to an alignment between the two sentences. Following Saers et al. (2010), the alignment corresponding to the best parse can be ext"
W11-2144,2011.eamt-1.42,1,0.822224,"e a grammar that is the most likely to have generated the observed corpus. The grammar induced will generate a parse forest for each sentence pair in the corpus, and each parse tree in that forest will correspond to an alignment between the two sentences. Following Saers et al. (2010), the alignment corresponding to the best parse can be extracted and used instead of other word alignment approaches such as GIZA++. There are several grammar types that generate linear transductions, and in this work, stochastic bracketing preterminalized linear inversion transduction grammars (PLITG) were used (Saers and Wu, 2011). Since we were mainly interested in the word alignments, we did not induce phrasal grammars. Although alignments from PLITGs may not reach the same level of translation quality as GIZA++, they make different mistakes, so both complement 2A transduction is a set of pairs of strings, and thus represents a relation between two languages. 375 each other. By duplicating the training corpus and aligning each copy of the corpus with a different alignment tool, the phrase extractor seems to be able to pick the best of both worlds, producing a phrase table that is superior to one produced with either"
W11-2144,N10-1050,1,0.845828,"dard GIZA++ toolbox implementing the IBM alignment models (Och and Ney, 2003) and extensions and the other is based on transduction grammars which will briefly be introduced in the next section. 2.1.1 Alignment with PLITGs By making the assumption that the parallel corpus constitutes a linear transduction (Saers, 2011)2 we can induce a grammar that is the most likely to have generated the observed corpus. The grammar induced will generate a parse forest for each sentence pair in the corpus, and each parse tree in that forest will correspond to an alignment between the two sentences. Following Saers et al. (2010), the alignment corresponding to the best parse can be extracted and used instead of other word alignment approaches such as GIZA++. There are several grammar types that generate linear transductions, and in this work, stochastic bracketing preterminalized linear inversion transduction grammars (PLITG) were used (Saers and Wu, 2011). Since we were mainly interested in the word alignments, we did not induce phrasal grammars. Although alignments from PLITGs may not reach the same level of translation quality as GIZA++, they make different mistakes, so both complement 2A transduction is a set of"
W11-2144,C08-1144,0,0.0330283,"lish test set with (=malt) or without (=hiero) English parse trees and various parse relaxation strategies. The final system submitted to WMT11 is malt(target)-samt2. rule extraction is based on tree manipulation and relaxed extraction algorithms. Moses implements several algorithms that have been proposed in the literature. Tree binarisation is one of them. This can be done in a left-branching and in a right-branching mode. We used a combination of both in the settings denoted as binarised. The other relaxation algorithms are based on methods proposed for syntaxaugmented machine translation (Zollmann et al., 2008). We used two of them: samt1 combines pairs of neighbouring children nodes into combined complex nodes and creates additional complex nodes of all children nodes except the first child and similar complex nodes for all but the last child. samt2 combines any pair of neighbouring nodes even if they are not children of the same parent. All of these relaxation algorithms lead to increased rule sets (table 4). In terms of translation performance there seems to 377 be a strong correlation between rule table size and translation quality as measured by BLEU. None of the dependency-based models beats t"
W12-3122,W11-2104,0,0.0497598,"estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-oriented features (e.g. “number of punctuation marks”). As a result, a simple surface form variation is given the same importance of a content word variation that changes the meaning of the sentence. To the best of our knowledge, only (Specia et al., 2011) proposed an approach to frame MT evaluation as an adequacy estimation problem. However, their method still includes many features which are not focused on ad"
W12-3122,P11-1022,0,0.0747271,"addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-oriented features (e.g. “number of punctuation marks”). As a result, a simple surface form variation is given the same importance of a content word variation that changes the meaning of the sentence. To the best of our knowledge, only (Specia et al., 2011) proposed an approach to frame MT evaluation as an adequacy estimation problem. However, their method still includes many features wh"
W12-3122,W05-0909,0,0.029602,"frequency counts with meaning representations. In order to integrate semantics more deeply into MT technology, in this paper we focus on the evaluation dimension. Restricting our investigation to some of the more pressing issues emerging from this area of research, we provide two main contributions. 1. An automatic evaluation method that avoids the use of reference translations. Most current metrics are based on comparisons between automatic translations and human references, and reward lexical similarity at the n-gram level (e.g. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006)). Due to the variability of natural languages in terms of possible ways to express the same meaning, reliable lexical similarity metrics depend on the availability of multiple hand-crafted (costly) realizations of the same source sentence in the target language. Our approach aims to avoid this bottleneck by adapting cross-lingual semantic inference capabilities and judging a translation only given the source sentence. 2. A method for evaluating translation adequacy. Most current solutions do not consistently reward translation adequacy (semantic equivalence between"
W12-3122,C04-1046,0,0.18869,"from a rich set of variants at five different linguistic levels: lexical, shallow-syntactic, syntactic, shallow-semantic and semantic. More similar to our approach, (Pad´o et al., 2009) proposed semantic adequacy metrics that exploit feature representations motivated by Textual Entailment (TE). Both metrics, however, highly depend on the availability of multiple reference translations. Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translati"
W12-3122,W07-0718,0,0.02155,"on. 4 Experiments and results 4.1 Datasets Datasets with manual evaluation of MT output have been made available through a number of shared evaluation tasks. However, most of these datasets are not specifically annotated for adequacy measurement purposes, and the available adequacy judgements are limited to few hundred sentences for some language pairs. Moreover, most datasets are created by comparing reference translations with MT systems’ output, disregarding the input sentences. Such judgements are hence biased towards the reference. Furthermore, the inter-annotator agreement is often low (Callison-Burch et al., 2007). In light of these limitations, most of the available datasets are per se not fully suitable for adequacy evaluation methods based on supervised learning, nor to provide stable and meaningful results. To partially cope with these problems, our experiments have been carried out over two different datasets: • 16K: 16.000 English-Spanish pairs, with Spanish translations produced by multiple MT systems, annotated by professional translators with quality scores in a 4-point scale (Specia et al., 2010a). • WMT07: 703 English-Spanish pairs derived from MT systems’ output, with explicit adequacy judg"
W12-3122,carreras-etal-2004-freeling,0,0.0213697,"Missing"
W12-3122,W07-0738,0,0.0366771,"Missing"
W12-3122,P07-2045,1,0.0115053,"rsers. Phrase Table (PT) matching features are calculated as in (Mehdad et al., 2011), with a phrasal matching algorithm that takes advantage of a lexical phrase table extracted from a bilingual parallel corpus. The algorithm determines the number of phrases in the source (1 to 5-grams, at the level of tokens, lemmas and stems) that can be mapped into target word sequences, and vice-versa. To build our English-Spanish phrase table, we used the Europarl, News Commentary and United Nations SpanishEnglish parallel corpora. After tokenization, the Giza++ (Och and Ney, 2000) and the Moses toolkit (Koehn et al., 2007) were respectively used to align the corpora and extract the phrase table. Although the phrase table was generated using MT technology, its use to compute our features is still compatible with a system-independent approach since the extraction is carried out without tuning the process towards any particular task. Moreover, our phrase matching algorithm integrates matches from overlapping n-grams of different size and nature (tokens, lemmas and stems) which current MT decoding algorithms cannot explore for complexity reasons. Dependency Relation (DR) matching features target the increase of CLT"
W12-3122,N10-1045,1,0.836751,"im, like (Xiong et al., 2010; Bach et al., 2011; Avramidis et al., 2011; Specia et al., 2010b; Specia et al., 2011) we rely on a large number of features, but focusing on source-target dependent ones, aiming at informed adequacy evaluation of a translation given the source instead of a more generic quality assessment based on surface features. 3 CLTE for adequacy evaluation We address adequacy evaluation by adapting crosslingual textual entailment recognition as a way to measure to what extent a source sentence and its automatic translation are semantically similar. CLTE has been proposed by (Mehdad et al., 2010) as an extension of textual entailment (Dagan and Glickman, 2004) that consists in deciding, given a text T and a hypothesis H in different languages, if the meaning of H can be inferred from the meaning of T. The main motivation in approaching adequacy evaluation using CLTE is that an adequate translation and the source text should convey the same meaning. In terms of entailment, this means that an adequate MT output and the source sentence should entail each other (bi-directional entailment). Losing or altering part of the meaning conveyed by the source sentence (i.e. having more, or differe"
W12-3122,P11-1134,1,0.925957,"will change the entailment direction and, consequently, the adequacy judgement. Framed in this way, CLTE-based adequacy evaluation methods can be designed to distinguish meaning-preserving variations from true divergence, regardless of reference translations. Similarly to many monolingual TE approaches, CLTE solutions proposed so far adopt supervised learning methods, with features that measure to what extent the hypotheses can be mapped into the texts. The underlying assumption is that the probability of entailment is proportional to the number of words in H that can be mapped to words in T (Mehdad et al., 2011). Such mapping can be carried out at different word representation levels (e.g. tokens, lemmas, stems), possibly with the support of lexical knowledge in order to cross the language barrier between T and H (e.g. dictionaries, phrase tables). Under the same assumption, since in the adequacy evaluation framework the entailment relation should hold in both directions, the mapping is performed both from the source to the target and vice-versa, building on features extracted from both sentences. Moreover, to improve over previous CLTE methods and boost MT adequacy evaluation performance, we explore"
W12-3122,P12-2024,1,0.497952,"okens, lemmas, stems), possibly with the support of lexical knowledge in order to cross the language barrier between T and H (e.g. dictionaries, phrase tables). Under the same assumption, since in the adequacy evaluation framework the entailment relation should hold in both directions, the mapping is performed both from the source to the target and vice-versa, building on features extracted from both sentences. Moreover, to improve over previous CLTE methods and boost MT adequacy evaluation performance, we explore the joint contribution of a number of lexical, syntactic and semantic features (Mehdad et al., 2012). Concerning the features used, it’s worth observing that the cost of implementing our approach (in terms of required resources and linguistic processors), and the need of reference translations are intrinsically different bottlenecks for MT. While the limited availability of processing tools for some language pairs is a “temporary” bottleneck, the acquisition of multiple references is a “permanent” one. The former cost is reducing over time due to the progress in NLP research; the latter represents a fixed cost that has to be eliminated. Similar considerations hold regarding the need of annot"
W12-3122,P00-1056,0,0.120809,"provides English and Spanish dependency parsers. Phrase Table (PT) matching features are calculated as in (Mehdad et al., 2011), with a phrasal matching algorithm that takes advantage of a lexical phrase table extracted from a bilingual parallel corpus. The algorithm determines the number of phrases in the source (1 to 5-grams, at the level of tokens, lemmas and stems) that can be mapped into target word sequences, and vice-versa. To build our English-Spanish phrase table, we used the Europarl, News Commentary and United Nations SpanishEnglish parallel corpora. After tokenization, the Giza++ (Och and Ney, 2000) and the Moses toolkit (Koehn et al., 2007) were respectively used to align the corpora and extract the phrase table. Although the phrase table was generated using MT technology, its use to compute our features is still compatible with a system-independent approach since the extraction is carried out without tuning the process towards any particular task. Moreover, our phrase matching algorithm integrates matches from overlapping n-grams of different size and nature (tokens, lemmas and stems) which current MT decoding algorithms cannot explore for complexity reasons. Dependency Relation (DR) m"
W12-3122,W09-0404,0,0.0341673,"Missing"
W12-3122,P02-1040,0,0.124085,"Missing"
W12-3122,quirk-2004-training,0,0.208721,"ariants at five different linguistic levels: lexical, shallow-syntactic, syntactic, shallow-semantic and semantic. More similar to our approach, (Pad´o et al., 2009) proposed semantic adequacy metrics that exploit feature representations motivated by Textual Entailment (TE). Both metrics, however, highly depend on the availability of multiple reference translations. Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequa"
W12-3122,2006.amta-papers.25,0,0.0853718,"presentations. In order to integrate semantics more deeply into MT technology, in this paper we focus on the evaluation dimension. Restricting our investigation to some of the more pressing issues emerging from this area of research, we provide two main contributions. 1. An automatic evaluation method that avoids the use of reference translations. Most current metrics are based on comparisons between automatic translations and human references, and reward lexical similarity at the n-gram level (e.g. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006)). Due to the variability of natural languages in terms of possible ways to express the same meaning, reliable lexical similarity metrics depend on the availability of multiple hand-crafted (costly) realizations of the same source sentence in the target language. Our approach aims to avoid this bottleneck by adapting cross-lingual semantic inference capabilities and judging a translation only given the source sentence. 2. A method for evaluating translation adequacy. Most current solutions do not consistently reward translation adequacy (semantic equivalence between source sentence and target"
W12-3122,2010.jec-1.5,0,0.0357492,"l, shallow-syntactic, syntactic, shallow-semantic and semantic. More similar to our approach, (Pad´o et al., 2009) proposed semantic adequacy metrics that exploit feature representations motivated by Textual Entailment (TE). Both metrics, however, highly depend on the availability of multiple reference translations. Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-oriented features (e.g. “number of"
W12-3122,specia-etal-2010-dataset,0,0.180153,"guistic levels: lexical, shallow-syntactic, syntactic, shallow-semantic and semantic. More similar to our approach, (Pad´o et al., 2009) proposed semantic adequacy metrics that exploit feature representations motivated by Textual Entailment (TE). Both metrics, however, highly depend on the availability of multiple reference translations. Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-orien"
W12-3122,2011.mtsummit-papers.58,0,0.354932,"Missing"
W12-3122,2011.eamt-1.12,0,0.0381818,"tic, shallow-semantic and semantic. More similar to our approach, (Pad´o et al., 2009) proposed semantic adequacy metrics that exploit feature representations motivated by Textual Entailment (TE). Both metrics, however, highly depend on the availability of multiple reference translations. Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-oriented features (e.g. “number of punctuation ma"
W12-3122,P10-1062,0,0.0457323,"Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-oriented features (e.g. “number of punctuation marks”). As a result, a simple surface form variation is given the same importance of a content word variation that changes the meaning of the sentence. To the best of our knowledge, only (Specia et al., 2011) proposed an approach to frame MT evaluation as an adequacy estimation problem. However,"
W12-3122,W07-0734,0,\N,Missing
W12-3155,P02-1040,0,\N,Missing
W12-3155,P07-2045,1,\N,Missing
W12-3155,W07-0717,0,\N,Missing
W12-3155,macklovitch-2006-transtype2,0,\N,Missing
W13-2231,P11-1022,0,0.029072,"racy in automatically predicting such quality. Higher productivity increases depend on the capability of the MT system to output useful material that is close to be publishable “as is” (Denkowski and Lavie, 2012), and the capability to automatically identify and present to human translators only such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics like BLEU (Papineni e"
W13-2231,2005.mtsummit-papers.11,0,0.0453967,"Missing"
W13-2231,W12-3123,0,0.0636256,"Missing"
W13-2231,2012.iwslt-papers.12,0,0.0437826,"post-editings (lower HTER values) and pairs marked as rewritings (higher HTER values). Such separation corresponds to an HTER value around 0.4, which is significantly lower than the threshold of 0.7 proposed by the WMT-12 guidelines as a criterion to label sentences for which “a translation from scratch is necessary”. This confirms that our separation differs from those produced by partition methods based on human annotations or arbitrary HTER thresholds. Furthermore, our au8 Monolingual stem-to-stem exact matches between TGT and correct translation are inferred by computing the HTER, as in (Blain et al., 2012). 9 All ROUGE scores, described in (Lin, 2004), have been calculated using the software available at http://www. berouge.com. 10 Such partitions are: average effort scores = 3, human scores = 3-3-3, HTER score = 0.45. 245 optimizing a metric that takes into account the number of true and false positives (see below). Seventeen features proposed in (Specia et al., 2009) were extracted from each source-target pair. This feature set, fully described in (CallisonBurch et al., 2012), mainly takes into account the complexity of the source sentence (e.g. number of tokens, number of translations per so"
W13-2231,W04-1013,0,0.0441569,"rewritings (higher HTER values). Such separation corresponds to an HTER value around 0.4, which is significantly lower than the threshold of 0.7 proposed by the WMT-12 guidelines as a criterion to label sentences for which “a translation from scratch is necessary”. This confirms that our separation differs from those produced by partition methods based on human annotations or arbitrary HTER thresholds. Furthermore, our au8 Monolingual stem-to-stem exact matches between TGT and correct translation are inferred by computing the HTER, as in (Blain et al., 2012). 9 All ROUGE scores, described in (Lin, 2004), have been calculated using the software available at http://www. berouge.com. 10 Such partitions are: average effort scores = 3, human scores = 3-3-3, HTER score = 0.45. 245 optimizing a metric that takes into account the number of true and false positives (see below). Seventeen features proposed in (Specia et al., 2009) were extracted from each source-target pair. This feature set, fully described in (CallisonBurch et al., 2012), mainly takes into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the target trans"
W13-2231,P11-1124,0,0.0238105,"Missing"
W13-2231,W12-3102,0,0.137719,"Missing"
W13-2231,P12-2024,1,0.911436,"such quality. Higher productivity increases depend on the capability of the MT system to output useful material that is close to be publishable “as is” (Denkowski and Lavie, 2012), and the capability to automatically identify and present to human translators only such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics like BLEU (Papineni et al., 2002), the current trend is"
W13-2231,W12-3122,1,0.542784,"such quality. Higher productivity increases depend on the capability of the MT system to output useful material that is close to be publishable “as is” (Denkowski and Lavie, 2012), and the capability to automatically identify and present to human translators only such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics like BLEU (Papineni et al., 2002), the current trend is"
W13-2231,P13-2135,1,0.879764,"Missing"
W13-2231,P02-1040,0,0.117005,"al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics like BLEU (Papineni et al., 2002), the current trend is to rely on human annotations, which seem to lead to more accurate models (Quirk, 2004; Specia et al., 2009). Along this direction, the QE task consists in predicting scores that reflect human quality judgements, by learning from manually annotated datasets (e.g. collections of source-target pairs laSupervised approaches to NLP tasks rely on high-quality data annotations, which typically result from expensive manual labelling procedures. For some tasks, however, the subjectivity of human judgements might reduce the usefulness of the annotation for real-world applications."
W13-2231,2012.amta-papers.6,0,0.0138318,"e Bruno Kessler, FBK-irst Trento , Italy {turchi|negri|federico}@fbk.eu Abstract instance, MT and TM translations can be automatically ranked to ease the selection of the most suitable one for post-editing (He et al., 2010), or the TM can be used to constrain and improve MT suggestions (Ma et al., 2011). In all cases, the effectiveness of the integration is conditioned by: i) the quality of MT, and ii) the accuracy in automatically predicting such quality. Higher productivity increases depend on the capability of the MT system to output useful material that is close to be publishable “as is” (Denkowski and Lavie, 2012), and the capability to automatically identify and present to human translators only such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al"
W13-2231,P10-1064,0,0.0195728,"Missing"
W13-2231,P07-2045,1,0.0080684,"Missing"
W13-2231,quirk-2004-training,0,0.200826,"ulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics like BLEU (Papineni et al., 2002), the current trend is to rely on human annotations, which seem to lead to more accurate models (Quirk, 2004; Specia et al., 2009). Along this direction, the QE task consists in predicting scores that reflect human quality judgements, by learning from manually annotated datasets (e.g. collections of source-target pairs laSupervised approaches to NLP tasks rely on high-quality data annotations, which typically result from expensive manual labelling procedures. For some tasks, however, the subjectivity of human judgements might reduce the usefulness of the annotation for real-world applications. In Machine Translation (MT) Quality Estimation (QE), for instance, using humanannotated data to train a bin"
W13-2231,W09-0441,0,0.0261665,", checking whether it can be partitioned in a way that reflects the distinction between good (useful for the translator, suitable for post editing) and bad translations (that need complete rewriting).2 To this aim we experiment with the QE data released within the 7th Workshop on Machine Translation (WMT-12). The corpus consists of source-target pairs annotated with manual QE labels (1-5 scores) indicating the post-editing needed to correct the translations. Besides explicit human judgements, the availability of post-edited translations makes also possible to calculate the actual HTER values (Snover et al., 2009), indicating the minimum edit distance between the machine translation and its manually post-edited version in the [0,1] interval. The second option is to automatically reannotate the same dataset, trying to produce labels that reflect an objective and more reliable binary distinction based on empirical observations. Our analysis aims to answer the following questions: and replicable way compared to current data annotation methods. By answering these questions, this paper provides the following main contributions: • We show that training a binary classifier on arbitrary partitions of an existi"
W13-2231,P10-1063,0,0.0542327,"lity of MT, and ii) the accuracy in automatically predicting such quality. Higher productivity increases depend on the capability of the MT system to output useful material that is close to be publishable “as is” (Denkowski and Lavie, 2012), and the capability to automatically identify and present to human translators only such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics li"
W13-2231,W12-3118,0,0.115285,"such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics like BLEU (Papineni et al., 2002), the current trend is to rely on human annotations, which seem to lead to more accurate models (Quirk, 2004; Specia et al., 2009). Along this direction, the QE task consists in predicting scores that reflect human quality judgements, by learning from manually annotated datasets (e.g. col"
W13-2231,2009.eamt-1.5,1,0.915027,"tioned by: i) the quality of MT, and ii) the accuracy in automatically predicting such quality. Higher productivity increases depend on the capability of the MT system to output useful material that is close to be publishable “as is” (Denkowski and Lavie, 2012), and the capability to automatically identify and present to human translators only such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automa"
W13-2231,2011.mtsummit-papers.58,0,0.0205377,"s with estimates of the expected quality of each MT suggestion. Such intuitive solution, however, disregards the fact that even precise QE scores would not alleviate translators from the effort of reading useless MT output (or at least the associated score). A more effective alternative is to use the estimated QE scores to filter out poor MT suggestions, presenting only those worth for post-editing. Binary classification, however, has to confront with the problem of setting reasonable cut-off criteria. The arbitrary thresholds, used in several previous works (Quirk, 2004; Specia et al., 2010; Specia et al., 2011) are in fact hard to justify, and even harder to learn from human-labelled training data. 1. Are human labels reliable and coherent enough to train accurate binary models? 2. Are arbitrarily-set thresholds useful to partition QE data for this task? 3. Is it possible to obtain reliable binary annotations from an automatic procedure? Negative answers to the first two questions would respectively call into question: i) the intuitive idea that human labels are the most reliable for a supervised approach to binary QE, and ii) the possibility that thresholds on a single metric (e.g. the HTER) can be"
W13-2231,2011.eamt-1.12,0,0.255609,"ly predicting such quality. Higher productivity increases depend on the capability of the MT system to output useful material that is close to be publishable “as is” (Denkowski and Lavie, 2012), and the capability to automatically identify and present to human translators only such suggestions. Recognizing good translations falls in the scope of research on automatic MT Quality Estimation (QE), which addresses the problem of estimating the quality of a translated sentence at run-time, without access to reference translations (Specia et al., 2009; Soricut and Echihabi, 2010; Bach et al., 2011; Specia, 2011; Mehdad et al., 2012b). In recent years QE gained increasing interest in the MT community, resulting in several datasets available for training and evaluation (Callison-Burch et al., 2012), the definition of features showing good correlation with human judgements (Soricut et al., 2012), and the release of open-source software.1 The proposed solutions to the QE problem rely on supervised methods that strongly depend on the availability of labelled data. While early works (Blatz et al., 2003) exploited annotations obtained with automatic MT evaluation metrics like BLEU (Papineni et al., 2002),"
W13-2231,S13-2023,1,0.863131,"Missing"
W13-2231,C04-1046,0,\N,Missing
W13-2237,2013.mtsummit-papers.5,1,0.522852,"l., 2006) computed using the MultEval script (Clark et al., 2011). Since the implementations of standard Giza and of incremental Giza combined with dynamic suffix arrays are not comparable, we constructed two baselines, a standard phrase based SMT system and an incremental Giza baseline (§5). Details on experimental SMT systems we built follow. actually generated from scratch. In our experiments, the extent of usefulness of online learning highly depends on the amount of repetition of text. A reasonable way to measure the quantity of repetition in each document is through the repetition rate (Bertoldi et al., 2013). It computes the rate of non-singleton n-grams, n=1...4, averaging the values over sub-samples S of thousand words from the text, and then combining the rate of each n-gram to a single score by using the geometric mean. Equation 7 shows the formula for calculating the repetition rate of a document, where dict(n) represents the total number of different n-grams and nr is the number of different n-grams occurring exactly r times: RR = !1/4 4 P Y dict(n) − n 1 S P S dict(n) Baseline This system was built on the parallel training data for each domain. We run 5 iterations of model 1, 5 of HMM (Vog"
W13-2237,J93-2003,0,0.0390959,"s, n=1...4, averaging the values over sub-samples S of thousand words from the text, and then combining the rate of each n-gram to a single score by using the geometric mean. Equation 7 shows the formula for calculating the repetition rate of a document, where dict(n) represents the total number of different n-grams and nr is the number of different n-grams occurring exactly r times: RR = !1/4 4 P Y dict(n) − n 1 S P S dict(n) Baseline This system was built on the parallel training data for each domain. We run 5 iterations of model 1, 5 of HMM (Vogel et al., 1996), 3 of model 3, 3 of model 4 (Brown et al., 1993) using MGiza (Gao and Vogel, 2008) toolkit to align the parallel corpus at word level. Translation and reordering models were built using Moses, while log-linear weights were optimized with MERT on the corresponding development sets. The same IT baseline system was used in the field test of MateCat and the references in the IT data are actual postedits of its translation. (7) n=1 Statistics of the parallel sets and their repetition rate on both sides are reported in Table 1. Domain ITen→it TEDen→f r Set #srcTok srcRR #tgtTok Train 57M na 60M Dev 3.3k 12.03 3.5k Test 3.3k 15.00 3.3k Train 2.6M"
W13-2237,P05-1032,0,0.011504,"e aggressive j subject to lj ≤ ∆hj · w + ξj 303 ∀j ∈ J ⊆ {1 . . . N } (5) word alignments and language models (Levenberg and Osborne, 2009). To get the word alignments on the new data they use a Stepwise online EM algorithm, where old counts (from previous alignment models) are interpolated with the new counts. Since we work at the sentence level, on-thefly computation of probabilities of translation and reordering models is expensive in terms of both computational and memory requirements. To save these costs, we prefer using dynamic suffix array approach described in (Levenberg et al., 2010; Callison-Burch et al., 2005; Lopez, 2008). They are used to efficiently store the source and the target corpus and alignments in efficient data structure, namely the suffix array. When a phrase translation is asked by the decoder, the corpus is searched, the counts are collected and its probabilities are computed on the fly. However, the current implementation in Moses of the stream based MT relying on the suffix arrays is severely limited as it allows the computation of only three translation features, namely the two direct translation probabilities and the phrase penalty. This results in a significant degradation of p"
W13-2237,2012.eamt-1.60,1,0.460333,"j = Optimize(lP j , hj , w, C); wi = wi−1 + η · j αj ∆hj ; end end end end 6 Experiments 6.1 Datasets We compared our online learning approaches (Sections 3 and 4) and the stream based adaptation method (Section 5) on two datasets from different domains, namely Information Technology (IT) and TED talks, and two different language pairs. The IT domain dataset is proprietary, it involves the translation of technical documents from English to Italian and has been used in the field test carried out under the MateCat project2 . Experiments are also conducted on English to French TED talks dataset (Cettolo et al., 2012) to assess the robustness of the proposed approaches in a different scenario and to provide results on a publicly available dataset for the sake of reproducibility. The training, development (dev2010) and evaluation (tst20103 ) sets are the same as used in the last IWSLT last evaluation campaigns. In experiments on TED data, we considered the human reference translations as post edits, even if they were In the following section we overview a stream based adaptation method with which we experimentally compared our two online learning approaches as it well fits the framework we are working in. 5"
W13-2237,C08-1064,0,0.0103257,"≤ ∆hj · w + ξj 303 ∀j ∈ J ⊆ {1 . . . N } (5) word alignments and language models (Levenberg and Osborne, 2009). To get the word alignments on the new data they use a Stepwise online EM algorithm, where old counts (from previous alignment models) are interpolated with the new counts. Since we work at the sentence level, on-thefly computation of probabilities of translation and reordering models is expensive in terms of both computational and memory requirements. To save these costs, we prefer using dynamic suffix array approach described in (Levenberg et al., 2010; Callison-Burch et al., 2005; Lopez, 2008). They are used to efficiently store the source and the target corpus and alignments in efficient data structure, namely the suffix array. When a phrase translation is asked by the decoder, the corpus is searched, the counts are collected and its probabilities are computed on the fly. However, the current implementation in Moses of the stream based MT relying on the suffix arrays is severely limited as it allows the computation of only three translation features, namely the two direct translation probabilities and the phrase penalty. This results in a significant degradation of performance. wh"
W13-2237,P11-2031,0,0.0118167,"as it well fits the framework we are working in. 5 Stream based adaptation Continuously updating an SMT system to an incoming stream of parallel data comes under stream based adaptation. Levenberg et. al. (2010) proposed an incremental adaptation technique for the core generative component of the SMT system, 2 www.matecat.com As the size of evaluation set in TED data is too large with respect to the current implementation of our algorithms, we performed evaluation on the first 200 sentences only. 3 304 measured in terms of BLEU and TER (Snover et al., 2006) computed using the MultEval script (Clark et al., 2011). Since the implementations of standard Giza and of incremental Giza combined with dynamic suffix arrays are not comparable, we constructed two baselines, a standard phrase based SMT system and an incremental Giza baseline (§5). Details on experimental SMT systems we built follow. actually generated from scratch. In our experiments, the extent of usefulness of online learning highly depends on the amount of repetition of text. A reasonable way to measure the quantity of repetition in each document is through the repetition rate (Bertoldi et al., 2013). It computes the rate of non-singleton n-g"
W13-2237,W02-1001,0,0.0910054,"following idea. For each N-best translation (candidate) in the search space, we compute a similarity score against the postedit using the sentence-level BLEU metric (Lin and Och, 2004), a smoothed variant of the popular BLEU metric (Papineni et al., 2001). We hence compare the similarity score of each candidate against the similarity score achieved by the bestHyp, that was also computed against the postedit. If the candidate 2.1 Online Adaptation during Tuning Liang et. al. (2006) improved SMT performance by online adaptation of scaling factors (λ in (1)) using averaged perceptron algorithm (Collins, 2002). They presented different strategies to update the SMT models towards reference or oracle translation: (1) aggressively updating towards reference, bold update; (2) update towards the oracle translation in N-Best list, local update; (3) a hybrid approach in which a bold update is performed when the reference is reachable, otherwise a local update is performed. Liang and Klein (2009) compared two online EM algorithms, stepwise online EM (Sato and Ishii, 2000; Capp´e and Moulines, 2007) and incremental EM (Neal and Hinton, 1998) which they use to update the alignment models (the generative comp"
W13-2237,2012.amta-papers.22,1,0.705074,"uctivity of a human translator. A CAT tool comes as a package of a Translation Memory (TM), builtin spell checkers, a dictionary, a terminology list etc. which help the translator while translating a sentence. Recent research has led to the integration of CAT tools with statistical machine translation (SMT) engines. SMT makes use of a large available parallel corpus to generate statistical models for translation. Due to their generalization capability, SMT systems are a good fit in this scenario and a seamless integration of SMT engines in CAT have shown to increase translator’s productivity (Federico et al., 2012). Although automatic systems generate reliable translations they are not accurate enough to be used directly and need postedition by human translators. In state-of-the-art CAT tools, the SMT systems are static in nature and so they cannot adapt In this paper, we implemented two online learning methods through which a phrase-based SMT system evolves over time, sentence after sentence, by taking advantage of the post-edition or translation of the previous sentence by the user.1 In the first approach, we focus on the translation model aspect of SMT which is represented by five conventional featur"
W13-2237,J03-1002,0,0.0193141,"for 1 Moses code is available in the github repository. https://github.com/mtresearcher/ mosesdecoder/tree/moses_onlinelearning 301 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 301–308, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics on the fly. However, stepwise EM is prone to failure if mini-batch size and stepsize parameters are not chosen correctly, while incremental EM requires substantial storage costs because it has to store sufficient statistics for each sample. Other works on online minimum error rate training in SMT (Och and Ney, 2003) that deserve mentioning are (Hopkins and May, 2011; Hasler et al., 2011). the translation hypothesis as shown in Equation 1. score(e∗ , f ) = Σi λi hi (e∗ , f ) (1) where hi (·) are the feature functions representing the models and λi are the linear weights. The highest scored translation is the best hypothesis e∗ output by the system. We extend the translation model with a new feature which provides extra phrase-pair scores changing according to the user feedback. The scores of the new feature are adapted in a discriminative fashion, by rewarding phrase-pairs observed in the search space and"
W13-2237,W08-0509,0,0.0318654,"over sub-samples S of thousand words from the text, and then combining the rate of each n-gram to a single score by using the geometric mean. Equation 7 shows the formula for calculating the repetition rate of a document, where dict(n) represents the total number of different n-grams and nr is the number of different n-grams occurring exactly r times: RR = !1/4 4 P Y dict(n) − n 1 S P S dict(n) Baseline This system was built on the parallel training data for each domain. We run 5 iterations of model 1, 5 of HMM (Vogel et al., 1996), 3 of model 3, 3 of model 4 (Brown et al., 1993) using MGiza (Gao and Vogel, 2008) toolkit to align the parallel corpus at word level. Translation and reordering models were built using Moses, while log-linear weights were optimized with MERT on the corresponding development sets. The same IT baseline system was used in the field test of MateCat and the references in the IT data are actual postedits of its translation. (7) n=1 Statistics of the parallel sets and their repetition rate on both sides are reported in Table 1. Domain ITen→it TEDen→f r Set #srcTok srcRR #tgtTok Train 57M na 60M Dev 3.3k 12.03 3.5k Test 3.3k 15.00 3.3k Train 2.6M na 2.8M Dev 20k 3.43 20k Test 32k"
W13-2237,P03-1021,0,0.026349,"update), and a learning rate for feature weights used by MIRA. These additional parameters were optimized by maximizing the BLEU 6.2 Systems The SMT systems were built using the Moses toolkit (Koehn et al., 2007). Training data in each domain was used to create translation and lexical reordering models. We created a 5-gram LM for TED talks and a 6-gram LM for the IT domain using IRSTLM (Federico et al., 2008) with improved Kneser-Ney smoothing (Chen and Goodman, 1996) on the target side of the training parallel corpora. The log linear weights for the baseline systems are optimized using MERT (Och, 2003) provided in the Moses toolkit. To counter the instability of MERT, we averaged the weights of three MERT runs in each case. Performance is 4 305 http://code.google.com/p/inc-giza-pp/ probability distribution. We empirically proved that this method is robust and works for different domain datasets be it Information Technology or TED talks. In addition, if the repetition rate is high in the text, online learning works much better than if the rate is low. We tested both with an unbounded and a bounded range on the online feature and found out that bounded values produce more stable and consisten"
W13-2237,2001.mtsummit-papers.68,0,0.0121164,"ce segment, post-edits it and finally approves it. From the SMT point of view, for each source segment the decoder explores a search space of possible translations and finally returns the best scoring one (bestHyp) to the user. The user possibly corrects this suggestion thus generating the final translation (postedit). Our online learning procedure is based on the following idea. For each N-best translation (candidate) in the search space, we compute a similarity score against the postedit using the sentence-level BLEU metric (Lin and Och, 2004), a smoothed variant of the popular BLEU metric (Papineni et al., 2001). We hence compare the similarity score of each candidate against the similarity score achieved by the bestHyp, that was also computed against the postedit. If the candidate 2.1 Online Adaptation during Tuning Liang et. al. (2006) improved SMT performance by online adaptation of scaling factors (λ in (1)) using averaged perceptron algorithm (Collins, 2002). They presented different strategies to update the SMT models towards reference or oracle translation: (1) aggressively updating towards reference, bold update; (2) update towards the oracle translation in N-Best list, local update; (3) a hy"
W13-2237,D11-1125,0,0.0271454,"sitory. https://github.com/mtresearcher/ mosesdecoder/tree/moses_onlinelearning 301 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 301–308, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics on the fly. However, stepwise EM is prone to failure if mini-batch size and stepsize parameters are not chosen correctly, while incremental EM requires substantial storage costs because it has to store sufficient statistics for each sample. Other works on online minimum error rate training in SMT (Och and Ney, 2003) that deserve mentioning are (Hopkins and May, 2011; Hasler et al., 2011). the translation hypothesis as shown in Equation 1. score(e∗ , f ) = Σi λi hi (e∗ , f ) (1) where hi (·) are the feature functions representing the models and λi are the linear weights. The highest scored translation is the best hypothesis e∗ output by the system. We extend the translation model with a new feature which provides extra phrase-pair scores changing according to the user feedback. The scores of the new feature are adapted in a discriminative fashion, by rewarding phrase-pairs observed in the search space and in the reference, and penalizing phrase-pairs obse"
W13-2237,P07-2045,1,0.0128033,"+O+NS as (1) but with the online feature normalized with the sigmoid function; (3) +W weights updated (Section 4) with Algorithm 2; (4) +O+W combination of online feature and weight update; (5) +O+NS+W as system (4) with normalized online feature score. In the online learning system we have three additional parameters: a weight for the online feature, a learning rate for features (used in the perceptron update), and a learning rate for feature weights used by MIRA. These additional parameters were optimized by maximizing the BLEU 6.2 Systems The SMT systems were built using the Moses toolkit (Koehn et al., 2007). Training data in each domain was used to create translation and lexical reordering models. We created a 5-gram LM for TED talks and a 6-gram LM for the IT domain using IRSTLM (Federico et al., 2008) with improved Kneser-Ney smoothing (Chen and Goodman, 1996) on the target side of the training parallel corpora. The log linear weights for the baseline systems are optimized using MERT (Och, 2003) provided in the Moses toolkit. To counter the instability of MERT, we averaged the weights of three MERT runs in each case. Performance is 4 305 http://code.google.com/p/inc-giza-pp/ probability distri"
W13-2237,D09-1079,0,0.0191269,"; end end end end end ∆h · w ˆ = score(y ∗ ) − score(ˆ y) (4) MIRA is an ultraconservative algorithm, meaning that the update of the current weight vector is the smallest possible value satisfying the constraint that the variation incurred by the objective function must not be larger than the variation incurred by the model (plus a non-negative slack variable ξ). Formally, weight update at ith iteration is defined as: X 1 wi = arg min C ||w − wi−1 ||2 + |{z} ξj w 2η | {z } conservative aggressive j subject to lj ≤ ∆hj · w + ξj 303 ∀j ∈ J ⊆ {1 . . . N } (5) word alignments and language models (Levenberg and Osborne, 2009). To get the word alignments on the new data they use a Stepwise online EM algorithm, where old counts (from previous alignment models) are interpolated with the new counts. Since we work at the sentence level, on-thefly computation of probabilities of translation and reordering models is expensive in terms of both computational and memory requirements. To save these costs, we prefer using dynamic suffix array approach described in (Levenberg et al., 2010; Callison-Burch et al., 2005; Lopez, 2008). They are used to efficiently store the source and the target corpus and alignments in efficient"
W13-2237,2006.amta-papers.25,0,0.102455,"perimentally compared our two online learning approaches as it well fits the framework we are working in. 5 Stream based adaptation Continuously updating an SMT system to an incoming stream of parallel data comes under stream based adaptation. Levenberg et. al. (2010) proposed an incremental adaptation technique for the core generative component of the SMT system, 2 www.matecat.com As the size of evaluation set in TED data is too large with respect to the current implementation of our algorithms, we performed evaluation on the first 200 sentences only. 3 304 measured in terms of BLEU and TER (Snover et al., 2006) computed using the MultEval script (Clark et al., 2011). Since the implementations of standard Giza and of incremental Giza combined with dynamic suffix arrays are not comparable, we constructed two baselines, a standard phrase based SMT system and an incremental Giza baseline (§5). Details on experimental SMT systems we built follow. actually generated from scratch. In our experiments, the extent of usefulness of online learning highly depends on the amount of repetition of text. A reasonable way to measure the quantity of repetition in each document is through the repetition rate (Bertoldi"
W13-2237,N10-1062,0,0.368748,"w 2η | {z } conservative aggressive j subject to lj ≤ ∆hj · w + ξj 303 ∀j ∈ J ⊆ {1 . . . N } (5) word alignments and language models (Levenberg and Osborne, 2009). To get the word alignments on the new data they use a Stepwise online EM algorithm, where old counts (from previous alignment models) are interpolated with the new counts. Since we work at the sentence level, on-thefly computation of probabilities of translation and reordering models is expensive in terms of both computational and memory requirements. To save these costs, we prefer using dynamic suffix array approach described in (Levenberg et al., 2010; Callison-Burch et al., 2005; Lopez, 2008). They are used to efficiently store the source and the target corpus and alignments in efficient data structure, namely the suffix array. When a phrase translation is asked by the decoder, the corpus is searched, the counts are collected and its probabilities are computed on the fly. However, the current implementation in Moses of the stream based MT relying on the suffix arrays is severely limited as it allows the computation of only three translation features, namely the two direct translation probabilities and the phrase penalty. This results in a"
W13-2237,C96-2141,0,0.0450371,"13). It computes the rate of non-singleton n-grams, n=1...4, averaging the values over sub-samples S of thousand words from the text, and then combining the rate of each n-gram to a single score by using the geometric mean. Equation 7 shows the formula for calculating the repetition rate of a document, where dict(n) represents the total number of different n-grams and nr is the number of different n-grams occurring exactly r times: RR = !1/4 4 P Y dict(n) − n 1 S P S dict(n) Baseline This system was built on the parallel training data for each domain. We run 5 iterations of model 1, 5 of HMM (Vogel et al., 1996), 3 of model 3, 3 of model 4 (Brown et al., 1993) using MGiza (Gao and Vogel, 2008) toolkit to align the parallel corpus at word level. Translation and reordering models were built using Moses, while log-linear weights were optimized with MERT on the corresponding development sets. The same IT baseline system was used in the field test of MateCat and the references in the IT data are actual postedits of its translation. (7) n=1 Statistics of the parallel sets and their repetition rate on both sides are reported in Table 1. Domain ITen→it TEDen→f r Set #srcTok srcRR #tgtTok Train 57M na 60M Dev"
W13-2237,N09-1069,0,0.0336169,"lso computed against the postedit. If the candidate 2.1 Online Adaptation during Tuning Liang et. al. (2006) improved SMT performance by online adaptation of scaling factors (λ in (1)) using averaged perceptron algorithm (Collins, 2002). They presented different strategies to update the SMT models towards reference or oracle translation: (1) aggressively updating towards reference, bold update; (2) update towards the oracle translation in N-Best list, local update; (3) a hybrid approach in which a bold update is performed when the reference is reachable, otherwise a local update is performed. Liang and Klein (2009) compared two online EM algorithms, stepwise online EM (Sato and Ishii, 2000; Capp´e and Moulines, 2007) and incremental EM (Neal and Hinton, 1998) which they use to update the alignment models (the generative component of SMT) 302 scores better than the bestHyp, then we promote the building blocks, i.e. phrase-pairs, of candidate that were not used in bestHyp and demote the phrase-pairs used in bestHyp that were not used for candidate. On the contrary, if the candidate scores worse than the bestHyp, we promote the building blocks of bestHyp that are not in candidate and demote those of candid"
W13-2237,P06-1096,0,0.0610588,"Missing"
W13-2237,C04-1072,0,0.0423539,"scenario, the user receives a translation suggestion for each source segment, post-edits it and finally approves it. From the SMT point of view, for each source segment the decoder explores a search space of possible translations and finally returns the best scoring one (bestHyp) to the user. The user possibly corrects this suggestion thus generating the final translation (postedit). Our online learning procedure is based on the following idea. For each N-best translation (candidate) in the search space, we compute a similarity score against the postedit using the sentence-level BLEU metric (Lin and Och, 2004), a smoothed variant of the popular BLEU metric (Papineni et al., 2001). We hence compare the similarity score of each candidate against the similarity score achieved by the bestHyp, that was also computed against the postedit. If the candidate 2.1 Online Adaptation during Tuning Liang et. al. (2006) improved SMT performance by online adaptation of scaling factors (λ in (1)) using averaged perceptron algorithm (Collins, 2002). They presented different strategies to update the SMT models towards reference or oracle translation: (1) aggressively updating towards reference, bold update; (2) updat"
W13-2237,P02-1040,0,\N,Missing
W13-2257,kim-etal-2000-machine,0,0.117241,"Missing"
W13-2257,2012.eamt-1.42,0,0.234824,"verb is separated from the inflected auxiliary or modal. The distance between the two parts of a verb phrase can be arbitrarily long as shown in the following example: selected by the decoder at translation time. In (Tromble and Eisner, 2009), pre-ordering is cast as a permutation problem and solved by a model that estimates the probability of reversing the relative order of any two input words. In the field of tree-based SMT, positive results in German-English were achieved by combining syntactic translation rules with unlabeled hierarchical SMT rules (Hoang and Koehn, 2010). More recently, Braune et al. (2012) proposed to improve the long-range reordering capability of an HSMT system by integrating constraints based on clausal boundaries and by manually selecting the rule patterns applicable to long word spans. The paper did not analyse the impact of the technique on efficiency. [DE] Jedoch konnten sie Kinder in Teilen von Helmand und Kandahar im Suden aus Sicherheitsgrund nicht er¨ reichen. [EN] But they could not reach children in parts of Helmand and Kandahar in the south for security reasons. Translating this sentence with a PSMT engine implies performing two very long jumps that are not even c"
W13-2257,W12-3102,0,0.0446434,"Missing"
W13-2257,2011.mtsummit-papers.1,1,0.769012,"d two lexical probability features), a 6-gram target language model, word and rule penalties. We set the span constraint (cf. Section 5) to the default value of 10 words for rule extraction, while for decoding we consider two different settings: the default 10 words and a large value of 20 to enable very longrange reorderings. Feature weights for all systems are optimized by minimum BLEU-error training (Och, 2003) on test08. To reduce the effects of the optimizer instability, we tune each configuration four times and use the average of the resulting weight vectors for testing, as suggested by Cettolo et al. (2011). The source-to-reference word alignments that are needed to compute the reordering scores are generated by the Berkeley Aligner previously trained on the training data. Source-to-output alignments are obtained from the decoder’s trace. 6.2 Figure 1: Standard vs early distortion cost performance measured in terms of BLEU and KRS on tests(09-11) under different distortion limits. Distortion function and limit We start by measuring the difference between standard and early distortion cost.7 Figure 1 shows the results in terms of BLEU and KRS, plotted against the distortion limit (DL). Indeed, ea"
W13-2257,P05-1033,0,0.537828,"the German-English language pair is known to be a challenging task for state-of-theart statistical machine translation (SMT) methods. A major factor of difficulty is given by word order differences that yield important long-range reordering phenomena. Thanks to specific reordering modeling components, phrase-based SMT (PSMT) systems (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2002) are generally good at handling local reordering phenomena that are not captured inside phrases. However, they typically fail to predict long reorderings. On the other hand, hierarchical SMT (HSMT) systems (Chiang, 2005) can learn reordering patterns by means of discontinuous translation rules, and are therefore considered a better choice for language pairs characterized by massive and hierarchical reordering. 2 Background Word order differences between German and English are mainly found at the clause (global) level, as opposed to the phrase (local) level. We refer to Collins et al. (2005) and Gojun and Fraser (2012) for a detailed description of the German clause structure. To briefly summarize, we can say that 440 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 440–451, c Sofia"
W13-2257,P06-1067,0,0.575662,"Missing"
W13-2257,J07-2003,0,0.0679615,"features), a 6-gram target language model, distortion cost, word and phrase penalties. As lexicalized reordering model, we use a hierarchical phrase orientation model (Galley and Manning, 2008) trained on all the parallel data using three orientation classes – monotone, swap or discontinuous – in bidirectional mode. Statistically Furthermore, the HSMT decoder is based on a chart parsing algorithm, whose complexity is cubic in the input length, and even higher when taking into account the target language model. This issue can be partially addressed by different strategies such as cube pruning (Chiang, 2007), which reduces the LM complexity to a constant, or rule application constraints. One of such constraints is the maximum number of source words that may be covered by non-terminal symbols (span constraint). Setting a span constraint – which is essential to obtain reasonable decoding times – means preventing long-range reordering similarly to setting a distortion limit in PSMT. In our experiments, we consider two settings for this parameter: 10 to capture short to medium-range reorderings, and 20 to also capture long-range reorderings. 6 Experimental setup Experiments In this section we evaluat"
W13-2257,P05-1066,0,0.814336,"2003; Och and Ney, 2002) are generally good at handling local reordering phenomena that are not captured inside phrases. However, they typically fail to predict long reorderings. On the other hand, hierarchical SMT (HSMT) systems (Chiang, 2005) can learn reordering patterns by means of discontinuous translation rules, and are therefore considered a better choice for language pairs characterized by massive and hierarchical reordering. 2 Background Word order differences between German and English are mainly found at the clause (global) level, as opposed to the phrase (local) level. We refer to Collins et al. (2005) and Gojun and Fraser (2012) for a detailed description of the German clause structure. To briefly summarize, we can say that 440 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 440–451, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics the verb-second order of German main clauses contrasts with the rigid SVO structure of English, as does the clause-final verb position of German subordinate clauses. A further difficulty is given by the German discontinuous verb phrases, where the main verb is separated from the inflected auxiliary o"
W13-2257,W05-0909,0,0.0128905,"its syntactic parse tree (Collins et al., 2005).1 Other approaches learn the pre-ordering rules automatically, from syntactic parses (Xia and McCord, 2004; Genzel, 2010) or from part-of-speech labels (Niehues and Kolss, 2009). In the former case, pre-ordering decisions are typically taken deterministically (i. e. one permuation per sentence), whereas in the latter, multiple alternatives are represented as word lattices, and the optimal path is A large number of previous works on word reordering measured their success with generalpurpose metrics such as BLEU (Papineni et al., 2001) or METEOR (Banerjee and Lavie, 2005). These metrics, however, are only indirectly sensitive to word order and do not sufficiently penalize long-range reordering errors, as demonstrated for instance by Birch et al. (2010). While BLEU remains a standard choice for many evaluation campaigns, we believe it is extremely important to complement it with metrics that are specifically designed to capture word order differences. In this work, we adopt two reordering-specific metrics in addition to BLEU and METEOR: Kendall Reordering Score (KRS). As proposed by Birch et al. (2010), the KRS measures the similarity between the input-output r"
W13-2257,D08-1078,0,0.465777,"ystems) the rule-based systems outperformed all SMT approaches, and among the best SMT systems we find a variety of approaches: pure phrase-based, phrase-based and hierarchical systems combination, n-gram based, a rich syntaxbased approach, and a phrase-based system coupled with POS-based pre-ordering. This gives an idea of how challenging this language pair is for SMT and raises the question of which SMT approach is best suited to model it. In this work, we aim at answering this question by focussing on the word reordering problem, which is known to be an important factor of SMT performance (Birch et al., 2008). We hypothesize that PSMT can be as successful for GermanEnglish as the more computationally costly HSMT approach, provided that the reordering-related parameters are carefully chosen and the best available reordering models are used. More specifically, our study covers the following topics: distortion functions and limits, and dynamic shaping of the reordering search space based on a discriminative reordering model. We first review these topics, and then evaluate them systematically on the WMT task using both generic and reordering-specific metrics, with the aim of providing a reference for"
W13-2257,2010.amta-papers.22,0,0.361518,"ring model that predicts what input word should be translated at a given decoding state (Bisazza, 2013; Bisazza and Federico, 2013). The model is similar to the one proposed by Visweswariah et al. (2011), however we use it differently: that is, not simply for data preprocessing but as an additional feature function fully integrated in the phrase-based decoder. More importantly, we propose to use the same model to dynamically shape the space of reorderings explored during decoding (cf. Section 4.2), which was never done before. Another related work is the source-side decoding sequence model by Feng et al. (2010), that is a generative n-gram model trained on a corpus of pre-ordered source sentences. Although reminiscent of a source-side bigram model, our model has two important differences: (i) the discriminative modeling framework enables us to design a much richer feature set including, for instance, the context of the next word to pick; (ii) all our features are independent from the decoding history, which allows for an efficient decoder-integration with no effect on hypothesis recombination. Finally, we have to mention the models by AlOnaizan and Papineni (2006) and Green et al. (2010), who predic"
W13-2257,Q13-1027,1,0.900171,"eful for short and medium-range reordering and are probably the most widely used in PSMT nowadays. However, their coarse classification of reordering steps makes them unsuitable to capture long-range reordering phenomena, such as those attested in German-English. Indeed, Galley and Manning (2008) reported a decrease of translation quality when the distortion limit was set beyond 6 in Chinese-English and beyond 4 in Arabic-English. To address this problem, we have developed a different reordering model that predicts what input word should be translated at a given decoding state (Bisazza, 2013; Bisazza and Federico, 2013). The model is similar to the one proposed by Visweswariah et al. (2011), however we use it differently: that is, not simply for data preprocessing but as an additional feature function fully integrated in the phrase-based decoder. More importantly, we propose to use the same model to dynamically shape the space of reorderings explored during decoding (cf. Section 4.2), which was never done before. Another related work is the source-side decoding sequence model by Feng et al. (2010), that is a generative n-gram model trained on a corpus of pre-ordered source sentences. Although reminiscent of"
W13-2257,D08-1089,0,0.0669847,"olkit (Koehn et al., 2007; Hoang et al., 2009), and word alignments are generated by the Berkeley Aligner (Liang et al., 2006). The target language model is estimated by the IRSTLM toolkit (Federico et al., 2008) with modified Kneser-Ney smoothing (Chen and Goodman, 1999). The phrase-based baseline decoder includes a phrase translation model (two phrasal and two lexical probability features), a lexicalized reordering model (six features), a 6-gram target language model, distortion cost, word and phrase penalties. As lexicalized reordering model, we use a hierarchical phrase orientation model (Galley and Manning, 2008) trained on all the parallel data using three orientation classes – monotone, swap or discontinuous – in bidirectional mode. Statistically Furthermore, the HSMT decoder is based on a chart parsing algorithm, whose complexity is cubic in the input length, and even higher when taking into account the target language model. This issue can be partially addressed by different strategies such as cube pruning (Chiang, 2007), which reduces the LM complexity to a constant, or rule application constraints. One of such constraints is the maximum number of source words that may be covered by non-terminal"
W13-2257,N06-1014,0,0.023422,"es one reference translation. Note that our goal is not to reach the performance of the best systems participating at the last WMT edition, but rather to assess the usefulness of our techniques on a larger and therefore more reliable test set, while starting from a reasonable baseline.5 For German tokenization and compound splitting we use Tree Tagger (Schmid, 1994) and the Gertwol morphological analyser (Koskenniemi and Haapalainen, 1994).6 All our SMT systems are built with the Moses toolkit (Koehn et al., 2007; Hoang et al., 2009), and word alignments are generated by the Berkeley Aligner (Liang et al., 2006). The target language model is estimated by the IRSTLM toolkit (Federico et al., 2008) with modified Kneser-Ney smoothing (Chen and Goodman, 1999). The phrase-based baseline decoder includes a phrase translation model (two phrasal and two lexical probability features), a lexicalized reordering model (six features), a 6-gram target language model, distortion cost, word and phrase penalties. As lexicalized reordering model, we use a hierarchical phrase orientation model (Galley and Manning, 2008) trained on all the parallel data using three orientation classes – monotone, swap or discontinuous –"
W13-2257,E12-1074,0,0.0515982,"re generally good at handling local reordering phenomena that are not captured inside phrases. However, they typically fail to predict long reorderings. On the other hand, hierarchical SMT (HSMT) systems (Chiang, 2005) can learn reordering patterns by means of discontinuous translation rules, and are therefore considered a better choice for language pairs characterized by massive and hierarchical reordering. 2 Background Word order differences between German and English are mainly found at the clause (global) level, as opposed to the phrase (local) level. We refer to Collins et al. (2005) and Gojun and Fraser (2012) for a detailed description of the German clause structure. To briefly summarize, we can say that 440 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 440–451, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics the verb-second order of German main clauses contrasts with the rigid SVO structure of English, as does the clause-final verb position of German subordinate clauses. A further difficulty is given by the German discontinuous verb phrases, where the main verb is separated from the inflected auxiliary or modal. The distance betwee"
W13-2257,2007.mtsummit-papers.43,0,0.802932,"approach includes a basic reordering model, called distortion cost, that exponentially penalizes longer jumps among consecutively translated phrases simply based on their distance. Thus, a completely monotonic translation has a total distortion cost of zero. A weakness of this model is that it penalizes long jumps only when they are performed, rather than accumulating their cost gradually. As an effect, hypotheses with gaps (i. e. uncovered input positions) can proliferate and cause the pruning of more monotonic hypotheses that could lead to overall better translations. To solve this problem, Moore and Quirk (2007) proposed an improved version of the distortion cost function that anticipates the gradual accumulation of the total distortion cost, making hypotheses with the same number of covered words more comparable with one another. Early distortion cost (as called in Moses, or “distortion penalty estimation” in the original paper) is computed by a simple algorithm that keeps track of the uncovered input positions. Note that this option affects the distortion feature function, but not the distortion limit, which always corresponds to the maximum distance allowed between consecutively translated phrases"
W13-2257,W09-0435,0,0.047549,"k, we only consider efficient solutions that are fully integrated into the decoding process, and that do not require syntactic parsers or manual reordering rules. Still, it has to be mentioned that several alternative solutions were proposed in the literature. A well-known strategy consists of preordering the German sentence in an English-like order by applying a set of manually written rules to its syntactic parse tree (Collins et al., 2005).1 Other approaches learn the pre-ordering rules automatically, from syntactic parses (Xia and McCord, 2004; Genzel, 2010) or from part-of-speech labels (Niehues and Kolss, 2009). In the former case, pre-ordering decisions are typically taken deterministically (i. e. one permuation per sentence), whereas in the latter, multiple alternatives are represented as word lattices, and the optimal path is A large number of previous works on word reordering measured their success with generalpurpose metrics such as BLEU (Papineni et al., 2001) or METEOR (Banerjee and Lavie, 2005). These metrics, however, are only indirectly sensitive to word order and do not sufficiently penalize long-range reordering errors, as demonstrated for instance by Birch et al. (2010). While BLEU rema"
W13-2257,N10-1129,0,0.289051,"nce model by Feng et al. (2010), that is a generative n-gram model trained on a corpus of pre-ordered source sentences. Although reminiscent of a source-side bigram model, our model has two important differences: (i) the discriminative modeling framework enables us to design a much richer feature set including, for instance, the context of the next word to pick; (ii) all our features are independent from the decoding history, which allows for an efficient decoder-integration with no effect on hypothesis recombination. Finally, we have to mention the models by AlOnaizan and Papineni (2006) and Green et al. (2010), who predict the direction and (binned) length of a jump to perform after a given input word. Those models too were only used as additional feature functions, and were not shown to maintain translation quality and efficiency at very high distortion limits. Early distortion cost In its original formulation, the PSMT approach includes a basic reordering model, called distortion cost, that exponentially penalizes longer jumps among consecutively translated phrases simply based on their distance. Thus, a completely monotonic translation has a total distortion cost of zero. A weakness of this mode"
W13-2257,P02-1038,0,0.17283,"ering phenomena. Through an extensive evaluation including diverse translation quality metrics, we show that these solutions can significantly narrow the gap between phrase-based and hierarchical SMT. 1 Introduction Modeling the German-English language pair is known to be a challenging task for state-of-theart statistical machine translation (SMT) methods. A major factor of difficulty is given by word order differences that yield important long-range reordering phenomena. Thanks to specific reordering modeling components, phrase-based SMT (PSMT) systems (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2002) are generally good at handling local reordering phenomena that are not captured inside phrases. However, they typically fail to predict long reorderings. On the other hand, hierarchical SMT (HSMT) systems (Chiang, 2005) can learn reordering patterns by means of discontinuous translation rules, and are therefore considered a better choice for language pairs characterized by massive and hierarchical reordering. 2 Background Word order differences between German and English are mainly found at the clause (global) level, as opposed to the phrase (local) level. We refer to Collins et al. (2005) an"
W13-2257,W10-1761,0,0.0130042,"continuous verb phrases, where the main verb is separated from the inflected auxiliary or modal. The distance between the two parts of a verb phrase can be arbitrarily long as shown in the following example: selected by the decoder at translation time. In (Tromble and Eisner, 2009), pre-ordering is cast as a permutation problem and solved by a model that estimates the probability of reversing the relative order of any two input words. In the field of tree-based SMT, positive results in German-English were achieved by combining syntactic translation rules with unlabeled hierarchical SMT rules (Hoang and Koehn, 2010). More recently, Braune et al. (2012) proposed to improve the long-range reordering capability of an HSMT system by integrating constraints based on clausal boundaries and by manually selecting the rule patterns applicable to long word spans. The paper did not analyse the impact of the technique on efficiency. [DE] Jedoch konnten sie Kinder in Teilen von Helmand und Kandahar im Suden aus Sicherheitsgrund nicht er¨ reichen. [EN] But they could not reach children in parts of Helmand and Kandahar in the south for security reasons. Translating this sentence with a PSMT engine implies performing tw"
W13-2257,P03-1021,0,0.0985419,"rom the translation model as proposed by Johnson et al. (2007). The hierarchical system is trained and tested using the standard Moses configuration which includes: a rule table (two phrasal and two lexical probability features), a 6-gram target language model, word and rule penalties. We set the span constraint (cf. Section 5) to the default value of 10 words for rule extraction, while for decoding we consider two different settings: the default 10 words and a large value of 20 to enable very longrange reorderings. Feature weights for all systems are optimized by minimum BLEU-error training (Och, 2003) on test08. To reduce the effects of the optimizer instability, we tune each configuration four times and use the average of the resulting weight vectors for testing, as suggested by Cettolo et al. (2011). The source-to-reference word alignments that are needed to compute the reordering scores are generated by the Berkeley Aligner previously trained on the training data. Source-to-output alignments are obtained from the decoder’s trace. 6.2 Figure 1: Standard vs early distortion cost performance measured in terms of BLEU and KRS on tests(09-11) under different distortion limits. Distortion fun"
W13-2257,2009.iwslt-papers.4,0,0.0202593,"nstead 1.2 billion entries – one order of magnitude larger. Each data set includes one reference translation. Note that our goal is not to reach the performance of the best systems participating at the last WMT edition, but rather to assess the usefulness of our techniques on a larger and therefore more reliable test set, while starting from a reasonable baseline.5 For German tokenization and compound splitting we use Tree Tagger (Schmid, 1994) and the Gertwol morphological analyser (Koskenniemi and Haapalainen, 1994).6 All our SMT systems are built with the Moses toolkit (Koehn et al., 2007; Hoang et al., 2009), and word alignments are generated by the Berkeley Aligner (Liang et al., 2006). The target language model is estimated by the IRSTLM toolkit (Federico et al., 2008) with modified Kneser-Ney smoothing (Chen and Goodman, 1999). The phrase-based baseline decoder includes a phrase translation model (two phrasal and two lexical probability features), a lexicalized reordering model (six features), a 6-gram target language model, distortion cost, word and phrase penalties. As lexicalized reordering model, we use a hierarchical phrase orientation model (Galley and Manning, 2008) trained on all the p"
W13-2257,2001.mtsummit-papers.68,0,0.0275534,"a set of manually written rules to its syntactic parse tree (Collins et al., 2005).1 Other approaches learn the pre-ordering rules automatically, from syntactic parses (Xia and McCord, 2004; Genzel, 2010) or from part-of-speech labels (Niehues and Kolss, 2009). In the former case, pre-ordering decisions are typically taken deterministically (i. e. one permuation per sentence), whereas in the latter, multiple alternatives are represented as word lattices, and the optimal path is A large number of previous works on word reordering measured their success with generalpurpose metrics such as BLEU (Papineni et al., 2001) or METEOR (Banerjee and Lavie, 2005). These metrics, however, are only indirectly sensitive to word order and do not sufficiently penalize long-range reordering errors, as demonstrated for instance by Birch et al. (2010). While BLEU remains a standard choice for many evaluation campaigns, we believe it is extremely important to complement it with metrics that are specifically designed to capture word order differences. In this work, we adopt two reordering-specific metrics in addition to BLEU and METEOR: Kendall Reordering Score (KRS). As proposed by Birch et al. (2010), the KRS measures the"
W13-2257,D07-1103,0,0.0212169,"pothesis that better reordering modeling and better reordering space definition can significantly improve the accuracy of PSMT in German-English without sacrificing its efficiency. 5 Our results on test12 are not directly comparable to the WMT12 submissions due to the different training data: that is, the WMT12 parallel data includes 50M German tokens of Europarl data and 4M of news-commentary, as opposed to the 41M and 2.5M released for WMT10 and used in our experiments. 6 http://www2.lingsoft.fi/cgi-bin/gertwol 444 improbable phrase pairs are pruned from the translation model as proposed by Johnson et al. (2007). The hierarchical system is trained and tested using the standard Moses configuration which includes: a rule table (two phrasal and two lexical probability features), a 6-gram target language model, word and rule penalties. We set the span constraint (cf. Section 5) to the default value of 10 words for rule extraction, while for decoding we consider two different settings: the default 10 words and a large value of 20 to enable very longrange reorderings. Feature weights for all systems are optimized by minimum BLEU-error training (Och, 2003) on test08. To reduce the effects of the optimizer i"
W13-2257,W05-0908,0,0.0712858,"Missing"
W13-2257,N03-1017,0,0.0311496,"ure long-range reordering phenomena. Through an extensive evaluation including diverse translation quality metrics, we show that these solutions can significantly narrow the gap between phrase-based and hierarchical SMT. 1 Introduction Modeling the German-English language pair is known to be a challenging task for state-of-theart statistical machine translation (SMT) methods. A major factor of difficulty is given by word order differences that yield important long-range reordering phenomena. Thanks to specific reordering modeling components, phrase-based SMT (PSMT) systems (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2002) are generally good at handling local reordering phenomena that are not captured inside phrases. However, they typically fail to predict long reorderings. On the other hand, hierarchical SMT (HSMT) systems (Chiang, 2005) can learn reordering patterns by means of discontinuous translation rules, and are therefore considered a better choice for language pairs characterized by massive and hierarchical reordering. 2 Background Word order differences between German and English are mainly found at the clause (global) level, as opposed to the phrase (local) level. We refer to Coll"
W13-2257,2005.iwslt-1.8,0,0.0954106,"Missing"
W13-2257,N04-4026,0,0.133324,"Missing"
W13-2257,P07-2045,1,0.0129816,"aint (10) contains instead 1.2 billion entries – one order of magnitude larger. Each data set includes one reference translation. Note that our goal is not to reach the performance of the best systems participating at the last WMT edition, but rather to assess the usefulness of our techniques on a larger and therefore more reliable test set, while starting from a reasonable baseline.5 For German tokenization and compound splitting we use Tree Tagger (Schmid, 1994) and the Gertwol morphological analyser (Koskenniemi and Haapalainen, 1994).6 All our SMT systems are built with the Moses toolkit (Koehn et al., 2007; Hoang et al., 2009), and word alignments are generated by the Berkeley Aligner (Liang et al., 2006). The target language model is estimated by the IRSTLM toolkit (Federico et al., 2008) with modified Kneser-Ney smoothing (Chen and Goodman, 1999). The phrase-based baseline decoder includes a phrase translation model (two phrasal and two lexical probability features), a lexicalized reordering model (six features), a 6-gram target language model, distortion cost, word and phrase penalties. As lexicalized reordering model, we use a hierarchical phrase orientation model (Galley and Manning, 2008)"
W13-2257,D09-1105,0,0.225795,"tatistical Machine Translation, pages 440–451, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics the verb-second order of German main clauses contrasts with the rigid SVO structure of English, as does the clause-final verb position of German subordinate clauses. A further difficulty is given by the German discontinuous verb phrases, where the main verb is separated from the inflected auxiliary or modal. The distance between the two parts of a verb phrase can be arbitrarily long as shown in the following example: selected by the decoder at translation time. In (Tromble and Eisner, 2009), pre-ordering is cast as a permutation problem and solved by a model that estimates the probability of reversing the relative order of any two input words. In the field of tree-based SMT, positive results in German-English were achieved by combining syntactic translation rules with unlabeled hierarchical SMT rules (Hoang and Koehn, 2010). More recently, Braune et al. (2012) proposed to improve the long-range reordering capability of an HSMT system by integrating constraints based on clausal boundaries and by manually selecting the rule patterns applicable to long word spans. The paper did not"
W13-2257,D11-1045,0,0.603126,"ly used in PSMT nowadays. However, their coarse classification of reordering steps makes them unsuitable to capture long-range reordering phenomena, such as those attested in German-English. Indeed, Galley and Manning (2008) reported a decrease of translation quality when the distortion limit was set beyond 6 in Chinese-English and beyond 4 in Arabic-English. To address this problem, we have developed a different reordering model that predicts what input word should be translated at a given decoding state (Bisazza, 2013; Bisazza and Federico, 2013). The model is similar to the one proposed by Visweswariah et al. (2011), however we use it differently: that is, not simply for data preprocessing but as an additional feature function fully integrated in the phrase-based decoder. More importantly, we propose to use the same model to dynamically shape the space of reorderings explored during decoding (cf. Section 4.2), which was never done before. Another related work is the source-side decoding sequence model by Feng et al. (2010), that is a generative n-gram model trained on a corpus of pre-ordered source sentences. Although reminiscent of a source-side bigram model, our model has two important differences: (i)"
W13-2257,C04-1073,0,0.0811749,"SMT has been widely studied and is still an open topic. In this work, we only consider efficient solutions that are fully integrated into the decoding process, and that do not require syntactic parsers or manual reordering rules. Still, it has to be mentioned that several alternative solutions were proposed in the literature. A well-known strategy consists of preordering the German sentence in an English-like order by applying a set of manually written rules to its syntactic parse tree (Collins et al., 2005).1 Other approaches learn the pre-ordering rules automatically, from syntactic parses (Xia and McCord, 2004; Genzel, 2010) or from part-of-speech labels (Niehues and Kolss, 2009). In the former case, pre-ordering decisions are typically taken deterministically (i. e. one permuation per sentence), whereas in the latter, multiple alternatives are represented as word lattices, and the optimal path is A large number of previous works on word reordering measured their success with generalpurpose metrics such as BLEU (Papineni et al., 2001) or METEOR (Banerjee and Lavie, 2005). These metrics, however, are only indirectly sensitive to word order and do not sufficiently penalize long-range reordering error"
W13-2257,2002.tmi-tutorials.2,0,0.0879086,"nd effectively capture long-range reordering phenomena. Through an extensive evaluation including diverse translation quality metrics, we show that these solutions can significantly narrow the gap between phrase-based and hierarchical SMT. 1 Introduction Modeling the German-English language pair is known to be a challenging task for state-of-theart statistical machine translation (SMT) methods. A major factor of difficulty is given by word order differences that yield important long-range reordering phenomena. Thanks to specific reordering modeling components, phrase-based SMT (PSMT) systems (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2002) are generally good at handling local reordering phenomena that are not captured inside phrases. However, they typically fail to predict long reorderings. On the other hand, hierarchical SMT (HSMT) systems (Chiang, 2005) can learn reordering patterns by means of discontinuous translation rules, and are therefore considered a better choice for language pairs characterized by massive and hierarchical reordering. 2 Background Word order differences between German and English are mainly found at the clause (global) level, as opposed to the phrase (local) lev"
W13-2257,P02-1040,0,\N,Missing
W13-2257,C10-1043,0,\N,Missing
W14-0313,2012.iwslt-papers.12,0,0.136526,"viding the learning task into word alignment and phrase extraction tasks, and replacing the standard wordalignment module, which is a variation of EM algorithm (Och and Ney, 2003), with a greedy search algorithm, they attempt to find a quick approximation of the word alignments of the newly translated sentence. They also use some heuristics to improve the obtained alignments, without supporting it with some proofs or even providing some experimental results. Furthermore, the running time of this approach is not discussed, and it is not clear how effective this approach is in online scenarios. Blain et al. (2012) have recently studied the problem of incremental learning from post-editing data, with minimum computational complexity and acceptable quality. They use the MT outTomeh et al. (2010) introduced a supervised discriminative word alignment model for producing higher quality word alignments, which is trained on a manually aligned training corpus. To reduce the search space of the word aligner, they propose to provide the system with a set of automatic word alignments and consider the union of these alignments as the possible search space. This transforms the word alignment process into 85 3.1 the"
W14-0313,N13-1073,0,0.0215216,"eir capability in handling the unknown words. For a fair comparison, all aligners are trained on the same training corpora described in Section 3.2. berkeley aligner (Liang et al., 2006) applies the co-training approach for training the IBM model 1 and HMM. We trained berkeley aligner using 5 iterations of model 1 followed by 5 iterations of HMM. When applied to new sentence pairs, the system produces bi-directional symmetrized alignment. fast-align is a recently developed unsupervised word aligner that uses a log-linear reparametrization of IBM model 2 for training the word alignment models (Dyer et al., 2013). We exploited the default configuration with 5 iterations for training. As the system is directional, we trained two systems (source-to-target and targetto-source). When applied to new sentence pairs, we first produced the two directional alignments, and then combined them into a symmetrized alignment by using the grow-diag-final-and heuristic (Och and Ney, 2003). mgiza++ (Gao and Vogel, 2008) and its ancestors, i.e. giza, and giza++, implement all the IBM models and HMM based alignment models. mgiza++ is a multithreaded version of giza++, which enables an efficient use of multi-core platform"
W14-0313,2010.amta-papers.18,0,0.0282127,"th a greedy search algorithm, they attempt to find a quick approximation of the word alignments of the newly translated sentence. They also use some heuristics to improve the obtained alignments, without supporting it with some proofs or even providing some experimental results. Furthermore, the running time of this approach is not discussed, and it is not clear how effective this approach is in online scenarios. Blain et al. (2012) have recently studied the problem of incremental learning from post-editing data, with minimum computational complexity and acceptable quality. They use the MT outTomeh et al. (2010) introduced a supervised discriminative word alignment model for producing higher quality word alignments, which is trained on a manually aligned training corpus. To reduce the search space of the word aligner, they propose to provide the system with a set of automatic word alignments and consider the union of these alignments as the possible search space. This transforms the word alignment process into 85 3.1 the alignment refinement task in which given a set of automatic word alignments, the system tries to find the best word alignment points. Similar to (McCarley et al., 2011), this approac"
W14-0313,C96-2141,0,0.173012,": T T |A P | |A S| P recision = , Recall = |A| |S| 1 F − measure = α 1−α P recision + Recall Word Alignment Word alignment is the task of finding the correspondence among the words of a sentence pair (Figure 1). From a mathematical point of view, it is a relation among the words, because any word in a sentence can be mapped into zero, one or more words of the other, and vice-versa; in other words, any kind of link is allowed, namely one-toone, many-to-one, many-to-many, as well as leaving words unaligned. So called IBM models 1-5 (Brown et al., 1993) as well as the HMM-based alignment models (Vogel et al., 1996), and their variations are extensively studied and widely used for this task. They are directional alignment models, because permit only many-to-one links; but often the alignments in the two opposite directions are combined in a so-called symmetrized alignment, which is obtained by intersection, union or other smart combination. Nowadays, word-aligners are mostly employed in an intermediate step of the training procedure of a SMT system; In this step, the training corpus is word aligned as a side effect of the estimation of the alignment models by means of the Expectation-Maximization algorit"
W14-0313,J07-3002,0,0.0598475,"of automatic word alignments and consider the union of these alignments as the possible search space. This transforms the word alignment process into 85 3.1 the alignment refinement task in which given a set of automatic word alignments, the system tries to find the best word alignment points. Similar to (McCarley et al., 2011), this approach relies on the manually annotated training corpora which is not available for most of the language pairs. 3 Evaluation Measures A word aligner is usually evaluated in terms of Precision, Recall, and F-measure (or shortly F ), which are defined as follows (Fraser and Marcu, 2007): T T |A P | |A S| P recision = , Recall = |A| |S| 1 F − measure = α 1−α P recision + Recall Word Alignment Word alignment is the task of finding the correspondence among the words of a sentence pair (Figure 1). From a mathematical point of view, it is a relation among the words, because any word in a sentence can be mapped into zero, one or more words of the other, and vice-versa; in other words, any kind of link is allowed, namely one-toone, many-to-one, many-to-many, as well as leaving words unaligned. So called IBM models 1-5 (Brown et al., 1993) as well as the HMM-based alignment models ("
W14-0313,2009.eamt-1.31,0,0.0197206,"ng oov-rate, for all language pairs. The oov-based F-measure for berkeley is not reported because it is undefined. depends on the quality of the employed word aligner. Once the alignments are computed and symmetrized (if required), phrase extraction procedure is applied to extract all valid phrase-pairs. Note that un-aligned words are included in the extracted phrase pairs, if their surrounding words are aligned. It has been shown that inclusion of un-aligned words in the phrase-pairs, generally, has negative effects on the translation quality and can produce errors in the translation output (Zhang et al., 2009). Nevertheless, the overlap among phrase-pairs, which contain un-aligned unknown words, can be considered as a valuable source of knowledge for inducing the correct alignment of these words. To get their alignments from the extracted phrase-pairs we follow an approach similar to (Espl´a-Gomis et al., 2012) in which the word alignment probabilities are determined by the alignment strength measure. Given the source and target segments (S = {s1 , . . . , sl } and T = {t1 , . . . , sm }), and the set of extracted parallel phrase-pairs (Φ), the alignment strength Ai,j (S, T, Φ) of the si and tj can"
W14-0313,W08-0509,0,0.0395269,"produces bi-directional symmetrized alignment. fast-align is a recently developed unsupervised word aligner that uses a log-linear reparametrization of IBM model 2 for training the word alignment models (Dyer et al., 2013). We exploited the default configuration with 5 iterations for training. As the system is directional, we trained two systems (source-to-target and targetto-source). When applied to new sentence pairs, we first produced the two directional alignments, and then combined them into a symmetrized alignment by using the grow-diag-final-and heuristic (Och and Ney, 2003). mgiza++ (Gao and Vogel, 2008) and its ancestors, i.e. giza, and giza++, implement all the IBM models and HMM based alignment models. mgiza++ is a multithreaded version of giza++, which enables an efficient use of multi-core platforms. We trained the system using the following configuration for model iterations: 15 h5 33 43 . mgiza++ also produces directional alignment; hence, we followed the same protocol to create a symmetrize alignment of sentence pairs as we did for fast-align. Differently from berkeley and fast-align, mgiza++ somehow adapts its models when applied to new sentence pairs. According to the so-called “for"
W14-0313,2010.amta-papers.21,0,0.0244442,"ons and to hence avoid repeating the same mistakes in future sentences. A typical application scenario is the usage by a professional translator of a Computer Assisted Translation (CAT) tool enhanced with a SMT system. For each input sentence, first the translator receives one or more translation suggestions from 84 Workshop on Humans and Computer-assisted Translation, pages 84–92, c Gothenburg, Sweden, 26 April 2014. 2014 Association for Computational Linguistics put (hypothesis) as a pivot to find the word alignments between the source sentence and its corresponding reference. Similarly to (Hardt and Elming, 2010), once the word alignment between the source and post-edit sentence pair is generated, they use the standard phrase extraction method to extract the parallel phrase pairs. This work is based on an implicit assumption that MT output is reliable enough to make a bridge between source and reference. However, in the real world this is not always true. The post-editor sometimes makes a lot of changes in the MT output, or even translates the entire sentence from scratch, which makes the post-edit very different from the automatic translation. Moreover, in the presence of new words in the source sent"
W14-0313,2005.mtsummit-papers.11,0,0.0275549,"is example, Precision and Recall will be about 0.85 (=11/13) and 0.91 (=10/11), respectively, and the corresponding F is hence about 0.88. Focusing on the oov-alignment only, Precisionoov is 1.00 (=1/1), Recalloov is 0.50 (=1/2), and Foov is 0.67. 3.2 Evaluation Benchmark In this paper, we compare word-alignment performance of three word-aligners introduced in Section 3.3 on three distinct tasks, namely EnglishItalian, English-French, and English-Spanish; the training corpora, common to all word-aligners, are subset of the JRC-legal corpus1 (Steinberger et al., ), of the Europarl corpus V7.0 (Koehn, 2005), and of the Hansard parallel corpus2 , respectively. 1 langtech.jrc.it/JRC-Acquis.html www.isi.edu/natural-language/ download/hansard/index.html 2 86 financial i meccanismi financial i meccanismi assistance di assistenza assistance di assistenza mechanisms finanziaria are sono attivabili mechanisms finanziaria less are less sono attivabili rapidly meno rapidly meno deployable than rapidamente deployable rispetto than rapidamente rispetto conventional ai meccanismi conventional ai budgetary di bilancio budgetary meccanismi di mechanisms convenzionali mechanisms bilancio convenzionali Figure 1:"
W14-0313,D11-1082,0,0.0592225,"Section 3, we describe three widely used toolkits, highlight their pros and cons in the online MT adaptation scenario, and compare their performance in aligning unknown terms. In Section 4 we propose a standalone module which refines the word alignment of unknown words; moreover, we present an enhanced faster implementation of the best performing word aligner, to make it usable in the online scenario. In Section 5 we show experimental results of this module on three different languages. Finally, we draw some final comments in Section 6. 2 In order to improve the quality of the word alignments McCarley et al. (2011) proposed a trainable correction model which given a sentence pair and their corresponding automatically produced word alignment, it tries to fix the wrong alignment links. Similar to the hill-climbing approach used in IBM models 3-5 (Brown et al., 1993), this approach iteratively performs small modifications in each step, based on the changes of the previous step. However, the use of additional sources of knowledge, such as POS tags of the words and their neighbours, helps the system to take more accurate decisions. But, requiring manual word alignments for learning the alignment moves makes"
W14-0313,P00-1056,0,0.375768,"Missing"
W14-0313,J03-1002,0,0.0507465,"ps the system to take more accurate decisions. But, requiring manual word alignments for learning the alignment moves makes this approach only applicable for a limited number of language pairs for which manual aligned gold references are available. Related works Hardt et al. (2010) presented an incremental retraining method which simulates the procedure of learning from post-edited MT outputs (references), in a real time fashion. By dividing the learning task into word alignment and phrase extraction tasks, and replacing the standard wordalignment module, which is a variation of EM algorithm (Och and Ney, 2003), with a greedy search algorithm, they attempt to find a quick approximation of the word alignments of the newly translated sentence. They also use some heuristics to improve the obtained alignments, without supporting it with some proofs or even providing some experimental results. Furthermore, the running time of this approach is not discussed, and it is not clear how effective this approach is in online scenarios. Blain et al. (2012) have recently studied the problem of incremental learning from post-editing data, with minimum computational complexity and acceptable quality. They use the MT"
W14-0313,steinberger-etal-2006-jrc,0,\N,Missing
W14-0313,J93-2003,0,\N,Missing
W17-4713,W14-3346,0,0.101337,"ssing steps. In this paper, we use Lucene (McCandless et al., 2010), an open-source information retrieval library that is highly optimized for text search purposes. However, since the similarity measure used in Lucene is based on tfidf counts (Baeza-Yates and Ribeiro-Neto, 2011), it does not consider the order of the words and ngrams in the query and in the retrieved sentences, which is an important aspect for MT model training. In order to take advantage also of this information, we first query Lucene to retrieve a large set of candidates and then re-score them using the sentence-level BLEU (Chen and Cherry, 2014), so that the sentences with higher BLEU score are ranked first. Finally, the top-n similar sentences are used to update the model. This approach is reasonably fast, since it takes advantage of Lucene in searching in a large set of data and then computes the BLEU scores on just few candidates. 5 Experimental Setup 5.1 Data Our experiments are carried out on an English to French translation task, where the training data is a collection of publicly available corpora from different domains: European Central Bank (ECB), Gnome, JRC-Acquis (JRC), KDE4, OpenOffice (OOffice), PHP, Ubuntu, and translat"
W17-4713,D14-1179,0,0.0100254,"Missing"
W17-4713,W07-0733,0,0.0106122,"use scenarios (language combinations, genres, domains). On the other side, the infrastructures required to reach this objective should be scalable 127 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 127–137 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics field mostly rely on the assumption of knowing the target domain in advance and having in-domain training data of reasonable size. This dataset is then used to train specific models that are interpolated with generic ones using standard log-linear methods (Koehn and Schroeder, 2007) or mixture models (Foster and Kuhn, 2007). In line with the work presented in this paper, (Eck et al., 2004) and (Zhao et al., 2004) proposed to perform an instance selection step in which for each test document/sentence a small set of similar documents/sentences is retrieved from the pool of training data and used to build more specific language models. (Hildebrand et al., 2005) further extended this approach and proposed to build local translation models using the set of retrieved sentence pairs. As in SMT, recent works in domain adaptation for neural MT share the assumption of knowing the"
W17-4713,L18-1146,0,0.207046,"Missing"
W17-4713,N13-1073,0,0.041585,"aximum sentence length is set to 50. The models are trained using Adagrad (Duchi et al., 2011) by reshuffling the training set at each epoch, and are evaluated every 10,000 mini-batches with BLEU (Papineni et al., 2002). 5.3 Terms of Comparison We compare our adaptive NMT system with a generic NMT and a strong PBMT system trained on the pool of all the training data. For training the PBMT system we used the open source Moses 4 131 https://github.com/rsennrich/nematus best sentence pair for updating the model. toolkit (Koehn et al., 2007). The word alignment models were trained with FastAlign (Dyer et al., 2013). We trained the 5-gram language models with the KenLM toolkit (Heafield et al., 2013) on the target side of the pooled corpora. Feature weights were tuned with batch MIRA (Cherry and Foster, 2012) to maximize BLEU on the dev set. Details of the generic NMT system are described in Section 5.2. In Table 3, the results on the dev set are reported. Although trained on the same dataset, it is interesting to note that, the performance of the generic NMT system is by far lower than the PBMT system. A possible explanation is that the PBMT system can explicitly memorise and use translation options lea"
W17-4713,eck-etal-2004-language,0,0.149515,"Missing"
W17-4713,2015.iwslt-evaluation.11,0,0.36339,"proposed to perform an instance selection step in which for each test document/sentence a small set of similar documents/sentences is retrieved from the pool of training data and used to build more specific language models. (Hildebrand et al., 2005) further extended this approach and proposed to build local translation models using the set of retrieved sentence pairs. As in SMT, recent works in domain adaptation for neural MT share the assumption of knowing the target domain in advance and report significant improvements by adapting the generic system to the target domain as an offline step (Luong and Manning, 2015). More recently, (Li et al., 2016)1 proposed an instance-based adaptation technique for NMT in which for each translation segment a set of similar sentence pairs is retrieved. This small training set is then used to update the model before translating the given test sentence. In order to reduce the cost of computing the similarity of the test segment and all the sentences in the pool, they suggest a three-step process in which, as the first step, all the sentence pairs containing at least one of the test words are retrieved. This large set is then filtered by measuring the similarity between t"
W17-4713,W07-0717,0,0.0167602,"domains). On the other side, the infrastructures required to reach this objective should be scalable 127 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 127–137 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics field mostly rely on the assumption of knowing the target domain in advance and having in-domain training data of reasonable size. This dataset is then used to train specific models that are interpolated with generic ones using standard log-linear methods (Koehn and Schroeder, 2007) or mixture models (Foster and Kuhn, 2007). In line with the work presented in this paper, (Eck et al., 2004) and (Zhao et al., 2004) proposed to perform an instance selection step in which for each test document/sentence a small set of similar documents/sentences is retrieved from the pool of training data and used to build more specific language models. (Hildebrand et al., 2005) further extended this approach and proposed to build local translation models using the set of retrieved sentence pairs. As in SMT, recent works in domain adaptation for neural MT share the assumption of knowing the target domain in advance and report signif"
W17-4713,P02-1040,0,0.120401,"ds. As recommended in (Sennrich et al., 2016), in order to increase the consistency in segmenting the source and target text, we combined both sides of the training data, and set the number of merge rules to 89,500, resulting in vocabularies of size 78K and 86K tokens respectively for English and French. We use mini-batches of size 100, word embeddings of size 500, and GRU layers of size 1,024. The maximum sentence length is set to 50. The models are trained using Adagrad (Duchi et al., 2011) by reshuffling the training set at each epoch, and are evaluated every 10,000 mini-batches with BLEU (Papineni et al., 2002). 5.3 Terms of Comparison We compare our adaptive NMT system with a generic NMT and a strong PBMT system trained on the pool of all the training data. For training the PBMT system we used the open source Moses 4 131 https://github.com/rsennrich/nematus best sentence pair for updating the model. toolkit (Koehn et al., 2007). The word alignment models were trained with FastAlign (Dyer et al., 2013). We trained the 5-gram language models with the KenLM toolkit (Heafield et al., 2013) on the target side of the pooled corpora. Feature weights were tuned with batch MIRA (Cherry and Foster, 2012) to"
W17-4713,P16-1162,0,0.0622272,"show, these corpora are extremely diverse in terms of average sentence length and word frequency, which are likely to correspond to different 5.2 Neural MT System All our experiments with NMT are conducted with an in-house developed and maintained branch of the Nematus toolkit4 which is an implementation of the attentional encoder-decoder architecture (Bahdanau et al., 2014). Since handling large vocabularies is one of the main bottlenecks for the existing NMT systems, state-of-the-art approaches are trained on corpora in which the less frequent words are segmented into their sub-word units (Sennrich et al., 2016) by applying a modified version of the byte pair encoding (BPE) compression algorithm (Gage, 1994). This makes the NMT systems capable of dealing with new and rare words. As recommended in (Sennrich et al., 2016), in order to increase the consistency in segmenting the source and target text, we combined both sides of the training data, and set the number of merge rules to 89,500, resulting in vocabularies of size 78K and 86K tokens respectively for English and French. We use mini-batches of size 100, word embeddings of size 500, and GRU layers of size 1,024. The maximum sentence length is set"
W17-4713,P13-2121,0,0.0167917,"Missing"
W17-4713,C04-1059,0,0.0395585,"lable 127 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 127–137 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics field mostly rely on the assumption of knowing the target domain in advance and having in-domain training data of reasonable size. This dataset is then used to train specific models that are interpolated with generic ones using standard log-linear methods (Koehn and Schroeder, 2007) or mixture models (Foster and Kuhn, 2007). In line with the work presented in this paper, (Eck et al., 2004) and (Zhao et al., 2004) proposed to perform an instance selection step in which for each test document/sentence a small set of similar documents/sentences is retrieved from the pool of training data and used to build more specific language models. (Hildebrand et al., 2005) further extended this approach and proposed to build local translation models using the set of retrieved sentence pairs. As in SMT, recent works in domain adaptation for neural MT share the assumption of knowing the target domain in advance and report significant improvements by adapting the generic system to the target domain as an offline step ("
W17-4713,2005.eamt-1.19,0,0.0514629,"wing the target domain in advance and having in-domain training data of reasonable size. This dataset is then used to train specific models that are interpolated with generic ones using standard log-linear methods (Koehn and Schroeder, 2007) or mixture models (Foster and Kuhn, 2007). In line with the work presented in this paper, (Eck et al., 2004) and (Zhao et al., 2004) proposed to perform an instance selection step in which for each test document/sentence a small set of similar documents/sentences is retrieved from the pool of training data and used to build more specific language models. (Hildebrand et al., 2005) further extended this approach and proposed to build local translation models using the set of retrieved sentence pairs. As in SMT, recent works in domain adaptation for neural MT share the assumption of knowing the target domain in advance and report significant improvements by adapting the generic system to the target domain as an offline step (Luong and Manning, 2015). More recently, (Li et al., 2016)1 proposed an instance-based adaptation technique for NMT in which for each translation segment a set of similar sentence pairs is retrieved. This small training set is then used to update the"
W17-4713,P07-2045,1,0.0170856,"f size 100, word embeddings of size 500, and GRU layers of size 1,024. The maximum sentence length is set to 50. The models are trained using Adagrad (Duchi et al., 2011) by reshuffling the training set at each epoch, and are evaluated every 10,000 mini-batches with BLEU (Papineni et al., 2002). 5.3 Terms of Comparison We compare our adaptive NMT system with a generic NMT and a strong PBMT system trained on the pool of all the training data. For training the PBMT system we used the open source Moses 4 131 https://github.com/rsennrich/nematus best sentence pair for updating the model. toolkit (Koehn et al., 2007). The word alignment models were trained with FastAlign (Dyer et al., 2013). We trained the 5-gram language models with the KenLM toolkit (Heafield et al., 2013) on the target side of the pooled corpora. Feature weights were tuned with batch MIRA (Cherry and Foster, 2012) to maximize BLEU on the dev set. Details of the generic NMT system are described in Section 5.2. In Table 3, the results on the dev set are reported. Although trained on the same dataset, it is interesting to note that, the performance of the generic NMT system is by far lower than the PBMT system. A possible explanation is t"
W17-4713,N12-1047,0,\N,Missing
W17-4716,C04-1046,0,0.0634356,"words unknown to the model. To test our approach, we experiment in two scenarios that pose different challenges to NMT. The first one is a translation task in which source sentences contain XML-annotated domain-specific terms. The presence of few annotated terms poses fewer constraints to the decoder in generating the output sentence. The second scenario is an automatic post-editing (APE) task, in which the NMT model is trained to translate “monolingually” from draft machine-translated sentences into humanquality post-edits. The external guidance is provided by word-level quality judgements (Blatz et al., 2004) indicating the “good” words in the machine-translated sentence that should be kept in the final APE output. In this case, the large number of “good” words already present in the original MT output poses more constraints to the decoding process. In both scenarios, our guidance mechanism achieves significant performance gains over the original NMT decoder. 2 Related Work In PBSMT, the injection of external knowledge in the decoder is usually handled with the socalled XML markup, a technique used to guide the decoder by supplying the desired translation for some of the source phrases. The suppli"
W17-4716,D16-1162,0,0.0331893,"replacing entries that cover the specific source phrase, or adding the alternative phrase translations to it, so that they are in competition. This problem has only recently started to be explored in NMT and, in most of the cases, the proposed solutions integrate external knowledge at training stage. Time-consuming training routines, however, limit the suitability of this strategy for applications requiring real-time translations. In Gulcehre et al. (2015), monolingual data is used to train a neural language model that is integrated in the NMT decoder by concatenating their hidden states. In Arthur et al. (2016), the probability of the next target word in the NMT decoder is biased by using lexicon probabilities computed from a bilingual lexicon. When the external knowledge is The closest approach to ours is the one by (Hokamp and Liu, 2017). They explore all the possible constraints (or translation options) at each time step making sure not to generate a constraint that have already been generated in the previous timestep. Their approach generates all the constraints in the final output, thus implicitly it assumes that only one translation options is provided as constraint for a given source word/phr"
W17-4716,E17-1050,1,0.888712,"Missing"
W17-4716,P17-1141,0,0.0370356,"the proposed solutions integrate external knowledge at training stage. Time-consuming training routines, however, limit the suitability of this strategy for applications requiring real-time translations. In Gulcehre et al. (2015), monolingual data is used to train a neural language model that is integrated in the NMT decoder by concatenating their hidden states. In Arthur et al. (2016), the probability of the next target word in the NMT decoder is biased by using lexicon probabilities computed from a bilingual lexicon. When the external knowledge is The closest approach to ours is the one by (Hokamp and Liu, 2017). They explore all the possible constraints (or translation options) at each time step making sure not to generate a constraint that have already been generated in the previous timestep. Their approach generates all the constraints in the final output, thus implicitly it assumes that only one translation options is provided as constraint for a given source word/phrase. However, in a more realistic scenario (e.g. in presence of a termbase or when the target language is more inflected than the source language), a source word can have multiple translation options from which the decoder should dec"
W17-4716,P15-1001,0,0.166045,"lberg et al., 2016) proposed an approach which can be used at decoding time. A hierarchical PBSMT system is used to generate the translation lattices, which are then re-scored by the NMT decoder. During decoding, the NMT posterior probabilities are adjusted using the posterior scores computed by the hierarchical model. However, by representing the additional information as a translation lattice, this approach does not allow the use of external knowledge in the form of bilingual terms or quality judgements as we do in §5 and §6. A different technique is postprocessing the translated sentences. Jean et al. (2015) and Luong and Manning (2015) replace the unknown words either with the most likely aligned source word or with the translation determined by another word alignment model. source words with the corresponding translation options. The guidance mechanism supervises the process, generating the final output with the expected translations, in the right place, including cases of external words unknown to the model. To test our approach, we experiment in two scenarios that pose different challenges to NMT. The first one is a translation task in which source sentences contain XML-annotated domain-speci"
W17-4716,W15-3025,1,0.843161,"signed&quot; helps GDec to correct other parts of the final translation, like generating “es gibt keine” (En: “There is no”) which is otherwise missing in the baseline translation. 6 Task 2: Automatic Post-Editing In our second experiment, we apply guided decoding in an automatic post-editing task. The goal of automatic post-editing (APE) is to correct errors in an MT-ed text. The problem is typically approached as a “monolingual translation” task, in which models are trained on parallel corpora containing (MT_output, MT_post-edit) pairs, with MT post-edits coming from humans (Simard et al., 2007; Chatterjee et al., 2015b, 2017). In their attempt to translate the entire input sentence, APE systems usually tend to over-correct the source words, i.e. to use all applicable correction options. This can happen even when the input is correct, often resulting in text deterioration (Bojar et al., 2015). To cope with this problem, neuralbased APE decoders would benefit from external knowledge indicating words in the input which are correct and thus should not be modified during decoding. For that we propose to use wordlevel binary quality estimation labels (Blatz et al., 2004; de Souza et al., 2014) to annotate the “g"
W17-4716,W16-2378,0,0.0509962,"on from the QE annotations. Base-APE improves the Base-MT up to 3.14 BLEU points. Similar to §5.2, the evaluation of our guided decoder is performed incrementally. GDec_base forces the “good” words in the automatic translation to appear in the output according to the mechanism described in §4.1. This basic guidance mechanism yields only marginal improvements over the Base-MT and is far behind the Base-APE. This can be explained by the large number of constraints (i.e. “good” words to be Experimental setting NMT models. We use the pre-trained model built for the best English-German submission (Junczys-Dowmunt and Grundkiewicz, 2016) at the WMT’16 APE task. This available model was trained with Nematus over a data set of ∼4M back-translated pairs, and then adapted to the taskspecific data segmented using the BPE technique. 164 Src: <n translation=&quot;durchsuchen||Durchsuchen&quot;&gt; browse </n&gt; all products Base: stöbern alle Produkte GDec: durchsuchen Sie alle Produkte Ref: durchsuchen Sie alle Produkte Src: <n translation=&quot;produkt||Produkt&quot;&gt; Product </n&gt; 4 - <n translation=&quot;plastics||Plastics&quot;&gt; Plastics </n&gt; <n translation=&quot;labs||Labs&quot;&gt; Labs </n&gt; Base: Produkt 4 - Kunststofflabore GDec: Produkt 4 - Plastics Labs Ref: Produkt 4 -"
W17-4716,P15-2026,1,0.665898,"signed&quot; helps GDec to correct other parts of the final translation, like generating “es gibt keine” (En: “There is no”) which is otherwise missing in the baseline translation. 6 Task 2: Automatic Post-Editing In our second experiment, we apply guided decoding in an automatic post-editing task. The goal of automatic post-editing (APE) is to correct errors in an MT-ed text. The problem is typically approached as a “monolingual translation” task, in which models are trained on parallel corpora containing (MT_output, MT_post-edit) pairs, with MT post-edits coming from humans (Simard et al., 2007; Chatterjee et al., 2015b, 2017). In their attempt to translate the entire input sentence, APE systems usually tend to over-correct the source words, i.e. to use all applicable correction options. This can happen even when the input is correct, often resulting in text deterioration (Bojar et al., 2015). To cope with this problem, neuralbased APE decoders would benefit from external knowledge indicating words in the input which are correct and thus should not be modified during decoding. For that we propose to use wordlevel binary quality estimation labels (Blatz et al., 2004; de Souza et al., 2014) to annotate the “g"
W17-4716,W04-3250,0,0.035147,"ed model built for the best EnglishGerman submission (Sennrich et al., 2016a) at the News Translation task at WMT’16 (Bojar et al., 2016). At test stage, it is supplied with terminology lists containing term recommendations in BPE format. In all the experiments we use a default beam size of 12. 5.2 Results and discussion Our results on the MT task are reported in Table 1, which shows system performance on the concatenation of the test sets from the two target domains. Performance is measured with BLEU (Papineni et al., 2002), and statistical significance is computed with bootstrap resampling (Koehn, 2004). The result of the word-level baseline system is computed after post-processing its output following the approach of Jean et al. (2015), which was customized to our scenario. This method (see §2) is driven by the attention model to replace the UNK tokens in the output with their corresponding recommendation supplied as external knowledge. This post-processing strategy is not used for the BPE-level baseline because it implicitly addresses the problem of OOVs. We evaluate our guided decoder incrementally, by adding one at a time the mechanisms described in §4. In the discussion, we do not compa"
W17-4716,W14-4012,0,0.0668892,"Missing"
W17-4716,2005.mtsummit-papers.11,0,0.126246,"r knowledge supplied as translation recommendations for domainspecific terms. The suggested terms (i.e. the constraints posed to the decoder) are usually few, thus leaving a large degree of freedom to the NMT decoder while generating the output. 5.1 Experimental setting NMT models. We evaluate guided decoding in its ability to improve the performance of two different English to German NMT models, both obtained with the Nematus toolkit (Sennrich et al., 2016a). The first system operates at word level and it is trained by using part of the JRC-Acquis corpus (Steinberger et al., 2006), Europarl (Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2009), which results in at total of about 1.8M parallel sentence pairs. The size of the vocabulary, word embedding, and hidden units is respectively set to 40K, 600, and 600, and parameters are optimised with Adagrad (Duchi et al., 2011) using a learning rate of 0.01. The batch size is set to 100, and the model is trained for 300K updates (∼17 epochs). At test stage, the word-level system is supplied with terminology lists containing term recommendations at the level of granularity of full words. The second system is trained on sub-word units by using the Byt"
W17-4716,W14-3340,1,0.91221,"Missing"
W17-4716,2015.iwslt-evaluation.11,0,0.0709613,"posed an approach which can be used at decoding time. A hierarchical PBSMT system is used to generate the translation lattices, which are then re-scored by the NMT decoder. During decoding, the NMT posterior probabilities are adjusted using the posterior scores computed by the hierarchical model. However, by representing the additional information as a translation lattice, this approach does not allow the use of external knowledge in the form of bilingual terms or quality judgements as we do in §5 and §6. A different technique is postprocessing the translated sentences. Jean et al. (2015) and Luong and Manning (2015) replace the unknown words either with the most likely aligned source word or with the translation determined by another word alignment model. source words with the corresponding translation options. The guidance mechanism supervises the process, generating the final output with the expected translations, in the right place, including cases of external words unknown to the model. To test our approach, we experiment in two scenarios that pose different challenges to NMT. The first one is a translation task in which source sentences contain XML-annotated domain-specific terms. The presence of fe"
W17-4716,P02-1040,0,0.122482,"a successful way to reduce the OOV rate. The system used in our evaluation is the pre-trained model built for the best EnglishGerman submission (Sennrich et al., 2016a) at the News Translation task at WMT’16 (Bojar et al., 2016). At test stage, it is supplied with terminology lists containing term recommendations in BPE format. In all the experiments we use a default beam size of 12. 5.2 Results and discussion Our results on the MT task are reported in Table 1, which shows system performance on the concatenation of the test sets from the two target domains. Performance is measured with BLEU (Papineni et al., 2002), and statistical significance is computed with bootstrap resampling (Koehn, 2004). The result of the word-level baseline system is computed after post-processing its output following the approach of Jean et al. (2015), which was customized to our scenario. This method (see §2) is driven by the attention model to replace the UNK tokens in the output with their corresponding recommendation supplied as external knowledge. This post-processing strategy is not used for the BPE-level baseline because it implicitly addresses the problem of OOVs. We evaluate our guided decoder incrementally, by addin"
W17-4716,C16-1172,0,0.0268576,"Missing"
W17-4716,W08-0509,0,0.019382,"Missing"
W17-4716,W16-2209,0,0.0179184,"ations of specific words or phrases, are a typical example of external knowledge used to guide the process to meet such constraints. Meeting predefined constraints, however, does not represent the only case in which an external guidance can support decoding. In ensemble MT architectures, for example, the output of a translation system 157 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 157–168 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics in the form of linguistic information, such as POS tags or lemmas, Sennrich and Haddow (2016) propose to compute separate embedding vectors for each linguistic information and then concatenate them, without altering the decoder. Other solutions exploit the strengths of PBSMT systems to improve NMT by pre-translating the source sentence. In Niehues et al. (2016), the NMT model is fed with a concatenation of the source and its PBSMT translation. Some of these solutions lead to improvements in performance, but they all require time-intensive training of the NMT models to use an enriched input representation or to optimize the parameters of the model. (Stahlberg et al., 2016) proposed an"
W17-4716,P16-1162,0,0.317137,"l knowledge without retraining of paramount importance. To address this gap, we investigate problems arising from the fact that NMT operates on implicit word and sentence representations in a continuous space, which makes influencing the process with external knowledge more complex. In particular, we attempt to answer the following questions: i) How to enforce the presence of a given translation recommendation in the decoder’s output? ii) How to place these word(s) in the right position? iii) How to guide the translation of outof-vocabulary terms? Our solution extends an existing NMT decoder (Sennrich et al., 2016a) by introducing the possibility to guide the translation process with constraints provided as XML annotations of the Differently from the phrase-based paradigm, neural machine translation (NMT) operates on word and sentence representations in a continuous space. This makes the decoding process not only more difficult to interpret, but also harder to influence with external knowledge. For the latter problem, effective solutions like the XML-markup used by phrase-based models to inject fixed translation options as constraints at decoding time are not yet available. We propose a “guide” mechani"
W17-4716,N07-1064,0,0.0222422,"f the source word &quot;assigned&quot; helps GDec to correct other parts of the final translation, like generating “es gibt keine” (En: “There is no”) which is otherwise missing in the baseline translation. 6 Task 2: Automatic Post-Editing In our second experiment, we apply guided decoding in an automatic post-editing task. The goal of automatic post-editing (APE) is to correct errors in an MT-ed text. The problem is typically approached as a “monolingual translation” task, in which models are trained on parallel corpora containing (MT_output, MT_post-edit) pairs, with MT post-edits coming from humans (Simard et al., 2007; Chatterjee et al., 2015b, 2017). In their attempt to translate the entire input sentence, APE systems usually tend to over-correct the source words, i.e. to use all applicable correction options. This can happen even when the input is correct, often resulting in text deterioration (Bojar et al., 2015). To cope with this problem, neuralbased APE decoders would benefit from external knowledge indicating words in the input which are correct and thus should not be modified during decoding. For that we propose to use wordlevel binary quality estimation labels (Blatz et al., 2004; de Souza et al.,"
W17-4716,2006.amta-papers.25,0,0.147198,"Missing"
W17-4716,P16-2049,0,0.0166403,"r lemmas, Sennrich and Haddow (2016) propose to compute separate embedding vectors for each linguistic information and then concatenate them, without altering the decoder. Other solutions exploit the strengths of PBSMT systems to improve NMT by pre-translating the source sentence. In Niehues et al. (2016), the NMT model is fed with a concatenation of the source and its PBSMT translation. Some of these solutions lead to improvements in performance, but they all require time-intensive training of the NMT models to use an enriched input representation or to optimize the parameters of the model. (Stahlberg et al., 2016) proposed an approach which can be used at decoding time. A hierarchical PBSMT system is used to generate the translation lattices, which are then re-scored by the NMT decoder. During decoding, the NMT posterior probabilities are adjusted using the posterior scores computed by the hierarchical model. However, by representing the additional information as a translation lattice, this approach does not allow the use of external knowledge in the form of bilingual terms or quality judgements as we do in §5 and §6. A different technique is postprocessing the translated sentences. Jean et al. (2015)"
W17-4716,C00-2137,0,0.0534122,"al., 2014) to annotate the “good” words that should be kept. Due to the relatively high quality of the MT outputs (62.11 BLEU), source sentences will usually contain many terms annotated as “good”. This, compared to the MT task, poses more constraints on the decoder. 6.1 6.2 Results and discussion Our results on the APE task are reported in Table 3. Performance is measured with the two WMT’16 APE task metrics, namely TER and BLEU (Bojar et al., 2016). The statistical significance for BLEU is computed using paired bootstrap resampling, while for TER we use stratified approximate randomization (Yeh, 2000). Our first baseline (Base-MT), the same used at WMT, corresponds to the original MT output left untouched. Our second baseline (Base-APE) is a neural APE system that was trained on (MT_output, MT_post-edit) pairs but ignores the information from the QE annotations. Base-APE improves the Base-MT up to 3.14 BLEU points. Similar to §5.2, the evaluation of our guided decoder is performed incrementally. GDec_base forces the “good” words in the automatic translation to appear in the output according to the mechanism described in §4.1. This basic guidance mechanism yields only marginal improvements"
W17-4716,steinberger-etal-2006-jrc,0,\N,Missing
W17-4716,P07-2045,1,\N,Missing
W17-4716,W16-2323,0,\N,Missing
W17-4723,W16-2308,0,0.0433237,"valuation, our baseline outperforms the 2016 best system’s baseline on the test sets 2015 and 2016. However, in our set-up backtranslations produced a smaller improvement than expected. The final submission is given by the combination of 7 systems, including a system trained only on true parallel data and two right-to-left systems, which improves over our single best system by 1.5 BLEU points. 1 2 Neural Machine Translation Neural machine translation [25] represents the state of the art for machine translation since the outstanding results obtained on IWSLT2015 [17] IWSLT2016 [1, 7] and WMT16 [24, 5] where the neural models greatly outperformed phrase-based systems. NMT is based on the encoder-decoderattention architecture [3] which jointly learns the translation and the alignment model with a sequence-to-sequence learning model. Given a sequence of words f1 , f2 , . . . , fm in the source language, they are used to index an embedding lookup table and retrieve the vectors x1 , x2 , . . . , xm representing the words. The embeddings are processed by a bi-directional RNN Introduction FBK’s participation to the news translations shared task in WMT 17 focused this year on the English-German la"
W17-4723,D17-1151,0,0.0307186,"ater than 50 or length ratio in one direction more than 1:9. In Table 1 we report the number of sentences before and after the cleaning step. The last step of the preprocessing is the BPE segmentation [23]. We trained 45, 000 BPE merge rules over the joint parallel data, which resulted in a vocabulary sizes of 43, 853 words for English and 47, 465 for German. The NMT architecture consists of 2 LSTM layers both in the encoder and in the decoder. We used LSTM RNNs instead of the GRU RNNs, as they performed better in our preliminary experiments. Our result is hence coherent with what reported in [6]. The word embeddings size and the number of hidden units for each LSTM layer are fixed to 500. The encoder is a bidirectional LSTM [21] with 500 hidden units equally divided among the two directions. The optimizer of choice is SGD [20] with exponential decay. In preliminary experiments, using different and smaller datasets, this optimizer outperformed Adagrad [10] and Adam [15]. Figure 1 shows the validation scores after each epoch on the validation sets with the different optimizers. In [7] Adagrad led to better results on the IWSLT En-Fr validation set, thus we argue that the choice of the"
W17-4723,W14-4012,0,0.103825,"tools. With respect to our participation in the IWSLT 2016 evaluation campaign, we switched from the Nematus-Theano framework to the OpenNMT-Torch framework [16]. The reasons were twofold: higher baseline performance and significantly faster training. In our primary submission we used backtranslations [22], BPE-encoding [23] and sys→ − → − h j = g(xj , h j−1 ), j = 1, ..m ← − ← − h j = g(xj , h j+1 ), j = m, .., 1 → − ← − hj = merge( h j , h j ) where merge is a function for merging the output of the RNNs, like the vector concatenation or the point-wise sum, and g is the LSTM [13] or the GRU [8] function. The sequence of vectors produced by the bidirectional RNN is the encoded 271 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 271–275 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics representation of the source sentence. The decoder takes as input the encoder outputs (or states) and produces a sequence of target words e1 , e2 , . . . , el . The decoder works by progressively predicting the probability of the next target word ei given the previously generated target words and the source context"
W17-4723,W11-2107,0,0.0896025,"e dataset. In the bottom Figure the trend is confirmed on IWSLT EN-FR data. System Combination Our primary submission has been produced by merging the outputs of different systems with Jane’s system combination tool [11]. For a system combination of m systems we build m confusion networks that are then merged to form a single confusion network. For each of the small networks, only one of the systems is chosen as the primary system, which is the system that decides the word order. The sentences from every secondary systems are then aligned to the primary. We perform word alignment using METEOR [9], a tool that uses four criteria for aligning words: 1) exact match; 2) stem, which matches two words if their stems computed with the Snowball Stemmer [19] are the same; 3) synonym, which uses the WordNet [18] synsets database; 4) paraphrase, which matches phrases if they are in an internal paraphrase table. When no criterion is matched, there is a match with the empty string. The confusion networks are initialized with the primary system sentences, then the words from the secondary hypothesis are added to the network according to the alignment. The final confusion network is obtained by the"
W17-4723,E14-2008,0,0.0539509,"ng of both the parallel sentences and 5M back-translated parallel sentences randomly sampled from the 30M set. As we describe in the following section, we used monolingual data also for the system combination. 5 Figure 1: Comparison between different optimizers in terms of BLEU. In the top Figure SGD with exponential decay is the best performing against Adam and Adagrad in a private dataset. In the bottom Figure the trend is confirmed on IWSLT EN-FR data. System Combination Our primary submission has been produced by merging the outputs of different systems with Jane’s system combination tool [11]. For a system combination of m systems we build m confusion networks that are then merged to form a single confusion network. For each of the small networks, only one of the systems is chosen as the primary system, which is the system that decides the word order. The sentences from every secondary systems are then aligned to the primary. We perform word alignment using METEOR [9], a tool that uses four criteria for aligning words: 1) exact match; 2) stem, which matches two words if their stems computed with the Snowball Stemmer [19] are the same; 3) synonym, which uses the WordNet [18] synset"
W17-4723,P13-2121,0,0.0831469,"tems that used different training data: 1. A NMT system trained on parallel + synthetic1 for 12 epochs 2. An NMT trained on parallel + synthetic right to left for 11 epochs2 3. The tuning of the baseline for 7 epochs more on parallel + synthetic data 4. The baseline system For each system, with the exception of the baseline, we used the weights of last two epochs. This gave us an improvement on the validation set of 0.5 Bleu points. We improved the system combination by adding a 5-grams language model with modified Kneser-Ney smoothing [14] without pruning, trained on ∼ 500M tokens with KenLM [12]. This improved the result by another +0.6 BLEU on the validation. In Table 2 we present the results of the single systems on newstest 2015 and 16. As expected, the systems are quite different also in terms of performance, especially for newstest2016, thus we expected significant improvements. Surprisingly, we found that our system trained from scratch on back-translated data performed worse than the baseline, while the right-to-left system trained on the same data is slightly better on newstest2015 and 1 Bleu point better on newstest2016. The best system is the one that was trained in two pha"
W17-4723,P17-4012,0,0.0414764,"lookup table and retrieve the vectors x1 , x2 , . . . , xm representing the words. The embeddings are processed by a bi-directional RNN Introduction FBK’s participation to the news translations shared task in WMT 17 focused this year on the English-German language direction. Our purpose was to explore the state of the art and build a competitive neural machine translation [3] system in order to gain a practical knowledge of the available tools. With respect to our participation in the IWSLT 2016 evaluation campaign, we switched from the Nematus-Theano framework to the OpenNMT-Torch framework [16]. The reasons were twofold: higher baseline performance and significantly faster training. In our primary submission we used backtranslations [22], BPE-encoding [23] and sys→ − → − h j = g(xj , h j−1 ), j = 1, ..m ← − ← − h j = g(xj , h j+1 ), j = m, .., 1 → − ← − hj = merge( h j , h j ) where merge is a function for merging the output of the RNNs, like the vector concatenation or the point-wise sum, and g is the LSTM [13] or the GRU [8] function. The sequence of vectors produced by the bidirectional RNN is the encoded 271 Proceedings of the Conference on Machine Translation (WMT), Volume 2: S"
W17-4723,2015.iwslt-evaluation.11,0,0.0764343,"a. With respect to last year’s evaluation, our baseline outperforms the 2016 best system’s baseline on the test sets 2015 and 2016. However, in our set-up backtranslations produced a smaller improvement than expected. The final submission is given by the combination of 7 systems, including a system trained only on true parallel data and two right-to-left systems, which improves over our single best system by 1.5 BLEU points. 1 2 Neural Machine Translation Neural machine translation [25] represents the state of the art for machine translation since the outstanding results obtained on IWSLT2015 [17] IWSLT2016 [1, 7] and WMT16 [24, 5] where the neural models greatly outperformed phrase-based systems. NMT is based on the encoder-decoderattention architecture [3] which jointly learns the translation and the alignment model with a sequence-to-sequence learning model. Given a sequence of words f1 , f2 , . . . , fm in the source language, they are used to index an embedding lookup table and retrieve the vectors x1 , x2 , . . . , xm representing the words. The embeddings are processed by a bi-directional RNN Introduction FBK’s participation to the news translations shared task in WMT 17 focused"
W17-4723,W16-2323,0,0.104557,"valuation, our baseline outperforms the 2016 best system’s baseline on the test sets 2015 and 2016. However, in our set-up backtranslations produced a smaller improvement than expected. The final submission is given by the combination of 7 systems, including a system trained only on true parallel data and two right-to-left systems, which improves over our single best system by 1.5 BLEU points. 1 2 Neural Machine Translation Neural machine translation [25] represents the state of the art for machine translation since the outstanding results obtained on IWSLT2015 [17] IWSLT2016 [1, 7] and WMT16 [24, 5] where the neural models greatly outperformed phrase-based systems. NMT is based on the encoder-decoderattention architecture [3] which jointly learns the translation and the alignment model with a sequence-to-sequence learning model. Given a sequence of words f1 , f2 , . . . , fm in the source language, they are used to index an embedding lookup table and retrieve the vectors x1 , x2 , . . . , xm representing the words. The embeddings are processed by a bi-directional RNN Introduction FBK’s participation to the news translations shared task in WMT 17 focused this year on the English-German la"
W18-1810,W14-3628,0,0.0151526,"on method, Word, are applied to ﬁt the same dictionary sizes (30,000) set in the NMT models. Since our training sets are small, choosing a small vocabulary size allows to illustrate large vocabulary reduction rates encountered in practical NMT tasks. We learn the merge rules of BPE at an equal size to the dictionary. Similarly, the LMVR models are trained with an output lexicon size of the same size. The rest of the settings are kept as default except for the perplexity threshold, for which we keep the default value of 10 for ﬁve languages, while for Arabic we use the value 70 as suggested by Al-Mannai et al. (2014). The translated sentences are desegmented based on the splitting characters (”@@” for BPE, ”+” for LMVR) before measuring the translation quality. 5.3 NMT Models The NMT models used in the evaluation are based on the Nematus toolkit (Sennrich et al., 2017). They have a hidden layer and embedding dimension of 1024 and a dictionary size of 30,000 for both source and target languages. We train the models using the Adagrad (Duchi et al., 2011) optimizer with a mini-batch size of 100, a learning rate of 0.01, and a dropout rate of 0.1 (in the input and output layers) and 0.2 (in the embeddings and"
W18-1810,D16-1025,1,0.820888,"method which also considers morphological properties of subwords. We compare both approaches on ten translation directions involving English and ﬁve other languages (Arabic, Czech, German, Italian and Turkish), each representing a distinct language family and morphological typology. LMVR obtains signiﬁcantly better performance in most languages, showing gains proportional to the sparseness of the vocabulary and the morphological complexity of the tested language. 1 Introduction Neural machine translation (NMT) has provided signiﬁcant improvements to the state-of-theart in machine translation (Bentivogli et al., 2016). However, it has also brought quite a few practical issues. A very important one of these is the low accuracy in translating rare words, caused by two of the main properties of the model. The ﬁrst is related to the requirement of observing many examples of a word until its internal representation becomes accurate, and the second is due to the difﬁculty of handling large vocabularies, as this has an impact on the computational complexity of the model. Current implementations of NMT models require long training time and large memory space due to the high number of parameters to optimize. Hence,"
W18-1810,2012.eamt-1.60,1,0.740277,"ing test2011, test2012 # types 220K(AR) - 120K(EN) 118K(CS) - 50K(EN) 144K(DE) - 69K(EN) 95K(IT) - 63K(EN) 171K(TR) - 53K(EN) # sentences 5,835 # tokens 89K(AR) - 114K(EN) 4,121 66K(AR) - 83K(EN) 3,112 52K(CS) - 61K(EN) 2,836 47K(CS) - 55K(EN) 5,777 108K(DE) - 113K(EN) 3,543 67K(DE) - 70K(EN) 3,517 74K(IT) - 79K(EN) 3,230 55K(IT) - 60K(EN) 2,433 34K(TR) - 47K(EN) 2,720 39K(TR) - 53K(EN) Table 2: Above: Training sets. Below: Development and Testing Sets. All data set are ofﬁcial evaluation sets from IWSLT. (M: Million, K: Thousand.) 5.1 Data We train our NMT models using the TED Talks corpora (Cettolo et al., 2012) and test them on ofﬁcial data sets of the IWSLT2 evaluation campaign from 2010 to 2015. This aids us in having a variety of languages with different morphological typology within the same benchmark. We select multiple development and testing sets from different years to obtain more reliable results. All data sets are tokenized and truecased using the Moses scripts3 (Koehn et al., 2007), except 2 The International Workshop on Spoken Language Translation with shared tasks organized between 2003-2017. 3 www.statmt.org/moses Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 2"
W18-1810,P11-2031,0,0.0634141,"e of 0.1 (in the input and output layers) and 0.2 (in the embeddings and hidden layers). In order to prevent over-ﬁtting, we stop training if the perplexity on the validation has not decreased for 5 epochs or the maximum number of epochs are reached. After 50 epochs, we choose the model with the highest performance on the development set for translating the test set. In order to present a comprehensive evaluation, we evaluated the accuracy of each model output using both BLEU (Papineni et al., 2002) and chrF3 (Popovic, 2015) metrics. Signiﬁcance tests are computed only for BLEU with Multeval (Clark et al., 2011). 6 Results The ﬁndings of the experiment, presented in Table 3, illustrates the translation qualities using different approaches and how these qualities vary among different languages. The results of the experiments where the English side is segmented with BPE show that LMVR generally achieves the best results by outperforming BPE with a signiﬁcant improvement in three out of four morphologically-rich languages. The improvements are 1.55 BLEU points in Turkish-toEnglish, 1.08 BLEU points in Arabic-English and 0.99 BLEU points in Czech-to-English. In the German-to-English translation task, the"
W18-1810,C14-1111,0,0.0439774,"Missing"
W18-1810,W17-4706,0,0.241076,"two languages. In addition to providing a new perspective to modeling translation at the sublexical level, these approaches have alleviated the out-of-vocabulary problem in NMT. The sore point of these methods, however, is that they disregard any linguistic notion during segmentation. Many studies have shown that using subword segmentation methods which do not preserve the morpheme boundaries inside words may lead to loss of information related to the semantic or syntactic properties of words and generate inaccurate translations (Niehues et al., 2016; Ataman et al., 2017; Pinnis et al., 2017; Huck et al., 2017; Tamchyna et al., 2017). A more linguistically motivated solution was recently proposed by Ataman et al. (2017), which segments words into subwords by estimating their likeliness of being morphemes and their morphological categories. This approach provided signiﬁcant improvements for translation of Turkish, an agglutinative language with a very sparse vocabulary. In this paper, we present a comparative study on two unsupervised word segmentation methods: Byte-Pair Encoding (BPE) (Sennrich et al., 2016) and the Linguistically-Motivated Vocabulary Reduction (LMVR) method by Ataman et al. (2017)"
W18-1810,P07-2045,1,0.011726,"able 2: Above: Training sets. Below: Development and Testing Sets. All data set are ofﬁcial evaluation sets from IWSLT. (M: Million, K: Thousand.) 5.1 Data We train our NMT models using the TED Talks corpora (Cettolo et al., 2012) and test them on ofﬁcial data sets of the IWSLT2 evaluation campaign from 2010 to 2015. This aids us in having a variety of languages with different morphological typology within the same benchmark. We select multiple development and testing sets from different years to obtain more reliable results. All data sets are tokenized and truecased using the Moses scripts3 (Koehn et al., 2007), except 2 The International Workshop on Spoken Language Translation with shared tasks organized between 2003-2017. 3 www.statmt.org/moses Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 103 for Arabic, which is normalized and tokenized using the QCRI normalization tool4 (Sajjad et al., 2013). The details of the statistical characteristics of each data set used in our experiments and the chosen development and testing sets are given in Table 2. 5.2 Segmentation Models The two subword segmentation methods that we compare in our experiments, BPE and LMVR, as"
W18-1810,Q17-1026,0,0.0315713,"leads to critical restrictions in translating morphologically-rich languages, where the word vocabulary tends to be very large and sparse. For example, in our case study, despite the relatively small size of our training corpora, the size of the source vocabulary found in the Turkish-English training corpus is around 170,000, i.e. much larger than the maximum size that is generally used. Some studies have tried to overcome this problem by redeﬁning the model vocabulary in terms of interior orthographic units compounding the words. These units could be individual characters (Ling et al., 2015; Lee et al., 2017), hybrid word/character units (Luong and Manning, 2016), or subwords (Sennrich et al., 2016), i.e. character sequences segmented according to their frequency in the training corpus. The prominent approach used today is to treat these subwords as individual lexical units. Hence, NMT is learned as a bilingual mapping between subword units of two languages. In addition to providing a new perspective to modeling translation at the sublexical level, these approaches have alleviated the out-of-vocabulary problem in NMT. The sore point of these methods, however, is that they disregard any linguistic"
W18-1810,P16-1100,0,0.0319729,"rphologically-rich languages, where the word vocabulary tends to be very large and sparse. For example, in our case study, despite the relatively small size of our training corpora, the size of the source vocabulary found in the Turkish-English training corpus is around 170,000, i.e. much larger than the maximum size that is generally used. Some studies have tried to overcome this problem by redeﬁning the model vocabulary in terms of interior orthographic units compounding the words. These units could be individual characters (Ling et al., 2015; Lee et al., 2017), hybrid word/character units (Luong and Manning, 2016), or subwords (Sennrich et al., 2016), i.e. character sequences segmented according to their frequency in the training corpus. The prominent approach used today is to treat these subwords as individual lexical units. Hence, NMT is learned as a bilingual mapping between subword units of two languages. In addition to providing a new perspective to modeling translation at the sublexical level, these approaches have alleviated the out-of-vocabulary problem in NMT. The sore point of these methods, however, is that they disregard any linguistic notion during segmentation. Many studies have shown tha"
W18-1810,D15-1166,0,0.717986,"ty obtained using LMVR (Ataman et al., 2017) in three of the languages (Arabic, Czech and Turkish) is signiﬁcantly better than that with BPE (Sennrich et al., 2016). All of these languages share the common feature of having a high level of sparseness or a morphology with agglutinating or concatenating properties. For the remaining two languages with fusional characteristics and lower sparseness: German and Italian, the two segmentation methods yield comparable performance. In general, both word segmentation methods outperform the simple frequency-based vocabulary reduction method proposed by (Luong et al., 2015). Our study suggests that considering the morphological characteristics of the chosen language pair is essential in order to choose the most appropriate subword segmentation approach in NMT. 2 Neural Machine Translation (NMT) In this paper, we use the NMT model described in (Bahdanau et al., 2014). The model essentially estimates the conditional probability of translating a source text x, represented by the input word sequence x = (x1 , x2 , . . . xm ) of length m, into a target text y, represented as the target word sequence y = (y1 , y2 , . . . yj . . . yl ) of length l. The conditional prob"
W18-1810,C16-1172,0,0.221546,"NMT is learned as a bilingual mapping between subword units of two languages. In addition to providing a new perspective to modeling translation at the sublexical level, these approaches have alleviated the out-of-vocabulary problem in NMT. The sore point of these methods, however, is that they disregard any linguistic notion during segmentation. Many studies have shown that using subword segmentation methods which do not preserve the morpheme boundaries inside words may lead to loss of information related to the semantic or syntactic properties of words and generate inaccurate translations (Niehues et al., 2016; Ataman et al., 2017; Pinnis et al., 2017; Huck et al., 2017; Tamchyna et al., 2017). A more linguistically motivated solution was recently proposed by Ataman et al. (2017), which segments words into subwords by estimating their likeliness of being morphemes and their morphological categories. This approach provided signiﬁcant improvements for translation of Turkish, an agglutinative language with a very sparse vocabulary. In this paper, we present a comparative study on two unsupervised word segmentation methods: Byte-Pair Encoding (BPE) (Sennrich et al., 2016) and the Linguistically-Motivat"
W18-1810,P02-1040,0,0.100159,"Missing"
W18-1810,W15-3049,0,0.0194652,"optimizer with a mini-batch size of 100, a learning rate of 0.01, and a dropout rate of 0.1 (in the input and output layers) and 0.2 (in the embeddings and hidden layers). In order to prevent over-ﬁtting, we stop training if the perplexity on the validation has not decreased for 5 epochs or the maximum number of epochs are reached. After 50 epochs, we choose the model with the highest performance on the development set for translating the test set. In order to present a comprehensive evaluation, we evaluated the accuracy of each model output using both BLEU (Papineni et al., 2002) and chrF3 (Popovic, 2015) metrics. Signiﬁcance tests are computed only for BLEU with Multeval (Clark et al., 2011). 6 Results The ﬁndings of the experiment, presented in Table 3, illustrates the translation qualities using different approaches and how these qualities vary among different languages. The results of the experiments where the English side is segmented with BPE show that LMVR generally achieves the best results by outperforming BPE with a signiﬁcant improvement in three out of four morphologically-rich languages. The improvements are 1.55 BLEU points in Turkish-toEnglish, 1.08 BLEU points in Arabic-English"
W18-1810,2013.iwslt-evaluation.8,0,0.0605815,"Missing"
W18-1810,E17-3017,0,0.0965449,"Missing"
W18-1810,P16-1162,0,0.809958,"word vocabulary tends to be very large and sparse. For example, in our case study, despite the relatively small size of our training corpora, the size of the source vocabulary found in the Turkish-English training corpus is around 170,000, i.e. much larger than the maximum size that is generally used. Some studies have tried to overcome this problem by redeﬁning the model vocabulary in terms of interior orthographic units compounding the words. These units could be individual characters (Ling et al., 2015; Lee et al., 2017), hybrid word/character units (Luong and Manning, 2016), or subwords (Sennrich et al., 2016), i.e. character sequences segmented according to their frequency in the training corpus. The prominent approach used today is to treat these subwords as individual lexical units. Hence, NMT is learned as a bilingual mapping between subword units of two languages. In addition to providing a new perspective to modeling translation at the sublexical level, these approaches have alleviated the out-of-vocabulary problem in NMT. The sore point of these methods, however, is that they disregard any linguistic notion during segmentation. Many studies have shown that using subword segmentation methods"
W18-1810,W17-4704,0,0.251615,"ddition to providing a new perspective to modeling translation at the sublexical level, these approaches have alleviated the out-of-vocabulary problem in NMT. The sore point of these methods, however, is that they disregard any linguistic notion during segmentation. Many studies have shown that using subword segmentation methods which do not preserve the morpheme boundaries inside words may lead to loss of information related to the semantic or syntactic properties of words and generate inaccurate translations (Niehues et al., 2016; Ataman et al., 2017; Pinnis et al., 2017; Huck et al., 2017; Tamchyna et al., 2017). A more linguistically motivated solution was recently proposed by Ataman et al. (2017), which segments words into subwords by estimating their likeliness of being morphemes and their morphological categories. This approach provided signiﬁcant improvements for translation of Turkish, an agglutinative language with a very sparse vocabulary. In this paper, we present a comparative study on two unsupervised word segmentation methods: Byte-Pair Encoding (BPE) (Sennrich et al., 2016) and the Linguistically-Motivated Vocabulary Reduction (LMVR) method by Ataman et al. (2017) for NMT. Our analysis a"
W18-2106,W18-2100,0,0.0626983,"Missing"
W18-6316,W17-1207,0,0.125252,"Missing"
W18-6316,E17-2068,0,0.0211287,"Missing"
W18-6316,W18-3931,0,0.193666,"Missing"
W18-6316,W17-3203,0,0.0311936,"nd the English−European/Brazilian Portuguese data with the corresponding OpenSubtitles2018 datasets from the OPUS corpus. Table 2 summarizes the augmented training data, while keeping the same dev and test sets. Experimental Settings We trained all systems using the Transformer model8 (Vaswani et al., 2018). We use the Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 0.2 and a dropout also set to 0.2. A shared source and target vocabulary of size 16k is generated via sub-word segmentation (Wu et al., 2016). The choice for the vocabulary size follows the recommendations in Denkowski and Neubig (2017) regarding training of NMT systems on TED Talks data. Overall we use a uniform setting for all our models, with a 512 embedding dimension and hidden units, and 6 layers of selfattention encoder-decoder network. The training batch size is of 6144 sub-word tokens and the max length after segmentation is set to 70. Following Vaswani et al. (2017) and for a fair comparison, experiments are run for 100k training steps, i.e., in the low-resource settings all models are observed to converge within these steps. Adaptation experiments are run to convergence, which requires roughly half of the steps (i."
W18-6316,W04-3250,0,0.0606918,"Missing"
W18-6316,P15-1166,0,0.0349469,"B and DE→A∪B as they are, by not specifying any label at training time for entries from DE→A∪B . The second model, named M-C2, works similarly to Mul, but relying on a language variety identification module (trained on the target data of DE→A and DE→B ) that maps each unlabeled data point either to A or B. The third model, named M-C3, can be seen as an enhancement of M-U, as the unlabeled data is automatically classified into one of three classes: A, B, or A ∪ B. For the third class, like with M-U, no label is applied on the source sentence. In a one-to-many multilingual translation scenario, Dong et al. (2015) proposed a multi-task learning approach that utilizes a single encoder for source languages and separate attention mechanisms and decoders for every target language. Luong et al. (2015) used distinct encoder and decoder networks for modeling language pairs in a manyto-many setting. Firat et al. (2016) introduced a way to share the attention mechanism across multiple languages. A simplified and efficient multilingual NMT approach is proposed by Johnson et al. (2016) and Ha et al. (2016) by prepending language tokens to the input string. This approach has greatly simplified multi-lingual NMT, b"
W18-6316,W17-3204,0,0.0336594,"relevant problems. First, languages varieties such as dialects might significantly overlap thus making differences among their texts quite subtle (e.g., particular grammatical constructs or lexical divergences like the ones reported in the example). Second, parallel data are not always labeled at the level of language variety, making it hard to develop specific NMT engines. Finally, training data might be very unbalanced among different varieties, due to the population sizes of their respective speakers or for other reasons. This clearly makes it harder to model the lower-resourced varieties (Koehn and Knowles, 2017). In this work we present our initial effort to systematically investigate ways to approach NMT from English into four pairs of language varieties: Both research and commercial machine translation have so far neglected the importance of properly handling the spelling, lexical and grammar divergences occurring among language varieties. Notable cases are standard national varieties such as Brazilian and European Portuguese, and Canadian and European French, which popular online machine translation services are not keeping distinct. We show that an evident side effect of modeling such varieties a"
W18-6316,P10-1048,0,0.0327783,"able from the OPUS collection5 . After presenting related work (Section 2) on NLP and MT of dialects and related languages, we introduce (in Section 3) baseline NMT systems, either language/dialect specific or generic, and multilingual NMT systems, either trained with fully supervised (or labeled) data or with partially supervised data. In Section 4, we introduce our datasets, NMT set-ups based on the Transformer architecture, and then present the results for each evaluated system. We conclude the paper with a discussion and conclusion in Sections 5 and 6. 2 2.1 et al., 2016), Hindi and Urdu (Durrani et al., 2010), or Arabic dialects (Harrat et al., 2017). Notably, Pourdamghani and Knight (2017) build an unsupervised deciphering model to translate between closely related languages without parallel data. Salloum et al. (2014) handle mixed Arabic dialect input in MT by using a sentence-level classifier to select the most suitable model from an ensemble of multiple SMT systems. In NMT, however, there have been fewer studies addressing language varieties. It is reported that an RNN model outperforms SMT when translating from Catalan to Spanish (Costa-juss`a, 2017) and from European to Brazilian Portuguese"
W18-6316,W16-4801,0,0.0348197,"Missing"
W18-6316,N16-1101,0,0.0274335,"or B. The third model, named M-C3, can be seen as an enhancement of M-U, as the unlabeled data is automatically classified into one of three classes: A, B, or A ∪ B. For the third class, like with M-U, no label is applied on the source sentence. In a one-to-many multilingual translation scenario, Dong et al. (2015) proposed a multi-task learning approach that utilizes a single encoder for source languages and separate attention mechanisms and decoders for every target language. Luong et al. (2015) used distinct encoder and decoder networks for modeling language pairs in a manyto-many setting. Firat et al. (2016) introduced a way to share the attention mechanism across multiple languages. A simplified and efficient multilingual NMT approach is proposed by Johnson et al. (2016) and Ha et al. (2016) by prepending language tokens to the input string. This approach has greatly simplified multi-lingual NMT, by eliminating the need of having separate encoder/decoder networks and attention mechanism for every new language pair. In this work we follow a similar strategy, by incorporating an artificial token as a unique variety flag. 3 NMT into Language Varieties Our assumption is to translate from language E"
W18-6316,2011.eamt-1.19,0,0.0410831,"el corpora for language identification: content comparability allows capturing subtle linguistic differences between dialects while avoiding content-related biases. The problem of ambiguous sentences, i.e., those for which it is impossible to decide upon the dialect tag, has been demonstrated for Portuguese by Goutte et al. (2016) through inspection of disagreement between human annotators. Related work Machine Translation of Varieties Most of the works on translation between and from/to written language varieties involve rulebased transformations, e.g., for European and Brazilian Portuguese (Marujo et al., 2011), Indonesian and Malay (Tan et al., 2012), Turkish and Crimean Tatar (Altintas and C ¸ ic¸ekli, 2002); or phrase-based statistical MT (SMT) systems, e.g., for Croatian, Serbian, and Slovenian (Popovi´c 3 According to Wikipedia, Brazilian Portuguese is a dialect of European Portuguese, Canadian French is a dialect of European French, Serbian and Croatian are standardized registers of Serbo-Croatian, and Indonesian is a standardized register of Malay. 4 http://wit3.fbk.eu/ 5 http://opus.nlpl.eu/ 157 2.3 Multilingual NMT (Spec) trained on the corresponding language variety training set. The third"
W18-6316,L16-1284,0,0.0271917,"Missing"
W18-6316,I17-2050,0,0.0414471,"Missing"
W18-6316,W16-4806,0,0.0605209,"Missing"
W18-6316,D17-1266,0,0.0960652,"on NLP and MT of dialects and related languages, we introduce (in Section 3) baseline NMT systems, either language/dialect specific or generic, and multilingual NMT systems, either trained with fully supervised (or labeled) data or with partially supervised data. In Section 4, we introduce our datasets, NMT set-ups based on the Transformer architecture, and then present the results for each evaluated system. We conclude the paper with a discussion and conclusion in Sections 5 and 6. 2 2.1 et al., 2016), Hindi and Urdu (Durrani et al., 2010), or Arabic dialects (Harrat et al., 2017). Notably, Pourdamghani and Knight (2017) build an unsupervised deciphering model to translate between closely related languages without parallel data. Salloum et al. (2014) handle mixed Arabic dialect input in MT by using a sentence-level classifier to select the most suitable model from an ensemble of multiple SMT systems. In NMT, however, there have been fewer studies addressing language varieties. It is reported that an RNN model outperforms SMT when translating from Catalan to Spanish (Costa-juss`a, 2017) and from European to Brazilian Portuguese (Costa-Juss`a et al., 2018). Hassan et al. (2017) propose a technique to augment tr"
W18-6316,P14-2125,0,0.0679797,"ic, and multilingual NMT systems, either trained with fully supervised (or labeled) data or with partially supervised data. In Section 4, we introduce our datasets, NMT set-ups based on the Transformer architecture, and then present the results for each evaluated system. We conclude the paper with a discussion and conclusion in Sections 5 and 6. 2 2.1 et al., 2016), Hindi and Urdu (Durrani et al., 2010), or Arabic dialects (Harrat et al., 2017). Notably, Pourdamghani and Knight (2017) build an unsupervised deciphering model to translate between closely related languages without parallel data. Salloum et al. (2014) handle mixed Arabic dialect input in MT by using a sentence-level classifier to select the most suitable model from an ensemble of multiple SMT systems. In NMT, however, there have been fewer studies addressing language varieties. It is reported that an RNN model outperforms SMT when translating from Catalan to Spanish (Costa-juss`a, 2017) and from European to Brazilian Portuguese (Costa-Juss`a et al., 2018). Hassan et al. (2017) propose a technique to augment training data for under-resourced dialects via projecting word embeddings from a resource-rich related language, thus enabling trainin"
W18-6316,C12-1160,0,0.0734731,"Missing"
W18-6316,W18-1819,0,0.0602212,"Missing"
W18-6316,1983.tc-1.13,0,0.686471,"Missing"
W18-6316,W14-5307,0,0.0418296,"Missing"
W18-6316,W15-5401,0,0.0487869,"Missing"
W18-6316,D16-1163,0,0.0486413,"Missing"
