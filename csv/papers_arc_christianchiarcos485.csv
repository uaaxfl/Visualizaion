2021.semdeep-1.3,Embeddings for the Lexicon: Modelling and Representation,2021,-1,-1,1,1,2108,christian chiarcos,Proceedings of the 6th Workshop on Semantic Deep Learning (SemDeep-6),0,None
2020.lrec-1.401,The {AC}o{L}i Dictionary Graph,2020,-1,-1,1,1,2108,christian chiarcos,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we report the release of the ACoLi Dictionary Graph, a large-scale collection of multilingual open source dictionaries available in two machine-readable formats, a graph representation in RDF, using the OntoLex-Lemon vocabulary, and a simple tabular data format to facilitate their use in NLP tasks, such as translation inference across dictionaries. We describe the mapping and harmonization of the underlying data structures into a unified representation, its serialization in RDF and TSV, and the release of a massive and coherent amount of lexical data under open licenses."
2020.lrec-1.695,Recent Developments for the Linguistic Linked Open Data Infrastructure,2020,-1,-1,5,0,2109,thierry declerck,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper we describe the contributions made by the European H2020 project {``}Pr{\^e}t-{\`a}-LLOD{''} ({`}Ready-to-use Multilingual Linked Language Data for Knowledge Services across Sectors{'}) to the further development of the Linguistic Linked Open Data (LLOD) infrastructure. Pr{\^e}t-{\`a}-LLOD aims to develop a new methodology for building data value chains applicable to a wide range of sectors and applications and based around language resources and language technologies that can be integrated by means of semantic technologies. We describe the methods implemented for increasing the number of language data sets in the LLOD. We also present the approach for ensuring interoperability and for porting LLOD data sets and services to other infrastructures, as well as the contribution of the projects to existing standards."
2020.lrec-1.696,Annotation Interoperability for the Post-{ISOC}at Era,2020,-1,-1,1,1,2108,christian chiarcos,Proceedings of the 12th Language Resources and Evaluation Conference,0,"With this paper, we provide an overview over ISOCat successor solutions and annotation standardization efforts since 2010, and we describe the low-cost harmonization of post-ISOCat vocabularies by means of modular, linked ontologies: The CLARIN Concept Registry, LexInfo, Universal Parts of Speech, Universal Dependencies and UniMorph are linked with the Ontologies of Linguistic Annotation and through it with ISOCat, the GOLD ontology, the Typological Database Systems ontology and a large number of annotation schemes."
2020.lrec-1.885,A Tree Extension for {C}o{NLL}-{RDF},2020,-1,-1,1,1,2108,christian chiarcos,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The technological bridges between knowledge graphs and natural language processing are of utmost importance for the future development of language technology. CoNLL-RDF is a technology that provides such a bridge for popular one-word-per-line formats as widely used in NLP (e.g., the CoNLL Shared Tasks), annotation (Universal Dependencies, Unimorph), corpus linguistics (Corpus WorkBench, CWB) and digital lexicography (SketchEngine): Every empty-line separated table (usually a sentence) is parsed into an graph, can be freely manipulated and enriched using W3C-standardized RDF technology, and then be serialized back into in a TSV format, RDF or other formats. An important limitation is that CoNLL-RDF provides native support for word-level annotations only. This does include dependency syntax and semantic role annotations, but neither phrase structures nor text structure. We describe the extension of the CoNLL-RDF technology stack for two vocabulary extensions of CoNLL-TSV, the PTB bracket notation used in earlier CoNLL Shared Tasks and the extension with XML markup elements featured by CWB and SketchEngine. In order to represent the necessary extensions of the CoNLL vocabulary in an adequate fashion, we employ the POWLA vocabulary for representing and navigating in tree structures."
2020.lrec-1.891,"Fintan - Flexible, Integrated Transformation and Annotation e{N}gineering",2020,-1,-1,2,0.625,17486,christian fath,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We introduce the Flexible and Integrated Transformation and Annotation eNgeneering (Fintan) platform for converting heterogeneous linguistic resources to RDF. With its modular architecture, workflow management and visualization features, Fintan facilitates the development of complex transformation pipelines by integrating generic RDF converters and augmenting them with extended graph processing capabilities: Existing converters can be easily deployed to the system by means of an ontological data structure which renders their properties and the dependencies between transformation steps. Development of subsequent graph transformation steps for resource transformation, annotation engineering or entity linking is further facilitated by a novel visual rendering of SPARQL queries. A graphical workflow manager allows to easily manage the converter modules and combine them to new transformation pipelines. Employing the stream-based graph processing approach first implemented with CoNLL-RDF, we address common challenges and scalability issues when transforming resources and showcase the performance of Fintan by means of a purely graph-based transformation of the Universal Morphology data to RDF."
2020.iwltp-1.2,On the Linguistic Linked Open Data Infrastructure,2020,-1,-1,1,1,2108,christian chiarcos,Proceedings of the 1st International Workshop on Language Technology Platforms,0,"In this paper we describe the current state of development of the Linguistic Linked Open Data (LLOD) infrastructure, an LOD(sub-)cloud of linguistic resources, which covers various linguistic data bases, lexicons, corpora, terminology and metadata repositories.We give in some details an overview of the contributions made by the European H2020 projects {``}Pr{\^e}t-{\`a}-LLOD{''} ({`}Ready-to-useMultilingual Linked Language Data for Knowledge Services across Sectors{'}) and {``}ELEXIS{''} ({`}European Lexicographic Infrastructure{'}) to the further development of the LLOD."
2020.iwltp-1.15,Towards an Interoperable Ecosystem of {AI} and {LT} Platforms: A Roadmap for the Implementation of Different Levels of Interoperability,2020,20,2,11,0.183455,60,georg rehm,Proceedings of the 1st International Workshop on Language Technology Platforms,0,"With regard to the wider area of AI/LT platform interoperability, we concentrate on two core aspects: (1) cross-platform search and discovery of resources and services; (2) composition of cross-platform service workflows. We devise five different levels (of increasing complexity) of platform interoperability that we suggest to implement in a wider federation of AI/LT platforms. We illustrate the approach using the five emerging AI/LT platforms AI4EU, ELG, Lynx, QURATOR and SPEAKER."
2020.globalex-1.1,Modelling Frequency and Attestations for {O}nto{L}ex-Lemon,2020,-1,-1,1,1,2108,christian chiarcos,Proceedings of the 2020 Globalex Workshop on Linked Lexicography,0,"The OntoLex vocabulary enjoys increasing popularity as a means of publishing lexical resources with RDF and as Linked Data. The recent publication of a new OntoLex module for lexicography, lexicog, reflects its increasing importance for digital lexicography. However, not all aspects of digital lexicography have been covered to the same extent. In particular, supplementary information drawn from corpora such as frequency information, links to attestations, and collocation data were considered to be beyond the scope of lexicog. Therefore, the OntoLex community has put forward the proposal for a novel module for frequency, attestation and corpus information (FrAC), that not only covers the requirements of digital lexicography, but also accommodates essential data structures for lexical information in natural language processing. This paper introduces the current state of the OntoLex-FrAC vocabulary, describes its structure, some selected use cases, elementary concepts and fundamental definitions, with a focus on frequency and attestations."
2020.globalex-1.16,Translation Inference by Concept Propagation,2020,-1,-1,1,1,2108,christian chiarcos,Proceedings of the 2020 Globalex Workshop on Linked Lexicography,0,"This paper describes our contribution to the Third Shared Task on Translation Inference across Dictionaries (TIAD-2020). We describe an approach on translation inference based on symbolic methods, the propagation of concepts over a graph of interconnected dictionaries: Given a mapping from source language words to lexical concepts (e.g., synsets) as a seed, we use bilingual dictionaries to extrapolate a mapping of pivot and target language words to these lexical concepts. Translation inference is then performed by looking up the lexical concept(s) of a source language word and returning the target language word(s) for which these lexical concepts have the respective highest score. We present two instantiations of this system: One using WordNet synsets as concepts, and one using lexical entries (translations) as concepts. With a threshold of 0, the latter configuration is the second among participant systems in terms of F1 score. We also describe additional evaluation experiments on Apertium data, a comparison with an earlier approach based on embedding projection, and an approach for constrained projection that outperforms the TIAD-2020 vanilla system by a large margin."
2020.coling-main.308,Towards the First Machine Translation System for {S}umerian Transliterations,2020,-1,-1,3,0,12425,ravneet punia,Proceedings of the 28th International Conference on Computational Linguistics,0,"The Sumerian cuneiform script was invented more than 5,000 years ago and represents one of the oldest in history. We present the first attempt to translate Sumerian texts into English automatically. We publicly release high-quality corpora for standardized training and evaluation and report results on experiments with supervised, phrase-based, and transfer learning techniques for machine translation. Quantitative and qualitative evaluations indicate the usefulness of the translations. Our proposed methodology provides a broader audience of researchers with novel access to the data, accelerates the costly and time-consuming manual translation process, and helps them better explore the relationships between Sumerian cuneiform and Mesopotamian culture."
L18-1090,The {AC}o{L}i {C}o{NLL} Libraries: Beyond Tab-Separated Values,2018,0,0,1,1,2108,christian chiarcos,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1387,Towards a Linked Open Data Edition of {S}umerian Corpora,2018,0,2,1,1,2108,christian chiarcos,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1417,Universal Morphologies for the Caucasus region,2018,0,0,1,1,2108,christian chiarcos,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1717,Analyzing {M}iddle {H}igh {G}erman Syntax with {RDF} and {SPARQL},2018,0,2,1,1,2108,christian chiarcos,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1721,Interoperability of Language-related Information: Mapping the {BLL} Thesaurus to Lexvo and Glottolog,2018,0,1,3,0,30322,vanya dimitrova,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-2202,Machine Translation and Automated Analysis of the {S}umerian Language,2017,9,3,4,0,12427,emilie pageperron,"Proceedings of the Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"This paper presents a newly funded international project for machine translation and automated analysis of ancient cuneiform languages where NLP specialists and Assyriologists collaborate to create an information retrieval system for Sumerian. This research is conceived in response to the need to translate large numbers of administrative texts that are only available in transcription, in order to make them accessible to a wider audience. The methodology includes creation of a specialized NLP pipeline and also the use of linguistic linked open data to increase access to the results."
W17-0910,Resource-Lean Modeling of Coherence in Commonsense Stories,2017,20,0,2,1,12426,niko schenk,"Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics",0,We present a resource-lean neural recognizer for modeling coherence in commonsense stories. Our lightweight system is inspired by successful attempts to modeling discourse relations and stands out due to its simplicity and easy optimization compared to prior approaches to narrative script learning. We evaluate our approach in the Story Cloze Test demonstrating an absolute improvement in accuracy of 4.7{\%} over state-of-the-art implementations.
P17-2040,A Recurrent Neural Model with Attention for the Recognition of {C}hinese Implicit Discourse Relations,2017,21,6,3,0,2649,samuel ronnqvist,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank. We also visualize its attention activity to illustrate the model{'}s ability to selectively focus on the relevant parts of an input sequence."
N16-1173,Unsupervised Learning of Prototypical Fillers for Implicit Semantic Role Labeling,2016,30,4,2,1,12426,niko schenk,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Gold annotations for supervised implicit semantic role labeling are extremely sparse and costly. As a lightweight alternative, this paper describes an approach based on unsupervised parsing which can do without iSRL-specific training data: We induce prototypical roles from large amounts of explicit SRL annotations paired with their distributed word representations. An evaluation shows competitive performance with supervised methods on the SemEval 2010 data, and our method can easily be applied to predicates (or languages) for which no training annotations are available."
L16-1234,Combining Ontologies and Neural Networks for Analyzing Historical Language Varieties. A Case Study in {M}iddle {L}ow {G}erman,2016,0,3,2,1,21154,maria sukhareva,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper, we describe experiments on the morphosyntactic annotation of historical language varieties for the example of Middle Low German (MLG), the official language of the German Hanse during the Middle Ages and a dominant language around the Baltic Sea by the time. To our best knowledge, this is the first experiment in automatically producing morphosyntactic annotations for Middle Low German, and accordingly, no part-of-speech (POS) tagset is currently agreed upon. In our experiment, we illustrate how ontology-based specifications of projected annotations can be employed to circumvent this issue: Instead of training and evaluating against a given tagset, we decomponse it into independent features which are predicted independently by a neural network. Using consistency constraints (axioms) from an ontology, then, the predicted feature probabilities are decoded into a sound ontological representation. Using these representations, we can finally bootstrap a POS tagset capturing only morphosyntactic features which could be reliably predicted. In this way, our approach is capable to optimize precision and recall of morphosyntactic annotations simultaneously with bootstrapping a tagset rather than performing iterative cycles."
L16-1386,The Open Linguistics Working Group: Developing the Linguistic Linked Open Data Cloud,2016,16,11,2,0.0876232,1255,john mccrae,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The Open Linguistics Working Group (OWLG) brings together researchers from various fields of linguistics, natural language processing, and information technology to present and discuss principles, case studies, and best practices for representing, publishing and linking linguistic data collections. A major outcome of our work is the Linguistic Linked Open Data (LLOD) cloud, an LOD (sub-)cloud of linguistic resources, which covers various linguistic databases, lexicons, corpora, terminologies, and metadata repositories. We present and summarize five years of progress on the development of the cloud and of advancements in open data in linguistics, and we describe recent community activities. The paper aims to serve as a guideline to orient and involve researchers with the community and/or Linguistic Linked Open Data."
L16-1642,Word Segmentation for {A}kkadian Cuneiform,2016,0,0,2,0,35355,timo homburg,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present experiments on word segmentation for Akkadian cuneiform, an ancient writing system and a language used for about 3 millennia in the ancient Near East. To our best knowledge, this is the first study of this kind applied to either the Akkadian language or the cuneiform writing system. As a logosyllabic writing system, cuneiform structurally resembles Eastern Asian writing systems, so, we employ word segmentation algorithms originally developed for Chinese and Japanese. We describe results of rule-based algorithms, dictionary-based algorithms, statistical and machine learning approaches. Our results may indicate possible promising steps in cuneiform word segmentation that can create and improve natural language processing in this area."
L16-1707,"{L}in|gu|is|tik: Building the Linguist{'}s Pathway to Bibliographies, Libraries, Language Resources and Linked Open Data",2016,0,3,1,1,2108,christian chiarcos,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper introduces a novel research tool for the field of linguistics: The Lin|gu|is|tik web portal provides a virtual library which offers scientific information on every linguistic subject. It comprises selected internet sources and databases as well as catalogues for linguistic literature, and addresses an interdisciplinary audience. The virtual library is the most recent outcome of the Special Subject Collection Linguistics of the German Research Foundation (DFG), and also integrates the knowledge accumulated in the Bibliography of Linguistic Literature. In addition to the portal, we describe long-term goals and prospects with a special focus on ongoing efforts regarding an extension towards integrating language resources and Linguistic Linked Open Data."
K16-2005,Do We Really Need All Those Rich Linguistic Features? A Neural Network-Based Approach to Implicit Sense Labeling,2016,29,7,2,1,12426,niko schenk,Proceedings of the {C}o{NLL}-16 shared task,0,None
2016.jeptalnrecital-invite.1,"Corpora and Linguistic Linked Open Data: Motivations, Applications, Limitations",2016,-1,-1,1,1,2108,christian chiarcos,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. Volume 4 : Conf{\\'e}rences invit{\\'e}es,0,"Linguistic Linked Open Data (LLOD) is a technology and a movement in several disciplines working with language resources, including Natural Language Processing, general linguistics, computational lexicography and the localization industry. This talk describes basic principles of Linguistic Linked Open Data and their application to linguistically annotated corpora, it summarizes the current status of the Linguistic Linked Open Data cloud and gives an overview over selected LLOD vocabularies and their uses."
W15-5505,An Ontology-based Approach To Automatic Part-of-Speech Tagging Using Heterogeneously Annotated Corpora,2015,-1,-1,2,1,21154,maria sukhareva,Proceedings of the Second Workshop on Natural Language Processing and Linked Open Data,0,None
W15-4626,Memory-Based Acquisition of Argument Structures and its Application to Implicit Role Detection,2015,23,1,1,1,2108,christian chiarcos,Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"We propose a generic, memory-based approach for the detection of implicit semantic roles. While state-of-the-art methods for this task combine hand-crafted rules with specialized and costly lexical resources, our models use large corpora with automated annotations for explicit semantic roles only to capture the distribution of predicates and their associated roles. We show that memory-based learning can increase the recognition rate of implicit roles beyond the state-of-the-art."
R15-1074,Towards the Unsupervised Acquisition of Implicit Semantic Roles,2015,24,1,2,1,12426,niko schenk,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"This paper describes a novel approach to find evidence for implicit semantic roles. Our data-driven models generalize over large amounts of explicit annotations only, in order to acquire information about implicit roles. We establish a generic background knowledge base of probablistic predicate-role co-occurrences in an unsupervised manner, and estimate thresholds which trigger the prediction of a missing role. Our approach outperforms the stateof-the-art in terms of recognition rate and offers a more flexible alternative to rulebased solutions which rely on costly, language and domain-specific lexica."
K15-2006,A Minimalist Approach to Shallow Discourse Parsing and Implicit Relation Recognition,2015,27,5,1,1,2108,christian chiarcos,Proceedings of the Nineteenth Conference on Computational Natural Language Learning - Shared Task,0,"We describe a minimalist approach to shallow discourse parsing in the context of the CoNLL 2015 Shared Task. 1 Our parser integrates a rule-based component for argument identification and datadriven models for the classification of explicit and implicit relations. We place special emphasis on the evaluation of implicit sense labeling, we present different feature sets and show that (i) word embeddings are competitive with traditional word-level features, and (ii) that they can be used to considerably reduce the total number of features. Despite its simplicity, our parser is competitive with other systems in terms of sense recognition and thus provides a solid ground for further refinement."
W14-5302,Diachronic proximity vs. data sparsity in cross-lingual parser projection. A case study on Germanic,2014,25,3,2,1,21154,maria sukhareva,"Proceedings of the First Workshop on Applying {NLP} Tools to Similar Languages, Varieties and Dialects",0,"For the study of historical language varieties, the sparsity of training data imposes immense problems on syntactic annotation and the development of NLP tools that automatize the process. In this paper, we explore strategies to compensate the lack of training data by including data from related varieties in a series of annotation projection experiments from English to four old Germanic languages: On dependency syntax projected from English to one or multiple language(s), we train a fragment-aware parser trained and apply it to the target language. For parser training, we consider small datasets from the target language as a baseline, and compare it with models trained on larger datasets from multiple varieties with different degrees of relatedness, thereby balancing sparsity and diachronic proximity. Our experiments show (a) that including related language data to training data in the target language can improve parsing performance,"
W14-0604,New Technologies for Old Germanic. Resources and Research on Parallel Bibles in Older Continental Western Germanic,2014,27,0,1,1,2108,christian chiarcos,"Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities ({L}a{T}e{CH})",0,"We provide an overview of on-going efforts to facilitate the study of older Germanic languages currently pursued at the Goethe-University Frankfurt, Germany. We describe created resources, such as a parallel corpus of Germanic Bibles and a morphosyntactically annotated corpus of Old High German (OHG) and Old Saxon, a lexicon of OHG in XML and a multilingual etymological database. We discuss NLP algorithms operating on this data, and their relevance for research in the Humanities. RDF and Linked Data represent new and promising aspects in our research, currently applied to establish cross-references between etymological dictionaries, infer new information from their symmetric closure and to formalize linguistic annotations in a corpus and grammatical categories in a lexicon in an interoperable way."
chiarcos-2014-towards,Towards interoperable discourse annotation. Discourse features in the Ontologies of Linguistic Annotation,2014,41,5,1,1,2108,christian chiarcos,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper describes the extension of the Ontologies of Linguistic Annotation (OLiA) with respect to discourse features. The OLiA ontologies provide a a terminology repository that can be employed to facilitate the conceptual (semantic) interoperability of annotations of discourse phenomena as found in the most important corpora available to the community, including OntoNotes, the RST Discourse Treebank and the Penn Discourse Treebank. Along with selected schemes for information structure and coreference, discourse relations are discussed with special emphasis on the Penn Discourse Treebank and the RST Discourse Treebank. For an example contained in the intersection of both corpora, I show how ontologies can be employed to generalize over divergent annotation schemes."
W13-5501,Linguistic Linked Open Data ({LLOD}). Introduction and Overview,2013,13,2,1,1,2108,christian chiarcos,"Proceedings of the 2nd Workshop on Linked Data in Linguistics ({LDL}-2013): Representing and linking lexicons, terminologies and other language data",0,None
W13-5201,Linguistic Linked Open Data ({LLOD}) {--} Building the cloud,2013,7,2,1,1,2108,christian chiarcos,"Proceedings of the Joint Workshop on {NLP}{\\&}{LOD} and {SWAIE}: Semantic Web, Linked Open Data and Information Extraction",0,"The last decades have seen an immense maturation of Natural Language Processing (NLP) and an increased interest to apply NLP techniques and resources to real-world applications in business and academia. This process has certainly been facilitated by the increased availability of language data in the internet age, and the subsequent paradigm shift to statistical approaches, but also it coincided with an increasing acceptance of empirical approaches in linguistics and related academic fields, including empirical approaches to typology (Greenberg, 1963), corpus linguistics (Francis and Kucera, 1979, Brown Corpus), and (computational) lexicography (Kucera, 1969), as well as the dawn of Digital Humanities (Busa, 1974). Given the complexity of language and the analysis of linguistic data on different levels, its investigation involves a broad band-width of formalisms and resources used to analyze, process and generate natural language. With the transition to empirical, data-driven research, the primary challenge in the field is thus to store, connect and exploit the wealth of language data available in all its heterogeneity. Interoperability of language resources has hence been an important issue addressed by the community since the late 1980s (Text Encoding Initiative, 1990), but still remains a problem that is solved only partially, i.e., on the level of specific sub-types of linguistic resources, such as lexical resources (Francopoulo et al., 2006) or annotated corpora (Ide and Suderman, 2007), respectively. A closely related challenge is information integration, i.e., how information from different sources can be retrieved and combined in an efficient way. Recently, both challenges have been addressed by means of Linked Data principles (Chiarcos et al., 2013a,b), eventually leading to the formation of a Linguistic Linked Open Data (LLOD) cloud (Chiarcos et al., 2012b). The talk describes its current state of development, it presents selected examples for main types of linguistic resources in the LLOD cloud, and objectives leading to the adaptation of Linked Data principles for any of these. Further, the talk elaborates on history and goals behind this effort, its relation to established standardization initiatives in the field, and on-going community activities conducted under the umbrella of the Open Linguistics Working Group (OWLG) of the Open Knowledge Foundation (Chiarcos et al., 2012a), an initiative of experts from various fields concerned with linguistic data, which works towards"
P12-2042,Towards the Unsupervised Acquisition of Discourse Relations,2012,14,5,1,1,2108,christian chiarcos,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper describes a novel approach towards the empirical approximation of discourse relations between different utterances in texts. Following the idea that every pair of events comes with preferences regarding the range and frequency of discourse relations connecting both parts, the paper investigates whether these preferences are manifested in the distribution of relation words (that serve to signal these relations).n n Experiments on two large-scale English web corpora show that significant correlations between pairs of adjacent events and relation words exist, that they are reproducible on different data sets, and for three relation words, that their distribution corresponds to theory-based assumptions."
chiarcos-2012-ontologies,Ontologies of Linguistic Annotation: Survey and perspectives,2012,41,32,1,1,2108,christian chiarcos,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper announces the release of the Ontologies of Linguistic Annotation (OLiA). The OLiA ontologies represent a repository of annotation terminology for various linguistic phenomena on a great band-width of languages. This paper summarizes the results of five years of research, it describes recent developments and directions for further research."
chiarcos-etal-2012-open,The Open Linguistics Working Group,2012,34,10,1,1,2108,christian chiarcos,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes the Open Linguistics Working Group (OWLG) of the Open Knowledge Foundation (OKFN). The OWLG is an initiative concerned with linguistic data by scholars from diverse fields, including linguistics, NLP, and information science. The primary goal of the working group is to promote the idea of open linguistic resources, to develop means for their representation and to encourage the exchange of ideas across different disciplines. This paper summarizes the progress of the working group, goals that have been identified, problems that we are going to address, and recent activities and ongoing developments. Here, we put particular emphasis on the development of a Linked Open Data (sub-)cloud of linguistic resources that is currently being pursued by several OWLG members."
chiarcos-2012-generic,A generic formalism to represent linguistic corpora in {RDF} and {OWL}/{DL},2012,45,7,1,1,2108,christian chiarcos,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes POWLA, a generic formalism to represent linguistic corpora by means of RDF and OWL/DL. Unlike earlier approaches in this direction, POWLA is not tied to a specific selection of annotation layers, but rather, it is designed to support any kind of text-oriented annotation. POWLA inherits its generic character from the underlying data model PAULA (Dipper, 2005; Chiarcos et al., 2009) that is based on early sketches of the ISO TC37/SC4 Linguistic Annotation Framework (Ide and Romary, 2004). As opposed to existing standoff XML linearizations for such generic data models, it uses RDF as representation formalism and OWL/DL for validation. The paper discusses advantages of this approach, in particular with respect to interoperability and queriability, which are illustrated for the MASC corpus, an open multi-layer corpus of American English (Ide et al., 2008)."
W11-2805,Evaluating Salience Metrics for the Context-Adequate Realization of Discourse Referents,2011,43,4,1,1,2108,christian chiarcos,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"We describe the application of a framework for salience metrics and linguistic variability with respect to the contextually adequate choice of referring expressions and grammatical roles: Where multiple meaning-equivalent candidate realizations are available that differ in one of these aspects, NLG systems can apply salience metrics to predict contextually adequate realization preferences. We evaluate this claim and a number of parameters of salience metrics found in the theoretical literature on two German newspaper corpora.n n Key features of the approach described here include the application of a two-dimensional model of salience, how its theoretical predictions can be exploited to develop salience metrics for a particular phenomenon, and that these salience metrics can be subsequently applied to other phenomena. This approach can be applied to develop classifiers to predict packaging preferences for phenomena where little training data is available."
W11-0402,{OWL}/{DL} formalization of the {MULTEXT}-East morphosyntactic specifications,2011,27,14,1,1,2108,christian chiarcos,Proceedings of the 5th Linguistic Annotation Workshop,0,"This paper describes the modeling of the morphosyntactic annotations of the MULTEXT-East corpora and lexicons as an OWL/DL ontology. Formalizing annotation schemes in OWL/DL has the advantages of enabling formally specifying interrelationships between the various features and making logical inferences based on the relationships between them. We show that this approach provides us with a top-down perspective on a large set of morphosyntactic specifications for multiple languages, and that this perspective helps to identify and to resolve conceptual problems in the original specifications. Furthermore, the ontological modeling allows us to link the MULTEXT-East specifications with repositories of annotation terminology such as the General Ontology of Linguistics Descriptions or the ISO TC37/SC4 Data Category Registry."
W10-1825,Creating and Exploiting a Resource of Parallel Parses,2010,19,3,1,1,2108,christian chiarcos,Proceedings of the Fourth Linguistic Annotation Workshop,0,"This paper describes the creation of a resource of German sentences with multiple automatically created alternative syntactic analyses (parses) for the same text, and how qualitative and quantitative investigations of this resource can be performed using ANNIS, a tool for corpus querying and visualization. Using the example of PP attachment, we show how parsing can benefit from the use of such a resource."
P10-1068,Towards Robust Multi-Tool Tagging. An {OWL}/{DL}-Based Approach,2010,62,9,1,1,2108,christian chiarcos,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes a series of experiments to test the hypothesis that the parallel application of multiple NLP tools and the integration of their results improves the correctness and robustness of the resulting analysis.n n It is shown how annotations created by seven NLP tools are mapped onto tool-independent descriptions that are defined with reference to an ontology of linguistic annotations, and how a majority vote and ontological consistency constraints can be used to integrate multiple alternative analyses of the same token in a consistent way.n n For morphosyntactic (parts of speech) and morphological annotations of three German corpora, the resulting merged sets of ontological descriptions are evaluated in comparison to (ontological representation of) existing reference annotations."
W09-3005,By all these lovely tokens... Merging Conflicting Tokenizations,2009,35,18,1,1,2108,christian chiarcos,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,"Given the contemporary trend to modular NLP architectures and multiple annotation frameworks, the existence of concurrent tokenizations of the same text represents a pervasive problem in everyday's NLP practice and poses a non-trivial theoretical problem to the integration of linguistic annotations and their interpretability in general. This paper describes a solution for integrating different tokenizations using a standoff XML format, and discusses the consequences for the handling of queries on annotated corpora."
W09-0703,Information Structure in {A}frican {L}anguages: Corpora and Tools,2009,30,4,1,1,2108,christian chiarcos,Proceedings of the First Workshop on Language Technologies for {A}frican Languages,0,"In this paper, we describe tools and resources for the study of African languages developed at the Collaborative Research Centre Information Structure. These include deeply annotated data collections of 25 subsaharan languages that are described together with their annotation scheme, and further, the corpus tool ANNIS that provides a unified access to a broad variety of annotations created with a range of different tools. With the application of ANNIS to several African data collections, we illustrate its suitability for the purpose of language documentation, distributed access and the creation of data archives."
rehm-etal-2008-ontology,Ontology-Based {XQ}uery{'}ing of {XML}-Encoded Language Resources on Multiple Annotation Layers,2008,20,18,3,0.183455,60,georg rehm,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We present an approach for querying collections of heterogeneous linguistic corpora that are annotated on multiple layers using arbitrary XML-based markup languages. An OWL ontology provides a homogenising view on the conceptually different markup languages so that a common querying framework can be established using the method of ontology-based query expansion. In addition, we present a highly flexible web-based graphical interface that can be used to query corpora with regard to several different linguistic properties such as, for example, syntactic tree fragments. This interface can also be used for ontology-based querying of multiple corpora simultaneously."
buyko-etal-2008-ontology,Ontology-Based Interface Specifications for a {NLP} Pipeline Architecture,2008,20,14,2,0,43183,ekaterina buyko,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The high level of heterogeneity between linguistic annotations usually complicates the interoperability of processing modules within an NLP pipeline. In this paper, a framework for the interoperation of NLP components, based on a data-driven architecture, is presented. Here, ontologies of linguistic annotation are employed to provide a conceptual basis for the tagset-neutral processing of linguistic annotations. The framework proposed here is based on a set of structured OWL ontologies: a reference ontology, a set of annotation models which formalize different annotation schemes, and a declarative linking between these, specified separately. This modular architecture is particularly scalable and flexible as it allows for the integration of different reference ontologies of linguistic annotations in order to overcome the absence of a consensus for an ontology of linguistic terminology. Our proposal originates from three lines of research from different fields: research on annotation type systems in UIMA; the ontological architecture OLiA, originally developed for sustainable documentation and annotation-independent corpus browsing, and the ontologies of the OntoTag model, targeted towards the processing of linguistic annotations in Semantic Web applications. We describe how UIMA annotations can be backed up by ontological specifications of annotation schemes as in the OLiA model, and how these are linked to the OntoTag ontologies, which allow for further ontological processing."
rehm-etal-2008-metadata,The Metadata-Database of a Next Generation Sustainability Web-Platform for Language Resources,2008,17,7,5,0.183455,60,georg rehm,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Our goal is to provide a web-based platform for the long-term preservation and distribution of a heterogeneous collection of linguistic resources. We discuss the corpus preprocessing and normalisation phase that results in sets of multi-rooted trees. At the same time we transform the original metadata records, just like the corpora annotated using different annotation approaches and exhibiting different levels of granularity, into the all-encompassing and highly flexible format eTEI for which we present editing and parsing tools. We also discuss the architecture of the sustainability platform. Its primary components are an XML database that contains corpus and metadata files and an SQL database that contains user accounts and access control lists. A staging area, whose structure, contents, and consistency can be checked using tools, is used to make sure that new resources about to be imported into the platform have the correct structure."
W07-1525,{P}o{C}o{S} - {P}otsdam {C}oreference {S}cheme,2007,13,22,2,0,48965,olga krasavina,Proceedings of the Linguistic Annotation Workshop,0,"This document outlines minimal design principles underlying annotation of coreference relations in PoCoS, a scheme for cross-linguistic anaphoric annotation. We identify language-independent principles for markable identification which are essential for comparability of annotations produced for different languages. We further suggest a clear and motivated structure of annotation stages, the separation of a coarse-grained core and a family of more elaborate extended schemes, and strategies for the systematic treatment of ambiguity. Explicit mark-up of ambiguities is a novel feature. We implemented three instantiations of PoCoS for German, English and Russian applied to corpora of newspaper texts."
