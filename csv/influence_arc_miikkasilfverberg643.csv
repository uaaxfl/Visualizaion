2020.coling-main.255,P17-1183,0,0.31789,"For example, given the input pair {run, V.PTCP;PRS}, a successful system should produce the present participial form: running. The shared tasks have illuminated several idiosyncrasies of inflection. Although the state-of-the-art methods have been inspired by Neural Machine Translation (NMT), inflection is, in many ways, a more straightforward task. Often, a majority of characters can be copied directly from input to output, and reordering of tokens is not necessary to the extent that it is in translation. Thus, systems with copymechanisms (Makarov et al., 2017), and hard, monotonic attention (Aharoni and Goldberg, 2017) tend to perform well, even when data is scarce. Recurrent neural architectures require that the output from a previous time step be fed back into the model. During training, existing systems all use a variant of teacher forcing, which feeds gold-standard tokens into the model at time t + 1. This methodology differs significantly from how the model progresses at test-time, where silver tokens must be used. This problem has been dubbed exposure bias by Wiseman and Rush (2016). We hypothesize that exposure bias can lead models to overfit the training data, particularly in low-resource settings."
2020.coling-main.255,K17-2001,0,0.0534173,"Missing"
2020.coling-main.255,K18-3001,1,0.942093,"different training and test conditions, exposure bias increases the likelihood that a system too closely models its training data. Experiments show that teacher-forced models struggle to recover when they enter unknown territory. However, a simple modification to the training algorithm to more closely mimic test conditions creates models that are better able to generalize to unseen environments. 1 Introduction Morphological inflection has gained substantial interest in recent years due to a number of shared tasks focusing on morphology learning (Cotterell et al., 2016; Cotterell et al., 2017; Cotterell et al., 2018; McCarthy et al., 2019; Vylomova et al., 2020; Kann et al., 2020). Systems for inflection are trained on a sequence of pairs: {Lemma, MorphoSyntactic Descriptor (MSD)}, free of context, and must produce as output an inflected word form. For example, given the input pair {run, V.PTCP;PRS}, a successful system should produce the present participial form: running. The shared tasks have illuminated several idiosyncrasies of inflection. Although the state-of-the-art methods have been inspired by Neural Machine Translation (NMT), inflection is, in many ways, a more straightforward task. Often, a ma"
2020.coling-main.255,P17-2058,0,0.0204612,"is continuously increased during training which they call scheduling. Our work can be seen as an application of this approach except that we make the choice between gold standard context and the model suggestion at the example-level instead of the level of individual characters. According to our preliminary experiments, this delivered superior results in the task of inflection. Another difference is that we fix the probability for choosing the model suggestion throughout training and treat this as a hyperparameter. We do this because we did not observe consistent improvements from scheduling. Goyal et al. (2017) present a modification of the approach by Bengio et al. (2015) for named-entity recognition and machine translation. They replace the argmax model prediction with an average of all output embeddings, weighted by the prediction scores. Also related to scheduled sampling is the approach by Wiseman and Rush (2016) who experiment with beam search. The downside of this approach is that it can substantially increase training times. 4 Data and Experiments In this section, we lay out our experiment schedule, and describe the system architectures, hyperparameters, and data specifications. We also moti"
2020.coling-main.255,2020.sigmorphon-1.3,1,0.729929,"kelihood that a system too closely models its training data. Experiments show that teacher-forced models struggle to recover when they enter unknown territory. However, a simple modification to the training algorithm to more closely mimic test conditions creates models that are better able to generalize to unseen environments. 1 Introduction Morphological inflection has gained substantial interest in recent years due to a number of shared tasks focusing on morphology learning (Cotterell et al., 2016; Cotterell et al., 2017; Cotterell et al., 2018; McCarthy et al., 2019; Vylomova et al., 2020; Kann et al., 2020). Systems for inflection are trained on a sequence of pairs: {Lemma, MorphoSyntactic Descriptor (MSD)}, free of context, and must produce as output an inflected word form. For example, given the input pair {run, V.PTCP;PRS}, a successful system should produce the present participial form: running. The shared tasks have illuminated several idiosyncrasies of inflection. Although the state-of-the-art methods have been inspired by Neural Machine Translation (NMT), inflection is, in many ways, a more straightforward task. Often, a majority of characters can be copied directly from input to output,"
2020.coling-main.255,P17-4012,0,0.0358293,"l of “dog”, accuracy will assign the same score to a system that produces “doggs“, and one that produces “elephants”. While we do not claim that inflectional models make mistakes of this magnitude, a high edit-distance paired with a high accuracy may suggest significant mistakes are being made in incorrect predictions. Furthermore, student forcing is applicable to other learning tasks where all-or-nothing evaluations are less frequent - BLEU score, for example, has much more in common with edit distance than accuracy. Our pointer-generator network uses the PyTorch implementation from OpenNMT (Klein et al., 2017). They were trained for 10,000 steps, except for the “high” setting, which was trained for 20,000, upon observation that 10,000 steps was not enough for convergence. Model checkpoints were saved every 500 steps, and the model with the highest development accuracy was used for inference. Under the low setting, the RNN hidden dimension was set to 50, while in the medium and high settings, the RNN size was set to 200. All models were run on 10 different random seeds, using general copy attention. All other system parameters were set to the OpenNMT defaults. Our HA model was trained using the best"
2020.coling-main.255,D18-1314,0,0.0740245,"ral inflection models: the Pointer-Generator network (PG), a soft attention model originally introduced for document summarization (See et al., 2017) and subsequently applied to inflection by Sharma et al. (2018), a neural transducer utilizing hard attention trained on aligned lemmata and inflected word forms (Aharoni and Goldberg, 2017; Makarov et al., 2017) (HA) and modified with an explicit copy mechanism, and a later development of the HA system which applies minimum risk training in order to avoid relying on an explicit aligner for the lemma and inflected word form during training (MRT) (Makarov and Clematide, 2018a). 2.1 Student forcing Teacher and student forcing are illustrated in Figure 1. On the left, the teacher-forced system feeds the gold character from time t into the decoder to predict the character at time t + 1. The student forcing system, on the right, instead feeds the predicted token from the model. Both systems in this toy example make a mistake in their predictions, but must learn different correction strategies – the teacher-forced system must learn to “get back on track” after an error, while the student-forced system must only be conditioned on its previous prediction, regardless of"
2020.coling-main.255,C18-1008,0,0.0331679,"ral inflection models: the Pointer-Generator network (PG), a soft attention model originally introduced for document summarization (See et al., 2017) and subsequently applied to inflection by Sharma et al. (2018), a neural transducer utilizing hard attention trained on aligned lemmata and inflected word forms (Aharoni and Goldberg, 2017; Makarov et al., 2017) (HA) and modified with an explicit copy mechanism, and a later development of the HA system which applies minimum risk training in order to avoid relying on an explicit aligner for the lemma and inflected word form during training (MRT) (Makarov and Clematide, 2018a). 2.1 Student forcing Teacher and student forcing are illustrated in Figure 1. On the left, the teacher-forced system feeds the gold character from time t into the decoder to predict the character at time t + 1. The student forcing system, on the right, instead feeds the predicted token from the model. Both systems in this toy example make a mistake in their predictions, but must learn different correction strategies – the teacher-forced system must learn to “get back on track” after an error, while the student-forced system must only be conditioned on its previous prediction, regardless of"
2020.coling-main.255,W19-4226,1,0.844022,"est conditions, exposure bias increases the likelihood that a system too closely models its training data. Experiments show that teacher-forced models struggle to recover when they enter unknown territory. However, a simple modification to the training algorithm to more closely mimic test conditions creates models that are better able to generalize to unseen environments. 1 Introduction Morphological inflection has gained substantial interest in recent years due to a number of shared tasks focusing on morphology learning (Cotterell et al., 2016; Cotterell et al., 2017; Cotterell et al., 2018; McCarthy et al., 2019; Vylomova et al., 2020; Kann et al., 2020). Systems for inflection are trained on a sequence of pairs: {Lemma, MorphoSyntactic Descriptor (MSD)}, free of context, and must produce as output an inflected word form. For example, given the input pair {run, V.PTCP;PRS}, a successful system should produce the present participial form: running. The shared tasks have illuminated several idiosyncrasies of inflection. Although the state-of-the-art methods have been inspired by Neural Machine Translation (NMT), inflection is, in many ways, a more straightforward task. Often, a majority of characters ca"
2020.coling-main.255,P19-2049,0,0.0278694,"Missing"
2020.coling-main.255,P17-1099,0,0.076512,"Missing"
2020.coling-main.255,D16-1137,0,0.364547,"that it is in translation. Thus, systems with copymechanisms (Makarov et al., 2017), and hard, monotonic attention (Aharoni and Goldberg, 2017) tend to perform well, even when data is scarce. Recurrent neural architectures require that the output from a previous time step be fed back into the model. During training, existing systems all use a variant of teacher forcing, which feeds gold-standard tokens into the model at time t + 1. This methodology differs significantly from how the model progresses at test-time, where silver tokens must be used. This problem has been dubbed exposure bias by Wiseman and Rush (2016). We hypothesize that exposure bias can lead models to overfit the training data, particularly in low-resource settings. In this paper, we compare teacher forcing to silver label propagation, or student forcing, where model predictions are fed into the decoder during training instead of gold standard labels. Our experiments on a geographically and linguistically diverse set of 10 languages show that student forcing typically outperforms teacher forcing in a low-resource setting. Our analysis of the results suggests that teacher forced models are, indeed, overfitting the training data by ignori"
2020.lrec-1.433,E09-2008,0,0.0177649,"uwenberg, 2011). 4 Forms with explicitly spelled long vowels, as ba-nu-u2 for banˆu ’to create’ are called plene-writing. 5 The spelling variants have been collected from Oracc. 3529 nadin ’it is being given’. The correct reading is only interpretable by the syntactic context of the word. 2.3. BabyFST BabyFST (Sahala et al., 2019 submitted) is the most comprehensive finite-state based morphological analyzer for Akkadian up-to-date. Its source code is written according to the Xerox finite-state syntax with the lexc and xfscript formalisms, and can be compiled using the Foma FiniteState tookit (Hulden, 2009) or HFST - the Helsinki Finite State Transducer toolkit (Lind´en et al., 2011). BabyFST is designed primarily for the Babylonian dialect and is capable of processing phonological words. Several Akkadian text corpora contain only the transliteration, so some robust support is needed for producing phonologically transcribed text for BabyFST to analyse. 3. Related Work To the best of our knowledge, this is the first system for automatically transcribing Akkadian cuneiform text but Smith (2007) presents a system for grapheme-to-phoneme transcription for the Elamite language, another extinct langua"
2020.lrec-1.433,W17-0504,0,0.0229124,"models like Rao et al. (2015) directly model the relation between the input grapheme sequence and output phoneme sequence which does not require string alignment. Unlike many grapheme-to-phoneme transcription settings, the transcription of cuneiform Akkadian text differs in the sense that the transcription of logograms is partly dependent on sentence context as explained in Section 2.2.. This suggests that it is beneficial to model the sentential context of the transliterated input token. In this respect, our task is related to historical text normalization where context can often be helpful. Korchagina (2017) investigates neural machine translation for historical text normalization. In contrast to token-based normalization methods applied in grapheme-to-phoneme transcription, the model here is a character-level encoder-decoder which is applied on complete sentences. This approach proved to be challenging in the context of Akkadian transcription based on our preliminary experiments. The character-based NMT model would overfit the training data possibly because of our rela6 tively small dataset.7 Instead of a full fledged NMT model, we present experiments on several more restricted ways to model the"
2020.lrec-1.433,W12-6208,0,0.0127373,". 3. Related Work To the best of our knowledge, this is the first system for automatically transcribing Akkadian cuneiform text but Smith (2007) presents a system for grapheme-to-phoneme transcription for the Elamite language, another extinct language using the cuneiform script. The system uses optimality constraints and applies the gradual learning algorithm for learning a ranking between the constraints. Many grapheme-to-phoneme transcription systems formulate the task as a probabilistic sequence model on grapheme-phoneme pairs. For example, Bisani and Ney (2008) use joint n-gram models and Novak et al. (2012) use weighted finite-state transducers. These methods rely on some alignment between the grapheme and phoneme strings because they gather statistics about symbol alignments.6 Because our strings contain logographic material which may not have a clear connection to the phonological representation, alignment is challenging. Therefore we opted for using models based on the neural encoderdecoder architecture. These models like Rao et al. (2015) directly model the relation between the input grapheme sequence and output phoneme sequence which does not require string alignment. Unlike many grapheme-t"
2020.lrec-1.433,W07-1308,0,0.0391319,"te syntax with the lexc and xfscript formalisms, and can be compiled using the Foma FiniteState tookit (Hulden, 2009) or HFST - the Helsinki Finite State Transducer toolkit (Lind´en et al., 2011). BabyFST is designed primarily for the Babylonian dialect and is capable of processing phonological words. Several Akkadian text corpora contain only the transliteration, so some robust support is needed for producing phonologically transcribed text for BabyFST to analyse. 3. Related Work To the best of our knowledge, this is the first system for automatically transcribing Akkadian cuneiform text but Smith (2007) presents a system for grapheme-to-phoneme transcription for the Elamite language, another extinct language using the cuneiform script. The system uses optimality constraints and applies the gradual learning algorithm for learning a ranking between the constraints. Many grapheme-to-phoneme transcription systems formulate the task as a probabilistic sequence model on grapheme-phoneme pairs. For example, Bisani and Ney (2008) use joint n-gram models and Novak et al. (2012) use weighted finite-state transducers. These methods rely on some alignment between the grapheme and phoneme strings because"
2020.lrec-1.479,E09-2008,0,0.201819,"Missing"
2020.lrec-1.479,C88-1064,0,0.639231,"83k as different stages of Babylonian. In total, 1.42M Akkadian and 614k Babylonian tokens have been lemmatized and POS-tagged.4 For Neo-Assyrian, the most notable collection of texts is the State Archives of Assyria online (504k tokens), initiated already in 1986 by Simo Parpola in Helsinki and later lemmatized and added to Oracc by Karen Radner and her team. Currently, none of the afore-mentioned corpora contain a morphological analysis of Akkadian beyond lemmatization and POS-tagging. 3. Description of the FST-based Computational Model of Babylonian 3.1 Previous and other relevant attempts Kataja and Koskenniemi (1988) created the first computational description of the Akkadian morphology using the two-level formalism. They handled interdigitation of verbs by intersecting two regular lexicons, of which one described the root and its affixation, and the other the pattern formalisms. As the intersection approach was highly overgenerating, Kataja and Koskenniemi experimented with constraining the morpheme combinatorics by using unification-based features. This work was, however, stated to be still in progress when their paper was published. Bamman and Andersson5 (2012) is a finite-state description of Old Assy"
2020.lrec-1.479,W02-0501,0,0.11783,"Missing"
2020.lrec-1.483,P19-1310,0,0.0958144,"Missing"
2020.lrec-1.483,C12-2009,1,0.843263,"Missing"
2020.lrec-1.483,P19-1156,0,0.050149,"Missing"
2020.lrec-1.483,P16-1156,1,0.881371,"Missing"
2020.lrec-1.483,K17-2001,1,0.904106,"Missing"
2020.lrec-1.483,N07-1048,0,0.0363457,"Missing"
2020.lrec-1.483,P08-1115,0,0.0323664,"Missing"
2020.lrec-1.483,K19-1014,1,0.901227,"Missing"
2020.lrec-1.483,N12-1032,0,0.0791982,"Missing"
2020.lrec-1.483,D19-1328,0,0.0253837,"Missing"
2020.lrec-1.483,S19-1026,1,0.819906,"Missing"
2020.lrec-1.483,L16-1498,1,0.928068,"Missing"
2020.lrec-1.483,L18-1293,1,0.890225,"Missing"
2020.lrec-1.483,W18-6011,1,0.883111,"Missing"
2020.lrec-1.483,W19-4226,1,0.885665,"Missing"
2020.lrec-1.483,D14-1095,0,0.0393603,"Missing"
2020.lrec-1.483,2020.lrec-1.488,1,0.822461,"Missing"
2020.lrec-1.483,L16-1262,0,0.126329,"Missing"
2020.lrec-1.483,Q15-1026,0,0.0701072,"Missing"
2020.lrec-1.483,W18-1813,1,0.887977,"Missing"
2020.lrec-1.483,P15-2111,1,0.855194,"Missing"
2020.lrec-1.483,A94-1008,0,0.317255,"Missing"
2020.lrec-1.483,N01-1026,1,0.569711,"Missing"
2020.sigmorphon-1.1,K18-3001,1,0.899071,"logical reinflection, we specifically focus on typological diversity and aim to investigate systems’ ability to generalize across typologically distinct languages many of which are low-resource. For example, if a neural network architecture works well for a sample of IndoEuropean languages, should the same architecture also work well for Tupi–Guarani languages (where nouns are “declined” for tense) or Austronesian languages (where verbal morphology is frequently prefixing)? 2 Task Description The 2020 iteration of our task is similar to CoNLL-SIGMORPHON 2017 (Cotterell et al., 2017) and 2018 (Cotterell et al., 2018) in that participants are required to design a model that learns to generate inflected forms from a lemma and a set of morphosyntactic features that derive the desired target form. For each language we provide a separate training, development, and test set. More historically, all of these tasks resemble the classic “wug”-test that Berko (1958) developed to test child and human knowledge of English nominal morphology. Unlike the task from earlier years, this year’s task proceeds in three phases: a Development Phase, a Generalization Phase, and an Evaluation Phase, in which each phase introduces"
2020.sigmorphon-1.1,K17-2001,1,0.928876,"e SIGMORPHON shared task on morphological reinflection, we specifically focus on typological diversity and aim to investigate systems’ ability to generalize across typologically distinct languages many of which are low-resource. For example, if a neural network architecture works well for a sample of IndoEuropean languages, should the same architecture also work well for Tupi–Guarani languages (where nouns are “declined” for tense) or Austronesian languages (where verbal morphology is frequently prefixing)? 2 Task Description The 2020 iteration of our task is similar to CoNLL-SIGMORPHON 2017 (Cotterell et al., 2017) and 2018 (Cotterell et al., 2018) in that participants are required to design a model that learns to generate inflected forms from a lemma and a set of morphosyntactic features that derive the desired target form. For each language we provide a separate training, development, and test set. More historically, all of these tasks resemble the classic “wug”-test that Berko (1958) developed to test child and human knowledge of English nominal morphology. Unlike the task from earlier years, this year’s task proceeds in three phases: a Development Phase, a Generalization Phase, and an Evaluation Pha"
2020.sigmorphon-1.1,W09-0106,0,0.0385713,"n variably surface as prefixes, suffixes, infixes, or circumfixes (Dryer, 2013). Most Eurasian and Australian languages strongly favor suffixation, and the same holds true, but to a lesser extent, for South American and New Guinean languages (Dryer, 2013). In Mesoamerican languages and African languages spoken below the Sahara, prefixation is dominant instead. These are just three dimensions of variation in morphology, and the cross-linguistic variation is already considerable. Such cross-lingual variation makes the development of natural language processing (NLP) applications challenging. As Bender (2009, 2016) notes, many current architectures and training and tuning algorithms still present language-specific biases. The most commonly used language for developing NLP applications is English. Along the above dimensions, English is productively concatenative, a mixture of analytic and synthetic, and largely suffixing in its inflectional morphology. With respect to languages that exhibit inflectional morphology, English is relatively impoverished.1 Importantly, English is just one morphological system among many. A larger goal of natural language processing is that the system work for any prese"
2020.sigmorphon-1.1,2020.lrec-1.344,1,0.878264,"Missing"
2020.sigmorphon-1.1,2020.sigmorphon-1.15,0,0.0565092,"Missing"
2020.sigmorphon-1.1,2020.sigmorphon-1.14,0,0.0439232,"Missing"
2020.sigmorphon-1.1,L16-1379,0,0.0190826,"Missing"
2020.sigmorphon-1.1,K17-2010,1,0.837279,"l baselines were based on a neural transducer (Wu and Cotterell, 2019), which is essentially a hard monotonic attention model (mono-*). The second baseline is a transformer (Vaswani et al., 2017) adopted for character-level tasks that currently holds the state-of-the-art on the 2017 SIGMORPHON shared task data (Wu et al., 2020, trm-*). Both models take the lemma and morphological tags as input and output the target inflection. The baseline is further expanded to include the data augmentation technique used by Anastasopoulos and Neubig (2019, -aug-) (conceptually similar to the one proposed by Silfverberg et al. (2017)). Relying on a simple characterlevel alignment between lemma and form, this technique replaces shared substrings of length &gt; 3 with random characters from the language’s alphabet, producing hallucinated lemma–tag–form triples. Both neural baselines were trained in mono- (*-single) and multilingual (shared parameters among the same family, *-shared) settings. 6 Many teams based their models on the transformer architecture. NYU-CUBoulder experimented with a vanilla transformer model (NYU-CUBoulder-04-0), a pointer-generator transformer that allows for a copy mechanism (NYU-CUBoulder-02-0), and"
2020.sigmorphon-1.1,2020.sigmorphon-1.4,0,0.0612223,"Missing"
2020.sigmorphon-1.1,W19-4207,0,0.0125147,"c attention model with improved alignment strategy. This model is further improved (flexica-03-1) by introducing a data hallucination technique which is based on phonotactic modelling of extremely low-resource languages (Shcherbakov et al., 2016). LTI focused on their earlier model (Anastasopoulos and Neubig, 2019), a neural multi-source encoder–decoder with two-step attention architecture, training it with hallucinated data, cross-lingual transfer, and romanization of scripts to improve performance on low-resource languages. DeepSpin reimplemented gated sparse two-headed attention model from Peters and Martins (2019) and trained it on all languages at once (massively multilingual). The team experimented with two modifications of the softmax function: sparsemax (Martins and Astudillo, 2016, deepspin-02-1) and 1.5-entmax (Peters et al., 2019, deepspin-01-1). Neural Neural baselines were based on a neural transducer (Wu and Cotterell, 2019), which is essentially a hard monotonic attention model (mono-*). The second baseline is a transformer (Vaswani et al., 2017) adopted for character-level tasks that currently holds the state-of-the-art on the 2017 SIGMORPHON shared task data (Wu et al., 2020, trm-*). Both"
2020.sigmorphon-1.1,P19-1146,0,0.0351308,"Missing"
2020.sigmorphon-1.1,P19-1148,1,0.838088,"lti-source encoder–decoder with two-step attention architecture, training it with hallucinated data, cross-lingual transfer, and romanization of scripts to improve performance on low-resource languages. DeepSpin reimplemented gated sparse two-headed attention model from Peters and Martins (2019) and trained it on all languages at once (massively multilingual). The team experimented with two modifications of the softmax function: sparsemax (Martins and Astudillo, 2016, deepspin-02-1) and 1.5-entmax (Peters et al., 2019, deepspin-01-1). Neural Neural baselines were based on a neural transducer (Wu and Cotterell, 2019), which is essentially a hard monotonic attention model (mono-*). The second baseline is a transformer (Vaswani et al., 2017) adopted for character-level tasks that currently holds the state-of-the-art on the 2017 SIGMORPHON shared task data (Wu et al., 2020, trm-*). Both models take the lemma and morphological tags as input and output the target inflection. The baseline is further expanded to include the data augmentation technique used by Anastasopoulos and Neubig (2019, -aug-) (conceptually similar to the one proposed by Silfverberg et al. (2017)). Relying on a simple characterlevel alignme"
2020.sigmorphon-1.1,2020.sigmorphon-1.5,0,0.0487999,"Missing"
2020.sigmorphon-1.16,P16-1038,0,0.0233822,"gre hin hun ice jpn kor lit rum vie 33.56 24.00 41.33 30.89 34.89 25.33 24.00 22.67 20.89 30.22 11.78 30.67 26.00 20.00 32.00 9.31 5.65 12.07 7.73 12.69 5.19 5.13 6.76 5.30 11.12 3.73 9.17 7.75 5.52 13.75 avg 27.22 8.06 7 Conclusion Table 9: Development set results for monolingual models. Multilingual training is a crucial component in our system. Our approach is closely related to multilingual neural machine translation (Johnson et al., 2017b), where a single model is trained to translate between multiple source and target languages. Others have also explored multilingual approaches to G2P. Deri and Knight (2016) use multilingual G2P conversion for the purpose of adapting models from high-resource languages to train weighted finite-state transducers for related low-resource languages. Ni et al. (2018) experiment with multilingual training for deep learning models. They use pretrained character embeddings with LSTM encoder-decoders in order to train multilingual G2P models for Chinese, Japanese, Korean and Thai. In contrast to Ni et al. (2018), we inspect multilingual training in the context of transformer models. For our second model, whose training data is augmented from Wikipedia, we use a selftaini"
2020.sigmorphon-1.16,P15-1166,0,0.0363059,"the shared task training data for each of the 15 individual languages. To ease this bottleneck, we decided to view the task through a multilingual machine translation lens where we build a single model mapping from input to output across all the languages simultaneously. In this, we hypothesized that a multilingual model would allow for shared representations across the various languages that may be more powerful than individual representations of monolingual models. Abundant evidence now exists for approaching machine translation tasks from a multilingual perspective (Johnson et al., 2017a; Dong et al., 2015; Firat et al., 2016), which inspired our choice. In order to make use of unlabeled data, we also explore a straightforward self-training approach. In particular, we employ our trained models to convert sequences of multilingual unlabeled graphemes, taken from Wikipedia data, into multilingual phonemes. We then select sequences of phonemes predicted with our models above a certain confidence threshold to augment the shared task training data, thus re-training our models with larger (gold and silver) training data from scratch. Our models are based on the Transformer architecture which exploits"
2020.sigmorphon-1.16,N16-1101,0,0.020224,"aining data for each of the 15 individual languages. To ease this bottleneck, we decided to view the task through a multilingual machine translation lens where we build a single model mapping from input to output across all the languages simultaneously. In this, we hypothesized that a multilingual model would allow for shared representations across the various languages that may be more powerful than individual representations of monolingual models. Abundant evidence now exists for approaching machine translation tasks from a multilingual perspective (Johnson et al., 2017a; Dong et al., 2015; Firat et al., 2016), which inspired our choice. In order to make use of unlabeled data, we also explore a straightforward self-training approach. In particular, we employ our trained models to convert sequences of multilingual unlabeled graphemes, taken from Wikipedia data, into multilingual phonemes. We then select sequences of phonemes predicted with our models above a certain confidence threshold to augment the shared task training data, thus re-training our models with larger (gold and silver) training data from scratch. Our models are based on the Transformer architecture which exploits effective self-atten"
2020.sigmorphon-1.16,2020.sigmorphon-1.2,0,0.132547,"me (G2P) conversion is an important component of both speech recognition and synthesis. In G2P conversion, sequences of graphemes (the symbols used to write words) are mapped to corresponding phonemes (pronunciation symbols, e.g., symbols of the International Phonetic Alphabet). Members of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON) have proposed a G2P shared task (SIGMORPHON 2020 Shared Task 1) 1 involving multiple languages. In this paper, we describe our submissions to the shared task. Organizers provide an overview of the task and submitted systems in Gorman et al. (2020) (this volume). 1 The shared task webpage is accessible at: https: //sigmorphon.github.io/sharedtasks/2020/task1. The task was introduced with data from 10 languages, with an additional 5 ‘surprise’ languages released during the task timeline. Our goal was to develop an effective system based on modern deep learning methods as a solution. However, deep learning technologies work best with sufficiently large training data. Hence, a clear challenge we came across is the limited size of the shared task training data for each of the 15 individual languages. To ease this bottleneck, we decided to v"
2020.sigmorphon-1.16,P17-4012,0,0.0398098,"ww.opengrm.org/twiki/bin/view/GRM. https://github.com/pytorch/fairseq. gual models, then we introduce our semisupervised models (also multilingual). Our semi-supervised models follow a self-training set up. We now explain each of these models. 3.1 Supervised, Multilingual Models We use a multilingual approach where we train a single model on data from all 15 languages. For this purpose, we prepend a token comprising a language code (e.g. fre) to each grapheme sequence source. For our implementation, we use the PyTorch Transformer architecture in the OpenNMT Neural Machine Translation Toolkit (Klein et al., 2017). We set the model hyper-parameters as shown in Table 3, which follows those adopted by Vaswani et al. (2017). Hyper-Parameter Number of layers Hidden state size Word embedding size Hidden feed-forward size Number of self-attention heads Optimizer Dropout probability Number of training steps Table 3: parameters. Multilingual Value 6 512 512 2,048 8 Adam 0.1 200K Transformer 3.2.1 Language arm bul dut fre geo gre hin hun ice kor lit rum Total hyperWe train the model with 3 different random seeds, and at inference we employ an ensemble consisting of the models from 4 training checkpoints (at 50k"
2020.sigmorphon-1.16,2020.lrec-1.521,0,0.459033,"baselines. The first is a pair n-gram model encoded as a weighted finitestate transducer (FST), implemented using the OpenGRMtoolkit 4 . The second is a biLSTM encoder-decoder sequence model implemented using the Fairseq toolkit 5 . The third is a Transformer model also implemented using the Fairseq toolkit. Organizer-provided shared task baselines are shown in Table 2 as WER and PER averages over the 15 languages. We now introduce our models. Task Data, Evaluation, and Baselines The data provided by the organizers of the shared task are extracted from Wiktionary 2 using the WikiPron library (Lee et al., 2020), and consist of 4,050 gold labeled graphemephoneme pairs for each of 15 languages, split into a training set (3,600 per language) and a development set (450 per language). The blind test data comprise 450 sources for each language. The data involves languages in the set {Adyghe (ady), Armenian (arm), Bulgarian (bul), Dutch (dut), French (fre), Georgian (geo), Modern Greek (gre), Hindi (hin), Hungarian (hun), Icelandic (ice), Japanese hiragana (jpn), Korean (kor), Lithuanian (lit), Romanian (rum), Vietnamese (vie)}. 3 This set of languages employ a variety of writing systems: alphabets (e.g. F"
2021.computel-1.1,N19-1388,0,0.0226523,"e and target set. Training Settings Preliminary experiments showed that multilingual systems trained on a single target corpus, i.e. the English Bible in our case, have a tendency to completely disregard the source sentence during test time and instead generate an unrelated English sentence as output. We dub this target overfitting. To counter this tendency, we employ four specialized training strategies: (1) Single Source translation (1Src) limits the number of training source texts to one even when we have multiple Bible translations in the same language 7 . (2) Heterogeneous batching (HB) (Aharoni et al., 2019) constructs minibatches by uniformly sampling sentences from the entire training data into each minibatch. In contrast, the common practice is to construct minibatches from training examples with similar length.8 (3) We increase the amount of English target data available to the model by adding monolingual English training examples where the source and target sentence are identical (E2E).9 (4) Finally, following Aharoni et al. (2019) we transform our many-to-English models into many-to-many models (M2M) by reversing the source and target language of our Bibles and combining the resulting data"
2021.computel-1.1,mayer-cysouw-2014-creating,0,0.0685248,"Missing"
2021.computel-1.1,2020.lrec-1.352,1,0.839547,"Missing"
2021.computel-1.1,2020.lrec-1.458,1,0.721072,"d low-resource languages. be less than beneficial for languages with high numbers of morphemes in each word. When we reduce the BPE vocabulary, we see a large increase in translation quality for all monolingual experiments, as the system sees many more short sequences. Unfortunately, we fail to leverage the increase in data as we add more languages from the same family, with the Algic (Cree) and Athabaskan (Navajo) family models still collapsing, and the Inuit-Aleut slightly decreasing. This result is not entirely unforeseen, although we didn’t expect it with such a small number of languages. Mueller et al. (2020) report that their models also completely devolved into translations that, while structurally fluent, were completely inadequate at representing the source translation. However, they did see small gains when the number of added languages was small. We hypothesize that our results degrade because of a lack of complete Bible translations. Mueller et al. (2020) start with complete translations, and the numbers only start failing as incomplete translations are added. We see small gains for the Inuit family, for which we have multiple complete Bibles. We hypothesize that many copies of an identical"
2021.computel-1.1,N19-4009,0,0.0184388,"ains a better BLEU score. When we extend our experiments to the entire Inuit-Aleut family, we see modest gains for both the Latin and Non-Latin languages. However, we also note that the translation quality collapses for the other language families. We suspect this may be due to a large BPE vocabulary - the Inuit-Aleut family, containing two scripts, is more likely to split words; the single script Athabaskan and Algic families, on the other hand, can simply memorize entire words, which may Model Details We use transformer systems for translation and train our models using the Fairseq toolkit (Ott et al., 2019), with 3 encoding and decoding layers, 4 attention heads, an embedding size of 512, and a maximum of 2000 tokens per batch6 . Models are trained for 100 epochs. We set aside the book of Revelation as an evaluation set: the first 100 verses serve as a validation set, and the final 304 verses form a held-out test set. 7 Discussions of dialects and languages aside, we include the largest source which contains the language name - thus, we choose one source only from Western, Eastern, Plains, and Moose Cree, for example. 8 According to our preliminary experiments, length-based batching can seriousl"
2021.computel-1.1,L18-1472,0,0.0256998,"Yes Yes No Yes No OT No No No No No No No No No No No Yes No No No No Yes No No No No No Yes No No No Yes Yes No No No No Books 27 1 27 27 27 30 41 8 2 2 27 67 28 1 28 27 67 27 27 27 1 4 67 27 5 27 67 67 27 17 27 2 Family Algic Athabaskan Inuit-Aleut Iroquoian Uto-Aztecan English Weighted TTR 12.74 9.12 36.92 22.04 8.04 2.22 Table 2: Weighted Type-to-Token ratios of collected language families. the same size. The languages that we collect exhibit a wide range of interesting linguistic phenomena. Several of the languages are predominantly SVO languages (if all arguments occur in the sentence) (Schmirler et al., 2018) but we also include languages like Haida where SOV constructions are prevalent (Enrico, 2003). We also have examples of both nominative-accusative alignment and ergative-absolutive alignment exemplified by Inuktitut in the Inuit-Aleut family (Nowak, 2011). Additionally, the languages display a large variety of interesting morphological features. We find examples of predominantly suffixing morphology in the Algic languages and extensive use of prefixes encountered in Athabaskan languages. Furthermore, animacy is an important grammatical category which is morphologically marked in Plains Cree ("
2021.naacl-main.435,P18-1198,0,0.0303327,"of the speech signal: specifically the presence or absence of the fricative [s] in the output of the network and the amplitude of frication. They show that manipulation of the variables changes these features in a predictable manner. Similarly to our work, Beguˇs (2020b) also scales state activations and observes the effect on the output of the network. In a related investigation of reduplication, Beguˇs (2020a) train GAN models on speech and identify variables which trigger reduplication in the speech signal. Extensive work exists on linguistic probing experiments for neural representations (Conneau et al., 2018a,b; Clark et al., 2019). A recent probing paper by Torroba Hennigen et al. (2020) is more directly related to our work. They present a decomposable probe for finding small sets of hidden states which encode for linguistically relevant information, particularly morphosyntactic information. Our work shares the aim of not only identifying if information is present in a neural system, but also examining how it is represented. However, we additionally perform experiments on manipulating network activations and examine how such manipulations influence the outputs of the network. Interpretation of n"
2021.naacl-main.435,D18-1269,0,0.0139971,"of the speech signal: specifically the presence or absence of the fricative [s] in the output of the network and the amplitude of frication. They show that manipulation of the variables changes these features in a predictable manner. Similarly to our work, Beguˇs (2020b) also scales state activations and observes the effect on the output of the network. In a related investigation of reduplication, Beguˇs (2020a) train GAN models on speech and identify variables which trigger reduplication in the speech signal. Extensive work exists on linguistic probing experiments for neural representations (Conneau et al., 2018a,b; Clark et al., 2019). A recent probing paper by Torroba Hennigen et al. (2020) is more directly related to our work. They present a decomposable probe for finding small sets of hidden states which encode for linguistically relevant information, particularly morphosyntactic information. Our work shares the aim of not only identifying if information is present in a neural system, but also examining how it is represented. However, we additionally perform experiments on manipulating network activations and examine how such manipulations influence the outputs of the network. Interpretation of n"
2021.naacl-main.435,K17-2001,1,0.890174,"Missing"
2021.naacl-main.435,2020.scil-1.5,0,0.0342658,"Missing"
2021.naacl-main.435,W19-4828,0,0.0254706,"cifically the presence or absence of the fricative [s] in the output of the network and the amplitude of frication. They show that manipulation of the variables changes these features in a predictable manner. Similarly to our work, Beguˇs (2020b) also scales state activations and observes the effect on the output of the network. In a related investigation of reduplication, Beguˇs (2020a) train GAN models on speech and identify variables which trigger reduplication in the speech signal. Extensive work exists on linguistic probing experiments for neural representations (Conneau et al., 2018a,b; Clark et al., 2019). A recent probing paper by Torroba Hennigen et al. (2020) is more directly related to our work. They present a decomposable probe for finding small sets of hidden states which encode for linguistically relevant information, particularly morphosyntactic information. Our work shares the aim of not only identifying if information is present in a neural system, but also examining how it is represented. However, we additionally perform experiments on manipulating network activations and examine how such manipulations influence the outputs of the network. Interpretation of neural representations in"
2021.naacl-main.435,2020.emnlp-main.15,0,0.0345602,"in the output of the network and the amplitude of frication. They show that manipulation of the variables changes these features in a predictable manner. Similarly to our work, Beguˇs (2020b) also scales state activations and observes the effect on the output of the network. In a related investigation of reduplication, Beguˇs (2020a) train GAN models on speech and identify variables which trigger reduplication in the speech signal. Extensive work exists on linguistic probing experiments for neural representations (Conneau et al., 2018a,b; Clark et al., 2019). A recent probing paper by Torroba Hennigen et al. (2020) is more directly related to our work. They present a decomposable probe for finding small sets of hidden states which encode for linguistically relevant information, particularly morphosyntactic information. Our work shares the aim of not only identifying if information is present in a neural system, but also examining how it is represented. However, we additionally perform experiments on manipulating network activations and examine how such manipulations influence the outputs of the network. Interpretation of neural representations in recurrent neural models has been an active area of resear"
2021.naacl-main.435,W16-2010,0,0.0450767,"Missing"
2021.naacl-main.435,W18-1817,0,0.0614385,"Missing"
2021.naacl-main.435,W19-4219,0,0.045635,"Missing"
2021.naacl-main.435,W97-1012,0,0.577375,"rticularly morphosyntactic information. Our work shares the aim of not only identifying if information is present in a neural system, but also examining how it is represented. However, we additionally perform experiments on manipulating network activations and examine how such manipulations influence the outputs of the network. Interpretation of neural representations in recurrent neural models has been an active area of research over a long period of time starting with Elman (1990). However, representations in models of phonology have received less attention than many other subfields of NLP. Rodd (1997) investigates learning of Turkish vowel harmony by a character-based RNN language model trained on Our approach was inspired by the now-classic word forms. The paper investigates hidden state ac- paper on visualization and interpretation of recurtivations of RNN models while varying the hidden rent networks by Karpathy et al. (2015) in that we state dimensionality between 1 and 4. It presents also seek individual interpretable dimensions. The evidence that RNN hidden states can capture Turk- work by Dalvi et al. (2019) on analyzing individish vowel harmony patterns when a sufficient num- ual n"
2021.naacl-main.435,W18-0314,1,0.84302,". The paper investigates hidden state ac- paper on visualization and interpretation of recurtivations of RNN models while varying the hidden rent networks by Karpathy et al. (2015) in that we state dimensionality between 1 and 4. It presents also seek individual interpretable dimensions. The evidence that RNN hidden states can capture Turk- work by Dalvi et al. (2019) on analyzing individish vowel harmony patterns when a sufficient num- ual neurons in networks trained for linguistic tasks ber of hidden dimensions are available. In a similar (POS tagging as well as semantic and morphologivein, Silfverberg et al. (2018) investigate phoneme cal tagging) is more closely related to the present representations for Finnish, Spanish and Turkish work. They present a general methodology for finding correlations between embedding represen- uncovering neurons which encode linguistic infortations and phonological distinctive features. Ko- mation by training a classifier to predict linguistic lachina and Magyar (2019) present an investiga- features of the input based on the representations tion of phone embeddings learned using word2vec generated by the network. They also show that it is (Mikolov et al., 2013) for simul"
2021.sigmorphon-1.11,N15-1107,0,0.0201075,"2020). However, paradigm clustering systems do not infer missing forms in paradigms. Our system resembles the baseline system for the paradigm completion task (Jin et al., 2020) which also extracts transformation rules, however, in the form of edit trees (Chrupala et al., 2008). Several approaches to unsupervised or minimally supervised morphology learning, which share characteristics with our system, have been proposed. Our rules are essentially identical to the FST rules used by Beemer et al. (2020) for the task of supervised morphological inflection. Likewise, Durrett and DeNero (2013) and Ahlberg et al. (2015) both extract inflectional rules after aligning forms from known paradigms. Yarowsky and Wicentowski (2000) also generate rules for morphological transformations but their system for minimally supervised morphological analysis requires additional information in the form of a list of morphemes as input. Erdmann et al. (2020) present a task called the paradigm discovery problem which is quite similar to the unsupervised paradigm clustering task. In their formulation of the task, inflected forms are clustered into paradigms and corre3.2 Transformation rules Our approach builds on the baseline par"
2021.sigmorphon-1.11,Q17-1010,0,0.254555,"Missing"
2021.sigmorphon-1.11,chrupala-etal-2008-learning,0,0.150233,"Missing"
2021.sigmorphon-1.11,K18-3001,1,0.913747,"Missing"
2021.sigmorphon-1.11,K17-2001,0,0.131853,"Missing"
2021.sigmorphon-1.11,N13-1138,0,0.187348,"digm completion (Kann et al., 2020). However, paradigm clustering systems do not infer missing forms in paradigms. Our system resembles the baseline system for the paradigm completion task (Jin et al., 2020) which also extracts transformation rules, however, in the form of edit trees (Chrupala et al., 2008). Several approaches to unsupervised or minimally supervised morphology learning, which share characteristics with our system, have been proposed. Our rules are essentially identical to the FST rules used by Beemer et al. (2020) for the task of supervised morphological inflection. Likewise, Durrett and DeNero (2013) and Ahlberg et al. (2015) both extract inflectional rules after aligning forms from known paradigms. Yarowsky and Wicentowski (2000) also generate rules for morphological transformations but their system for minimally supervised morphological analysis requires additional information in the form of a list of morphemes as input. Erdmann et al. (2020) present a task called the paradigm discovery problem which is quite similar to the unsupervised paradigm clustering task. In their formulation of the task, inflected forms are clustered into paradigms and corre3.2 Transformation rules Our approach"
2021.sigmorphon-1.11,2020.acl-main.695,0,0.169931,"ly supervised morphology learning, which share characteristics with our system, have been proposed. Our rules are essentially identical to the FST rules used by Beemer et al. (2020) for the task of supervised morphological inflection. Likewise, Durrett and DeNero (2013) and Ahlberg et al. (2015) both extract inflectional rules after aligning forms from known paradigms. Yarowsky and Wicentowski (2000) also generate rules for morphological transformations but their system for minimally supervised morphological analysis requires additional information in the form of a list of morphemes as input. Erdmann et al. (2020) present a task called the paradigm discovery problem which is quite similar to the unsupervised paradigm clustering task. In their formulation of the task, inflected forms are clustered into paradigms and corre3.2 Transformation rules Our approach builds on the baseline paradigms discovered in the previous step. We start by extracting transformation rules between all word forms in a single baseline paradigm. For each pair of strings like dog and dogs belonging to a paradigm, we generate a rule like ?+ 0:s which translates the first form into the second one. From a paradigm of size n, we can t"
2021.sigmorphon-1.11,2020.acl-main.598,0,0.343372,"ers (Wiemerslage et al., 2021). Here all forms sharing a given substring of length n are clustered into a paradigm. Duplicate paradigms are removed. The hyperparameter n can be tuned on validation data if such data is available (we use n = 5 in all our experiments). Related Work The unsupervised paradigm clustering task is closely related to the 2020 SIGMORPHON shared task on unsupervised morphological paradigm completion (Kann et al., 2020). However, paradigm clustering systems do not infer missing forms in paradigms. Our system resembles the baseline system for the paradigm completion task (Jin et al., 2020) which also extracts transformation rules, however, in the form of edit trees (Chrupala et al., 2008). Several approaches to unsupervised or minimally supervised morphology learning, which share characteristics with our system, have been proposed. Our rules are essentially identical to the FST rules used by Beemer et al. (2020) for the task of supervised morphological inflection. Likewise, Durrett and DeNero (2013) and Ahlberg et al. (2015) both extract inflectional rules after aligning forms from known paradigms. Yarowsky and Wicentowski (2000) also generate rules for morphological transforma"
2021.sigmorphon-1.11,2020.sigmorphon-1.3,1,0.867261,"uages with a high morpheme-to-word ratio like Basque. 2 Methods 3.1 Baseline As a baseline, we use the character n-gram clustering method provided by the shared task organizers (Wiemerslage et al., 2021). Here all forms sharing a given substring of length n are clustered into a paradigm. Duplicate paradigms are removed. The hyperparameter n can be tuned on validation data if such data is available (we use n = 5 in all our experiments). Related Work The unsupervised paradigm clustering task is closely related to the 2020 SIGMORPHON shared task on unsupervised morphological paradigm completion (Kann et al., 2020). However, paradigm clustering systems do not infer missing forms in paradigms. Our system resembles the baseline system for the paradigm completion task (Jin et al., 2020) which also extracts transformation rules, however, in the form of edit trees (Chrupala et al., 2008). Several approaches to unsupervised or minimally supervised morphology learning, which share characteristics with our system, have been proposed. Our rules are essentially identical to the FST rules used by Beemer et al. (2020) for the task of supervised morphological inflection. Likewise, Durrett and DeNero (2013) and Ahlbe"
2021.sigmorphon-1.11,W19-4226,1,0.867098,"utilize word embeddings when extracting rules due to the very small size of the shared task datasets. In addition to prefix and suffix rules, we also experiment with more general discontinuous transformation rules which can apply transformations to infixes as well as prefixes and suffixes. For example, the rule Introduction Supervised sequence-to-sequence models for word inflection have delivered impressive results in the past few years and a number of shared tasks on supervised learning of morphology have helped to raise the state of the art of this task (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019; Vylomova et al., 2020). In contrast, unsupervised approaches to morphology have received far less attention in recent years. Nevertheless, the question of whether the morphological system of a language can be discovered from raw text data alone is certainly an interesting one. This paper describes the submission of the CU-UBC team for the SIGMORPHON 2021 Shared Task 2: Unsupervised morphological paradigm clustering (Wiemerslage et al., 2021).1 The objective of this task is to group 1 ?+ i:0 ?+ e:i ?+ 0:t would transform the input form gidem (‘to bite’ in Maltese) to gdimt. Our results github"
2021.sigmorphon-1.11,2020.lrec-1.352,1,0.495796,"other words in the paradigm, we then compare their cosine similarity r0 to one of the reference forms. Forms fail the embedding-similarity test if r0 &lt; 0.5 and r − r0 > 0.3. 4 Experiments and Results In this section, we describe experiments on the shared task development and test languages. 4.1 Data and Resources The shared task uses two data resources. Corpus data for the four development languages (Maltese, Persian, Russian and Swedish) and nine test languages (Basque Bulgarian, English, Finnish, German, Kannada, Navajo, Spanish and Turkish) are sourced from the Johns Hopkins Bible Corpus (McCarthy et al., 2020b). For most of the languages, complete Bibles were provided but for some of them, we only had access to a subset (see Wiemerslage et al. (2021) for details). Gold standard paradigms were automatically generated using the Unimorph 3.0 database (McCarthy et al., 2020a). 4.2 Experiments on validation languages Since our transformation rules are generated from paradigms discovered by the baseline system, which contain incorrect items, it is to be expected that some incorrect rules are generated. We filter out infrequent rules, as they are less likely to represent genuine morphological transformat"
2021.sigmorphon-1.11,N15-1186,0,0.023322,"best overall performance is delivered by prefix and suffix rules but more general transformation rules perform better for languages with templatic morphology and very high morpheme-to-word ratios. 1 [w a l k e d] .o. [?+ e:i d:n 0:g] will result in an output form w a l k i n g. We cluster forms into the same paradigm if we can find morphological transformation rules which map one of the forms into the other. Our approach is illustrated in Figure 2. We experiment with two methods for discovering rules, described in Section 3.3. Our first approach is inspired by work on morphology discovery by Soricut and Och (2015), who generate prefix and suffix transformations between similar strings. This idea closely parallels our approach for extracting rules. Unlike Soricut and Och (2015), however, we do not utilize word embeddings when extracting rules due to the very small size of the shared task datasets. In addition to prefix and suffix rules, we also experiment with more general discontinuous transformation rules which can apply transformations to infixes as well as prefixes and suffixes. For example, the rule Introduction Supervised sequence-to-sequence models for word inflection have delivered impressive re"
2021.sigmorphon-1.11,2020.sigmorphon-1.1,1,0.787901,"Missing"
2021.sigmorphon-1.11,P00-1027,0,0.0621708,"esembles the baseline system for the paradigm completion task (Jin et al., 2020) which also extracts transformation rules, however, in the form of edit trees (Chrupala et al., 2008). Several approaches to unsupervised or minimally supervised morphology learning, which share characteristics with our system, have been proposed. Our rules are essentially identical to the FST rules used by Beemer et al. (2020) for the task of supervised morphological inflection. Likewise, Durrett and DeNero (2013) and Ahlberg et al. (2015) both extract inflectional rules after aligning forms from known paradigms. Yarowsky and Wicentowski (2000) also generate rules for morphological transformations but their system for minimally supervised morphological analysis requires additional information in the form of a list of morphemes as input. Erdmann et al. (2020) present a task called the paradigm discovery problem which is quite similar to the unsupervised paradigm clustering task. In their formulation of the task, inflected forms are clustered into paradigms and corre3.2 Transformation rules Our approach builds on the baseline paradigms discovered in the previous step. We start by extracting transformation rules between all word forms"
2021.sigmorphon-1.21,E09-2008,0,0.328041,"he base form and morphosyntactic description of inflected word forms: a word gupdiit ‘they ate’ is annotated gup-TR-3PL. Our Gitksan analyzer is based on two core documentary resources: a wordlist spanning approximately 1250 tokens, and an 18,000 token interlinear-annotated text collection. Due to the scarcity of available lexical and corpus resources, we take a rule-based approach to modeling of morphology which is less dependent on large datasets than machine learning methods. Our analyzer is based on finite-state technology (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009b). Our work has three central goals: (1) We want to build a flexible morphological analyzer to supplement lexical and textual resources in support of language learning. Such an analyzer can support learners in identifying the base-form of inflected words where the morpheme-to-word ratio might be particularly high, in a way not addressed by a traditional dictionary. It may also productively generate inflected forms of words. (2) We want to facilitate ongoing efforts to expand the aforementioned 1250 token wordlist into a broad-coverage dictionary of the Gitksan language. Running our analyzer o"
2021.sigmorphon-1.21,L18-1416,0,0.0169016,"s in broad use today across the Gitxsan community for all diIi ii get, al’algaltgathl CVC-algal-t=gat=hl get CCNJ PL -watch-3. II = REPORT = CN people ‘And they stood by and watched,’ The analyzed corpus provides insight into the use of clitics in running speech, and is the dataset against which we test the results of the analyzer. 190 3 Related Work While considering different approaches to computational modeling of Gitksan morphology, finitestate morphology arose as a natural choice. At the present time, finite-state methods are quite widely applied for Indigenous languages of the Americas. Chen and Schwartz (2018) present a morphological analyzer for St. Lawrence Island / Central Siberian Yupik for aid in language preservation and revitalization work. Strunk (2020) present another analyzer for Central Alaskan Yupik. Snoek et al. (2014) present a morphological analyzer for Plains Cree nouns and Harrigan et al. (2017) present one for Plains Cree verbs. Littell (2018) build a finitestate analyzer for Kwak’wala. All of the above are languages which present similar challenges to the ones encountered in the case of Gitksan: word forms consisting of a large number of morphemes, both prefixing and suffixing mo"
2021.sigmorphon-1.21,N19-4021,0,0.0149726,"ic lexical resource for the Gitksan language. Littell et al. (2017) present an electronic dictionary interface Waldayu for endangered languages and apply it to Gitksan. The model is capable of performing fuzzy dictionary search which is an important extension in the presence of orthographic variation which widely occurs in Gitksan. While this represents an important development for computational lexicography for Gitksan, the method cannot model productive inflection which is important particularly for language learners who might not be able to easily deduce the base-form of an inflected word (Hunt et al., 2019). As mentioned earlier, our model can analyze inflected forms of lexemes. We extend the coverage of our finite-state analyzers by incorporating a neural morphological guesser which can be used to analyze word forms which are rejected by the finite-state analyzer. Similar mechanisms have been explored for other American Indigenous languages. Micher (2017) use segmental recurrent neural networks (Kong et al., 2015) to augment a finite-state morphological analyzer for Inuktitut.2 These jointly segment the 2 input word into morphemes and label each morpheme with one or more grammatical tags. Very"
2021.sigmorphon-1.21,W18-4803,0,0.024875,"ng different approaches to computational modeling of Gitksan morphology, finitestate morphology arose as a natural choice. At the present time, finite-state methods are quite widely applied for Indigenous languages of the Americas. Chen and Schwartz (2018) present a morphological analyzer for St. Lawrence Island / Central Siberian Yupik for aid in language preservation and revitalization work. Strunk (2020) present another analyzer for Central Alaskan Yupik. Snoek et al. (2014) present a morphological analyzer for Plains Cree nouns and Harrigan et al. (2017) present one for Plains Cree verbs. Littell (2018) build a finitestate analyzer for Kwak’wala. All of the above are languages which present similar challenges to the ones encountered in the case of Gitksan: word forms consisting of a large number of morphemes, both prefixing and suffixing morphology and morphophonological alternations. Finite-state morphology is well-suited for dealing with these challenges. It is noteworthy that similarly to Gitksan, a number of the aforementioned languages are also undergoing active documentation efforts. While we present the first morphological analyzer for Gitksan which is capable of productive inflection"
2021.sigmorphon-1.21,W17-0119,0,0.0282912,"ich present similar challenges to the ones encountered in the case of Gitksan: word forms consisting of a large number of morphemes, both prefixing and suffixing morphology and morphophonological alternations. Finite-state morphology is well-suited for dealing with these challenges. It is noteworthy that similarly to Gitksan, a number of the aforementioned languages are also undergoing active documentation efforts. While we present the first morphological analyzer for Gitksan which is capable of productive inflection, this is not the first electronic lexical resource for the Gitksan language. Littell et al. (2017) present an electronic dictionary interface Waldayu for endangered languages and apply it to Gitksan. The model is capable of performing fuzzy dictionary search which is an important extension in the presence of orthographic variation which widely occurs in Gitksan. While this represents an important development for computational lexicography for Gitksan, the method cannot model productive inflection which is important particularly for language learners who might not be able to easily deduce the base-form of an inflected word (Hunt et al., 2019). As mentioned earlier, our model can analyze inf"
2021.sigmorphon-1.21,C18-1008,0,0.0624235,"Missing"
2021.sigmorphon-1.21,W17-0114,0,0.0238523,"portant development for computational lexicography for Gitksan, the method cannot model productive inflection which is important particularly for language learners who might not be able to easily deduce the base-form of an inflected word (Hunt et al., 2019). As mentioned earlier, our model can analyze inflected forms of lexemes. We extend the coverage of our finite-state analyzers by incorporating a neural morphological guesser which can be used to analyze word forms which are rejected by the finite-state analyzer. Similar mechanisms have been explored for other American Indigenous languages. Micher (2017) use segmental recurrent neural networks (Kong et al., 2015) to augment a finite-state morphological analyzer for Inuktitut.2 These jointly segment the 2 input word into morphemes and label each morpheme with one or more grammatical tags. Very silmilarly to the approach that we adopt, Schwartz et al. (2019) and Moeller et al. (2018) use attentional LSTM encoder-decoder models to augment morphological analyzers for extending morphological analyzers for St. Lawrence Island / Central Siberian Yupik and Arapaho, respectively. 4 The Model Our morphological analyzer was designed with several conside"
2021.sigmorphon-1.21,W18-4802,0,0.0442155,"mes. We extend the coverage of our finite-state analyzers by incorporating a neural morphological guesser which can be used to analyze word forms which are rejected by the finite-state analyzer. Similar mechanisms have been explored for other American Indigenous languages. Micher (2017) use segmental recurrent neural networks (Kong et al., 2015) to augment a finite-state morphological analyzer for Inuktitut.2 These jointly segment the 2 input word into morphemes and label each morpheme with one or more grammatical tags. Very silmilarly to the approach that we adopt, Schwartz et al. (2019) and Moeller et al. (2018) use attentional LSTM encoder-decoder models to augment morphological analyzers for extending morphological analyzers for St. Lawrence Island / Central Siberian Yupik and Arapaho, respectively. 4 The Model Our morphological analyzer was designed with several considerations in mind. First, given the small amount of data at our disposal, we chose to construct a rule-based finite state transducer, built from a predefined lexicon and morphological description. The dependence of this type of analyzer on a lexicon supports one of the major goals of this project: lexical discovery from texts. Words w"
2021.sigmorphon-1.21,N19-4009,0,0.0256833,"Missing"
2021.sigmorphon-1.21,W19-6012,0,0.0201879,"yze inflected forms of lexemes. We extend the coverage of our finite-state analyzers by incorporating a neural morphological guesser which can be used to analyze word forms which are rejected by the finite-state analyzer. Similar mechanisms have been explored for other American Indigenous languages. Micher (2017) use segmental recurrent neural networks (Kong et al., 2015) to augment a finite-state morphological analyzer for Inuktitut.2 These jointly segment the 2 input word into morphemes and label each morpheme with one or more grammatical tags. Very silmilarly to the approach that we adopt, Schwartz et al. (2019) and Moeller et al. (2018) use attentional LSTM encoder-decoder models to augment morphological analyzers for extending morphological analyzers for St. Lawrence Island / Central Siberian Yupik and Arapaho, respectively. 4 The Model Our morphological analyzer was designed with several considerations in mind. First, given the small amount of data at our disposal, we chose to construct a rule-based finite state transducer, built from a predefined lexicon and morphological description. The dependence of this type of analyzer on a lexicon supports one of the major goals of this project: lexical dis"
2021.sigmorphon-1.21,W14-2205,0,0.0329881,"se of clitics in running speech, and is the dataset against which we test the results of the analyzer. 190 3 Related Work While considering different approaches to computational modeling of Gitksan morphology, finitestate morphology arose as a natural choice. At the present time, finite-state methods are quite widely applied for Indigenous languages of the Americas. Chen and Schwartz (2018) present a morphological analyzer for St. Lawrence Island / Central Siberian Yupik for aid in language preservation and revitalization work. Strunk (2020) present another analyzer for Central Alaskan Yupik. Snoek et al. (2014) present a morphological analyzer for Plains Cree nouns and Harrigan et al. (2017) present one for Plains Cree verbs. Littell (2018) build a finitestate analyzer for Kwak’wala. All of the above are languages which present similar challenges to the ones encountered in the case of Gitksan: word forms consisting of a large number of morphemes, both prefixing and suffixing morphology and morphophonological alternations. Finite-state morphology is well-suited for dealing with these challenges. It is noteworthy that similarly to Gitksan, a number of the aforementioned languages are also undergoing a"
2021.sigmorphon-1.8,K17-2001,1,0.900551,"Missing"
2021.sigmorphon-1.8,W16-2002,1,0.895884,"Missing"
2021.sigmorphon-1.8,N13-1138,0,0.0713896,"Missing"
2021.sigmorphon-1.8,2020.acl-main.695,1,0.883705,"Missing"
2021.sigmorphon-1.8,2021.sigmorphon-1.12,1,0.8311,"Missing"
2021.sigmorphon-1.8,Q17-1010,0,0.106213,"Missing"
2021.sigmorphon-1.8,2020.acl-main.598,1,0.881267,"Missing"
2021.sigmorphon-1.8,K18-3001,1,0.886365,"Missing"
2021.sigmorphon-1.8,2020.lrec-1.497,0,0.0616438,"Missing"
2021.sigmorphon-1.8,2020.sigmorphon-1.3,1,0.815511,"Missing"
2021.sigmorphon-1.8,2021.sigmorphon-1.10,0,0.061232,"Missing"
2021.sigmorphon-1.8,2020.acl-demos.14,0,0.0404485,"Missing"
2021.sigmorphon-1.8,W19-4226,1,0.902221,"Missing"
2021.sigmorphon-1.8,2021.sigmorphon-1.11,1,0.696729,"Missing"
2021.sigmorphon-1.8,2020.lrec-1.352,1,0.814695,"Missing"
2021.sigmorphon-1.8,2021.sigmorphon-1.9,0,0.0885046,"Missing"
C18-1137,L18-1293,1,0.898204,"Missing"
C18-1137,N15-2022,0,0.0272718,"h as ‘koiralla’ in Figure 1b) will be generalized into 3 Some authors call the generalization an inflectional class (Haspelmath and Sims, 2013). 1616 a form pattern, such as xi +lla. The features corresponding to a form, such as NOM;SG will be referred to as the morphosyntactic description (MSD). 2 Related Work As noted above, we build on the work of Ahlberg et al. (2014), Ahlberg et al. (2015), and Forsberg and Hulden (2016) which introduced the LCS as a core strategy in formal definitions of morphological paradigm and analogy. Further linguistic arguments favoring the LCS-model are given in Lee (2015). Learning to generalize from inflection tables to unseen forms has attracted much recent interest in natural language processing (Dreyer and Eisner, 2011; Durrett and DeNero, 2013). In addition, two recent shared tasks hosted by SIGMORPHON and CoNLL have contributed to the research by producing comparable large multilingual data sets (Cotterell et al., 2016; Cotterell et al., 2017) consistently annotated with the Unimorph scheme (Sylak-Glassman et al., 2015; Kirov et al., 2018). Overwhelmingly, most recent work in inflection generation has employed neural sequence-to-sequence models with atte"
C18-1137,W16-2003,0,0.0308496,"g (Dreyer and Eisner, 2011; Durrett and DeNero, 2013). In addition, two recent shared tasks hosted by SIGMORPHON and CoNLL have contributed to the research by producing comparable large multilingual data sets (Cotterell et al., 2016; Cotterell et al., 2017) consistently annotated with the Unimorph scheme (Sylak-Glassman et al., 2015; Kirov et al., 2018). Overwhelmingly, most recent work in inflection generation has employed neural sequence-to-sequence models with attention (Sutskever et al., 2014; Bahdanau et al., 2015) in supervised scenarios (Kann and Sch¨utze, 2016; Faruqui ¨ et al., 2016; Ostling, 2016; Makarov et al., 2017; Aharoni and Goldberg, 2017), often with data augmentation mechanisms in low-resource settings (Bergmanis et al., 2017; Silfverberg et al., 2017; Kann and Sch¨utze, 2017). The PCFP has been explicitly addressed by recurrent neural generators as well (Malouf, 2016; Malouf, 2017). While neural models perform quite well on such tasks—even in low-resource settings—their parameter opacity makes it difficult to extract concise linguistic generalizations that can be interpreted and compared with linguist analyses. Since morphological annotation is usually done at the word-level"
C18-1137,W17-0418,1,0.782897,"ic generalizations that can be interpreted and compared with linguist analyses. Since morphological annotation is usually done at the word-level (flowers ↔ l+PL),4 recent work has also attempted to learn the latent segmentation and recover the different allomorphs for stems and affixes (flower = l, s=PL). This is arguably similar to what an L1-learner does, and the kind of evidence L1 learners have—words with semantic (l) and grammatical (+PL) content deducible from context, but crucially missing information about how these correspond to some subsequence of phonemes in an inflected word form. Silfverberg and Hulden (2017a) and Silfverberg and Hulden (2017b) propose a datadriven method that searches the space of possible allomorph and grammatical tag associations, and favors a small model. Similar approaches are found in Hayes (2018) who similarly argues that allomorphs can be detected “even if we don’t yet understand the phonology”. Our work, apart from providing an explicit model for paradigms, also implicitly extracts the different allomorphs used, something that falls out as a byproduct of the paradigm generalizations in section 4 and inference from partial inflection tables in section 5. 3 Data We use two"
C18-1137,W17-4107,1,0.839424,"ic generalizations that can be interpreted and compared with linguist analyses. Since morphological annotation is usually done at the word-level (flowers ↔ l+PL),4 recent work has also attempted to learn the latent segmentation and recover the different allomorphs for stems and affixes (flower = l, s=PL). This is arguably similar to what an L1-learner does, and the kind of evidence L1 learners have—words with semantic (l) and grammatical (+PL) content deducible from context, but crucially missing information about how these correspond to some subsequence of phonemes in an inflected word form. Silfverberg and Hulden (2017a) and Silfverberg and Hulden (2017b) propose a datadriven method that searches the space of possible allomorph and grammatical tag associations, and favors a small model. Similar approaches are found in Hayes (2018) who similarly argues that allomorphs can be detected “even if we don’t yet understand the phonology”. Our work, apart from providing an explicit model for paradigms, also implicitly extracts the different allomorphs used, something that falls out as a byproduct of the paradigm generalizations in section 4 and inference from partial inflection tables in section 5. 3 Data We use two"
C18-1137,K17-2010,1,0.796272,"by producing comparable large multilingual data sets (Cotterell et al., 2016; Cotterell et al., 2017) consistently annotated with the Unimorph scheme (Sylak-Glassman et al., 2015; Kirov et al., 2018). Overwhelmingly, most recent work in inflection generation has employed neural sequence-to-sequence models with attention (Sutskever et al., 2014; Bahdanau et al., 2015) in supervised scenarios (Kann and Sch¨utze, 2016; Faruqui ¨ et al., 2016; Ostling, 2016; Makarov et al., 2017; Aharoni and Goldberg, 2017), often with data augmentation mechanisms in low-resource settings (Bergmanis et al., 2017; Silfverberg et al., 2017; Kann and Sch¨utze, 2017). The PCFP has been explicitly addressed by recurrent neural generators as well (Malouf, 2016; Malouf, 2017). While neural models perform quite well on such tasks—even in low-resource settings—their parameter opacity makes it difficult to extract concise linguistic generalizations that can be interpreted and compared with linguist analyses. Since morphological annotation is usually done at the word-level (flowers ↔ l+PL),4 recent work has also attempted to learn the latent segmentation and recover the different allomorphs for stems and affixes (flower = l, s=PL). This"
C18-1137,P15-2111,0,0.0312277,"d the LCS as a core strategy in formal definitions of morphological paradigm and analogy. Further linguistic arguments favoring the LCS-model are given in Lee (2015). Learning to generalize from inflection tables to unseen forms has attracted much recent interest in natural language processing (Dreyer and Eisner, 2011; Durrett and DeNero, 2013). In addition, two recent shared tasks hosted by SIGMORPHON and CoNLL have contributed to the research by producing comparable large multilingual data sets (Cotterell et al., 2016; Cotterell et al., 2017) consistently annotated with the Unimorph scheme (Sylak-Glassman et al., 2015; Kirov et al., 2018). Overwhelmingly, most recent work in inflection generation has employed neural sequence-to-sequence models with attention (Sutskever et al., 2014; Bahdanau et al., 2015) in supervised scenarios (Kann and Sch¨utze, 2016; Faruqui ¨ et al., 2016; Ostling, 2016; Makarov et al., 2017; Aharoni and Goldberg, 2017), often with data augmentation mechanisms in low-resource settings (Bergmanis et al., 2017; Silfverberg et al., 2017; Kann and Sch¨utze, 2017). The PCFP has been explicitly addressed by recurrent neural generators as well (Malouf, 2016; Malouf, 2017). While neural model"
D18-1315,P17-1183,0,0.0625453,";PRS;2;SG haga ponga SBJV;PRS;1;SG NFIN SBJV;PRS;3;PL NFIN SBJV;PRS;1;SG … pondrá IND;FUT;3;SG SBJV;PRS;2;SG IND;FUT;3;SG hiciera … SBJV;PST;3;SG SBJV;PST;1;SG … SBJV;PST;3;SG SBJV;PST;1;SG … SBJV;PST;3;SG SBJV;PST;1;SG Figure 1: Illustration of the PCFP using a fraction of Spanish verb tables: given such partially filled paradigms, the task is to fill in all the missing forms. Related Work Neural models have recently been shown to be highly competitive in many different tasks of learning supervised morphological inflection (Faruqui et al., 2016; Kann and Sch¨utze, 2016; Makarov et al., 2017; Aharoni and Goldberg, 2017) and derivation (Cotterell et al., 2017b). Most current architectures are based on encoderdecoder models (Sutskever et al., 2014), and usually contain an attention component (Bahdanau et al., 2015). The SIGMORPHON (Cotterell et al., 2016) and CoNLL-SIGMORPHON (Cotterell et al., 2017a, 2018) shared tasks in recent years have explored morphological inflection but not explicitly the PCFP. In the 2017 task, participants were given full paradigms—i.e. a listing of all forms—of 2883 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2883–2889 c Brussels, Be"
D18-1315,E14-1060,1,0.806534,"Missing"
D18-1315,N15-1107,1,0.900177,"Missing"
D18-1315,W13-3520,0,0.032896,"ed collections of morphological inflection tables based on Wiktionary. We conduct experiments on noun and verb paradigms from eight languages.6 Not all languages have 1,000 noun and verb tables. Hence, our selection is not complete as seen in Table 3. We conduct experiments on two different sets of tables: (1) we randomly sample 1,000 tables for each language and part-of-speech, and (2) we select Unimorph tables including some of the 10,000 most common word forms according to Wikipedia frequency. The Wikipedia word frequencies are based on plain Wikipedia text dumps from the Polyglot project (Al-Rfou et al., 2013). Georgian and Latin did not have a Polyglot Wikipedia so we excluded those. Moreover, we excluded Latvian verbs because there was very little overlap between the most frequent Wikipedia word forms and Unimorph table entries (< 200 forms occurred in both). Details for both types of data sets are given in Tables 3 and 2. F INNISH N OUNS F INNISH V ERBS F RENCH V ERBS G ERMAN V ERBS L ATVIAN N OUNS S PANISH V ERBS T URKISH N OUNS # Tables Table Size 1,335 513 1,131 657 802 1,067 884 27.3 38.9 47.8 24.9 12.8 62.8 78.5 Table 2: Details for inflection tables chosen according to Wikipedia word frequ"
D18-1315,K18-3001,1,0.840202,"Missing"
D18-1315,K17-2001,1,0.9136,"Missing"
D18-1315,D17-1074,0,0.0344831,"Missing"
D18-1315,N16-1077,0,0.054421,";PRS;3;SG SBJV;PRS;1;SG SBJV;PRS;3;SG SBJV;PRS;2;SG IND;FUT;3;SG SBJV;PRS;2;SG haga ponga SBJV;PRS;1;SG NFIN SBJV;PRS;3;PL NFIN SBJV;PRS;1;SG … pondrá IND;FUT;3;SG SBJV;PRS;2;SG IND;FUT;3;SG hiciera … SBJV;PST;3;SG SBJV;PST;1;SG … SBJV;PST;3;SG SBJV;PST;1;SG … SBJV;PST;3;SG SBJV;PST;1;SG Figure 1: Illustration of the PCFP using a fraction of Spanish verb tables: given such partially filled paradigms, the task is to fill in all the missing forms. Related Work Neural models have recently been shown to be highly competitive in many different tasks of learning supervised morphological inflection (Faruqui et al., 2016; Kann and Sch¨utze, 2016; Makarov et al., 2017; Aharoni and Goldberg, 2017) and derivation (Cotterell et al., 2017b). Most current architectures are based on encoderdecoder models (Sutskever et al., 2014), and usually contain an attention component (Bahdanau et al., 2015). The SIGMORPHON (Cotterell et al., 2016) and CoNLL-SIGMORPHON (Cotterell et al., 2017a, 2018) shared tasks in recent years have explored morphological inflection but not explicitly the PCFP. In the 2017 task, participants were given full paradigms—i.e. a listing of all forms—of 2883 Proceedings of the 2018 Conference on Empi"
D18-1315,W14-2804,1,0.754779,"Missing"
D18-1315,E17-1049,0,0.194971,"Missing"
D18-1315,P16-2090,0,0.0802553,"Missing"
D18-1315,L18-1293,1,0.841548,"chs without batching. We train 10 models for every language and part-of-speech and apply majority voting to get the final output forms. All models were implemented using DyNet (Neubig et al., 2017). 3 Table Size Unique Forms per Table 27.7 39.0 47.5 19.0 28.9 11.9 99.8 11.6 62.5 74.4 25.7 37.6 36.1 16.9 12.3 7.2 94.8 7.6 52.1 54.8 F IN N F IN V F RE V G EO N G ER V L AT N L AT V L AV N S PA V T UR N Table 3: Details for randomly sampled inflection tables. The data for each language and part-of-speech consist of 1,000 tables. Data We use UniMorph morphological paradigm data in our experiments (Kirov et al., 2018). Unimorph data sets are crowd-sourced collections of morphological inflection tables based on Wiktionary. We conduct experiments on noun and verb paradigms from eight languages.6 Not all languages have 1,000 noun and verb tables. Hence, our selection is not complete as seen in Table 3. We conduct experiments on two different sets of tables: (1) we randomly sample 1,000 tables for each language and part-of-speech, and (2) we select Unimorph tables including some of the 10,000 most common word forms according to Wikipedia frequency. The Wikipedia word frequencies are based on plain Wikipedia te"
D18-1315,C18-1137,1,0.834426,"Missing"
drobac-etal-2014-heuristic,W98-1312,0,\N,Missing
drobac-etal-2014-heuristic,W13-5631,1,\N,Missing
E14-4015,D10-1095,0,0.0216271,"mber of tags in the data set and number of tags in the training set, respectively. POS tagging. Our preliminary experiments using the latest violation updates supported this. Consequently, we employ the early updates. We also provide results using the CRFsuite toolkit (Okazaki, 2007), which implements a 1storder CRF model. To best of our knowledge, CRFsuite is currently the fastest freely available CRF implementation.3 In addition to the averaged perceptron algorithm (Collins, 2002), the toolkit implements several training procedures (Nocedal, 1980; Crammer et al., 2006; Andrew and Gao, 2007; Mejer and Crammer, 2010; Shalev-Shwartz et al., 2011). We run CRFsuite using these algorithms employing their default parameters and the feature extraction scheme and stopping criterion described in Section 3.3. We then report results provided by the most accurate algorithm on each language. Experimental Setup Data For a quick overview of the data sets, see Table 1. Penn Treebank. The first data set we consider is the classic Penn Treebank. The complete treebank is divided into 25 sections of newswire text extracted from the Wall Street Journal. We split the data into training, development, and test sets using the s"
E14-4015,P04-1015,0,0.060625,"ity, firstname.lastname@aalto.fi b Department of Modern Languages, University of Helsinki, firstname.lastname@helsinki.fi Abstract quired by the stochastic gradient descent algorithm employed in ML estimation (Vishwanathan et al., 2006). Additionally, while ML and perceptron training share an identical time complexity, the perceptron is in practice faster due to sparser parameter updates. Despite its simplicity, running the perceptron algorithm can be tedious in case the data contains a large number of labels. Previously, this problem has been addressed using, for example, k-best beam search (Collins and Roark, 2004; Zhang and Clark, 2011; Huang et al., 2012) and parallelization (McDonald et al., 2010). In this work, we explore an alternative strategy, in which we modify the perceptron algorithm in spirit of the classic pseudo-likelihood approximation for ML estimation (Besag, 1975). The resulting novel algorithm has linear complexity w.r.t. the label set size and contains only a single hyper-parameter, namely, the number of passes taken over the training data set. We evaluate the algorithm, referred to as the pseudo-perceptron, empirically in POS tagging on five languages. The results suggest that the a"
E14-4015,W02-1001,0,0.871589,"l sequence labeling tasks in natural language processing, including part-of-speech (POS) tagging. In this work, we discuss accelerating the CRF model estimation in presence of a large number of labels, say, hundreds or thousands. Large label sets occur in POS tagging of morphologically rich languages (Erjavec, 2010; Haverinen et al., 2013). CRF training is most commonly associated with the (conditional) maximum likelihood (ML) criterion employed in the original work of Lafferty et al. (2001). In this work, we focus on an alternative training approach using the averaged perceptron algorithm of Collins (2002). While yielding competitive accuracy (Collins, 2002; Zhang and Clark, 2011), the perceptron algorithm avoids extensive tuning of hyper-parameters and regularization re2 2.1 Methods Pseudo-Perceptron Algorithm The (unnormalized) CRF model for input and output sequences x = (x1 , x2 , . . . , x|x |) and 74 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 74–78, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics are decoded in a standard manner using the Viterbi search. The appeal of PP is that"
E14-4015,W96-0213,0,0.394504,"modified using the early update rule (Huang et al., 2012). While Huang et al. (2012) experimented with several violation-fixing methods (early, latest, maximum, hybrid), they appeared to reach termination at the same rate in 3 See benchmark results at http://www.chokkan. org/software/crfsuite/benchmark.html 76 has not increased during last three iterations. After termination, we apply the averaged parameters yielding highest performance on the development set to test instances. Test and development instances are decoded using a combination of Viterbi search and the tag dictionary approach of Ratnaparkhi (1996). In this approach, candidate tags for known word forms are limited to those observed in the training data. Meanwhile, word forms that were unseen during training consider the full label set. 3.4 method English PP PW-PP 1-best beam Pas.-Agg. Romanian PP PW-PP 1-best beam Pas.-Agg. Estonian PP PW-PP 1-best beam Pas.-Agg. Czech PP PW-PP 1-best beam Pegasos Finnish PP PW-PP 1-best beam Pas.-Agg. Software and Hardware The experiments are run on a standard desktop computer. We use our own C++-based implementation of the methods discussed in Section 2. 4 Results The obtained training times and test"
E14-4015,erjavec-2010-multext,0,0.111636,". We present experiments on five languages. Despite its heuristic nature, the algorithm provides surprisingly competetive accuracies and running times against reference methods. 1 Introduction The conditional random field (CRF) model (Lafferty et al., 2001) has been successfully applied to several sequence labeling tasks in natural language processing, including part-of-speech (POS) tagging. In this work, we discuss accelerating the CRF model estimation in presence of a large number of labels, say, hundreds or thousands. Large label sets occur in POS tagging of morphologically rich languages (Erjavec, 2010; Haverinen et al., 2013). CRF training is most commonly associated with the (conditional) maximum likelihood (ML) criterion employed in the original work of Lafferty et al. (2001). In this work, we focus on an alternative training approach using the averaged perceptron algorithm of Collins (2002). While yielding competitive accuracy (Collins, 2002; Zhang and Clark, 2011), the perceptron algorithm avoids extensive tuning of hyper-parameters and regularization re2 2.1 Methods Pseudo-Perceptron Algorithm The (unnormalized) CRF model for input and output sequences x = (x1 , x2 , . . . , x|x |) an"
E14-4015,N12-1015,0,0.0303934,"Missing"
E14-4015,J11-1005,0,0.28515,"alto.fi b Department of Modern Languages, University of Helsinki, firstname.lastname@helsinki.fi Abstract quired by the stochastic gradient descent algorithm employed in ML estimation (Vishwanathan et al., 2006). Additionally, while ML and perceptron training share an identical time complexity, the perceptron is in practice faster due to sparser parameter updates. Despite its simplicity, running the perceptron algorithm can be tedious in case the data contains a large number of labels. Previously, this problem has been addressed using, for example, k-best beam search (Collins and Roark, 2004; Zhang and Clark, 2011; Huang et al., 2012) and parallelization (McDonald et al., 2010). In this work, we explore an alternative strategy, in which we modify the perceptron algorithm in spirit of the classic pseudo-likelihood approximation for ML estimation (Besag, 1975). The resulting novel algorithm has linear complexity w.r.t. the label set size and contains only a single hyper-parameter, namely, the number of passes taken over the training data set. We evaluate the algorithm, referred to as the pseudo-perceptron, empirically in POS tagging on five languages. The results suggest that the approach can yield compe"
E14-4015,N10-1069,0,\N,Missing
K17-2010,W16-2007,0,0.0570554,"Missing"
K17-2010,N15-1107,0,0.0534045,"and conclusions in Section 6. 2 Related Work Several existing approaches to morphological reinflection are based on traditional structured prediction models. For example, Liu and Mao (2016) and King (2016) use Conditional Random Fields (CRF) and Alegria and Etxeberria (2016) and Nicolai et al. (2016) employ different phoneme-tographeme translation systems. Other approaches include learning a morphological analyzer from training data and applying it to reinflect test examples (Taji et al., 2016) and extracting morphological paradigms from the training data which are then applied on test words (Ahlberg et al., 2015; Sorokin, 2016). The results of the 2016 SIGMORPHON Shared Task on Morphological Reinflection indicate that none of these approaches can compete with deep learning models. The deep learning systems outperformed all other systems by a wide margin. The three best performing teams (Kann and 3 Task Description and Data The shared task consists of two subtasks: (1) generation of word-forms based on a lemma and a set of morphological features (for example, dog+N+Pl → dogs), and (2) completion of morphological paradigms given a small number of known forms (see Figure 1). Systems are evaluated on 52"
K17-2010,W16-2004,0,0.0372079,"d as follows: Section 2 presents related work on morphological reinflection and data augmentation for natural language processing. In Section 3, we describe the shared task and associated data sets. We provide a detailed description of our system in Section 4 and present experiments and results in Section 5. Finally, we provide a discussion of results and conclusions in Section 6. 2 Related Work Several existing approaches to morphological reinflection are based on traditional structured prediction models. For example, Liu and Mao (2016) and King (2016) use Conditional Random Fields (CRF) and Alegria and Etxeberria (2016) and Nicolai et al. (2016) employ different phoneme-tographeme translation systems. Other approaches include learning a morphological analyzer from training data and applying it to reinflect test examples (Taji et al., 2016) and extracting morphological paradigms from the training data which are then applied on test words (Ahlberg et al., 2015; Sorokin, 2016). The results of the 2016 SIGMORPHON Shared Task on Morphological Reinflection indicate that none of these approaches can compete with deep learning models. The deep learning systems outperformed all other systems by a wide margin. The thr"
K17-2010,W16-2010,0,0.12687,"Missing"
K17-2010,W16-2008,0,0.0233089,"n in the low resource setting. The paper is organized as follows: Section 2 presents related work on morphological reinflection and data augmentation for natural language processing. In Section 3, we describe the shared task and associated data sets. We provide a detailed description of our system in Section 4 and present experiments and results in Section 5. Finally, we provide a discussion of results and conclusions in Section 6. 2 Related Work Several existing approaches to morphological reinflection are based on traditional structured prediction models. For example, Liu and Mao (2016) and King (2016) use Conditional Random Fields (CRF) and Alegria and Etxeberria (2016) and Nicolai et al. (2016) employ different phoneme-tographeme translation systems. Other approaches include learning a morphological analyzer from training data and applying it to reinflect test examples (Taji et al., 2016) and extracting morphological paradigms from the training data which are then applied on test words (Ahlberg et al., 2015; Sorokin, 2016). The results of the 2016 SIGMORPHON Shared Task on Morphological Reinflection indicate that none of these approaches can compete with deep learning models. The deep lea"
K17-2010,W16-2006,1,0.796399,"rell et al., 2017), even in the low resource setting. The paper is organized as follows: Section 2 presents related work on morphological reinflection and data augmentation for natural language processing. In Section 3, we describe the shared task and associated data sets. We provide a detailed description of our system in Section 4 and present experiments and results in Section 5. Finally, we provide a discussion of results and conclusions in Section 6. 2 Related Work Several existing approaches to morphological reinflection are based on traditional structured prediction models. For example, Liu and Mao (2016) and King (2016) use Conditional Random Fields (CRF) and Alegria and Etxeberria (2016) and Nicolai et al. (2016) employ different phoneme-tographeme translation systems. Other approaches include learning a morphological analyzer from training data and applying it to reinflect test examples (Taji et al., 2016) and extracting morphological paradigms from the training data which are then applied on test words (Ahlberg et al., 2015; Sorokin, 2016). The results of the 2016 SIGMORPHON Shared Task on Morphological Reinflection indicate that none of these approaches can compete with deep learning mode"
K17-2010,P17-2090,0,0.041541,"Missing"
K17-2010,W16-2005,0,0.0221793,"elated work on morphological reinflection and data augmentation for natural language processing. In Section 3, we describe the shared task and associated data sets. We provide a detailed description of our system in Section 4 and present experiments and results in Section 5. Finally, we provide a discussion of results and conclusions in Section 6. 2 Related Work Several existing approaches to morphological reinflection are based on traditional structured prediction models. For example, Liu and Mao (2016) and King (2016) use Conditional Random Fields (CRF) and Alegria and Etxeberria (2016) and Nicolai et al. (2016) employ different phoneme-tographeme translation systems. Other approaches include learning a morphological analyzer from training data and applying it to reinflect test examples (Taji et al., 2016) and extracting morphological paradigms from the training data which are then applied on test words (Ahlberg et al., 2015; Sorokin, 2016). The results of the 2016 SIGMORPHON Shared Task on Morphological Reinflection indicate that none of these approaches can compete with deep learning models. The deep learning systems outperformed all other systems by a wide margin. The three best performing teams ("
K17-2010,W16-2003,0,0.0736544,"Missing"
K17-2010,P16-1009,0,0.0285861,"nau et al. (2014). Although the RNN Encoder-Decoder framework has proven to be highly successful in morphological reinflection, an out-of-the-box RNN Encoder-Decoder system performs poorly in presence of small training sets due to overfitting. To alleviate this problem, we employ data augmentation, that is, augmentation of the training set with artificial, generated, training examples. The technique is well known in the field of image processing (Krizhevsky et al., 2012; Chatfield et al., 2014). Even though the technique is used less frequently in NLP, a number of notable approaches do exist. Sennrich et al. (2016) use monolingual target language data to improve the performance of an Encoder-Decoder translation system. They first train a translation system from the target language to the source language, which is used to back-translate target language sentences to source language sentences. The sentence pairs consisting of a translated source sentence and a genuine target sentence are then added to the training data. Other approaches to data augmentation in NLP include substitution of words by synonyms (Fadaee et al., 2017; Zhang and LeCun, 2015) and paraphrasing. system introduced by Kann and Sch¨utze"
K17-2010,W16-2009,0,0.0209448,"tion 6. 2 Related Work Several existing approaches to morphological reinflection are based on traditional structured prediction models. For example, Liu and Mao (2016) and King (2016) use Conditional Random Fields (CRF) and Alegria and Etxeberria (2016) and Nicolai et al. (2016) employ different phoneme-tographeme translation systems. Other approaches include learning a morphological analyzer from training data and applying it to reinflect test examples (Taji et al., 2016) and extracting morphological paradigms from the training data which are then applied on test words (Ahlberg et al., 2015; Sorokin, 2016). The results of the 2016 SIGMORPHON Shared Task on Morphological Reinflection indicate that none of these approaches can compete with deep learning models. The deep learning systems outperformed all other systems by a wide margin. The three best performing teams (Kann and 3 Task Description and Data The shared task consists of two subtasks: (1) generation of word-forms based on a lemma and a set of morphological features (for example, dog+N+Pl → dogs), and (2) completion of morphological paradigms given a small number of known forms (see Figure 1). Systems are evaluated on 52 languages.3 3 Al"
K18-3001,K18-3001,1,0.103672,"Missing"
K18-3001,P16-2090,1,0.838493,"Missing"
K18-3001,K17-2003,1,0.836665,"Missing"
K18-3001,K17-2010,1,0.734971,"Missing"
K18-3001,W18-6011,1,0.913422,"and a target UniMorph sentence is shown in Figure 3. Since the selection of languages in task 2 is small and we do not attempt to correct annotation errors in the UD source materials, conversion between UD and UniMorph morphosyntactic descriptions is generally straightforward.11 However, UD descriptions are more fine-grained than their UniMorph equivalents. For example, UD denotes lexical features such as noun gender which are inherent features of a lexeme possessed by all of its word forms. Such inherent features are missing from UniMorph which exclusively annotates inflectional morphology (McCarthy et al., 2018). Therefore, UD fea$ → sta$ ti$ → dista$ koti$ → kodista$ i$ → ista$ oti$ → odista$ Such rules are then extracted from each example inflection in the training data. At generation time, the longest matching left hand side of a rule is identified and applied to the citation form. For example, if the Finnish noun luoti ‘bullet’ were to be inflected in the elative (N;IN+ABL;SG) using only the extracted rules given above, the transformation oti$ → odista$ would be triggered, producing the output luodista. In case there are multiple candidate rules of equally long left hand sides that all match, tie"
K18-3001,K18-3012,0,0.30177,"Missing"
K18-3001,K18-3015,0,0.276525,"Missing"
K18-3001,P15-2111,1,\N,Missing
K18-3001,K17-2002,1,\N,Missing
K18-3001,K17-3001,0,\N,Missing
K18-3001,W17-4110,1,\N,Missing
K18-3001,N18-2087,1,\N,Missing
K18-3001,P18-1245,1,\N,Missing
K18-3001,L18-1293,1,\N,Missing
K18-3001,K18-3004,0,\N,Missing
K18-3001,K18-3010,0,\N,Missing
K18-3001,K18-3013,0,\N,Missing
K18-3001,K18-3003,0,\N,Missing
K18-3001,K18-3016,0,\N,Missing
K18-3001,K18-3005,0,\N,Missing
K18-3001,W16-2006,0,\N,Missing
K18-3001,K17-2008,1,\N,Missing
K18-3001,K17-2005,0,\N,Missing
K18-3001,K18-3008,0,\N,Missing
K18-3001,K18-3007,0,\N,Missing
K19-1014,P17-1183,0,0.155657,"Missing"
K19-1014,D16-1162,0,0.0150884,"rn three sub-categories of target errors. 3 This label is applied regardless of whether the predicted inﬂected form is correct or not, and therefore is independent of system predictions. Furthermore, it is possible that both the gold data and prediction have the same incorrect inﬂected form, but detecting such cases is challenging. Silly errors This category consists of those “bizarre” errors which defy any purely linguistic characterization. In addition to the aforementioned case of *membled, such errors have also been reported for other language generation tasks such as machine translation (Arthur et al. 2016) and text normalization (Gorman and Sproat 2016, Sproat and Jaitly 2017, Zhang et al. 2019). Allomorphy errors This category consists of those errors which are characterized by misapplication of existing (i.e., independently attested) allomorphic patterns in the target language. Our annotation scheme recognizes four sub-categories of allomorphy error, but we set aside their their description for reasons of space. Spelling errors This category includes inﬂected forms that do not follow language-speciﬁc orthographic conventions but are otherwise correct. 4 Results We performed full error annotat"
K19-1014,J08-4004,0,0.048686,"ined to produce reliable annotations. 142 speakers; the remaining eight were annotated by second-language speakers. In addition to the annotation guidelines, annotators were encouraged to consult authoritative dictionaries and reference grammars—such as the Iso suomen kielioppi (Hakulinen et al. 2008) for Finnish, the Duden for German, the Oxford Latin Dictionary (Lee 1968), or the Diccionario de la lengua española for Spanish—and native speakers. Table 2 reports summary statistics for fully-annotated languages. 4.1 Inter-annotator agreement Table 3 provides raw agreement and Krippendorf’s α (Artstein and Poesio 2008) for those languages known to two annotators. As mentioned above, each annotator is a specialist in computational linguistics, and annotated at least one other language as well. Raw agreement is high, and while chancecorrected agreement statistics like α are notoriously difﬁcult to interpret, α ≥ 0.8, a threshold obtained for all three double-coded languages, is generally considered to indicate substantial reliability (Krippendorff 2004:241f.). 4.2 Errors Table 4 provides the counts of the four major categories of error for all twelve languages and for both systems. We now proceed to describe"
K19-1014,K17-2002,0,0.080358,"Missing"
K19-1014,P19-1376,0,0.0500582,"errors in the Wiktionary data itself. SPELLING Free variation Extraction Wiktionary Figure 1: Overview of our annotation scheme, including subcategories. Annotators are instructed to proceed through the taxonomy from left to right. Pinker and Prince (1988) and Sproat (1992:216f.) dispute this characterization, pointing out bizarre errors like *membled for mailed. More recently, Kirov and Cotterell (2018) claim that modern neural network architectures—such as those used in the CoNLL–SIGMORPHON 2017 Shared Task— generalize reasonably well while largely eliminating these bizarre errors. However, Corkery et al. (2019) argue that the Kirov and Cotterell model predictions align poorly with human productions, and suggest that the reported results may be uncharacteristic due to fortuitous random seeding. We desired a somewhat richer set of errors than this prior work. The ﬁnal taxonomy— incorporating feedback from a ten-language pilot study—consists of four major error categories, with several additional sub-categories. The categories are applied sequentially, as in Figure 1. We now describe these categories. Target errors This category consists of cases where the gold data is incorrect or incomplete.3 We disc"
K19-1014,N13-1138,0,0.119163,"Missing"
K19-1014,W12-3105,0,0.0612779,"Missing"
K19-1014,Q16-1036,1,0.842948,"This label is applied regardless of whether the predicted inﬂected form is correct or not, and therefore is independent of system predictions. Furthermore, it is possible that both the gold data and prediction have the same incorrect inﬂected form, but detecting such cases is challenging. Silly errors This category consists of those “bizarre” errors which defy any purely linguistic characterization. In addition to the aforementioned case of *membled, such errors have also been reported for other language generation tasks such as machine translation (Arthur et al. 2016) and text normalization (Gorman and Sproat 2016, Sproat and Jaitly 2017, Zhang et al. 2019). Allomorphy errors This category consists of those errors which are characterized by misapplication of existing (i.e., independently attested) allomorphic patterns in the target language. Our annotation scheme recognizes four sub-categories of allomorphy error, but we set aside their their description for reasons of space. Spelling errors This category includes inﬂected forms that do not follow language-speciﬁc orthographic conventions but are otherwise correct. 4 Results We performed full error annotation on twelve of the 52 languages. Several othe"
K19-1014,Q13-1035,0,0.0583587,"Missing"
K19-1014,L18-1293,1,0.834532,"Missing"
K19-1014,L16-1498,0,0.140004,"Missing"
K19-1014,W10-2211,0,0.14519,"Missing"
K19-1014,W19-4226,1,0.788689,"undles and asked to produce the appropriate inﬂected forms. In sub-task 2, training data consists of complete inﬂectional paradigms, and at inference time, the system is asked to produce full paradigms for unseen lemmata. We focus on the results from subtask 1, primarily because only two of the twelve teams chose to compete in sub-task 2. However, the proposed error taxonomy could easily be applied to sub-task 2, or to later morphological generation challenges such as sub-task 2 of the CoNLL– SIGMORPHON 2018 shared task (Cotterell et al. 2018) or sub-task 1 of the SIGMORPHON 2019 shared task (McCarthy et al. 2019). 2.1.1 Data The data in both sub-tasks is primarily sampled from UniMorph (Kirov et al. 2016, 2018), a free morphological database. In turn, UniMorph data for our twelve languages is automatically extracted from Wiktionary, a collaborative multilingual online dictionary. UniMorph pairs the cells of Wiktionary morphological paradigms, which bear prose labels like “genitive plural”, to feature bundles in a language-independent morphological schema (Sylak-Glassman et al. 2015; also see Sylak-Glassman 2016). The data consist of the aforementioned triples of lemma, inﬂectional bundle, and inﬂected"
K19-1014,J11-4002,0,0.089003,"Missing"
K19-1014,J19-2004,1,0.83344,"predicted inﬂected form is correct or not, and therefore is independent of system predictions. Furthermore, it is possible that both the gold data and prediction have the same incorrect inﬂected form, but detecting such cases is challenging. Silly errors This category consists of those “bizarre” errors which defy any purely linguistic characterization. In addition to the aforementioned case of *membled, such errors have also been reported for other language generation tasks such as machine translation (Arthur et al. 2016) and text normalization (Gorman and Sproat 2016, Sproat and Jaitly 2017, Zhang et al. 2019). Allomorphy errors This category consists of those errors which are characterized by misapplication of existing (i.e., independently attested) allomorphic patterns in the target language. Our annotation scheme recognizes four sub-categories of allomorphy error, but we set aside their their description for reasons of space. Spelling errors This category includes inﬂected forms that do not follow language-speciﬁc orthographic conventions but are otherwise correct. 4 Results We performed full error annotation on twelve of the 52 languages. Several other languages were initially targeted for anno"
K19-1014,P16-1208,0,0.0460418,"Missing"
K19-1014,P15-2111,0,0.317018,"Missing"
L18-1294,E09-2008,1,0.873366,"Verb themes fall into larger lexicalinflectional classes referred to in Dene linguistics as verb theme categories. These are characterized by shared conjugation markers in the Imperfective and Perfective, a shared primary aspect indicated by (historic) stem suffixation pattern, derivational potential, and semantic properties (durativity, telicity...). There are 10 verb theme categories, as shown in Table 4. Adverbial-derivational Table 3: aa-perfective of ch’+L+dzüh (op.) ‘dance’ are currently several open source implementations of FSM compilers, e.g. xfst (Beesley and Karttunen, 2003), foma (Hulden, 2009) and HFST (Lindén et al., 2011). The key advantages of FSMs are most crucially that they provide a calculus for powerful manipulations and are designed for rulebased definition of paradigms, which does not require large corpora from which to learn such rules, usually lacking for endangered languages. Furthermore, as well-established computational data structures, FSMs allow for easy integration with other software applications, for instance as spellchecking modules within word-processors, morphologically “intelligent” electronic dictionaries, and “intelligent” computer-aided language-learning"
P14-2043,H05-1060,0,0.0437204,"considering b = 1, 2, 4, 8, 16, 32, 64, 128 until the model accuracy does not improve by at least 0.01 (absolute). Development and test instances are decoded using Viterbi search in combination with the tag dictionary approach of Ratnaparkhi (1996). In this approach, candidate tags for known word forms are limited to those observed in the training data. Meanwhile, word forms that were unseen during training consider the full label set. 3.4 4 In this section, we compare the approach presented in Section 2 to two prior systems which attempt to utilize sub-label dependencies in a similar manner. Smith et al. (2005) use a CRF-based system for tagging Czech, in which they utilize expanded emission features similar to our (5). However, they do not utilize the full expanded transition features (6). More specifically, instead of utilizing a single chain as in our approach, Smith et al. employ five parallel structured chains. One of the chains models the sequence of word-class labels such as noun and adjective. The other four chains model gender, number, case, and lemma sequences, respectively. Therefore, in contrast to our approach, their system does not capture cross-dependencies between inflectional catego"
P14-2043,A00-1031,0,0.0409617,"uded in the CRF model using a relatively straightforward feature expansion scheme. Experiments on five languages showed that the approach can yield significant improvement in tagging accuracy given sufficiently fine-grained label sets. In future work, we aim to perform a more fine-grained error analysis to gain a better understanding where the improvement in accuracy takes place. One could also attempt to optimize the compound label splits to maximize prediction accuracy instead of applying a priori partitions. Table 2: Results. proved open-source implementation of the wellknown TnT tagger of Brants (2000). The obtained HunPos results are presented in Table 3. HunPos Eng 96.58 Rom 96.96 Est 92.76 Cze 89.57 Conclusions Fin 85.77 Table 3: Results using a generative HMM-based HunPos tagger of Halacsy et al. (2007). Acknowledgements This work was financially supported by Langnet (Finnish doctoral programme in language studies) and the Academy of Finland under the grant no 251170 (Finnish Centre of Excellence Program (2012-2017)). We would like to thank the anonymous reviewers for their useful comments. Ceau¸su (2006) uses a maximum entropy Markov model (MEMM) based system for tagging Romanian which"
P14-2043,W02-1001,0,0.622696,"er, hyphen, dash, or digit. Binary functions have a return value of either zero (inactive) or one (active). Meanwhile, the transition features 0 ) . . . 1(y = y 0 ) | {1(yi−k = yi−k i i 0 , . . . , y 0 ∈ Y , ∀k ∈ 1 . . . n} yi−k i (6) (4) capture dependencies between adjacent labels irrespective of the input x. 1. A standard CRF model incorporating (2) and (4) is denoted as CRF(n,-). 2.2.1 Expanded Feature Set Leveraging Sub-Label Dependencies The baseline feature set described above can yield a high tagging accuracy given a conveniently simple label set, exemplified by the tagging results of Collins (2002) on the Penn Treebank (Marcus et al., 1993). (Note that conditional random fields correspond to discriminatively trained hidden Markov models and Collins (2002) employs the latter terminology.) However, it does to some extent overlook some beneficial dependency information in case the labels have a rich sub-structure. In what follows, we describe expanded feature sets which explicitly model the sub-label dependencies. We begin by defining a function P(yi ) which partitions any label yi into its sub-label components and returns them in an unordered set. For example, we could define P(PRON+1+SG)"
P14-2043,J11-1005,0,0.0130834,"not report results using CRF(2,2) since, based on preliminary experiments, this model overfits on all languages. The CRF model parameters are estimated using the averaged perceptron algorithm (Collins, 2002). The model parameters are initialized with a zero vector. We evaluate the latest averaged parameters on the held-out development set after each pass over the training data and terminate training if no improvement in accuracy is obtained during three last passes. The best-performing parameters are then applied on the test instances. We accelerate the perceptron learning using beam search (Zhang and Clark, 2011). The beam width, b, is optimized separately for each language on the development sets by considering b = 1, 2, 4, 8, 16, 32, 64, 128 until the model accuracy does not improve by at least 0.01 (absolute). Development and test instances are decoded using Viterbi search in combination with the tag dictionary approach of Ratnaparkhi (1996). In this approach, candidate tags for known word forms are limited to those observed in the training data. Meanwhile, word forms that were unseen during training consider the full label set. 3.4 4 In this section, we compare the approach presented in Section 2"
P14-2043,erjavec-2010-multext,0,0.159768,"I Mikko Kurimob Department of Modern Languages, University of Helsinki, firstname.lastname@helsinki.fi Abstract 1 Krister Lindéna V+NON3SG+PRES like 2 2.1 Methods Conditional Random Fields The (unnormalized) CRF model (Lafferty et al., 2001) for a sentence x = (x1 , . . . , x|x |) and a POS sequence y = (y1 , . . . , y|x |) is defined as N+SG , ham where the compound labels PRON+1SG, V+NON3SG+PRES, and N+SG stand for pronoun first person singular, verb non-third singular present tense, and noun singular, respectively. Fine-grained labels occur frequently in morphologically complex languages (Erjavec, 2010; Haverinen et al., 2013). We propose improving tagging accuracy by utilizing dependencies within the sub-labels (PRON, 1SG, V, NON3SG, N, and SG in the above example) of the compound labels. From a technical perspective, we accomplish this by making use of the fundamental ability of the CRFs to incorporate arbitrarily defined feature functions. The newlydefined features are expected to alleviate data sparp (y |x; w) ∝ |x| Y   exp w·φ(yi−n , . . . , yi , x, i) , i=n (1) where n denotes the model order, w the model parameter vector, and φ the feature extraction function. We denote the tag set"
P14-2043,P01-1035,0,0.0792301,"Missing"
P14-2043,P07-2053,0,0.0912677,"et al. employ five parallel structured chains. One of the chains models the sequence of word-class labels such as noun and adjective. The other four chains model gender, number, case, and lemma sequences, respectively. Therefore, in contrast to our approach, their system does not capture cross-dependencies between inflectional categories, such as the dependence between the word-class and case of adjacent words. Unsurprisingly, Smith et al. fail to achieve improvement over a generative HMMbased POS tagger of Hajiˇc (2001). Meanwhile, our system outperforms the generative trigram tagger HunPos (Halácsy et al., 2007) which is an imSoftware and Hardware The experiments are run on a standard desktop computer (Intel Xeon E5450 with 3.00 GHz and 64 GB of memory). The methods discussed in Section 2 are implemented in C++. 3.5 Related Work Results The obtained tagging accuracies and training times are presented in Table 2. The times include running the averaged perceptron algorithm and evaluation of the development sets. The column labeled it. corresponds to the number of passes over the training data made by the perceptron algorithm before termination. We summarize the results as follows. First, compared to st"
P14-2043,J93-2004,0,0.0496644,"nctions have a return value of either zero (inactive) or one (active). Meanwhile, the transition features 0 ) . . . 1(y = y 0 ) | {1(yi−k = yi−k i i 0 , . . . , y 0 ∈ Y , ∀k ∈ 1 . . . n} yi−k i (6) (4) capture dependencies between adjacent labels irrespective of the input x. 1. A standard CRF model incorporating (2) and (4) is denoted as CRF(n,-). 2.2.1 Expanded Feature Set Leveraging Sub-Label Dependencies The baseline feature set described above can yield a high tagging accuracy given a conveniently simple label set, exemplified by the tagging results of Collins (2002) on the Penn Treebank (Marcus et al., 1993). (Note that conditional random fields correspond to discriminatively trained hidden Markov models and Collins (2002) employs the latter terminology.) However, it does to some extent overlook some beneficial dependency information in case the labels have a rich sub-structure. In what follows, we describe expanded feature sets which explicitly model the sub-label dependencies. We begin by defining a function P(yi ) which partitions any label yi into its sub-label components and returns them in an unordered set. For example, we could define P(PRON+1+SG) = 2. A CRF model incorporating (2), (4), a"
P14-2043,W96-0213,0,0.926591,"ure functions X with all sub-labels s ∈ S by defining the corresponding label as {χj (x, i)1(yi = yi0 ) |j ∈ 1 . . . |X |, ∀yi0 ∈ Y} , (2) where the function 1(q) returns one if and only if the proposition q is true and zero otherwise, that is  1 if yi = yi0 0 1(yi = yi ) = , (3) 0 otherwise {χj (x, i)1(s ∈ P(yi )) |∀j ∈ 1 . . . |X |, ∀s ∈ S} , (5) where 1(s ∈ P(yi )) returns one in case s is in P(yi ) and zero otherwise. Second, we exploit sublabel transitions using features |X | and X = {χj (x, i)}j=1 is the set of functions characterizing the word position i. Following the classic work of Ratnaparkhi (1996), our X comprises simple binary functions: {1(si−k ∈ P(yi−k )) . . . 1(si ∈ P(yi )) | ∀si−k , . . . , si ∈ S , ∀k ∈ 1 . . . m} . 1. Bias (always active irrespective of input). Note that we define the sub-label transitions up to order m, 1 ≤ m ≤ n, that is, an nth-order CRF model is not obliged to utilize sub-label transitions all the way up to order n. This is because employing high-order sub-label transitions may potentially cause overfitting to training data due to substantially increased number of features (equivalent to the number of model parameters, |w |= |φ|). For example, in a second-o"
W09-4625,C94-1066,0,0.822217,"ounterparts. A very significant use is for debugging two-level grammars. Since forms are not filtered out in a conflict situation, the linguist, who is writing the two-level grammar gets a clearer picture of the way the grammar is broken. In section 5 we use a new transducer operation, weighted intersecting composition to combine a two-level lexicon and a two-level grammar. Weighted intersecting composition allows the weights in the rules to be passed to the resulting lexical transducer. The operation has been modelled on unweighted intersecting composition, which was introduced by Karttunen (Karttunen, 1994). We test our method using an example lexicon and grammar in sections 6 and 7. The example concerns gradation of stops in Finnish. The example lexicon was compiled using the open source two-level lexicon compiler HFST-L EX C 1 and the example grammar was compiled using the open source two-level grammar compiler HFSTT WOL C2 . Both compilers belong to the finite-state morphology toolkit HFST Morphology Tools 3 . 2 Rule Conflicts Rule conflicts occur when two-level rules require, that a lexical symbol is realized in two different ways in the same context. Conflict resolution is a process, which"
W10-2205,J94-3001,0,0.550918,"t-arrow rule. p u k:v u n In our example we require that the correspondence k:v may occur only between two identical rounded close vowels, i.e. either between two letters u or between two letters y. Multiple contexts are needed in the right-arrow rule which expresses this constraint. As a two-level grammar, this would be: Alphabet a b … k … u v w … k:v; Rules k:v =&gt; u _ u; y _ y; This grammar would permit sequences such as: p k p l t u y u u u k:v k:v k:v k:v k k u y u u u Previous compilation methods n n k:v u n n k y k:v y n 2.1 but it would exclude sequences: Method based on Kaplan and Kay Kaplan and Kay (1994) developed a method around 1980 for compiling rewriting rules into finite-state transducers 2 . The method was adapted by Koskenniemi to the compilation of two-level rules by modifying the formula p u k:v y n t u k:v a n 1 In Xerox terminology, the input or lexical characters are called the upper characters, and the output or surface characters are called the lower characters. Other orientations are used by some authors. 2 Douglas Johnson (1972) presented a similar technique earlier but his work was not well known in early 1980s. 39 pile multi-context right-arrow rules. The rule centre x:z can"
W10-2205,C94-1066,0,0.0924988,"bserved somewhere around 1990 at Xerox that the rule sets may be composed with the lexicon transducers in an efficient way and that the resulting transducer was roughly similar in size as the lexicon transducer itself (Karttunen et al., 1992). This observation gives room to the new approach presented below. At that time, it was not practical to intersect complete two-level grammars if they contained many elaborate rules (and this is still a fairly heavy operation). Another useful observation was that the intersection of the rules could be done in a joint single operation with the composition (Karttunen, 1994). Avoiding the separate intersection made the combining of the lexicon and rules feasible and faster. In addition to Xerox LEXC program, e.g. the HFST finite-state software contains this operation and it is routinely used when lexicons and two-level grammars are combined into lexicon transducers (Lindén et al., 2009). Måns Huldén has noted (2009) that the composing of the lexicon and the rules is sometimes a heavy operation, but can be optimized if one first composes the output side of the lexicon transducer with the rules, and thereafter the original lexicon with this intermediate result. Met"
W10-2205,C92-1025,0,0.160112,"ransducer. (It is easy to see that only one auxiliary character is needed when the length of the centres is one.) The compilation of rules with centres whose length is one using the GR seems very similar to that of Grimley-Evans et al. The nice thing about GR is that one can easily express various rule types, including but not limited to the four types listed above. 2.4 It was observed somewhere around 1990 at Xerox that the rule sets may be composed with the lexicon transducers in an efficient way and that the resulting transducer was roughly similar in size as the lexicon transducer itself (Karttunen et al., 1992). This observation gives room to the new approach presented below. At that time, it was not practical to intersect complete two-level grammars if they contained many elaborate rules (and this is still a fairly heavy operation). Another useful observation was that the intersection of the rules could be done in a joint single operation with the composition (Karttunen, 1994). Avoiding the separate intersection made the combining of the lexicon and rules feasible and faster. In addition to Xerox LEXC program, e.g. the HFST finite-state software contains this operation and it is routinely used when"
W10-2205,E87-1003,0,0.622458,"Missing"
W10-2205,J92-1003,0,\N,Missing
W10-2205,C96-1077,0,\N,Missing
W11-4625,C04-1080,0,0.0545361,"Missing"
W11-4625,A00-1031,0,0.618951,"s t1 ..., tn ranges over all analyses of the sentence. The term p(ti |ti−1 , ti−2 ) is the standard second order HMM approximation for the probability of the tag ti . The term p(wi |ti−1 , ti , ti+1 ) conditions the probability of the word wi on its tag context. Finally the term p(wi |ti ) is the standard HMM lexical probability. In order to get the indices to match in the formula above, three additional symbols are needed, i.e. t−1 , t0 and tn+1 denote sentence boundary symbols, which are added during training and tagging for improved accuracy. Using sentence boundary symbols is adopted from Brants (2000). In order to get some estimates for the probability of tag trigrams, which did not occur in the training data, we use tag bigram p(ti |ti−1 ) and tag unigram p(ti ) models in parallel to the trigram model. Similarly we use models which assign probability p(wi |ti−1 , ti ) and p(wi |ti , ti+1 ) in order to deal with previous unseen tag trigrams and wordforms. Of course the lexical model also weights analyses of words, serving as a backup model even in the case where the tag bigrams with the wordform were previously unseen. 185 ´ Miikka Silfverberg and Krister Linden 5.1 Lexical Models For each"
W11-4625,J75-4040,0,0.815948,"Missing"
W11-4625,A88-1019,0,0.165442,"Missing"
W11-4625,W02-1001,0,0.135468,"Missing"
W11-4625,W04-1903,0,0.0375253,"Missing"
W11-4625,P08-2009,0,0.0702412,"Missing"
W11-4625,W09-4625,1,0.890444,"Missing"
W11-4625,N03-1033,0,0.200074,"Missing"
W11-4625,E95-1022,0,0.118662,"Missing"
W11-4625,2005.mtsummit-papers.11,0,0.0184772,"Missing"
W11-4625,H94-1020,0,0.355062,"Missing"
W12-6210,W00-1804,0,0.0785957,"Missing"
W12-6210,C96-2105,0,\N,Missing
W12-6210,C00-1038,0,\N,Missing
W12-6210,E99-1017,0,\N,Missing
W12-6210,W09-4106,0,\N,Missing
W12-6210,P95-1003,0,\N,Missing
W13-5618,A00-1031,0,0.854081,"information into a small number of parameters. Experiments in biomedical entity recognition on the Genia corpus show that the approach can alleviate the OOV problem resulting in improvement in overall model performance. KEYWORDS: Biomedical Entity Recognition, CRF, HMM, Letter N-Grams, OOV, Tagging. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 181 of 474] 1 Introduction We discuss sequential tagging problems in natural language processing (NLP) including tasks such as part-of-speech (POS) tagging (Brants, 2000), shallow parsing (Tjong Kim Sang and Buchholz, 2000), and named entity recognition (NER) (Tjong Kim Sang and De Meulder, 2003). Modern NLP approaches rely on data-driven statistical models trained using manually tagged training corpora. Commonly applied models include Hidden Markov Models (HMMs) (Brants, 2000), Maximum Entropy Markov Models (MEMMs) (McCallum et al., 2000), and Conditional Random Fields (CRFs) (Lafferty et al., 2001). In this work, we present preliminary work on an automatic, domain-independent approach for extracting orthographic features useful for modeling rare words and ou"
W13-5618,W02-1001,0,0.072623,"ndard Bayes’ rule as png (t a g|wor d) = P png (wor d|t a g)p(t a g) t ag 0 (7) png (wor d|t a g 0 )p(t a g 0 ) where the likelihoods png (wor d|t a g) are obtained as presented in Section 3.1. The tag priors p(t ag) are simply the counts of tags in the training data normalized to sum up to one. Parameter estimation. In general, the CRF model parameters w are estimated using a training set of annotated text using, for example, the maximum likelihood criterion as in (Lafferty et al., 2001). In this work, we estimate the parameters using the averaged structured perceptron algorithm presented by Collins (2002). The perceptron algorithm proceeds one training instance at a time by performing maximum a posteriori (MAP) inference, that is prediction of most likely output configuration over the graph, using the standard Viterbi search, and by updating the model parameters using a simple additive rule if an erroneous prediction occurs. Subsequent to training, test instances are tagged again using the Viterbi search. The structured perceptron has a single hyper-parameter, namely the number of passes over training data, which is optimized using a separate development set. 4 Experiments We present experimen"
W13-5618,W04-1213,0,0.208531,"h are then used in feature extraction. An alternative domain independent and automatic approach is to utilize sub-strings as features. In this work, we discuss a modification of this approach. In contrast to using a large amount of sub-string features, out approach is based on a small number of features utilizing letter n-gram models, which are easy to employ using existing efficient toolkits. We describe how to incorporate the features with HMM and CRF models. In order to evaluate our method, we present experiments in biomedical entity recognition, a special case of NER, on the Genia corpus (Kim et al., 2004). Our results show, that the letter n-gram features can improve the model performance with small training sets. The improvements are more notable with CRFs, which as a discriminative log-linear model can naturally incorporate complex, non-independent features. Although the present work focuses on biomedical entity extraction, we believe that the approach could be beneficial in other domains as well. The rest of the paper is as follows. In Section 2, we briefly discuss previous, related research. In Section 3, we describe the letter n-gram models and how to use them to extract orthographic Proc"
W13-5618,W04-1218,0,0.0268521,"tilizing letter n-grams is not new. Probabilistic letter n-gram models have been used in term recognition. For example (Vasserman, 2004) employed them as classifiers in chemical name recognition in a similar manner they have been utilized in language identification (Vatanen et al., 2010). Our use of the n-gram models to capture word orthography closely resembles theirs. However, we investigate incorporating letter n-gram models as feature extractors in statistical taggers. Also, plain letter n-grams (raw sub-strings of words) have been used extensively as features in taggers, see for example (Rössler, 2004). However, this approach results in an explosion in the number of model parameters, which could be avoided by using the feature extraction approach presented in this work. Automatic and domain-independent orthographic feature extraction was recently discussed by (Fujii and Sakurai, 2012). The central idea in their work was to employ combined tag level and character level language models based on a hierarchical Bayesian approach. They also described experiments on the Genia corpus. However, our CRF model appears to outperforms their model. Additionally, although orthographic features are used t"
W13-5618,W03-1307,0,0.469591,"e provide results on the Genia corpus, the data set used in the shared task of biomedical entity recognition in the joint workshop of BioNLP/NLPBA 2004 (Kim et al., 2004). Among the participants of this task, the most commonly adopted approach to modeling OOV words was to use manually constructed regular expressions and affixes combined with gazetteers and POS features. This was the approach taken by the best performing system described by (Zhou and Su, 2004). We compare our n-gram-based feature extraction approach with the regular expression set used in their system, originally described by (Shen et al., 2003), and show that our method performs better1 . The idea of constructing a domain-independent set of orthographic features utilizing letter n-grams is not new. Probabilistic letter n-gram models have been used in term recognition. For example (Vasserman, 2004) employed them as classifiers in chemical name recognition in a similar manner they have been utilized in language identification (Vatanen et al., 2010). Our use of the n-gram models to capture word orthography closely resembles theirs. However, we investigate incorporating letter n-gram models as feature extractors in statistical taggers."
W13-5618,W00-0726,0,0.176063,"Missing"
W13-5618,W03-0419,0,0.0665921,"Missing"
W13-5618,N04-2002,0,0.0374189,"words was to use manually constructed regular expressions and affixes combined with gazetteers and POS features. This was the approach taken by the best performing system described by (Zhou and Su, 2004). We compare our n-gram-based feature extraction approach with the regular expression set used in their system, originally described by (Shen et al., 2003), and show that our method performs better1 . The idea of constructing a domain-independent set of orthographic features utilizing letter n-grams is not new. Probabilistic letter n-gram models have been used in term recognition. For example (Vasserman, 2004) employed them as classifiers in chemical name recognition in a similar manner they have been utilized in language identification (Vatanen et al., 2010). Our use of the n-gram models to capture word orthography closely resembles theirs. However, we investigate incorporating letter n-gram models as feature extractors in statistical taggers. Also, plain letter n-grams (raw sub-strings of words) have been used extensively as features in taggers, see for example (Rössler, 2004). However, this approach results in an explosion in the number of model parameters, which could be avoided by using the fe"
W13-5618,vatanen-etal-2010-language,0,0.0326351,"best performing system described by (Zhou and Su, 2004). We compare our n-gram-based feature extraction approach with the regular expression set used in their system, originally described by (Shen et al., 2003), and show that our method performs better1 . The idea of constructing a domain-independent set of orthographic features utilizing letter n-grams is not new. Probabilistic letter n-gram models have been used in term recognition. For example (Vasserman, 2004) employed them as classifiers in chemical name recognition in a similar manner they have been utilized in language identification (Vatanen et al., 2010). Our use of the n-gram models to capture word orthography closely resembles theirs. However, we investigate incorporating letter n-gram models as feature extractors in statistical taggers. Also, plain letter n-grams (raw sub-strings of words) have been used extensively as features in taggers, see for example (Rössler, 2004). However, this approach results in an explosion in the number of model parameters, which could be avoided by using the feature extraction approach presented in this work. Automatic and domain-independent orthographic feature extraction was recently discussed by (Fujii and"
W13-5618,W04-1219,0,0.0335208,"gnition are described In Section 4 along with a discussion. We provide conclusions in Section 5. 2 Related Work In the experimental section of this work, we provide results on the Genia corpus, the data set used in the shared task of biomedical entity recognition in the joint workshop of BioNLP/NLPBA 2004 (Kim et al., 2004). Among the participants of this task, the most commonly adopted approach to modeling OOV words was to use manually constructed regular expressions and affixes combined with gazetteers and POS features. This was the approach taken by the best performing system described by (Zhou and Su, 2004). We compare our n-gram-based feature extraction approach with the regular expression set used in their system, originally described by (Shen et al., 2003), and show that our method performs better1 . The idea of constructing a domain-independent set of orthographic features utilizing letter n-grams is not new. Probabilistic letter n-gram models have been used in term recognition. For example (Vasserman, 2004) employed them as classifiers in chemical name recognition in a similar manner they have been utilized in language identification (Vatanen et al., 2010). Our use of the n-gram models to c"
W13-5641,A92-1016,0,0.325318,"Missing"
W13-5641,carreras-etal-2004-freeling,0,0.0359181,"ng A classic byproduct of encoding a morphological analyzer as an FSM is the ability to quickly implement spell checking tools. This involves extracting the domain or range of the morphological transducer, yielding an automaton that (presumably) contains only valid surface forms of words. This automaton can then be consulted for checking correctness of words—usually with much larger coverage than word lists can provide. For our experiments, we have done so using morphological transducers for Basque (Agirre et al., 1992), English, Finnish (Pirinen, 2011), German (Schmid et al., 2004), Spanish (Carreras et al., 2004), and Swedish. As is seen from table 1, except for the German transducer which encodes many circumfixation phenomena and is rather large to begin with, the compressed sizes of the automata are small enough to be used and integrated into, for instance, web-based text editing environments. 1 3.1 Real-word errors with perceptrons encoded as weighted automata To demonstrate more advanced usage, we have implemented a real-word-error-aware spell checker using weighted automata. Catching real-word spelling errors is a difficult problem 1 Performance is also reasonable. When using a determinized autom"
W13-5641,W02-1001,0,0.0392425,"Missing"
W13-5641,E09-2008,1,0.825819,"ion that also detects real-word errors. KEYWORDS: Finite-state technology, Javascript, spell checking, perceptrons. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 441 of 474] 1 Introduction Finite-state machine (FSM) technology is widely used in language-processing and FSMs are applied in many contexts ranging from text processing to more complex tasks. Currently, several high-quality toolkits exist for the manipulation and construction of weighted and unweighted finite-state machines, such as foma (Hulden, 2009), HFST (Lindén et al., 2009), OpenFST (Allauzen et al., 2007), SFST (Schmid, 2006), and xfst (Beesley and Karttunen, 2003), to name a few. These toolkits, once compiled, also provide programming interfaces for the real-time use of automata in language processing tasks. However, for online and mobile applications, the use of finite-state machines is complicated by the unavailability of generic application interfaces for applying the finite-state machines built with any of the above toolkits. In the following, we demonstrate a simple Javascript API that allows one to take advantage of finite-sta"
W13-5641,W11-4644,0,0.013839,"compressed formats. 3 Example application: spell checking A classic byproduct of encoding a morphological analyzer as an FSM is the ability to quickly implement spell checking tools. This involves extracting the domain or range of the morphological transducer, yielding an automaton that (presumably) contains only valid surface forms of words. This automaton can then be consulted for checking correctness of words—usually with much larger coverage than word lists can provide. For our experiments, we have done so using morphological transducers for Basque (Agirre et al., 1992), English, Finnish (Pirinen, 2011), German (Schmid et al., 2004), Spanish (Carreras et al., 2004), and Swedish. As is seen from table 1, except for the German transducer which encodes many circumfixation phenomena and is rather large to begin with, the compressed sizes of the automata are small enough to be used and integrated into, for instance, web-based text editing environments. 1 3.1 Real-word errors with perceptrons encoded as weighted automata To demonstrate more advanced usage, we have implemented a real-word-error-aware spell checker using weighted automata. Catching real-word spelling errors is a difficult problem 1"
W13-5641,schmid-etal-2004-smor,0,0.023253,"ample application: spell checking A classic byproduct of encoding a morphological analyzer as an FSM is the ability to quickly implement spell checking tools. This involves extracting the domain or range of the morphological transducer, yielding an automaton that (presumably) contains only valid surface forms of words. This automaton can then be consulted for checking correctness of words—usually with much larger coverage than word lists can provide. For our experiments, we have done so using morphological transducers for Basque (Agirre et al., 1992), English, Finnish (Pirinen, 2011), German (Schmid et al., 2004), Spanish (Carreras et al., 2004), and Swedish. As is seen from table 1, except for the German transducer which encodes many circumfixation phenomena and is rather large to begin with, the compressed sizes of the automata are small enough to be used and integrated into, for instance, web-based text editing environments. 1 3.1 Real-word errors with perceptrons encoded as weighted automata To demonstrate more advanced usage, we have implemented a real-word-error-aware spell checker using weighted automata. Catching real-word spelling errors is a difficult problem 1 Performance is also reasonable"
W13-5641,P94-1013,0,0.425195,"Missing"
W15-1842,P98-1013,0,0.264985,"Missing"
W15-1842,C98-1013,0,\N,Missing
W15-4806,W98-1312,0,0.160798,"me quite large. This can be a problem e.g. when analyzers are used on mobile devices where a moderate memory footprint is required. The usual way to reduce the size of FSMs is to use a minimization algorithm (Hopcroft, 1971). Minimization can have a substantial effect on the size of the FSM but, as it is only able to combine suffix-equivalent states, there may still be residual redundancy in the state space of the machine. Further size reduction can be accomplished by introducing a limited form of context-free structure into the finite-state graph using special symbols called flag diacritics (Beesley, 1998). Using flag diacritics, it is possible to combine sub-graphs which are equivalent, i.e. accept the same strings, but which are not necessarily suffix-equivalent. Flag diacritics are used to couple entrance points of the sub-graphs with appropriate exit points. During lookup, paths whose flag diacritics do not match are filtered out. Thus, the original language of the machine is preserved. Traditionally, the lexicon writer manually inserts flag diacritics into the lexicon of the morphological analyzer. There are two major problems with this approach: (1) In practice, manually inserted flag dia"
W15-4806,drobac-etal-2014-heuristic,1,0.810366,"Missing"
W16-2406,P00-1037,0,0.213666,"normalization processes to the input word to make the relationship between the incorrect form and the correct form more transparent and generate a set of candidates within a certain edit distance. In addition to comparing orthographic forms, they also consider the phonetic realization of the input word and find correction candidates whose pronunciation is within a certain edit distance from the pronunciation of the input word. Related Work Spelling correction is an old NLP task. The earliest approaches used plain edit distance combined with a lexicon. The edit distance approach was refined by Brill and Moore (2000) who added weights for edit operations. These systems ignored the context of the edit operation, which can nevertheless be quite useful. Dreyer et al. (2008) investigate string-to-string translation which is a more general task than spelling correction. In order to incorporate symbol contexts into their models, they formulate stringto-string translation as a sequence labeling task. Their sequence labeling model is discriminative In recent work, Eger et al. (2016) survey four systems for string-to-string translation on spelling correction of Tweets and normalization of historical Latin text. (1"
W16-2406,vor-der-bruck-etal-2014-collex,0,0.0401661,"Missing"
W16-2406,W02-1001,0,0.117579,"d to a unique context. Therefore, the collection of contexts L  xt  R is chosen 53 in such a way that it forms a partition of Σ∗ ΣΣ∗ , where Σ is the set of all input symbols. In Section 4, we give a more detailed explanation of how these contexts are chosen. 3.2 For each context Lxt R, we include a number of back-off contexts. For example, let Σ∗ a  xt  b c Σ∗ be a context, then back-off contexts are the following contexts. Σ∗ a  xt  b Σ∗ Σ∗ a  xt  Σ∗ Σ∗  xt  Σ∗ Peceptron Tagger Our structured spelling correction system is formulated as a traditional averaged perceptron tagger (Collins, 2002) as shown in Equation 1. Given an input sequence x of length T , the model assigns a score s(·) for each output sequence y of length T as determined by the model parameters w and a vector valued feature extraction function φ. The n best normalization candidates given by the system can be extracted by finding the n highest scoring outputs y. s(x, y; w) = T X In order to ensure that no two contexts overlap, we need to modify the contexts slightly: Σ∗ a  xt  b [Σ − c] Σ∗ Σ∗ a  xt  [Σ − b] Σ∗ Σ∗ [Σ − a]  xt  Σ∗ 4.2 The structured correction system extracts unstructured and structured feature"
W16-2406,Q15-1031,0,0.0194461,"ion system, which splits the input string into character sequences, and a discriminative sequence labeling system which translates the character sequences into output symbols. DirecTL+ utilizes joint character n-grams in the discriminative sequence labeling system. (3) The AliSeTra system is based on the work of Eger (2012). Like DirecTL+, it also views string-tostring translation as a pipeline of segmentation and sequence labeling. (4) The final system surveyed by Eger et al. (2016) represents the stringto-string translation task as a series of contextual edit operations on the input string (Cotterell et al., 2015). The operations are compiled into a weighted finite-state machine. The edit operations are weighted using a probabilistic model which resembles the maximum entropy Markov model (MEMM) (McCallum et al., 2000). This system is similar to our structured system but we use a different feature set and estimate weights using the average perceptron algorithm. This avoids the well-known label bias problem (Lafferty et al., 2001) associated with MEMMs. Systems 1, 2 and 3 surveyed by Eger et al. (2016) form an interesting contrast to our systems because we do not use segmentation of the input string. In"
W16-2406,D08-1113,0,0.717392,"social media. When spelling correction is applied as a preprocessing step, performance can be better. Digitization of documents is another domain where spelling correction is useful. Digitization often aims to transform physical documents into digital representations which support free text search. This requires the use of an optical character recog51 Proceedings of the ACL Workshop on Statistical NLP and Weighted Automata, pages 51–59, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics and the alignment between the input and output string is a latent variable. Dreyer et al. (2008) implement their model as a finite-state machine. This model is similar to ours but we do not treat the alignment between input and output strings as a latent variable. Instead, the training data for our model is aligned in advance. correction candidates from input strings. These substitutions and their contexts are extracted from training data. This approach was first presented by Lind´en (2006) for generating multilingual spelling variants of scientific and medical terms originating from Latin and Greek, but it also suitable for other tasks involving probabilistic string-to-string translatio"
W16-2406,P96-1031,0,0.475889,"he training data. Another parameter is lC which is the length of the maximal right-hand context. We have set the value of lC as 2 based on preliminary experiments. If xt−1 xt xt+1 ...xt+lC occurs at least nT H times in the training data, In addition, we extract the structured features 1. (yt ) 2. (yt−1 , yt ) 3. (yt−2 , yt−1 , yt ) The unstructured features are aimed at capturing the context of edit operations. Meanwhile, the structured features act as a language model. 5 Implementation This section describes the finite-state implementation of our correction systems as weighted replace rules (Mohri and Sproat, 1996). Formally, the systems can be seen as sets of weighted parallel replace rules. As explained below, we however implement them using a cascade of weighted rules for efficiency reasons. This section will also describe the combination of replace rules and lexicon which is used in some of the experiments. Σ∗ xt−1  xt  xt+1 ...xt+lC Σ∗ is chosen as context. If it occurs fewer times, each of the sub-strings xt−1 xt xt+1 ...xt+k , where 0 ≤ k &lt; nC is considered in turn. The longest one that occurs at least nT H times in the training data is used to define a context. If none of them occur more than"
W16-2406,C12-1048,0,0.0132108,"of pairs of input and output strings. In order to accelerate training, we use aligned training data (consisting of symbol pairs) instead of treating the alignment of input and output strings as a latent variable. 2010) represents the translation task as a pipeline of a string segmentation system, which splits the input string into character sequences, and a discriminative sequence labeling system which translates the character sequences into output symbols. DirecTL+ utilizes joint character n-grams in the discriminative sequence labeling system. (3) The AliSeTra system is based on the work of Eger (2012). Like DirecTL+, it also views string-tostring translation as a pipeline of segmentation and sequence labeling. (4) The final system surveyed by Eger et al. (2016) represents the stringto-string translation task as a series of contextual edit operations on the input string (Cotterell et al., 2015). The operations are compiled into a weighted finite-state machine. The edit operations are weighted using a probabilistic model which resembles the maximum entropy Markov model (MEMM) (McCallum et al., 2000). This system is similar to our structured system but we use a different feature set and estim"
W16-2406,N03-1009,0,0.0531166,"as defined by Allauzen et al. (2007). Because we use a series of compositions spanning several thousands of rule transducers for compiling the unstructured and structured feature transducers U and S, efficient determinization and minimization algorithms are crucial. The minimization algorithm presented by Mohri and Sproat (1996) is available through the HFST interface and applicable to transducers with tropical weights where the weights are non-negative. Unfortunately, the structured correction system incorporates both positive and negative weights. One solution to this problem is provided by Eisner (2003) who introduces a more general formulation of transducer minimization which is applicable to transducers with tropical weights in the entire range R ∪ {∞, −∞} and many other weight classes as well. We have, however, resorted to a simpler approach which is applicable in the special case of tropical weights. After epsilon removal and determinization but before minimization, we traverse the transitions and the final state of the transducer M once and find the minimal weight wmin . Subsequently, we increment all transition and final weights in transducer M by |wmin |which results in a transducer M"
W16-2406,P11-1038,0,0.0653367,"Missing"
W16-2406,N10-1103,0,0.0807002,"Missing"
W16-2406,C96-2105,0,0.00981383,"ted accordingly. The sole exception to this are context-free insertions that, unlike other context-free substitutions, are disallowed altoghether. u → ε::0.05 ||u The rule matches in a context where the input contains two consecutive symbols u, deletes the second of them and assigns a penalty weight of 0.05 ≈ − log(0.95). The HFST library (Lind´en et al., 2011) implements these weighted rules. The unstructured system described in Section 3.1 uses a set of mutually exclusive features as explained in Section 4.1. Conceptually, the system can therefore be seen as a set of parallel replace rules (Kempe and Karttunen, 1996) acting on the same input strings. Although this formulation is theoretically pleasing and weighted parallel replace rules are available through the HFST interface (Lind´en et al., 2011), preliminary experiments revealed that compilation of the system represented using parallel replace rules is slow in presence of training data of realistic scope. However, the subset of parallel replace rules needed in our two systems can be reformulated as normal replace rules to take advantage of a sequence of compose operations eliminating the speed issue in practice, see Section 5.3. 5.2 5.3 Cascaded Weigh"
W17-0418,D16-1097,0,0.0233489,"Missing"
W17-0418,N16-1080,0,0.0224562,"Missing"
W17-0418,W10-2210,0,0.107231,"to provide a more detailed morphosyntactic labeling and segmentation of the UD data. 1 2 Related Work Morphological segmentation, particularly in unsupervised scenarios, is a standard problem in NLP, and has been explored in numerous works (Goldsmith (2001), Creutz and Lagus (2005), Poon et al. (2009), Dreyer and Eisner (2011) inter alia). We recommend Ruokolainen et al. (2016) for an overview. Likewise, semi-supervised, or minimally supervised models—where the supervision usually implies access to some small number of segmented words—have also been widely investigated (Dasgupta and Ng, 2007; Kohonen et al., 2010; Gr¨onroos et al., 2014; Sirts and Goldwater, 2013). Many approaches also take advantage of a semantic signal, or a proxy for semantic similarity between words such as Latent Semantic Analysis (Schone and Jurafsky, 2000) or its more modern counterpart, word embeddings (Soricut and Och, 2015). The specific formulation of an inference problem like the one presented in this paper has to our knowledge not been directly addressed previously, probably due to the necessity of annotated resource schemas such as those present in UD 2.0. A related problem, dealt with in Cotterell et al. (2016b) and Kan"
W17-0418,N09-1024,0,0.0870541,"ings of word forms. We evaluate the method on three languages where we have manually labeled part of the Universal Dependencies data—Finnish, Swedish, and Spanish—and show that the method is robust enough to use for automatic discovery, segmentation, and labeling of allomorphs in the data sets. The model allows us to provide a more detailed morphosyntactic labeling and segmentation of the UD data. 1 2 Related Work Morphological segmentation, particularly in unsupervised scenarios, is a standard problem in NLP, and has been explored in numerous works (Goldsmith (2001), Creutz and Lagus (2005), Poon et al. (2009), Dreyer and Eisner (2011) inter alia). We recommend Ruokolainen et al. (2016) for an overview. Likewise, semi-supervised, or minimally supervised models—where the supervision usually implies access to some small number of segmented words—have also been widely investigated (Dasgupta and Ng, 2007; Kohonen et al., 2010; Gr¨onroos et al., 2014; Sirts and Goldwater, 2013). Many approaches also take advantage of a semantic signal, or a proxy for semantic similarity between words such as Latent Semantic Analysis (Schone and Jurafsky, 2000) or its more modern counterpart, word embeddings (Soricut and"
W17-0418,W00-0712,0,0.0384042,"plored in numerous works (Goldsmith (2001), Creutz and Lagus (2005), Poon et al. (2009), Dreyer and Eisner (2011) inter alia). We recommend Ruokolainen et al. (2016) for an overview. Likewise, semi-supervised, or minimally supervised models—where the supervision usually implies access to some small number of segmented words—have also been widely investigated (Dasgupta and Ng, 2007; Kohonen et al., 2010; Gr¨onroos et al., 2014; Sirts and Goldwater, 2013). Many approaches also take advantage of a semantic signal, or a proxy for semantic similarity between words such as Latent Semantic Analysis (Schone and Jurafsky, 2000) or its more modern counterpart, word embeddings (Soricut and Och, 2015). The specific formulation of an inference problem like the one presented in this paper has to our knowledge not been directly addressed previously, probably due to the necessity of annotated resource schemas such as those present in UD 2.0. A related problem, dealt with in Cotterell et al. (2016b) and Kann et al. (2016), concerns simultaneous segmentation and canonicalization— Introduction Recent versions of Universal Dependencies (UD) (Nivre et al., 2017) provide not only part-ofspeech labeling, but also universal lexica"
W17-0418,D11-1057,0,0.0119875,"We evaluate the method on three languages where we have manually labeled part of the Universal Dependencies data—Finnish, Swedish, and Spanish—and show that the method is robust enough to use for automatic discovery, segmentation, and labeling of allomorphs in the data sets. The model allows us to provide a more detailed morphosyntactic labeling and segmentation of the UD data. 1 2 Related Work Morphological segmentation, particularly in unsupervised scenarios, is a standard problem in NLP, and has been explored in numerous works (Goldsmith (2001), Creutz and Lagus (2005), Poon et al. (2009), Dreyer and Eisner (2011) inter alia). We recommend Ruokolainen et al. (2016) for an overview. Likewise, semi-supervised, or minimally supervised models—where the supervision usually implies access to some small number of segmented words—have also been widely investigated (Dasgupta and Ng, 2007; Kohonen et al., 2010; Gr¨onroos et al., 2014; Sirts and Goldwater, 2013). Many approaches also take advantage of a semantic signal, or a proxy for semantic similarity between words such as Latent Semantic Analysis (Schone and Jurafsky, 2000) or its more modern counterpart, word embeddings (Soricut and Och, 2015). The specific"
W17-0418,Q13-1021,0,0.0729929,"eling and segmentation of the UD data. 1 2 Related Work Morphological segmentation, particularly in unsupervised scenarios, is a standard problem in NLP, and has been explored in numerous works (Goldsmith (2001), Creutz and Lagus (2005), Poon et al. (2009), Dreyer and Eisner (2011) inter alia). We recommend Ruokolainen et al. (2016) for an overview. Likewise, semi-supervised, or minimally supervised models—where the supervision usually implies access to some small number of segmented words—have also been widely investigated (Dasgupta and Ng, 2007; Kohonen et al., 2010; Gr¨onroos et al., 2014; Sirts and Goldwater, 2013). Many approaches also take advantage of a semantic signal, or a proxy for semantic similarity between words such as Latent Semantic Analysis (Schone and Jurafsky, 2000) or its more modern counterpart, word embeddings (Soricut and Och, 2015). The specific formulation of an inference problem like the one presented in this paper has to our knowledge not been directly addressed previously, probably due to the necessity of annotated resource schemas such as those present in UD 2.0. A related problem, dealt with in Cotterell et al. (2016b) and Kann et al. (2016), concerns simultaneous segmentation"
W17-0418,J01-2001,0,0.0509452,"t between morphosyntactic labels and substrings of word forms. We evaluate the method on three languages where we have manually labeled part of the Universal Dependencies data—Finnish, Swedish, and Spanish—and show that the method is robust enough to use for automatic discovery, segmentation, and labeling of allomorphs in the data sets. The model allows us to provide a more detailed morphosyntactic labeling and segmentation of the UD data. 1 2 Related Work Morphological segmentation, particularly in unsupervised scenarios, is a standard problem in NLP, and has been explored in numerous works (Goldsmith (2001), Creutz and Lagus (2005), Poon et al. (2009), Dreyer and Eisner (2011) inter alia). We recommend Ruokolainen et al. (2016) for an overview. Likewise, semi-supervised, or minimally supervised models—where the supervision usually implies access to some small number of segmented words—have also been widely investigated (Dasgupta and Ng, 2007; Kohonen et al., 2010; Gr¨onroos et al., 2014; Sirts and Goldwater, 2013). Many approaches also take advantage of a semantic signal, or a proxy for semantic similarity between words such as Latent Semantic Analysis (Schone and Jurafsky, 2000) or its more mod"
W17-0418,P07-1094,0,0.013796,"obability P (s|f )P (f |s), provided that no features have yet been assigned to s, and f has not been assigned to a substring. When each substring in c has been assigned exactly one label, assign remaining labels to substrings in c which maximize the symmetric conditional probability. (1) Using c(s, f ) we express the probability of the co-occurrence of a feature and substring in Equation 2. P(s, f ) ∝ c(s, f ) + αB(s, f ) (2) The function B in Equation 2 expresses a prior belief about the joint counts of segments and labels, and hyper-parameter α controls the weight of the prior information (Goldwater and Griffiths, 2007). A large α will result in P(s, f ) which very closely reflects the prior belief while a smaller α lets P adapt more closely to the current segmentation and label assignment. We set α to 0.1 in all experiments. We use the joint distribution of substrings and labels in the unsegmented data set D as prior information. Thus B(s, f ) = #(s, f )/#(f ), where #(s, f ) is the count of substrings s in words with morphological feature f and #(f ) is the count of feature f in D. For lemma features, for example lemma=dog, we add an additional factor to the co-occurrence probability P(s, f ) as shown in E"
W17-0418,N15-1186,0,0.0306464,"al. (2009), Dreyer and Eisner (2011) inter alia). We recommend Ruokolainen et al. (2016) for an overview. Likewise, semi-supervised, or minimally supervised models—where the supervision usually implies access to some small number of segmented words—have also been widely investigated (Dasgupta and Ng, 2007; Kohonen et al., 2010; Gr¨onroos et al., 2014; Sirts and Goldwater, 2013). Many approaches also take advantage of a semantic signal, or a proxy for semantic similarity between words such as Latent Semantic Analysis (Schone and Jurafsky, 2000) or its more modern counterpart, word embeddings (Soricut and Och, 2015). The specific formulation of an inference problem like the one presented in this paper has to our knowledge not been directly addressed previously, probably due to the necessity of annotated resource schemas such as those present in UD 2.0. A related problem, dealt with in Cotterell et al. (2016b) and Kann et al. (2016), concerns simultaneous segmentation and canonicalization— Introduction Recent versions of Universal Dependencies (UD) (Nivre et al., 2017) provide not only part-ofspeech labeling, but also universal lexical and inflectional features on most word forms. Table 1 illustrates a fe"
W17-0418,C14-1111,0,0.130796,"Missing"
W17-4107,J93-2003,0,0.142346,"Missing"
W17-4107,D11-1057,0,0.0276573,"abels and allomorphs have similar distributions throughout a data set.1 We also compare the performance of the various methods to a baseline unsupervised model, Morfessor2 , augmented with the capacity to also provide labels of allomorphs in addition to segmenting. 1 Our code is freely available at https://github. com/mpsilfve/learn-allomorphs 2 http://www.cis.hut.fi/projects/ morpho/morfessor2.shtml 2 Related Work In the realm of natural language processing, morphological segmentation is a well-researched and established problem (Goldsmith (2001), Creutz and Lagus (2005), Poon et al. (2009), Dreyer and Eisner (2011), Ruokolainen et al. (2016)). While most approaches to pure segmentation are unsupervised, semi-supervised work usually assumes the availability of a limited number of gold segmentations (Dasgupta and Ng, 2007; Kohonen et al., 2010; Grönroos et al., 2014; Sirts and Goldwater, 2013). Using vector space representations of words to produce a weak labeling that identifies related forms has also been investigated (Schone and Jurafsky, 2000; Soricut and Och, 2015). Kann et al. (2016) perform unsupervised canonicalization of allomorphs, transforming words such as having to have ing, a task which is s"
W17-4107,Q16-1001,0,0.0182115,"onen et al., 2010; Grönroos et al., 2014; Sirts and Goldwater, 2013). Using vector space representations of words to produce a weak labeling that identifies related forms has also been investigated (Schone and Jurafsky, 2000; Soricut and Och, 2015). Kann et al. (2016) perform unsupervised canonicalization of allomorphs, transforming words such as having to have ing, a task which is somewhat related to the problem addressed in this paper. Many methods that tackle specific morphology-related NLP tasks implicitly learn some model of allomorphy. This includes semi-supervised vocabulary expansion (Faruqui et al., 2016), and morphological inflection from examples (Cotterell et al., 2016a). To our knowledge, the weakly supervised learning problem addressed in this paper has not been considered in the literature. Cotterell et al. (2015) present a closely related task. They investigate labeled morphological segmentation, that is, simultaneous segmentation and labeling of segments with morphological features. The crucial difference between our work and the work by Cotterell et al. (2015) is that our models are learned in a weakly supervised manner from plain word forms and sets of morphological features. In cont"
W17-4107,J01-2001,0,0.183776,", and a model of Kullback-Leibler divergence that favors that labels and allomorphs have similar distributions throughout a data set.1 We also compare the performance of the various methods to a baseline unsupervised model, Morfessor2 , augmented with the capacity to also provide labels of allomorphs in addition to segmenting. 1 Our code is freely available at https://github. com/mpsilfve/learn-allomorphs 2 http://www.cis.hut.fi/projects/ morpho/morfessor2.shtml 2 Related Work In the realm of natural language processing, morphological segmentation is a well-researched and established problem (Goldsmith (2001), Creutz and Lagus (2005), Poon et al. (2009), Dreyer and Eisner (2011), Ruokolainen et al. (2016)). While most approaches to pure segmentation are unsupervised, semi-supervised work usually assumes the availability of a limited number of gold segmentations (Dasgupta and Ng, 2007; Kohonen et al., 2010; Grönroos et al., 2014; Sirts and Goldwater, 2013). Using vector space representations of words to produce a weak labeling that identifies related forms has also been investigated (Schone and Jurafsky, 2000; Soricut and Och, 2015). Kann et al. (2016) perform unsupervised canonicalization of allom"
W17-4107,C14-1111,0,0.0230225,"menting. 1 Our code is freely available at https://github. com/mpsilfve/learn-allomorphs 2 http://www.cis.hut.fi/projects/ morpho/morfessor2.shtml 2 Related Work In the realm of natural language processing, morphological segmentation is a well-researched and established problem (Goldsmith (2001), Creutz and Lagus (2005), Poon et al. (2009), Dreyer and Eisner (2011), Ruokolainen et al. (2016)). While most approaches to pure segmentation are unsupervised, semi-supervised work usually assumes the availability of a limited number of gold segmentations (Dasgupta and Ng, 2007; Kohonen et al., 2010; Grönroos et al., 2014; Sirts and Goldwater, 2013). Using vector space representations of words to produce a weak labeling that identifies related forms has also been investigated (Schone and Jurafsky, 2000; Soricut and Och, 2015). Kann et al. (2016) perform unsupervised canonicalization of allomorphs, transforming words such as having to have ing, a task which is somewhat related to the problem addressed in this paper. Many methods that tackle specific morphology-related NLP tasks implicitly learn some model of allomorphy. This includes semi-supervised vocabulary expansion (Faruqui et al., 2016), and morphological"
W17-4107,K15-1017,0,0.078384,"iple morphological features. We treat the problem of joint segmentation and feature assignment as a search problem in the space of all possible segmentations and labelings of each word form in a (weakly) annotated corpus. The crucial constraint provided by the weak labeling is that not all labels can be present in a word form—the set of labels present for each inflected word must be restricted to those given by the resource. To our knowledge, this weakly supervised task has not previously been explored although joint segmentation and labeling has been explored in a fully supervised setting by Cotterell et al. (2015). To solve the problem, we explore global metrics that indirectly favor re-use of allomorphs according to the intuition given above. We formalize a generic objective function that scores the goodness of segmentations and labeling globally in a corpus. The scoring portion of this objective function is tested with several metrics: symmetric conditional probability, which favors that allomorphs be good predictors of labels and vice versa, a perceptron learner that weights allomorph-label association, a Rescorla-Wagner model based on classical conditioning that also learns such association weights"
W17-4107,N16-1080,0,0.06286,"). Using vector space representations of words to produce a weak labeling that identifies related forms has also been investigated (Schone and Jurafsky, 2000; Soricut and Och, 2015). Kann et al. (2016) perform unsupervised canonicalization of allomorphs, transforming words such as having to have ing, a task which is somewhat related to the problem addressed in this paper. Many methods that tackle specific morphology-related NLP tasks implicitly learn some model of allomorphy. This includes semi-supervised vocabulary expansion (Faruqui et al., 2016), and morphological inflection from examples (Cotterell et al., 2016a). To our knowledge, the weakly supervised learning problem addressed in this paper has not been considered in the literature. Cotterell et al. (2015) present a closely related task. They investigate labeled morphological segmentation, that is, simultaneous segmentation and labeling of segments with morphological features. The crucial difference between our work and the work by Cotterell et al. (2015) is that our models are learned in a weakly supervised manner from plain word forms and sets of morphological features. In contrast, Cotterell et al. (2015) learn segmentation models in a fully s"
W17-4107,D16-1097,0,0.0142084,"on is a well-researched and established problem (Goldsmith (2001), Creutz and Lagus (2005), Poon et al. (2009), Dreyer and Eisner (2011), Ruokolainen et al. (2016)). While most approaches to pure segmentation are unsupervised, semi-supervised work usually assumes the availability of a limited number of gold segmentations (Dasgupta and Ng, 2007; Kohonen et al., 2010; Grönroos et al., 2014; Sirts and Goldwater, 2013). Using vector space representations of words to produce a weak labeling that identifies related forms has also been investigated (Schone and Jurafsky, 2000; Soricut and Och, 2015). Kann et al. (2016) perform unsupervised canonicalization of allomorphs, transforming words such as having to have ing, a task which is somewhat related to the problem addressed in this paper. Many methods that tackle specific morphology-related NLP tasks implicitly learn some model of allomorphy. This includes semi-supervised vocabulary expansion (Faruqui et al., 2016), and morphological inflection from examples (Cotterell et al., 2016a). To our knowledge, the weakly supervised learning problem addressed in this paper has not been considered in the literature. Cotterell et al. (2015) present a closely related t"
W17-4107,W10-2210,0,0.0213661,"phs in addition to segmenting. 1 Our code is freely available at https://github. com/mpsilfve/learn-allomorphs 2 http://www.cis.hut.fi/projects/ morpho/morfessor2.shtml 2 Related Work In the realm of natural language processing, morphological segmentation is a well-researched and established problem (Goldsmith (2001), Creutz and Lagus (2005), Poon et al. (2009), Dreyer and Eisner (2011), Ruokolainen et al. (2016)). While most approaches to pure segmentation are unsupervised, semi-supervised work usually assumes the availability of a limited number of gold segmentations (Dasgupta and Ng, 2007; Kohonen et al., 2010; Grönroos et al., 2014; Sirts and Goldwater, 2013). Using vector space representations of words to produce a weak labeling that identifies related forms has also been investigated (Schone and Jurafsky, 2000; Soricut and Och, 2015). Kann et al. (2016) perform unsupervised canonicalization of allomorphs, transforming words such as having to have ing, a task which is somewhat related to the problem addressed in this paper. Many methods that tackle specific morphology-related NLP tasks implicitly learn some model of allomorphy. This includes semi-supervised vocabulary expansion (Faruqui et al., 2"
W17-4107,W00-0712,0,0.201791,"tural language processing, morphological segmentation is a well-researched and established problem (Goldsmith (2001), Creutz and Lagus (2005), Poon et al. (2009), Dreyer and Eisner (2011), Ruokolainen et al. (2016)). While most approaches to pure segmentation are unsupervised, semi-supervised work usually assumes the availability of a limited number of gold segmentations (Dasgupta and Ng, 2007; Kohonen et al., 2010; Grönroos et al., 2014; Sirts and Goldwater, 2013). Using vector space representations of words to produce a weak labeling that identifies related forms has also been investigated (Schone and Jurafsky, 2000; Soricut and Och, 2015). Kann et al. (2016) perform unsupervised canonicalization of allomorphs, transforming words such as having to have ing, a task which is somewhat related to the problem addressed in this paper. Many methods that tackle specific morphology-related NLP tasks implicitly learn some model of allomorphy. This includes semi-supervised vocabulary expansion (Faruqui et al., 2016), and morphological inflection from examples (Cotterell et al., 2016a). To our knowledge, the weakly supervised learning problem addressed in this paper has not been considered in the literature. Cottere"
W17-4107,Q13-1021,0,0.0152242,"freely available at https://github. com/mpsilfve/learn-allomorphs 2 http://www.cis.hut.fi/projects/ morpho/morfessor2.shtml 2 Related Work In the realm of natural language processing, morphological segmentation is a well-researched and established problem (Goldsmith (2001), Creutz and Lagus (2005), Poon et al. (2009), Dreyer and Eisner (2011), Ruokolainen et al. (2016)). While most approaches to pure segmentation are unsupervised, semi-supervised work usually assumes the availability of a limited number of gold segmentations (Dasgupta and Ng, 2007; Kohonen et al., 2010; Grönroos et al., 2014; Sirts and Goldwater, 2013). Using vector space representations of words to produce a weak labeling that identifies related forms has also been investigated (Schone and Jurafsky, 2000; Soricut and Och, 2015). Kann et al. (2016) perform unsupervised canonicalization of allomorphs, transforming words such as having to have ing, a task which is somewhat related to the problem addressed in this paper. Many methods that tackle specific morphology-related NLP tasks implicitly learn some model of allomorphy. This includes semi-supervised vocabulary expansion (Faruqui et al., 2016), and morphological inflection from examples (C"
W17-4107,N15-1186,0,0.0288879,"morphological segmentation is a well-researched and established problem (Goldsmith (2001), Creutz and Lagus (2005), Poon et al. (2009), Dreyer and Eisner (2011), Ruokolainen et al. (2016)). While most approaches to pure segmentation are unsupervised, semi-supervised work usually assumes the availability of a limited number of gold segmentations (Dasgupta and Ng, 2007; Kohonen et al., 2010; Grönroos et al., 2014; Sirts and Goldwater, 2013). Using vector space representations of words to produce a weak labeling that identifies related forms has also been investigated (Schone and Jurafsky, 2000; Soricut and Och, 2015). Kann et al. (2016) perform unsupervised canonicalization of allomorphs, transforming words such as having to have ing, a task which is somewhat related to the problem addressed in this paper. Many methods that tackle specific morphology-related NLP tasks implicitly learn some model of allomorphy. This includes semi-supervised vocabulary expansion (Faruqui et al., 2016), and morphological inflection from examples (Cotterell et al., 2016a). To our knowledge, the weakly supervised learning problem addressed in this paper has not been considered in the literature. Cotterell et al. (2015) present"
W17-4107,N09-1024,0,0.029905,"e that favors that labels and allomorphs have similar distributions throughout a data set.1 We also compare the performance of the various methods to a baseline unsupervised model, Morfessor2 , augmented with the capacity to also provide labels of allomorphs in addition to segmenting. 1 Our code is freely available at https://github. com/mpsilfve/learn-allomorphs 2 http://www.cis.hut.fi/projects/ morpho/morfessor2.shtml 2 Related Work In the realm of natural language processing, morphological segmentation is a well-researched and established problem (Goldsmith (2001), Creutz and Lagus (2005), Poon et al. (2009), Dreyer and Eisner (2011), Ruokolainen et al. (2016)). While most approaches to pure segmentation are unsupervised, semi-supervised work usually assumes the availability of a limited number of gold segmentations (Dasgupta and Ng, 2007; Kohonen et al., 2010; Grönroos et al., 2014; Sirts and Goldwater, 2013). Using vector space representations of words to produce a weak labeling that identifies related forms has also been investigated (Schone and Jurafsky, 2000; Soricut and Och, 2015). Kann et al. (2016) perform unsupervised canonicalization of allomorphs, transforming words such as having to h"
W18-0209,K17-2001,1,0.829145,"a given sentence context. Because hand-crafted morphological analyzers have been shown to improve the performance of neural taggers (Sagot and Martínez Alonso, 2017), the task of data-driven morphological analysis is, nevertheless, important. The task explored in this paper is closely related to the construction of morphological guessers (Lindén, 2009), where the aim is to guess the inflectional type of a word. To the best of our knowledge deep learning methods have, however, not been applied to this task. In contrast, there is a growing body of work on deep learning for word form generation (Cotterell et al., 2017, 2016). In word form generation, or morphological reinflection, the aim is to generate word forms given lemmas and morphological analyses. Therefore, it can be seen as a natural counterpart to morphological analysis. Our work is inspired by the encoder-decoder models commonly applied in morphological reinflection (for example Kann and Schütze (2017)) but the task at hand is naturally quite different. Several approaches have been explored for returning one analysis, or a small set of possible analyses, for a word form in context. For example, Kudo et al. (2004) apply 101 Conditional Random Fie"
W18-0209,W04-3230,0,0.0912063,"ng for word form generation (Cotterell et al., 2017, 2016). In word form generation, or morphological reinflection, the aim is to generate word forms given lemmas and morphological analyses. Therefore, it can be seen as a natural counterpart to morphological analysis. Our work is inspired by the encoder-decoder models commonly applied in morphological reinflection (for example Kann and Schütze (2017)) but the task at hand is naturally quite different. Several approaches have been explored for returning one analysis, or a small set of possible analyses, for a word form in context. For example, Kudo et al. (2004) apply 101 Conditional Random Fields for morphological analysis of Japanese but their system only returns one tokenization for a sentence and one analysis per token. This is not the same task as the one we are exploring, where the objective is to return the complete set of possible analyses. Similar in spirit is the work on Kazakh morphological analysis by Makhambetov et al. (2015). Their system, based on Hidden Markov Models, returns a subset of the analyses of a token which could plausibly occur in a given context. Sequence models are a natural choice when the aim is to generate one analysis"
W18-0209,W15-1821,0,0.0565816,"o hand-crafted analyzers, namely, data-driven morphological analyzers which are learned from annotated training data. In our case, the training data consists of words and complete sets of analyses. During test time, the system takes a Finnish word such as kisaan (‘into the competition’ or ‘I am competing’) as input and gives a set of analyses {Noun+Sg+Ill, Verb+Act+Indv+Pres+Sg1} as output. We present experiments in data-driven morphological analysis of Finnish. We learn to mimic the OMorFi analyzer (Pirinen et al., 2017) on the Finnish portion of the Universal Dependency treebank collection (Pyysalo et al., 2015). The data sets and OMorFi analyzer are further discussed in Section 3. We use a deep learning model encompassing a character-level recurrent model, which maps words onto sets of analyses as explained in Section 4. Our results, described in Section 5, show that this line of research is encouraging. We present related work in Section 2 and present concluding remarks in Section 6. 2 Related Work The task of data-driven morphological analysis has received far less attention than morphological tagging and disambiguation which aim at producing exactly one analysis, which is correct in a given sente"
W18-0209,W17-6304,0,0.0408634,"Missing"
W18-0314,N16-1077,0,0.0386412,"el, coronal/non-coronal, and front/back distinctions from unlabeled phonetic data or orthographic data from phonemic writing systems. 137 Learning features directly from waveform representations (see e.g. Lin (2005))—while not addressed in this paper—is also highly relevant to the current study, and is indeed a question to which some lower-level, speech signal-based form of distributed representations may be adapted. The idea explored in this paper—that phonemes (or graphemes) might exhibit linguistically apt correlations in an embedding space—has been implied by earlier research, for example Faruqui et al. (2016). In that work, a neural encoder-decoder model (Cho et al., 2014; Sutskever et al., 2014) was trained to perform a transformation of words from their citation forms to a ‘target’ inflected form and, after training, the vowels in the embedding layer of the long-short term memory (LSTM) neural model trained for Finnish were found to clearly group themselves according to known harmony patterns in the language. Li et al. (2016) take advantage of phoneme transcriptions in a neural speech synthesis application, showing improvements on this task and indicating that similar phonemes in a bidirectional"
W18-0314,K17-1030,1,0.843096,"nt often take advantage of such patterns (Guy, 1991; Sukhotin, 1962, 1973). Local co-occurrence counts have also been analyzed through spectral methods, such as singular value decomposition (Moler and Morrison, 1983; Goldsmith and Xanthos, 2009; Thaine and Penn, 2017), revealing that significant latent structure can be recovered, mainly with respect to vowels and consonants. Recent works along the same lines of inquiry include Kim and Snyder (2013) that presents a Bayesian approach that simultaneously clusters languages and reveals consonant/vowel/nasal distinctions in an unsupervised manner. Hulden (2017) shows that an algorithm based on the obligatory contour principle (Leben, 1973) and an additional assumption of phonological tiers being present (Goldsmith, 1976) robustly reveals at least consonant/vowel, coronal/non-coronal, and front/back distinctions from unlabeled phonetic data or orthographic data from phonemic writing systems. 137 Learning features directly from waveform representations (see e.g. Lin (2005))—while not addressed in this paper—is also highly relevant to the current study, and is indeed a question to which some lower-level, speech signal-based form of distributed represen"
W18-0314,P13-1150,0,0.0284827,"way in phonemic writing systems is an early observation that some articulatory features can be recovered in an unsupervised way. Algorithms for cryptographic decipherment often take advantage of such patterns (Guy, 1991; Sukhotin, 1962, 1973). Local co-occurrence counts have also been analyzed through spectral methods, such as singular value decomposition (Moler and Morrison, 1983; Goldsmith and Xanthos, 2009; Thaine and Penn, 2017), revealing that significant latent structure can be recovered, mainly with respect to vowels and consonants. Recent works along the same lines of inquiry include Kim and Snyder (2013) that presents a Bayesian approach that simultaneously clusters languages and reveals consonant/vowel/nasal distinctions in an unsupervised manner. Hulden (2017) shows that an algorithm based on the obligatory contour principle (Leben, 1973) and an additional assumption of phonological tiers being present (Goldsmith, 1976) robustly reveals at least consonant/vowel, coronal/non-coronal, and front/back distinctions from unlabeled phonetic data or orthographic data from phonemic writing systems. 137 Learning features directly from waveform representations (see e.g. Lin (2005))—while not addressed"
W18-0314,W14-1618,0,0.23462,"1 1 “the meaning of a word is its use in the language” (Wittgenstein, 1953, p.43) Firth,2 Harris,3 and other contemporaries. Often overlooked is that this hypothesis among linguists has extended itself much wider to include phonology and grammar: ”all elements of speech (phonological, lexical, and grammatical) are now to be defined and classified in terms of their relations to one another” (Haas, 1954, p.54). Given the successes of distributional models not only in specifying semantic similarity, but also addressing proportional analogy tasks (Turney and Pantel, 2010; Mikolov et al., 2013a,b; Levy et al., 2014), we want to investigate if distributional representations of phonemes induce a similarly coherent space as lexical items do, and if the properties of such spaces conform to linguistic expectations, a question posed in another form as early as in Fischer-Jørgensen (1952). In particular, we address two questions: (1) whether learned vector representations of phonemes are congruent with commonly assumed binary phonological distinctive feature spaces, and (2) whether a proportional analogy of the type a:b::c:d (a is to b as c is to d) discovered in a phoneme embedding space is also a valid analog"
W18-0314,N13-1090,0,0.393283,"he likes of Wittgenstein,1 1 “the meaning of a word is its use in the language” (Wittgenstein, 1953, p.43) Firth,2 Harris,3 and other contemporaries. Often overlooked is that this hypothesis among linguists has extended itself much wider to include phonology and grammar: ”all elements of speech (phonological, lexical, and grammatical) are now to be defined and classified in terms of their relations to one another” (Haas, 1954, p.54). Given the successes of distributional models not only in specifying semantic similarity, but also addressing proportional analogy tasks (Turney and Pantel, 2010; Mikolov et al., 2013a,b; Levy et al., 2014), we want to investigate if distributional representations of phonemes induce a similarly coherent space as lexical items do, and if the properties of such spaces conform to linguistic expectations, a question posed in another form as early as in Fischer-Jørgensen (1952). In particular, we address two questions: (1) whether learned vector representations of phonemes are congruent with commonly assumed binary phonological distinctive feature spaces, and (2) whether a proportional analogy of the type a:b::c:d (a is to b as c is to d) discovered in a phoneme embedding space"
W18-0314,W17-4112,0,0.0162977,"ored in the literature for unsupervised discovery of phonological features. The observation of Markov (1913, 2006) that vowels and consonants tend to alternate in a statistically robust way in phonemic writing systems is an early observation that some articulatory features can be recovered in an unsupervised way. Algorithms for cryptographic decipherment often take advantage of such patterns (Guy, 1991; Sukhotin, 1962, 1973). Local co-occurrence counts have also been analyzed through spectral methods, such as singular value decomposition (Moler and Morrison, 1983; Goldsmith and Xanthos, 2009; Thaine and Penn, 2017), revealing that significant latent structure can be recovered, mainly with respect to vowels and consonants. Recent works along the same lines of inquiry include Kim and Snyder (2013) that presents a Bayesian approach that simultaneously clusters languages and reveals consonant/vowel/nasal distinctions in an unsupervised manner. Hulden (2017) shows that an algorithm based on the obligatory contour principle (Leben, 1973) and an additional assumption of phonological tiers being present (Goldsmith, 1976) robustly reveals at least consonant/vowel, coronal/non-coronal, and front/back distinctions"
W18-0314,W16-2010,0,\N,Missing
W18-3904,Q17-1010,0,0.0239651,"beddings and pretrained embeddings and the system emits morphological tags using an LSTM generator. This allows us to both emit tags, which we have not seen in the training data, and emit combinations of several tags for one token. This is necessary for handling contractions present in the shared task datasets, as explained above. Embedding layer Our word embedding layer combines three types of word embeddings: pretrained word embeddings, randomly initialized word embeddings and character-based embeddings. See Figure 3 for a visualization. Pretrained embeddings are initialized using FastText (Bojanowski et al., 2017) which treats word forms as a bags of character n-grams. We use FastText because it can provide an embedding vector both for tokens that were observed during training and for other tokens. This is important when dealing with morphologically complex languages, where out-of-vocabulary (OOV) rates are typically high. We train pretrained FastText embeddings using large quantities of plain text. In addition to pretrained embeddings, we use regular randomly initialized token based embeddings. It is common practice to include both types of embeddings in a tagger. 2 Many authors including (Heigold et"
W18-3904,K17-3002,0,0.0206555,"son to believe that our approach also captures information about complete MULTEXT-East tags. The paper is structured in the following way: In Section 2 we present related approaches neural to morphosyntactic tagging and tagging in the social media domain. In Section 3, we present our LSTM tagger. Section 4 presents the data sets used in the VarDial task and Section 5 presents our experiments 38 and results. Finally, we present discussion and directions for future work in Section 6 and conclude the paper in Section 7. 2 Related Work Our system is inspired by the neural POS tagger introduced by Dozat et al. (2017), however, we have extended their approach to handle morphological tagging. In the past two years, POS tagging for morphologically complex languages has received a fair amount of attention. Starting with the work by Plank et al. (2016a), neural approaches, particularly bidirectional LSTM taggers, have dominated the field. This is exemplified by the entry of Dozat et al. (2017) for the 2017 CoNLL shared task on multilingual parsing, where their neural POS tagger delivered the best results by far for nearly all languages (Zeman et al., 2017). Even though work on neural POS tagging has received m"
W18-3904,P07-2053,0,0.0774178,"Missing"
W18-3904,W14-0405,0,0.0316891,"Missing"
W18-3904,D13-1032,0,0.0813836,"Missing"
W18-3904,P16-2067,0,0.0455276,"social media domain. In Section 3, we present our LSTM tagger. Section 4 presents the data sets used in the VarDial task and Section 5 presents our experiments 38 and results. Finally, we present discussion and directions for future work in Section 6 and conclude the paper in Section 7. 2 Related Work Our system is inspired by the neural POS tagger introduced by Dozat et al. (2017), however, we have extended their approach to handle morphological tagging. In the past two years, POS tagging for morphologically complex languages has received a fair amount of attention. Starting with the work by Plank et al. (2016a), neural approaches, particularly bidirectional LSTM taggers, have dominated the field. This is exemplified by the entry of Dozat et al. (2017) for the 2017 CoNLL shared task on multilingual parsing, where their neural POS tagger delivered the best results by far for nearly all languages (Zeman et al., 2017). Even though work on neural POS tagging has received more attention, there are a number of papers on neural morphosyntactic tagging. Heigold et al. (2016a) evaluate several architectures for morphosyntactic tagging2 of German and Czech. They find that pretrained word embeddings bring lar"
W18-3904,P14-2043,1,0.867734,"Missing"
W18-3904,W18-3901,0,0.0926786,"Missing"
W18-5818,K17-2003,0,0.0571645,"Missing"
W18-5818,D15-1166,0,0.0432203,"segments. c a t s <EOS> E E E E E E Decoder RNN Attn Encoder Bi-RNN E E E E E E E <EOS> c a t N PL <EOS> Figure 1: The encoder-decoder with an attention mechanism used for morphological inflection [k,æ,t] -syl -son … -ant +hi +syl +son … 0ant -hi -syl -son … +ant -hi Figure 2: PanPhon transforms a sequence of IPA segments into a matrix of features output embedding, and all of the encoder states ei ∈ Encoder. We then use an attention mechanism (Bahdanau et al., 2014) to ‘attend’ over the encoder states, assigning a score to each ei given the previous decoder state dj−1 . The scoring function (Luong et al., 2015) is calculated as Related Work Phonetic distributional vectors have been explored for their effectiveness in several NLP applications; especially for informing scenarios that utilize borrowing or transfer learning (Tsvetkov et al., 2016). Phonological distinctive features have also been successfully used to inform NER (Bharadwaj et al., 2016). However, to our knowledge, there does not seem to be work in learning distributional properties of phonological features that compares them directly to vectors of IPA segments. 2 <EOS> score(ei ; dj−1 ) = tanh([dj−1 ; ei ] × W ) (1) where W is a paramete"
W18-5818,K17-2002,0,0.0587406,"Missing"
W18-5818,D16-1153,0,0.0658242,"f features output embedding, and all of the encoder states ei ∈ Encoder. We then use an attention mechanism (Bahdanau et al., 2014) to ‘attend’ over the encoder states, assigning a score to each ei given the previous decoder state dj−1 . The scoring function (Luong et al., 2015) is calculated as Related Work Phonetic distributional vectors have been explored for their effectiveness in several NLP applications; especially for informing scenarios that utilize borrowing or transfer learning (Tsvetkov et al., 2016). Phonological distinctive features have also been successfully used to inform NER (Bharadwaj et al., 2016). However, to our knowledge, there does not seem to be work in learning distributional properties of phonological features that compares them directly to vectors of IPA segments. 2 <EOS> score(ei ; dj−1 ) = tanh([dj−1 ; ei ] × W ) (1) where W is a parameter matrix that is learned during training, and [x; y] indicates the concatenation of x and y. These scores are then normalized by applying a softmax over all encoder states in Encoder to compute each i,j−1 . Finally, the attention vector is computed as the weighted mean of all encoder states according to their normalized score: A(dj−1 , E) ="
W18-5818,C16-1328,0,0.0513823,"ll have been found. In the medium setting, the difference in accuracy is much more apparent. This is due to the fact that all of the top performing systems in the shared task also used either some type of data augmentation method (Zhou and Neubig (2017), Silfverberg et al. (2017), Sudhakar and Kumar (2017), Kann and Sch¨utze (2017), Bergmanis et al. (2017)) a hard alignment method (Makarov et al., 2017), or both (Nicolai et al., 2017). These results illustrate the common observation that neural systems require a large amount of data to be very accurate, We then use the Python library PanPhon (Mortensen et al., 2016), which maps IPA segments to features as in Figure 2, to obtain vectors of phonological distinctive features. The features are represented numerically whereby each index of the vector corresponds to a specific feature such as [±coronal] and stores a value from the set {1, 0, -1}. These values correspond to ‘exhibits feature’, ‘unspecified for given class of sounds’, and ‘does not exhibit feature’, respectively. In practice we map -1 to 0 to obtain strictly binary feature vectors. Now, each IPA segment can be represented as a vector v which has a 1 for each feature that it exhibits, and a 0 oth"
W18-5818,K17-2008,0,0.0264296,"red task did (Cotterell et al., 2017), and that this is a comparison to the maximum score of 25 systems per language, which increases the likelihood that the optimal initialization will have been found. In the medium setting, the difference in accuracy is much more apparent. This is due to the fact that all of the top performing systems in the shared task also used either some type of data augmentation method (Zhou and Neubig (2017), Silfverberg et al. (2017), Sudhakar and Kumar (2017), Kann and Sch¨utze (2017), Bergmanis et al. (2017)) a hard alignment method (Makarov et al., 2017), or both (Nicolai et al., 2017). These results illustrate the common observation that neural systems require a large amount of data to be very accurate, We then use the Python library PanPhon (Mortensen et al., 2016), which maps IPA segments to features as in Figure 2, to obtain vectors of phonological distinctive features. The features are represented numerically whereby each index of the vector corresponds to a specific feature such as [±coronal] and stores a value from the set {1, 0, -1}. These values correspond to ‘exhibits feature’, ‘unspecified for given class of sounds’, and ‘does not exhibit feature’, respectively."
W18-5818,W18-0314,1,0.739806,"from phonological features should quickly be able to generalize that this English past tense is realized as /t/ before voiceless segments. Similarly, in the example of ”rob”: /ôAb/ → /ôAbd/, the generated /d/ can be conditioned on [+voice] rather than the individual segment /b/. An alternative hypothesis is that the proposed distinctive feature representation may, however, not have such a profound effect on the inflection model. This is because distributional representations of IPA segments or phonemic graphemes have been shown to capture good approximations of the distinctive feature space (Silfverberg et al., 2018). In order to test these two hypotheses, we experiment on a subset of data provided by task 1 of the CoNLL-SIGMORPHON 2017 Shared Task on Universal Morphological Reinflection (Cotterell et al., 2017), which introduced 42 more languages than the year before (Cotterell et al., 2016) for a total of 52 languages. We use an existing tool to perform G2P on the data, and, as a second step, to produce distinctive feature vectors from the resulting IPA segments. We evaluate the resulting models on their ability to generate IPA segments. c a t s <EOS> E E E E E E Decoder RNN Attn Encoder Bi-RNN E E E E"
W18-5818,K17-2010,1,0.824103,"r each language. The slight degradation in performance can be attributed to the fact that we did not use ensemble voting, as the top performing systems in the shared task did (Cotterell et al., 2017), and that this is a comparison to the maximum score of 25 systems per language, which increases the likelihood that the optimal initialization will have been found. In the medium setting, the difference in accuracy is much more apparent. This is due to the fact that all of the top performing systems in the shared task also used either some type of data augmentation method (Zhou and Neubig (2017), Silfverberg et al. (2017), Sudhakar and Kumar (2017), Kann and Sch¨utze (2017), Bergmanis et al. (2017)) a hard alignment method (Makarov et al., 2017), or both (Nicolai et al., 2017). These results illustrate the common observation that neural systems require a large amount of data to be very accurate, We then use the Python library PanPhon (Mortensen et al., 2016), which maps IPA segments to features as in Figure 2, to obtain vectors of phonological distinctive features. The features are represented numerically whereby each index of the vector corresponds to a specific feature such as [±coronal] and stores a value f"
W18-5818,K17-2007,0,0.0276726,"degradation in performance can be attributed to the fact that we did not use ensemble voting, as the top performing systems in the shared task did (Cotterell et al., 2017), and that this is a comparison to the maximum score of 25 systems per language, which increases the likelihood that the optimal initialization will have been found. In the medium setting, the difference in accuracy is much more apparent. This is due to the fact that all of the top performing systems in the shared task also used either some type of data augmentation method (Zhou and Neubig (2017), Silfverberg et al. (2017), Sudhakar and Kumar (2017), Kann and Sch¨utze (2017), Bergmanis et al. (2017)) a hard alignment method (Makarov et al., 2017), or both (Nicolai et al., 2017). These results illustrate the common observation that neural systems require a large amount of data to be very accurate, We then use the Python library PanPhon (Mortensen et al., 2016), which maps IPA segments to features as in Figure 2, to obtain vectors of phonological distinctive features. The features are represented numerically whereby each index of the vector corresponds to a specific feature such as [±coronal] and stores a value from the set {1, 0, -1}. The"
W18-5818,N16-1161,0,0.0288235,"ll have been found. In the medium setting, the difference in accuracy is much more apparent. This is due to the fact that all of the top performing systems in the shared task also used either some type of data augmentation method (Zhou and Neubig (2017), Silfverberg et al. (2017), Sudhakar and Kumar (2017), Kann and Sch¨utze (2017), Bergmanis et al. (2017)) a hard alignment method (Makarov et al., 2017), or both (Nicolai et al., 2017). These results illustrate the common observation that neural systems require a large amount of data to be very accurate, We then use the Python library PanPhon (Mortensen et al., 2016), which maps IPA segments to features as in Figure 2, to obtain vectors of phonological distinctive features. The features are represented numerically whereby each index of the vector corresponds to a specific feature such as [±coronal] and stores a value from the set {1, 0, -1}. These values correspond to ‘exhibits feature’, ‘unspecified for given class of sounds’, and ‘does not exhibit feature’, respectively. In practice we map -1 to 0 to obtain strictly binary feature vectors. Now, each IPA segment can be represented as a vector v which has a 1 for each feature that it exhibits, and a 0 oth"
W18-5818,K17-2005,0,0.0284175,"t shared task systems for each language. The slight degradation in performance can be attributed to the fact that we did not use ensemble voting, as the top performing systems in the shared task did (Cotterell et al., 2017), and that this is a comparison to the maximum score of 25 systems per language, which increases the likelihood that the optimal initialization will have been found. In the medium setting, the difference in accuracy is much more apparent. This is due to the fact that all of the top performing systems in the shared task also used either some type of data augmentation method (Zhou and Neubig (2017), Silfverberg et al. (2017), Sudhakar and Kumar (2017), Kann and Sch¨utze (2017), Bergmanis et al. (2017)) a hard alignment method (Makarov et al., 2017), or both (Nicolai et al., 2017). These results illustrate the common observation that neural systems require a large amount of data to be very accurate, We then use the Python library PanPhon (Mortensen et al., 2016), which maps IPA segments to features as in Figure 2, to obtain vectors of phonological distinctive features. The features are represented numerically whereby each index of the vector corresponds to a specific feature such as [±co"
W18-6011,P17-1080,0,0.0204505,"ph represents both categories as a single templatic value. 2. We discard any values that UniMorph doesn’t annotate for a particular part of speech, like gender and number in French verb participles, or German noun genders. Our approach to converting UD MSDs to UniMorph MSDs begins with the attribute-value lookup, then amends it on a language-specific basis. Alterations informed by the MSD and the word form, like insertion, substitution, and deletion, increase the number of agreeing annotations. They are critical for work that examines the MSD monolithically instead of feature-by-feature (e.g. Belinkov et al., 2017; Cotterell and Heigold, 2017): Without exact matches, converting the individual tags becomes hollow. 3. We make MSD additions when they are unambiguously implied by the resources, like PFV to accompany PST in Spanish “pasado simple”, but PST to accompany IPFV in Spanish “pasado continuo”. 4. We also incorporate fixes using information outside of the MSD like the L G S PEC 1 tag for Spanish’s “-ra” forms, as described in §4, and other language-specific corrections, like mapping the various dative cases to the crosslingually comparable case annotations used in UniMorph. Beginning our process, w"
W18-6011,J93-2003,0,0.0985893,"ords to be a match if their form and lemma are present in both resources. Syncretism allows a single surface form to realize multiple MSDs (Spanish “mandaba” can be first- or third-person), so we define success as the computed MSD matching any of the word’s UniMorph MSDs. This gives rise to an equation for recall: of the word–lemma pairs found in both resources, how many of their UniMorph-converted MSDs are present in the UniMorph tables? Why not a learned mapping? One can imagine learning the UniMorph MSD corresponding to a UD dataset’s MSD by a set-to-set translation model like IBM Model 1 (Brown et al., 1993). Unfortunately, statistical (and especially neural) machine translation generalizes in unreliable ways. Our goal is a straightforward, easily manipulable and extensible conversion that prioritizes correctness over coverage. 6 Intrinsic evaluation Experiments Why no held-out test set? Our problem here is not a learning problem, so the question is ill-posed. There is no training set, and the two resources for a given language make up a test set. The quality of our model—the conversion tool—comes from how well we encode prior knowledge about the relationship between the UD and UniMorph corpora."
W18-6011,D17-1078,1,0.850043,"gories as a single templatic value. 2. We discard any values that UniMorph doesn’t annotate for a particular part of speech, like gender and number in French verb participles, or German noun genders. Our approach to converting UD MSDs to UniMorph MSDs begins with the attribute-value lookup, then amends it on a language-specific basis. Alterations informed by the MSD and the word form, like insertion, substitution, and deletion, increase the number of agreeing annotations. They are critical for work that examines the MSD monolithically instead of feature-by-feature (e.g. Belinkov et al., 2017; Cotterell and Heigold, 2017): Without exact matches, converting the individual tags becomes hollow. 3. We make MSD additions when they are unambiguously implied by the resources, like PFV to accompany PST in Spanish “pasado simple”, but PST to accompany IPFV in Spanish “pasado continuo”. 4. We also incorporate fixes using information outside of the MSD like the L G S PEC 1 tag for Spanish’s “-ra” forms, as described in §4, and other language-specific corrections, like mapping the various dative cases to the crosslingually comparable case annotations used in UniMorph. Beginning our process, we relied on documentation of t"
W18-6011,K18-3001,1,0.882683,"Missing"
W18-6011,K17-2001,1,0.892721,"Missing"
W18-6011,W17-0405,0,0.0486603,"Missing"
W18-6011,W05-0807,1,0.760167,"Missing"
W18-6011,L18-1293,1,0.652734,"Missing"
W18-6011,E17-2018,1,0.849189,"Morph’s schema does not indicate the type of pronoun (demonstrative, interrogative, etc.), and when lexical information is not recorded in UniMorph, we delete it from the MSD during transformation. On the other hand, UniMorph’s atomic tags have more parts to guess, but they are often related. (E.g. I PFV always entails P ST in Spanish.) Altogether, these forces seem to have little impact on tagging performance. 8 In addition to using the number of extra rules as a proxy for harmony between resources, one could perform cross-lingual projection of morphological tags (Dr´abek and Yarowsky, 2005; Kirov et al., 2017). Our approach succeeds even without parallel corpora. 9 Conclusion and Future Work We created a tool for annotating Universal Dependencies CoNLL-U files with UniMorph annotations. Our tool is ready to use off-the-shelf today, requires no training, and is deterministic. While under-specification necessitates a lossy and imperfect conversion, ours is interpretable. Patterns of mistakes can be identified and ameliorated. The tool allows a bridge between resources annotated in the Universal Dependencies and Universal Morphology (UniMorph) schemata. As the Universal Dependencies project provides a"
W18-6011,L16-1498,1,0.848261,"directly comparable across languages. Its features are informed by a distinction between universal categories, which are widespread and psychologically “real” to speakers; and comparative concepts, only used by linguistic typologists to compare languages (Haspelmath, 2010). Additionally, it strives for identity of meaning across languages, not simply similarity of terminology. As a prime example, it does not regularly label a dative case for nouns, for reasons explained in depth by Haspelmath (2010).4 The UniMorph resources for a language contain complete paradigms extracted from Wiktionary (Kirov et al., 2016, 2018). Word types are annotated to form a database, mapping a lemma–tag pair to a surface form. The schema is explained in detail in Sylak-Glassman (2016). It has been used in the SIGMORPHON shared task (Cotterell et al., 2016) and the CoNLL–SIGMORPHON shared tasks (Cotterell et al., 2017, 2018). Several components of the UniMorph schema have been adopted by UD.5 Universal Dependencies The Universal Dependencies morphological schema comprises part of speech and 23 additional attributes (also called features in UD) annotating meaning or syntax, as well as language-specific attributes. In orde"
W18-6011,P18-1247,0,0.200895,"tated datasets. UD’s v2.1 release (Nivre et al., 2017) has 102 treebanks in 60 languages. The large resource, constructed by independent parties, evinces problems in the goal of a universal inventory of annotations. Annotators may choose to omit certain values (like the coerced gender of refrescante in Figure 1), and they may disagree on how a linguistic concept is encoded. (See, e.g., Haspelmath’s (2010) description of the dative case.) Additionally, many of the treebanks “were created by fully- or semi-automatic conversion from treebanks with less comprehensive annotation schemata than UD” (Malaviya et al., 2018). For instance, the Spanish word “vas” “you go” is incorrectly labeled G ENDER : F EM|N UMBER : P L because it ends in a character sequence which is common among feminine plural nouns. (Nevertheless, the part of speech field for “vas” is correct.) UniMorph’s development is more centralized and pipelined.7 Inflectional paradigms are scraped from Wiktionary, annotators map positions in the scraped data to MSDs, and the mapping is automatically applied to all of the scraped paradigms. Because annotators handle languages they are familiar with (or related ones), realization of the schema is also d"
W18-6011,J93-2004,0,0.0604809,"a given lemma and part of speech gives a paradigm: a mapping from slots to surface forms. Regular English verbs have five slots in their paradigm (Long, 1957), which we illustrate for the verb prove, using simple labels for the forms in Table 1. A morphosyntactic schema prescribes how language can be annotated—giving stricter categories than our simple labels for prove—and can vary in the level of detail provided. Part of speech tags are an example of a very coarse schema, ignoring details of person, gender, and number. A slightly finer-grained schema for English is the Penn Treebank tagset (Marcus et al., 1993), which includes signals for English morphology. For instance, its VBZ tag pertains to the specially inflected 3rd-person singular, present-tense verb form (e.g. “proves” in Table 1). If the tag in a schema is detailed enough that it exactly specifies a slot in a paradigm, it is • We detail a deterministic mapping from UD morphological annotations to UniMorph. Language-specific edits of the tags in 31 languages increase harmony between converted UD and existing UniMorph data (§5). • We provide an implementation of this mapping and post-editing, which replaces the UD features in a CoNLL-U file"
W18-6011,W17-0419,0,0.0599591,"Missing"
W18-6011,petrov-etal-2012-universal,0,0.0495453,"nguage. Our approach, by contrast, is a direct flight from the source to the target.) Because UniMorph corpora are noisy, the encoding from the interlingua would have to be rewritten for each target. Further, decoding the UD MSD into the interlingua cannot leverage external information like the lemma and form. The creators of HamleDT sought to harmonize dependency annotations among treebanks, similar to our goal of harmonizing across resources (Zeman et al., 2014). The treebanks they sought to harmonize used multiple diverse annotation schemes, which the authors unified under a single scheme. Petrov et al. (2012) present mappings into a coarse, “universal” part of speech for 22 languages. Working with POS tags rather than morphological tags (which have far more dimensions), their space of options to harmonize is much smaller than ours. Our extrinsic evaluation is most in line with the paradigm of Wisniewski and Lacroix (2017) (and similar work therein), who compare syntactic parser performance on UD treebanks annotated with two styles of dependency representation. Our problem differs, though, in that the dependency representations express different relationships, while our two schemata vastly overlap."
W18-6011,W17-0412,0,0.0275171,"ject releases type-level annotated tables, the newfound compatibility opens up new experiments. A prime example of exploiting tokenand type-level data is T¨ackstr¨om et al. (2013). That work presents a part-of-speech (POS) dictionary built from Wiktionary, where the POS tagger is also constrained to options available in their typelevel POS dictionary, improving performance. Our transformation means that datasets are prepared for similar experiments with morphological tagging. It would also be reasonable to incorporate this tool as a subroutine to UDPipe (Straka and Strakov´a, 2017) and Udapi (Popel et al., 2017). We leave open the task of converting in the opposite direction, turning UniMorph MSDs into Universal Dependencies MSDs. Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources. These findings will harden both resources and better align them with their goal of universal, crosslingual annotation. Related Work The goal of a tagset-to-tagset mapping of morpholo"
W18-6011,K17-3009,0,0.0457031,"Missing"
W18-6011,P15-2111,1,0.851167,"Missing"
W18-6011,K17-3001,0,0.0560233,"alled a morphosyntactic description (MSD).2 These descriptions require varying amounts of detail: While the English verbal paradigm is small enough to fit on a page, the verbal paradigm of the Northeast Caucasian language Archi can have over 1,500,000 slots (Kibrik, 1998). 3 is an annotated treebank, making it a resource of token-level annotations. The schema is guided by these treebanks, with feature names chosen for relevance to native speakers. (In §3.2, we will contrast this with UniMorph’s treatment of morphosyntactic categories.) The UD datasets have been used in the CoNLL shared tasks (Zeman et al., 2017, 2018 to appear). Two Schemata, Two Philosophies Unlike the Penn Treebank tags, the UD and UniMorph schemata are cross-lingual and include a fuller lexicon of attribute-value pairs, such as P ER SON : 1. Each was built according to a different set of principles. UD’s schema is constructed bottomup, adapting to include new features when they’re identified in languages. UniMorph, conversely, is top-down: A cross-lingual survey of the literature of morphological phenomena guided its design. UniMorph aims to be linguistically complete, containing all known morphosyntactic attributes. Both schemat"
W19-0301,K17-2002,0,0.0170852,"because the field of data-driven morphological analysis in general remains underexplored at the present time. Classically, morphological analyzers have been extended using morphological guessers (Lindén, 2009), however, the premise for such work is quite different—An existing analyzer is modified to analyze unknown word forms based on orthographically similar known word forms. In contrast, we explore a setting, where the starting point is a morphologically analyzed corpus and the aim is to learn a model for analyzing unseen text. Outside of the domain of Uralic languages, Nicolai and Kondrak (2017) frame morphological analysis as a discriminative string transduction task. They present experiments on Dutch, English, German and Spanish. In contrast to Nicolai and Kondrak (2017), Moeller et al. (2018) use a neural encoder-decoder system for morphological analysis of Arapaho verbs. Their system returns both lemmas and morphological tags but it cannot handle ambiguous analyses in general. Our work is inspired by the neural encoder-decoder approach presented by Moeller et al. (2018) but we do handle unrestricted ambiguity. In contrast to data-driven morphological analysis, data-driven morpho"
W19-0301,K18-3001,1,0.911079,"Missing"
W19-0301,K17-2001,0,0.0608936,"Missing"
W19-0301,K18-2013,0,0.110969,"work in the area has been going on for many years (cf. Koskenniemi (1983)). There is also a growing body of work on data-driven morphological tagging for Uralic languages, especially Finnish. Here, a system is trained to find a single contextually appropriate analysis for each token in a text. Examples of Although novel lexical items can cause problems for data-driven systems as well, most data-driven systems are still able to analyze any word form in principle. Code available at https://github.com/mpsilfve/morphnet. 2 work exploring morphological tagging for Finnish include Kanerva et al. (2018) and Silfverberg et al. (2015). However, work on full data-driven morphological analysis, where the task is to return all and only the valid analyses for each token irrespective of sentence context, is almost non-existent for Uralic languages. The only system known to the authors is the recent neural analyzer for Finnish presented by Silfverberg and Hulden (2018). The system first encodes an input word form into a vector representation using an LSTM encoder. It then applies one binary logistic classifier conditioned on this vector representation for each morphological tag (for example NOUN|Num"
W19-0301,W16-2010,0,0.0284187,"(2018) use a neural encoder-decoder system for morphological analysis of Arapaho verbs. Their system returns both lemmas and morphological tags but it cannot handle ambiguous analyses in general. Our work is inspired by the neural encoder-decoder approach presented by Moeller et al. (2018) but we do handle unrestricted ambiguity. In contrast to data-driven morphological analysis, data-driven morphological generation has received a great deal of attention lately due to several shared tasks organized by CoNLL and SIGMORPHON (Cotterell et al., 2016, 2017, 2018). The most successful approaches (Kann and Schütze, 2016; Bergmanis et al., 2017; Makarov et al., 2017; Makarov and Clematide, 2018) to the generation task involve different flavors of the neural encoder-decoder model. Therefore, we opted for applying it in our morphological analyzer. 3 Model This section presents the encoder-decoder model used in the experiments. 3.1 An Encoder-Decoder Model for Morphological Analysis Following Moeller et al. (2018), we formulate morphological analysis as a characterlevel string transduction task and use an LSTM (Hochreiter and Schmidhuber, 1997) encoder-decoder model with attention (Bahdanau et al., 2014) for per"
W19-0301,P17-4012,0,0.0434466,"cting several output candidates from the model using beam search and selecting the most probable candidates as model outputs. The number of outputs is controlled by a probability threshold hyperparameter p. We extract the least number of top scoring candidates whose combined probability mass is greater than p. Additionally, we restrict the maximal number of output candidates using a single hyperparameter N . The hyperparamaters p and N are tuned on the development data. 3.2 Implementation Details We implement our LSTM encoder-decoder model using the OpenNMT neural machine translation toolkit (Klein et al., 2017). We use 500-dimensional character and tag embeddings for input and output characters as well as POS and MSD tags. These are processed by a 2-layer bidirectional LSTM encoder with hidden state size 500. Encoder representations are fed into a 2-layer LSTM decoder with hidden state size 500. During inference, we use beam search with beam width 10. When training, we use a batch size of 64 and train for 10,000 steps where one step corresponds to updating on a single mini-batch. Model parameters are optimized using the Adam optimization algorithm (Kingma and Ba, 2014). 4 4 Data We use two datasets"
W19-0301,W18-4802,0,0.137683,"essers (Lindén, 2009), however, the premise for such work is quite different—An existing analyzer is modified to analyze unknown word forms based on orthographically similar known word forms. In contrast, we explore a setting, where the starting point is a morphologically analyzed corpus and the aim is to learn a model for analyzing unseen text. Outside of the domain of Uralic languages, Nicolai and Kondrak (2017) frame morphological analysis as a discriminative string transduction task. They present experiments on Dutch, English, German and Spanish. In contrast to Nicolai and Kondrak (2017), Moeller et al. (2018) use a neural encoder-decoder system for morphological analysis of Arapaho verbs. Their system returns both lemmas and morphological tags but it cannot handle ambiguous analyses in general. Our work is inspired by the neural encoder-decoder approach presented by Moeller et al. (2018) but we do handle unrestricted ambiguity. In contrast to data-driven morphological analysis, data-driven morphological generation has received a great deal of attention lately due to several shared tasks organized by CoNLL and SIGMORPHON (Cotterell et al., 2016, 2017, 2018). The most successful approaches (Kann an"
W19-0301,L16-1247,0,0.0263337,"Missing"
W19-0301,E17-2034,0,0.0768008,"ages is unsurprising because the field of data-driven morphological analysis in general remains underexplored at the present time. Classically, morphological analyzers have been extended using morphological guessers (Lindén, 2009), however, the premise for such work is quite different—An existing analyzer is modified to analyze unknown word forms based on orthographically similar known word forms. In contrast, we explore a setting, where the starting point is a morphologically analyzed corpus and the aim is to learn a model for analyzing unseen text. Outside of the domain of Uralic languages, Nicolai and Kondrak (2017) frame morphological analysis as a discriminative string transduction task. They present experiments on Dutch, English, German and Spanish. In contrast to Nicolai and Kondrak (2017), Moeller et al. (2018) use a neural encoder-decoder system for morphological analysis of Arapaho verbs. Their system returns both lemmas and morphological tags but it cannot handle ambiguous analyses in general. Our work is inspired by the neural encoder-decoder approach presented by Moeller et al. (2018) but we do handle unrestricted ambiguity. In contrast to data-driven morphological analysis, data-driven morpho"
W19-0301,W15-1821,0,0.0232865,"big. Lemmas Tags MSDs Th. forms 1.80 — 137 2452 — Table 2: Quantitative description of the Finnish treebank dataset. We give the number of unique train, dev and test word forms, as well as, the average number of analyses per word form (Ambig.), the number of unique lemmas (Lemmas), the number of unique tags such as NOUN and Number=Sing (Tags) and the number of unique morphosyntactic descriptions such as NOUN|Number=Sing|Case=Nom (MSDs). 4.2 Finnish Treebank Data Our second dataset was presented by Silfverberg and Hulden (2018). It is the Finnish part of the Universal Dependencies treebank v1 (Pyysalo et al., 2015) which has been analyzed using the OMorFi morphological analyzer (Pirinen et al., 2017). We used the splits into training, development and test sets provided by Silfverberg and Hulden (2018). In contrast to the Uralic Wikipedia datasets, which is a type-level resource consisting of analyses for unique word forms, the Finnish treebank data is a token-level resource consisting of morphologically analyzed running text. Therefore, the same word form can occur multiple times in the dataset. This means that the training, development and test sets are not disjoint which makes the task somewhat easier"
W19-0301,W18-0210,1,0.873504,"Missing"
W19-0301,W17-0607,1,0.818739,"Missing"
W19-0301,W18-0209,1,0.390326,"cause problems for data-driven systems as well, most data-driven systems are still able to analyze any word form in principle. Code available at https://github.com/mpsilfve/morphnet. 2 work exploring morphological tagging for Finnish include Kanerva et al. (2018) and Silfverberg et al. (2015). However, work on full data-driven morphological analysis, where the task is to return all and only the valid analyses for each token irrespective of sentence context, is almost non-existent for Uralic languages. The only system known to the authors is the recent neural analyzer for Finnish presented by Silfverberg and Hulden (2018). The system first encodes an input word form into a vector representation using an LSTM encoder. It then applies one binary logistic classifier conditioned on this vector representation for each morphological tag (for example NOUN|Number=Sg|Case=Nom). The classifier is used to determine if the tag is a valid analysis for the given input word form. Similarly to Silfverberg and Hulden (2018), our system is also a neural morphological analyzer but unlike Silfverberg and Hulden (2018) we incorporate lemmatization. Moreover, the design of our system considerably differs from their system as explai"
W19-0301,W17-0608,0,0.0678951,"Missing"
W19-1401,W19-1402,0,0.268136,"Missing"
W19-1401,P19-1068,1,0.913629,"guages and asked to predict the valid morphological analyses for a seventh, unseen language. In the “Semi-Closed” track, the process was the same, only participants were provided with additional raw data by the organisers. This was in the form of raw text Wikipedia dumps, bilingual dictionaries from the Apertium project and any treebanks available in the known languages from the Universal Dependencies project. Moldavian vs. Romanian Cross-dialect Topic identification (MRC): In the Moldavian vs. Romanian Cross-topic Identification shared task, we provided participants with the MOROCO data set (Butnaru and Ionescu, 2019) which contains Moldavian and Romanian samples of text collected from the news domain. The samples belong to one of the following six topics: culture, finance, politics, science, sports, and tech. The samples are pre-processed in order to eliminate named entities. For each sample, the data set provides corresponding dialectal and category labels. To this end, we proposed three subtasks for the 2019 VarDial Evaluation Campaign. The first sub-task was a binary classification by dialect task, in which a classification model is required to discriminate between the Moldavian and the Romanian dialec"
W19-1401,W18-3929,1,0.894916,"Missing"
W19-1401,W19-1413,1,0.836296,"Missing"
W19-1401,W19-1416,0,0.056458,"Missing"
W19-1401,W18-3907,1,0.895659,"Missing"
W19-1401,Y96-1018,1,0.197287,"k. 5.5 Summary Three teams participated in this first iteration of the cross-lingual analysis task. Two of the teams employed variations of neural encoderdecoder systems. Apart from lemmatization performance, it proved to be difficult to attain consistent improvements over the neural baseline systems. However, the suffix stripping approach used by the HSE team did deliver clear improvements in lemmatization for both Turkic and Romance languages. 6 6.1 Dataset Texts to distinguish between the two variations were compiled from the two existing corpora of news: Sinica Corpus for Taiwan Mandarin (Chen et al., 1996) and LCMC (The Lancaster Corpus of Mandarin Chinese, (McEnery and Xiao, 2003)) for Mainland Mandarin. Both corpora are segmented and tokenized. We remove the punctuation and unify the orthography used to eliminate orthographic cues. Since both corpora are balanced corpora, our initial thought was to provide genre-aware classification. However, inspection of both corpora suggested the genres were not defined in the same way and are not distributed homogeneously. In the next edition this idea may be exploited by using some additional resources as genre vs. regional variations which is an importa"
W19-1401,W19-1419,1,0.847943,"Missing"
W19-1401,W19-1414,0,0.0601978,"Missing"
W19-1401,W16-4801,1,0.6692,"Missing"
W19-1401,W19-1420,0,0.0913091,"ces from newspapers for each Mandarin variety. The main task is to determine if a sentence is written in the Mandarin Cuneiform Language Identification (CLI): This shared task focused on discriminating between languages and dialects originally written using the cuneiform script. The task included 2 dif2 Team Adaptcenter BAM dkosmajac DTeam SharifCL ghpaetzold gretelliz92 ekh IUCL HSE itsalexyang lonewolf MineriaUNAM NRC-CNRC R2I LIS PZ SC-UPB situx SUKI tearsofjoy T¨ubingenOslo Twist Bytes Total GDI CMA DMT X MRC CLI X X System Description Papers (Butnaru, 2019) X X X X X X (Tudoreanu, 2019) (Doostmohammadi and Nassajian, 2019) X X (Hu et al., 2019) (Mikhailov et al., 2019) (Yang and Xiang, 2019) X X X X X X X X (Bernier-Colborne et al., 2019) (Chifu, 2019) (Paetzold and Zampieri, 2019) (Onose and Cercel, 2019) X X X X X X X 7 5 X 8 X X 6 3 (Jauhiainen et al., 2019b) (Wu et al., 2019) (C¸o¨ ltekin and Barnes, 2019) (Benites et al., 2019) 14 Table 1: The teams that participated in the Third VarDial Evaluation Campaign. took part in, and a reference to each of the 14 system description papers published in the VarDial workshop proceedings. ferent languages: Sumerian and Akkadian. Furthermore, the Akkadian language was"
W19-1401,W19-1415,0,0.0347692,"Missing"
W19-1401,L18-1550,0,0.0287077,"sed on a majority voting scheme applied on five classification models: kNearest Neighbors, Logistic Regression, Support Vector Machines, Neural Networks and Random Forests. For the first and the third runs, the models are trained on both training and development sets. For the second run, the model is trained only on the training set. SC-UPB. The SC-UPB team first cleaned the dataset by removing stopwords as well as special characters. The first run submitted to each of the three subtasks is based on a model that represents text as the mean of word vectors given by a pretrained FastText model (Grave et al., 2018). The representation is provided as input to a Recurrent Neural Network with gated recurrent units, which is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. The second run submitted to each of the three subtasks is based on a hierarchical attention network introduced by Yang et al. (2016). The model is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. tearsofjoy. The tearsofjoy team used a linear SVM classifier with a combination of character and word n-gram features, which are weighted with the BM25 weighting"
W19-1401,W18-4802,0,0.0115539,"zers is a substantial task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjectives, adverbs and verbs). We then removed any form whic"
W19-1401,W19-1417,0,0.0619352,"Missing"
W19-1401,E17-2034,0,0.0280356,"e-art for this task, however, developing rule-based analyzers is a substantial task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjective"
W19-1401,D18-1135,1,0.824726,"016). The model is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. tearsofjoy. The tearsofjoy team used a linear SVM classifier with a combination of character and word n-gram features, which are weighted with the BM25 weighting scheme. Their model’s parameters are tuned independently for each subtask, using random search and 5-fold crossvalidation. The tearsofjoy team also tried a transductive learning approach which is based on retraining the model by adding confident predictions from the test set to the training set, an idea previously studied in (Ionescu and Butnaru, 2018). • Binary classification by dialect (subtask 1) – the task is to discriminate between the Moldavian and the Romanian dialects. • MD→RO cross-dialect multi-class categorization by topic (subtask 2) – the task is to classify the samples written in the Romanian dialect into six topics, using a model trained on samples written in the Moldavian dialect. • RO→MD cross-dialect multi-class categorization by topic (subtask 3) – the task is to classify the samples written in the Moldavian dialect into six topics, using a model trained on samples written in the Romanian dialect. 7.2 Participants and App"
W19-1401,W19-1409,1,0.877823,"Missing"
W19-1401,W19-1418,0,0.0615736,"Missing"
W19-1401,N16-1174,0,0.0228023,"t. SC-UPB. The SC-UPB team first cleaned the dataset by removing stopwords as well as special characters. The first run submitted to each of the three subtasks is based on a model that represents text as the mean of word vectors given by a pretrained FastText model (Grave et al., 2018). The representation is provided as input to a Recurrent Neural Network with gated recurrent units, which is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. The second run submitted to each of the three subtasks is based on a hierarchical attention network introduced by Yang et al. (2016). The model is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. tearsofjoy. The tearsofjoy team used a linear SVM classifier with a combination of character and word n-gram features, which are weighted with the BM25 weighting scheme. Their model’s parameters are tuned independently for each subtask, using random search and 5-fold crossvalidation. The tearsofjoy team also tried a transductive learning approach which is based on retraining the model by adding confident predictions from the test set to the training set, an idea previously studied in (Ione"
W19-1401,W19-1423,1,0.825557,"e ranking for subtask 3, as shown in Table 8. 7.4 Dataset Summary We proposed three MRC subtasks for VarDial 2019. Three participants submitted runs for all three subtasks, and another two participants submitted runs only for subtask 1. Two teams (DTeam 5 12 http://oracc.museum.upenn.edu Language or Dialect Sumerian Old Babylonian Middle Babylonian peripheral Standard Babylonian Neo-Babylonian Late Babylonian Neo-Assyrian two systems in more detail. The PZ team used a SVM metaclassifier ensemble of several linear SVM classifiers trained using character n-gram and character skip-gram features. Paetzold and Zampieri (2019) give further details. The SharifCL team submitted three runs and their best performing system was an ensemble of a SVM and a NB classifier (Doostmohammadi and Nassajian, 2019). The ghpaetzold team submitted only one run using 2-layer compositional recurrent neural network that learns numerical representations of sentences based on their words, and of words based on their characters. Their system is described in more detail by Paetzold and Zampieri (2019). The ekh team used a sum of relative frequencies of character bigrams together with a penalty value for those bigrams or unigrams that were"
W19-1401,W17-1201,1,0.803354,"Missing"
W19-1401,L16-1641,1,0.880892,"Missing"
W19-1401,W14-5307,1,0.821127,"Missing"
W19-1401,W18-0209,1,0.707638,"r, developing rule-based analyzers is a substantial task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjectives, adverbs and verbs). We then"
W19-1401,W19-0301,1,0.916428,"task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjectives, adverbs and verbs). We then removed any form which did not include at least one"
W19-1401,E14-2006,0,0.0437089,"Missing"
W19-1401,W19-1422,0,0.0730531,"Missing"
W19-1401,W19-1412,0,0.0741523,"Missing"
W19-4226,P17-2107,0,0.0357656,"Missing"
W19-4226,N18-1126,0,0.0330462,"gs (BERT) provided by Google (Devlin et al., 2019). CBNU1 used a mix of pre-trained embeddings from the CoNLL 2017 shared task and fastText. Further, some teams trained their own embeddings to aid performance. CUNI–Malta performs lemmatization as operations over edit actions with LSTM and ReLU. Tagging is a bidirectional LSTM augmented by the edit actions (i.e., two-stage decoding), predicting features separately. The Edinburgh system is a character-based LSTM encoder-decoder with attention, implemented in OpenNMT. It can be seen as an extension of the contextual lemmatization system Lematus (Bergmanis and Goldwater, 2018) to include morphological tagging, or alternatively as an adaptation of the morphological re-inflection system MED (Kann and Sch¨utze, 2016) to incorporate context and perform analysis rather than re-inflection. Like these systems it uses a completely generic encoderdecoder architecture with no specific adaptation to the morphological processing task other than the form of the input. In the submitted version of the system, the input is split into short chunks corresponding to the target word plus one word of context on either side, and the system is trained to output the corresponding lemmas a"
W19-4226,K18-3001,1,0.746475,"previous shared task (Cotterell et al., 2018), training a neural network on unambiguous forms to estimate the distribution over all, even ambiguous, forms. We then sampled 12,000 triples without replacement from this distribution. The first 100 were taken as training data for low-resource settings. The first 10,000 were used as high-resource training sets. As these sets are nested, the highest-count triples tend to appear in the smaller training sets.3 Data conversion The morphological annotations for the 2019 shared task were converted to the UniMorph schema (Kirov et al., 2018) according to McCarthy et al. (2018), who provide a deterministic mapping that increases agreement across languages. This also moves the part of speech into the bundle of morphological features. We do not attempt to individually correct any errors in the UD source material. Further, some languages received additional pre-processing. In the Finnish data, we removed morpheme boundaries that were present in the lemmata (e.g., puhe#kieli 7→ puhekieli ‘spoken+language’). Russian lemmata in the GSD treebank were presented in all uppercase; to match Swahili. Likewise, the low-resource language Telugu had fewer than 100 forms. 4 When su"
W19-4226,K17-2001,1,0.692718,"Missing"
W19-4226,N19-1155,1,0.843499,"ive teams participated in the first Task, with a variety of methods aimed at leveraging the crosslingual data to improve system performance. The University of Alberta (UAlberta) performed a focused investigation on four language pairs, training cognate-projection systems from external cognate lists. Two methods were considered: one which trained a high-resource neural encoderdecoder, and projected the test data into the HRL, and one that projected the HRL data into the LRL, and trained a combined system. Results demonstrated that certain language pairs may be amenable to such methods. Neural (Malaviya et al., 2019): This is a stateof-the-art neural model that also performs joint morphological tagging and lemmatization, but also accounts for the exposure bias with the application of maximum likelihood (MLE). The model stitches the tagger and lemmatizer together with the use of jackknifing (Agi´c and Schluter, 2017) to expose the lemmatizer to the errors made by the tagger model during training. The morphological tagger is based on a character-level biLSTM embedder that produces the embedding for a word, 4 HRL–LRL adyghe–kabardian albanian–breton arabic–classical-syriac arabic–maltese arabic–turkmen armen"
W19-4226,N19-1423,0,0.0153778,"only applied to a subset of languages, making scores incomparable. † indicates that additional external resources were used for training, and ‡ indicates that training data were shared across languages or treebanks. ging). Although they predict complete tags, they use the individual features to regularize the decoder. Small gains are also obtained from joining multilingual corpora and ensembling. improve their lemmatization and feature analysis. Several teams made use of pre-trained embeddings. CHARLES-SAARLAND-2 and UFALPRAGUE1 used pretrained contextual embeddings (BERT) provided by Google (Devlin et al., 2019). CBNU1 used a mix of pre-trained embeddings from the CoNLL 2017 shared task and fastText. Further, some teams trained their own embeddings to aid performance. CUNI–Malta performs lemmatization as operations over edit actions with LSTM and ReLU. Tagging is a bidirectional LSTM augmented by the edit actions (i.e., two-stage decoding), predicting features separately. The Edinburgh system is a character-based LSTM encoder-decoder with attention, implemented in OpenNMT. It can be seen as an extension of the contextual lemmatization system Lematus (Bergmanis and Goldwater, 2018) to include morpholo"
W19-4226,Q18-1032,0,0.0456659,"Missing"
W19-4226,W17-4110,0,0.124781,"Missing"
W19-4226,D15-1272,1,0.898864,"Missing"
W19-4226,W16-2010,0,0.0549216,"Missing"
W19-4226,D18-1103,0,0.0261867,"he system, the input is split into short chunks corresponding to the target word plus one word of context on either side, and the system is trained to output the corresponding lemmas and tags for each three-word chunk. 6 Future Directions In general, the application of typology to natural language processing (e.g., Gerz et al., 2018; Ponti et al., 2018) provides an interesting avenue for multilinguality. Further, our shared task was designed to only leverage a single helper language, though many may exist with lexical or morphological overlap with the target language. Techniques like those of Neubig and Hu (2018) may aid in designing universal inflection architectures. Neither task this year included unannotated monolingual corpora. Using such data is well-motivated from an L1-learning point of view, and may affect the performance of low-resource data settings. In the case of inflection an interesting future topic could involve departing from orthographic representation and using more IPA-like representations, i.e. transductions over pronunciations. DifferSeveral teams relied on external resources to 8 Table 6: Task 2 Lemma Accuracy scores 9 UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD"
W19-4226,L18-1293,1,0.829575,"previous shared task (Cotterell et al., 2018), training a neural network on unambiguous forms to estimate the distribution over all, even ambiguous, forms. We then sampled 12,000 triples without replacement from this distribution. The first 100 were taken as training data for low-resource settings. The first 10,000 were used as high-resource training sets. As these sets are nested, the highest-count triples tend to appear in the smaller training sets.3 Data conversion The morphological annotations for the 2019 shared task were converted to the UniMorph schema (Kirov et al., 2018) according to McCarthy et al. (2018), who provide a deterministic mapping that increases agreement across languages. This also moves the part of speech into the bundle of morphological features. We do not attempt to individually correct any errors in the UD source material. Further, some languages received additional pre-processing. In the Finnish data, we removed morpheme boundaries that were present in the lemmata (e.g., puhe#kieli 7→ puhekieli ‘spoken+language’). Russian lemmata in the GSD treebank were presented in all uppercase; to match Swahili. Likewise, the low-resource language Telugu had fewer than 100 forms. 4 When su"
W19-4226,D15-1166,0,0.0843899,"ani Kurdish were created as part of the Alexina project (Walther et al., 2010; Walther and Sagot, 2010). 2 These datasets can be obtained from https:// sigmorphon.github.io/sharedtasks/2019/ 3 Several high-resource languages had necessarily fewer, but on a similar order of magnitude. Bengali, Uzbek, Kannada, 3 the 2018 shared task, we lowercased these. In development and test data, all fields except for form and index within the sentence were struck. 4 4.1 Team Baselines Task 1 Baseline We include four neural sequence-to-sequence models mapping lemma into inflected word forms: soft attention (Luong et al., 2015), non-monotonic hard attention (Wu et al., 2018), monotonic hard attention and a variant with offset-based transition distribution (Wu and Cotterell, 2019). Neural sequenceto-sequence models with soft attention (Luong et al., 2015) have dominated previous SIGMORPHON shared tasks (Cotterell et al., 2017). Wu et al. (2018) instead models the alignment between characters in the lemma and the inflected word form explicitly with hard attention and learns this alignment and transduction jointly. Wu and Cotterell (2019) shows that enforcing strict monotonicity with hard attention is beneficial in tas"
W19-4226,P18-1247,1,0.817396,"ciently prescribed by the lemma, as with the Romanian verbal inflection classes or nominal gender in German. As we move toward multilingual models for morphology, it becomes important to understand which representations are critical or irrelevant for adapting to new languages; this may be probed in the style of (Thompson et al., 2018), and it can be used as a first step toward designing systems that avoid “catastrophic forgetting” as they learn to inflect new languages (Thompson et al., 2019). Future directions for Task 2 include exploring cross-lingual analysis—in stride with both Task 1 and Malaviya et al. (2018)—and leveraging these analyses in downstream tasks. 7 In the second task, several methods were implemented by multiple groups, with the most successful systems implementing variations of multiheaded attention, multi-level encoding, multiple decoders, and ELMo and BERT contextual embeddings. We have released the training, development, and test sets, and expect these datasets to provide a useful benchmark for future research into learning of inflectional morphology and string-to-string transduction. Acknowledgments MS has received funding from the European Research Council (ERC) under the Europe"
W19-4226,P15-2111,1,0.881319,"Missing"
W19-4226,N19-1209,0,0.0249795,"tasks. One pertinent facet of this is information about inflectional categories—often the inflectional information is insufficiently prescribed by the lemma, as with the Romanian verbal inflection classes or nominal gender in German. As we move toward multilingual models for morphology, it becomes important to understand which representations are critical or irrelevant for adapting to new languages; this may be probed in the style of (Thompson et al., 2018), and it can be used as a first step toward designing systems that avoid “catastrophic forgetting” as they learn to inflect new languages (Thompson et al., 2019). Future directions for Task 2 include exploring cross-lingual analysis—in stride with both Task 1 and Malaviya et al. (2018)—and leveraging these analyses in downstream tasks. 7 In the second task, several methods were implemented by multiple groups, with the most successful systems implementing variations of multiheaded attention, multi-level encoding, multiple decoders, and ELMo and BERT contextual embeddings. We have released the training, development, and test sets, and expect these datasets to provide a useful benchmark for future research into learning of inflectional morphology and str"
W19-4226,W18-6313,1,0.850382,"sentangle.8 Creating new data sets that accurately reflect learner exposure (whether L1 or L2) is also an important consideration in the design of future shared tasks. One pertinent facet of this is information about inflectional categories—often the inflectional information is insufficiently prescribed by the lemma, as with the Romanian verbal inflection classes or nominal gender in German. As we move toward multilingual models for morphology, it becomes important to understand which representations are critical or irrelevant for adapting to new languages; this may be probed in the style of (Thompson et al., 2018), and it can be used as a first step toward designing systems that avoid “catastrophic forgetting” as they learn to inflect new languages (Thompson et al., 2019). Future directions for Task 2 include exploring cross-lingual analysis—in stride with both Task 1 and Malaviya et al. (2018)—and leveraging these analyses in downstream tasks. 7 In the second task, several methods were implemented by multiple groups, with the most successful systems implementing variations of multiheaded attention, multi-level encoding, multiple decoders, and ELMo and BERT contextual embeddings. We have released the t"
W19-4226,W18-5818,1,0.888961,"Missing"
W19-4226,P19-1148,1,0.888951,"sigmorphon.github.io/sharedtasks/2019/ 3 Several high-resource languages had necessarily fewer, but on a similar order of magnitude. Bengali, Uzbek, Kannada, 3 the 2018 shared task, we lowercased these. In development and test data, all fields except for form and index within the sentence were struck. 4 4.1 Team Baselines Task 1 Baseline We include four neural sequence-to-sequence models mapping lemma into inflected word forms: soft attention (Luong et al., 2015), non-monotonic hard attention (Wu et al., 2018), monotonic hard attention and a variant with offset-based transition distribution (Wu and Cotterell, 2019). Neural sequenceto-sequence models with soft attention (Luong et al., 2015) have dominated previous SIGMORPHON shared tasks (Cotterell et al., 2017). Wu et al. (2018) instead models the alignment between characters in the lemma and the inflected word form explicitly with hard attention and learns this alignment and transduction jointly. Wu and Cotterell (2019) shows that enforcing strict monotonicity with hard attention is beneficial in tasks such as morphological inflection where the transduction is mostly monotonic. The encoder is a biLSTM while the decoder is a left-to-right LSTM. All mode"
W19-4226,D18-1473,1,0.906631,"Missing"
W19-4226,D16-1163,0,0.0280678,"larger number of examples in either a related or unrelated language. Each test example asked participants to produce some other inflected form when given a lemma and a bundle of morphosyntactic features as input. The goal, thus, is to perform morphological inflection in the low-resource language, having hopefully exploited some similarity to the high-resource language. Models which perform well here can aid downstream tasks like machine translation in lowresource settings. All datasets were resampled from UniMorph, which makes them distinct from past years. The mode of the task is inspired by Zoph et al. (2016), who fine-tune a model pre-trained on a high-resource language to perform well on a lowresource language. We do not, though, require that models be trained by fine-tuning. Joint modeling or any number of methods may be explored instead. Task 2: Morphological analysis in context Although inflection of words in a context-agnostic manner is a useful evaluation of the morphological quality of a system, people do not learn morphology in isolation. In 2018, the second task of the CoNLL– SIGMORPHON Shared Task (Cotterell et al., 2018) required submitting systems to complete an inflectional cloze tas"
W19-4226,W18-6011,1,\N,Missing
W19-6132,K18-3001,1,0.142482,"Missing"
W19-6132,W17-3203,0,0.0673932,"weighted and weighted voting strategies to combine outputs of different models. We also investigate different ways of training the component models in the ensemble using both random initialization of model parameters and varying the training data using bootstrap aggregation commonly known as bagging (Breiman, 1996). Bagging is a popular ensemble method where new training sets are created by resampling from an existing training set. Both bagging and majority voting are known to reduce the variance of the model. This makes them suitable for neural models which are known to obtain high variance (Denkowski and Neubig, 2017). Due to practicality concerns, we limit the scope of the paper to methods which can combine existing models without changes to model architecture. Therefore, we do not explore merging model predictions during beam search in decoding or averaging model parameters. We perform experiments on a selection of ten languages: Arabic, Finnish, Georgian, German, Hindi, Italian, Khaling, Navajo, Russian, and Turkish. Our experiments on this morphologically and areally diverse set of languages show that model ensembles tend to deliver the best results confirming results presented in earlier work. However"
W19-6132,1993.eamt-1.1,0,0.734178,"Missing"
W19-6132,W16-2010,0,0.0661152,"Missing"
W19-6132,K18-3011,0,0.0155497,"al. (2018) and Silfverberg et al. (2017) weighting did not deliver clear benefits over unweighted model ensembles. Bagging, in general, does deliver improvements in model accuracy compared to a baseline of a single model but does not outperform plain majority voting. 2 Related Work Following Kann and Sch¨utze (2016) and many others, we explore learning of MI systems in the context of bidirectional LSTM encoder-decoder models with attention. Several papers have employed straightforward majority voting for the task of MI (Kann and Sch¨utze, 2016; Kann et al., 2018; Makarov and Clematide, 2018; Kementchedjhieva et al., 2018; Sharma et al., 2018). However, work on more advanced ensembling methods is scarce for the MI task. Najafi et al. (2018) and Silfverberg et al. (2017) explored weighted variants of majority voting. Both of these approaches are based on weighting models according to their performance on a heldout development set. Silfverberg et al. (2017) use sampling-based methods for finding good weighting coefficients for the component models in an ensemble. Najafi et al. (2018) instead simply weight models according to their accuracy on the development set. We opt for using the latter weighing scheme in ou"
W19-6132,P17-4012,0,0.0865271,"Missing"
W19-6132,K18-3015,0,0.354536,"Neural Morphological Inflection Models Ilmari Kylli¨ainen and Miikka Silfverberg Department of Digital Humanities, University of Helsinki, Finland {ilmari.kylliainen,miikka.silfverberg}@helsinki.fi Abstract We investigate different ensemble learning techniques for neural morphological inflection using bidirectional LSTM encoder-decoder models with attention. We experiment with weighted and unweighted majority voting and bagging. We find that all investigated ensemble methods lead to improved accuracy over a baseline of a single model. However, contrary to expectation based on earlier work by Najafi et al. (2018) and Silfverberg et al. (2017), weighting does not deliver clear benefits. Bagging was found to underperform plain voting ensembles in general. 1 Introduction Natural language processing (NLP) systems for languages which exhibit rich inflectional morphology often suffer from data sparsity. The root cause of this sparsity is a prohibitively high typetoken ratio which is typical for morphologically complex languages. A common way to alleviate the problem is to incorporate modeling of inflectional morphology instead of building purely word-based NLP systems—by representing word forms as combinati"
W19-6132,K17-2010,1,0.774662,"lection Models Ilmari Kylli¨ainen and Miikka Silfverberg Department of Digital Humanities, University of Helsinki, Finland {ilmari.kylliainen,miikka.silfverberg}@helsinki.fi Abstract We investigate different ensemble learning techniques for neural morphological inflection using bidirectional LSTM encoder-decoder models with attention. We experiment with weighted and unweighted majority voting and bagging. We find that all investigated ensemble methods lead to improved accuracy over a baseline of a single model. However, contrary to expectation based on earlier work by Najafi et al. (2018) and Silfverberg et al. (2017), weighting does not deliver clear benefits. Bagging was found to underperform plain voting ensembles in general. 1 Introduction Natural language processing (NLP) systems for languages which exhibit rich inflectional morphology often suffer from data sparsity. The root cause of this sparsity is a prohibitively high typetoken ratio which is typical for morphologically complex languages. A common way to alleviate the problem is to incorporate modeling of inflectional morphology instead of building purely word-based NLP systems—by representing word forms as combinations of a lemma and morphosynta"
