2011.mtsummit-papers.13,2008.amta-papers.2,0,0.0702532,"ment contexts into current SMT systems for document-level translation. In particular, we focus on translation consistency which is one of the most important issues in document-level MT. We propose a 3-step approach to incorporating document contexts into a traditional SMT system, and demonstrate that our approach can effectively reduce the errors caused by inconsistent translation. More interestingly, it is observed that using document contexts is promising for BLEU improvement. 2 Related Work To date, only a few studies have improved MT systems with the use of document contexts. For example, Brown (2008) proposed a method to improve SMT and Example-Based Machine Translation (EBMT) systems using document-level similarity between the documents in the training corpus and the input document. Another example is (Zhao and Xing, 2007) in which a bilingual topic model was proposed to capture the document-level topical aspects of SMT. However, no previous work has addressed the issue of translation consistency in document-level MT. The problem discussed in this paper is similar to the lexical selection problem in SMT (Wu and Palmer, 1994). There have been some attempts at using context-dependent featu"
2011.mtsummit-papers.13,P05-1048,0,0.0299149,"slation (EBMT) systems using document-level similarity between the documents in the training corpus and the input document. Another example is (Zhao and Xing, 2007) in which a bilingual topic model was proposed to capture the document-level topical aspects of SMT. However, no previous work has addressed the issue of translation consistency in document-level MT. The problem discussed in this paper is similar to the lexical selection problem in SMT (Wu and Palmer, 1994). There have been some attempts at using context-dependent features to select appropriate target lexical items for SMT systems (Carpuat and Wu, 2005; Carpuat and Wu, 2007; Chan et al., 2007). However, these studies were all in the scenario of sentence-level MT. By contrast, we focus more on using document contexts to address the issue in document translation. Actually, the translation consistency issue has been discussed in some related tasks. For example, Wang et al. (2007)’s work showed that consistency information was very helpful in dealing with the out-ofvocabulary (OOV) problem for Chinese word segmentation. 3 Document-level Consistency Verification Given a source document Df, the task of document-level SMT is to find an optimal tar"
2011.mtsummit-papers.13,2007.tmi-papers.6,0,0.0313509,"using document-level similarity between the documents in the training corpus and the input document. Another example is (Zhao and Xing, 2007) in which a bilingual topic model was proposed to capture the document-level topical aspects of SMT. However, no previous work has addressed the issue of translation consistency in document-level MT. The problem discussed in this paper is similar to the lexical selection problem in SMT (Wu and Palmer, 1994). There have been some attempts at using context-dependent features to select appropriate target lexical items for SMT systems (Carpuat and Wu, 2005; Carpuat and Wu, 2007; Chan et al., 2007). However, these studies were all in the scenario of sentence-level MT. By contrast, we focus more on using document contexts to address the issue in document translation. Actually, the translation consistency issue has been discussed in some related tasks. For example, Wang et al. (2007)’s work showed that consistency information was very helpful in dealing with the out-ofvocabulary (OOV) problem for Chinese word segmentation. 3 Document-level Consistency Verification Given a source document Df, the task of document-level SMT is to find an optimal target document De* by: D"
2011.mtsummit-papers.13,P07-1005,0,0.0660032,"similarity between the documents in the training corpus and the input document. Another example is (Zhao and Xing, 2007) in which a bilingual topic model was proposed to capture the document-level topical aspects of SMT. However, no previous work has addressed the issue of translation consistency in document-level MT. The problem discussed in this paper is similar to the lexical selection problem in SMT (Wu and Palmer, 1994). There have been some attempts at using context-dependent features to select appropriate target lexical items for SMT systems (Carpuat and Wu, 2005; Carpuat and Wu, 2007; Chan et al., 2007). However, these studies were all in the scenario of sentence-level MT. By contrast, we focus more on using document contexts to address the issue in document translation. Actually, the translation consistency issue has been discussed in some related tasks. For example, Wang et al. (2007)’s work showed that consistency information was very helpful in dealing with the out-ofvocabulary (OOV) problem for Chinese word segmentation. 3 Document-level Consistency Verification Given a source document Df, the task of document-level SMT is to find an optimal target document De* by: De* arg max Pr( De |D"
2011.mtsummit-papers.13,P07-2045,0,0.00568597,"manually removed the candidate checkpoints where the consistency of translation is not strongly required. The system was evaluated by the number of errors at checkpoints. We also reported the BLEU(-SBP) (Chiang et al., 2008) score to show the impact of our approach on translation accuracy. 4 4.1 Experiments Baseline System Our experiments were conducted on ChineseEnglish translation based on the open-source phrase-based MT system NiuTrans 5 . The NiuTrans uses two reordering models, including a maximum entropy-based lexicalized reordering model (Xiong et al., 2006) and a MSD reordering model (Koehn et al., 2007). In addition, it adopts all standard features used in the state-of-the-art SMT system Moses (Koehn et al., 2007), such as bi-directional phrase translation probabilities and n-gram language model. The feature weights were optimized using MERT (Och, 2003). By default, the distortion limit was set to 8, k was set to 1 (Equations 3, 5-7), and D was set to 0 (Equation 5). 4.3 5 Results in Default Settings We first investigate the effectiveness of our methods on error reduction in the default settings. Table 1 compares various methods in terms of the number of errors at checkpoints, where Post and"
2011.mtsummit-papers.13,P03-1021,0,0.0566132,"h on translation accuracy. 4 4.1 Experiments Baseline System Our experiments were conducted on ChineseEnglish translation based on the open-source phrase-based MT system NiuTrans 5 . The NiuTrans uses two reordering models, including a maximum entropy-based lexicalized reordering model (Xiong et al., 2006) and a MSD reordering model (Koehn et al., 2007). In addition, it adopts all standard features used in the state-of-the-art SMT system Moses (Koehn et al., 2007), such as bi-directional phrase translation probabilities and n-gram language model. The feature weights were optimized using MERT (Och, 2003). By default, the distortion limit was set to 8, k was set to 1 (Equations 3, 5-7), and D was set to 0 (Equation 5). 4.3 5 Results in Default Settings We first investigate the effectiveness of our methods on error reduction in the default settings. Table 1 compares various methods in terms of the number of errors at checkpoints, where Post and Rede stand for the post-editing and the re-decoding methods used in final translation generation (Step 3) respectively, M1 and M2 stand for the two counting methods shown in Equations (6-7). We see that all our proposed methods are effective to reduce th"
2011.mtsummit-papers.13,P94-1019,0,0.0549308,"ave improved MT systems with the use of document contexts. For example, Brown (2008) proposed a method to improve SMT and Example-Based Machine Translation (EBMT) systems using document-level similarity between the documents in the training corpus and the input document. Another example is (Zhao and Xing, 2007) in which a bilingual topic model was proposed to capture the document-level topical aspects of SMT. However, no previous work has addressed the issue of translation consistency in document-level MT. The problem discussed in this paper is similar to the lexical selection problem in SMT (Wu and Palmer, 1994). There have been some attempts at using context-dependent features to select appropriate target lexical items for SMT systems (Carpuat and Wu, 2005; Carpuat and Wu, 2007; Chan et al., 2007). However, these studies were all in the scenario of sentence-level MT. By contrast, we focus more on using document contexts to address the issue in document translation. Actually, the translation consistency issue has been discussed in some related tasks. For example, Wang et al. (2007)’s work showed that consistency information was very helpful in dealing with the out-ofvocabulary (OOV) problem for Chine"
2011.mtsummit-papers.13,P06-1066,0,0.0486244,"ords7 as the candidate checkpoints; 2) and then manually removed the candidate checkpoints where the consistency of translation is not strongly required. The system was evaluated by the number of errors at checkpoints. We also reported the BLEU(-SBP) (Chiang et al., 2008) score to show the impact of our approach on translation accuracy. 4 4.1 Experiments Baseline System Our experiments were conducted on ChineseEnglish translation based on the open-source phrase-based MT system NiuTrans 5 . The NiuTrans uses two reordering models, including a maximum entropy-based lexicalized reordering model (Xiong et al., 2006) and a MSD reordering model (Koehn et al., 2007). In addition, it adopts all standard features used in the state-of-the-art SMT system Moses (Koehn et al., 2007), such as bi-directional phrase translation probabilities and n-gram language model. The feature weights were optimized using MERT (Och, 2003). By default, the distortion limit was set to 8, k was set to 1 (Equations 3, 5-7), and D was set to 0 (Equation 5). 4.3 5 Results in Default Settings We first investigate the effectiveness of our methods on error reduction in the default settings. Table 1 compares various methods in terms of the"
2020.acl-main.585,D18-1015,0,0.286653,"f learning rate and gradient clipping of 1.0. Dropout (Srivastava et al., 2014) of 0.2 is applied to prevent overfitting. 4.3 Datasets We conduct experiments on three benchmark datasets: Charades-STA (Gao et al., 2017), ActivityNet Caption (Krishna et al., 2017), and TACoS (Regneri et al., 2013), summarized in Table 1. Charades-STA is prepared by Gao et al. (2017) based on Charades dataset (Sigurdsson et al., 2016). Experimental Settings Comparison with State-of-the-Arts We compare VSLBase and VSLNet with the following state-of-the-arts: CTRL (Gao et al., 2017), ACRN (Liu et al., 2018a), TGN (Chen et al., 2018), ACL-K (Ge et al., 2019), QSPN (Xu et al., 2019), SAP (Chen and Jiang, 2019), MAN (Zhang et al., 2019), SM-RL (Wang et al., 2019), RWMRL (He et al., 2019), L-Net (Chen et al., 2019), ExCL (Ghosh et al., 2019), ABLR (Yuan et al., 6547 # Annotations Nvocab ¯ video L ¯ query L ¯ moment ∆moment L 5, 338/ − /1, 334 12, 408/ − /3, 720 1, 303 30.59s 7.22 8.22s 3.59s Open 10, 009/ − /4, 917 37, 421/ − /17, 505 12, 460 117.61s 14.78 36.18s 40.18s Cooking 75/27/25 10.05 5.45s 7.56s Dataset Domain # Videos (train/val/test) Charades-STA Indoors ActivityNet Cap TACoS 10, 146/4, 589/4, 083 2, 033 287.14s ¯"
2020.acl-main.585,P19-1285,0,0.0639323,"Missing"
2020.acl-main.585,N19-1423,0,0.0219609,"video as text passage, the above frameworks are all applicable to NLVL in principle. However, these frameworks are not designed to consider the differences between video and text passage. Their modeling complexity arises from the interactions between query and text passage, both are text. In our solution, VSLBase adopts a simple and standard span-based QA framework, making it easier to model the differences between video and text through adding additional modules. Our VSLNet addresses the differences by introducing the QGH module. Very recently, pre-trained transformer based language models (Devlin et al., 2019; Dai et al., 2019; Liu et al., 2019; Yang et al., 2019) have elevated the performance of span-based QA tasks by a large margin. Meanwhile, similar pre-trained models (Sun et al., 2019a,b; Yu and Jiang, 2019; Rahman et al., 2019; Nguyen and Okatani, 2019; Lu et al., 2019b; Tan and Bansal, 2019) are being proposed to learn joint distributions over multimodality sequence of visual and linguistic inputs. Exploring the pre-trained models for NLVL is part of our future work and is out of the scope of this study. 3 Methodology We now describe how to address NLVL task by adopting a span-based QA fram"
2020.acl-main.585,N19-1198,0,0.447668,"each frame in target moment, and select the ones with highest confidence as final result. Yuan et al. (2019a) proposes a semantic conditioned dynamic modulation for better correlating sentence related video contents over time, and establishing a precise matching relationship between sentence and video. There are also works (Wang et al., 2019; He et al., 2019) that formulate NLVL as a sequence decision making problem, and adopt reinforcement learning based approaches, to progressively observe candidate moments conditioned on language query. Most similar to our work are (Chen et al., 2019) and (Ghosh et al., 2019), as both studies are considered using the concept of question answering to address NLVL. However, both studies do not explain the similarity and differences between NLVL and traditional span-based QA, and they do not adopt the standard span-based QA framework. In our study, VSLBase adopts standard span-based QA framework; and VSLNet explicitly addresses the differences between NLVL and traditional spanbased QA tasks. Span-based Question Answering. Span-based QA has been widely studied in past years. Wang and Jiang (2017) combines match-LSTM (Wang and Jiang, 2016) and Pointer-Net (Vinyals et a"
2020.acl-main.585,D18-1168,0,0.176438,"Missing"
2020.acl-main.585,P19-1564,0,0.0218609,"1 Timeline (second) 0.00 Corresponding author. https://github.com/IsaacChanghau/VSLNet 194.69 Figure 1: An illustration of localizing a temporal moment in an untrimmed video by a given language query. Given an untrimmed video, natural language video localization (NLVL) is to retrieve or localize a temporal moment that semantically corresponds to a given language query. An example is shown in Figure 1. As an important vision-language understanding task, NLVL involves both computer vision and natural language processing techniques (Krishna et al., 2017; Hendricks et al., 2017; Gao et al., 2018; Le et al., 2019; Yu et al., 2019). Clearly, cross-modal reasoning is essential for NLVL to correctly locate the target moment from a video. Prior works primarily treat NLVL as a ranking task, which is solved by applying multimodal ∗ 139.20 The Ground Truth Moment Introduction 1 127.52 matching architecture to find the best matching video segment for a given language query (Gao et al., 2017; Hendricks et al., 2018; Liu et al., 2018a; Ge et al., 2019; Xu et al., 2019; Chen and Jiang, 2019; Zhang et al., 2019). Recently, some works explore to model cross-interactions between video and query, and to regress the"
2020.acl-main.585,2021.ccl-1.108,0,0.108927,"Missing"
2020.acl-main.585,D19-1518,0,0.559887,"r NLVL to correctly locate the target moment from a video. Prior works primarily treat NLVL as a ranking task, which is solved by applying multimodal ∗ 139.20 The Ground Truth Moment Introduction 1 127.52 matching architecture to find the best matching video segment for a given language query (Gao et al., 2017; Hendricks et al., 2018; Liu et al., 2018a; Ge et al., 2019; Xu et al., 2019; Chen and Jiang, 2019; Zhang et al., 2019). Recently, some works explore to model cross-interactions between video and query, and to regress the temporal locations of target moment directly (Yuan et al., 2019b; Lu et al., 2019a). There are also studies to formulate NLVL as a sequence decision making problem and to solve it by reinforcement learning (Wang et al., 2019; He et al., 2019). We address the NLVL task from a different perspective. The essence of NLVL is to search for a video moment as the answer to a given language query from an untrimmed video. By treating the video as a text passage, and the target moment as the answer span, NLVL shares significant similarities with span-based question answering (QA) task. The span-based QA framework (Seo et al., 2017; Wang et al., 2017; Huang et al., 2018) can be adopte"
2020.acl-main.585,D14-1162,0,0.0840244,"ere 10, 146, 4, 589 and 4, 083 annotations are used for training, validation and test, respectively. 4.2 Metrics. We adopt “R@n, IoU = µ” and “mIoU” as the evaluation metrics, following (Gao et al., 2017; Liu et al., 2018a; Yuan et al., 2019b). The “R@n, IoU = µ” denotes the percentage of language queries having at least one result whose Intersection over Union (IoU) with ground truth is larger than µ in top-n retrieved moments. “mIoU” is the average IoU over all testing samples. In our experiments, we use n = 1 and µ ∈ {0.3, 0.5, 0.7}. Implementation. For language query Q, we use 300d GloVe (Pennington et al., 2014) vectors to initialize each lowercase word; the word embeddings are fixed during training. For untrimmed video V , we downsample frames and extract RGB visual features using the 3D ConvNet which was pre-trained on Kinetics dataset (Carreira and Zisserman, 2017). We set the dimension of all the hidden layers in the model as 128; the kernel size of convolution layer is 7; the head size of multi-head attention is 8. For all datasets, the model is trained for 100 epochs with batch size of 16 and early stopping strategy. Parameter optimization is performed by Adam (Kingma and Ba, 2015) with learnin"
2020.acl-main.585,D19-1514,0,0.0247589,"our solution, VSLBase adopts a simple and standard span-based QA framework, making it easier to model the differences between video and text through adding additional modules. Our VSLNet addresses the differences by introducing the QGH module. Very recently, pre-trained transformer based language models (Devlin et al., 2019; Dai et al., 2019; Liu et al., 2019; Yang et al., 2019) have elevated the performance of span-based QA tasks by a large margin. Meanwhile, similar pre-trained models (Sun et al., 2019a,b; Yu and Jiang, 2019; Rahman et al., 2019; Nguyen and Okatani, 2019; Lu et al., 2019b; Tan and Bansal, 2019) are being proposed to learn joint distributions over multimodality sequence of visual and linguistic inputs. Exploring the pre-trained models for NLVL is part of our future work and is out of the scope of this study. 3 Methodology We now describe how to address NLVL task by adopting a span-based QA framework. We then present VSLBase (Sections 3.2 to 3.4) and VSLNet in detail. Their architectures are shown in Figure 2. 3.1 Span-based QA for NLVL We denote the untrimmed video as V = {ft }Tt=1 and the language query as Q = {qj }m j=1 , where T and m are the number of frames and words, respective"
2020.acl-main.585,D16-1264,0,0.0605245,"odology We now describe how to address NLVL task by adopting a span-based QA framework. We then present VSLBase (Sections 3.2 to 3.4) and VSLNet in detail. Their architectures are shown in Figure 2. 3.1 Span-based QA for NLVL We denote the untrimmed video as V = {ft }Tt=1 and the language query as Q = {qj }m j=1 , where T and m are the number of frames and words, respectively. τ s and τ e represent the start and end time of the temporal moment i.e., answer span. To address NLVL with span-based QA framework, its data is transformed into a set of SQuAD style triples (Context, Question, Answer) (Rajpurkar et al., 2016). For each video V , we extract its visual features V = {vi }ni=1 by a pre-trained 3D ConvNet (Carreira and Zisserman, 2017), where n is the number of extracted features. Here, V can be regarded as the sequence of word embeddings for a text passage with n tokens. Similar to word embeddings, each feature vi here is a video feature vector. Since span-based QA aims to predict start and end boundaries of an answer span, the start/end time of a video sequence needs to be mapped to the corresponding boundaries in the visual feature sequence V. Suppose the video duration is T , the start (end) span i"
2020.acl-main.585,Q13-1003,0,0.420952,"s in the model as 128; the kernel size of convolution layer is 7; the head size of multi-head attention is 8. For all datasets, the model is trained for 100 epochs with batch size of 16 and early stopping strategy. Parameter optimization is performed by Adam (Kingma and Ba, 2015) with learning rate of 0.0001, linear decay of learning rate and gradient clipping of 1.0. Dropout (Srivastava et al., 2014) of 0.2 is applied to prevent overfitting. 4.3 Datasets We conduct experiments on three benchmark datasets: Charades-STA (Gao et al., 2017), ActivityNet Caption (Krishna et al., 2017), and TACoS (Regneri et al., 2013), summarized in Table 1. Charades-STA is prepared by Gao et al. (2017) based on Charades dataset (Sigurdsson et al., 2016). Experimental Settings Comparison with State-of-the-Arts We compare VSLBase and VSLNet with the following state-of-the-arts: CTRL (Gao et al., 2017), ACRN (Liu et al., 2018a), TGN (Chen et al., 2018), ACL-K (Ge et al., 2019), QSPN (Xu et al., 2019), SAP (Chen and Jiang, 2019), MAN (Zhang et al., 2019), SM-RL (Wang et al., 2019), RWMRL (He et al., 2019), L-Net (Chen et al., 2019), ExCL (Ghosh et al., 2019), ABLR (Yuan et al., 6547 # Annotations Nvocab ¯ video L ¯ query L ¯"
2020.acl-main.585,N16-1170,0,0.0177214,"rk are (Chen et al., 2019) and (Ghosh et al., 2019), as both studies are considered using the concept of question answering to address NLVL. However, both studies do not explain the similarity and differences between NLVL and traditional span-based QA, and they do not adopt the standard span-based QA framework. In our study, VSLBase adopts standard span-based QA framework; and VSLNet explicitly addresses the differences between NLVL and traditional spanbased QA tasks. Span-based Question Answering. Span-based QA has been widely studied in past years. Wang and Jiang (2017) combines match-LSTM (Wang and Jiang, 2016) and Pointer-Net (Vinyals et al., 2015) to estimate boundaries of the answer span. BiDAF (Seo et al., 2017) introduces bi-directional attention to obtain query-aware context representation. Xiong et al. (2017) proposes a coattention network to capture the interactions between context and query. R-Net (Wang et al., 2017) integrates mutual and self attentions into RNN encoder for feature refinement. QANet (Yu et al., 2018) lever6544 & ? … ? ?′ Feature Encoder ?&quot; stove Feature Extractor (fixed during training) (a) VSLBase & ? ?! QGH &! ? Conditioned Span Predictor Shared Context-Query Attention F"
2020.acl-main.585,P17-1018,0,0.136074,"moment directly (Yuan et al., 2019b; Lu et al., 2019a). There are also studies to formulate NLVL as a sequence decision making problem and to solve it by reinforcement learning (Wang et al., 2019; He et al., 2019). We address the NLVL task from a different perspective. The essence of NLVL is to search for a video moment as the answer to a given language query from an untrimmed video. By treating the video as a text passage, and the target moment as the answer span, NLVL shares significant similarities with span-based question answering (QA) task. The span-based QA framework (Seo et al., 2017; Wang et al., 2017; Huang et al., 2018) can be adopted for NLVL. Hence, we attempt to solve this task with a multimodal span-based QA approach. There are two main differences between traditional text span-based QA and NLVL tasks. First, video is continuous and causal relations between video events are usually adjacent. Natural language, on the other hand, is inconsecutive and words in a sentence demonstrate syntactic structure. For instance, changes between adjacent video frames are usually very small, while adjacent word to6543 Proceedings of the 58th Annual Meeting of the Association for Computational Linguis"
2020.acl-main.654,C16-1236,0,0.0281047,"t in the passage, the answers of MCQA may not appear in the text passage and may involve complex Corresponding author. MMT model Meta model Introduction ∗ Multi-source Meta Transfer language inference. Thus, MCQA usually requires more advanced reading comprehension abilities, including arithmetic operation, summarization, logic reasoning and commonsense reasoning (Richardson et al., 2013; Sun et al., 2019a), and etc. In addition, the size of most existing MCQA datasets is much smaller than that of the extractive/abstractive QA datasets. For instance, all the span-based QA datasets, except CQ (Bao et al., 2016), contain more than 100k samples. In contrast, the data size of most existing MCQA datasets are far less than 100k (see Table 1), and the smallest one only contains 660 samples. The above two major challenges make MCQA much more difficult to optimize and generalize, especially for the low resource issue. In order to achieve better performance on downstream NLP tasks, it is inevitable to fine-tune the pre-trained deep language models (Devlin et al., 2019; Raffel et al., 2019; Dai et al., 2019; Liu et al., 2019; Yang et al., 2019) with a large number of supervised target data for reducing the di"
2020.acl-main.654,P18-2044,0,0.0257258,"lt through considering different contexts and scenes. For instance, Guo et al. (2017) present an open-domain comprehension dataset; Lai et al. (2017) build a QA dataset from examinations, which requires more complex reasoning on questions; and Zellers et al. (2018) introduce a QA dataset that requires both natural language inference and commonsense reasoning. Meanwhile, various approaches have been proposed to address the MCQA task using different neural network architectures. Some works propose to compute the similarity between question and each of the choices through an attention mechanism (Chaturvedi et al., 2018; Wang et al., 2018). Kumar et al. (2016) construct the context embedding for semantic representation. Liu et al. (2018) and Yu et al. (2019) apply the recurrent memory network for question reasoning. Chung et al. (2018) and Jin et al. (2019) further incorporate an attention mechanism into recurrent memory networks for multi-step reasoning. Most existing works only strive to increase the reasoning capability by constructing complex models, but ignore the low resource nature of those available MCQA datasets. 7332 3 Supervised MMT Methodology MMT model Many existing MCQA tasks suffer from the lo"
2020.acl-main.654,N19-1405,0,0.0140376,"tion to new tasks under both supervised (Qian and Yu, 2019; Obamuyide and Vlachos, 2019) and unsupervised (Srivastava et al., 2018) scenarios. Unfortunately, meta learning is seldom studied in multiple-choice question answering in existing methods. To our best knowledge, it is also the first time to extend meta learning into multi-source scenarios. 2.2 Multiple-Choice Question Answering Multiple-choice question answering (MCQA) is a challenging task, which requires understanding the relationships and handle the interactions between passages, questions and choices to select the correct answer (Chen and Durrett, 2019). As one of the hot track of question answering tasks, MCQA has seen a great surge of challenging datasets and novel architectures recently. These datasets are built through considering different contexts and scenes. For instance, Guo et al. (2017) present an open-domain comprehension dataset; Lai et al. (2017) build a QA dataset from examinations, which requires more complex reasoning on questions; and Zellers et al. (2018) introduce a QA dataset that requires both natural language inference and commonsense reasoning. Meanwhile, various approaches have been proposed to address the MCQA task u"
2020.acl-main.654,D18-1398,0,0.0263431,"“learning to learn”, intends to design models that can learn general data representation and adapt to new tasks with a few training samples (Finn et al., 2017; Nichol et al., 2018). Early works have demonstrated that meta learning is capable of boosting the performance of natural language processing (NLP) tasks, such as named entity recognition (Munro et al., 2003) and grammatical error correction (Seo et al., 2012). Recently, meta learning gains more and more attention. Many works explore to adopt meta learning to address low resource issues in various NLP tasks, such as machine translation (Gu et al., 2018; Sennrich and Zhang, 2019), semantic parsing (Guo et al., 2019), query generation (Huang et al., 2018), emotion distribution learning (Zhao and Ma, 2019), relation classification (Wu et al., 2019; Obamuyide and Vlachos, 2019) and etc. These methods have all achieved good performance due to their powerful data representation ability. Meanwhile, the strong learning capability of meta learning also provides deep models with a better initialization, and boosts deep models fast adaptation to new tasks under both supervised (Qian and Yu, 2019; Obamuyide and Vlachos, 2019) and unsupervised (Srivasta"
2020.acl-main.654,P19-1082,0,0.0280682,"eneral data representation and adapt to new tasks with a few training samples (Finn et al., 2017; Nichol et al., 2018). Early works have demonstrated that meta learning is capable of boosting the performance of natural language processing (NLP) tasks, such as named entity recognition (Munro et al., 2003) and grammatical error correction (Seo et al., 2012). Recently, meta learning gains more and more attention. Many works explore to adopt meta learning to address low resource issues in various NLP tasks, such as machine translation (Gu et al., 2018; Sennrich and Zhang, 2019), semantic parsing (Guo et al., 2019), query generation (Huang et al., 2018), emotion distribution learning (Zhao and Ma, 2019), relation classification (Wu et al., 2019; Obamuyide and Vlachos, 2019) and etc. These methods have all achieved good performance due to their powerful data representation ability. Meanwhile, the strong learning capability of meta learning also provides deep models with a better initialization, and boosts deep models fast adaptation to new tasks under both supervised (Qian and Yu, 2019; Obamuyide and Vlachos, 2019) and unsupervised (Srivastava et al., 2018) scenarios. Unfortunately, meta learning is seld"
2020.acl-main.654,N18-1143,0,0.0488782,"Missing"
2020.acl-main.654,P19-1285,0,0.0470254,"Missing"
2020.acl-main.654,N19-1423,0,0.527407,"ting MCQA datasets is much smaller than that of the extractive/abstractive QA datasets. For instance, all the span-based QA datasets, except CQ (Bao et al., 2016), contain more than 100k samples. In contrast, the data size of most existing MCQA datasets are far less than 100k (see Table 1), and the smallest one only contains 660 samples. The above two major challenges make MCQA much more difficult to optimize and generalize, especially for the low resource issue. In order to achieve better performance on downstream NLP tasks, it is inevitable to fine-tune the pre-trained deep language models (Devlin et al., 2019; Raffel et al., 2019; Dai et al., 2019; Liu et al., 2019; Yang et al., 2019) with a large number of supervised target data for reducing the discrepancy between the training source and target data. Due to the low resource nature, the performance of most existing 7331 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7331–7341 c July 5 - 10, 2020. 2020 Association for Computational Linguistics MCQA methods is far from satisfactory. To alleviate such issue in MCQA, one straightforward solution is to merge all available data resources for training (Pal"
2020.acl-main.654,N19-1246,0,0.0219145,"Missing"
2020.acl-main.654,N18-2115,0,0.0203572,"to new tasks with a few training samples (Finn et al., 2017; Nichol et al., 2018). Early works have demonstrated that meta learning is capable of boosting the performance of natural language processing (NLP) tasks, such as named entity recognition (Munro et al., 2003) and grammatical error correction (Seo et al., 2012). Recently, meta learning gains more and more attention. Many works explore to adopt meta learning to address low resource issues in various NLP tasks, such as machine translation (Gu et al., 2018; Sennrich and Zhang, 2019), semantic parsing (Guo et al., 2019), query generation (Huang et al., 2018), emotion distribution learning (Zhao and Ma, 2019), relation classification (Wu et al., 2019; Obamuyide and Vlachos, 2019) and etc. These methods have all achieved good performance due to their powerful data representation ability. Meanwhile, the strong learning capability of meta learning also provides deep models with a better initialization, and boosts deep models fast adaptation to new tasks under both supervised (Qian and Yu, 2019; Obamuyide and Vlachos, 2019) and unsupervised (Srivastava et al., 2018) scenarios. Unfortunately, meta learning is seldom studied in multiple-choice question"
2020.acl-main.654,N18-1023,0,0.014552,"k in source 4 MMT representation Source representation MML MTL Figure 1: Comparison of meta learning and multisource meta transfer learning (MMT). “MTL” denotes meta transfer learning, and “MML” denotes multisource meta learning. Recently, there has been a growing interest in making machines to understand human languages, and a great progress has been made in machine reading comprehension (MRC). There are two main types of MRC task: 1) extractive/abstractive question answering (QA) such as SQuAD (Rajpurkar et al., 2018) and DROP (Dua et al., 2019); 2) multiplechoice QA (MCQA) such as MultiRC (Khashabi et al., 2018) and DREAM (Sun et al., 2019a). Different from extractive/abstractive QA whose answers are usually limited to the text spans exist in the passage, the answers of MCQA may not appear in the text passage and may involve complex Corresponding author. MMT model Meta model Introduction ∗ Multi-source Meta Transfer language inference. Thus, MCQA usually requires more advanced reading comprehension abilities, including arithmetic operation, summarization, logic reasoning and commonsense reasoning (Richardson et al., 2013; Sun et al., 2019a), and etc. In addition, the size of most existing MCQA datase"
2020.acl-main.654,D17-1082,0,0.115158,"ing into multi-source scenarios. 2.2 Multiple-Choice Question Answering Multiple-choice question answering (MCQA) is a challenging task, which requires understanding the relationships and handle the interactions between passages, questions and choices to select the correct answer (Chen and Durrett, 2019). As one of the hot track of question answering tasks, MCQA has seen a great surge of challenging datasets and novel architectures recently. These datasets are built through considering different contexts and scenes. For instance, Guo et al. (2017) present an open-domain comprehension dataset; Lai et al. (2017) build a QA dataset from examinations, which requires more complex reasoning on questions; and Zellers et al. (2018) introduce a QA dataset that requires both natural language inference and commonsense reasoning. Meanwhile, various approaches have been proposed to address the MCQA task using different neural network architectures. Some works propose to compute the similarity between question and each of the choices through an attention mechanism (Chaturvedi et al., 2018; Wang et al., 2018). Kumar et al. (2016) construct the context embedding for semantic representation. Liu et al. (2018) and Y"
2020.acl-main.654,D19-1282,0,0.0153003,"then on SWAG. Unsupervised MMT(S+R) denotes that the meta model is trained on the sources of SWAG and RACE. the target domain. For a more fair comparison, we also create several transfer learning baselines that can utilize multiple training sources such as TL(RS) and TL(S-R). From the results, we can conclude that unsupervised MMT is a better solution to make full use of multiple training sources than sequential transfer learning. Similar observations hold on SWAG dataset. Reported in Table 4, unsupervised MMT outperforms other methods significantly. Note we follow the same setting in KagNet (Lin et al., 2019) that only the development set of SWAG is evaluated. Method LSTM+GLV (Zellers et al., 2018) DA+GlV (Zellers et al., 2018) DA+ELMo (Zellers et al., 2018) TL(R) TL(M) TL(R-M) TL(M-R) TL(M+R) Unsupervised MMT(R+M) Sup. Yes Yes Yes No No No No No No Dev 43.1 47.4 47.7 44.83 50.03 46.48 46.91 48.65 50.77 Table 4: Unsupervised domain adaptation on SWAG, where “M” denotes MCTEST, “R” denotes RACE, “DA” denotes decomposable attention, and “GLV” denotes GloVe vectors. 5 5.1 Discussion Ablation Study We conduct ablative experiments to analyze the two modules of MMT, i.e., multi-source meta learning (MML"
2020.acl-main.654,P18-1157,0,0.0227274,"aset; Lai et al. (2017) build a QA dataset from examinations, which requires more complex reasoning on questions; and Zellers et al. (2018) introduce a QA dataset that requires both natural language inference and commonsense reasoning. Meanwhile, various approaches have been proposed to address the MCQA task using different neural network architectures. Some works propose to compute the similarity between question and each of the choices through an attention mechanism (Chaturvedi et al., 2018; Wang et al., 2018). Kumar et al. (2016) construct the context embedding for semantic representation. Liu et al. (2018) and Yu et al. (2019) apply the recurrent memory network for question reasoning. Chung et al. (2018) and Jin et al. (2019) further incorporate an attention mechanism into recurrent memory networks for multi-step reasoning. Most existing works only strive to increase the reasoning capability by constructing complex models, but ignore the low resource nature of those available MCQA datasets. 7332 3 Supervised MMT Methodology MMT model Many existing MCQA tasks suffer from the lowresource issue, which requires a special training strategy to tackle it. Recent advance of meta learning shows its adva"
2020.acl-main.654,2021.ccl-1.108,0,0.166245,"Missing"
2020.acl-main.654,W03-0431,0,0.0867366,"upstream framework, i.e., it can be seamlessly incorporated into any existing backbone language models to improve performance. Figure 1 briefly illustrates both meta learning and the proposed MMT. 2 2.1 Related Work Meta Learning Meta learning, a.k.a “learning to learn”, intends to design models that can learn general data representation and adapt to new tasks with a few training samples (Finn et al., 2017; Nichol et al., 2018). Early works have demonstrated that meta learning is capable of boosting the performance of natural language processing (NLP) tasks, such as named entity recognition (Munro et al., 2003) and grammatical error correction (Seo et al., 2012). Recently, meta learning gains more and more attention. Many works explore to adopt meta learning to address low resource issues in various NLP tasks, such as machine translation (Gu et al., 2018; Sennrich and Zhang, 2019), semantic parsing (Guo et al., 2019), query generation (Huang et al., 2018), emotion distribution learning (Zhao and Ma, 2019), relation classification (Wu et al., 2019; Obamuyide and Vlachos, 2019) and etc. These methods have all achieved good performance due to their powerful data representation ability. Meanwhile, the s"
2020.acl-main.654,P19-1589,0,0.0891817,"hat meta learning is capable of boosting the performance of natural language processing (NLP) tasks, such as named entity recognition (Munro et al., 2003) and grammatical error correction (Seo et al., 2012). Recently, meta learning gains more and more attention. Many works explore to adopt meta learning to address low resource issues in various NLP tasks, such as machine translation (Gu et al., 2018; Sennrich and Zhang, 2019), semantic parsing (Guo et al., 2019), query generation (Huang et al., 2018), emotion distribution learning (Zhao and Ma, 2019), relation classification (Wu et al., 2019; Obamuyide and Vlachos, 2019) and etc. These methods have all achieved good performance due to their powerful data representation ability. Meanwhile, the strong learning capability of meta learning also provides deep models with a better initialization, and boosts deep models fast adaptation to new tasks under both supervised (Qian and Yu, 2019; Obamuyide and Vlachos, 2019) and unsupervised (Srivastava et al., 2018) scenarios. Unfortunately, meta learning is seldom studied in multiple-choice question answering in existing methods. To our best knowledge, it is also the first time to extend meta learning into multi-source s"
2020.acl-main.654,S18-1119,0,0.0169343,"metic, commonsense, etc. MCTEST (Richardson et al., 2013) is a fictional stories dataset which aims to evaluate open-domain machine comprehension. The stories contain open domain topics, and the questions and choices are created by crowd-sourcing with strict grammar, quality guarantee. RACE (Lai et al., 2017) is a dataset about passage reading comprehension, which collected from middle/high school English examinations. Human experts design the questions, and the passages cover various categories of human articles: news, stories, advertisements, biography, philosophy, etc. SemEval-2018-Task11 (Ostermann et al., 2018) consists of scenario-related narrative text and various types of questions. The goal is to evaluate the machine comprehension for commonsense knowledge. SWAG (Zellers et al., 2018) is a dataset about rich grounded situations, which is constructed debiased with adversarial filtering and explores the gap between machine comprehension and human. The statistics of DREAM, MCTEST, RACE, SemEval-2018-Task11 (SemEval) and SWAG are summarized in Table 1. 7335 Name Type Ages Generator Level Choices Samples Questions DREAM Dialogue 15+ Expert High School/College 3 6,444 10,197 RACE Exam 12-18 Expert Hig"
2020.acl-main.654,W19-2305,0,0.0462084,"Missing"
2020.acl-main.654,P19-1253,0,0.0362434,"issues in various NLP tasks, such as machine translation (Gu et al., 2018; Sennrich and Zhang, 2019), semantic parsing (Guo et al., 2019), query generation (Huang et al., 2018), emotion distribution learning (Zhao and Ma, 2019), relation classification (Wu et al., 2019; Obamuyide and Vlachos, 2019) and etc. These methods have all achieved good performance due to their powerful data representation ability. Meanwhile, the strong learning capability of meta learning also provides deep models with a better initialization, and boosts deep models fast adaptation to new tasks under both supervised (Qian and Yu, 2019; Obamuyide and Vlachos, 2019) and unsupervised (Srivastava et al., 2018) scenarios. Unfortunately, meta learning is seldom studied in multiple-choice question answering in existing methods. To our best knowledge, it is also the first time to extend meta learning into multi-source scenarios. 2.2 Multiple-Choice Question Answering Multiple-choice question answering (MCQA) is a challenging task, which requires understanding the relationships and handle the interactions between passages, questions and choices to select the correct answer (Chen and Durrett, 2019). As one of the hot track of questi"
2020.acl-main.654,P18-2124,0,0.0171105,"domain adaptation settings. 1 Target 2 1 3 Task in source 1 Task in source 2 Task in source 3 Task in source 4 MMT representation Source representation MML MTL Figure 1: Comparison of meta learning and multisource meta transfer learning (MMT). “MTL” denotes meta transfer learning, and “MML” denotes multisource meta learning. Recently, there has been a growing interest in making machines to understand human languages, and a great progress has been made in machine reading comprehension (MRC). There are two main types of MRC task: 1) extractive/abstractive question answering (QA) such as SQuAD (Rajpurkar et al., 2018) and DROP (Dua et al., 2019); 2) multiplechoice QA (MCQA) such as MultiRC (Khashabi et al., 2018) and DREAM (Sun et al., 2019a). Different from extractive/abstractive QA whose answers are usually limited to the text spans exist in the passage, the answers of MCQA may not appear in the text passage and may involve complex Corresponding author. MMT model Meta model Introduction ∗ Multi-source Meta Transfer language inference. Thus, MCQA usually requires more advanced reading comprehension abilities, including arithmetic operation, summarization, logic reasoning and commonsense reasoning (Richard"
2020.acl-main.654,D13-1020,0,0.105758,", 2018) and DROP (Dua et al., 2019); 2) multiplechoice QA (MCQA) such as MultiRC (Khashabi et al., 2018) and DREAM (Sun et al., 2019a). Different from extractive/abstractive QA whose answers are usually limited to the text spans exist in the passage, the answers of MCQA may not appear in the text passage and may involve complex Corresponding author. MMT model Meta model Introduction ∗ Multi-source Meta Transfer language inference. Thus, MCQA usually requires more advanced reading comprehension abilities, including arithmetic operation, summarization, logic reasoning and commonsense reasoning (Richardson et al., 2013; Sun et al., 2019a), and etc. In addition, the size of most existing MCQA datasets is much smaller than that of the extractive/abstractive QA datasets. For instance, all the span-based QA datasets, except CQ (Bao et al., 2016), contain more than 100k samples. In contrast, the data size of most existing MCQA datasets are far less than 100k (see Table 1), and the smallest one only contains 660 samples. The above two major challenges make MCQA much more difficult to optimize and generalize, especially for the low resource issue. In order to achieve better performance on downstream NLP tasks, it"
2020.acl-main.654,P19-1021,0,0.0373089,"n”, intends to design models that can learn general data representation and adapt to new tasks with a few training samples (Finn et al., 2017; Nichol et al., 2018). Early works have demonstrated that meta learning is capable of boosting the performance of natural language processing (NLP) tasks, such as named entity recognition (Munro et al., 2003) and grammatical error correction (Seo et al., 2012). Recently, meta learning gains more and more attention. Many works explore to adopt meta learning to address low resource issues in various NLP tasks, such as machine translation (Gu et al., 2018; Sennrich and Zhang, 2019), semantic parsing (Guo et al., 2019), query generation (Huang et al., 2018), emotion distribution learning (Zhao and Ma, 2019), relation classification (Wu et al., 2019; Obamuyide and Vlachos, 2019) and etc. These methods have all achieved good performance due to their powerful data representation ability. Meanwhile, the strong learning capability of meta learning also provides deep models with a better initialization, and boosts deep models fast adaptation to new tasks under both supervised (Qian and Yu, 2019; Obamuyide and Vlachos, 2019) and unsupervised (Srivastava et al., 2018) scenarios."
2020.acl-main.654,P12-2064,0,0.0152435,"orated into any existing backbone language models to improve performance. Figure 1 briefly illustrates both meta learning and the proposed MMT. 2 2.1 Related Work Meta Learning Meta learning, a.k.a “learning to learn”, intends to design models that can learn general data representation and adapt to new tasks with a few training samples (Finn et al., 2017; Nichol et al., 2018). Early works have demonstrated that meta learning is capable of boosting the performance of natural language processing (NLP) tasks, such as named entity recognition (Munro et al., 2003) and grammatical error correction (Seo et al., 2012). Recently, meta learning gains more and more attention. Many works explore to adopt meta learning to address low resource issues in various NLP tasks, such as machine translation (Gu et al., 2018; Sennrich and Zhang, 2019), semantic parsing (Guo et al., 2019), query generation (Huang et al., 2018), emotion distribution learning (Zhao and Ma, 2019), relation classification (Wu et al., 2019; Obamuyide and Vlachos, 2019) and etc. These methods have all achieved good performance due to their powerful data representation ability. Meanwhile, the strong learning capability of meta learning also prov"
2020.acl-main.654,P18-1029,0,0.0282454,"l., 2018; Sennrich and Zhang, 2019), semantic parsing (Guo et al., 2019), query generation (Huang et al., 2018), emotion distribution learning (Zhao and Ma, 2019), relation classification (Wu et al., 2019; Obamuyide and Vlachos, 2019) and etc. These methods have all achieved good performance due to their powerful data representation ability. Meanwhile, the strong learning capability of meta learning also provides deep models with a better initialization, and boosts deep models fast adaptation to new tasks under both supervised (Qian and Yu, 2019; Obamuyide and Vlachos, 2019) and unsupervised (Srivastava et al., 2018) scenarios. Unfortunately, meta learning is seldom studied in multiple-choice question answering in existing methods. To our best knowledge, it is also the first time to extend meta learning into multi-source scenarios. 2.2 Multiple-Choice Question Answering Multiple-choice question answering (MCQA) is a challenging task, which requires understanding the relationships and handle the interactions between passages, questions and choices to select the correct answer (Chen and Durrett, 2019). As one of the hot track of question answering tasks, MCQA has seen a great surge of challenging datasets a"
2020.acl-main.654,D19-1408,0,0.0195087,"., 2017; Nichol et al., 2018). Early works have demonstrated that meta learning is capable of boosting the performance of natural language processing (NLP) tasks, such as named entity recognition (Munro et al., 2003) and grammatical error correction (Seo et al., 2012). Recently, meta learning gains more and more attention. Many works explore to adopt meta learning to address low resource issues in various NLP tasks, such as machine translation (Gu et al., 2018; Sennrich and Zhang, 2019), semantic parsing (Guo et al., 2019), query generation (Huang et al., 2018), emotion distribution learning (Zhao and Ma, 2019), relation classification (Wu et al., 2019; Obamuyide and Vlachos, 2019) and etc. These methods have all achieved good performance due to their powerful data representation ability. Meanwhile, the strong learning capability of meta learning also provides deep models with a better initialization, and boosts deep models fast adaptation to new tasks under both supervised (Qian and Yu, 2019; Obamuyide and Vlachos, 2019) and unsupervised (Srivastava et al., 2018) scenarios. Unfortunately, meta learning is seldom studied in multiple-choice question answering in existing methods. To our best knowledg"
2020.acl-main.654,Q19-1014,0,0.20542,"ource representation MML MTL Figure 1: Comparison of meta learning and multisource meta transfer learning (MMT). “MTL” denotes meta transfer learning, and “MML” denotes multisource meta learning. Recently, there has been a growing interest in making machines to understand human languages, and a great progress has been made in machine reading comprehension (MRC). There are two main types of MRC task: 1) extractive/abstractive question answering (QA) such as SQuAD (Rajpurkar et al., 2018) and DROP (Dua et al., 2019); 2) multiplechoice QA (MCQA) such as MultiRC (Khashabi et al., 2018) and DREAM (Sun et al., 2019a). Different from extractive/abstractive QA whose answers are usually limited to the text spans exist in the passage, the answers of MCQA may not appear in the text passage and may involve complex Corresponding author. MMT model Meta model Introduction ∗ Multi-source Meta Transfer language inference. Thus, MCQA usually requires more advanced reading comprehension abilities, including arithmetic operation, summarization, logic reasoning and commonsense reasoning (Richardson et al., 2013; Sun et al., 2019a), and etc. In addition, the size of most existing MCQA datasets is much smaller than that"
2020.acl-main.654,N19-1270,0,0.0340912,"Missing"
2020.acl-main.654,P18-2118,0,0.113173,"fferent contexts and scenes. For instance, Guo et al. (2017) present an open-domain comprehension dataset; Lai et al. (2017) build a QA dataset from examinations, which requires more complex reasoning on questions; and Zellers et al. (2018) introduce a QA dataset that requires both natural language inference and commonsense reasoning. Meanwhile, various approaches have been proposed to address the MCQA task using different neural network architectures. Some works propose to compute the similarity between question and each of the choices through an attention mechanism (Chaturvedi et al., 2018; Wang et al., 2018). Kumar et al. (2016) construct the context embedding for semantic representation. Liu et al. (2018) and Yu et al. (2019) apply the recurrent memory network for question reasoning. Chung et al. (2018) and Jin et al. (2019) further incorporate an attention mechanism into recurrent memory networks for multi-step reasoning. Most existing works only strive to increase the reasoning capability by constructing complex models, but ignore the low resource nature of those available MCQA datasets. 7332 3 Supervised MMT Methodology MMT model Many existing MCQA tasks suffer from the lowresource issue, whi"
2020.acl-main.654,D19-1444,0,0.0243763,"ve demonstrated that meta learning is capable of boosting the performance of natural language processing (NLP) tasks, such as named entity recognition (Munro et al., 2003) and grammatical error correction (Seo et al., 2012). Recently, meta learning gains more and more attention. Many works explore to adopt meta learning to address low resource issues in various NLP tasks, such as machine translation (Gu et al., 2018; Sennrich and Zhang, 2019), semantic parsing (Guo et al., 2019), query generation (Huang et al., 2018), emotion distribution learning (Zhao and Ma, 2019), relation classification (Wu et al., 2019; Obamuyide and Vlachos, 2019) and etc. These methods have all achieved good performance due to their powerful data representation ability. Meanwhile, the strong learning capability of meta learning also provides deep models with a better initialization, and boosts deep models fast adaptation to new tasks under both supervised (Qian and Yu, 2019; Obamuyide and Vlachos, 2019) and unsupervised (Srivastava et al., 2018) scenarios. Unfortunately, meta learning is seldom studied in multiple-choice question answering in existing methods. To our best knowledge, it is also the first time to extend met"
2020.acl-main.654,P19-1217,0,0.255815,") build a QA dataset from examinations, which requires more complex reasoning on questions; and Zellers et al. (2018) introduce a QA dataset that requires both natural language inference and commonsense reasoning. Meanwhile, various approaches have been proposed to address the MCQA task using different neural network architectures. Some works propose to compute the similarity between question and each of the choices through an attention mechanism (Chaturvedi et al., 2018; Wang et al., 2018). Kumar et al. (2016) construct the context embedding for semantic representation. Liu et al. (2018) and Yu et al. (2019) apply the recurrent memory network for question reasoning. Chung et al. (2018) and Jin et al. (2019) further incorporate an attention mechanism into recurrent memory networks for multi-step reasoning. Most existing works only strive to increase the reasoning capability by constructing complex models, but ignore the low resource nature of those available MCQA datasets. 7332 3 Supervised MMT Methodology MMT model Many existing MCQA tasks suffer from the lowresource issue, which requires a special training strategy to tackle it. Recent advance of meta learning shows its advantages in solving the"
2020.acl-main.654,D18-1009,0,0.378392,") is a challenging task, which requires understanding the relationships and handle the interactions between passages, questions and choices to select the correct answer (Chen and Durrett, 2019). As one of the hot track of question answering tasks, MCQA has seen a great surge of challenging datasets and novel architectures recently. These datasets are built through considering different contexts and scenes. For instance, Guo et al. (2017) present an open-domain comprehension dataset; Lai et al. (2017) build a QA dataset from examinations, which requires more complex reasoning on questions; and Zellers et al. (2018) introduce a QA dataset that requires both natural language inference and commonsense reasoning. Meanwhile, various approaches have been proposed to address the MCQA task using different neural network architectures. Some works propose to compute the similarity between question and each of the choices through an attention mechanism (Chaturvedi et al., 2018; Wang et al., 2018). Kumar et al. (2016) construct the context embedding for semantic representation. Liu et al. (2018) and Yu et al. (2019) apply the recurrent memory network for question reasoning. Chung et al. (2018) and Jin et al. (2019)"
2020.coling-main.411,P17-1183,0,0.0217803,"at the current position, as well as the embedding of the previous tag is fed to GRUdec . The output of GRUdec is fed to a softmax layer to generate the output tagging sequence. We model P (Y |X) with an encoder-decoder RNN. The encoder generates hidden state sequences of length I. At decoding step i, the decoder attends to position i in both the hidden state sequences and the input embedding sequence. Figure 1 illustrates the architecture of the model. Our architecture is a modification of the stacking LSTM architecture of Ma et al. (2018) which can also be understood as using hard attention (Aharoni and Goldberg, 2017) in the decoder. We do not use bigram character features. Instead, we rely on forward and backward RNNs for implicit input feature extraction. We make the decoder auto-regressive by feeding the previously predicted tag to the decoder RNN and applying beam search in inference. 3.1 Pre-training RNN models demand a large number of training examples. While labelled data is often scarce, un-labelled data with matching domain is often abundant. Pre-training the encoder component of a sequence-tosequence model for a different task such as language modelling has been extremely successful, as manifeste"
2020.coling-main.411,N19-1423,0,0.0119081,"Missing"
2020.coling-main.411,D18-1295,0,0.0147265,"(2009).1 A closely related problem is compound splitting. Macherey et al. (2011) presented an unsupervised probabilistic model for splitting compound words into parts, with the compound part sub-model being a zero-order model to enable efficient dynamic programming inference. The model is optimized for the task of machine translation. They only reported results for seven (Germanic, Hellenic, and Uralic) languages other than English. More recently, fully supervised letter sequence labelling models have been introduced for German compound splitting (Ma et al., 2016) and Sanskrit word splitting (Hellwig and Nehrdich, 2018). Pre-training can potentially further improve these models. There is a large body of research on Chinese word segmentation. The best models are supervised ones using structured prediction (Peng et al., 2004), transition-based models (Zhang and Clark, 2007), and most recently RNNs (Ma et al., 2018) and BERT-based models (Huang et al., 2019). The superior results of the BERT-based models demonstrate that pre-training is effective on word segmentation. Unlike BERT, we pre-train the entire model, not just the encoder. 3 RNN Tagging Model We formulate the segmentation problem as a character sequen"
2020.coling-main.411,W16-2012,0,0.0158912,"r FSTs following the work of Goldwater et al. (2009).1 A closely related problem is compound splitting. Macherey et al. (2011) presented an unsupervised probabilistic model for splitting compound words into parts, with the compound part sub-model being a zero-order model to enable efficient dynamic programming inference. The model is optimized for the task of machine translation. They only reported results for seven (Germanic, Hellenic, and Uralic) languages other than English. More recently, fully supervised letter sequence labelling models have been introduced for German compound splitting (Ma et al., 2016) and Sanskrit word splitting (Hellwig and Nehrdich, 2018). Pre-training can potentially further improve these models. There is a large body of research on Chinese word segmentation. The best models are supervised ones using structured prediction (Peng et al., 2004), transition-based models (Zhang and Clark, 2007), and most recently RNNs (Ma et al., 2018) and BERT-based models (Huang et al., 2019). The superior results of the BERT-based models demonstrate that pre-training is effective on word segmentation. Unlike BERT, we pre-train the entire model, not just the encoder. 3 RNN Tagging Model We"
2020.coling-main.411,D18-1529,0,0.102267,"sk of machine translation. They only reported results for seven (Germanic, Hellenic, and Uralic) languages other than English. More recently, fully supervised letter sequence labelling models have been introduced for German compound splitting (Ma et al., 2016) and Sanskrit word splitting (Hellwig and Nehrdich, 2018). Pre-training can potentially further improve these models. There is a large body of research on Chinese word segmentation. The best models are supervised ones using structured prediction (Peng et al., 2004), transition-based models (Zhang and Clark, 2007), and most recently RNNs (Ma et al., 2018) and BERT-based models (Huang et al., 2019). The superior results of the BERT-based models demonstrate that pre-training is effective on word segmentation. Unlike BERT, we pre-train the entire model, not just the encoder. 3 RNN Tagging Model We formulate the segmentation problem as a character sequence tagging model. Given an input character sequence X = x1 , . . . , xI , the model predicts an output tag sequence Y = y1 , . . . , yI , with yi ∈ {B, I} being the tag for the character xi . B indicates the underlying character starts a new segment. I indicates the underlying character continues t"
2020.coling-main.411,P11-1140,0,0.0097515,"models as features. Fine-tuning on in-domain data is the counterpart in our system. Both have created training or evaluation data sets of URL domain names for their experiments, but these have not been publicly released. We contribute a data set of URLs crawled from a public repository of Web texts with their internal segments annotated by crowd sourcing. Chiang et al. (2010) reported experiments on a related artificial problem of splitting of space-free English through Bayesian inference for FSTs following the work of Goldwater et al. (2009).1 A closely related problem is compound splitting. Macherey et al. (2011) presented an unsupervised probabilistic model for splitting compound words into parts, with the compound part sub-model being a zero-order model to enable efficient dynamic programming inference. The model is optimized for the task of machine translation. They only reported results for seven (Germanic, Hellenic, and Uralic) languages other than English. More recently, fully supervised letter sequence labelling models have been introduced for German compound splitting (Ma et al., 2016) and Sanskrit word splitting (Hellwig and Nehrdich, 2018). Pre-training can potentially further improve these"
2020.coling-main.411,C04-1081,0,0.0715852,"er model to enable efficient dynamic programming inference. The model is optimized for the task of machine translation. They only reported results for seven (Germanic, Hellenic, and Uralic) languages other than English. More recently, fully supervised letter sequence labelling models have been introduced for German compound splitting (Ma et al., 2016) and Sanskrit word splitting (Hellwig and Nehrdich, 2018). Pre-training can potentially further improve these models. There is a large body of research on Chinese word segmentation. The best models are supervised ones using structured prediction (Peng et al., 2004), transition-based models (Zhang and Clark, 2007), and most recently RNNs (Ma et al., 2018) and BERT-based models (Huang et al., 2019). The superior results of the BERT-based models demonstrate that pre-training is effective on word segmentation. Unlike BERT, we pre-train the entire model, not just the encoder. 3 RNN Tagging Model We formulate the segmentation problem as a character sequence tagging model. Given an input character sequence X = x1 , . . . , xI , the model predicts an output tag sequence Y = y1 , . . . , yI , with yi ∈ {B, I} being the tag for the character xi . B indicates the"
2020.coling-main.411,P17-1161,0,0.0306123,"Missing"
2020.emnlp-main.35,N18-1150,0,0.0562786,"Missing"
2020.emnlp-main.35,W04-1013,0,0.0832214,"Missing"
2020.emnlp-main.35,W19-4828,0,0.134704,"ng, pages 485–497, c November 16–20, 2020. 2020 Association for Computational Linguistics model considers these explicit semantics. In this paper, we rearrange and further explore the semantics of the topic model and develop a friendly topic assistant (TA) for Transformer-based abstractive summarization models. By introducing only a small number of parameters into the finetuning stage, TA is a flexible plug-and-play model, consisting of three modules: • Semantic-informed attention (SIA): It is often observed that the learned attentive patterns of many heads are not as reasonable as we expect (Clark et al., 2019; Michel et al., 2019). This motivates us to employ the semantic “distribution over topics” as a token representation to construct an explicit semantic-similarity matrix among tokens, which is further used as the attention weights of a newly added head. • Topic embedding with masked attention (TEMA): Since a topic is a distribution over tokens in the vocabulary, we use the mixture of token embeddings to represent the corresponding topic embedding. Thus, topics with large proportions for a document can be considered as extra input tokens of the decoder. Further, a topic describes a co-occurrenc"
2020.emnlp-main.35,W19-5203,0,0.025169,"; ii) if two fields are semantically related, their corresponding tokens are closely distributed, such as “Economic-Government” and “Astronomy-Airplane”. These phenomena indicate that 1 Linear summary summary values, respectively, dk (dv ) is the dimension of the queries or keys (values). However, recent works have illustrated that most attention heads learn simple, and often redundant, positional patterns (Clark et al., 2019; Michel et al., 2019). To improve the representation, some works incorporate external information, such as syntax, into the Transformer-based neural machine translation (Currey and Heafield, 2019; Deguchi et al., 2019). Inspired by their achievements and to focus on our summarization task, we attempts to inject the semantics learned from a topic model into the attention mechanism. Besides, Raganato et al. (2020) tried to fix the attention matrices of many heads according to token positions, finding that the performance do not drop and is even better in some cases. Motivated by this phenomenon, we introduce an extra head (the (h + 1)-th head) with a fixed attention matrix to express a semantic-informed attentive pattern. Recapping Φ in (2), each column, φk , is a distribution over all"
2020.emnlp-main.35,R19-1028,0,0.0279863,"antically related, their corresponding tokens are closely distributed, such as “Economic-Government” and “Astronomy-Airplane”. These phenomena indicate that 1 Linear summary summary values, respectively, dk (dv ) is the dimension of the queries or keys (values). However, recent works have illustrated that most attention heads learn simple, and often redundant, positional patterns (Clark et al., 2019; Michel et al., 2019). To improve the representation, some works incorporate external information, such as syntax, into the Transformer-based neural machine translation (Currey and Heafield, 2019; Deguchi et al., 2019). Inspired by their achievements and to focus on our summarization task, we attempts to inject the semantics learned from a topic model into the attention mechanism. Besides, Raganato et al. (2020) tried to fix the attention matrices of many heads according to token positions, finding that the performance do not drop and is even better in some cases. Motivated by this phenomenon, we introduce an extra head (the (h + 1)-th head) with a fixed attention matrix to express a semantic-informed attentive pattern. Recapping Φ in (2), each column, φk , is a distribution over all tokens, representing a"
2020.emnlp-main.35,N19-1423,0,0.173383,"any structure of the original Transformer network, and hence is able to be jointly learned with a pre-trained model during the fine-tuning stage. Besides, SIA, TEMA, and DRM are cooperated with some basic Transformer modules, such as embedding and multi-head attention. Therefore, we can plug an arbitrary combination of these three modules into various Transformerbased models. 2 2.1 Related work Transformer-based models for document summarization Pre-training and fine-tuning have attracted much attention in Transformer-based models for various NLP tasks. Equipped with pre-trained Bert encoder (Devlin et al., 2019), Liu (2019); Liu and Lapata (2019) propose the BertSUM for both extractive and abstractive tasks; Zhang et al. (2019c) propose a hierarchical Bert model for extractive summarization, where the low-level and high-level Berts are built for sentence and document understanding, respectively. Although the above methods achieve better performance than LSTM-based models, their Bert encoder pre-trained for document understanding may not well match the decoder trained from scratch for the summary generation (Rothe et al., 2019; Yang et al., 2019). To consider document understanding and generation in a"
2020.emnlp-main.35,D18-1443,0,0.146771,"Missing"
2020.emnlp-main.35,D19-1387,0,0.0652016,"Missing"
2020.emnlp-main.35,D18-1206,0,0.202192,"+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models. 1 Introduction Automatic summarization, requiring both document understanding and text generation, is a comprehensive task in natural language processing (NLP). Extractive approaches (Wong et al., 2008; Liu, 2019; Zhang et al., 2019c) identify and then concatenate the most representative sentences as a summary. By contrast, abstractive summarization (See et al., 2017; Narayan et al., 2018) is more challenging, aiming to generate a summary via rephrasing and introducing new concepts/words. Our work focuses on abstractive summarization, for which sequence-to-sequence (S2S) models are widely studied. * Equal contribution. † Corresponding author. Recently, equipped with the attention mechanism (Vaswani et al., 2017), some Transformerbased language models (Subramanian et al., 2019; Zhang et al., 2019b; Dong et al., 2019; Liu and Lapata, 2019; Lewis et al., 2019; Raffel et al., 2019) are built with an encoder-decoder structure. These models benefit from pre-training on large-scale co"
2020.emnlp-main.35,2020.findings-emnlp.49,0,0.0175846,"tively, dk (dv ) is the dimension of the queries or keys (values). However, recent works have illustrated that most attention heads learn simple, and often redundant, positional patterns (Clark et al., 2019; Michel et al., 2019). To improve the representation, some works incorporate external information, such as syntax, into the Transformer-based neural machine translation (Currey and Heafield, 2019; Deguchi et al., 2019). Inspired by their achievements and to focus on our summarization task, we attempts to inject the semantics learned from a topic model into the attention mechanism. Besides, Raganato et al. (2020) tried to fix the attention matrices of many heads according to token positions, finding that the performance do not drop and is even better in some cases. Motivated by this phenomenon, we introduce an extra head (the (h + 1)-th head) with a fixed attention matrix to express a semantic-informed attentive pattern. Recapping Φ in (2), each column, φk , is a distribution over all tokens, representing a topic. From another view, each row, Φv,: , is a token representation, as shown in Fig. 1. With normalization where, [·] denotes a matrix. Then, the output of multi-head attention is obtained by: Co"
2020.emnlp-main.35,P17-1099,0,0.648715,"e-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models. 1 Introduction Automatic summarization, requiring both document understanding and text generation, is a comprehensive task in natural language processing (NLP). Extractive approaches (Wong et al., 2008; Liu, 2019; Zhang et al., 2019c) identify and then concatenate the most representative sentences as a summary. By contrast, abstractive summarization (See et al., 2017; Narayan et al., 2018) is more challenging, aiming to generate a summary via rephrasing and introducing new concepts/words. Our work focuses on abstractive summarization, for which sequence-to-sequence (S2S) models are widely studied. * Equal contribution. † Corresponding author. Recently, equipped with the attention mechanism (Vaswani et al., 2017), some Transformerbased language models (Subramanian et al., 2019; Zhang et al., 2019b; Dong et al., 2019; Liu and Lapata, 2019; Lewis et al., 2019; Raffel et al., 2019) are built with an encoder-decoder structure. These models benefit from pre-tra"
2020.emnlp-main.35,P19-1499,0,0.502552,"riendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models. 1 Introduction Automatic summarization, requiring both document understanding and text generation, is a comprehensive task in natural language processing (NLP). Extractive approaches (Wong et al., 2008; Liu, 2019; Zhang et al., 2019c) identify and then concatenate the most representative sentences as a summary. By contrast, abstractive summarization (See et al., 2017; Narayan et al., 2018) is more challenging, aiming to generate a summary via rephrasing and introducing new concepts/words. Our work focuses on abstractive summarization, for which sequence-to-sequence (S2S) models are widely studied. * Equal contribution. † Corresponding author. Recently, equipped with the attention mechanism (Vaswani et al., 2017), some Transformerbased language models (Subramanian et al., 2019; Zhang et al., 2019b; Dong et al., 2019; Liu"
2020.emnlp-main.35,2020.emnlp-main.748,0,0.221549,"Missing"
2020.emnlp-main.35,C08-1124,0,0.0226936,"sformerbased models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models. 1 Introduction Automatic summarization, requiring both document understanding and text generation, is a comprehensive task in natural language processing (NLP). Extractive approaches (Wong et al., 2008; Liu, 2019; Zhang et al., 2019c) identify and then concatenate the most representative sentences as a summary. By contrast, abstractive summarization (See et al., 2017; Narayan et al., 2018) is more challenging, aiming to generate a summary via rephrasing and introducing new concepts/words. Our work focuses on abstractive summarization, for which sequence-to-sequence (S2S) models are widely studied. * Equal contribution. † Corresponding author. Recently, equipped with the attention mechanism (Vaswani et al., 2017), some Transformerbased language models (Subramanian et al., 2019; Zhang et al.,"
2021.acl-long.230,2020.emnlp-main.35,1,0.858275,"bone single LM to multiple LMs, corresponding to different clusters. Therefore, our proposed model can be seen as a type of ensemble learning, and hence we call it ensemble language model (EnsLM). Our proposed mATM and EnsLM enjoy the following distinguished properties: • The mATM learns the mixture-prior latent semantic space to define a soft clustering assignment for each sample. • Guided by clustering assignments that describe the data diversity, EnsLM learns both shared and cluster-specific knowledge by weight modulations. combine them to obtain benefits from both. Dieng et al. (2016) and Wang et al. (2020) incorporate the TM with RNN-based model to capture the longrange dependencies. To move beyond single-layer TM for RNNs, Guo et al. (2020) propose the recurrent hierarchical topic-guided RNN with the help of multi-layer TM (Zhou et al., 2015; Zhang et al., 2018). To extract explicit document semantics for summarization, Wang et al. (2020) propose three different modules to plug knowledge from TM into Transformer-based LMs (Vaswani et al., 2017; Devlin et al., 2018). Our work can be seen as a parallel work to combine their advantages together but focuses on dealing with data diversity in NLP wi"
2021.acl-long.479,D14-1179,0,0.0143241,"Missing"
2021.acl-long.479,I17-1047,0,0.017256,"ibe the main objects in the image and omit details, to facilitate understanding details of an image along with the reasoning behind them, Antol et al. (2015) introduced VQA which contains three question answer pairs for each image. A further work is VCR (Zellers et al., 2019) that not only requires a model to answer the question derived from the image but also provides a rationale explaining why its answer is right. It was created to teach the model to learn higher-order cognition and commonsense reasoning about the world. Compared to the work above, Image-Chat (Shuster et al., 2020) and IGA (Mostafazadeh et al., 2017), which focus on the dialogues grounded in the image, are the most related work to ours. IGA includes 4k dialogues where each contains an image with a textual description of it, along with the questions and responses around the image. Due to its small scale, IGA can only be used for evaluation. Image-Chat is a larger scale dataset that consists of 202k image-grounded dialogues. However, both of them were created by asking the crowd workers to talk about a shared image to generate engaging conversation, which is different from the scenario of photo sharing where only one side can access the pho"
2021.acl-long.479,P18-1238,0,0.0300314,"proposed based on them. These datasets have greatly stimulated the development of joint image-text models. In this section, we review the widely used image-text datasets and the state-of-the-art (SOTA) approaches for solving the image-text problems. 2.1 Image-text Dataset Image-captioning datasets are first widely used for joint image-text modeling. MSCOCO (Lin et al., 2014) and Flickr30k (Young et al., 2014) that both contain five written caption descriptions for each image are the representative ones used for automated caption generation and cross-modal retrieval tasks. Conceptual Caption (Sharma et al., 2018) is yet another popular image caption dataset but contains an order of magnitude more images than MSCOCO. Because image captions usually only describe the main objects in the image and omit details, to facilitate understanding details of an image along with the reasoning behind them, Antol et al. (2015) introduced VQA which contains three question answer pairs for each image. A further work is VCR (Zellers et al., 2019) that not only requires a model to answer the question derived from the image but also provides a rationale explaining why its answer is right. It was created to teach the model"
2021.acl-long.479,2020.acl-main.219,0,0.0719706,"unicate with each other, e.g. the natural language human speak, but also perceive images as human do. How to facilitate building such multimodal system is the goal of this paper. Though recently many image-text tasks have been proposed and are being actively studied to bridge language and vision, the majority of them are formulated as choosing or composing the text based on the understanding of given images, e.g. image captioning (Anderson et al., 2018), visual question answering (Antol et al., 2015), visual commonsense reasoning (Zellers et al., 2019), and image-grounded dialogue generation (Shuster et al., 2020). Contrary to these tasks, the photo sharing task focuses on the reverse process, i.e. selecting the image based on the understanding of text, as well as proposing different and unique challenges. Firstly, different from the above popular multimodal tasks, in photo-sharing task, the dialogue doesn’t often explicitly mention the main visible content in the image. Instead of the main object of the photo, sometimes the background story, complemented by human imaginations, can be the focus of the chat. Figure 1 shows such an example, in which the person who shares the photo describes the event loc"
2021.acl-long.48,D19-1607,0,0.0281088,"ntax; and 3) extensive experiments on three benchmarks demonstrate the effectiveness of our method for cross-lingual tasks. 2 Related Work Cross-lingual Transfer. Large-scale pre-trained language models (Devlin et al., 2019; Liu et al., 2019) have achieved sequential success in various natural language processing tasks. Recent studies (Lample and Conneau, 2019; Conneau et al., 2020a) extend the pre-trained language models to multilingual tasks and demonstrate their prominent capability on cross-lingual knowledge transfer, even under zero-shot scenario (Wu and Dredze, 2019; Pires et al., 2019; Hsu et al., 2019). Motivated by the success of multilingual language models on cross-lingual transfer, several works explore how these models work and what their bottleneck is. On the one hand, some studies ﬁnd that the shared sub-words (Wu and Dredze, 2019; Dufter and Sch¨utze, 2020) and the parameters of top layers (Conneau et al., 2020b) are crucial for cross-lingual transfer. On the other hand, the bottleneck is attributed to two issues: (i) catastrophic forgetting (Keung et al., 2020; Liu et al., 2020), where knowledge learned in the pre-training stage is forgotten in downstream ﬁne-tuning; (ii) lack of l"
2021.acl-long.48,2020.acl-main.654,1,0.826781,"Missing"
2021.acl-long.48,2020.acl-main.87,0,0.0275965,"(1) XMAML-one (Nooralahzadeh et al., 2020) borrows the idea from meta-learning. Speciﬁcally, XMAML-one utilizes an auxiliary language development data in training, e.g., using the development set of Spanish in training to assist German on MLQA. XMAML-One reports the results based on the most beneﬁcial auxiliary language. (2) STILT (Phang et al., 2020) augments intermediate task training before ﬁne-tuning on the target task, e.g., adding training of HellaSwag (Zellers et al., 2019) before training on the NLI task. STILT also reports results with the most beneﬁcial intermediate task. (3) LAKM (Yuan et al., 2020) ﬁrst mines knowledge phrases along with passages from the Web. Then these Web data are used to enhance the phrase boundaries through a masked language model objective. Note that LAKM is only evaluated on three languages of MLQA. On the one hand, we observe that COSY surpasses the compared SOTA methods over all evaluation metrics. Although meta-learning methods (Finn et al., 2017; Gu et al., 2018; Sun et al., 2019) advance the state-of-the-art performance for few-shot learning, our COSY still outperforms the meta-learning-based method, i.e., XMAML-One, with 1.1 percentage points in the few-sho"
2021.acl-long.48,P19-1472,0,0.0280583,". Comparison with the State of the Art. We ﬁrst outline the SOTA zero-shot (few-shot) crosslingual methods we compared with as follows: (1) XMAML-one (Nooralahzadeh et al., 2020) borrows the idea from meta-learning. Speciﬁcally, XMAML-one utilizes an auxiliary language development data in training, e.g., using the development set of Spanish in training to assist German on MLQA. XMAML-One reports the results based on the most beneﬁcial auxiliary language. (2) STILT (Phang et al., 2020) augments intermediate task training before ﬁne-tuning on the target task, e.g., adding training of HellaSwag (Zellers et al., 2019) before training on the NLI task. STILT also reports results with the most beneﬁcial intermediate task. (3) LAKM (Yuan et al., 2020) ﬁrst mines knowledge phrases along with passages from the Web. Then these Web data are used to enhance the phrase boundaries through a masked language model objective. Note that LAKM is only evaluated on three languages of MLQA. On the one hand, we observe that COSY surpasses the compared SOTA methods over all evaluation metrics. Although meta-learning methods (Finn et al., 2017; Gu et al., 2018; Sun et al., 2019) advance the state-of-the-art performance for few-"
2021.acl-long.48,P19-1336,1,0.88758,"Missing"
2021.acl-long.48,2020.emnlp-main.276,0,0.0162897,"teel, 2004). Recently, counterfactual reasoning has motivated studies in applications. In the community of computer vision, counterfactual analysis has been successfully applied in explanation (Goyal et al., 2019a,b), long-tailed classiﬁcation (Tang et al., 2020a), scene graph generation (Tang et al., 2020b), and visual question answering (Chen et al., 2020; Niu et al., 2020; Abbasnejad et al., 2020). In the community of natural language processing, counterfactual methods are also emerging recently in text classiﬁcation (Choi et al., 2020), story generation (Qin et al., 2019), dialog systems (Zhu et al., 2020), gender bias (Vig et al., 2020; Shin et al., 2020), question answering (Yu et al., 2020), and sentiment bias (Huang et al., 2020). To the best of our knowledge, we are the ﬁrst to conduct counterfactual analysis in cross-lingual understanding. Different from previous works (Zhu et al., 2020; Qin et al., 2019) that generate word-level or sentence-level counterfactual samples, our counterfactual analysis dives into syntax level that is more controllable than text and free from complex language generation module. 3 COSY: COunterfactual SYntax COSY aims to leverage the syntactic information, e.g."
2021.findings-acl.69,D18-1015,0,0.214423,"ng and moment boundary prediction. Video grounding is a fundamental and challenging problem in vision-language understanding research area (Hu et al., 2019; Yu et al., 2019; Zhu and Yang, 2020). It aims to retrieve a temporal video moment that semantically corresponds to a given language query, as shown in Figure 1. This task requires techniques from both computer vision (Tran et al., 2015; Shou et al., 2016; Feichtenhofer et al., 2019), natural language processing (Yu et al., 2018; Yang et al., 2019), and more importantly, the crossmodal interactions between the two. Many existing solutions (Chen et al., 2018; Liu et al., 2018a; Xu et al., 2019) tackle video grounding problem with proposal-based approach. This approach generates proposals with pre-set sliding windows or anchors, computes the similarity between the query and each https://github.com/IsaacChanghau/SeqPAN Cross-Attention Cross-Attention Integration Query: They continue forward very slowly taking their time and enjoying the experience. Introduction 1 0s 0.0 Feature Extraction 1 Video Cross Gating .2 136 Given a video, video grounding aims to retrieve a temporal moment that semantically corresponds to a language query. In this work, we"
2021.findings-acl.69,N19-1198,0,0.0607699,"., 2020b) sequentially assign each frame with multiscale temporal anchors and select the anchor with highest confidence as the result. However, these methods are sensitive to the proposal quality; and comparison of all proposal-query pairs is computational expensive and inefficient. Proposal-free framework includes regression and span-based methods. Regression-based methods (Yuan et al., 2019b; Lu et al., 2019a; Chen et al., 2020a,b) tackle video grounding by learning cross-modal interactions between video and query, and directly regressing temporal time of target moments. Span-based methods (Ghosh et al., 2019; Rodriguez et al., 2020; Zhang et al., 2020a; Lei et al., 2020; Zhang et al., 2021) address video grounding by borrowing the concept of extractive question answering (Seo et al., 2017; Huang et al., 2018), and to predict the start and end boundaries of target moment directly. In addition, there are several works (He et al., 777 Encoder SGPA O Additive Attention E-M O e Addition Operator C Start Index Predictor Positional Embeddings Query-to-Video Fusion Feature Extractor Self-Guided Parallel Attention Conv Block el in I-M An illustration of Sequence Matching Module Gumbel Noise Label Embeddin"
2021.findings-acl.69,P19-1655,0,0.0171705,"e answer. These methods are sensitive to the quality of proposals and are inefficient because all proposal-query pairs are compared. Recently, several one-stage proposal-free solutions (Chen et al., 2019; Lu et al., 2019a; Mun et al., 2020) are proposed to directly predict start/end boundaries of target moments, through modeling video-text interactions. Our solution, SeqPAN, is a proposal-free method; hence our key focuses are video-text interaction modeling and moment boundary prediction. Video grounding is a fundamental and challenging problem in vision-language understanding research area (Hu et al., 2019; Yu et al., 2019; Zhu and Yang, 2020). It aims to retrieve a temporal video moment that semantically corresponds to a given language query, as shown in Figure 1. This task requires techniques from both computer vision (Tran et al., 2015; Shou et al., 2016; Feichtenhofer et al., 2019), natural language processing (Yu et al., 2018; Yang et al., 2019), and more importantly, the crossmodal interactions between the two. Many existing solutions (Chen et al., 2018; Liu et al., 2018a; Xu et al., 2019) tackle video grounding problem with proposal-based approach. This approach generates proposals with"
2021.findings-acl.69,D19-1518,0,0.0955478,"un@}ntu.edu.sg, 21wjing@gmail.com, {zhenll,zhouty,gohsm}@ihpc.a-star.edu.sg Abstract Self-Attention line e Tim 6s Target 3s 7.2 Moment 5 90.6 1s Localization Self-Attention Figure 1: The overview of our procedures for video grounding, with an example of retrieving the temporal moment from an untrimmed video by a given language query. proposal. The proposal with highest score is selected as the answer. These methods are sensitive to the quality of proposals and are inefficient because all proposal-query pairs are compared. Recently, several one-stage proposal-free solutions (Chen et al., 2019; Lu et al., 2019a; Mun et al., 2020) are proposed to directly predict start/end boundaries of target moments, through modeling video-text interactions. Our solution, SeqPAN, is a proposal-free method; hence our key focuses are video-text interaction modeling and moment boundary prediction. Video grounding is a fundamental and challenging problem in vision-language understanding research area (Hu et al., 2019; Yu et al., 2019; Zhu and Yang, 2020). It aims to retrieve a temporal video moment that semantically corresponds to a given language query, as shown in Figure 1. This task requires techniques from both co"
2021.findings-acl.69,P16-1101,0,0.0277781,"rames among thousands. Recent studies attempt to address this issue by auxiliary objectives, e.g., to discriminate whether each frame is foreground (positive) or background (negative) (Yuan et al., 2019b; Mun et al., 2020), or to regress distances of each frame within target moment to ground truth boundaries (Lu et al., 2019a; Zeng et al., 2020). However, the “sequence” nature of frames or videos is not considered. We emphasize the “sequence” nature of video frames and adopt the concept of sequence labeling in NLP to video grounding. We use named entity recognition (NER) (Lample et al., 2016; Ma and Hovy, 2016) as an example sequence labeling task for illustration in Figure 2. Video grounding is to retrieve a sequence of frames with start/end boundaries of target moment from video. This is analogous to extract a multi-word named entity from a sentence. The main difference is that, words are discrete, so word annotations (i.e., B, I, E, and O tags) in sentence are discrete. In contrast, video is continuous and the changes between consecutive frames are smooth. Hence, it is difficult (and also not necessary) to precisely annotate each frame. We relax the annotations on video sequence by specifying vid"
2021.findings-acl.69,D14-1162,0,0.0868918,"enotes the percentage of test samples that have at least one result whose IoU with ground-truth is larger than µ in top-n predictions; (ii) “mIoU”, which denotes the average IoU over all test samples. We set n = 1 and µ ∈ {0.3, 0.5, 0.7}. Implementation Details. We follow (Ghosh et al., 2019; Mun et al., 2020; Rodriguez et al., 2020; Zhang et al., 2020a) and use 3D ConvNet pretrained on Kinetics dataset (Carreira and Zisserman, 2017) to extract RGB visual features from videos; then we downsample the feature sequence to a fixed length. The query words are lowercased and initialized with GloVe (Pennington et al., 2014) embedding. We set hidden dimension d to 128; SGPA blocks to 2; annealing temperature to 0.3; and heads in multi-head attention to 8; Adam (Kingma and Ba, 2015) optimizer with batch size of 16 and learning rate of 0.0001 is used for training. More details of dataset statistics and the hyperparameter settings are summarized in Appendix. 4.2 Comparison with State-of-the-Arts We compare SeqPAN with the following state-ofthe-arts. 1) Proposal-based methods: TGN (Chen et al., 2018), ACL (Ge et al., 2019), CBP (Wang et al., 2020b), SCDM (Yuan et al., 2019a), MAN (Zhang et al., 2019a); 2) Proposal-fr"
2021.findings-acl.69,Q13-1003,0,0.0380915,"b , Ylab ) + kElab Lseq = fXE (L Elab (1 − I)k2F (11) where Ylab denotes the ground truth sequence labels, 1 is the matrix with all elements being 1 and I is the identity matrix. The second term in Eq. 11 is the orthogonal regularization (Brock et al., 2019), which ensures Elab to keep the orthogonality.  1  × fXE (Ps , Ys ) + fXE (Pe , Ye ) (13) 2 More details about Gumbel Tricks are in Appendix. 780 4 4.1 Experiments Experimental Setting Datasets. We conduct the experiments on three benchmark datasets: Charades-STA (Gao et al., 2017), ActivityNet Captions (Krishna et al., 2017) and TACoS (Regneri et al., 2013). Charades-STA, collected by Gao et al. (2017) from Charades (Sigurdsson et al., 2016) dataset, contains 16, 128 annotations (i.e., moment-query pairs), where 12, 408 and 3, 720 annotations are for train and test. ActivityNet Captions (ANetCaps) contains 20K videos taken from ActivityNet (Heilbron et al., 2015) dataset. We follow the setup in (Chen et al., 2020a; Lu et al., 2019a; Wu et al., 2020b; Yuan Methods DEBUG ExCL MAN SCDM CBP GDP 2D-TAN TSP-PRL TMLGA VSLNet DRN LGI SeqPAN µ = 0.3 54.95 61.50 54.54 67.53 70.46 72.96 73.84 R@1, IoU = µ µ = 0.5 µ = 0.7 37.39 17.69 44.10 22.40 46.53 22.72"
2021.findings-acl.69,D19-1514,0,0.0280473,"art/end boundary predictions using region labels. Experimental results on three datasets show that SeqPAN is superior to state-of-theart methods. Furthermore, the effectiveness of the self-guided parallel attention module and the sequence matching module is verified.1 Video-text interaction modeling. In order to model video-text interaction, various attentionbased methods have been proposed (Gao et al., 2017; Yuan et al., 2019a; Mun et al., 2020). In particular, transformer block (Vaswani et al., 2017) is widely used in vision-language tasks and proved to be effective for multimodal learning (Tan and Bansal, 2019; Lu et al., 2019b; Su et al., 2020; Li et al., 2020). In video grounding task, fine-grain scale unimodal representations are important to achieve good localization performance. However, existing solutions do not refine unimodal representations of video and text when doing cross-modal reasoning, and thus limit the performance. To better capture informative features for multimodalities, we encode both self-attentive contexts and cross-modal interactions from video and query. That is, instead of solely relying on sophisticated 776 Findings of the Association for Computational Linguistics: ACL-IJ"
2021.findings-acl.69,2020.acl-main.585,1,0.749908,"h multiscale temporal anchors and select the anchor with highest confidence as the result. However, these methods are sensitive to the proposal quality; and comparison of all proposal-query pairs is computational expensive and inefficient. Proposal-free framework includes regression and span-based methods. Regression-based methods (Yuan et al., 2019b; Lu et al., 2019a; Chen et al., 2020a,b) tackle video grounding by learning cross-modal interactions between video and query, and directly regressing temporal time of target moments. Span-based methods (Ghosh et al., 2019; Rodriguez et al., 2020; Zhang et al., 2020a; Lei et al., 2020; Zhang et al., 2021) address video grounding by borrowing the concept of extractive question answering (Seo et al., 2017; Huang et al., 2018), and to predict the start and end boundaries of target moment directly. In addition, there are several works (He et al., 777 Encoder SGPA O Additive Attention E-M O e Addition Operator C Start Index Predictor Positional Embeddings Query-to-Video Fusion Feature Extractor Self-Guided Parallel Attention Conv Block el in I-M An illustration of Sequence Matching Module Gumbel Noise Label Embeddings FFN Ti m B-M Timeline End Index Predictor"
C04-1060,J90-2002,0,0.126615,"ranslation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other. The trees may be learned directly from parallel corpora (Wu, 1997), or provided by a parser trained on hand-annotated treebanks (Yamada and Knight, 2001). In this paper, we compare these approaches on Chinese-English and French-English datasets, and find that automatically derived trees result in better agreement with human-annotated word-level alignments for unseen test data. 1 Introduction Statistical approaches to machine translation, pioneered by Brown et al. (1990), estimate parameters for a probabilistic model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text. In recent years, a number of syntactically motivated approaches to statistical machine translation have been proposed. These approaches assign a parallel tree structure to the two sides of each sentence pair, and model the translation process with reordering operations defined on the tree structure. The tree-based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences i"
C04-1060,J93-2003,0,0.0331963,"Missing"
C04-1060,J94-4004,0,0.0553805,"ring model in the opposite direction, using Chinese trees and English strings. The Chinese training data was parsed with the Bikel (2002) parser, and used the Chinese Treebank parses for our test data. Results are shown in Table 3. Because the ITG is a symmetric, generative model, the ITG results in Table 3 are identical to those in Table 1. While the experiment does not show a significant improvement, it is possible that better parses for the training data might be equally important. Even when the automatic parser output is correct, the tree structure of the two languages may not correspond. Dorr (1994) categorizes sources of syntactic divergence between languages, and Fox (2002) analyzed a parallel French-English corpus, quantifying how often parse dependencies cross when projecting an English tree onto a French string. Even in this closely related language pair with generally similar word order, crossed dependencies were caused by such common occurrences as adverb modification of a verb, or the correspondence of “not” to “ne pas”. Galley et al. (2004) extract translation rules from a large parsed parallel corpus that extend in scope to tree fragments beyond a single node; we believe that a"
C04-1060,W02-1039,0,0.0693678,"The Chinese training data was parsed with the Bikel (2002) parser, and used the Chinese Treebank parses for our test data. Results are shown in Table 3. Because the ITG is a symmetric, generative model, the ITG results in Table 3 are identical to those in Table 1. While the experiment does not show a significant improvement, it is possible that better parses for the training data might be equally important. Even when the automatic parser output is correct, the tree structure of the two languages may not correspond. Dorr (1994) categorizes sources of syntactic divergence between languages, and Fox (2002) analyzed a parallel French-English corpus, quantifying how often parse dependencies cross when projecting an English tree onto a French string. Even in this closely related language pair with generally similar word order, crossed dependencies were caused by such common occurrences as adverb modification of a verb, or the correspondence of “not” to “ne pas”. Galley et al. (2004) extract translation rules from a large parsed parallel corpus that extend in scope to tree fragments beyond a single node; we believe that adding such larger-scale operations to the translation model is likely to signi"
C04-1060,N04-1035,0,0.0365642,"aining data might be equally important. Even when the automatic parser output is correct, the tree structure of the two languages may not correspond. Dorr (1994) categorizes sources of syntactic divergence between languages, and Fox (2002) analyzed a parallel French-English corpus, quantifying how often parse dependencies cross when projecting an English tree onto a French string. Even in this closely related language pair with generally similar word order, crossed dependencies were caused by such common occurrences as adverb modification of a verb, or the correspondence of “not” to “ne pas”. Galley et al. (2004) extract translation rules from a large parsed parallel corpus that extend in scope to tree fragments beyond a single node; we believe that adding such larger-scale operations to the translation model is likely to significantly improve the performance of syntactically supervised alignment. The syntactically supervised model has been found to outperform the IBM word-level alignment models of Brown et al. (1993) for translation by Yamada and Knight (2002). An evaluation for the alignment task, measuring agreement with human judges, also found the syntax-based model to outperform the IBM models."
C04-1060,P03-1011,1,0.805062,"to indicate that the syntactic structure in one language is given to the training procedure. It is important to note, however, that both algorithms are unsupervised in that they are not provided any hand-aligned training data. Rather, they both use Expectation Maximization to find an alignment model by iteratively improving the likelihood assigned to unaligned parallel sentences. Our evaluation is in terms of agreement with word-level alignments created by bilingual human annotators. We describe each of the models used in more detail in the next two sections, including the clone operation of Gildea (2003). The reader who is familiar with these models may proceed directly to our experiments in Section 4, and further discussion in Section 5. 2 The Inversion Transduction Grammar The Inversion Transduction Grammar of Wu (1997) can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context-free grammar productions. The grammar is restricted to binary rules, which can have the symbols in the right hand side appear in the same order in both languages, represented with square brackets: X → [Y Z] or the symbols may appear in reve"
C04-1060,P02-1050,0,0.0170174,"airs with a total of 276,113 Chinese words and 315,415 English words. The Chinese data were automatically segmented into tokens, and English capitalization was retained. We replace words occurring only once with an unknown word token, resulting in a Chinese vocabulary of 23,783 words and an English vocabulary of 27,075 words. Our hand-aligned data consisted of 48 sentence pairs also with less than 25 words in either language, for a total of 788 English words and 580 Chinese words. A separate development set of 49 sentence pairs was used to control overfitting. These sets were the data used by Hwa et al. (2002). The hand aligned test data consisted of 745 individual aligned word pairs. Words could be aligned oneto-many in either direction. This limits the performance achievable by our models; the IBM models allow one-to-many alignments in one direction only, while the tree-based models allow only one-to-one alignment unless the cloning operation is used. Our French-English experiments were based on data from the Canadian Hansards made available by Ulrich German. We used as training data 20,000 sentence pairs of no more than 25 words in either language. Our test data consisted of 447 sentence pairs o"
C04-1060,W03-0301,0,0.0367821,"ur French-English experiments were based on data from the Canadian Hansards made available by Ulrich German. We used as training data 20,000 sentence pairs of no more than 25 words in either language. Our test data consisted of 447 sentence pairs of no more than 30 words, hand aligned by Och and Ney (2000). A separate development set of 37 sentences was used to control overfitting. We used of vocabulary of words occurring at least 10 times in the entire Hansard corpus, resulting in 19,304 English words and 22,906 French words. Our test set is that used in the alignment evaluation organized by Mihalcea and Pederson (2003), though we retained sentence-initial capitalization, used a closed vocabulary, and restricted ourselves to a smaller training corpus. We parsed the English side of the data with the Collins parser. As an artifact of the parser’s probability model, it outputs sentence-final punctuation attached at the lowest level of the tree. We raised sentence-final punctuation to be a daughter of the tree’s root before training our parse-based model. As our Chinese-English test data did not include sentence-final punctuation, we also removed it from our French-English test set. We evaluate our translation m"
C04-1060,P00-1056,0,0.637948,"745 individual aligned word pairs. Words could be aligned oneto-many in either direction. This limits the performance achievable by our models; the IBM models allow one-to-many alignments in one direction only, while the tree-based models allow only one-to-one alignment unless the cloning operation is used. Our French-English experiments were based on data from the Canadian Hansards made available by Ulrich German. We used as training data 20,000 sentence pairs of no more than 25 words in either language. Our test data consisted of 447 sentence pairs of no more than 30 words, hand aligned by Och and Ney (2000). A separate development set of 37 sentences was used to control overfitting. We used of vocabulary of words occurring at least 10 times in the entire Hansard corpus, resulting in 19,304 English words and 22,906 French words. Our test set is that used in the alignment evaluation organized by Mihalcea and Pederson (2003), though we retained sentence-initial capitalization, used a closed vocabulary, and restricted ourselves to a smaller training corpus. We parsed the English side of the data with the Collins parser. As an artifact of the parser’s probability model, it outputs sentence-final punc"
C04-1060,J97-3002,0,0.687951,"ucture to the two sides of each sentence pair, and model the translation process with reordering operations defined on the tree structure. The tree-based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences in word order in the grammars of the two languages. Furthermore, the tree structure allows us to make probabilistic independence assumptions that result in polynomial time algorithms for estimating a translation model from parallel training data, and for finding the highest probability translation given a new sentence. Wu (1997) modeled the reordering process with binary branching trees, where each production could be either in the same or in reverse order going from source to target language. The trees of Wu’s Inversion Transduction Grammar were derived by synchronously parsing a parallel corpus, using a grammar with lexical translation probabilities at the leaves and a simple grammar with a single nonterminal providing the tree structure. While this grammar did not represent traditional syntactic categories such as verb phrases and noun phrases, it served to restrict the word-level alignments considered by the syst"
C04-1060,C02-1145,0,0.0209827,"years old, will join the board as a nonexecutive director Nov. 29. contrasts dramatically with In the past when education on opposing Communists and on resisting Russia was stressed, retaking the mainland and unifying China became a slogan for the authoritarian system, which made the unification under the martial law a tool for oppressing the Taiwan people. a typical sentence from our corpus. While we did not have human-annotated goldstandard parses for our training data, we did have human annotated parses for the Chinese side of our test data, which was taken from the Penn Chinese Treebank (Xue et al., 2002). We trained a second tree-to-string model in the opposite direction, using Chinese trees and English strings. The Chinese training data was parsed with the Bikel (2002) parser, and used the Chinese Treebank parses for our test data. Results are shown in Table 3. Because the ITG is a symmetric, generative model, the ITG results in Table 3 are identical to those in Table 1. While the experiment does not show a significant improvement, it is possible that better parses for the training data might be equally important. Even when the automatic parser output is correct, the tree structure of the tw"
C04-1060,P01-1067,0,0.881871,"e this grammar did not represent traditional syntactic categories such as verb phrases and noun phrases, it served to restrict the word-level alignments considered by the system to those allowable by reordering operations on binary trees. This restriction corresponds to intuitions about the alignments that could be produced by systematic differences between the two language’s grammars, and allows for a polynomial time algorithm for finding the highest-probability alignment, and for re-estimation of the lexical translation and grammar probabilities using the Expectation Maximization algorithm. Yamada and Knight (2001) present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures. This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in Wu (1997), but the specific bracketing of the parse tree provided. In this paper, we make a direct comparison of a syntactically unsupervised alignment model, base"
C04-1060,P02-1039,0,0.0154414,"d order, crossed dependencies were caused by such common occurrences as adverb modification of a verb, or the correspondence of “not” to “ne pas”. Galley et al. (2004) extract translation rules from a large parsed parallel corpus that extend in scope to tree fragments beyond a single node; we believe that adding such larger-scale operations to the translation model is likely to significantly improve the performance of syntactically supervised alignment. The syntactically supervised model has been found to outperform the IBM word-level alignment models of Brown et al. (1993) for translation by Yamada and Knight (2002). An evaluation for the alignment task, measuring agreement with human judges, also found the syntax-based model to outperform the IBM models. However, a relatively small corpus was used to train both models (2121 Japanese-English sentence pairs), and the evaluations were performed on the same data for training, meaning that one or both models might be significantly overfitting. Zens and Ney (2003) provide a thorough analysis of alignment constraints from the perspective of decoding algorithms. They train the models of Wu Perplexity 700 0.55 600 0.5 0.45 500 AER 0.4 400 1 2 3 4 5 6 7 8 9 Itera"
C04-1060,P03-1019,0,0.657901,"j, k) β(X, l, n, i, k)+= P (hY Zi|X)β(Y, m, n, i, j)β(Z, l, m, j, k) end for end for end for A similar recursion is used to compute outside probabilities for each chart item, and the inside and outside probabilities are combined to derive expected counts for occurrence of each grammar rule, including the rules corresponding to individual lexical translations. In our experiments we use a grammar with a start symbol S, a single preterminal C, and two nonterminals A and B used to ensure that only one parse can generate any given word-level alignment (ignoring insertions and deletions) (Wu, 1997; Zens and Ney, 2003). The individual lexical translations produced by the grammar may include a NULL word on either side, in order to represent insertions and deletions. 3 The Tree-To-String Model The model of Yamada and Knight (2001) can be thought of as a generative process taking a tree in one language as input and producing a string in the other through a sequence of probabilistic operations. If we follow the process of an English sentence’s transformation into French, the English sentence is first given a syntactic tree representation by a statistical parser (Collins, 1999). As the first step in the translat"
C04-1060,J03-4003,0,\N,Missing
C08-1136,J04-4002,0,0.779057,"put is a permutation, but in machine translation it is common to work with word-level alignments that are many-to-many; in general any set of pairs of words, one from each language, is a valid alignment for a given bilingual sentence pair. In this paper, we consider a generalized concept of common intervals given such an alignment: a common interval is a pair of phrases such that no word pair in the alignment links a word inside the phrase to a word outside the phrase. Extraction of such phrases is a common feature of state-of-the-art phrase-based and syntax-based machine translation systems (Och and Ney, 2004a; Chiang, 2005). We generalize Uno and Yagiura’s algorithm to this setting, and demonstrate a linear time algorithm for a pair of aligned sequences. The output is a tree representation of possible phrases, which directly provides a set of minimal synchronous grammar 1081 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1081–1088 Manchester, August 2008 rules for an SCFG-based machine translation system. For phrase-based machine translation, one can also read all phrase pairs consistent with the original alignment off of the tree in time linear"
C08-1136,H05-1101,0,0.0302716,"he algorithm of the previous section outputs the normalized decomposition tree depicted in Figure 2. From this tree, it is straightforward to obtain 3 It can be shown further that in this region, f shifts up or is unchanged. Therefore any reductions in step (4) must be in region (a). C → e4 F (1) e6 , f1 F (1) f3 E → e3 , f4 F → e5 , f2 G → e1 , f6 Figure 5: Each node from the normalized decomposition tree of Figure 2 is converted into an SCFG rule. a set of maximally-decomposed SCFG rules. As an example, the tree of Figure 2 produces the rules shown in Figure 5. We adopt the SCFG notation of Satta and Peserico (2005). Each rule has a right-hand side sequence for both languages, separated by a comma. Superscript indices in the right-hand side of grammar rules such as: A → B (1) C (2) , C (2) B (1) indicate that the nonterminals with the same index are linked across the two languages, and will eventually be rewritten by the same rule application. The example above inverts the order of B and C when translating from the source language to the target language. The SCFG rule extraction proceeds as follows. Assign a nonterminal label to each node in the tree. Then for each node (S, T ) in the tree top-down, wher"
C08-1136,P06-1123,0,0.103616,"all the children of the nodes in the chain. Then, each of the subsequences {ηi , . . . , ηj |1 < i < j ≤ k} yields a valid phrase pair. In our example, the root of the tree of Figure 2 and its left child form such a chain, with three children; the subsequence {([3, 3], [4, 4]), ([4, 6], [1, 3])} yields the phrase ([3, 6], [1, 4]). In the case of unaligned words, we can also consider all combinations of their attachments, as discussed for SCFG rule extraction. 5 Experiments on Analyzing Word Alignments One application of our factorization algorithm is analyzing human-annotated word alignments. Wellington et al. (2006) argue for the necessity of discontinuous spans (i.e., for a formalism beyond Synchronous CFG) in order for synchronous parsing to cover human-annotated word alignment data under the constraint that rules have a rank of no more than two. In a related study, Zhang and Gildea (2007) analyze the rank of the Synchronous CFG derivation trees needed to parse the same data. The number of discontinuous spans and the rank determine the complexity of dynamic programming algorithms for synchronous parsing (alignment) or machine translation decoding. Both studies make simplifying assumptions on the alignm"
C08-1136,W07-0404,1,0.887063,"on interval is a set of numbers that are consecutive in both. The breakthrough algorithm of Uno and Yagiura (2000) computes all K common intervals of two length n permutations in O(n + K) time. This is achieved by designing data structures to index possible boundaries of common intervals as the computation proceeds, so that not all possible pairs of beginning and end points need to be considered. Landau et al. (2005) and Bui-Xuan et al. (2005) show that all common intervals can be encoded in O(n) space, and adapt Uno and Yagiura’s algorithm to produce this compact representation in O(n) time. Zhang and Gildea (2007) use similar techniques to factorize Synchronous Context Free Grammars in linear time. These previous algorithms assume that the input is a permutation, but in machine translation it is common to work with word-level alignments that are many-to-many; in general any set of pairs of words, one from each language, is a valid alignment for a given bilingual sentence pair. In this paper, we consider a generalized concept of common intervals given such an alignment: a common interval is a pair of phrases such that no word pair in the alignment links a word inside the phrase to a word outside the phr"
C08-1136,J93-2003,0,0.0412268,"Alignments and Phrase Pairs Let [x, y] denote the sequence of integers between x and y inclusive, and [x, y) the integers between x and y − 1 inclusive. An aligned sequence pair or simply an alignment is a tuple (E, F, A), where E = e1 · · · en and F = f1 · · · fm are strings, and A is a set of links (x, y), where 1 ≤ x ≤ n and 1 ≤ y ≤ m, connecting E and F . For most of this paper, since we are not concerned with the identity of the symbols in E and F , we will assume for simplicity that ei = i and fj = j, so that E = [1, n] and F = [1, m]. In the context of statistical machine translation (Brown et al., 1993), we may interpret E as an English sentence, F its translation in French, and A a representation of how the words correspond to each other in the two sentences. A pair of substrings [s, t] ⊂ E and [u, v] ⊂ F is a phrase pair (Och and Ney, 2004b) if and only if the subset of links emitted from [s, t] in E is equal to the subset of links emitted from [u, v] in F , and both are nonempty. Figure 1a shows an example of a many-tomany alignment, where E = [1, 6], F = [1, 7], and A = {(1, 6), (2, 5), (2, 7), (3, 4), (4, 1), (4, 3), (5, 2), (6, 1), (6, 3)}. The eight phrase pairs in this alignment are:"
C08-1136,P05-1033,1,0.933656,"analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs. 1 Introduction Many recent syntax-based statistical machine translation systems fall into the general formalism of Synchronous Context-Free Grammars (SCFG), where the grammar rules are found by first aligning parallel text at the word level. From wordlevel alignments, such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages (Galley et al., 2004), or with the the word-level alignments alone without reference to external syntactic analysis (Chiang, 2005), which is the scenario we address here. In this paper, we derive an optimal, linear-time algorithm for the problem of decomposing an arbitrary word-level alignment into SCFG rules such that each rule has at least one aligned word and is minimal in the sense that it cannot be further decomposed into smaller rules. Extracting minimal rules is of interest both because rules with fewer words are more likely to generalize to new data, c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some righ"
C08-1136,N04-1035,0,0.105225,"a Synchronous Context-Free Grammar (SCFG) with the simplest rules possible. We also use the algorithm to precisely analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs. 1 Introduction Many recent syntax-based statistical machine translation systems fall into the general formalism of Synchronous Context-Free Grammars (SCFG), where the grammar rules are found by first aligning parallel text at the word level. From wordlevel alignments, such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages (Galley et al., 2004), or with the the word-level alignments alone without reference to external syntactic analysis (Chiang, 2005), which is the scenario we address here. In this paper, we derive an optimal, linear-time algorithm for the problem of decomposing an arbitrary word-level alignment into SCFG rules such that each rule has at least one aligned word and is minimal in the sense that it cannot be further decomposed into smaller rules. Extracting minimal rules is of interest both because rules with fewer words are more likely to generalize to new data, c 2008. Licensed under the Creative Commons Attribution-"
C10-2154,D07-1079,0,0.0844003,"extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule coverage has been proved to be an important one that affects the translation accuracy of syntax-based systems (DeNeefe et al., 2007; Shen et al., 2008). However, these systems suffer from a problem that translation rules are extracted using only 1-best parse tree generated by a single parser, which generally results in relatively low rule coverage due to the limited scope in rule extraction (Mi and Huang, 2008). To alleviate this problem, a straightforward solution is to enlarge the scope of rule extraction, and obtain translation rules by using a group of diversified parse trees instead of a single parse tree. For example, Mi and Huang (2008) used k-best parses and forest to extract translation rules for improving the ru"
C10-2154,P05-1067,0,0.0120742,"nglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule coverage has been prove"
C10-2154,P03-2041,0,0.0924117,"od on ChineseEnglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule"
C10-2154,N04-1035,0,0.20786,"terminals) and variables (non-terminals) at leaves. Figure 1 shows the translation rules extracted from a word-aligned sentence pair with a targetside parse tree. Figure 1: Translation rules extracted from a string-tree pair. 1346 Figure 2: Rule extraction using two different parsers (Berkeley Parser and Collins Parser). The shaded rectangles denote the translation rules that can be extracted from the parse tree generated by one parser but cannot be extracted from the parse tree generated by the other parser. To obtain basic translation rules, the (minimal) GHKM extraction method proposed in (Galley et al, 2004) is utilized. The basic idea of GHKM extraction is to compute the set of the minimally-sized translation rules that can explain the mappings between source-language string and target-language tree while respecting the alignment and reordering between the two languages. For example, from the string-tree pair shown at the top of Figure 1, we extract the minimal GHKM translation rules r1-6. In addition to GHKM extraction, the SPMT models (Marcu et al., 2006) are employed to obtain phrasal rules that are not covered by GHKM extraction. For example, rule r8 in Figure 1 is a SPMT rule that is not ob"
C10-2154,P07-1019,0,0.0312328,"5.3 Parser Indicator Features For each rule, we define N indicator features (i.e. τ (r , i ) ) to indicate a rule is extracted by using which parsers, and add them into the translation model. By training the feature weights with Minimum Error Rate Training (MERT), the system can learn preferences for different parsers automatically. 6 Experiments The experiments are conducted on ChineseEnglish translation in a state-of-the-art string-totree SMT system. 6.1 and the composed rules are generated by composing two or three minimal GHKM and SPMT rules3. We use a CKY-style decoder with cube pruning (Huang and Chiang, 2007) and beam search to decode new Chinese sentences. By default, the beam size is set to 30. For integrating n-gram language model into decoding efficiently, rules containing more than two variables or source word sequences are binarized using the synchronous binarization method (Zhang et al., 2006; Xiao et al., 2009). The system is evaluated in terms of the caseinsensitive NIST version BLEU (using the shortest reference length), and statistical significant test is conducted using the re-sampling method proposed by Koehn (2004). 6.2 Four syntactic parsers are chosen for the experiments. They are"
C10-2154,2006.amta-papers.8,0,0.642775,"ts show that our method improves the baseline system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora, even achieves a +1 BLEU improvement when working with the kbest extraction method. More interestingly, we observe that the MT performance is not very sensitive to the parsing performance of the parsers used in rule extraction. Actually, the MT system does not show different preferences for different parsers. 2 Related Work In machine translation, some efforts have been made to improve rule coverage and advance the performance of syntax-based systems. For example, Galley et al. (2006) proposed the idea of rule composing which composes two or more rules with shared states to form a larger, composed rule. Their experimental results showed that the rule composing method could significantly improve the translation accuracy of their syntax-based system. Following Galley et al. (2006)’s work, Marcu et al. (2006) proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models. Wang et al. (2007) described a binarization method that binarized parse trees to improve the rule coverag"
C10-2154,W04-3250,0,0.0602451,"SPMT rules3. We use a CKY-style decoder with cube pruning (Huang and Chiang, 2007) and beam search to decode new Chinese sentences. By default, the beam size is set to 30. For integrating n-gram language model into decoding efficiently, rules containing more than two variables or source word sequences are binarized using the synchronous binarization method (Zhang et al., 2006; Xiao et al., 2009). The system is evaluated in terms of the caseinsensitive NIST version BLEU (using the shortest reference length), and statistical significant test is conducted using the re-sampling method proposed by Koehn (2004). 6.2 Four syntactic parsers are chosen for the experiments. They are Stanford Parser4, Berkeley Parser 5 , Collins Parser (Dan Bikel’s reimplementation of Collins Model 2) 6 and Charniak Parser7. The former two are state-of-the-art nonlexicalized parsers, while the latter two are stateof-the-art lexicalized parsers. All the parsers are trained on sections 02-21 of the Wall Street Journal (WSJ) Treebank, and tuned on section 22. Table 2 summarizes the performance of the parsers. Experimental Setup Our bilingual data consists of 370K sentence pairs (9M Chinese words + 10M English words) which h"
C10-2154,P06-1077,0,0.134429,"imple and effective method to improve rule coverage by using multiple parsers in translation rule extraction, and then empirically investigate the effectiveness of our method on ChineseEnglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performa"
C10-2154,D08-1022,0,0.719676,"shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule coverage has been proved to be an important one that affects the translation accuracy of syntax-based systems (DeNeefe et al., 2007; Shen et al., 2008). However, these systems suffer from a problem that translation rules are extracted using only 1-best parse tree generated by a single parser, which generally results in relatively low rule coverage due to the limited scope in rule extraction (Mi and Huang, 2008). To alleviate this problem, a straightforward solution is to enlarge the scope of rule extraction, and obtain translation rules by using a group of diversified parse trees instead of a single parse tree. For example, Mi and Huang (2008) used k-best parses and forest to extract translation rules for improving the rule coverage in their forest-based SMT system, and achieved promising results. However, most previous work used the parse trees generated by only one parser, which still suffered somewhat from the relatively low diversity in the outputs of a single parser. Addressing this issue, we i"
C10-2154,P05-1034,0,0.0951562,"Missing"
C10-2154,P08-1066,0,0.0390355,"empirically investigate the effectiveness of our method on ChineseEnglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributi"
C10-2154,2008.amta-papers.18,0,0.0357989,"Missing"
C10-2154,D07-1078,0,0.0361915,"Missing"
C10-2154,D09-1038,1,0.814184,"utomatically. 6 Experiments The experiments are conducted on ChineseEnglish translation in a state-of-the-art string-totree SMT system. 6.1 and the composed rules are generated by composing two or three minimal GHKM and SPMT rules3. We use a CKY-style decoder with cube pruning (Huang and Chiang, 2007) and beam search to decode new Chinese sentences. By default, the beam size is set to 30. For integrating n-gram language model into decoding efficiently, rules containing more than two variables or source word sequences are binarized using the synchronous binarization method (Zhang et al., 2006; Xiao et al., 2009). The system is evaluated in terms of the caseinsensitive NIST version BLEU (using the shortest reference length), and statistical significant test is conducted using the re-sampling method proposed by Koehn (2004). 6.2 Four syntactic parsers are chosen for the experiments. They are Stanford Parser4, Berkeley Parser 5 , Collins Parser (Dan Bikel’s reimplementation of Collins Model 2) 6 and Charniak Parser7. The former two are state-of-the-art nonlexicalized parsers, while the latter two are stateof-the-art lexicalized parsers. All the parsers are trained on sections 02-21 of the Wall Street Jo"
C10-2154,P01-1067,0,0.0737178,"rsers in translation rule extraction, and then empirically investigate the effectiveness of our method on ChineseEnglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al."
C10-2154,N06-1033,1,0.816388,"different parsers automatically. 6 Experiments The experiments are conducted on ChineseEnglish translation in a state-of-the-art string-totree SMT system. 6.1 and the composed rules are generated by composing two or three minimal GHKM and SPMT rules3. We use a CKY-style decoder with cube pruning (Huang and Chiang, 2007) and beam search to decode new Chinese sentences. By default, the beam size is set to 30. For integrating n-gram language model into decoding efficiently, rules containing more than two variables or source word sequences are binarized using the synchronous binarization method (Zhang et al., 2006; Xiao et al., 2009). The system is evaluated in terms of the caseinsensitive NIST version BLEU (using the shortest reference length), and statistical significant test is conducted using the re-sampling method proposed by Koehn (2004). 6.2 Four syntactic parsers are chosen for the experiments. They are Stanford Parser4, Berkeley Parser 5 , Collins Parser (Dan Bikel’s reimplementation of Collins Model 2) 6 and Charniak Parser7. The former two are state-of-the-art nonlexicalized parsers, while the latter two are stateof-the-art lexicalized parsers. All the parsers are trained on sections 02-21 o"
C10-2154,P08-1064,0,0.0131556,"lts show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule coverage has been proved to be an important one that affects th"
C10-2154,C10-1151,1,0.824466,"k-best parses to improve multi-parser based rule extraction in practice. z The MT performance is not influenced by the parsing performance of the parsers used in rule extraction very much. Actually, the MT system does not show different preferences for different parsers. 1-best 8.5 Figure 4: Multi-parser based rule extraction & rule extraction with k-best parses (MT05). 7 8 In this paper, we present a simple and effective method to improve rule coverage by using multiple parsers in translation rule extraction. Experimental results show that 38.4 38.2 geneous decoding (or parsing) techniques (Zhu et al., 2010) to make use of heterogeneous grammars in the stage of decoding. Both topics are very interesting and worth studying in our future work. Besides k-best extraction, our method can also be applied to other rule extraction schemes, such as forest-based rule extraction. As (Mi and Huang, 2008) has shown that forest-based extraction is more effective than k-best extraction in improving translation accuracy, it is expected to achieve further improvements by using multiparser based rule extraction and forest-based rule extraction together. Discussion and Future Work In this work, all the parsers are"
C10-2154,W06-1606,0,0.380496,"Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule coverage has been proved to be an important one that affects the translation accuracy of syntax-based systems (DeNeefe et al., 2007; Shen et al., 2008). However, these systems suffer from a problem that translation rules are extracted using only 1-best parse tree generated by a single parser, which generally results in relatively low rule coverage due to the limited scope in rule extraction (Mi and Huang, 2008). To alleviate this problem, a straightforward solution is to enlarge the scope of rule extraction, and"
C10-2154,P09-1063,0,\N,Missing
C10-2154,W06-1628,0,\N,Missing
C10-2154,P06-1121,0,\N,Missing
C18-1123,P17-1183,0,0.122352,"comes with a high computational cost. The time complexity of the attention mechanism is O(N 2 ) if the number of decoding steps is proportional to the input length N . In addition to the computational concern, for dominantly-monotonic translation tasks, such as text normalization (Sproat and Jaitly, 2017), soft attention can be too relaxed and sub-optimal in terms of modeling efficiency. Hence, to reduce computational complexity as well as to improve model accuracy, recently there has been a surge of research interest in enforcing monotonic attention (Raffel et al., 2017) and hard attention (Aharoni and Goldberg, 2017) based on the observation that many sequence-to-sequence tasks are monotonic, including speech recognition and morphological inflection. In reality, the monotonicity assumption has to be made carefully. Even in the highly monotonic translation task of text normalization, which is mapping written text to spoken text, there are systematic reordering patterns like from “2018-10-03” to “October third, two thousand eighteen”, and from “$100” to “one hundred dollars”. These reordering patterns can involve arbitrarily long chunks of input. Without handling the infrequent but systematic reordering, mo"
C18-1123,P17-1177,0,0.025046,"with our approach to modeling is that we do not encode the entire stack explicitly. At each time step, we only feed the context indexed by the current stack and buffer configuration. We do not have an stack RNN, which can be expensive. Our transition RNN can be viewed as a special case of DRAGNN (Kong et al., 2017) which combines fixed features as input with recurrence links at each time step. Our recurrence link is only to the previous time step. For the approach of incorporating syntactic constraints into neural translation, the following papers are most relevant. Eriguchi et al. (2016) and Chen et al. (2017) assume the existence of source parse trees and enhance the encoder and the attention mechanism to attend to both words and syntactic phrases. We do not rely on external parsers. Stahlberg et al. (2016) let a hierarchical phrase-based decoder guide neural machine translation decoding. Reordering decisions can only be indirectly influenced by the hierarchical decoder. In contrast, we have an explicit hierarchical reordering model applied pre-translation. Eriguchi et al. (2017) train a joint parsing and translation model to maximize the log likelihood of output sequence and input parsing action"
C18-1123,P05-1066,0,0.254977,"Missing"
C18-1123,D11-1018,0,0.0259011,"1/S X log p(P |S) + log p(T |P (S)) (2) (T,S)∈S and the decoding objective is Tˆ = arg max p(P |S)·p(T |P (S)) T,P (3) The term − log p(P |S) is the reordering loss and the term − log p(T |P (S)) is the translation loss. Both share the input S. In a computation graph, they share the BiRNN component. The hidden states (h1 , . . . , hTx ) are trained for both tasks. At decoding time, we approximate the decoding objective with a beam search over P , followed by another beam search over T . 1458 5 Related Work There are three papers that are most relevant to ours that relate to ITG pre-ordering. DeNero and Uszkoreit (2011) induce binary source trees first and learn pre-reordering rules for these binary trees from parallel data. Neubig et al. (2012) discriminatively train an ITG parser with CYK parsing for pre-reordering, essentially combining the two steps in DeNero and Uszkoreit (2011) into one. Nakagawa (2015) improve upon Neubig et al. (2012) with a linear time top-down ITG parsing algorithm. They all rely on feature engineering as they use linear models for training. None of them does transition-based parsing for ITG. There are two papers most relevant to ours that relate to combining transition systems wit"
C18-1123,P15-1033,0,0.0256749,"binary source trees first and learn pre-reordering rules for these binary trees from parallel data. Neubig et al. (2012) discriminatively train an ITG parser with CYK parsing for pre-reordering, essentially combining the two steps in DeNero and Uszkoreit (2011) into one. Nakagawa (2015) improve upon Neubig et al. (2012) with a linear time top-down ITG parsing algorithm. They all rely on feature engineering as they use linear models for training. None of them does transition-based parsing for ITG. There are two papers most relevant to ours that relate to combining transition systems with RNNs. Dyer et al. (2015) propose a stack-LSTM for transition-based parsing. The difference with our approach to modeling is that we do not encode the entire stack explicitly. At each time step, we only feed the context indexed by the current stack and buffer configuration. We do not have an stack RNN, which can be expensive. Our transition RNN can be viewed as a special case of DRAGNN (Kong et al., 2017) which combines fixed features as input with recurrence links at each time step. Our recurrence link is only to the previous time step. For the approach of incorporating syntactic constraints into neural translation,"
C18-1123,P16-1078,0,0.0264784,"ed parsing. The difference with our approach to modeling is that we do not encode the entire stack explicitly. At each time step, we only feed the context indexed by the current stack and buffer configuration. We do not have an stack RNN, which can be expensive. Our transition RNN can be viewed as a special case of DRAGNN (Kong et al., 2017) which combines fixed features as input with recurrence links at each time step. Our recurrence link is only to the previous time step. For the approach of incorporating syntactic constraints into neural translation, the following papers are most relevant. Eriguchi et al. (2016) and Chen et al. (2017) assume the existence of source parse trees and enhance the encoder and the attention mechanism to attend to both words and syntactic phrases. We do not rely on external parsers. Stahlberg et al. (2016) let a hierarchical phrase-based decoder guide neural machine translation decoding. Reordering decisions can only be indirectly influenced by the hierarchical decoder. In contrast, we have an explicit hierarchical reordering model applied pre-translation. Eriguchi et al. (2017) train a joint parsing and translation model to maximize the log likelihood of output sequence an"
C18-1123,P17-2012,0,0.0205351,"rporating syntactic constraints into neural translation, the following papers are most relevant. Eriguchi et al. (2016) and Chen et al. (2017) assume the existence of source parse trees and enhance the encoder and the attention mechanism to attend to both words and syntactic phrases. We do not rely on external parsers. Stahlberg et al. (2016) let a hierarchical phrase-based decoder guide neural machine translation decoding. Reordering decisions can only be indirectly influenced by the hierarchical decoder. In contrast, we have an explicit hierarchical reordering model applied pre-translation. Eriguchi et al. (2017) train a joint parsing and translation model to maximize the log likelihood of output sequence and input parsing action sequence. This is similar to our multi-task training setup. The key difference is our subtask is ITG parsing for reordering instead of linguistically-motivated parsing. The idea of adding a reordering layer into neural MT models has also been studied by Huang et al. (2018). They use a simple feed-forward soft and local reordering layer similar to the soft attention mechanism. A fixed window size is used for local reordering. Our RNN reordering layer can handle long distance r"
C18-1123,D18-1549,0,0.0230176,"l machine translation decoding. Reordering decisions can only be indirectly influenced by the hierarchical decoder. In contrast, we have an explicit hierarchical reordering model applied pre-translation. Eriguchi et al. (2017) train a joint parsing and translation model to maximize the log likelihood of output sequence and input parsing action sequence. This is similar to our multi-task training setup. The key difference is our subtask is ITG parsing for reordering instead of linguistically-motivated parsing. The idea of adding a reordering layer into neural MT models has also been studied by Huang et al. (2018). They use a simple feed-forward soft and local reordering layer similar to the soft attention mechanism. A fixed window size is used for local reordering. Our RNN reordering layer can handle long distance reordering. Another important difference is that we use discrete variables (permutations) for reordering while the soft reordering mechanism has no latent variables. We leave it as future work to train the end-to-end system by treating ITG transitions and permutations as latent variables. 6 Experiments 6.1 Reordering Experiments In this part, we analyze the effectiveness of the ITG RNN reord"
C18-1123,D13-1049,0,0.022378,"rough swapping of adjacent source spans. This step “normalizes” the input in terms of word order which opens up the capacity for improvement in translation accuracy. It also opens up the opportunity for more “lightweight” neural translation models such as monotonic attention models of Raffel et al. (2017) and Aharoni and Goldberg (2017). Of course, the burden of reordering has shifted from the attention mechanism to the dedicated reordering model. This idea has produced fruitful results in the era of phrase-based machine translation (Collins et al., 2005; Xu et al., 2009; Neubig et al., 2012; Lerner and Petrov, 2013; Nakagawa, 2015). In this paper, we revive the old idea with a neural treatment. First of all, we modify the encoder-decoder architecture by adding a reorderer which shares the encoder with the decoder. Figure 1 shows the architecture. This change enables multi-task training of the encoder states and turns the inference into three steps: encoding, reordering, decoding, each of which is a linear chain RNN. Our main algorithmic contribution is an ITG-transition-based RNN reorderer. What we feed to the RNN for training are input strings paired with their permutations, for example $100 with (1, 2"
C18-1123,P15-1021,0,0.0788692,"t source spans. This step “normalizes” the input in terms of word order which opens up the capacity for improvement in translation accuracy. It also opens up the opportunity for more “lightweight” neural translation models such as monotonic attention models of Raffel et al. (2017) and Aharoni and Goldberg (2017). Of course, the burden of reordering has shifted from the attention mechanism to the dedicated reordering model. This idea has produced fruitful results in the era of phrase-based machine translation (Collins et al., 2005; Xu et al., 2009; Neubig et al., 2012; Lerner and Petrov, 2013; Nakagawa, 2015). In this paper, we revive the old idea with a neural treatment. First of all, we modify the encoder-decoder architecture by adding a reorderer which shares the encoder with the decoder. Figure 1 shows the architecture. This change enables multi-task training of the encoder states and turns the inference into three steps: encoding, reordering, decoding, each of which is a linear chain RNN. Our main algorithmic contribution is an ITG-transition-based RNN reorderer. What we feed to the RNN for training are input strings paired with their permutations, for example $100 with (1, 2, 3, 0) to indica"
C18-1123,D12-1077,0,0.124436,"tematic reordering through swapping of adjacent source spans. This step “normalizes” the input in terms of word order which opens up the capacity for improvement in translation accuracy. It also opens up the opportunity for more “lightweight” neural translation models such as monotonic attention models of Raffel et al. (2017) and Aharoni and Goldberg (2017). Of course, the burden of reordering has shifted from the attention mechanism to the dedicated reordering model. This idea has produced fruitful results in the era of phrase-based machine translation (Collins et al., 2005; Xu et al., 2009; Neubig et al., 2012; Lerner and Petrov, 2013; Nakagawa, 2015). In this paper, we revive the old idea with a neural treatment. First of all, we modify the encoder-decoder architecture by adding a reorderer which shares the encoder with the decoder. Figure 1 shows the architecture. This change enables multi-task training of the encoder states and turns the inference into three steps: encoding, reordering, decoding, each of which is a linear chain RNN. Our main algorithmic contribution is an ITG-transition-based RNN reorderer. What we feed to the RNN for training are input strings paired with their permutations, fo"
C18-1123,W03-3017,0,0.124468,"i X → w0 /w0 ... X → wn−1 /wn−1 The first rule is called the straight rule because it keeps the order of two constituents unchanged on the target side. The second rule is called the inverted rule because it inverts the order of two constituents on the target side. For example, a sentence pair (w0 , w1 , w2 |w0 , w2 , w1 ) can be derived with three pre-terminal rules to link the words with the same subscripts on both sides, plus one inverted rule for grouping w1 and w2 and one straight rule on the top for grouping w0 and the phrase of w1 , w2 . We can also devise a transition system following (Nivre, 2003). The following is the deductive description of the transition system. Each configuration in the transition system consists of a stack and a pointer to the next word in the input buffer. At the beginning, we have an empty stack and a pointer to the first input word. At each time step, we can choose S HIFT if the buffer is not empty and choose to apply R EDUCE S or R EDUCE I if the stack has a height of at least two. We use four indices to uniquely represent each synchronous constituent that is constructed in the parsing process. The first two index into the source side and the last two index i"
C18-1123,D15-1044,0,0.046712,"eature engineering. In experiments, we apply the model to the task of text normalization. Compared to a strong baseline of attention-based RNN, our ITG RNN reordering model can reach the same reordering accuracy with only 1/10 of the training data and is 2.5x faster in decoding. 1 Introduction The encoder-decoder neural network architecture for sequence-to-sequence problems has achieved enormous success especially after the introduction of the attention mechanism (Bahdanau et al., 2014). Its applications in NLP range from machine translation (Bahdanau et al., 2014) and sentence summarization (Rush et al., 2015) to text normalization (Sproat and Jaitly, 2017). The attention mechanism is effectively a random memory access mechanism, enabling access to any source sequence position at any decoding step. In principle, it can handle arbitrary reordering of any input length. But its power comes with a high computational cost. The time complexity of the attention mechanism is O(N 2 ) if the number of decoding steps is proportional to the input length N . In addition to the computational concern, for dominantly-monotonic translation tasks, such as text normalization (Sproat and Jaitly, 2017), soft attention"
C18-1123,P16-2049,0,0.026732,"an stack RNN, which can be expensive. Our transition RNN can be viewed as a special case of DRAGNN (Kong et al., 2017) which combines fixed features as input with recurrence links at each time step. Our recurrence link is only to the previous time step. For the approach of incorporating syntactic constraints into neural translation, the following papers are most relevant. Eriguchi et al. (2016) and Chen et al. (2017) assume the existence of source parse trees and enhance the encoder and the attention mechanism to attend to both words and syntactic phrases. We do not rely on external parsers. Stahlberg et al. (2016) let a hierarchical phrase-based decoder guide neural machine translation decoding. Reordering decisions can only be indirectly influenced by the hierarchical decoder. In contrast, we have an explicit hierarchical reordering model applied pre-translation. Eriguchi et al. (2017) train a joint parsing and translation model to maximize the log likelihood of output sequence and input parsing action sequence. This is similar to our multi-task training setup. The key difference is our subtask is ITG parsing for reordering instead of linguistically-motivated parsing. The idea of adding a reordering l"
C18-1123,J97-3002,0,0.445659,"work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 1454 Proceedings of the 27th International Conference on Computational Linguistics, pages 1454–1463 Santa Fe, New Mexico, USA, August 20-26, 2018. one hundred 1 0 1 2 dollars 0 3 Decoder $ 0 Reorderer 0 $ 1 1 2 3 0 0 Encoder Figure 1: The encoder-reorderer-decoder architecture. The input encoder is shared by the reorderer and the decoder. The reorderer permutes the RNN states of the encoder. key to our solution is the Inversion Transduction Grammars (Wu, 1997), a type of synchronous context free grammar limiting reordering to adjacent source spans. For machine translation across very different languages, ITGs have been reported to cover most of the alignments observed in parallel data (Zhang et al., 2006). For text normalization, we have not found a single example of reordering that cannot be covered by an ITG. This observation motivates us to factorize translation into two steps: an ITG reordering step followed by a monotonic translation step. The task of the reordering step is to handle systematic reordering through swapping of adjacent source sp"
C18-1123,N09-1028,0,0.0319698,"is to handle systematic reordering through swapping of adjacent source spans. This step “normalizes” the input in terms of word order which opens up the capacity for improvement in translation accuracy. It also opens up the opportunity for more “lightweight” neural translation models such as monotonic attention models of Raffel et al. (2017) and Aharoni and Goldberg (2017). Of course, the burden of reordering has shifted from the attention mechanism to the dedicated reordering model. This idea has produced fruitful results in the era of phrase-based machine translation (Collins et al., 2005; Xu et al., 2009; Neubig et al., 2012; Lerner and Petrov, 2013; Nakagawa, 2015). In this paper, we revive the old idea with a neural treatment. First of all, we modify the encoder-decoder architecture by adding a reorderer which shares the encoder with the decoder. Figure 1 shows the architecture. This change enables multi-task training of the encoder states and turns the inference into three steps: encoding, reordering, decoding, each of which is a linear chain RNN. Our main algorithmic contribution is an ITG-transition-based RNN reorderer. What we feed to the RNN for training are input strings paired with t"
C18-1123,N06-1033,1,0.855184,"1463 Santa Fe, New Mexico, USA, August 20-26, 2018. one hundred 1 0 1 2 dollars 0 3 Decoder $ 0 Reorderer 0 $ 1 1 2 3 0 0 Encoder Figure 1: The encoder-reorderer-decoder architecture. The input encoder is shared by the reorderer and the decoder. The reorderer permutes the RNN states of the encoder. key to our solution is the Inversion Transduction Grammars (Wu, 1997), a type of synchronous context free grammar limiting reordering to adjacent source spans. For machine translation across very different languages, ITGs have been reported to cover most of the alignments observed in parallel data (Zhang et al., 2006). For text normalization, we have not found a single example of reordering that cannot be covered by an ITG. This observation motivates us to factorize translation into two steps: an ITG reordering step followed by a monotonic translation step. The task of the reordering step is to handle systematic reordering through swapping of adjacent source spans. This step “normalizes” the input in terms of word order which opens up the capacity for improvement in translation accuracy. It also opens up the opportunity for more “lightweight” neural translation models such as monotonic attention models of"
D12-1030,P11-1045,0,0.0108961,"in machine translation. This also distinguishes it from previous work on dependency parse re-ranking (Hall, 2007) as we are not re-ranking/re-scoring the output of a base model but using a single decoding algorithm and learned model at training and testing. This work is largely orthogonal to other attempts to speed up chart parsing algorithms. This includes work on coarse-to-fine parsing (Charniak and Johnson, 2005; Petrov and Klein, 2007; Rush and Petrov, 2012), chart-cell closing and pruning (Roark and Hollingshead, 2008; Roark and Hollingshead, 329 2009), and dynamic beam-width prediction (Bodenstab et al., 2011). Of particular note, Rush and Petrov (2012) report run-times far better than our cube pruning system. At the heart of their system is a linear time vine-parsing stage that prunes most of the search space before higher-order parsing. This effectively makes their final system linear time in practice as the higher order models have far fewer parts to consider. One could easily use the same first-pass pruner in our cube-pruning framework. In our study we use cube pruning only for decoding and rely on inference-based learning algorithms to train model parameters. Gimpel and Smith (2009) extended c"
D12-1030,W06-2920,0,0.462257,"Missing"
D12-1030,D07-1101,0,0.738168,"constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a high cost in terms of efficiency. Furthermore, specialized algorithms are required that deeply exploit the structural properties of the given model. Upgrading a parser to score new types of higher-order dependencies thus requires significant changes to the underlying decoding algorithm. This is in stark co"
D12-1030,P05-1022,0,0.199893,"a generative base model from a following discriminative re-ranking model. Hence, our formulation is more akin to the one pass decoding algorithm of Chiang (2007) for integrated decoding with a language model in machine translation. This also distinguishes it from previous work on dependency parse re-ranking (Hall, 2007) as we are not re-ranking/re-scoring the output of a base model but using a single decoding algorithm and learned model at training and testing. This work is largely orthogonal to other attempts to speed up chart parsing algorithms. This includes work on coarse-to-fine parsing (Charniak and Johnson, 2005; Petrov and Klein, 2007; Rush and Petrov, 2012), chart-cell closing and pruning (Roark and Hollingshead, 2008; Roark and Hollingshead, 329 2009), and dynamic beam-width prediction (Bodenstab et al., 2011). Of particular note, Rush and Petrov (2012) report run-times far better than our cube pruning system. At the heart of their system is a linear time vine-parsing stage that prunes most of the search space before higher-order parsing. This effectively makes their final system linear time in practice as the higher order models have far fewer parts to consider. One could easily use the same firs"
D12-1030,J07-2003,0,0.323506,"higher-order features only involves changing the scoring function of 320 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 320–331, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics potential parses in each chart cell by expanding the signature of each chart item to include all the nonlocal context required to compute features. The core chart-parsing algorithm remains the same regardless of which features are incorporated. To control complexity we use cube pruning (Chiang, 2007) with the beam size k in each cell. Furthermore, dynamic programming in the style of Huang and Sagae (2010) can be done by merging k-best items that are equivalent in scoring. Thus, our method is an application of integrated decoding with a language model in MT (Chiang, 2007) to dependency parsing, which has previously been applied to constituent parsing (Huang, 2008). However, unlike Huang, we only have one decoding pass and a single trained model, while Huang’s constituent parser maintains a separate generative base model from a following discriminative re-ranking model. We draw connections"
D12-1030,de-marneffe-etal-2006-generating,0,0.0123389,"Missing"
D12-1030,D07-1098,0,0.0197449,"well documented (McDonald and Nivre, 2007; Nivre and McDonald, 2008). Graph-based parsers typically tradeoff rich feature scope for exact (or near exact) decoding, whereas transition-based parsers make the opposite trade-off. Recent research on both parsing paradigms has attempted to address this. In the transition-based parsing literature, the focus has been on increasing the search space of the system at decoding time, as expanding the feature scope is often trivial and in most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear progr"
D12-1030,C96-1058,0,0.561994,"s expanding the feature scope is often trivial and in most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a high cost in terms of efficiency. Furthermore, specialized algorithms are required that deeply exploit the structural properties of the given model. Upgrading a parser to score new types of higher-order dependencies thus requi"
D12-1030,E09-1037,0,0.0167381,"rediction (Bodenstab et al., 2011). Of particular note, Rush and Petrov (2012) report run-times far better than our cube pruning system. At the heart of their system is a linear time vine-parsing stage that prunes most of the search space before higher-order parsing. This effectively makes their final system linear time in practice as the higher order models have far fewer parts to consider. One could easily use the same first-pass pruner in our cube-pruning framework. In our study we use cube pruning only for decoding and rely on inference-based learning algorithms to train model parameters. Gimpel and Smith (2009) extended cube pruning concepts to partitionfunction and marginal calculations, which would enable the training of probabilistic graphical models. Finally, due to its use of the Eisner chart-parsing algorithm as a backbone, our model is fundamentally limited to predicting projective dependency structures. Investigating extensions of this work to the non-projective case is an area of future study. Work on defining bottom-up chart-parsing algorithms for non-projective dependency trees could potentially serve as a mechanism to solving this problem (G´omez-Rodr´ıguez et al., 2009; Kuhlmann and Sat"
D12-1030,N09-1061,0,0.0383786,"Missing"
D12-1030,N10-1035,0,0.036726,"Missing"
D12-1030,P07-1050,0,0.0551976,"the extension of such ideas to dependency parsing, also giving state-ofthe-art results. An important difference between our formulation and forest rescoring is that we only have one decoding pass and a single trained model, while forest rescoring, as formulated by Huang (2008), separates a generative base model from a following discriminative re-ranking model. Hence, our formulation is more akin to the one pass decoding algorithm of Chiang (2007) for integrated decoding with a language model in machine translation. This also distinguishes it from previous work on dependency parse re-ranking (Hall, 2007) as we are not re-ranking/re-scoring the output of a base model but using a single decoding algorithm and learned model at training and testing. This work is largely orthogonal to other attempts to speed up chart parsing algorithms. This includes work on coarse-to-fine parsing (Charniak and Johnson, 2005; Petrov and Klein, 2007; Rush and Petrov, 2012), chart-cell closing and pruning (Roark and Hollingshead, 2008; Roark and Hollingshead, 329 2009), and dynamic beam-width prediction (Bodenstab et al., 2011). Of particular note, Rush and Petrov (2012) report run-times far better than our cube pru"
D12-1030,N12-1015,0,0.017798,"Missing"
D12-1030,P10-1110,0,0.481055,"rsers make the opposite trade-off. Recent research on both parsing paradigms has attempted to address this. In the transition-based parsing literature, the focus has been on increasing the search space of the system at decoding time, as expanding the feature scope is often trivial and in most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a"
D12-1030,P08-1067,0,0.742354,"ure of each chart item to include all the nonlocal context required to compute features. The core chart-parsing algorithm remains the same regardless of which features are incorporated. To control complexity we use cube pruning (Chiang, 2007) with the beam size k in each cell. Furthermore, dynamic programming in the style of Huang and Sagae (2010) can be done by merging k-best items that are equivalent in scoring. Thus, our method is an application of integrated decoding with a language model in MT (Chiang, 2007) to dependency parsing, which has previously been applied to constituent parsing (Huang, 2008). However, unlike Huang, we only have one decoding pass and a single trained model, while Huang’s constituent parser maintains a separate generative base model from a following discriminative re-ranking model. We draw connections to related work in Section 6. Our chart-based approximate search algorithm allows for features on dependencies of an arbitrary order — as well as over non-local structural properties of the parse trees — to be scored at will. In this paper, we use first to third-order features of greater varieties than Koo and Collins (2010). Additionally, we look at higher-order depe"
D12-1030,D07-1123,0,0.034191,"Donald and Nivre, 2007; Nivre and McDonald, 2008). Graph-based parsers typically tradeoff rich feature scope for exact (or near exact) decoding, whereas transition-based parsers make the opposite trade-off. Recent research on both parsing paradigms has attempted to address this. In the transition-based parsing literature, the focus has been on increasing the search space of the system at decoding time, as expanding the feature scope is often trivial and in most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of"
D12-1030,P10-1001,0,0.0902925,"ncrease in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a high cost in terms of efficiency. Furthermore, specialized algorithms are required that deeply exploit the structural properties of the given model. Upgrading a parser to score new types of higher-order dependencies thus requires significant changes to the underlying decoding algorithm. This is in stark contrast to transition-bas"
D12-1030,D10-1125,0,0.25314,"007; Koo and Collins, 2010), but at a high computational cost as increasing the order of a model typically results in an asymptotic increase in running time. ILP formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010) also allow for exact inference with higherorder features, but again at a high computational cost as ILP’s have, in the worst-case, exponential run-time with respect to the sentence length. Studies that have abandoned exact inference have focused on sampling (Nakagawa, 2007), belief propagation (Smith and Eisner, 2008), Lagrangian relaxation (Koo et al., 2010; Martins et al., 2011), and more recently structured prediction cascades (Weiss and Taskar, 2010; Rush and Petrov, 2012). However, these approximations themselves are often computationally expensive, requiring multiple decoding/sampling stages in order to produce an output. All the methods above, both exact and approximate, require specialized algorithms for every new feature that is beyond the scope of the previous factorization. In our method, the same parsing algorithm can be utilized (Eisner’s + cube pruning) just with slight different feature signatures. Our proposed parsing model draws"
D12-1030,E09-1055,0,0.0109773,"and Smith (2009) extended cube pruning concepts to partitionfunction and marginal calculations, which would enable the training of probabilistic graphical models. Finally, due to its use of the Eisner chart-parsing algorithm as a backbone, our model is fundamentally limited to predicting projective dependency structures. Investigating extensions of this work to the non-projective case is an area of future study. Work on defining bottom-up chart-parsing algorithms for non-projective dependency trees could potentially serve as a mechanism to solving this problem (G´omez-Rodr´ıguez et al., 2009; Kuhlmann and Satta, 2009; G´omez-Rodr´ıguez et al., 2010). 7 Conclusion In this paper we presented a method for generalized higher-order dependency parsing. The method works by augmenting the dynamic programming signatures of the Eisner chart-parsing algorithm and then controlling complexity via cube pruning. The resulting system has the flexibility to incorporate arbitrary feature history while still exploring an exponential search space efficiently. Empirical results show that the system gives state-of-the-art accuracies across numerous data sets while still maintaining practical parsing speeds – as much as 4-5 tim"
D12-1030,P09-1039,0,0.617285,"and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a high cost in terms of efficiency. Furthermore, specialized algorithms are required that deeply exploit the structural properties of the given model. Upgrading a parser to score new types of higher-order dependencies thus requires significant changes to the underlying decoding algorithm. This is in stark contrast to transition-based systems, which simply require the definition of new feature extractors. In this paper, we abandon exact search in graphbased parsing in favor of freedom"
D12-1030,D10-1004,0,0.322372,"and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a high cost in terms of efficiency. Furthermore, specialized algorithms are required that deeply exploit the structural properties of the given model. Upgrading a parser to score new types of higher-order dependencies thus requires significant changes to the underlying decoding algorithm. This is in stark contrast to transition-based systems, which simply require the definition of new feature extractors. In this paper, we abandon exact search in graphbased parsing in favor of freedom in feature scope. We pr"
D12-1030,D11-1022,0,0.0527376,"ns, 2010), but at a high computational cost as increasing the order of a model typically results in an asymptotic increase in running time. ILP formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010) also allow for exact inference with higherorder features, but again at a high computational cost as ILP’s have, in the worst-case, exponential run-time with respect to the sentence length. Studies that have abandoned exact inference have focused on sampling (Nakagawa, 2007), belief propagation (Smith and Eisner, 2008), Lagrangian relaxation (Koo et al., 2010; Martins et al., 2011), and more recently structured prediction cascades (Weiss and Taskar, 2010; Rush and Petrov, 2012). However, these approximations themselves are often computationally expensive, requiring multiple decoding/sampling stages in order to produce an output. All the methods above, both exact and approximate, require specialized algorithms for every new feature that is beyond the scope of the previous factorization. In our method, the same parsing algorithm can be utilized (Eisner’s + cube pruning) just with slight different feature signatures. Our proposed parsing model draws heavily on the work of"
D12-1030,D07-1013,1,0.778257,"Missing"
D12-1030,E06-1011,1,0.911376,"n most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a high cost in terms of efficiency. Furthermore, specialized algorithms are required that deeply exploit the structural properties of the given model. Upgrading a parser to score new types of higher-order dependencies thus requires significant changes to the underlying decoding algorithm. Thi"
D12-1030,P05-1012,1,0.569981,"of head-modifier dependency arcs y ∗ such that y ∗ = argmaxy∈Y(x) f (x, y), where f is a scoring function. As mentioned before, y ∗ must represent a directed tree. |Y(x) |is then the set of valid dependency trees for x and grows exponentially with respect to its length |x|. We further define L as the set of possible arc labels and use l the notation (i − → j) ∈ y to indicate that there is a dependency from head word xi to modifier xj with label l in dependency tree y. In practice, f (x, y) is factorized into scoring functions on parts of (x, y). For example, in firstorder dependency parsing (McDonald et al., 2005), f (x, y) is factored by the individual arcs: X l f (i − → j) y ∗ = argmax f (x, y) = argmax y∈Y(x) y∈Y(x) l (i− →j)∈y The factorization of dependency structures into arcs enables an efficient dynamic programming algorithm with running time O(|x|3 ) (Eisner, 1996), for the large family of projective dependency structures. Figure 2 shows the parsing logic for the Eisner algorithm. It has two types of dynamic programming states: complete items and incomplete items. Complete items correspond to half-constituents, and are represented as triangles graphically. Incomplete items correspond to depend"
D12-1030,D07-1100,0,0.0337083,"order features have been studied extensively (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), but at a high computational cost as increasing the order of a model typically results in an asymptotic increase in running time. ILP formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010) also allow for exact inference with higherorder features, but again at a high computational cost as ILP’s have, in the worst-case, exponential run-time with respect to the sentence length. Studies that have abandoned exact inference have focused on sampling (Nakagawa, 2007), belief propagation (Smith and Eisner, 2008), Lagrangian relaxation (Koo et al., 2010; Martins et al., 2011), and more recently structured prediction cascades (Weiss and Taskar, 2010; Rush and Petrov, 2012). However, these approximations themselves are often computationally expensive, requiring multiple decoding/sampling stages in order to produce an output. All the methods above, both exact and approximate, require specialized algorithms for every new feature that is beyond the scope of the previous factorization. In our method, the same parsing algorithm can be utilized (Eisner’s + cube pru"
D12-1030,P08-1108,1,0.937438,"Missing"
D12-1030,N07-1051,0,0.0301882,"a following discriminative re-ranking model. Hence, our formulation is more akin to the one pass decoding algorithm of Chiang (2007) for integrated decoding with a language model in machine translation. This also distinguishes it from previous work on dependency parse re-ranking (Hall, 2007) as we are not re-ranking/re-scoring the output of a base model but using a single decoding algorithm and learned model at training and testing. This work is largely orthogonal to other attempts to speed up chart parsing algorithms. This includes work on coarse-to-fine parsing (Charniak and Johnson, 2005; Petrov and Klein, 2007; Rush and Petrov, 2012), chart-cell closing and pruning (Roark and Hollingshead, 2008; Roark and Hollingshead, 329 2009), and dynamic beam-width prediction (Bodenstab et al., 2011). Of particular note, Rush and Petrov (2012) report run-times far better than our cube pruning system. At the heart of their system is a linear time vine-parsing stage that prunes most of the search space before higher-order parsing. This effectively makes their final system linear time in practice as the higher order models have far fewer parts to consider. One could easily use the same first-pass pruner in our cub"
D12-1030,W06-1616,0,0.222765,"d Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a high cost in terms of efficiency. Furthermore, specialized algorithms are required that deeply exploit the structural properties of the given model. Upgrading a parser to score new types of higher-order dependencies thus requires significant changes to the underlying decoding algorithm. This is in stark contrast to transition-based systems, which simply require the definition of new feature extractors. In this paper, we abandon exact search in graphbased parsin"
D12-1030,C08-1094,0,0.0239384,"n to the one pass decoding algorithm of Chiang (2007) for integrated decoding with a language model in machine translation. This also distinguishes it from previous work on dependency parse re-ranking (Hall, 2007) as we are not re-ranking/re-scoring the output of a base model but using a single decoding algorithm and learned model at training and testing. This work is largely orthogonal to other attempts to speed up chart parsing algorithms. This includes work on coarse-to-fine parsing (Charniak and Johnson, 2005; Petrov and Klein, 2007; Rush and Petrov, 2012), chart-cell closing and pruning (Roark and Hollingshead, 2008; Roark and Hollingshead, 329 2009), and dynamic beam-width prediction (Bodenstab et al., 2011). Of particular note, Rush and Petrov (2012) report run-times far better than our cube pruning system. At the heart of their system is a linear time vine-parsing stage that prunes most of the search space before higher-order parsing. This effectively makes their final system linear time in practice as the higher order models have far fewer parts to consider. One could easily use the same first-pass pruner in our cube-pruning framework. In our study we use cube pruning only for decoding and rely on in"
D12-1030,N09-1073,0,0.0582924,"Missing"
D12-1030,N12-1054,0,0.771249,".66 93.20 / 91.25 91.36 / 87.22 90.50 / 83.01 86.63 / 84.95 3rd -order exact (reimpl.) 92.96 / 94.07 / 91.29 / 87.26 / 86.49 / 93.36 / 91.66 / 90.32 / 86.77 / - this paper 93.08 / 88.23 94.00 / 88.08 91.35 / 88.42 87.48 / 84.05 86.54 / 82.15 93.24 / 91.45 91.69 / 87.70 91.44 / 84.58 86.87 / 85.19 89.05 / 84.74 90.14 / 85.89 90.46 / - 90.63 / 86.65 Table 2: UAS/LAS for experiments on non-English treebanks. Numbers in bold are the highest scoring system. Zhang and Nivre is a reimplementation of Zhang and Nivre (2011) with beams of size 64 and 256. Rush and Petrov are the UAS results reported in Rush and Petrov (2012). Nth -order exact are implementations of exact 1st-3rd order dependency parsing. † For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best result reported on this data set. ‡ It should be noted that Rush and Petrov (2012) do not jointly optimize labeled and unlabeled dependency structure, which we found to often help. This, plus extra features, accounts for the differences in UAS. bels in the Penn2Malt label set, which results in little non-structural ambiguity. In contrast, Stanfordstyle dependencies contain a much larger set of labels (50) with more fine-grained"
D12-1030,D08-1016,0,0.0429328,"nsively (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), but at a high computational cost as increasing the order of a model typically results in an asymptotic increase in running time. ILP formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010) also allow for exact inference with higherorder features, but again at a high computational cost as ILP’s have, in the worst-case, exponential run-time with respect to the sentence length. Studies that have abandoned exact inference have focused on sampling (Nakagawa, 2007), belief propagation (Smith and Eisner, 2008), Lagrangian relaxation (Koo et al., 2010; Martins et al., 2011), and more recently structured prediction cascades (Weiss and Taskar, 2010; Rush and Petrov, 2012). However, these approximations themselves are often computationally expensive, requiring multiple decoding/sampling stages in order to produce an output. All the methods above, both exact and approximate, require specialized algorithms for every new feature that is beyond the scope of the previous factorization. In our method, the same parsing algorithm can be utilized (Eisner’s + cube pruning) just with slight different feature sign"
D12-1030,D07-1099,0,0.0217787,"e and McDonald, 2008). Graph-based parsers typically tradeoff rich feature scope for exact (or near exact) decoding, whereas transition-based parsers make the opposite trade-off. Recent research on both parsing paradigms has attempted to address this. In the transition-based parsing literature, the focus has been on increasing the search space of the system at decoding time, as expanding the feature scope is often trivial and in most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke"
D12-1030,D08-1059,0,0.442873,"h-based parsers typically tradeoff rich feature scope for exact (or near exact) decoding, whereas transition-based parsers make the opposite trade-off. Recent research on both parsing paradigms has attempted to address this. In the transition-based parsing literature, the focus has been on increasing the search space of the system at decoding time, as expanding the feature scope is often trivial and in most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al.,"
D12-1030,P11-2033,0,0.739432,"ly tradeoff rich feature scope for exact (or near exact) decoding, whereas transition-based parsers make the opposite trade-off. Recent research on both parsing paradigms has attempted to address this. In the transition-based parsing literature, the focus has been on increasing the search space of the system at decoding time, as expanding the feature scope is often trivial and in most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2"
D12-1030,D07-1096,1,\N,Missing
D13-1093,E12-1009,0,0.153822,"onald (2012). 3 Experiments We ran a number of experiments on the cubepruning dependency parser of Zhang and McDonald (2012), whose search space can be represented as a hypergraph in which the nodes are the complete and incomplete states and the hyperedges are the instantiations of the two parsing rules in the Eisner algorithm (Eisner, 1996). The feature templates we used are a superset of Zhang and McDonald (2012). These features include first-, second-, and third-order features and their labeled counterparts, as well as valency features. In addition, we also included a feature template from Bohnet and Kuhn (2012). This template examines the leftmost child and the rightmost child of a modifier simultaneously. All other highorder features of Zhang and McDonald (2012) only look at arcs on the same side of their head. We trained the parser with hamming-loss-augmented MIRA (Crammer et al., 2006), following Martins et al. (2010). Based on results on the English validation data, in all the experiments, we trained MIRA with 8 epochs and used a beam of size 6 per node. To speed up the parser, we used an unlabeled first-order model to prune unlikely dependency arcs at both training and testing time (Koo and Col"
D13-1093,W06-2920,0,0.0316564,"1.49 93.80 89.65 87.79 83.59 91.62 85.00 80.60 70.12 76.86 66.56 92.00 87.07 92.19 88.40 86.46 78.55 85.77 76.62 88.48 82.38 79.61 71.65 86.49 81.67 91.79 89.28 83.35 80.09 87.80 82.14 Best Published† UAS LAS 87.48 84.05 94.07 89.09 93.72 91.793.50 88.23 87.47 83.50 91.44 85.42 81.12 66.977.55 65.791.86 84.893.03 87.70 86.05 77.87 86.95 73.490.32 80.280.23 73.18 86.81 81.86 92.41 88.42 86.19 79.2- Table 2: Parsing Results for languages from CoNLL 2006/2007 shared tasks. When a language is in both years, we use the 2006 data set. The best results with † are the maximum in the following papers: Buchholz and Marsi (2006), Nivre et al. (2007), Zhang and McDonald (2012), Bohnet and Kuhn (2012), and Martins et al. (2013), For consistency, we scored the CoNLL 2007 best systems with the CoNLL 2006 evaluation script. ZN 2011 (reimpl.) is our reimplementation of Zhang and Nivre (2011), with a beam of 64. Results in bold are the best among ZN 2011 reimplementation and different update strategies from this paper. 3.3 CoNLL Results We also report parsing results for 17 languages from the CoNLL 2006/2007 shared-task (Buchholz and Marsi, 2006; Nivre et al., 2007). The parser in our experiments can only produce projective"
D13-1093,D08-1024,0,0.190379,"Missing"
D13-1093,J07-2003,0,0.0261665,"Missing"
D13-1093,P04-1015,0,0.10193,"this, they generalized the original update rule to select an output y ′ within the pruned search space that scores higher than yˆ, but is not necessarily the highest among all possibilities, which represents a true violation of the model on that training instance. This violation fixing perceptron thus relaxes the argmax function to accommodate inexact search and becomes provably convergent as a result. In the sequential cases where yˆ has a linear structure such as tagging and incremental parsing, the violation fixing perceptron boils down to finding and updating along a certain prefix of yˆ. Collins and Roark (2004) locate the earliest position in a ′ chain structure where yˆpref is worse than ypref by a margin large enough to cause yˆ to be dropped from the beam. Huang et al. (2012) locate the position where the violation is largest among all prefixes of yˆ, where size of a violation is defined as ′ w · f (x, ypref ) − w · f (x, yˆpref ). For hypergraphs, the notion of prefix must be generalized to subtrees. Figure 1 shows the packedforest representation of the union of gold subtrees and highest-scoring (Viterbi) subtrees at every gold node for an input. At each gold node, there are two incoming hypered"
D13-1093,W02-1001,0,0.0711707,"ollins and Roark, 2004; Daum´e and Marcu, 2005; Zhang and Clark, 2008; Huang et al., 2012). However, sequential search algorithms, and in particular left-to-right beam search (Collins and Roark, 2004; Zhang and Clark, 2008), squeeze inference into a very narrow space. To address this, Huang (2008) formulated constituency parsing as approximate bottom-up inference in order to compactly represent an exponential number of outputs while scoring features of arbitrary scope. This idea was adapted to graph-based Structured Perceptron for Inexact Hypergraph Search The structured perceptron algorithm (Collins, 2002) is a general learning algorithm. Given training instances (x, yˆ), the algorithm first solves the decoding problem y ′ = argmaxy∈Y(x) w · f (x, y) given the weight vector w for the high-dimensional feature representation f of the mapping (x, y), where y ′ is the prediction under the current model, yˆ is the gold output and Y(x) is the space of all valid outputs for input x. The perceptron update rule is simply: w′ = w + f (x, yˆ) − f (x, y ′ ). The convergence of original perceptron algorithm relies on the argmax function being exact so that the condition w · f (x, y ′ ) &gt; w · f (x, yˆ) (modu"
D13-1093,de-marneffe-etal-2006-generating,0,0.00528498,"Missing"
D13-1093,C96-1058,0,0.144046,"Missing"
D13-1093,N12-1015,1,0.222192,"ecific instances of inexact hypergraph search. Typically, the approximation is accomplished by cube-pruning throughout the hypergraph (Chiang, 2007). Unfortunately, as the scope of features at each node increases, the inexactness of search and its negative impact on learning can potentially be exacerbated. Unlike sequential search, the impact on learning of approximate hypergraph search – as well as methods to mitigate any ill effects – has not been studied. Motivated by this, we develop online learning algorithms for inexact hypergraph search by generalizing the violation-fixing percepron of Huang et al. (2012). We empirically validate the benefit of this approach within the cube-pruning dependency parser of Zhang and McDonald (2012). Online learning algorithms like the perceptron are widely used for structured prediction tasks. For sequential search problems, like left-to-right tagging and parsing, beam search has been successfully combined with perceptron variants that accommodate search errors (Collins and Roark, 2004; Huang et al., 2012). However, perceptron training with inexact search is less studied for bottom-up parsing and, more generally, inference over hypergraphs. In this paper, we gener"
D13-1093,P08-1067,1,0.864538,"prediction problems generally deal with exponentially many outputs, often making exact search infeasible. For sequential search problems, such as tagging and incremental parsing, beam search coupled with perceptron algorithms that account for potential search errors have been shown to be a powerful combination (Collins and Roark, 2004; Daum´e and Marcu, 2005; Zhang and Clark, 2008; Huang et al., 2012). However, sequential search algorithms, and in particular left-to-right beam search (Collins and Roark, 2004; Zhang and Clark, 2008), squeeze inference into a very narrow space. To address this, Huang (2008) formulated constituency parsing as approximate bottom-up inference in order to compactly represent an exponential number of outputs while scoring features of arbitrary scope. This idea was adapted to graph-based Structured Perceptron for Inexact Hypergraph Search The structured perceptron algorithm (Collins, 2002) is a general learning algorithm. Given training instances (x, yˆ), the algorithm first solves the decoding problem y ′ = argmaxy∈Y(x) w · f (x, y) given the weight vector w for the high-dimensional feature representation f of the mapping (x, y), where y ′ is the prediction under the"
D13-1093,P10-1001,0,0.158246,"Missing"
D13-1093,C12-2077,0,0.171635,"Missing"
D13-1093,D10-1004,0,0.150365,"Missing"
D13-1093,P13-2109,0,0.225319,"ported by the fourth-order unlabeled dependency parser of Ma and Zhao (2012), although we did not utilize fourth-order features. The LAS score on Penn-YM is on par with the best reported by Bohnet and Kuhn (2012). On Penn-S, there are not many existing results to compare with, due to the tradition of reporting results on Penn-YM in the past. Nevertheless, our result is higher than the second best by a large margin. Our Chinese parsing scores are the highest reported results. 1 http://stp.lingfil.uu.se//∼nivre/research/Penn2Malt.html The data was prepared by Andr´e F. T. Martins as was done in Martins et al. (2013). 2 Parser Zhang and Nivre (2011) Zhang and Nivre (reimpl.) (beam=64) Zhang and Nivre (reimpl.) (beam=128) Koo and Collins (2010) Zhang and McDonald (2012) Rush and Petrov (2012) Martins et al. (2013) Qian and Liu (2013) Bohnet and Kuhn (2012) Ma and Zhao (2012) cube-pruning w/ skip w/ s-max w/ p-max UAS 92.993.00 92.94 93.04 93.06 93.07 93.17 93.39 93.493.21 93.50 93.44 Penn-YM LAS Toks/Sec † 680 91.891.98 800 91.91 400 91.86 220 740 180 † 120 92.38 92.07 300 92.41 300 92.33 300 UAS 92.96 93.11 92.792.82 92.92 93.59 93.64 Penn-S LAS Toks/Sec 90.74 500 90.84 250 4460 600 90.35 200 91.17 200 91"
D13-1093,Q13-1004,0,0.0970471,"Missing"
D13-1093,N12-1054,0,0.429959,"Missing"
D13-1093,D08-1059,0,0.0931437,"Missing"
D13-1093,D12-1030,1,0.913328,"the hypergraph (Chiang, 2007). Unfortunately, as the scope of features at each node increases, the inexactness of search and its negative impact on learning can potentially be exacerbated. Unlike sequential search, the impact on learning of approximate hypergraph search – as well as methods to mitigate any ill effects – has not been studied. Motivated by this, we develop online learning algorithms for inexact hypergraph search by generalizing the violation-fixing percepron of Huang et al. (2012). We empirically validate the benefit of this approach within the cube-pruning dependency parser of Zhang and McDonald (2012). Online learning algorithms like the perceptron are widely used for structured prediction tasks. For sequential search problems, like left-to-right tagging and parsing, beam search has been successfully combined with perceptron variants that accommodate search errors (Collins and Roark, 2004; Huang et al., 2012). However, perceptron training with inexact search is less studied for bottom-up parsing and, more generally, inference over hypergraphs. In this paper, we generalize the violation-fixing perceptron of Huang et al. (2012) to hypergraphs and apply it to the cube-pruning parser of Zhang"
D13-1093,P11-2033,0,0.103421,"eled dependency parser of Ma and Zhao (2012), although we did not utilize fourth-order features. The LAS score on Penn-YM is on par with the best reported by Bohnet and Kuhn (2012). On Penn-S, there are not many existing results to compare with, due to the tradition of reporting results on Penn-YM in the past. Nevertheless, our result is higher than the second best by a large margin. Our Chinese parsing scores are the highest reported results. 1 http://stp.lingfil.uu.se//∼nivre/research/Penn2Malt.html The data was prepared by Andr´e F. T. Martins as was done in Martins et al. (2013). 2 Parser Zhang and Nivre (2011) Zhang and Nivre (reimpl.) (beam=64) Zhang and Nivre (reimpl.) (beam=128) Koo and Collins (2010) Zhang and McDonald (2012) Rush and Petrov (2012) Martins et al. (2013) Qian and Liu (2013) Bohnet and Kuhn (2012) Ma and Zhao (2012) cube-pruning w/ skip w/ s-max w/ p-max UAS 92.993.00 92.94 93.04 93.06 93.07 93.17 93.39 93.493.21 93.50 93.44 Penn-YM LAS Toks/Sec † 680 91.891.98 800 91.91 400 91.86 220 740 180 † 120 92.38 92.07 300 92.41 300 92.33 300 UAS 92.96 93.11 92.792.82 92.92 93.59 93.64 Penn-S LAS Toks/Sec 90.74 500 90.84 250 4460 600 90.35 200 91.17 200 91.28 200 UAS 86.085.93 86.05 86.87"
D13-1093,D07-1096,1,\N,Missing
J09-4009,P05-1033,0,0.869126,"mars and present a linear-time algorithm for binarizing synchronous rules when possible. In our large-scale experiments, we found that almost all rules are binarizable and the resulting binarized rule set significantly improves the speed and accuracy of a state-of-the-art syntaxbased machine translation system. We also discuss the more general, and computationally more difficult, problem of finding good parsing strategies for non-binarizable rules, and present an approximate polynomial-time algorithm for this problem. 1. Introduction Several recent syntax-based models for machine translation (Chiang 2005; Galley et al. 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right-hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization as a technique to factor each synchronous grammar rule into a series of binary rules. Although monolingual context-free grammars (CFGs) can always be binarized, this is not the case ∗ Informatio"
J09-4009,P03-2041,0,0.106836,"reorderings of nonterminals are denoted by variables xi . In the treetransducer formalism of Rounds (1970), the right-hand (target) side subtree can have multiple levels, as in the first rule above. This system can model non-isomorphic transformations on English parse trees to “fit” another language, learning, for example, that the (V S O) structure in Arabic should be transformed into a (S (V O)) structure in English, by looking at two-level tree fragments (Knight and Graehl 2005). From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner 2003; Shieber 2004) (see Figure 3). This larger locality captures more linguistic phenomena and leads to better parameter estimation. By creating a 563 Computational Linguistics Volume 35, Number 4 Figure 3 Two equivalent representations of the first rule in Example (5): (a) tree transducer; (b) Synchronous Tree Subsitution Grammar (STSG). The ↓ arrows denote substitution sites, which correspond to variables in tree transducers. nonterminal for each right-hand-side tree, we can convert the transducer representation to an SCFG with the same generative capacity. We can again create a projected CFG w"
J09-4009,N04-1035,1,0.843738,"ent a linear-time algorithm for binarizing synchronous rules when possible. In our large-scale experiments, we found that almost all rules are binarizable and the resulting binarized rule set significantly improves the speed and accuracy of a state-of-the-art syntaxbased machine translation system. We also discuss the more general, and computationally more difficult, problem of finding good parsing strategies for non-binarizable rules, and present an approximate polynomial-time algorithm for this problem. 1. Introduction Several recent syntax-based models for machine translation (Chiang 2005; Galley et al. 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right-hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization as a technique to factor each synchronous grammar rule into a series of binary rules. Although monolingual context-free grammars (CFGs) can always be binarized, this is not the case ∗ Information Science Institute,"
J09-4009,N07-1019,1,0.850542,"Missing"
J09-4009,W05-1506,1,0.824427,"ecting the constraints from the other side. This scheme generalizes to the case where we have n nonterminals in a SCFG rule, and the decoder conservatively assumes nothing can be done on language model scoring (because target-language spans are non-contiguous in general) until the real nonterminal has been recognized. In other words, target-language boundary words 2 An alternative to integrated decoding is rescoring, where one first computes the k-best translations according to the TM only, and then reranks the k-best list with the language model costs. This method runs very fast in practice (Huang and Chiang 2005), but often produces a considerable number of search errors because the true best translation is often outside of the k-best list, especially for longer sentences. 562 Huang et al. Binarization of Synchronous Context-Free Grammars from each child nonterminal of the rule will be cached in all virtual nonterminals derived from this rule. In the case of m-gram integrated decoding, we have to maintain 2(m − 1) boundary words for each child nonterminal, which leads to a prohibitive overall complexity of O(|w|3+2n(m−1) ), which is exponential in rule size (Huang, Zhang, and Gildea 2005). Aggressive"
J09-4009,W05-1507,1,0.919163,"Missing"
J09-4009,P05-1057,0,0.0471912,"Missing"
J09-4009,N03-1021,0,0.294945,"real examples of non-binarizable cases verified by native speakers. In the final, theoretical, sections of this article, we investigate the general problem of finding the most efficient synchronous parsing or decoding strategy for arbitrary synchronous context-free grammar (SCFG) rules, including non-binarizable cases. Although this problem is believed to be NP-complete, we prove two results that substantially reduce the search space over strategies. We also present an optimal algorithm that runs tractably in practice and a polynomial-time algorithm that is a good approximation of the former. Melamed (2003) discusses binarization of multi-text grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rule"
J09-4009,W03-0301,0,0.0464804,"Missing"
J09-4009,J03-1006,0,0.1198,"Missing"
J09-4009,J04-4002,0,0.117917,"e 13 Comparing the two binarization methods in terms of translation quality against search effort. Table 2 Machine translation results for syntax-based systems vs. the phrase-based Alignment Template System. System BLEU monolingual binarization synchronous binarization alignment-template system 36.25 38.44 37.00 decoding is used as a measure of the size of search space, or time efficiency. Our system is consistently faster and more accurate than the baseline system. We also compare the top result of our synchronous binarization system with the state-of-the-art alignment-template system (ATS) (Och and Ney 2004). The results are shown in Table 2. Our system has a promising improvement over the ATS system, which is trained on a larger data set but tuned independently. A larger-scale system based on our best result performs very well in the 2006 NIST MT Evaluation (ISI Machine Translation Team 2006), achieving the best overall BLEU scores in the Chineseto-English track among all participants.4 The readers are referred to Galley et al. (2004) for details of the decoder and the overall system. 6. One-Sided Binarization In this section and the following section, we discuss techniques for handling rules th"
J09-4009,C69-0101,0,0.65771,"th source- and target-sides, so that we can generate a binary-branching SCFG: (4) S PP-VP → → NP 1 PP-VP 2 , VP 1 PP 2 , NP 1 PP-VP 2 PP 2 VP 1 In this case m-gram integrated decoding can be done in O(|w|3+4(m−1) ) time, which is a much lower-order polynomial and no longer depends on rule size (Wu 1996), allowing the search to be much faster and more accurate, as is evidenced in the Hiero system of Chiang (2005), which restricts the hierarchical phrases to form binary-branching SCFG rules. Some recent syntax-based MT systems (Galley et al. 2004) have adopted the formalism of tree transducers (Rounds 1970), modeling translation as a set of rules for a transducer that takes a syntax tree in one language as input and transforms it into a tree (or string) in the other language. The same decoding algorithms are used for machine translation in this formalism, and the following example shows that the same issues of binarization arise. Suppose we have the following transducer rules: (5) S(x1 :NP x2 :PP x3 :VP) NP( / B`aow¯eier) VP( / jux´ ˇ ıng le hu`ıt´an) PP( / yuˇ Sh¯al´ong)  >L     → → → → S(x1 VP(x3 x2 )) NP(NNP(Powell)) VP(VBD(held) NP(DT(a) NPS(meeting))) PP(TO(with) NP(NNP(Sharon))) w"
J09-4009,H05-1101,0,0.806039,"binarizing a tree-transducer rule, and consider only the alignment (or permutation) of the nonterminal variables. Again, rightmost binarization is preferable for the first rule. In SCFG-based frameworks, the problem of finding a word-level alignment between two sentences is an instance of the synchronous parsing problem: Given two strings and a synchronous grammar, find a parse tree that generates both input strings. The benefit of binary grammars also applies in this case. Wu (1997) shows that parsing a binary-branching SCFG is in O(|w|6 ), while parsing SCFG with arbitrary rules is NP-hard (Satta and Peserico 2005). For example, in Figure 2, the complexity of synchronous parsing for the original grammar (a) is O(|w|8 ), because we have to maintain four indices on either side, giving a total of eight; parsing the monolingually binarized grammar (b) involves seven indices, three on the Chinese side and four on the English side. In contrast, the synchronously binarized version (c) requires only 3 + 3 = 6 indices, which can be thought of as “CKY in two dimensions.” An efficient alignment algorithm is guaranteed if a binarization is found, and the same binarization can be used for decoding and alignment. We"
J09-4009,W04-3312,0,0.0458485,"of nonterminals are denoted by variables xi . In the treetransducer formalism of Rounds (1970), the right-hand (target) side subtree can have multiple levels, as in the first rule above. This system can model non-isomorphic transformations on English parse trees to “fit” another language, learning, for example, that the (V S O) structure in Arabic should be transformed into a (S (V O)) structure in English, by looking at two-level tree fragments (Knight and Graehl 2005). From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner 2003; Shieber 2004) (see Figure 3). This larger locality captures more linguistic phenomena and leads to better parameter estimation. By creating a 563 Computational Linguistics Volume 35, Number 4 Figure 3 Two equivalent representations of the first rule in Example (5): (a) tree transducer; (b) Synchronous Tree Subsitution Grammar (STSG). The ↓ arrows denote substitution sites, which correspond to variables in tree transducers. nonterminal for each right-hand-side tree, we can convert the transducer representation to an SCFG with the same generative capacity. We can again create a projected CFG which will be ex"
J09-4009,C90-3045,0,0.379979,"arizable SCFGs, and are mainly of theoretical interest. Algorithms 1–3 make fewer and fewer assumptions on the strategy space, and produce parsing strategies closer and closer to the optimal. Algorithm 4 further improves Algorithm 3. Section Algorithm Assumptions of Strategy Space Complexity 3–4 6 Alg. 1 (synchronous) Alg. 2 (one-sided, CKY) Alg. 3 (optimal) ⇒ Alg. 4 (best-first) Contiguous on both sides Contiguous on one side O(n) O(n3 ) O(3n ) O(9k n2k ) 7.2 No assumptions systems improve. Synchronous grammars that go beyond the power of SCFG (and therefore binary SCFG) have been defined by Shieber and Schabes (1990) and Rambow and Satta (1999), and motivated for machine translation by Melamed (2003), although previous work has not given algorithms for finding efficient and optimal parsing strategies for general SCFGs, which we believe is an important problem. In the remainder of this section and the next section, we will present a series of algorithms that produce increasingly faster parsing strategies, by gradually relaxing the strong “continuity” constraint made by the synchronous binarization technique. As that technique requires continuity on both languages, we will first study a relaxation where bin"
J09-4009,P06-1123,0,0.0494632,"Missing"
J09-4009,P96-1021,0,0.347375,"NP and PP into an intermediate state which contains a gap on the English side. (c) This scheme groups PP and VP into an intermediate state which is contiguous on both sides. These two binarizations are no different in the translation-model-only decoding described previously, just as in monolingual parsing. However, in the source-channel approach to machine translation, we need to combine probabilities from the translation model (TM) (an SCFG) with the language model (an n-gram), which has been shown to be very important for translation quality (Chiang 2005). To do bigram-integrated decoding (Wu 1996), we need to augment each chart item (X, i, j) with twoÃ target! u ··· v language boundary words u and v to produce a bigram-item which we denote i X j .2 Now the two binarizations have very different effects. In the first case, we first combine NP with PP. This step is written as follows in the weighted deduction notation of Nederhof (2003): ¶ µ ¶ µ Powell ··· Powell with ··· Sharon :q :p NP PP 2 4 2 µ1 ¶ Powell ··· Powell ··· with ··· Sharon : pq NP-PP 1 4 where p and q are the scores of antecedent items. This situation is unpleasant because in the target language NP and PP are not contiguou"
J09-4009,J97-3002,0,0.790505,"e cases. Although this problem is believed to be NP-complete, we prove two results that substantially reduce the search space over strategies. We also present an optimal algorithm that runs tractably in practice and a polynomial-time algorithm that is a good approximation of the former. Melamed (2003) discusses binarization of multi-text grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rules that apply to the parses. This approach results in rules with many nonterminals, making good binarization techniques critical. We explain how synchronous rule binarization interacts with n-gram language models and affects decoding for machine translation in Section 2. We define binarization formally in"
J09-4009,W07-0404,1,0.868562,"g one nonterminal at a time. The optimal grouping of nonterminals is shown on the right. time O(|w|10 ) by adding one nonterminal at a time. All permutations of less than eight elements can be optimally parsed by adding one element at a time. 7.4 Discontinuous Parsing Is Necessary Only for Non-Decomposable Permutations In this subsection, we show that an optimal parsing strategy can be found by first factoring an SCFG rule into a sequence of shorter SCFG rules, if possible, and then considering each of the new rules independently. The first step can be done efficiently using the algorithms of Zhang and Gildea (2007). The second step can be done in time O(9kc · n2kc ) using Algorithm 4, where kc is the complexity of the longest SCFG rule after factorizations, implying that kc ≤ (n + 4). We show that this two-step process is optimal, by proving that the optimal parsing strategy for the initial rule will not need to build subsets of children that cross the boundaries of the factorization into shorter SCFG rules. Figure 19 shows a permutation that contains permutations of fewer numbers within itself so that the entire permutation can be decomposed hierarchically. We prove that if there is a contiguous block"
J09-4009,N06-1033,1,0.518184,"present a decoding strategy for these rules in Section 6. Section 7 gives a solution to the general theoretical problem of finding optimal decoding and synchronous parsing strategies for arbitrary SCFGs, and presents complexity results on the nonbinarizable rules from our Chinese–English data. These final two sections are of primarily theoretical interest, as nonbinarizable rules have not been shown to benefit real-world machine translation systems. However, the algorithms presented may become relevant as machine translation systems improve. 1 A preliminary version of Section 1–5 appeared in Zhang et al. (2006). 560 Huang et al. Binarization of Synchronous Context-Free Grammars 2. Motivation Consider the following Chinese sentence and its English translation: (1)    >L  B`aow¯eier yuˇ Sh¯al´ong jux´ ˇ ıng le Powell with Sharon hold [past.] “Powell held a meeting with Sharon”  hu`ıt´an meeting Suppose we have the following SCFG, where superscripts indicate reorderings (formal definitions of SCFGs with a more flexible notation can be found in Section 3): (2) S NP VP PP → → → → NP 1 PP 2 VP 3 , / B`aow¯eier, / jux´ ˇ ıng le hu`ıt´an, / yuˇ Sh¯al´ong,  >L     NP 1 VP 3 PP 2 Powell held"
J09-4009,W07-0405,1,\N,Missing
J09-4009,P06-1121,1,\N,Missing
J09-4009,W90-0102,0,\N,Missing
J19-2004,D16-1162,0,0.0776354,"Missing"
J19-2004,P12-3006,0,0.0949107,"ow the input maps to the output verbalization, and in part on the kind of entity denoted by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. For example, how on"
J19-2004,P10-1079,0,0.0565589,"categories finer-grained classifications depend in part on how the input maps to the output verbalization, and in part on the kind of entity denoted by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normali"
J19-2004,P18-1008,0,0.0293253,"ls pass through digits. # Then try digits @ o, and if that fails pass through digits. # Then try digits @ cardinals, which shall surely work. # Oh, and then make it a disjunction with thousand to allow both # &quot;twenty ten&quot; and &quot;two thousand ten&quot; readings. export YEAR = Optimize[ LenientlyCompose[ LenientlyCompose[ LenientlyCompose[ LenientlyCompose[digits, hundreds, sigstar], pairwise, sigstar], o, sigstar], cardinal, sigstar] | thousand]; A.2 Transformer Model Details We utilize a Transformer sequence-to-sequence model (Vaswani et al. 2017), using the architecture described in Appendix A.2 of Chen et al. (2018), with: • 6 Transformer layers for both the encoder and the decoder, • 8 attention heads, • a model dimension of 512, and • a hidden dimension of 2,048. 333 Computational Linguistics Volume 45, Number 2 Table A.1 Default parameters for the sliding window model. Input embedding size Output embedding size Number of encoder layers Number of decoder layers Number of encoder units Number of decoder units Attention mechanism size 256 512 1 1 256 512 256 Dropout probabilities are uniformly set to 0.1. We use a dictionary of 32k word pieces (Schuster and Nakajima 2012) covering both input and output v"
J19-2004,P14-2111,0,0.484934,"dern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. For example, how one pronounces number sequences is generally of little or no concern in the normalization of social media text,"
J19-2004,P13-1155,0,0.070932,"Missing"
J19-2004,C08-1056,0,0.0378842,"Missing"
J19-2004,P12-1109,0,0.0772642,"Missing"
J19-2004,P11-2013,0,0.0562114,"ned classifications depend in part on how the input maps to the output verbalization, and in part on the kind of entity denoted by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at s"
J19-2004,P12-1055,0,0.0199831,"n, and in part on the kind of entity denoted by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. For example, how one pronounces number sequences is generally o"
J19-2004,D16-1096,0,0.0278276,"m the GRU← layer. 5.5 Incorporating Reconstruction Loss We observe that unrecoverable errors usually involve linguistically coherent output, but simply fail to correspond to the input. In the terminology used in machine translation, 311 Computational Linguistics Volume 45, Number 2 one might say that they favor fluency over adequacy. The same pattern has been identified in neural machine translation (Arthur, Neubig, and Nakamura 2016), which motivates a branch of research that can be summarized as enforcing the attention-based decoder to pay more “attention” to the input. Tu et al. (2016) and Mi et al. (2016) argue that the root problem lies in the attention mechanism itself. Unlike traditional phrase-based machine translation, there is no guarantee that the entire input can be “covered” at the end of decoding, and thus they strengthen the attention mechanism to approximate a notion of input coverage. Tu et al. (2017) suggest that the fix can also be made in the decoder RNN. The key insight here is that the hidden states in the decoder RNN should keep memory of the correspondence with the input. In addition to the standard translation loss, there should also be a reconstruction loss, which is the"
J19-2004,W15-4317,0,0.613959,"e, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. For example, how one pronounces number sequences is generally of little or no concern in the normalization of social media text, though it is essent"
J19-2004,I11-1109,0,0.0148426,"ns depend in part on how the input maps to the output verbalization, and in part on the kind of entity denoted by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. Fo"
J19-2004,P14-2060,1,0.776607,"o occur naturally. Machine translation can to a large extent rely on “found” data because people translate texts for practical reasons, such as providing access to documents to people not able to read the source language. In contrast, there is no reason why people would spend resources producing verbalized equivalents of ordinary written text: in most cases, English speakers 3 As a consequence, much of the subsequent work on applying machine learning to text normalization for speech applications focuses on specific semiotic classes, like letter sequences (Sproat and Hall 2014), abbreviations (Roark and Sproat 2014), or cardinal numbers (Gorman and Sproat 2016). 4 In fact, Kestrel (Ebden and Sproat 2014) uses a machine-learned morphosyntactic tagger for Russian. 298 Zhang et al. Neural Models of Text Normalization do not need a gloss to know how to read $10 million. Thus, if one wants to train neural models to verbalize written text, one must produce the data.5 Second, the bar for success in this domain seems to be higher than it is in other domains in that users expect TTS systems to correctly read numbers, dates, times, currency amounts, and so on. As we show subsequently, deep learning models produce"
J19-2004,P12-3011,1,0.93381,"arate words. Thus the Kestrel grammars recognize Jan.1, 2012 as a date and parse it as a single token, identifying the month, day, and year, and represent it internally using a protocol-buffer representation like the following:6 date { month: &quot;January&quot; day: &quot;1&quot; year: &quot;2012&quot;} Verbalization grammars then convert from a serialization of the protocol buffer representation into actual word sequences, such as January the first twenty twelve. Tokenization/classification and verbalization grammars are compiled into weighted finite-state transducers (WFSTs) using the Thrax grammar development library (Roark et al. 2012). One advantage of separating tokenization/classification from verbalization via the intermediate protocol buffer representation is that it allows for reordering of elements, something that is challenging with WFSTs.7 The need for reordering arises, for example, in the treatment of currency expressions where currency symbols such as ‘$’ or ‘’ often occur before digits, but are verbalized after the corresponding digits. An input $30 might be parsed as something like money { currency: &quot;USD&quot; amount { integer: &quot;30&quot; } } 6 https://developers.google.com/protocol-buffers/. 7 As we will show subsequent"
J19-2004,P16-1162,0,0.148348,"Missing"
J19-2004,N10-1023,0,0.0238932,"a, and in that we define more precisely how the covering grammars are actually used during decoding. Finally, we report results on new data sets. Arik et al. (2017) present a neural network TTS system that mimics the traditional separation into linguistic analysis (or front-end) and synthesis (or back-end) modules. It is unclear to what degree this system in fact performs text normalization since the only front-end component they describe is grapheme-to-phoneme conversion, which is a separate process from text normalization and usually performed later in the pipeline. Some prior work, such as Shugrina (2010), focuses on the inverse problem of denormalizing spoken sequences into written text in the context of ASR so that two hundred fifty would get converted to 250, or three thirty as a time would get formatted as 3:30. Pusateri et al. (2017) describe a system in which denormalization is treated as a neural network sequence labeling problem using a rich tag set. 8 See http://github.com/google/sparrowhawk. 301 Computational Linguistics Volume 45, Number 2 The data we report on in this article was recently released and was the subject of a Kaggle competition (see later in this article), and a few re"
J19-2004,P16-1008,0,0.0268692,"ame word position from the GRU← layer. 5.5 Incorporating Reconstruction Loss We observe that unrecoverable errors usually involve linguistically coherent output, but simply fail to correspond to the input. In the terminology used in machine translation, 311 Computational Linguistics Volume 45, Number 2 one might say that they favor fluency over adequacy. The same pattern has been identified in neural machine translation (Arthur, Neubig, and Nakamura 2016), which motivates a branch of research that can be summarized as enforcing the attention-based decoder to pay more “attention” to the input. Tu et al. (2016) and Mi et al. (2016) argue that the root problem lies in the attention mechanism itself. Unlike traditional phrase-based machine translation, there is no guarantee that the entire input can be “covered” at the end of decoding, and thus they strengthen the attention mechanism to approximate a notion of input coverage. Tu et al. (2017) suggest that the fix can also be made in the decoder RNN. The key insight here is that the hidden states in the decoder RNN should keep memory of the correspondence with the input. In addition to the standard translation loss, there should also be a reconstructio"
J19-2004,P06-1125,0,0.033252,"Missing"
J19-2004,D13-1007,0,0.0413732,"ed by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. For example, how one pronounces number sequences is generally of little or no concern in the normalization of soci"
J19-2004,N18-1122,0,0.0317744,"able errors, and for these we have argued that using trainable finite-state covering grammars is a reasonable approach, but we continue to look for ways to improve covering grammar training and coverage. 18 At the same time, we are currently exploring whether other neural approaches can help mitigate against unrecoverable errors. One approach that seems plausible is generative adversarial networks (Goodfellow et al. 2014; Goodfellow 2016), which have achieved impressive results in vision-related tasks but which have been also applied to NLP tasks including machine translation (Wu et al. 2017; Yang et al. 2018). Given the great success of deep learning for many problems, it is tempting to simply accrete speech and language tasks to a general class of problems and to worry less about the underlying problem being solved. For example, at a certain level of abstraction, all of text-to-speech synthesis can be thought of as a sequence-to-sequence problem where the input sequence is a string of characters and the output sequence is some representation of a waveform. “End-to-end” TTS models such as Char2Wav (Sotelo et al. 2017) treat the problem in this way, with no attempt to consider the many subproblems"
J19-2004,D14-1179,0,\N,Missing
J19-2004,Q16-1036,1,\N,Missing
N06-1033,P03-2041,0,0.286386,"juxing le huitan PP(TO(with), NP(NNP(Sharon))) → yu Shalong where the reorderings of nonterminals are denoted by variables xi . Notice that the first rule has a multi-level lefthand side subtree. This system can model nonisomorphic transformations on English parse trees to “fit” another language, for example, learning that 258 the (S (V O)) structure in English should be transformed into a (V S O) structure in Arabic, by looking at two-level tree fragments (Knight and Graehl, 2005). From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner, 2003). This larger locality is linguistically motivated and leads to a better parameter estimation. By imagining the left-hand-side trees as special nonterminals, we can virtually create an SCFG with the same generative capacity. The technical details will be explained in Section 3.2. In general, if we are given an arbitrary synchronous rule with many nonterminals, what are the good decompositions that lead to a binary grammar? Figure 2 suggests that a binarization is good if every virtual nonterminal has contiguous spans on both sides. We formalize this idea in the next section. 2 Synchronous Bina"
N06-1033,N04-1035,1,0.655736,"re-orderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system. 1 • We examine the effect of this binarization method on end-to-end machine translation quality, compared to a more typical baseline method. • We examine cases of non-binarizable rules in a large, empirically-derived rule set, and we investigate the effect on translation quality when excluding such rules. Introduction Several recent syntax-based models for machine translation (Chiang, 2005; Galley et al., 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization to factor the synchronous grammar to a smaller branching factor, although it is not guaranteed to be successful for any synchronous rule with arbitrary permutation. In particular: Melamed (2003) discusses binarizati"
N06-1033,W05-1507,1,0.837659,"e n nonterminals in a SCFG rule, and the decoder conservatively assumes nothing can be done on language model scoring (because target-language spans are non-contiguous in general) until the real nonterminal has been recognized. In other words, targetlanguage boundary words from each child nonterminal of the rule will be cached in all virtual nonterminals derived from this rule. In the case of m-gram integrated decoding, we have to maintain 2(m − 1) boundary words for each child nonterminal, which leads to a prohibitive overall complexity of O(|w|3+2n(m−1) ), which is exponential in rule size (Huang et al., 2005). Aggressive pruning must be used to make it tractable in practice, which in general introduces many search errors and adversely affects translation quality. In the second case, however:  with ··· Sharon 2  PP held 2 4 ··· VPP-VP  :r Sharon 7   held ··· meeting 4 VP 7  :s : rs · Pr(with |meeting) Here since PP and VP are contiguous (but swapped) in the target-language, we can include the source (Chinese) NP NP PP VP VP PP target (English) English boundary words VPP-VP VPP-VP Sharon PP with meeting held Powell Powell VP NP 1 2 4 7 Chinese indices Figure 2: The alignment pattern (left) and"
N06-1033,N03-1021,0,0.0662649,"(Chiang, 2005; Galley et al., 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization to factor the synchronous grammar to a smaller branching factor, although it is not guaranteed to be successful for any synchronous rule with arbitrary permutation. In particular: Melamed (2003) discusses binarization of multitext grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in inversion-transduction grammar (ITG) (Wu, 1997) and the binary synchronous context-free grammar (SCFG) employed by the Hiero system (Chiang, 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language"
N06-1033,J04-4002,0,0.236624,"Missing"
N06-1033,H05-1101,0,0.681458,"VPP-VP → NP(1) VP(1) (2) , NP(1) VPP-VP VPP-VP (2) (2) PP , PP VP(1) (2) In this case m-gram integrated decoding can be done in O(|w|3+4(m−1) ) time which is much lowerorder polynomial and no longer depends on rule size (Wu, 1996), allowing the search to be much faster and more accurate facing pruning, as is evidenced in the Hiero system of Chiang (2005) where he restricts the hierarchical phrases to be a binary SCFG. The benefit of binary grammars also lies in synchronous parsing (alignment). Wu (1997) shows that parsing a binary SCFG is in O(|w|6 ) while parsing SCFG is NP-hard in general (Satta and Peserico, 2005). The same reasoning applies to tree transducer rules. Suppose we have the following tree-to-string rules, following Galley et al. (2004): (3) S(x0 :NP, VP(x2 :VP, x1 :PP)) → x0 x1 x2 NP(NNP(Powell)) → Baoweier VP(VBD(held), NP(DT(a) NPS(meeting))) → juxing le huitan PP(TO(with), NP(NNP(Sharon))) → yu Shalong where the reorderings of nonterminals are denoted by variables xi . Notice that the first rule has a multi-level lefthand side subtree. This system can model nonisomorphic transformations on English parse trees to “fit” another language, for example, learning that 258 the (S (V O)) struct"
N06-1033,C90-3045,0,0.251755,"Missing"
N06-1033,P96-1021,0,0.41387,"s production. language model score by adding Pr(with |meeting), and the resulting item again has two boundary words. Later we add Pr(held | Powell) whenthe Powell ··· Powell resulting item is combined with to NP 1 2 form an S item. As illustrated in Figure 2, VPP-VP has contiguous spans on both source and target sides, so that we can generate a binary-branching SCFG: (2) S→ VPP-VP → NP(1) VP(1) (2) , NP(1) VPP-VP VPP-VP (2) (2) PP , PP VP(1) (2) In this case m-gram integrated decoding can be done in O(|w|3+4(m−1) ) time which is much lowerorder polynomial and no longer depends on rule size (Wu, 1996), allowing the search to be much faster and more accurate facing pruning, as is evidenced in the Hiero system of Chiang (2005) where he restricts the hierarchical phrases to be a binary SCFG. The benefit of binary grammars also lies in synchronous parsing (alignment). Wu (1997) shows that parsing a binary SCFG is in O(|w|6 ) while parsing SCFG is NP-hard in general (Satta and Peserico, 2005). The same reasoning applies to tree transducer rules. Suppose we have the following tree-to-string rules, following Galley et al. (2004): (3) S(x0 :NP, VP(x2 :VP, x1 :PP)) → x0 x1 x2 NP(NNP(Powell)) → Baow"
N06-1033,J97-3002,0,0.851118,"s on the right hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization to factor the synchronous grammar to a smaller branching factor, although it is not guaranteed to be successful for any synchronous rule with arbitrary permutation. In particular: Melamed (2003) discusses binarization of multitext grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in inversion-transduction grammar (ITG) (Wu, 1997) and the binary synchronous context-free grammar (SCFG) employed by the Hiero system (Chiang, 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rules that apply to the parses. This approach results in rules with many nonterminals, making good binarization techniques critical. Suppose we have the following SCFG, where superscripts indicate reorderings (formal definitions of 256 Proceedings of the Human Language Tech"
N06-1033,W90-0102,0,\N,Missing
N06-1033,P05-1033,0,\N,Missing
P05-1059,J00-1004,0,0.0201849,"t sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences. 1 Introduction The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The algorithm builds a synchronous parse tree for both sentences, and assumes that the trees have the same underlying structure but that the ordering of constituents may differ in the two languages. This probabilistic, syntax-based approach has inspired much subsequent reasearch. Alshawi et al. (2000) use hierarchical finite-state transducers. In the tree-to-string model of Yamada and Knight (2001), a parse tree for one sentence of a translation pair is projected onto the other string. Melamed (2003) presents algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars. Despite being one of the earliest probabilistic syntax-based translation models, ITG remains stateof-the art. Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than th"
P05-1059,J93-2003,0,0.0169638,"Missing"
P05-1059,N03-1016,0,0.039107,"t strings to make the algorithm feasible. Our technique involves computing an estimate of how likely each of the n4 cells in the chart is before considering all ways of building the cell by combining smaller subcells. Our figure of merit for a cell involves an estimate of both the inside probability of the cell (how likely the words within the box in both dimensions are to align) and the outside probability (how likely the words outside the box in both dimensions are to align). In including an estimate of the outside probability, our technique is related to A* methods for monolingual parsing (Klein and Manning, 2003), although our estimate is not guaranteed to be lower than complete outside probabity assigned by ITG. Figure 3(a) displays the tic-tac-toe pattern for the inside and outside components of a particular cell. We use IBM Model 1 as our estimate of both the inside and 478 outside probabilities. In the Model 1 estimate of the outside probability, source and target words can align using any combination of points from the four outside corners of the tic-tac-toe pattern. Thus in Figure 3(a), there is one solid cell (corresponding to the Model 1 Viterbi alignment) in each column, falling either in the"
P05-1059,N03-1021,0,0.0585084,"rithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The algorithm builds a synchronous parse tree for both sentences, and assumes that the trees have the same underlying structure but that the ordering of constituents may differ in the two languages. This probabilistic, syntax-based approach has inspired much subsequent reasearch. Alshawi et al. (2000) use hierarchical finite-state transducers. In the tree-to-string model of Yamada and Knight (2001), a parse tree for one sentence of a translation pair is projected onto the other string. Melamed (2003) presents algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars. Despite being one of the earliest probabilistic syntax-based translation models, ITG remains stateof-the art. Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al. (1996). Zhang and Gildea (2004) found ITG to outperform the tree-to-string model for word-level alignment, as measured against human gold-standard al"
P05-1059,P00-1056,0,0.473219,"led the pruning techniques for the LITG with the beam ratio for the tic-tac-toe pruning as 10−5 and the number k for the top-k pruning as 25. We ran the experiments on sentences up to 25 words long in both languages. The resulting training corpus had 18,773 sentence pairs with a total of 276,113 Chinese words and 315,415 English words. We evaluate our translation models in terms of agreement with human-annotated word-level alignments between the sentence pairs. For scoring the Viterbi alignments of each system against goldstandard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words: AER = 1 − P (hY (∗)Zi |X(∗)) P (hY Z(∗)i |X(∗)) where ∗ stands for any lexical pair. For instance, P ([Y (e/f )Z] |X(e/f )) = (1 − λ)PEM ([Y (e/f )Z] |X(e/f )) + λP ([Y (∗)Z] |X(∗)) where λ = 1/(1 + Expected Counts(X(e/f ))) 480 |A ∩ GP |+ |A ∩ GS | |A |+ |GS | where A is the set of word pairs aligned by the automatic system, GS is the set marked in the gold standard as “sure”, and GP is the set marked as “possible” (including the “sure” pairs). In our Chinese-English data, only one type of alignment was marked, meaning that GP = GS ."
P05-1059,J97-3002,0,0.800639,"the two languages, whereas the rules with pointed brackets expand the left hand side symbol into the two right hand side symbols in reverse order in the two languages. One special case of ITG is the bracketing ITG that has only one nonterminal that instantiates exactly one straight rule and one inverted rule. The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar: it has a start symbol S, a single preterminal C, and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment, as discussed by Wu (1997) and Zens and Ney (2003). As an example, Figure 1 shows the alignment and the corresponding parse tree for the sentence pair Je les vois / I see them using the unambiguous bracketing ITG. A stochastic ITG can be thought of as a stochastic CFG extended to the space of bitext. The independence assumptions typifying S-CFGs are also valid for S-ITGs. Therefore, the probability of an S-ITG parse is calculated as the product of the probabilities of all the instances of rules in the parse tree. For instance, the probability of the parse in Figure 1 is: X → X(e/f ) The word pair e/f is representative"
P05-1059,P01-1067,0,0.0308662,"sentences. 1 Introduction The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The algorithm builds a synchronous parse tree for both sentences, and assumes that the trees have the same underlying structure but that the ordering of constituents may differ in the two languages. This probabilistic, syntax-based approach has inspired much subsequent reasearch. Alshawi et al. (2000) use hierarchical finite-state transducers. In the tree-to-string model of Yamada and Knight (2001), a parse tree for one sentence of a translation pair is projected onto the other string. Melamed (2003) presents algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars. Despite being one of the earliest probabilistic syntax-based translation models, ITG remains stateof-the art. Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al. (1996). Zhang and Gildea (2004) found ITG to"
P05-1059,P03-1019,0,0.487169,"his probabilistic, syntax-based approach has inspired much subsequent reasearch. Alshawi et al. (2000) use hierarchical finite-state transducers. In the tree-to-string model of Yamada and Knight (2001), a parse tree for one sentence of a translation pair is projected onto the other string. Melamed (2003) presents algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars. Despite being one of the earliest probabilistic syntax-based translation models, ITG remains stateof-the art. Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al. (1996). Zhang and Gildea (2004) found ITG to outperform the tree-to-string model for word-level alignment, as measured against human gold-standard alignments. One explanation for this result is that, while a tree representation is helpful for modeling translation, the trees assigned by the traditional monolingual parsers (and the treebanks on which they are trained) may not be optimal for translation of a specific language pair. ITG has the advantage of being"
P05-1059,C04-1060,1,0.854847,"ing model of Yamada and Knight (2001), a parse tree for one sentence of a translation pair is projected onto the other string. Melamed (2003) presents algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars. Despite being one of the earliest probabilistic syntax-based translation models, ITG remains stateof-the art. Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al. (1996). Zhang and Gildea (2004) found ITG to outperform the tree-to-string model for word-level alignment, as measured against human gold-standard alignments. One explanation for this result is that, while a tree representation is helpful for modeling translation, the trees assigned by the traditional monolingual parsers (and the treebanks on which they are trained) may not be optimal for translation of a specific language pair. ITG has the advantage of being entirely data-driven – the trees are derived from an expectation maximization procedure given only the original strings as input. In this paper, we extend ITG to condi"
P06-2036,P05-1033,0,0.0371941,"Science Dept. University of Rochester Rochester, NY 14627 Introduction Synchronous Context-Free Grammars (SCFGs) are a generalization of the Context-Free Grammar (CFG) formalism to simultaneously produce strings in two languages. SCFGs have a wide range of applications, including machine translation, word and phrase alignments, and automatic dictionary construction. Variations of SCFGs go back to Aho and Ullman (1972)’s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (2004). When viewed as a rewriting system, an SCFG generates a set of string pairs, representing some translation relation. We are concerned here with the time complexity of parsing such a pair, according to the grammar. Assume then a pair with each 279 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 279–286, c Sydney, July 2006. 2006 Association for Computat"
P06-2036,N04-1035,0,0.0453813,"in two languages. SCFGs have a wide range of applications, including machine translation, word and phrase alignments, and automatic dictionary construction. Variations of SCFGs go back to Aho and Ullman (1972)’s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (2004). When viewed as a rewriting system, an SCFG generates a set of string pairs, representing some translation relation. We are concerned here with the time complexity of parsing such a pair, according to the grammar. Assume then a pair with each 279 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 279–286, c Sydney, July 2006. 2006 Association for Computational Linguistics 1,2 1,2 2,1 2 1 7 3,1,4,2 1,2 3 7 rank of a CFG, in this paper we focus on each single synchronous rule and factorize it into synchronous rules of lower rank. If we view the bijective relation associat"
P06-2036,N03-1021,0,0.0211477,"t-Free Grammars (SCFGs) are a generalization of the Context-Free Grammar (CFG) formalism to simultaneously produce strings in two languages. SCFGs have a wide range of applications, including machine translation, word and phrase alignments, and automatic dictionary construction. Variations of SCFGs go back to Aho and Ullman (1972)’s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (2004). When viewed as a rewriting system, an SCFG generates a set of string pairs, representing some translation relation. We are concerned here with the time complexity of parsing such a pair, according to the grammar. Assume then a pair with each 279 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 279–286, c Sydney, July 2006. 2006 Association for Computational Linguistics 1,2 1,2 2,1 2 1 7 3,1,4,2 1,2 3 7 rank of a CFG, in this paper we focus"
P06-2036,H05-1101,1,0.929841,"Rochester Rochester, NY 14627 Giorgio Satta Dept. of Information Eng’g University of Padua I-35131 Padua, Italy string having a maximum length of N , and consider an SCFG G of size |G|, with a bound of n nonterminals in the right-hand side of each rule in a single dimension, which we call below the rank of G. As an upper bound, parsing can be carried out in time O(|G |N n+4 ) by a dynamic programming algorithm maintaining continuous spans in one dimension. As a lower bound, parsing strategies with discontinuous√spans in both dimensions can take time Ω(|G |N c n ) for unfriendly permutations (Satta and Peserico, 2005). A natural question to ask then is: What if we could reduce the rank of G, preserving the generated translation? As in the case of CFGs, one way of doing this would be to factorize each single rule into several rules of rank strictly smaller than n. It is not difficult to see that this would result in a new grammar of size at most 2 · |G|. In the time complexities reported above, we see that such a size increase would be more than compensated by the reduction in the degree of the polynomial in N . We thus conclude that a reduction in the rank of an SCFG would result in more efficient parsing"
P06-2036,J97-3002,0,0.221999,"problem about recognizing permutations that can be factored. 1 Hao Zhang Computer Science Dept. University of Rochester Rochester, NY 14627 Introduction Synchronous Context-Free Grammars (SCFGs) are a generalization of the Context-Free Grammar (CFG) formalism to simultaneously produce strings in two languages. SCFGs have a wide range of applications, including machine translation, word and phrase alignments, and automatic dictionary construction. Variations of SCFGs go back to Aho and Ullman (1972)’s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (2004). When viewed as a rewriting system, an SCFG generates a set of string pairs, representing some translation relation. We are concerned here with the time complexity of parsing such a pair, according to the grammar. Assume then a pair with each 279 Proceedings of the COLING/ACL 2006 Main Confere"
P06-2036,P01-1067,0,0.0784028,"multaneously produce strings in two languages. SCFGs have a wide range of applications, including machine translation, word and phrase alignments, and automatic dictionary construction. Variations of SCFGs go back to Aho and Ullman (1972)’s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al. (2004). When viewed as a rewriting system, an SCFG generates a set of string pairs, representing some translation relation. We are concerned here with the time complexity of parsing such a pair, according to the grammar. Assume then a pair with each 279 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 279–286, c Sydney, July 2006. 2006 Association for Computational Linguistics 1,2 1,2 2,1 2 1 7 3,1,4,2 1,2 3 7 rank of a CFG, in this paper we focus on each single synchronous rule and factorize it into synchronous rules of lower rank. If we view the bi"
P06-2036,N06-1033,1,0.761511,"torizing a permutation of arity n into the composition of several permutations of arity k < n. Such factorization can be represented as a tree of composed permutations, called in what follows a permutation tree. A permutation tree can be converted into a set of k-ary SCFG rules equivalent to the input rule. For example, the input rule: 4,1,3,5,2 5 8 6 1 4 2,4,1,3 6 3 8 2 5 4 Figure 1: Two permutation trees. The permutations associated with the leaves can be produced by composing the permutations at the internal nodes. spans in one dimension. Previous work on this problem has been presented in Zhang et al. (2006), where a method is provided for casting an SCFG to a form with rank k = 2. If generalized to any value of k, that algorithm would run in time O(n2 ). We thus improve existing factorization methods by almost a factor of n. We also solve an open problem mentioned by Albert et al. (2003), who pose the question of whether irreducible, or simple, permutations can be recognized in time less than Θ(n2 ). [ X → A(1) B (2) C (3) D(4) E (5) F (6) G(7) H (8) , X → B (2) A(1) C (3) D(4) G(7) E (5) H (8) F (6) ] yields the permutation tree of Figure 1(left). Introducing a new grammar nonterminal Xi for ea"
P06-2122,P04-1060,0,0.0538395,"Missing"
P06-2122,P05-1057,0,0.0260683,"ults on English side of the test data set. The dependency results on Chinese are similar. The gold standard dependencies were extracted from Collins’ parser output on the sentences. The LITG and BLITG dependencies were extracted from the Viterbi synchronous trees by following the head words. For comparison, we also included two base-line results. ITG-lh is unlexicalized ITG with left-head assumption, meaning the head words always come from the left branches. ITG-rh is ITG with righthead assumption. To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al. (2005). We used 165 sentence pairs that are up to 25 words in length on both sides. 5 Discussion The BLITG model has two components, namely the dependency model on the upper levels of the tree structure and the word-level translation model at the bottom. We hope that the two components will mutually improve one another. The current experiments indicate clearly that the word level alignment does help inducing dependency structures on both sides. The precision and recall on the dependency retrieval sub-task are almost doubled for both languages from LITG which only has a kind of uni-lexical dependency"
P06-2122,N03-1021,0,0.26991,"ficulty in statistical machine translation is the trade-off between representational power and computational complexity. Real-world corpora for language pairs such as Chinese-English have complex reordering relationships that are not captured by current phrase-based MT systems, despite their state-of-the-art performance measured in competitive evaluations. Synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the context-free property in each language have been proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, each piece of lexical information consid"
P06-2122,P00-1056,0,0.0593994,"t comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bilexical ITG (BLITG), on a hand-aligned bilingual corpus. All the models were trained using the same amount of data. We ran the experiments on sentences up to 25 words long in both languages. The resulting training corpus had 18,773 sentence pairs with a total of 276,113 Chinese words and 315,415 English words. For scoring the Viterbi alignments of each system against gold-standard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words: The two levels of distributions are interpolated using a technique inspired by Witten-Bell smoothing (Chen and Goodman, 1996). We use the expected count of the left hand side lexical nonterminal to adjust the weight for the EM-trained bilexical probability. For example, P ([Y (e) Z(e0 )] |X(e)) = (1 − λ)PEM ([Y (e) Z(e0 )] |X(e)) + λP ([Y (∗) Z(e0 )] |X(∗)) where λ = 1/(1 + Expected Counts(X(e))) 4 Experiments First of all, we are interested in finding out how much speedup can be achieved by doing the hook trick for EM. We implemented"
P06-2122,J97-3002,0,0.752045,"available. Introduction A major difficulty in statistical machine translation is the trade-off between representational power and computational complexity. Real-world corpora for language pairs such as Chinese-English have complex reordering relationships that are not captured by current phrase-based MT systems, despite their state-of-the-art performance measured in competitive evaluations. Synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the context-free property in each language have been proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However,"
P06-2122,P01-1067,0,0.0566379,"Introduction A major difficulty in statistical machine translation is the trade-off between representational power and computational complexity. Real-world corpora for language pairs such as Chinese-English have complex reordering relationships that are not captured by current phrase-based MT systems, despite their state-of-the-art performance measured in competitive evaluations. Synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the context-free property in each language have been proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, each piece of lexical inf"
P06-2122,J00-1004,0,0.0244868,"een proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, each piece of lexical information considered by a model multiplies the number of states of dynamic programming algorithms for inference, meaning In order to better understand the model, we analyze its performance in terms of both agreement with human-annotated alignments, and agreement with the dependencies produced by monolingual parsers. We find that within-language bilexicalization does not improve alignment over crosslanguage bilexicalization, but does improve recovery of dependencies. We find that the hook trick significantly speeds training, even in"
P06-2122,P03-1019,0,0.0183984,"head-modifier relations in either language into account. However, modeling complete bilingual bilexical dependencies as theorized in Melamed (2003) implies a huge parameter space of O(|V |2 |T |2 ), where |V |and |T |are the vocabulary sizes of the two languages. So, instead of modeling cross-language word translations and within-language word dependencies in P (S → A) · P (A → [C B]) · P (C → I/Je) · P (B → hC Ci) · P (C → see/vois) · P (C → them/les) The structural constraint of ITG, which is that only binary permutations are allowed on each level, has been demonstrated to be reasonable by Zens and Ney (2003) and Zhang and Gildea (2004). However, in the space of ITG-constrained 954 S S A A(see/vois) C B C I/Je S B(see/vois) C(I/Je) C(see/vois) see/vois them/les I/Je C(them/les) B(see) C(I) C C C A(see) C(see) see/vois C(them) them/les Figure 1: Parses for an example sentence pair under unlexicalized ITG (left), cross-language bilexicalization (center), and head-modifier bilexicaliztion (right). Thick lines indicate head child; crossbar indicates inverted production. a joint fashion, we factor them apart. We lexicalize the dependencies in the synchronous tree using words from only one language and"
P06-2122,P96-1041,0,0.0205626,"hand-aligned bilingual corpus. All the models were trained using the same amount of data. We ran the experiments on sentences up to 25 words long in both languages. The resulting training corpus had 18,773 sentence pairs with a total of 276,113 Chinese words and 315,415 English words. For scoring the Viterbi alignments of each system against gold-standard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words: The two levels of distributions are interpolated using a technique inspired by Witten-Bell smoothing (Chen and Goodman, 1996). We use the expected count of the left hand side lexical nonterminal to adjust the weight for the EM-trained bilexical probability. For example, P ([Y (e) Z(e0 )] |X(e)) = (1 − λ)PEM ([Y (e) Z(e0 )] |X(e)) + λP ([Y (∗) Z(e0 )] |X(∗)) where λ = 1/(1 + Expected Counts(X(e))) 4 Experiments First of all, we are interested in finding out how much speedup can be achieved by doing the hook trick for EM. We implemented both versions in C++ and turned off pruning for both. We ran the two inside-outside parsing algorithms on a small test set of 46 sentence pairs that are no longer than 25 words in both"
P06-2122,C04-1060,1,0.850492,"in either language into account. However, modeling complete bilingual bilexical dependencies as theorized in Melamed (2003) implies a huge parameter space of O(|V |2 |T |2 ), where |V |and |T |are the vocabulary sizes of the two languages. So, instead of modeling cross-language word translations and within-language word dependencies in P (S → A) · P (A → [C B]) · P (C → I/Je) · P (B → hC Ci) · P (C → see/vois) · P (C → them/les) The structural constraint of ITG, which is that only binary permutations are allowed on each level, has been demonstrated to be reasonable by Zens and Ney (2003) and Zhang and Gildea (2004). However, in the space of ITG-constrained 954 S S A A(see/vois) C B C I/Je S B(see/vois) C(I/Je) C(see/vois) see/vois them/les I/Je C(them/les) B(see) C(I) C C C A(see) C(see) see/vois C(them) them/les Figure 1: Parses for an example sentence pair under unlexicalized ITG (left), cross-language bilexicalization (center), and head-modifier bilexicaliztion (right). Thick lines indicate head child; crossbar indicates inverted production. a joint fashion, we factor them apart. We lexicalize the dependencies in the synchronous tree using words from only one language and translate the words into the"
P06-2122,P05-1033,0,0.0800153,"istical machine translation is the trade-off between representational power and computational complexity. Real-world corpora for language pairs such as Chinese-English have complex reordering relationships that are not captured by current phrase-based MT systems, despite their state-of-the-art performance measured in competitive evaluations. Synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the context-free property in each language have been proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, each piece of lexical information considered by a model"
P06-2122,P05-1059,1,0.844585,"s, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, each piece of lexical information considered by a model multiplies the number of states of dynamic programming algorithms for inference, meaning In order to better understand the model, we analyze its performance in terms of both agreement with human-annotated alignments, and agreement with the dependencies produced by monolingual parsers. We find that within-language bilexicalization does not improve alignment over crosslanguage bilexicalization, but does improve recovery of dependencies. We find that the hook trick significantly speeds training, even in the presence of pruning. Se"
P06-2122,P99-1059,0,0.203941,"benefits from the dynamic programming “hook trick”. The model produces improved dependency structure for both languages. 1 In this paper we compare two approaches to lexicalization, both of which incorporate bilexical probabilities. One model uses bilexical probabilities across languages, while the other uses bilexical probabilities within one language. We compare results on word-level alignment, and investigate the implications of the choice of lexicalization on the specifics of our alignment algorithms. The new model, which bilexicalizes within languages, allows us to use the “hook trick” (Eisner and Satta, 1999) and therefore reduces complexity. We describe the application of the hook trick to estimation with Expectation Maximization (EM). Despite the theoretical benefits of the hook trick, it is not widely used in statistical monolingual parsers, because the savings do not exceed those obtained with simple pruning. We speculate that the advantages may be greater in an EM setting, where parameters to guide pruning are not (initially) available. Introduction A major difficulty in statistical machine translation is the trade-off between representational power and computational complexity. Real-world co"
P08-1012,J93-2003,0,0.0547987,"models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches. 1 Introduction Most state-of-the-art statistical machine translation systems are based on large phrase tables extracted from parallel text using word-level alignments. These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al. (1996). As these word-level alignment models restrict the word alignment complexity by requiring each target word to align to zero or one source words, results are improved by aligning both source-to-target as well as target-to-source, then heuristically combining these alignments. Finally, the set of phrases consistent with the word alignments are extracted from every sentence pair; these form the basis of the decoding process. While this approach has been very successful, poor wordlevel alignments are nonetheless a common source of error in machine translation systems. A na"
P08-1012,W07-0403,0,0.541593,"ly speaking, the goal of this section is the same as the previous section, namely, to limit the set of phrase pairs that needs to be considered in the training process. The tic-tac-toe pruning relies on IBM model 1 for scoring a given aligned area. In this part, we use word-based ITG alignments as anchor points in the alignment space to pin down the potential phrases. The scope of iterative phrasal ITG training, therefore, is limited to determining the boundaries of the phrases anchored on the given one-toone word alignments. The heuristic method is based on the NonCompositional Constraint of Cherry and Lin (2007). Cherry and Lin (2007) use GIZA++ intersections which have high precision as anchor points in the 102 bitext space to constraint ITG phrases. We use ITG Viterbi alignments instead. The benefit is two-fold. First of all, we do not have to run a GIZA++ aligner. Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns. GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG. Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the p"
P08-1012,P05-1033,0,0.0486337,"recision as anchor points in the 102 bitext space to constraint ITG phrases. We use ITG Viterbi alignments instead. The benefit is two-fold. First of all, we do not have to run a GIZA++ aligner. Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns. GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG. Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of Och and Ney (2004) and Chiang (2005) with the additional constraint that each phrase pair contains at most one word link. Mathematically, let e(i, j) count the number of word links that are emitted from the substring ei...j , and f (l, m) count the number of word links emitted from the substring fl...m . The non-compositional phrase pairs satisfy e(i, j) = f (l, m) ≤ 1. Figure 3 (a) shows all possible non-compositional phrases given the Viterbi word alignment of the example sentence pair. 6 Summary of the Pipeline We summarize the pipeline of our system, demonstrating the interactions between the three main contributions of this"
P08-1012,P07-1094,0,0.0064648,"r non-zero values. Our second approach was to constrain the search space using simpler alignment models, which has the further benefit of significantly speeding up training. First we train a lower level word alignment model, then we place hard constraints on the phrasal alignment space using confident word links from this simpler model. Combining the two approaches, we have a staged training procedure going from the simplest unconstrained word based model to a constrained Bayesian word-level ITG model, and finally proceeding to a constrained Bayesian phrasal model. 3 Variational Bayes for ITG Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging. In this section, we describe a Bayesian estimator for ITG: we select parameters that optimize the probability of the data given a prior. The traditional estimation method for word alignment models is the EM algorithm (Brown et al., 1993) which iteratively updates parameters to maximize the likelihood of the data. The drawback of maximum likelihood is obvious for phrase-based models. If we do not put any co"
P08-1012,D07-1031,0,0.0815598,"ach was to constrain the search space using simpler alignment models, which has the further benefit of significantly speeding up training. First we train a lower level word alignment model, then we place hard constraints on the phrasal alignment space using confident word links from this simpler model. Combining the two approaches, we have a staged training procedure going from the simplest unconstrained word based model to a constrained Bayesian word-level ITG model, and finally proceeding to a constrained Bayesian phrasal model. 3 Variational Bayes for ITG Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging. In this section, we describe a Bayesian estimator for ITG: we select parameters that optimize the probability of the data given a prior. The traditional estimation method for word alignment models is the EM algorithm (Brown et al., 1993) which iteratively updates parameters to maximize the likelihood of the data. The drawback of maximum likelihood is obvious for phrase-based models. If we do not put any constraint on the dis"
P08-1012,koen-2004-pharaoh,0,0.0360953,"translation tables using five iterations of Model 1. These values were used to perform tic-tac-toe pruning with τb = 1 × 10−3 and τs = 1 × 10−6 . Over the pruned charts, we ran 10 iterations of word-based ITG using EM or VB. The charts were then pruned further by applying the non-compositional constraint from the Viterbi alignment links of that model. Finally we ran 10 iterations of phrase-based ITG over the residual charts, using EM or VB, and extracted the Viterbi alignments. For translation, we used the standard phrasal decoding approach, based on a re-implementation of the Pharaoh system (Koehn, 2004). The output of the word alignment systems (GIZA++ or ITG) were fed to a standard phrase extraction procedure that extracted all phrases of length up to 7 and estimated the conditional probabilities of source given target and target given source using relative frequencies. Thus our phrasal ITG learns only the minimal non-compositional phrases; the standard phrase-extraction algorithm learns larger combinations of these minimal units. In addition the phrases were annotated with lexical weights using the IBM Model 1 tables. The decoder also used a trigram language model trained on the target sid"
P08-1012,W02-1018,0,0.18838,"ues is unite the word-level and phrase-level models into one learning procedure. Ideally, such a procedure would remedy the deficiencies of word-level alignment models, including the strong restrictions on the form of the alignment, and the strong independence assumption between words. Furthermore it would obviate the need for heuristic combination of word alignments. A unified procedure may also improve the identification of non-compositional phrasal translations, and the attachment decisions for unaligned words. In this direction, Expectation Maximization at the phrase level was proposed by Marcu and Wong (2002), who, however, experienced two major difficulties: computational complexity and controlling overfitting. Computational complexity arises from the exponentially large number of decompositions of a sentence pair into phrase pairs; overfitting is a problem because as EM attempts to maximize the likelihood of its training data, it prefers to directly explain a sentence pair with a single phrase pair. In this paper, we attempt to address these two issues in order to apply EM above the word level. 97 Proceedings of ACL-08: HLT, pages 97–105, c Columbus, Ohio, USA, June 2008. 2008 Association for Co"
P08-1012,E03-1035,1,0.44657,"ee (either as terminals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j, l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j, l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005). Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases. The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j]. Our pruning differs from Zhang and Gildea (2005) in two major ways. First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and"
P08-1012,J03-1002,0,0.0141074,"7.2 End-to-end Evaluation Given an unlimited amount of time, we would tune the prior to maximize end-to-end performance, using an objective function such as BLEU. Unfortunately these experiments are very slow. Since we observed monotonic increases in alignment performance with smaller values of αC , we simply fixed the prior at a very small value (10−100 ) for all translation experiments. We do compare VB against EM in terms of final BLEU scores in the translation experiments to ensure that this sparse prior has a significant impact on the output. We also trained a baseline model with GIZA++ (Och and Ney, 2003) following a regimen of 5 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4. We computed Chinese-toEnglish and English-to-Chinese word translation tables using five iterations of Model 1. These values were used to perform tic-tac-toe pruning with τb = 1 × 10−3 and τs = 1 × 10−6 . Over the pruned charts, we ran 10 iterations of word-based ITG using EM or VB. The charts were then pruned further by applying the non-compositional constraint from the Viterbi alignment links of that model. Finally we ran 10 iterations of phrase-based ITG over the residual charts, using EM or VB"
P08-1012,J04-4002,0,0.0390477,"tions which have high precision as anchor points in the 102 bitext space to constraint ITG phrases. We use ITG Viterbi alignments instead. The benefit is two-fold. First of all, we do not have to run a GIZA++ aligner. Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns. GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG. Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of Och and Ney (2004) and Chiang (2005) with the additional constraint that each phrase pair contains at most one word link. Mathematically, let e(i, j) count the number of word links that are emitted from the substring ei...j , and f (l, m) count the number of word links emitted from the substring fl...m . The non-compositional phrase pairs satisfy e(i, j) = f (l, m) ≤ 1. Figure 3 (a) shows all possible non-compositional phrases given the Viterbi word alignment of the example sentence pair. 6 Summary of the Pipeline We summarize the pipeline of our system, demonstrating the interactions between the three main con"
P08-1012,P03-1021,0,0.03586,"d all phrases of length up to 7 and estimated the conditional probabilities of source given target and target given source using relative frequencies. Thus our phrasal ITG learns only the minimal non-compositional phrases; the standard phrase-extraction algorithm learns larger combinations of these minimal units. In addition the phrases were annotated with lexical weights using the IBM Model 1 tables. The decoder also used a trigram language model trained on the target side of the training data, as well as word count, phrase count, and distortion penalty features. Minimum Error Rate training (Och, 2003) over BLEU was used to optimize the weights for each of these models over the development test data. We used the NIST 2002 evaluation datasets for tuning and evaluation; the 10-reference development set was used for minimum error rate training, and the 4-reference test set was used for evaluation. We trained several phrasal translation systems, varying only the word alignment (or phrasal alignment) method. Table 1 compares the four systems: the GIZA++ baseline, the ITG word-based model, the ITG multiword model using EM training, and the ITG multiword model using VB training. ITG-mwm-VB is our"
P08-1012,C96-2141,0,0.614103,"ble, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches. 1 Introduction Most state-of-the-art statistical machine translation systems are based on large phrase tables extracted from parallel text using word-level alignments. These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al. (1996). As these word-level alignment models restrict the word alignment complexity by requiring each target word to align to zero or one source words, results are improved by aligning both source-to-target as well as target-to-source, then heuristically combining these alignments. Finally, the set of phrases consistent with the word alignments are extracted from every sentence pair; these form the basis of the decoding process. While this approach has been very successful, poor wordlevel alignments are nonetheless a common source of error in machine translation systems. A natural solution to severa"
P08-1012,2005.mtsummit-papers.33,0,0.067956,"minals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j, l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j, l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005). Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases. The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j]. Our pruning differs from Zhang and Gildea (2005) in two major ways. First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and VB . Only those"
P08-1012,J97-3002,0,0.894319,"arning small noncompositional phrases. We address the tendency of EM to overfit by using Bayesian methods, where sparse priors assign greater mass to parameter vectors with fewer non-zero values therefore favoring shorter, more frequent phrases. We test our model by extracting longer phrases from our model’s alignments using traditional phrase extraction, and find that a phrase table based on our system improves MT results over a phrase table extracted from traditional word-level alignments. 2 Phrasal Inversion Transduction Grammar We use a phrasal extension of Inversion Transduction Grammar (Wu, 1997) as the generative framework. Our ITG has two nonterminals: X and C, where X represents compositional phrase pairs that can have recursive structures and C is the preterminal over terminal phrase pairs. There are three rules with X on the left-hand side: X → [X X], X → hX Xi, X → C. The first two rules are the straight rule and inverted rule respectively. They split the left-hand side constituent which represents a phrase pair into two smaller phrase pairs on the right-hand side and order them according to one of the two possible permutations. The rewriting process continues until the third ru"
P08-1012,P05-1059,1,0.873,"nals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j, l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j, l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005). Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases. The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j]. Our pruning differs from Zhang and Gildea (2005) in two major ways. First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and VB . Only those spans that pass the pruning th"
P08-1025,P05-1033,0,0.0881751,"s. Thus, we are focusing on Inversion Transduction Grammars (Wu, 1997) which are an important subclass of SCFG. Formally, the rules in our grammar include preterminal unary rules: X → e/f for pairing up words or phrases in the two languages and binary production rules with straight or inverted orders that are responsible for building up upperlevel synchronous structures. They are straight rules written: X → [Y Z] and inverted rules written: X → hY Zi. Most practical non-binary SCFGs can be binarized using the synchronous binarization technique by Zhang et al. (2006). The Hiero-style rules of (Chiang, 2005), which are not strictly binary but binary only on nonterminals: X → yu X (1) you X (2) ; have X (2) with X (1) can be handled similarly through either offline binarization or allowing a fixed maximum number of gap words between the right hand side nonterminals in the decoder. For these reasons, the parsing problems for more realistic synchronous CFGs such as in Chiang (2005) and Galley et al. (2006) are formally equivalent to ITG. Therefore, we believe our focus on ITG 210 for the search efficiency issue is likely to generalize to other SCFG-based methods. Without an n-gram language model, de"
P08-1025,J07-2003,0,0.511542,"i, j, u1,..,n−1 , v1,..,n−1 ]) + α(X[i, j, u1 , vn−1 ]) + hBB (X, i, j, u1,...,n , v1,...,n ) (1) where β is the Viterbi inside cost and α is the Viterbi outside cost, to globally prioritize the n-gram integrated states on the agenda for exploration. The complexity of n-gram integrated decoding for SCFG has been tackled using other methods. The hook trick of Huang et al. (2005) factorizes the dynamic programming steps and lowers the asymptotic complexity of the n-gram integrated decoding, but has not been implemented in large-scale systems where massive pruning is present. The cube-pruning by Chiang (2007) and the lazy cube-pruning of Huang and Chiang (2007) turn the computation of beam pruning of CYK decoders into a top-k selection problem given two columns of translation hypotheses that need to be combined. The insight for doing the expansion top-down lazily is that there is no need to uniformly explore every cell. The algorithm starts with requesting the first best hypothesis from the root. The request translates into requests for the k-bests of some of its children and grandchildren and so on, because re-ranking at each node is needed to get the top ones. Venugopal et al. (2007) also take a"
P08-1025,P06-1121,0,0.0497188,"s written: X → [Y Z] and inverted rules written: X → hY Zi. Most practical non-binary SCFGs can be binarized using the synchronous binarization technique by Zhang et al. (2006). The Hiero-style rules of (Chiang, 2005), which are not strictly binary but binary only on nonterminals: X → yu X (1) you X (2) ; have X (2) with X (1) can be handled similarly through either offline binarization or allowing a fixed maximum number of gap words between the right hand side nonterminals in the decoder. For these reasons, the parsing problems for more realistic synchronous CFGs such as in Chiang (2005) and Galley et al. (2006) are formally equivalent to ITG. Therefore, we believe our focus on ITG 210 for the search efficiency issue is likely to generalize to other SCFG-based methods. Without an n-gram language model, decoding using SCFG is not much different from CFG parsing. At each time a CFG rule is applied on the input string, we apply the synchronized CFG rule for the output language. From a dynamic programming point of view, the DP states are X[i, j], where X ranges over all possible nonterminals and i and j range over 0 to the input string length |w|. Each state stores the best translations obtainable. When"
P08-1025,P96-1024,0,0.032353,"since each synchronous constituent in the tree adds a new 4-gram to the translation at the point where its children are concatenated, the additional pass approximately maximizes BLEU. Kumar and Byrne (2004) proposed the framework of Minimum Bayesian Risk (MBR) decoding that minimizes the expected loss given a loss function. Their MBR decoding is a reranking pass over an nbest list of translations returned by the decoder. Our algorithm is another dynamic programming decoding pass on the trigram forest, and is similar to the parsing algorithm for maximizing expected labelled recall presented by Goodman (1996). where α is the outside probability and β is the inside probability. We approximate β and α using the Viterbi probabilities. Since decoding from bottom up in the trigram pass already gives us the inside Viterbi scores, we only have to visit the nodes in the reverse order once we reach the root to compute the Viterbi outside scores. The outside-pass Algorithm 1 for bigram decoding can be generalized to the trigram case. We want to maximize over all translations (synchronous trees) T in the forest after the trigram decoding pass according to X EC([X, i, j, u, u′ , v ′ , v]). max T [X,i,j,u,u′ ,"
P08-1025,W05-1507,1,0.95154,"] can interact with each other by “peeping into” the leading and trailing n − 1 words on the output side for each state. Different boundary words differentiate the spanparameterized states. Thus, to preserve the dynamic programming property, we need to refine the states by adding the boundary words into the parameterization. The LM -integrated states are represented as X[i, j, u1,..,n−1 , v1,..,n−1 ]. Since the number of variables involved at each DP step has increased to 3 + 4(n − 1), the decoding algorithm is asymptotically O(|w|3+4(n−1) ). Although it is possible to use the “hook” trick of Huang et al. (2005) to factorize the DP operations to reduce the complexity to O(|w|3+3(n−1) ), when n is greater than 2, the complexity is still prohibitive. 3 Multi-pass LM-Integrated Decoding In this section, we describe a multi-pass progressive decoding technique that gradually augments the LM -integrated states from lower orders to higher orders. For instance, a bigram-integrated state [X, i, j, u, v] is said to be a coarse-level state of a trigram-integrate state [X, i, j, u, u′ , v ′ , v], because the latter state refines the previous by specifying more inner words. Progressive search has been used for HM"
P08-1025,N03-1016,0,0.0422729,"ive pass for the following n-gram pass. We need to do insideoutside parsing as coarse-to-fine parsers do. However, we use the outside probability or cost information differently. We do not combine the inside and outside costs of a simpler model to prune the space for a more complex model. Instead, for a given finergained state, we combine its true inside cost with the outside cost of its coarse-level counter-part to estimate its worthiness of being explored. The use of the outside cost from a coarser-level as the outside estimate makes our method naturally fall in the framework of A* parsing. Klein and Manning (2003) describe an A* parsing framework for monolingual parsing and admissible outside estimates that are computed using inside/outside parsing algorithm on simplified PCFGs compared to the original PCFG. Zhang and Gildea (2006) describe A* for ITG and develop admissible heuristics for both alignment and decoding. Both have shown the effectiveness of A* in situations where the outside estimate approximates the true cost closely such as when the sentences are short. For decoding long sentences, it is difficult to come up with good admissible (or inadmissible) heuristics. If we can afford a bigram dec"
P08-1025,N04-1022,0,0.0875274,"u, u′ ]) + rule(X → hY Zi) + bigram(u′ , v ′ )} ′ α(Z[k, j, u, u ]) = max {α(Z[k, j, u, u′ ]), α(X[i, j, u, v]) + β(Y [i, k, v ′ , v]) + rule(X → hY Zi) + bigram(u′ , v ′ )} end if end for end for we deal with the mismatch by introducing another decoding pass that maximizes the expected count of synchronous constituents in the tree corresponding to the translation returned. BLEU is based on n-gram precision, and since each synchronous constituent in the tree adds a new 4-gram to the translation at the point where its children are concatenated, the additional pass approximately maximizes BLEU. Kumar and Byrne (2004) proposed the framework of Minimum Bayesian Risk (MBR) decoding that minimizes the expected loss given a loss function. Their MBR decoding is a reranking pass over an nbest list of translations returned by the decoder. Our algorithm is another dynamic programming decoding pass on the trigram forest, and is similar to the parsing algorithm for maximizing expected labelled recall presented by Goodman (1996). where α is the outside probability and β is the inside probability. We approximate β and α using the Viterbi probabilities. Since decoding from bottom up in the trigram pass already gives us"
P08-1025,H05-1101,0,0.0154863,"egin by introducing Synchronous Context Free Grammars and their decoding algorithms when an n-gram language model is integrated into the grammatical search space. A synchronous CFG (SCFG) is a set of contextfree rewriting rules for recursively generating string pairs. Each synchronous rule is a pair of CFG rules 209 Proceedings of ACL-08: HLT, pages 209–217, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics with the nonterminals on the right hand side of one CFG rule being one-to-one mapped to the other CFG rule via a permutation π. We adopt the SCFG notation of Satta and Peserico (2005). Superscript indices in the right-hand side of grammar rules: (1) (π(1)) (π(n)) X → X1 ...Xn(n) , Xπ(1) ...Xπ(n) indicate that the nonterminals with the same index are linked across the two languages, and will eventually be rewritten by the same rule application. Each Xi is a variable which can take the value of any nonterminal in the grammar. In this paper, we focus on binary SCFGs and without loss of generality assume that only the preterminal unary rules can generate terminal string pairs. Thus, we are focusing on Inversion Transduction Grammars (Wu, 1997) which are an important subclass o"
P08-1025,N07-1063,0,0.0938584,". The cube-pruning by Chiang (2007) and the lazy cube-pruning of Huang and Chiang (2007) turn the computation of beam pruning of CYK decoders into a top-k selection problem given two columns of translation hypotheses that need to be combined. The insight for doing the expansion top-down lazily is that there is no need to uniformly explore every cell. The algorithm starts with requesting the first best hypothesis from the root. The request translates into requests for the k-bests of some of its children and grandchildren and so on, because re-ranking at each node is needed to get the top ones. Venugopal et al. (2007) also take a two-pass decoding approach, with the first pass leaving the language model boundary words out of the dynamic programming state, such that only one hypothesis is retained for each span and grammar symbol. 4 Decoding to Maximize BLEU The ultimate goal of efficient decoding to find the translation that has a highest evaluation score using the least time possible. Section 3 talks about utilizing the outside cost of a lower-order model to estimate the outside cost of a higher-order model, boosting the search for the higher-order model. By doing so, we hope the intrinsic metric of our m"
P08-1025,J97-3002,0,0.0265446,"SCFG notation of Satta and Peserico (2005). Superscript indices in the right-hand side of grammar rules: (1) (π(1)) (π(n)) X → X1 ...Xn(n) , Xπ(1) ...Xπ(n) indicate that the nonterminals with the same index are linked across the two languages, and will eventually be rewritten by the same rule application. Each Xi is a variable which can take the value of any nonterminal in the grammar. In this paper, we focus on binary SCFGs and without loss of generality assume that only the preterminal unary rules can generate terminal string pairs. Thus, we are focusing on Inversion Transduction Grammars (Wu, 1997) which are an important subclass of SCFG. Formally, the rules in our grammar include preterminal unary rules: X → e/f for pairing up words or phrases in the two languages and binary production rules with straight or inverted orders that are responsible for building up upperlevel synchronous structures. They are straight rules written: X → [Y Z] and inverted rules written: X → hY Zi. Most practical non-binary SCFGs can be binarized using the synchronous binarization technique by Zhang et al. (2006). The Hiero-style rules of (Chiang, 2005), which are not strictly binary but binary only on nonter"
P08-1025,W06-1627,1,0.941301,"ts of a simpler model to prune the space for a more complex model. Instead, for a given finergained state, we combine its true inside cost with the outside cost of its coarse-level counter-part to estimate its worthiness of being explored. The use of the outside cost from a coarser-level as the outside estimate makes our method naturally fall in the framework of A* parsing. Klein and Manning (2003) describe an A* parsing framework for monolingual parsing and admissible outside estimates that are computed using inside/outside parsing algorithm on simplified PCFGs compared to the original PCFG. Zhang and Gildea (2006) describe A* for ITG and develop admissible heuristics for both alignment and decoding. Both have shown the effectiveness of A* in situations where the outside estimate approximates the true cost closely such as when the sentences are short. For decoding long sentences, it is difficult to come up with good admissible (or inadmissible) heuristics. If we can afford a bigram decoding pass, the outside cost from a bigram model is conceivably a 211 very good estimate of the outside cost using a trigram model since a bigram language model and a trigram language model must be strongly correlated. Alt"
P08-1025,N06-1033,1,0.857507,"l unary rules can generate terminal string pairs. Thus, we are focusing on Inversion Transduction Grammars (Wu, 1997) which are an important subclass of SCFG. Formally, the rules in our grammar include preterminal unary rules: X → e/f for pairing up words or phrases in the two languages and binary production rules with straight or inverted orders that are responsible for building up upperlevel synchronous structures. They are straight rules written: X → [Y Z] and inverted rules written: X → hY Zi. Most practical non-binary SCFGs can be binarized using the synchronous binarization technique by Zhang et al. (2006). The Hiero-style rules of (Chiang, 2005), which are not strictly binary but binary only on nonterminals: X → yu X (1) you X (2) ; have X (2) with X (1) can be handled similarly through either offline binarization or allowing a fixed maximum number of gap words between the right hand side nonterminals in the decoder. For these reasons, the parsing problems for more realistic synchronous CFGs such as in Chiang (2005) and Galley et al. (2006) are formally equivalent to ITG. Therefore, we believe our focus on ITG 210 for the search efficiency issue is likely to generalize to other SCFG-based meth"
P11-1084,W10-1703,0,0.0364522,"Missing"
P11-1084,P05-1033,0,0.832123,"Combining the two techniques, we show that using a fast shift-reduce parser we can achieve significant quality gains in NIST 2008 English-to-Chinese track (1.3 BLEU points over a phrase-based system, 0.8 BLEU points over a hierarchical phrase-based system). Consistent and significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks. 1 Introduction In recent years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the ta"
P11-1084,J07-2003,0,0.635373,"nslation forest generated by all applicable translation rules, which is not necessarily binary, we apply the synchronous binarization algorithm (Zhang et al., 2006) to generate a binary translation forest. Finally, we use a bottom-up de836 coding algorithm with intergrated LM intersection using the cube pruning technique (Chiang, 2005). The rest of the paper is organized as follows. In Section 2, we give an overview of the forest-tostring models. In Section 2.1, we introduce a more efficient and flexible algorithm for extracting composed GHKM rules based on the same principle as cube pruning (Chiang, 2007). In Section 3, we introduce our source tree binarization algorithm for producing binarized forests. In Section 4, we explain how to do synchronous rule factorization in a forest-to-string decoder. Experimental results are in Section 5. 2 Forest-to-string Translation Forest-to-string models can be described as e = Y( arg max P (d|T ) ) (1) d∈D(T ), T ∈F (f ) where f stands for a source string, e stands for a target string, F stands for a forest, D stands for a set of synchronous derivations on a given tree T , and Y stands for the target side yield of a derivation. The search problem is findin"
P11-1084,D07-1079,0,0.0930875,"). The pruning parameters that control the size of forests are normally handtuned. Such forests encode both syntactic variants and structural variants. By syntactic variants, we refer to the fact that a parser can parse a substring into either a noun phrase or verb phrase in certain cases. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 835–845, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics We believe that structural variants which allow more source spans to be explored during translation are more important (DeNeefe et al., 2007), while syntactic variants might improve word sense disambiguation but also introduce more spurious ambiguities (Chiang, 2005) during decoding. To focus on structural variants, we propose a family of binarization algorithms to expand one single constituent tree into a packed forest of binary trees containing combinations of adjacent tree nodes. We control the freedom of tree node binary combination by restricting the distance to the lowest common ancestor of two tree nodes. We show that the best results are achieved when the distance is two, i.e., when combining tree nodes sharing a common gra"
P11-1084,P03-2041,0,0.278399,"e syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-tostring models (Mi et al., 20"
P11-1084,P08-1109,0,0.0959059,"Missing"
P11-1084,N04-1035,0,0.730036,"e lowest common ancestor of two tree nodes. We show that the best results are achieved when the distance is two, i.e., when combining tree nodes sharing a common grand-parent. In contrast to conventional parser-produced-forestto-string models, in our model: • Forests are not generated by a parser but by combining sub-structures using a tree binarizer. • Instead of using arbitary pruning parameters, we control forest size by an integer number that defines the degree of tree structure violation. • There is at most one nonterminal per span so that the grammar constant is small. Since GHKM rules (Galley et al., 2004) can cover multi-level tree fragments, a synchronous grammar extracted using the GHKM algorithm can have synchronous translation rules with more than two nonterminals regardless of the branching factor of the source trees. For the first time, we show that similar to string-to-tree decoding, synchronous binarization significantly reduces search errors and improves translation quality for forest-to-string decoding. To summarize, the whole pipeline is as follows. First, a parser produces the highest-scored tree for an input sentence. Second, the parse tree is restructured using our binarization a"
P11-1084,P06-1121,0,0.628031,"n the English to German, French, Spanish and Czech tracks. 1 Introduction In recent years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are tak"
P11-1084,J99-4004,0,0.0528737,"ization forest that allows combining any two nodes with common ancestors. The ancestor chain labeled at each node licenses the node to only combine with nodes having common ancestors in the past n generations. The algorithm creates new tree nodes on the fly. 838 New tree nodes need to have their own states indicated by a node label representing what is covered internally by the node and an ancestor chain representing which nodes the node attaches to externally. Line 22 and Line 23 of Algorithm 1 update the label and ancestor annotations of new tree nodes. Using the parsing semiring notations (Goodman, 1999), the ancestor computation can be summarized by the (∩, ∪) pair. ∩ produces the ancestor chain of a hyper-edge. ∪ produces the ancestor chain of a hyper-node. The node label computation can be summarized by the (concatenate, min) pair. concatenate produces a concatenation of node labels. min yields the label with the shortest length. A tree-sequence (Liu et al., 2007) is a sequence of sub-trees covering adjacent spans. It can be proved that the final label of each new node in the forest corresponds to the tree sequence which has the minimum length among all sequences covered by the node span."
P11-1084,N04-1014,0,0.117909,"ow that using a fast shift-reduce parser we can achieve significant quality gains in NIST 2008 English-to-Chinese track (1.3 BLEU points over a phrase-based system, 0.8 BLEU points over a hierarchical phrase-based system). Consistent and significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks. 1 Introduction In recent years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search o"
P11-1084,2006.amta-papers.8,0,0.0613912,"explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and"
P11-1084,W07-0405,0,0.0168931,"is the closest to our work. But their goal was to augment a k-best forest. They did not binarize the tree sequences. They also did not put constraint on the tree-sequence nodes according to how many brackets are crossed. Wang et al. (2007) used target tree binarization to improve rule extraction for their string-to-tree system. Their binarization forest is equivalent to our cyk-1 forest. In contrast to theirs, our binarization scheme affects decoding directly because we match tree-to-string rules on a binarized forest. Different methods of translation rule binarization have been discussed in Huang (2007). Their argument is that for tree-to-string decoding target side binarization is simpler than synchronous binarization and works well because creating discontinous source spans does not explode the state space. The forest-to-string senario is more similar to string-totree decoding in which state-sharing is important. Our experiments show that synchronous binarization helps significantly in the forest-to-string case. 7 Conclusion We have presented a new approach to tree-to-string translation. It involves a source tree binarization step and a standard forest-to-string translation step. The metho"
P11-1084,P08-1067,0,0.483355,"al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-tostring models (Mi et al., 2008; Mi and Huang, 2008) use a compact representation of exponentially many trees to improve tree-to-string models. Traditionally, such forests are obtained through hyper-edge pruning in the k-best search space of a monolingual parser (Huang, 2008). The pruning parameters that control the size of forests are normally handtuned. Such forests encode both syntactic variants and structural variants. By syntactic variants, we refer to the fact that a parser can parse a substring into either a noun phrase or verb phrase in certain cases. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistic"
P11-1084,N03-1017,0,0.0665333,"Missing"
P11-1084,W04-3250,0,0.47233,"Missing"
P11-1084,P09-1019,0,0.0530621,"Missing"
P11-1084,C04-1090,0,0.0760463,"1 Introduction In recent years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000),"
P11-1084,P06-1077,0,0.182452,", researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To bal"
P11-1084,P07-1089,0,0.0653768,"rnally by the node and an ancestor chain representing which nodes the node attaches to externally. Line 22 and Line 23 of Algorithm 1 update the label and ancestor annotations of new tree nodes. Using the parsing semiring notations (Goodman, 1999), the ancestor computation can be summarized by the (∩, ∪) pair. ∩ produces the ancestor chain of a hyper-edge. ∪ produces the ancestor chain of a hyper-node. The node label computation can be summarized by the (concatenate, min) pair. concatenate produces a concatenation of node labels. min yields the label with the shortest length. A tree-sequence (Liu et al., 2007) is a sequence of sub-trees covering adjacent spans. It can be proved that the final label of each new node in the forest corresponds to the tree sequence which has the minimum length among all sequences covered by the node span. The ancestor chain of a new node is the common ancestors of the nodes in its minimum tree sequence. For clarity, we do full CYK loops over all O(|w|2 ) spans and O(|w|3 ) potential hyper-edges, where |w| is the length of a source string. In reality, only descendants under a shared ancestor can combine. If we assume trees have a bounded branching factor b, the number o"
P11-1084,D08-1022,0,0.572915,"ang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-tostring models (Mi et al., 2008; Mi and Huang, 2008) use a compact representation of exponentially many trees to improve tree-to-string models. Traditionally, such forests are obtained through hyper-edge pruning in the k-best search space of a monolingual parser (Huang, 2008). The pruning parameters that control the size of forests are normally handtuned. Such forests encode both syntactic variants and structural variants. By syntactic variants, we refer to the fact that a parser can parse a substring into either a noun phrase or verb phrase in certain cases. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistic"
P11-1084,P08-1023,0,0.437881,"ctrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-t"
P11-1084,C04-1010,0,0.0431539,"Missing"
P11-1084,J04-4002,0,0.219564,"Missing"
P11-1084,P03-1021,0,0.0827367,"Missing"
P11-1084,P02-1040,0,0.0813322,"Missing"
P11-1084,C00-2092,0,0.0302934,"ring (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-tostring models (Mi et al., 2008; Mi and Huang, 2008) use a compact representation of exponentially many trees to improve tree-to-string models. Traditionally, such forests are obtained through hyper-edge pruning in the k-best search space of a monolingual parser (Huang, 2008). The pruning parameters that control the size of forests are normally handtuned. Such forests encode both syntactic variants and structural variants. By syntactic variants, we refer to the fact that a parser can parse a substring into either a no"
P11-1084,P05-1034,0,0.113255,"tion In recent years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling"
P11-1084,P08-1066,0,0.279402,"an, French, Spanish and Czech tracks. 1 Introduction In recent years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for"
P11-1084,D07-1078,0,0.247228,"Missing"
P11-1084,W06-3108,0,0.0974778,"Missing"
P11-1084,N06-1033,1,0.965494,"duces search errors and improves translation quality for forest-to-string decoding. To summarize, the whole pipeline is as follows. First, a parser produces the highest-scored tree for an input sentence. Second, the parse tree is restructured using our binarization algorithm, resulting in a binary packed forest. Third, we apply the forest-based variant of the GHKM algorithm (Mi and Huang, 2008) on the new forest for rule extraction. Fourth, on the translation forest generated by all applicable translation rules, which is not necessarily binary, we apply the synchronous binarization algorithm (Zhang et al., 2006) to generate a binary translation forest. Finally, we use a bottom-up de836 coding algorithm with intergrated LM intersection using the cube pruning technique (Chiang, 2005). The rest of the paper is organized as follows. In Section 2, we give an overview of the forest-tostring models. In Section 2.1, we introduce a more efficient and flexible algorithm for extracting composed GHKM rules based on the same principle as cube pruning (Chiang, 2007). In Section 3, we introduce our source tree binarization algorithm for producing binarized forests. In Section 4, we explain how to do synchronous rul"
P11-1084,P08-1064,0,0.222725,"tructure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-tostring models (Mi et al., 2008; Mi and Huang, 200"
P11-1084,P09-1020,0,0.206536,"do the forest pruning on a forest generated by a k-best algorithm, while we do the forest-pruning on the full CYK chart. As a result, we need more aggressive pruning to control forest size. 842 Related Work The idea of concatenating adjacent syntactic categories has been explored in various syntax-based models. Zollmann and Venugopal (2006) augmented hierarchial phrase based systems with joint syntactic categories. Liu et al. (2007) proposed treesequence-to-string translation rules but did not provide a good solution to place joint subtrees into connection with the rest of the tree structure. Zhang et al. (2009) is the closest to our work. But their goal was to augment a k-best forest. They did not binarize the tree sequences. They also did not put constraint on the tree-sequence nodes according to how many brackets are crossed. Wang et al. (2007) used target tree binarization to improve rule extraction for their string-to-tree system. Their binarization forest is equivalent to our cyk-1 forest. In contrast to theirs, our binarization scheme affects decoding directly because we match tree-to-string rules on a binarized forest. Different methods of translation rule binarization have been discussed in"
P11-1084,W06-3119,0,0.538983,"d significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks. 1 Introduction In recent years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source si"
P11-1084,D08-1076,0,\N,Missing
P12-3004,J07-2003,0,0.61242,"it, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1 Introduction We present NiuTrans, a new open source machine translation toolkit, which was developed for constructing high quality machine translation systems. The NiuTrans toolkit supports most statistical machine translation (SMT) paradigms developed over the past decade, and allows for training and decoding with several state-of-the-art models, including: the phrase-based model (Koehn et al., 2003), the hierarchical phrase-based model (Chiang, 2007), and various syntax-based models (Galley et al., 2004; Liu et al., 2006). In particular, a unified framework was adopted to decode with different models and ease the implementation of decoding algorithms. Moreover, some useful utilities were distributed with the toolkit, such as: a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training that allows for various evaluation metrics for tuning the system. In addition, the toolkit provides easy-to-use APIs for the development of new features. The toolkit has been used to build transla"
P12-3004,P03-2041,0,0.010524,"o-tree rules on all pairs of source and target tree-fragments. 21 Decoding the hierarchical phrase-based and syntaxbased models. For efficient integration of ngram language model into decoding, rules containing more than two variables are binarized into binary rules. In addition to the rules learned from bilingual data, glue rules are employed to glue the translations of a sequence of chunks. z z 4 Decoding as tree-parsing (or tree-based decoding). If the parse tree of source sentence is provided, decoding (for tree-tostring and tree-to-tree models) can also be cast as a tree-parsing problem (Eisner, 2003). In tree-parsing, translation rules are first mapped onto the nodes of input parse tree. This results in a translation tree/forest (or a hypergraph) where each edge represents a rule application. Then decoding can proceed on the hypergraph as usual. That is, we visit in bottom-up order each node in the parse tree, and calculate the model score for each edge rooting at the node. The final output is the 1-best/k-best translations maintained by the root node of the parse tree. Since tree-parsing restricts its search space to the derivations that exactly match with the input parse tree, it in gen"
P12-3004,N04-1035,0,0.0610923,"a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1 Introduction We present NiuTrans, a new open source machine translation toolkit, which was developed for constructing high quality machine translation systems. The NiuTrans toolkit supports most statistical machine translation (SMT) paradigms developed over the past decade, and allows for training and decoding with several state-of-the-art models, including: the phrase-based model (Koehn et al., 2003), the hierarchical phrase-based model (Chiang, 2007), and various syntax-based models (Galley et al., 2004; Liu et al., 2006). In particular, a unified framework was adopted to decode with different models and ease the implementation of decoding algorithms. Moreover, some useful utilities were distributed with the toolkit, such as: a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training that allows for various evaluation metrics for tuning the system. In addition, the toolkit provides easy-to-use APIs for the development of new features. The toolkit has been used to build translation systems that have placed well at recent MT evalua"
P12-3004,D08-1089,0,0.0327335,"ls. To build new translation systems, all you need is a collection of wordaligned sentences 3 , and a set of additional sentences with one or more reference translations for weight tuning and test. Once the data is prepared, the MT system can be created using a 2 http://www.nlp.org.cn/project/project.php?proj_id=14 To obtain word-to-word alignments, several easy-to-use toolkits are available, such as GIZA++ and Berkeley Aligner. 3 20 4 Term MSD refers to the three orientations (reordering types), including Monotone (M), Swap (S), and Discontinuous (D). probabilities of the three orientations (Galley and Manning, 2008). 3.2 Translation Rule Extraction For the hierarchical phrase-based model, we follow the general framework of SCFG where a grammar rule has three parts – a source-side, a target-side and alignments between source and target nonterminals. To learn SCFG rules from word-aligned sentences, we choose the algorithm proposed in (Chiang, 2007) and estimate the associated feature values as in the phrase-based system. For the syntax-based models, all non-terminals in translation rules are annotated with syntactic labels. We use the GHKM algorithm to extract (minimal) translation rules from bilingual sen"
P12-3004,N03-1017,0,0.0449568,"Missing"
P12-3004,P07-2045,0,0.0149649,"current approaches to statistical machine translation, NiuTrans is based on a log-linear 1 http://www.gnu.org/licenses/gpl-2.0.html 19 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 19–24, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics model where a number of features are defined to model the translation process. Actually NiuTrans is not the first system of this kind. To date, several open-source SMT systems (based on either phrasebased models or syntax-based models) have been developed, such as Moses (Koehn et al., 2007), Joshua (Li et al., 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al., 2010), cdec (Dyer et al., 2010), Jane (Vilar et al., 2010) and SilkRoad 2 , and offer good references for the development of the NiuTrans toolkit. While our toolkit includes all necessary components as provided within the above systems, we have additional goals for this project, as follows: z 3 It fully supports most state-of-the-art SMT models. Among these are: the phrase-based model, the hierarchical phrase-based model, and the syntax-based models that explicitly use syntactic information on either (both) s"
P12-3004,W09-0424,0,0.0618017,"tical machine translation, NiuTrans is based on a log-linear 1 http://www.gnu.org/licenses/gpl-2.0.html 19 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 19–24, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics model where a number of features are defined to model the translation process. Actually NiuTrans is not the first system of this kind. To date, several open-source SMT systems (based on either phrasebased models or syntax-based models) have been developed, such as Moses (Koehn et al., 2007), Joshua (Li et al., 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al., 2010), cdec (Dyer et al., 2010), Jane (Vilar et al., 2010) and SilkRoad 2 , and offer good references for the development of the NiuTrans toolkit. While our toolkit includes all necessary components as provided within the above systems, we have additional goals for this project, as follows: z 3 It fully supports most state-of-the-art SMT models. Among these are: the phrase-based model, the hierarchical phrase-based model, and the syntax-based models that explicitly use syntactic information on either (both) source and (or) target lang"
P12-3004,P06-1077,0,0.106485,"nguage model, and an implementation of minimum error rate training for weight tuning. 1 Introduction We present NiuTrans, a new open source machine translation toolkit, which was developed for constructing high quality machine translation systems. The NiuTrans toolkit supports most statistical machine translation (SMT) paradigms developed over the past decade, and allows for training and decoding with several state-of-the-art models, including: the phrase-based model (Koehn et al., 2003), the hierarchical phrase-based model (Chiang, 2007), and various syntax-based models (Galley et al., 2004; Liu et al., 2006). In particular, a unified framework was adopted to decode with different models and ease the implementation of decoding algorithms. Moreover, some useful utilities were distributed with the toolkit, such as: a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training that allows for various evaluation metrics for tuning the system. In addition, the toolkit provides easy-to-use APIs for the development of new features. The toolkit has been used to build translation systems that have placed well at recent MT evaluations, such as the"
P12-3004,P03-1021,0,0.231643,"threading techniques to speed-up the system. sequence of commands. Given a number of sentence-pairs and the word alignments between them, the toolkit first extracts a phrase table and two reordering models for the phrase-based system, or a Synchronous Context-free/Tree-substitution Grammar (SCFG/STSG) for the hierarchical phrase-based and syntax-based systems. Then, an n-gram language model is built on the targetlanguage corpus. Finally, the resulting models are incorporated into the decoder which can automatically tune feature weights on the development set using minimum error rate training (Och, 2003) and translate new sentences with the optimized weights. In the following, we will give a brief review of the above components and the main features provided by the toolkit. 3.1 Phrase Extraction and Reordering Model We use a standard way to implement the phrase extraction module for the phrase-based model. That is, we extract all phrase-pairs that are consistent with word alignments. Five features are associated with each phrase-pair. They are two phrase translation probabilities, two lexical weights, and a feature of phrase penalty. We follow the method proposed in (Koehn et al., 2003) to es"
P12-3004,P11-1027,0,0.0234971,"otated with syntactic labels. We use the GHKM algorithm to extract (minimal) translation rules from bilingual sentences with parse trees on source-language side and/or target-language side 5 . Also, two or more minimal rules can be composed together to obtain larger rules and involve more contextual information. For unaligned words, we attach them to all nearby rules, instead of using the most likely attachment as in (Galley et al., 2006). 3.3 N-gram Language Modeling The toolkit includes a simple but effective n-gram language model (LM). The LM builder is basically a “sorted” trie structure (Pauls and Klein, 2011), where a map is developed to implement an array of key/value pairs, guaranteeing that the keys can be accessed in sorted order. To reduce the size of resulting language model, low-frequency n-grams are filtered out by some thresholds. Moreover, an n-gram cache is implemented to speed up n-gram probability requests for decoding. 3.4 is repeated for several times until no better weights (i.e., weights with a higher BLEU score) are found. In this way, our program can introduce some randomness into weight training. Hence users do not need to repeat MERT for obtaining stable and optimized weights"
P12-3004,W10-1738,0,0.0161795,"ngs of the 50th Annual Meeting of the Association for Computational Linguistics, pages 19–24, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics model where a number of features are defined to model the translation process. Actually NiuTrans is not the first system of this kind. To date, several open-source SMT systems (based on either phrasebased models or syntax-based models) have been developed, such as Moses (Koehn et al., 2007), Joshua (Li et al., 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al., 2010), cdec (Dyer et al., 2010), Jane (Vilar et al., 2010) and SilkRoad 2 , and offer good references for the development of the NiuTrans toolkit. While our toolkit includes all necessary components as provided within the above systems, we have additional goals for this project, as follows: z 3 It fully supports most state-of-the-art SMT models. Among these are: the phrase-based model, the hierarchical phrase-based model, and the syntax-based models that explicitly use syntactic information on either (both) source and (or) target language side(s). z It offers a wide choice of decoding algorithms. For example, the toolkit has several useful decoding o"
P12-3004,P96-1021,0,0.0239638,"s to obtain new items. Once a new item is created, the associated scores are computed (with an integrated n-gram language model). Then, the item is added into the list of the corresponding cell. This procedure stops when we reach the final state (i.e., the cell associates with the entire source span). The decoder can work with all (hierarchical) phrase-based and syntax-based models. In particular, our toolkit provides the following decoding modes. z Phrase-based decoding. To fit the phrasebased model into the CKY paring framework, we restrict the phrase-based decoding with the ITG constraint (Wu, 1996). In this way, each pair of items in adjunct cells can be composed in either monotone order or inverted order. Hence the decoding can be trivially implemented by a three-loop structure as in standard CKY parsing. This algorithm is actually the same as that used in parsing with bracketing transduction grammars. z Decoding as parsing (or string-based decoding). This mode is designed for decoding with SCFGs/STSGs which are used in the hierarchical phrase-based and syntax-based systems. In the general framework of synchronous grammars and tree transducers, decoding can be regarded as a parsing pro"
P12-3004,P06-1066,0,0.0492136,"Missing"
P12-3004,W06-3119,0,0.0205075,"n, NiuTrans is based on a log-linear 1 http://www.gnu.org/licenses/gpl-2.0.html 19 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 19–24, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics model where a number of features are defined to model the translation process. Actually NiuTrans is not the first system of this kind. To date, several open-source SMT systems (based on either phrasebased models or syntax-based models) have been developed, such as Moses (Koehn et al., 2007), Joshua (Li et al., 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al., 2010), cdec (Dyer et al., 2010), Jane (Vilar et al., 2010) and SilkRoad 2 , and offer good references for the development of the NiuTrans toolkit. While our toolkit includes all necessary components as provided within the above systems, we have additional goals for this project, as follows: z 3 It fully supports most state-of-the-art SMT models. Among these are: the phrase-based model, the hierarchical phrase-based model, and the syntax-based models that explicitly use syntactic information on either (both) source and (or) target language side(s). z It offers a wide choi"
P12-3004,P08-1023,0,\N,Missing
P12-3004,P10-4002,0,\N,Missing
P12-3004,P06-1121,0,\N,Missing
P13-2017,W06-2920,0,0.808379,"ebank is made freely available in order to facilitate research on multilingual dependency parsing.1 1 Introduction In recent years, syntactic representations based on head-modifier dependency relations between words have attracted a lot of interest (K¨ubler et al., 2009). Research in dependency parsing – computational methods to predict such representations – has increased dramatically, due in large part to the availability of dependency treebanks in a number of languages. In particular, the CoNLL shared tasks on dependency parsing have provided over twenty data sets in a standardized format (Buchholz and Marsi, 2006; Nivre et al., 2007). While these data sets are standardized in terms of their formal representation, they are still heterogeneous treebanks. That is to say, despite them all being dependency treebanks, which annotate each sentence with a dependency tree, they subscribe to different annotation schemes. This can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/. 92 Proceedings of the 51st Annual Mee"
P13-2017,W02-1503,0,0.0535563,"Missing"
P13-2017,W09-2307,0,0.0712427,"Missing"
P13-2017,P11-1061,1,0.243183,"aking fine-grained label distinctions was discouraged. Once these guidelines were fixed, annotators selected roughly an equal amount of sentences to be annotated from each domain in the unlabeled data. As the sentences were already randomly selected from a larger corpus, annotators were told to view the sentences in order and to discard a sentence only if it was 1) fragmented because of a sentence splitting error; 2) not from the language of interest; 3) incomprehensible to a native speaker; or 4) shorter than three words. The selected sentences were pre-processed using cross-lingual taggers (Das and Petrov, 2011) and parsers (McDonald et al., 2011). The annotators modified the pre-parsed trees using the TrEd2 tool. At the beginning of the annotation process, double-blind annotation, followed by manual arbitration and consensus, was used iteratively for small batches of data until the guidelines were finalized. Most of the data was annotated using single-annotation and full review: one annotator annotating the data and another reviewing it, making changes in close collaboration with the original annotator. As a final step, all annotated data was semi-automatically checked for annotation consistency. 2."
P13-2017,W08-1301,0,0.173029,"Missing"
P13-2017,D11-1006,1,0.855635,"icient if one’s goal is to build monolingual parsers and evaluate their quality without reference to other languages, as in the original CoNLL shared tasks, but there are many cases where heterogenous treebanks are less than adequate. First, a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to cr"
P13-2017,P07-1122,1,0.763455,"as head in copula constructions. For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In addition, a small number of constructions required structural conversion, notably coordination, which in the Swedish Treebank is given a Prague style analysis (Nilsson et al., 2007). For both English and Swedish, we mapped the language-specific partof-speech tags to universal tags using the mappings of Petrov et al. (2012). Towards A Universal Treebank The Stanford typed dependencies for English (De Marneffe et al., 2006; de Marneffe and Manning, 2008) serve as the point of departure for our ‘universal’ dependency representation, together with the tag set of Petrov et al. (2012) as the underlying part-of-speech representation. The Stanford scheme, partly inspired by the LFG framework, has emerged as a de facto standard for dependency annotation in English and has recentl"
P13-2017,de-marneffe-etal-2006-generating,0,0.333356,"Missing"
P13-2017,P09-1042,1,0.250489,"arsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to create homogenous syntactic dependency annotation in multiple languages. In terms of automatic construction, Zeman et al. (2012) attempt to harmonize a large number of dependency treebanks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common synWe present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages: German, English,"
P13-2017,W12-1909,0,0.0247816,"Missing"
P13-2017,petrov-etal-2012-universal,1,0.702758,"nal Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do the same for syntactic dependencies and present cross-lingual parsing experiments to highlight some of the benefits of cross-lingually consistent annotation. First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of McDonald et al. (2011). Second, the evaluation scores in general are significantly higher than previous cross-lingual studies, su"
P13-2017,D09-1086,0,0.0317978,"ross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to create homogenous syntactic dependency annotation in multiple languages. In terms of automatic construction, Zeman et al. (2012) attempt to harmonize a large number of dependency treebanks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common synWe present a new collection of treebanks with homogeneous syntactic dependency annotation for six lang"
P13-2017,Q13-1001,1,0.0665003,"Missing"
P13-2017,W04-2709,0,0.0429789,"annotation schemes. This can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/. 92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 201"
P13-2017,P13-2103,0,0.129897,"Missing"
P13-2017,N06-2015,0,0.0347787,"s can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/. 92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do th"
P13-2017,zeman-etal-2012-hamledt,0,0.060315,"Missing"
P13-2017,P03-1054,0,0.0144203,"nch data set is shown in Figure 1. We take two approaches to generating data. The first is traditional manual annotation, as previously used by Helmreich et al. (2004) for multilingual syntactic treebank construction. The second, used only for English and Swedish, is to automatically convert existing treebanks, as in Zeman et al. (2012). 2.1 Automatic Conversion Since the Stanford dependencies for English are taken as the starting point for our universal annotation scheme, we begin by describing the data sets produced by automatic conversion. For English, we used the Stanford parser (v1.6.8) (Klein and Manning, 2003) to convert the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) to basic dependency trees, including punctuation and with the copula verb as head in copula constructions. For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In"
P13-2017,P11-2033,1,0.192695,"Missing"
P13-2017,P04-1061,0,0.0673447,"word expressions (Nilsson et al., 2007; K¨ubler et al., 2009; Zeman et al., 2012). These data sets can be sufficient if one’s goal is to build monolingual parsers and evaluate their quality without reference to other languages, as in the original CoNLL shared tasks, but there are many cases where heterogenous treebanks are less than adequate. First, a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2"
P13-2017,D12-1125,0,0.0145332,"for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do the same for syntactic dependencies and present cross-lingual parsing experiments to highlight some of the benefits of cross-lingually consistent annotation. First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of McDonald et al. (2011). Second, the evaluation scores in general are significantly higher than previous cross-lingual studies, suggesting that most of these studies underestimate true accuracy. Finally, unlike all previous cross-ling"
P13-2017,J93-2004,0,\N,Missing
P13-2017,W08-1300,0,\N,Missing
P13-2017,D07-1096,1,\N,Missing
P14-2107,P06-1055,0,0.0613812,"Missing"
P14-2107,C10-1007,0,0.0176072,"yntactic dependency tree to an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart parsing using the lower-order algorithms. Similar to k-best inference, each chart cell maintains a beam of kbest partial dependency structures. Higher-order features are scored when combining beams during inference. Cube-pruning is an approximation, as the highest scoring tree may fall out of the beam before being fully scored with"
P14-2107,N12-1054,0,0.0637066,"o an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart parsing using the lower-order algorithms. Similar to k-best inference, each chart cell maintains a beam of kbest partial dependency structures. Higher-order features are scored when combining beams during inference. Cube-pruning is an approximation, as the highest scoring tree may fall out of the beam before being fully scored with higher-order features."
P14-2107,D12-1133,0,0.022542,"Missing"
P14-2107,2008.iwslt-papers.8,0,0.0618534,"Missing"
P14-2107,Q13-1034,0,0.039911,"Missing"
P14-2107,W06-2920,0,0.307722,"Missing"
P14-2107,D12-1030,1,0.960738,"ngle arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart parsing using the lower-order algorithms. Similar to k-best inference, each chart cell maintains a beam of kbest partial dependency structures. Higher-order features are scored when combining beams during inference. Cube-pruning is an approximation, as the highest scoring tree may fall out of the beam before being fully scored with higher-order features. However, Zhang et al. (2013) observe stateof-the-art results when training accounts for errors that arise due to such approximations. 656 Proceedings of the 52nd Annual Meeting of"
P14-2107,D07-1101,0,0.0520694,"identity of unlabeled structure. By limiting the size of the secondary beam, we restrict label ambiguity and enforce structural diversity within the primary beam. The resulting parser consistently improves on the state-of-the-art parser of Zhang et al. (2013). In Introduction Dependency parsers assign a syntactic dependency tree to an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart parsing using the l"
P14-2107,P11-2033,0,0.0712916,"orces every tree to be different at the dependency level and the second stores the remaining highest scoring options, which can include outputs that differ only at the token level. The present work looks at beam diversity in graph-based dependency parsing, in particular label versus structural diversity. It was shown that by keeping a diverse beam significant improvements could be achieved on standard benchmarks, in particular with respect to difficult attachment decisions. It is worth pointing out that other dependency parsing frameworks (e.g., transitionbased parsing (Zhang and Clark, 2008; Zhang and Nivre, 2011)) could also benefit from modeling structural diversity in search. Discussion Keeping multiple beams in approximate search has been explored in the past. In machine translation, multiple beams are used to prune translation hypotheses at different levels of granularity (Zens and Ney, 2008). However, the focus is improving the speed of translation decoder rather than improving translation quality through enforcement of hypothesis diversity. In parsing, Bohnet and Nivre (2012) and Bohnet et al. (2013) propose a model for joint morphological analysis, part-ofspeech tagging and dependency parsing u"
P14-2107,J07-2003,0,0.24393,"endency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart parsing using the lower-order algorithms. Similar to k-best inference, each chart cell maintains a beam of kbest partial dependency structures. Higher-order features are scored when combining beams during inference. Cube-pruning is an approximation, as the highest scoring tree may fall out of the beam before being fully scored with higher-order features. However, Zhang et al. (2013) observe stateof-the-art results when training acco"
P14-2107,D13-1093,1,0.908662,"ambiguity around noun objects to the right of the verb (DOBJ vs. IOBJ vs. TMP) could lead one or more of the structural ambiguities falling out of the beam, especially if the beam is small. To combat this, we introduce a secondary beam for each unique unlabeled structure. That is, we partition the primary (entire) beam into disjoint groups according to the identity of unlabeled structure. By limiting the size of the secondary beam, we restrict label ambiguity and enforce structural diversity within the primary beam. The resulting parser consistently improves on the state-of-the-art parser of Zhang et al. (2013). In Introduction Dependency parsers assign a syntactic dependency tree to an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on"
P14-2107,de-marneffe-etal-2006-generating,0,0.357732,"Missing"
P14-2107,C96-1058,0,0.311999,"during inference. Cube-pruning is an approximation, as the highest scoring tree may fall out of the beam before being fully scored with higher-order features. However, Zhang et al. (2013) observe stateof-the-art results when training accounts for errors that arise due to such approximations. 656 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 656–661, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 0: (a) l + l1 + l l (b) l = = l1 + l2 Figure 2: Structures and rules for parsing with the (Eisner, 1996) algorithm. Solid lines show only the construction of right-pointing first-order dependencies. l is the predicted arc label. Dashed lines are the additional sibling modifier signatures in a generalized algorithm, specifically the previous modifier in complete chart items. = 0: l1 1: l2 2: l3 l1 0: + particular, data sets with large label sets (and thus a large number of label confusions) typically see the largest jumps in accuracy. Finally, we show that the same result cannot be achieved by simply increasing the size of the beam, but requires explicit enforcing of beam diversity. 2 1: l = 0: l"
P14-2107,D13-1152,0,0.00944236,"Missing"
P14-2107,P10-1001,0,0.0949339,"e size of the secondary beam, we restrict label ambiguity and enforce structural diversity within the primary beam. The resulting parser consistently improves on the state-of-the-art parser of Zhang et al. (2013). In Introduction Dependency parsers assign a syntactic dependency tree to an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart parsing using the lower-order algorithms. Similar to k-best inference, e"
P14-2107,C12-2077,0,0.0685642,"beam, we restrict label ambiguity and enforce structural diversity within the primary beam. The resulting parser consistently improves on the state-of-the-art parser of Zhang et al. (2013). In Introduction Dependency parsers assign a syntactic dependency tree to an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart parsing using the lower-order algorithms. Similar to k-best inference, each chart cell maint"
P14-2107,P13-2109,0,0.204142,"Missing"
P14-2107,E06-1011,1,0.174032,"int groups according to the identity of unlabeled structure. By limiting the size of the secondary beam, we restrict label ambiguity and enforce structural diversity within the primary beam. The resulting parser consistently improves on the state-of-the-art parser of Zhang et al. (2013). In Introduction Dependency parsers assign a syntactic dependency tree to an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart pa"
P14-2107,P05-1012,1,0.703326,"is, we partition the primary (entire) beam into disjoint groups according to the identity of unlabeled structure. By limiting the size of the secondary beam, we restrict label ambiguity and enforce structural diversity within the primary beam. The resulting parser consistently improves on the state-of-the-art parser of Zhang et al. (2013). In Introduction Dependency parsers assign a syntactic dependency tree to an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-p"
P14-2107,D08-1059,0,\N,Missing
P14-2107,D07-1096,1,\N,Missing
P16-1169,P14-1098,0,0.0431053,"Missing"
P16-1169,I13-1095,0,0.0245992,"of our system. (a) Input: a collection of label items, represented by text and images; (b) Output: we build a taxonomy from scratch by extracting features based on distributed representations of text and images. Introduction Human knowledge is naturally organized as semantic hierarchies. For example, in WordNet (Miller, 1995), specific concepts are categorized and assigned to more general ones, leading to a semantic hierarchical structure (a.k.a taxonomy). A variety of NLP tasks, such as question answering (Harabagiu et al., 2003), document clustering (Hotho et al., 2002) and text generation (Biran and McKeown, 2013) can benefit from the conceptual relationship present in these hierarchies. Traditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowledge or time intensive, and the results have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 20"
P16-1169,S13-1005,0,0.0372593,"ion of category items (synsets), with associated images and a label hierarchy (sampled from WordNet) over them. The original ImageNet taxonomy is preprocessed, resulting in a tree structure with 28231 nodes. Word embedding training. We train word embedding for synsets by replacing each word/phrase in a synset with a unique token and then using Google’s word2vec tool (Mikolov et al., 2013). We combine three public available corpora together, including the latest Wikipedia dump (Wikipedia, 2014), the One Billion Word Language Modeling Benchmark (Chelba et al., 2013) and the UMBC webbase corpus (Han et al., 2013), resulting in a corpus with total 6 billion tokens. The dimension of the embedding is set to 200. Image processing. we employ the ILSVRC12 pre-trained convolutional neural networks (Simonyan and Zisserman, 2014) to embed each image into the vector space. Then, for each category xn with images, we estimate a multivariate Gaussian parameterized by Nxn = (µxn , Σxn ), and constrain Σxn to be diagonal to prevent overfitting. For categories with very few images, we only estimate a mean vector µxn . For nodes that do not have images, we ignore the visual feature. Training configuration. The feature"
P16-1169,D14-1005,0,0.0260877,". Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2015). These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; Zhang et al., 2015; Chen et al., 2013). On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008). The resulting hierarchies are limited in interpretability and usability for knowledge transfer. Hence, we propose to combine both visual and textual knowledge to automatically build taxonomies. We induce is-a taxonomies by supervised learning from existing entity ontologies where each concept category (entity) is associated with images, either from existing dataset (e.g. ImageNet (Deng et al.,"
P16-1169,P15-2020,0,0.0188338,"nstrate the effectiveness of integrating visual features with language features for taxonomy induction. We also provide qualitative analysis on our features, the learned model, and the taxonomies induced to provide further insights (Section 5.3). 2 Related Work Many approaches have been recently developed that build hierarchies purely by identifying either lexical patterns or statistical features in text corpora (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Zhu et al., 2013; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2014; Tuan et al., 2015; Kiela et al., 2015). The approaches in Yang and Callan (2009) and Snow et al. (2006) assume a starting incomplete hierarchy and try to extend it by inserting new terms. Kozareva and Hovy (2010) and Navigli et al. (2011) first find leaf nodes and then use lexical patterns to find intermediate terms and all the attested hypernymy links between them. In (Tuan et al., 2014), syntactic contextual similarity is exploited to construct the taxonomy, while Tuan et al. (2015) go one step further to consider trustiness and collective synonym/contrastive evidence. Different from them, our model is discriminatively trained w"
P16-1169,D10-1108,0,0.40272,"et al., 2002) and text generation (Biran and McKeown, 2013) can benefit from the conceptual relationship present in these hierarchies. Traditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowledge or time intensive, and the results have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2015). These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; Zhang et al., 2015; Chen et al., 2013). On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008). The resulting hierarchies are limited in interpretability and"
P16-1169,P14-1068,0,0.0263621,"sults have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2015). These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; Zhang et al., 2015; Chen et al., 2013). On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008). The resulting hierarchies are limited in interpretability and usability for knowledge transfer. Hence, we propose to combine both visual and textual knowledge to automatically build taxonomies. We induce is-a taxonomies by supervised learning from existing entity ontologies where each concept category (entity) is associated with images, either from existing dataset (e.g."
P16-1169,P06-1101,0,0.272427,"t clustering (Hotho et al., 2002) and text generation (Biran and McKeown, 2013) can benefit from the conceptual relationship present in these hierarchies. Traditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowledge or time intensive, and the results have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2015). These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; Zhang et al., 2015; Chen et al., 2013). On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008). The resulting hierarchies are limite"
P16-1169,D14-1088,0,0.0124141,"approaches. Extensive comparisons demonstrate the effectiveness of integrating visual features with language features for taxonomy induction. We also provide qualitative analysis on our features, the learned model, and the taxonomies induced to provide further insights (Section 5.3). 2 Related Work Many approaches have been recently developed that build hierarchies purely by identifying either lexical patterns or statistical features in text corpora (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Zhu et al., 2013; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2014; Tuan et al., 2015; Kiela et al., 2015). The approaches in Yang and Callan (2009) and Snow et al. (2006) assume a starting incomplete hierarchy and try to extend it by inserting new terms. Kozareva and Hovy (2010) and Navigli et al. (2011) first find leaf nodes and then use lexical patterns to find intermediate terms and all the attested hypernymy links between them. In (Tuan et al., 2014), syntactic contextual similarity is exploited to construct the taxonomy, while Tuan et al. (2015) go one step further to consider trustiness and collective synonym/contrastive evidence. Different from them,"
P16-1169,D15-1117,0,0.0449436,"ceptual relationship present in these hierarchies. Traditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowledge or time intensive, and the results have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2015). These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; Zhang et al., 2015; Chen et al., 2013). On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008). The resulting hierarchies are limited in interpretability and usability for knowledge transfer. Hence, we propose to combine both visual and"
P16-1169,P09-1031,0,0.430576,"et al., 2003), document clustering (Hotho et al., 2002) and text generation (Biran and McKeown, 2013) can benefit from the conceptual relationship present in these hierarchies. Traditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowledge or time intensive, and the results have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2015). These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; Zhang et al., 2015; Chen et al., 2013). On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008). The resulting hie"
P19-1336,E17-1088,0,0.0152084,"y in the low resource scenario (Zhang et al., 2016), the performance of these methods degrades significantly since the hidden feature representations cannot be learned adequately. Recently, more and more approaches have been proposed to address low-resource NER. Early works (Chen et al., 2010; Li et al., 2012) primarily assumed a large parallel corpus and focused on exploiting them to project information from high- to low-resource. Unfortunately, such a large parallel corpus may not be available for many low-resource languages. More recently, crossresource word embedding (Fang and Cohn, 2017; Adams et al., 2017; Yang et al., 2017) was proposed to bridge the low- and high-resources and enable knowledge transfer. Although the aforementioned transfer-based methods show promising performance in low-resource NER, there are two issues remain further study: 1) Representation Difference - they did not consider the representation difference across resources and enforced the feature representation to be shared across languages/domains; 2) Resource Data Imbalance the training size of high-resource is usually much larger than that of low-resource. The existing methods neglect such difference in their models, re"
P19-1336,W17-4419,0,0.388671,"proach based on deep hierarchical recurrent neural network, where full/partial hidden features between source and target tasks are shared. (Al-Rfou’ et al., 2015) built massive multilingual annotators with minimal human expertise by using language agnostic techniques. (Cotterell and Duh, 2017) proposed character-level neural CRFs to jointly train and predict low- and highresource languages. (Pan et al., 2017) proposes a large-scale cross-lingual named entity dataset which contains 282 languages for evaluation. In addition, multi-task learning (Yang et al., 2016; Luong et al., 2016; Rei, 2017; Aguilar et al., 2017; Hashimoto et al., 2017; Lin et al., 2018) shows that jointly training on multiple tasks/languages helps improve performance. Different from transfer learning methods, multi-task learning aims at improving the performance of all the resources instead of low resource only. 3462 GRAD Source CRF Layer Self-Attention Target CRF Layer concat Gradient Reversal concat Gradient Reversal Source Bi-LSTM Shared Bi-LSTM Target Bi-LSTM Shared Bidirectional LSTM CRF Layer Bidirectional LSTM concat concat Char CNN ηwS Word Emb + Shared Char CNN (a) Base Model Self-Attention concat + Source Word Emb Source C"
P19-1336,C18-1139,0,0.0206105,"In addition, we create adversarial samples to conduct the Adversarial Training (AT), further improving the generalization and alleviating over-fitting problem. We unify two kinds of adversarial learning, i.e., GRAD and AT, into one transfer learning model, termed Dual Adversarial Transfer Network (DATNet), to achieve end-toend training and obtain significant improvements on a series of NER tasks In contrast with prior methods, we do not use additional hand-crafted features and do not use cross-lingual word embeddings as well as pre-trained language models (Peters et al., 2018; Radford, 2018; Akbik et al., 2018; Devlin et al., 2018) when addressing the crosslanguage tasks. 2 Related Work Named Entity Recognition NER is typically framed as a sequence labeling task which aims at automatic detection of named entities (e.g., person, organization, location and etc.) from free text (Marrero et al., 2013). The early works applied CRF, SVM, and perception models with handcrafted features (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). With the advent of deep learning, research focus has been shifting towards deep neural networks (DNN), which requires little feature engineering and domain kn"
P19-1336,D18-1017,0,0.0359689,"word-level features, we introduce two kinds of transferable word-level encoder in our model, namely DATNet-Full Share (DATNet-F) and DATNet-Part Share (DATNet-P). In DATNetF, all the BiLSTM units are shared by both resources while word embeddings for different resources are disparate. The illustrative figure is depicted in the Figure 1(c). Different from the DATNet-F, the DATNet-P decomposes the BiLSTM units into the shared component and the resource-related one, which is shown in the Figure 1(b). Different from existing works (Yang et al., 2017; Fang and Cohn, 2017; Cotterell and Duh, 2017; Cao et al., 2018), in this work, we investigate the performance of two different shared representation architectures on different tasks and give the corresponding guidance (see Section 4.5). 3.3 Generalized Resource-Adversarial Discriminator In order to make the feature representation extracted from the source domain more compatible with those from the target domain, we encourage the outputs of the shared BiLSTM part to be resource-agnostic by constructing a resourceadversarial discriminator, which is inspired by the Language-Adversarial Discriminator proposed by (Kim et al., 2017). Unfortunately, previous wor"
P19-1336,P10-1065,0,0.209136,"11; Huang et al., 2015; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016). These end-to-end models generalize well on new entities based on features automatically learned from the data. However, when † ‡ The first two authors contributed equally. Corresponding author. the annotated corpora is small, especially in the low resource scenario (Zhang et al., 2016), the performance of these methods degrades significantly since the hidden feature representations cannot be learned adequately. Recently, more and more approaches have been proposed to address low-resource NER. Early works (Chen et al., 2010; Li et al., 2012) primarily assumed a large parallel corpus and focused on exploiting them to project information from high- to low-resource. Unfortunately, such a large parallel corpus may not be available for many low-resource languages. More recently, crossresource word embedding (Fang and Cohn, 2017; Adams et al., 2017; Yang et al., 2017) was proposed to bridge the low- and high-resources and enable knowledge transfer. Although the aforementioned transfer-based methods show promising performance in low-resource NER, there are two issues remain further study: 1) Representation Difference -"
P19-1336,Q16-1026,0,0.248377,"sing (NLP) applications. It detects not only the type of named entity, but also the entity boundaries, which requires deep understanding of the contextual semantics to disambiguate the different entity types of same tokens. To tackle this challenging problem, most early studies were based on handcrafted rules, which suffered from limited performance in practice. Current methods are devoted to developing learning based algorithms, especially neural network based methods, and have been advancing the state-of-the-art progressively (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016). These end-to-end models generalize well on new entities based on features automatically learned from the data. However, when † ‡ The first two authors contributed equally. Corresponding author. the annotated corpora is small, especially in the low resource scenario (Zhang et al., 2016), the performance of these methods degrades significantly since the hidden feature representations cannot be learned adequately. Recently, more and more approaches have been proposed to address low-resource NER. Early works (Chen et al., 2010; Li et al., 2012) primarily assumed a large paral"
P19-1336,I17-2016,0,0.167521,"d data of high-resource into lowresource. On the other hand, the shared representation methods do not require the parallel correspondence (Rei and Søgaard, 2018). For instance, (Fang and Cohn, 2017) proposed cross-lingual word embeddings to transfer knowledge across resources. (Yang et al., 2017) presented a transfer learning approach based on deep hierarchical recurrent neural network, where full/partial hidden features between source and target tasks are shared. (Al-Rfou’ et al., 2015) built massive multilingual annotators with minimal human expertise by using language agnostic techniques. (Cotterell and Duh, 2017) proposed character-level neural CRFs to jointly train and predict low- and highresource languages. (Pan et al., 2017) proposes a large-scale cross-lingual named entity dataset which contains 282 languages for evaluation. In addition, multi-task learning (Yang et al., 2016; Luong et al., 2016; Rei, 2017; Aguilar et al., 2017; Hashimoto et al., 2017; Lin et al., 2018) shows that jointly training on multiple tasks/languages helps improve performance. Different from transfer learning methods, multi-task learning aims at improving the performance of all the resources instead of low resource only."
P19-1336,W17-4422,0,0.060585,"Missing"
P19-1336,P17-2093,1,0.930263,"a is small, especially in the low resource scenario (Zhang et al., 2016), the performance of these methods degrades significantly since the hidden feature representations cannot be learned adequately. Recently, more and more approaches have been proposed to address low-resource NER. Early works (Chen et al., 2010; Li et al., 2012) primarily assumed a large parallel corpus and focused on exploiting them to project information from high- to low-resource. Unfortunately, such a large parallel corpus may not be available for many low-resource languages. More recently, crossresource word embedding (Fang and Cohn, 2017; Adams et al., 2017; Yang et al., 2017) was proposed to bridge the low- and high-resources and enable knowledge transfer. Although the aforementioned transfer-based methods show promising performance in low-resource NER, there are two issues remain further study: 1) Representation Difference - they did not consider the representation difference across resources and enforced the feature representation to be shared across languages/domains; 2) Resource Data Imbalance the training size of high-resource is usually much larger than that of low-resource. The existing methods neglect such difference"
P19-1336,N16-1155,0,0.0417765,"Missing"
P19-1336,D17-1256,0,0.0222998,"ayer concat ηc Char Emb GRAD ηwT Target Word Emb ηwS Target CRF Layer concat + Source Word Emb + + Shared Char CNN ηc ηwT Target Word Emb + Shared Char Emb Shared Char Emb (b) DATNet-P (c) DATNet-F Figure 1: The general architecture of proposed models. Adversarial Learning Adversarial learning originates from Generative Adversarial Nets (GAN) (Goodfellow et al., 2014), which shows impressing results in computer vision. Recently, many papers have tried to apply adversarial learning to NLP tasks. (Liu et al., 2017) presented an adversarial multi-task learning framework for text classification. (Gui et al., 2017) applied the adversarial discriminator to POS tagging for Twitter. (Kim et al., 2017) proposed a language discriminator to enable language-adversarial training for cross-language POS tagging. Apart from adversarial discriminator, adversarial training is another concept originally introduced by (Szegedy et al., 2014; Goodfellow et al., 2015) to improve the robustness of image classification model by injecting malicious perturbations into input images. Recently, (Miyato et al., 2017) proposed a semisupervised text classification method by applying adversarial training, where for the first time a"
P19-1336,D17-1206,0,0.0267161,"ierarchical recurrent neural network, where full/partial hidden features between source and target tasks are shared. (Al-Rfou’ et al., 2015) built massive multilingual annotators with minimal human expertise by using language agnostic techniques. (Cotterell and Duh, 2017) proposed character-level neural CRFs to jointly train and predict low- and highresource languages. (Pan et al., 2017) proposes a large-scale cross-lingual named entity dataset which contains 282 languages for evaluation. In addition, multi-task learning (Yang et al., 2016; Luong et al., 2016; Rei, 2017; Aguilar et al., 2017; Hashimoto et al., 2017; Lin et al., 2018) shows that jointly training on multiple tasks/languages helps improve performance. Different from transfer learning methods, multi-task learning aims at improving the performance of all the resources instead of low resource only. 3462 GRAD Source CRF Layer Self-Attention Target CRF Layer concat Gradient Reversal concat Gradient Reversal Source Bi-LSTM Shared Bi-LSTM Target Bi-LSTM Shared Bidirectional LSTM CRF Layer Bidirectional LSTM concat concat Char CNN ηwS Word Emb + Shared Char CNN (a) Base Model Self-Attention concat + Source Word Emb Source CRF Layer concat ηc Char"
P19-1336,D17-1302,0,0.187177,"Word Emb + + Shared Char CNN ηc ηwT Target Word Emb + Shared Char Emb Shared Char Emb (b) DATNet-P (c) DATNet-F Figure 1: The general architecture of proposed models. Adversarial Learning Adversarial learning originates from Generative Adversarial Nets (GAN) (Goodfellow et al., 2014), which shows impressing results in computer vision. Recently, many papers have tried to apply adversarial learning to NLP tasks. (Liu et al., 2017) presented an adversarial multi-task learning framework for text classification. (Gui et al., 2017) applied the adversarial discriminator to POS tagging for Twitter. (Kim et al., 2017) proposed a language discriminator to enable language-adversarial training for cross-language POS tagging. Apart from adversarial discriminator, adversarial training is another concept originally introduced by (Szegedy et al., 2014; Goodfellow et al., 2015) to improve the robustness of image classification model by injecting malicious perturbations into input images. Recently, (Miyato et al., 2017) proposed a semisupervised text classification method by applying adversarial training, where for the first time adversarial perturbations were added onto word embeddings. (Yasunaga et al., 2018) app"
P19-1336,W02-2024,0,0.115718,"onvenience of presentation. For different samples, the loss and parameters should correspond to their counterparts. For example, for the source data with word embedding wS , the loss can be defined as follows, `AT = `(Θ; wS )+`(Θ; wS,adv ) with wS,adv = wS +ηwS and ` = `GRAD + `S . Similarly, we can compute the perturbations ηc for char-embedding and ηwT for target word embedding. 4 4.1 Experiments Datasets In order to evaluate the performance of DATNet, we conduct the experiments on following widely used NER datasets: CoNLL-2003 English NER (Kim and De, 2003), CoNLL-2002 Spanish & Dutch NER (Kim, 2002), WNUT-2016 & 2017 English Twitter NER (Zeman, 2017). The statistics of these datasets are described in Table 1. We use the official split of train/validation/test sets. Different from previous works that either append the one-hot gazetteer feature to the input of 3465 Mode Additional Features POS Gazetteers Orthographic Methods (Gillick et al., 2016) (Lample et al., 2016) (Partalas et al., 2016) Mono-language (Limsopatham and Collier, 2016) /domain (Lin et al., 2017a) Best Our Base Model Mean & Std (Yang et al., 2017) (Lin et al., 2018) (Feng et al., 2018) (von D¨aniken and Cieliebak, 2017) C"
P19-1336,W03-0419,0,0.314184,"that we present the AT in a general form for the convenience of presentation. For different samples, the loss and parameters should correspond to their counterparts. For example, for the source data with word embedding wS , the loss can be defined as follows, `AT = `(Θ; wS )+`(Θ; wS,adv ) with wS,adv = wS +ηwS and ` = `GRAD + `S . Similarly, we can compute the perturbations ηc for char-embedding and ηwT for target word embedding. 4 4.1 Experiments Datasets In order to evaluate the performance of DATNet, we conduct the experiments on following widely used NER datasets: CoNLL-2003 English NER (Kim and De, 2003), CoNLL-2002 Spanish & Dutch NER (Kim, 2002), WNUT-2016 & 2017 English Twitter NER (Zeman, 2017). The statistics of these datasets are described in Table 1. We use the official split of train/validation/test sets. Different from previous works that either append the one-hot gazetteer feature to the input of 3465 Mode Additional Features POS Gazetteers Orthographic Methods (Gillick et al., 2016) (Lample et al., 2016) (Partalas et al., 2016) Mono-language (Limsopatham and Collier, 2016) /domain (Lin et al., 2017a) Best Our Base Model Mean & Std (Yang et al., 2017) (Lin et al., 2018) (Feng et al."
P19-1336,D17-1269,0,0.0881514,"Missing"
P19-1336,N16-1030,0,0.661745,"tural language processing (NLP) applications. It detects not only the type of named entity, but also the entity boundaries, which requires deep understanding of the contextual semantics to disambiguate the different entity types of same tokens. To tackle this challenging problem, most early studies were based on handcrafted rules, which suffered from limited performance in practice. Current methods are devoted to developing learning based algorithms, especially neural network based methods, and have been advancing the state-of-the-art progressively (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016). These end-to-end models generalize well on new entities based on features automatically learned from the data. However, when † ‡ The first two authors contributed equally. Corresponding author. the annotated corpora is small, especially in the low resource scenario (Zhang et al., 2016), the performance of these methods degrades significantly since the hidden feature representations cannot be learned adequately. Recently, more and more approaches have been proposed to address low-resource NER. Early works (Chen et al., 2010; Li et al., 2012) primari"
P19-1336,P17-1135,0,0.122277,"Missing"
P19-1336,W16-3920,0,0.148917,"aluate the performance of DATNet, we conduct the experiments on following widely used NER datasets: CoNLL-2003 English NER (Kim and De, 2003), CoNLL-2002 Spanish & Dutch NER (Kim, 2002), WNUT-2016 & 2017 English Twitter NER (Zeman, 2017). The statistics of these datasets are described in Table 1. We use the official split of train/validation/test sets. Different from previous works that either append the one-hot gazetteer feature to the input of 3465 Mode Additional Features POS Gazetteers Orthographic Methods (Gillick et al., 2016) (Lample et al., 2016) (Partalas et al., 2016) Mono-language (Limsopatham and Collier, 2016) /domain (Lin et al., 2017a) Best Our Base Model Mean & Std (Yang et al., 2017) (Lin et al., 2018) (Feng et al., 2018) (von D¨aniken and Cieliebak, 2017) Cross-language (Aguilar et al., 2017) /domain Best DATNet-P Mean & Std Best DATNet-F Mean & Std × × √ × √ √ × √ × √ × × × √ × √ × × √ √ × × × √ √ × × × × √ × √ × × × × × × × CoNLL Datasets Spanish Dutch WNUT Datasets WNUT-2016 WNUT-2017 82.59 82.84 85.75 81.74 46.16 52.41 85.53 85.55 44.96 85.35±0.15 85.24±0.21 44.37±0.31 40.42 35.20 34.67±0.34 85.77 85.19 85.88 86.55 86.42 88.39 88.16 88.32 50.85 87.89±0.18 88.09±0.13 50.41±0.32 53.43 87.04"
P19-1336,W17-4421,0,0.113758,"gnostic by constructing a resourceadversarial discriminator, which is inspired by the Language-Adversarial Discriminator proposed by (Kim et al., 2017). Unfortunately, previous works did not consider the imbalance of training size for two resources. Specifically, the target domain consists of very limited labeled training data, e.g., 10 sentences. In contrast, labeled training data in the source domain are much richer, e.g., 10k sentences. If such imbalance was not considered during training, the stochastic gradient descent (SGD) optimization would make the model more biased to high resource (Lin et al., 2017b). To address this imbalance problem, we impose a weight α on two resources to balance their influences. However, in the experiment we also observe that the easily classified samples from high resource comprise the majority of the loss and dominate the gradient. To overcome this issue, we further propose Generalized Resource-Adversarial Discriminator (GRAD) to enable adaptive weights for each sample which focuses the model training on hard samples. To compute the loss of GRAD, the output sequence of the shared BiLSTM is firstly encoded into a single vector via a self-attention module (Bahdana"
P19-1336,P18-1074,0,0.463558,"ural network, where full/partial hidden features between source and target tasks are shared. (Al-Rfou’ et al., 2015) built massive multilingual annotators with minimal human expertise by using language agnostic techniques. (Cotterell and Duh, 2017) proposed character-level neural CRFs to jointly train and predict low- and highresource languages. (Pan et al., 2017) proposes a large-scale cross-lingual named entity dataset which contains 282 languages for evaluation. In addition, multi-task learning (Yang et al., 2016; Luong et al., 2016; Rei, 2017; Aguilar et al., 2017; Hashimoto et al., 2017; Lin et al., 2018) shows that jointly training on multiple tasks/languages helps improve performance. Different from transfer learning methods, multi-task learning aims at improving the performance of all the resources instead of low resource only. 3462 GRAD Source CRF Layer Self-Attention Target CRF Layer concat Gradient Reversal concat Gradient Reversal Source Bi-LSTM Shared Bi-LSTM Target Bi-LSTM Shared Bidirectional LSTM CRF Layer Bidirectional LSTM concat concat Char CNN ηwS Word Emb + Shared Char CNN (a) Base Model Self-Attention concat + Source Word Emb Source CRF Layer concat ηc Char Emb GRAD ηwT Target"
P19-1336,P17-1001,0,0.0361641,"ηwS Word Emb + Shared Char CNN (a) Base Model Self-Attention concat + Source Word Emb Source CRF Layer concat ηc Char Emb GRAD ηwT Target Word Emb ηwS Target CRF Layer concat + Source Word Emb + + Shared Char CNN ηc ηwT Target Word Emb + Shared Char Emb Shared Char Emb (b) DATNet-P (c) DATNet-F Figure 1: The general architecture of proposed models. Adversarial Learning Adversarial learning originates from Generative Adversarial Nets (GAN) (Goodfellow et al., 2014), which shows impressing results in computer vision. Recently, many papers have tried to apply adversarial learning to NLP tasks. (Liu et al., 2017) presented an adversarial multi-task learning framework for text classification. (Gui et al., 2017) applied the adversarial discriminator to POS tagging for Twitter. (Kim et al., 2017) proposed a language discriminator to enable language-adversarial training for cross-language POS tagging. Apart from adversarial discriminator, adversarial training is another concept originally introduced by (Szegedy et al., 2014; Goodfellow et al., 2015) to improve the robustness of image classification model by injecting malicious perturbations into input images. Recently, (Miyato et al., 2017) proposed a sem"
P19-1336,D15-1104,0,0.0215493,"use additional hand-crafted features and do not use cross-lingual word embeddings as well as pre-trained language models (Peters et al., 2018; Radford, 2018; Akbik et al., 2018; Devlin et al., 2018) when addressing the crosslanguage tasks. 2 Related Work Named Entity Recognition NER is typically framed as a sequence labeling task which aims at automatic detection of named entities (e.g., person, organization, location and etc.) from free text (Marrero et al., 2013). The early works applied CRF, SVM, and perception models with handcrafted features (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). With the advent of deep learning, research focus has been shifting towards deep neural networks (DNN), which requires little feature engineering and domain knowledge (Lample et al., 2016; Zukov Gregoric et al., 2018; Zhou et al., 2019). (Collobert et al., 2011) proposed a feed-forward neural network with a fixed sized window for each word, which failed in considering useful relations between long-distance words. To overcome this limitation, (Chiu and Nichols, 2016) presented a bidirectional LSTM-CNNs architecture that automatically detects word- and character-level features. Ma and Hovy (201"
P19-1336,P16-1101,0,0.430498,"It detects not only the type of named entity, but also the entity boundaries, which requires deep understanding of the contextual semantics to disambiguate the different entity types of same tokens. To tackle this challenging problem, most early studies were based on handcrafted rules, which suffered from limited performance in practice. Current methods are devoted to developing learning based algorithms, especially neural network based methods, and have been advancing the state-of-the-art progressively (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016). These end-to-end models generalize well on new entities based on features automatically learned from the data. However, when † ‡ The first two authors contributed equally. Corresponding author. the annotated corpora is small, especially in the low resource scenario (Zhang et al., 2016), the performance of these methods degrades significantly since the hidden feature representations cannot be learned adequately. Recently, more and more approaches have been proposed to address low-resource NER. Early works (Chen et al., 2010; Li et al., 2012) primarily assumed a large parallel corpus and focus"
P19-1336,D16-1135,0,0.0158653,"sfer Learning for NER Transfer learning can be a powerful tool to low resource NER tasks. To bridge high and low resource, transfer learning methods for NER can be divided into two types: the parallel corpora based and the shared representation based transfer. Early works mainly focused on exploiting parallel corpora to project information between the high- and low-resource languages (Yarowsky et al., 2001; Chen et al., 2010; Li et al., 2012; Feng et al., 2018). For example, (Chen et al., 2010) and (Feng et al., 2018) proposed to jointly identify and align bilingual named entities. Ni et al. (Ni and Florian, 2016; Ni et al., 2017) utilized the Wikipedia entity type mappings to improve low-resource NER. (Mayhew et al., 2017) created a cross-language NER system, which works well for very minimal resources by translate annotated data of high-resource into lowresource. On the other hand, the shared representation methods do not require the parallel correspondence (Rei and Søgaard, 2018). For instance, (Fang and Cohn, 2017) proposed cross-lingual word embeddings to transfer knowledge across resources. (Yang et al., 2017) presented a transfer learning approach based on deep hierarchical recurrent neural net"
P19-1336,P17-1178,0,0.0303843,"rrespondence (Rei and Søgaard, 2018). For instance, (Fang and Cohn, 2017) proposed cross-lingual word embeddings to transfer knowledge across resources. (Yang et al., 2017) presented a transfer learning approach based on deep hierarchical recurrent neural network, where full/partial hidden features between source and target tasks are shared. (Al-Rfou’ et al., 2015) built massive multilingual annotators with minimal human expertise by using language agnostic techniques. (Cotterell and Duh, 2017) proposed character-level neural CRFs to jointly train and predict low- and highresource languages. (Pan et al., 2017) proposes a large-scale cross-lingual named entity dataset which contains 282 languages for evaluation. In addition, multi-task learning (Yang et al., 2016; Luong et al., 2016; Rei, 2017; Aguilar et al., 2017; Hashimoto et al., 2017; Lin et al., 2018) shows that jointly training on multiple tasks/languages helps improve performance. Different from transfer learning methods, multi-task learning aims at improving the performance of all the resources instead of low resource only. 3462 GRAD Source CRF Layer Self-Attention Target CRF Layer concat Gradient Reversal concat Gradient Reversal Source Bi"
P19-1336,W16-3923,0,0.0468573,".1 Experiments Datasets In order to evaluate the performance of DATNet, we conduct the experiments on following widely used NER datasets: CoNLL-2003 English NER (Kim and De, 2003), CoNLL-2002 Spanish & Dutch NER (Kim, 2002), WNUT-2016 & 2017 English Twitter NER (Zeman, 2017). The statistics of these datasets are described in Table 1. We use the official split of train/validation/test sets. Different from previous works that either append the one-hot gazetteer feature to the input of 3465 Mode Additional Features POS Gazetteers Orthographic Methods (Gillick et al., 2016) (Lample et al., 2016) (Partalas et al., 2016) Mono-language (Limsopatham and Collier, 2016) /domain (Lin et al., 2017a) Best Our Base Model Mean & Std (Yang et al., 2017) (Lin et al., 2018) (Feng et al., 2018) (von D¨aniken and Cieliebak, 2017) Cross-language (Aguilar et al., 2017) /domain Best DATNet-P Mean & Std Best DATNet-F Mean & Std × × √ × √ √ × √ × √ × × × √ × √ × × √ √ × × × √ √ × × × × √ × √ × × × × × × × CoNLL Datasets Spanish Dutch WNUT Datasets WNUT-2016 WNUT-2017 82.59 82.84 85.75 81.74 46.16 52.41 85.53 85.55 44.96 85.35±0.15 85.24±0.21 44.37±0.31 40.42 35.20 34.67±0.34 85.77 85.19 85.88 86.55 86.42 88.39 88.16 88.32 50.85"
P19-1336,W14-1609,0,0.0162528,"or methods, we do not use additional hand-crafted features and do not use cross-lingual word embeddings as well as pre-trained language models (Peters et al., 2018; Radford, 2018; Akbik et al., 2018; Devlin et al., 2018) when addressing the crosslanguage tasks. 2 Related Work Named Entity Recognition NER is typically framed as a sequence labeling task which aims at automatic detection of named entities (e.g., person, organization, location and etc.) from free text (Marrero et al., 2013). The early works applied CRF, SVM, and perception models with handcrafted features (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). With the advent of deep learning, research focus has been shifting towards deep neural networks (DNN), which requires little feature engineering and domain knowledge (Lample et al., 2016; Zukov Gregoric et al., 2018; Zhou et al., 2019). (Collobert et al., 2011) proposed a feed-forward neural network with a fixed sized window for each word, which failed in considering useful relations between long-distance words. To overcome this limitation, (Chiu and Nichols, 2016) presented a bidirectional LSTM-CNNs architecture that automatically detects word- and character-level feature"
P19-1336,N18-1202,0,0.318651,"mples can be paid more attention to. In addition, we create adversarial samples to conduct the Adversarial Training (AT), further improving the generalization and alleviating over-fitting problem. We unify two kinds of adversarial learning, i.e., GRAD and AT, into one transfer learning model, termed Dual Adversarial Transfer Network (DATNet), to achieve end-toend training and obtain significant improvements on a series of NER tasks In contrast with prior methods, we do not use additional hand-crafted features and do not use cross-lingual word embeddings as well as pre-trained language models (Peters et al., 2018; Radford, 2018; Akbik et al., 2018; Devlin et al., 2018) when addressing the crosslanguage tasks. 2 Related Work Named Entity Recognition NER is typically framed as a sequence labeling task which aims at automatic detection of named entities (e.g., person, organization, location and etc.) from free text (Marrero et al., 2013). The early works applied CRF, SVM, and perception models with handcrafted features (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). With the advent of deep learning, research focus has been shifting towards deep neural networks (DNN), which requires littl"
P19-1336,W09-1119,0,0.131653,"sks In contrast with prior methods, we do not use additional hand-crafted features and do not use cross-lingual word embeddings as well as pre-trained language models (Peters et al., 2018; Radford, 2018; Akbik et al., 2018; Devlin et al., 2018) when addressing the crosslanguage tasks. 2 Related Work Named Entity Recognition NER is typically framed as a sequence labeling task which aims at automatic detection of named entities (e.g., person, organization, location and etc.) from free text (Marrero et al., 2013). The early works applied CRF, SVM, and perception models with handcrafted features (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). With the advent of deep learning, research focus has been shifting towards deep neural networks (DNN), which requires little feature engineering and domain knowledge (Lample et al., 2016; Zukov Gregoric et al., 2018; Zhou et al., 2019). (Collobert et al., 2011) proposed a feed-forward neural network with a fixed sized window for each word, which failed in considering useful relations between long-distance words. To overcome this limitation, (Chiu and Nichols, 2016) presented a bidirectional LSTM-CNNs architecture that automatically detects word- and ch"
P19-1336,P17-1194,0,0.0291761,"learning approach based on deep hierarchical recurrent neural network, where full/partial hidden features between source and target tasks are shared. (Al-Rfou’ et al., 2015) built massive multilingual annotators with minimal human expertise by using language agnostic techniques. (Cotterell and Duh, 2017) proposed character-level neural CRFs to jointly train and predict low- and highresource languages. (Pan et al., 2017) proposes a large-scale cross-lingual named entity dataset which contains 282 languages for evaluation. In addition, multi-task learning (Yang et al., 2016; Luong et al., 2016; Rei, 2017; Aguilar et al., 2017; Hashimoto et al., 2017; Lin et al., 2018) shows that jointly training on multiple tasks/languages helps improve performance. Different from transfer learning methods, multi-task learning aims at improving the performance of all the resources instead of low resource only. 3462 GRAD Source CRF Layer Self-Attention Target CRF Layer concat Gradient Reversal concat Gradient Reversal Source Bi-LSTM Shared Bi-LSTM Target Bi-LSTM Shared Bidirectional LSTM CRF Layer Bidirectional LSTM concat concat Char CNN ηwS Word Emb + Shared Char CNN (a) Base Model Self-Attention concat + So"
P19-1336,N18-1027,0,0.0130892,"e languages (Yarowsky et al., 2001; Chen et al., 2010; Li et al., 2012; Feng et al., 2018). For example, (Chen et al., 2010) and (Feng et al., 2018) proposed to jointly identify and align bilingual named entities. Ni et al. (Ni and Florian, 2016; Ni et al., 2017) utilized the Wikipedia entity type mappings to improve low-resource NER. (Mayhew et al., 2017) created a cross-language NER system, which works well for very minimal resources by translate annotated data of high-resource into lowresource. On the other hand, the shared representation methods do not require the parallel correspondence (Rei and Søgaard, 2018). For instance, (Fang and Cohn, 2017) proposed cross-lingual word embeddings to transfer knowledge across resources. (Yang et al., 2017) presented a transfer learning approach based on deep hierarchical recurrent neural network, where full/partial hidden features between source and target tasks are shared. (Al-Rfou’ et al., 2015) built massive multilingual annotators with minimal human expertise by using language agnostic techniques. (Cotterell and Duh, 2017) proposed character-level neural CRFs to jointly train and predict low- and highresource languages. (Pan et al., 2017) proposes a large-s"
P19-1336,D17-1035,0,0.0172143,"Hovy, 2016) for NER task, as shown in Figure 1(a). 3.1 Character-level Encoder Previous works have shown that character features can boost sequence labeling performance by capturing morphological and semantic information (Lin et al., 2018). For low-resource dataset to obtain high-quality word features, character features learned from other language/domain may provide crucial information for labeling, especially for rare and out-of-vocabulary words. Characterlevel encoder usually contains BiLSTM (Lample et al., 2016) and CNN (Chiu and Nichols, 2016; Ma and Hovy, 2016) approaches. In practice, (Reimers and Gurevych, 2017) observed that the difference between the two approaches is statistically insignificant in sequence labeling tasks, but character-level CNN is more efficient and has less parameters. Thus, we use character-level CNN and share character features between high- and low-resource tasks to enhance the representations of low-resource. 3.2 Word-level Encoder To learn a better word-level representation, we concatenate the character-level features of each word with a latent word embedding as wi = [wichar , wiemb ], where the latent word embedding wiemb is initialized with pre-trained embeddings and fixe"
P19-1336,H01-1035,0,0.0185224,"u et al. (2018) proposed task-aware neural language model termed LM-LSTM-CRF, where character-aware neural language models were incorporated to extract character-level embedding under a multi-task framework. Transfer Learning for NER Transfer learning can be a powerful tool to low resource NER tasks. To bridge high and low resource, transfer learning methods for NER can be divided into two types: the parallel corpora based and the shared representation based transfer. Early works mainly focused on exploiting parallel corpora to project information between the high- and low-resource languages (Yarowsky et al., 2001; Chen et al., 2010; Li et al., 2012; Feng et al., 2018). For example, (Chen et al., 2010) and (Feng et al., 2018) proposed to jointly identify and align bilingual named entities. Ni et al. (Ni and Florian, 2016; Ni et al., 2017) utilized the Wikipedia entity type mappings to improve low-resource NER. (Mayhew et al., 2017) created a cross-language NER system, which works well for very minimal resources by translate annotated data of high-resource into lowresource. On the other hand, the shared representation methods do not require the parallel correspondence (Rei and Søgaard, 2018). For instan"
P19-1336,N16-1029,0,0.0165901,"which suffered from limited performance in practice. Current methods are devoted to developing learning based algorithms, especially neural network based methods, and have been advancing the state-of-the-art progressively (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016). These end-to-end models generalize well on new entities based on features automatically learned from the data. However, when † ‡ The first two authors contributed equally. Corresponding author. the annotated corpora is small, especially in the low resource scenario (Zhang et al., 2016), the performance of these methods degrades significantly since the hidden feature representations cannot be learned adequately. Recently, more and more approaches have been proposed to address low-resource NER. Early works (Chen et al., 2010; Li et al., 2012) primarily assumed a large parallel corpus and focused on exploiting them to project information from high- to low-resource. Unfortunately, such a large parallel corpus may not be available for many low-resource languages. More recently, crossresource word embedding (Fang and Cohn, 2017; Adams et al., 2017; Yang et al., 2017) was proposed"
P19-1336,P18-2012,0,0.0139426,"ng the crosslanguage tasks. 2 Related Work Named Entity Recognition NER is typically framed as a sequence labeling task which aims at automatic detection of named entities (e.g., person, organization, location and etc.) from free text (Marrero et al., 2013). The early works applied CRF, SVM, and perception models with handcrafted features (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). With the advent of deep learning, research focus has been shifting towards deep neural networks (DNN), which requires little feature engineering and domain knowledge (Lample et al., 2016; Zukov Gregoric et al., 2018; Zhou et al., 2019). (Collobert et al., 2011) proposed a feed-forward neural network with a fixed sized window for each word, which failed in considering useful relations between long-distance words. To overcome this limitation, (Chiu and Nichols, 2016) presented a bidirectional LSTM-CNNs architecture that automatically detects word- and character-level features. Ma and Hovy (2016) further extended it into bidirectional LSTM-CNNs-CRF architecture, where the CRF module was added to optimize the output label sequence. Liu et al. (2018) proposed task-aware neural language model termed LM-LSTM-CR"
W03-1709,Y98-1021,0,\N,Missing
W03-1709,W02-1808,0,\N,Missing
W03-1709,C02-1055,0,\N,Missing
W03-1709,W02-1817,1,\N,Missing
W03-1709,C02-1080,0,\N,Missing
W03-1709,W02-1815,0,\N,Missing
W03-1709,C02-1012,0,\N,Missing
W03-1709,P97-1041,0,\N,Missing
W03-1709,J00-3004,0,\N,Missing
W05-1507,P97-1003,0,0.0620211,"m number of interacting variables is 4, implying that the algorithmic complexity is O(n4 ) after binarizing the factors cleverly. The intermediate result max [β(B[i, k, h0 ]) · P (A[h] → B[h0 ]C[h])] 0 h ,B A C[h] k can be represented pictorially as i . The same trick works for the second max term in Equation 1. The intermediate result coming from binarizing the second term can be visualized as A[h] → B[h]C[h0 ] or A[h] → B[h0 ]C[h] A depending on which child is the head child that agrees with the parent on head word selection. Bilexical CFG is at the heart of most modern statistical parsers (Collins, 1997; Charniak, 1997), because the statistics associated with word-specific rules are more informative for disambiguation purposes. If we use A[i, j, h] to represent a lexicalized constituent, β(·) to represent the Viterbi score function applicable to any constituent, and P (·) to represent the rule probability function applicable to any rule, Figure 2 shows the equation for the dynamic programming computation of the Viterbi parse. The two terms of the outermost max operator are symmetric cases for heads coming from left and right. Containing five free variables i,j,k,h0 ,h, ranging over 1 to n, t"
W05-1507,P99-1059,0,0.0890971,"Bilexical Parsing A traditional CFG generates words at the bottom of a parse tree and uses nonterminals as abstract representations of substrings to build higher level tree nodes. Nonterminals can be made more specific to the actual substrings they are covering by associating a representative word from the nonterminal’s yield. When the maximum number of lexicalized nonterminals in any rule is two, a CFG is bilexical. A typical bilexical CFG in Chomsky normal form has two types of rule templates: instantiated in n5 possible ways, implying that the complexity of the parsing algorithm is O(n5 ). Eisner and Satta (1999) pointed out we don’t have to enumerate k and h0 simultaneously. The trick, shown in mathematical form in Figure 2 (bottom) is very simple. When maximizing over h0 , j is irrelevant. After getting the intermediate result of maximizing over h0 , we have one less free variable than before. Throughout the two steps, the maximum number of interacting variables is 4, implying that the algorithmic complexity is O(n4 ) after binarizing the factors cleverly. The intermediate result max [β(B[i, k, h0 ]) · P (A[h] → B[h0 ]C[h])] 0 h ,B A C[h] k can be represented pictorially as i . The same trick works"
W05-1507,N03-1021,0,0.0689172,"se A[i, j, h] to represent a lexicalized constituent, β(·) to represent the Viterbi score function applicable to any constituent, and P (·) to represent the rule probability function applicable to any rule, Figure 2 shows the equation for the dynamic programming computation of the Viterbi parse. The two terms of the outermost max operator are symmetric cases for heads coming from left and right. Containing five free variables i,j,k,h0 ,h, ranging over 1 to n, the length of input sentence, both terms can be 67 B[h] k j. The shape of the intermediate results gave rise to the nickname of “hook”. Melamed (2003) discussed the applicability of the hook trick for parsing bilexical multitext grammars. The analysis of the hook trick in this section shows that it is essentially an algebraic manipulation. We will formulate the ITG Viterbi decoding algorithm in a dynamic programming equation in the following section and apply the same algebraic manipulation to produce hooks that are suitable for ITG decoding. 4 Hook Trick for ITG Decoding We start from the bigram case, in which each decoding constituent keeps a left boundary word and X X [Y u11 u12 &lt;Y Z] v11 v12 u21 u22 s S u21 u22 v21 v22 t Z> v21 v22 u11"
W05-1507,P96-1021,0,0.532503,"g the chart with an item for each possible translation of each foreign word in f , and then applying ITG rules from the bottom up. However, ITG’s independence assumptions are too strong to use the ITG probability alone for machine translation. In particular, the context-free assumption that each foreign word’s translation is chosen independently will lead to simply choosing each foreign word’s single most probable English translation with no reordering. In practice it is beneficial to combine the probability given by ITG with a local m-gram language model for English: and q e 2.1 ITG Decoding Wu (1996) presented a polynomial-time algorithm for decoding ITG combined with an m-gram lan66 e∗ = argmax max P (e, f, q)Plm (e)α e q with some constant language model weight α. The language model will lead to more fluent output by influencing both the choice of English words and the reordering, through the choice of straight or inverted rules. While the use of a language model complicates the CKY-based algorithm for finding the best translation, a dynamic programming solution is still possible. We extend the algorithm by storing in each chart item the English boundary words that will affect the m-gra"
W05-1507,J97-3002,0,0.829471,"en translation and monolingual parsing with lexicalized grammars. Chart items in translation must be augmented with words from the output language in order to capture language model state. This can be thought of as a form of lexicalization with some similarity to that of head-driven lexicalized grammars, despite being unrelated to any notion of syntactic head. We show 1 We speak of m-gram language models to avoid confusion with n, which here is the length of the input sentence for translation. Machine Translation using Inversion Transduction Grammar The Inversion Transduction Grammar (ITG) of Wu (1997) is a type of context-free grammar (CFG) for generating two languages synchronously. To model the translational equivalence within a sentence pair, ITG employs a synchronous rewriting mechanism to relate two sentences recursively. To deal with the syntactic divergence between two languages, ITG allows the inversion of rewriting order going from one language to another at any recursive level. ITG in Chomsky normal form consists of unary production rules that are responsible for generating word pairs: X → e/f X → e/ X → /f where e is a source language word, f is a foreign language word, and"
W06-1627,J93-2003,0,0.00603012,"ents are possible, and these constraints have been shown to be a good match for real bitext data (Zens and Ney, 2003). A major motivation for the introduction of ITG was the existence of polynomial-time algorithms both for alignment and translation. Alignment, whether for training a translation model using EM or for finding the Viterbi alignment of test data, is O(n6 ) (Wu, 1997), while translation (decoding) is O(n7 ) using a bigram language model, and O(n11 ) with trigrams. While polynomial-time algorithms are a major improvement over the NPcomplete problems posed by the alignment models of Brown et al. (1993), the degree of these polyno2 Inversion Transduction Grammar An Inversion Transduction Grammar can generate pairs of sentences in two languages by recursively applying context-free bilingual production rules. Most work on ITG has focused on the 2-normal form, which consists of unary production rules that are responsible for generating word pairs: X → e/f 224 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 224–231, c Sydney, July 2006. 2006 Association for Computational Linguistics source node to the destination node. The cost in this s"
W06-1627,W98-1115,0,0.0284701,"anslation decoding. We also combine the dynamic programming hook trick with A* search for decoding. These techniques make it possible to find optimal alignments much more quickly, and make it possible to find optimal translations for the first time. Even in the presence of pruning, we are able to achieve higher BLEU scores with the same amount of computation. 1 Our search heuristics are a conservative estimate of the outside probability of a bitext cell in the complete synchronous parse. Some estimate of this outside probability is a common element of modern statistical (monolingual) parsers (Charniak et al., 1998; Collins, 1999), and recent work has developed heuristics that are admissible for A* search, guaranteeing that the optimal parse will be found (Klein and Manning, 2003). We extend this type of outside probability estimate to include both word translation and n-gram language model probabilities. These measures have been used to guide search in word- or phrase-based MT systems (Wang and Waibel, 1997; Och et al., 2001), but in such models optimal search is generally not practical even with good heuristics. In this paper, we show that the same assumptions that make ITG polynomial-time can be used"
W06-1627,P99-1059,0,0.0376819,"abilities. Figure 2 is the picture of the outside translations and bigrams of a particular translation hypothesis X[i, j, u, v]. Our heuristic involves precomputing two values for each word in the input string, involving forward- and backward-looking language model probabilities. For the forward looking value hf at input position n, we take a maximum over the set of words Sn that the input word tn can be translated as:   0 hf (n) = max Pt (s |tn ) max Plm (s |s) 0 4.2 Combining the Hook Trick with A* The hook trick is a factorization technique for dynamic programming. For bilexical parsing, Eisner and Satta (1999) pointed out we can reduce the complexity of parsing from O(n5 ) to O(n4 ) by combining the non-head constituents with the bilexical rules first, and then combining the resultant hook constituents with the head constituents. By doing so, the maximal number of interactive variables ranging over n is reduced from 5 to 4. For ITG decoding, we can apply a similar factors∈Sn s ∈S ization trick. We describe the bigram-integrated where: [ decoding case here, and refer to Huang et al. S= Sn (2005) for more detailed discussion. Figure 3 n shows how to decompose the expression for the is the set of all"
W06-1627,W05-1507,1,0.909662,"Missing"
W06-1627,W01-1408,0,0.045366,"side probability of a bitext cell in the complete synchronous parse. Some estimate of this outside probability is a common element of modern statistical (monolingual) parsers (Charniak et al., 1998; Collins, 1999), and recent work has developed heuristics that are admissible for A* search, guaranteeing that the optimal parse will be found (Klein and Manning, 2003). We extend this type of outside probability estimate to include both word translation and n-gram language model probabilities. These measures have been used to guide search in word- or phrase-based MT systems (Wang and Waibel, 1997; Och et al., 2001), but in such models optimal search is generally not practical even with good heuristics. In this paper, we show that the same assumptions that make ITG polynomial-time can be used to efficiently compute heuristics which guarantee us that we will find the optimal alignment or translation, while significantly speeding the search. Introduction The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The algorithm builds a synchronous parse tree for both sente"
W06-1627,P97-1047,0,0.028531,"ive estimate of the outside probability of a bitext cell in the complete synchronous parse. Some estimate of this outside probability is a common element of modern statistical (monolingual) parsers (Charniak et al., 1998; Collins, 1999), and recent work has developed heuristics that are admissible for A* search, guaranteeing that the optimal parse will be found (Klein and Manning, 2003). We extend this type of outside probability estimate to include both word translation and n-gram language model probabilities. These measures have been used to guide search in word- or phrase-based MT systems (Wang and Waibel, 1997; Och et al., 2001), but in such models optimal search is generally not practical even with good heuristics. In this paper, we show that the same assumptions that make ITG polynomial-time can be used to efficiently compute heuristics which guarantee us that we will find the optimal alignment or translation, while significantly speeding the search. Introduction The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The algorithm builds a synchronous parse"
W06-1627,P96-1021,0,0.116518,"n from the null target string of [i, i] into source language words as many times as necessary, the decoder can translate an input sentence into a longer output sentence. When there is the null symbol in the bag of candidate words, the decoder can choose to translate a word into null to decrease the output length. Both insertions and deletions are special cases of the bitext parsing items. nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in O(n4 ) time (Zhang and Gildea, 2005). 4 A* Decoding The of ITG decoding algorithm of Wu (1996) can be viewed as a variant of the Viterbi parsing algorithm for alignment selection. The task of standard alignment is to find word level links between two fixed-order strings. In the decoding situation, while the input side is a fixed sequence of words, the output side is a bag of words to be linked with the input words and then reordered. Under the ITG constraint, if the target language substring [i, j] is translated into s1 in the source language and the target substring [j, k] is translated into s2 , then s1 and s2 must be consecutive in the source language as well and two possible orderi"
W06-1627,J97-3002,0,0.524612,"ility estimate to include both word translation and n-gram language model probabilities. These measures have been used to guide search in word- or phrase-based MT systems (Wang and Waibel, 1997; Och et al., 2001), but in such models optimal search is generally not practical even with good heuristics. In this paper, we show that the same assumptions that make ITG polynomial-time can be used to efficiently compute heuristics which guarantee us that we will find the optimal alignment or translation, while significantly speeding the search. Introduction The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The algorithm builds a synchronous parse tree for both sentences, and assumes that the trees have the same underlying structure but that the ordering of constituents may differ in the two languages. ITG imposes constraints on which alignments are possible, and these constraints have been shown to be a good match for real bitext data (Zens and Ney, 2003). A major motivation for the introduction of ITG was the existence of polynomial-time algorithms both"
W06-1627,P03-1019,0,0.0398662,"translation, while significantly speeding the search. Introduction The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The algorithm builds a synchronous parse tree for both sentences, and assumes that the trees have the same underlying structure but that the ordering of constituents may differ in the two languages. ITG imposes constraints on which alignments are possible, and these constraints have been shown to be a good match for real bitext data (Zens and Ney, 2003). A major motivation for the introduction of ITG was the existence of polynomial-time algorithms both for alignment and translation. Alignment, whether for training a translation model using EM or for finding the Viterbi alignment of test data, is O(n6 ) (Wu, 1997), while translation (decoding) is O(n7 ) using a bigram language model, and O(n11 ) with trigrams. While polynomial-time algorithms are a major improvement over the NPcomplete problems posed by the alignment models of Brown et al. (1993), the degree of these polyno2 Inversion Transduction Grammar An Inversion Transduction Grammar can"
W06-1627,J03-4003,0,\N,Missing
W06-1627,P05-1059,1,\N,Missing
W06-1627,N03-1016,0,\N,Missing
W07-0404,P05-1033,0,0.0886606,"al. (2006) discuss methods for binarizing SCFGs, ignoring the nonbinarizable grammars; in Section 2 we discuss the generalized problem of factoring to k-ary grammars for any k and formalize the problem as permutation factorization in Section 3. Introduction A number of recent syntax-based approaches to statistical machine translation make use of Synchronous Context Free Grammar (SCFG) as the underlying model of translational equivalence. Wu (1997)’s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho In Section 4, we describe an O(k · n) left-toright shift-reduce algorithm for analyzing permutations that can be k-arized. Its time complexity becomes O(n2 ) when k is not specified beforehand and the minimal k is to be discovered. Instead of linearly shifting in one number at a time, Gildea et al. (2006) employ a balanced binary tree as the control structure, produc"
W07-0404,N04-1035,0,0.0767373,"ear in the rules. Zhang et al. (2006) discuss methods for binarizing SCFGs, ignoring the nonbinarizable grammars; in Section 2 we discuss the generalized problem of factoring to k-ary grammars for any k and formalize the problem as permutation factorization in Section 3. Introduction A number of recent syntax-based approaches to statistical machine translation make use of Synchronous Context Free Grammar (SCFG) as the underlying model of translational equivalence. Wu (1997)’s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho In Section 4, we describe an O(k · n) left-toright shift-reduce algorithm for analyzing permutations that can be k-arized. Its time complexity becomes O(n2 ) when k is not specified beforehand and the minimal k is to be discovered. Instead of linearly shifting in one number at a time, Gildea et al. (2006) employ a balanced binary tree as the contro"
W07-0404,P06-2036,1,0.402249,"anslation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho In Section 4, we describe an O(k · n) left-toright shift-reduce algorithm for analyzing permutations that can be k-arized. Its time complexity becomes O(n2 ) when k is not specified beforehand and the minimal k is to be discovered. Instead of linearly shifting in one number at a time, Gildea et al. (2006) employ a balanced binary tree as the control structure, producing an algorithm similar in spirit to merge-sort with a reduced time complexity of O(n log n). However, both algorithms rely on reduction tests on emerging spans which involve redundancies with the spans that have already been tested. 25 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 25–32, c Rochester, New York, April 2007. 2007 Association for Computational Linguistics Uno and Yagiura (2000) describe a clever algorithm for the problem of finding all common intervals o"
W07-0404,H05-1101,0,0.0898736,"les as much as possible. This paper focuses on converting an SCFG to the equivalent grammar with smallest possible maximum rule size. The algorithm processes each rule in the input grammar independently, and determines whether the rule can be factored into smaller SCFG rules by analyzing the rule’s permutation π. As an example, given the input rule: [ X → A(1) B (2) C (3) D(4) E (5) F (6) G(7) , X → E (5) G(7) D(4) F (6) C (3) A(1) B (2) ] we consider the associated permutation: We begin by describing the synchronous CFG formalism, which is more rigorously defined by Aho and Ullman (1972) and Satta and Peserico (2005). We adopt the SCFG notation of Satta and Peserico (2005). Superscript indices in the right-hand side of grammar rules: (5, 7, 4, 6, 3, 1, 2) We determine that this permutation can be factored into the following permutation tree: (2,1) (π(1)) (1,2) (2,1) (2,4,1,3) (1) (1) 3 1 2 (π(n)) X → X1 ...Xn(n) , Xπ(1) ...Xπ(n) 5 indicate that the nonterminals with the same index are linked across the two languages, and will eventually be rewritten by the same rule application. Each Xi is a variable which can take the value of any nonterminal in the grammar. We say an SCFG is n-ary if and only if the max"
W07-0404,P06-1123,0,0.711946,"hop on Syntax and Structure in Statistical Translation, pages 25–32, c Rochester, New York, April 2007. 2007 Association for Computational Linguistics Uno and Yagiura (2000) describe a clever algorithm for the problem of finding all common intervals of two permutations in time O(n + K), where K is the number of common intervals, which can itself be Ω(n2 ). In Section 5, we adapt their approach to the problem of factoring SCFGs, and show that, given this problem definition, running time can be improved to O(n), the optimum given the time needed to read the input permutation. The methodology in Wellington et al. (2006) measures the complexity of word alignment using the number of gaps that are necessary for their synchronous parser which allows discontinuous spans to succeed in parsing. In Section 6, we provide a more direct measurement using the minimal branching factor yielded by the permutation factorization algorithm. 2 Synchronous CFG and Synchronous Parsing n-ary SCFG, the parsing complexity can be as high as O(N n+4 ). The reason is even if we binarize on one side to maintain 3 indices, for many unfriendly permutations, at most n + 1 boundary variables in the other language are necessary. The fact th"
W07-0404,J97-3002,0,0.927362,"with maximum rule length n into a simpler grammar with a maximum of k nonterminals in any one rule, if not all n! permutations appear in the rules. Zhang et al. (2006) discuss methods for binarizing SCFGs, ignoring the nonbinarizable grammars; in Section 2 we discuss the generalized problem of factoring to k-ary grammars for any k and formalize the problem as permutation factorization in Section 3. Introduction A number of recent syntax-based approaches to statistical machine translation make use of Synchronous Context Free Grammar (SCFG) as the underlying model of translational equivalence. Wu (1997)’s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho In Section 4, we describe an O(k · n) left-toright shift-reduce algorithm for analyzing permutations that can be k-arized. Its time complexity becomes O(n2 ) when k is not specified beforehand and the minimal"
W07-0404,P01-1067,0,0.0514932,"ot all n! permutations appear in the rules. Zhang et al. (2006) discuss methods for binarizing SCFGs, ignoring the nonbinarizable grammars; in Section 2 we discuss the generalized problem of factoring to k-ary grammars for any k and formalize the problem as permutation factorization in Section 3. Introduction A number of recent syntax-based approaches to statistical machine translation make use of Synchronous Context Free Grammar (SCFG) as the underlying model of translational equivalence. Wu (1997)’s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (2005) all fall into this category. A crucial question for efficient computation in approaches based on SCFG is the length of the grammar rules. Grammars with longer rules can represent a larger set of reorderings between languages (Aho In Section 4, we describe an O(k · n) left-toright shift-reduce algorithm for analyzing permutations that can be k-arized. Its time complexity becomes O(n2 ) when k is not specified beforehand and the minimal k is to be discovered. Instead of linearly shifting in one number at a time, Gildea et al. (2006) employ a balanced bin"
W07-0404,N06-1033,1,0.868868,"related problem of finding all common intervals of two permutations, we achieve a linear time algorithm for the permutation factorization problem. We also use the algorithm to analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs. 1 However, parsing complexity depends not only on rule length, but also on the specific permutations represented by the individual rules. It may be possible to factor an SCFG with maximum rule length n into a simpler grammar with a maximum of k nonterminals in any one rule, if not all n! permutations appear in the rules. Zhang et al. (2006) discuss methods for binarizing SCFGs, ignoring the nonbinarizable grammars; in Section 2 we discuss the generalized problem of factoring to k-ary grammars for any k and formalize the problem as permutation factorization in Section 3. Introduction A number of recent syntax-based approaches to statistical machine translation make use of Synchronous Context Free Grammar (SCFG) as the underlying model of translational equivalence. Wu (1997)’s Inversion Transduction Grammar, as well as tree-transformation models of translation such as Yamada and Knight (2001), Galley et al. (2004), and Chiang (200"
W10-4168,H05-1097,0,0.0601431,"Missing"
W10-4168,P95-1026,0,0.398386,"Missing"
W10-4168,P96-1006,0,0.281134,"Missing"
W10-4168,P04-3026,0,0.0410563,"Missing"
W10-4168,J98-1004,0,\N,Missing
W15-3118,P11-1052,0,0.0346496,"ummaries of news items in each category sorted according to PopuRank together with a picture, if there is any, on http://www.kuaiwenbao.com and mobile apps. We will describe in this paper the system architecture of KWB, the data crawler structure, the functionalities of the central database, and the definition of PopuRank. We will show, through experiments, the running time of obtaining PopuRank. We will also demonstrate the use of KWB. 1 2 Related Work 2.1 Web crawling Web-crawling technologies are important mechanisms for collecting data from the Internet (see, e.g., (Emamdadi et al., 2014; Lin and Bilmes, 2011; Li et al., 2011; Li et al., 2009; Li et al., 2009; Li and Teng, 2010; Zheng et al., 2008)). The general framework of a crawling is given below: Introduction We are living in the era of information explosion. To help people obtain information quickly, we would want to construct an automated system that collects information and provides accurate summarization to the user in a timely fashion. This would be a system that integrates advanced technologies and current research results on text automation, including data collection, storage, classification, ranking, summarization, web displaying, and"
W15-3118,P13-1100,0,0.0434212,"Missing"
W18-5516,D15-1075,0,0.0681706,"an entity. Candidate article search: We use the MediaWiki API3 to search through the titles of all Wikipedia articles for matches with the potential entity mentions found in the claim. The MediaWiki API uses the Wikipedia search engine to find matching articles. The top match is the article whose title has the largest overlap with the query. For each entity mention, we store the seven highest-ranked Wikipedia article matches. The MediaWiki API uses the online version of Wikipedia and since there are some discrepancies Enhanced Sequential Inference Model Originally developed for the SNLI task (Bowman et al., 2015) of determining entailment between two statements, the ESIM (Enhanced Sequential Inference Model) (Chen et al., 2016) creates a rich representation of statement-pairs. Since the FEVER task requires the handling of claimsentence pairs, we use the ESIM as the basis for both sentence selection and textual entailment. The ESIM solves the entailment problem in three consecutive steps, taking two statements as input. Input encoding: Using a bidirectional LSTM (BiLSTM) (Graves and Schmidhuber, 2005), representations of the individual tokens of the two input statements are computed. Local inference mo"
W18-5516,D07-1074,0,0.117948,"present underlying methods that we adopted for the development of our system. 2.1 3 Entity linking In this section, we describe the models that we developed for the three FEVER sub-tasks. The document retrieval step requires matching a given claim with the content of a Wikipedia article. A claim frequently features one or multiple entities that form the main content of the claim. Furthermore, Wikipedia can be viewed as a knowledge base, where each article describes a particular entity, denoted by the article title. Thus, the document retrieval step can be framed as an entity linking problem (Cucerzan, 2007). That is, identifying entity mentions in the claim and linking them to the Wikipedia articles of this entity. The linked Wikipedia articles can then be used as the set of the retrieved documents for the subsequent steps. 2.2 Our system for fact extraction and claim verification 3.1 Document retrieval As explained in Section 2.1, we propose an entity linking approach to the document retrieval subtask. That is, we find entities in the claims that match the titles of Wikipedia articles (documents). Following the typical entity linking pipeline, we develop a document retrieval component that has"
W18-5516,D14-1162,0,0.0809006,"tiveness of our ad-hoc entity linking approach (see Section 4). 3.2 3.3 Recognizing textual entailment In order to classify the claim as Supported, Ref uted or N otEnoughInf o, we use the five sentences retrieved by our sentence selection model described in the previous section. For the classification, we propose another extension to the ESIM, which can predict the entailment relation between multiple input sentences and the claim. Fig. 2 gives an overview of our extended ESIM for the FEVER textual entailment task. As word representation for both claim and sentences, we concatenate the Glove (Pennington et al., 2014) and FastText (Bojanowski et al., 2016) embeddings for each word. Since both types of embeddings are pretrained on Wikipedia, they are particularly suitable for our problem setting. Sentence selection In this step, we select candidate sentences as a potential evidence set for a claim from the Wikipedia articles retrieved in the previous step. This is achieved by extending the ESIM to generate a ranking score on the basis of two input statements, instead of predicting the entailment relation between these two statements. Architecture: The modified ESIM takes as input a claim and a sentence. To"
W18-5516,S18-2007,1,0.829524,"ference Local Inference Input Encoding Input Encoding claim evidence sentences claim sampled sentences Figure 1: Sentence selection model between the 2017 dump used in the shared task and the latest version, we also perform an exact search over all Wikipedia article titles in the dump. We add these results to the set of the retrieved articles. Candidate filtering: The MediaWiki API retrieves articles whose title overlaps with the query. Thus, the results may contain articles with a title longer or shorter than the entity mention used in the query. Similarly to previous work on entity linking (Sorokin and Gurevych, 2018), we remove results that are longer than the entity mention and do not overlap with the rest of the claim. To check this overlap, we first remove the content in parentheses from the Wikipedia article titles and stem the remaining words in the titles and the claim. Then, we discard a Wikipedia article if its stemmed article title is not completely included in the stemmed claim. We collect all retrieved Wikipedia articles for all identified entity mentions in the claim after filtering and supply them to the next step in the pipeline. The evaluation of the document retrieval system on the develop"
W18-5516,N18-1074,0,0.0717481,"Missing"
