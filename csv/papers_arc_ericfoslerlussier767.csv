2021.naacl-demos.13,{T}ext{E}ssence: A Tool for Interactive Analysis of Semantic Shifts Between Corpora,2021,-1,-1,4,1,804,denis newmangriffis,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations,0,"Embeddings of words and concepts capture syntactic and semantic regularities of language; however, they have seen limited use as tools to study characteristics of different corpora and how they relate to one another. We introduce TextEssence, an interactive system designed to enable comparative analysis of corpora using embeddings. TextEssence includes visual, neighbor-based, and similarity-based modes of embedding analysis in a lightweight, web-based interface. We further propose a new measure of embedding confidence based on nearest neighborhood overlap, to assist in identifying high-quality embeddings for corpus analysis. A case study on COVID-19 scientific literature illustrates the utility of the system. TextEssence can be found at https://textessence.github.io."
2021.acl-long.525,Learning Latent Structures for Cross Action Phrase Relations in Wet Lab Protocols,2021,-1,-1,3,0,13454,chaitanya kulkarni,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Wet laboratory protocols (WLPs) are critical for conveying reproducible procedures in biological research. They are composed of instructions written in natural language describing the step-wise processing of materials by specific actions. This process flow description for reagents and materials synthesis in WLPs can be captured by material state transfer graphs (MSTGs), which encode global temporal and causal relationships between actions. Here, we propose methods to automatically generate a MSTG for a given protocol by extracting all action relationships across multiple sentences. We also note that previous corpora and methods focused primarily on local intra-sentence relationships between actions and entities and did not address two critical issues: (i) resolution of implicit arguments and (ii) establishing long-range dependencies across sentences. We propose a new model that incrementally learns latent structures and is better suited to resolving inter-sentence relations and implicit arguments. This model draws upon a new corpus WLP-MSTG which was created by extending annotations in the WLP corpora for inter-sentence relations and implicit arguments. Our model achieves an F1 score of 54.53{\%} for temporal and causal relations in protocols from our corpus, which is a significant improvement over previous models - DyGIE++:28.17{\%}; spERT:27.81{\%}. We make our annotated WLP-MSTG corpus available to the research community."
2020.sigdial-1.24,How Self-Attention Improves Rare Class Performance in a Question-Answering Dialogue Agent,2020,-1,-1,3,0,14951,adam stiff,Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Contextualized language modeling using deep Transformer networks has been applied to a variety of natural language processing tasks with remarkable success. However, we find that these models are not a panacea for a question-answering dialogue agent corpus task, which has hundreds of classes in a long-tailed frequency distribution, with only thousands of data points. Instead, we find substantial improvements in recall and accuracy on rare classes from a simple one-layer RNN with multi-headed self-attention and static word embeddings as inputs. While much research has used attention weights to illustrate what input is important for a task, the complexities of our dialogue corpus offer a unique opportunity to examine how the model represents what it attends to, and we offer a detailed analysis of how that contributes to improved performance on rare classes. A particularly interesting phenomenon we observe is that the model picks up implicit meanings by splitting different aspects of the semantics of a single word across multiple attention heads."
2020.coling-main.198,Contextualized Embeddings for Enriching Linguistic Analyses on Politeness,2020,-1,-1,2,0,21293,ahmad aljanaideh,Proceedings of the 28th International Conference on Computational Linguistics,0,"Linguistic analyses in natural language processing (NLP) have often been performed around the static notion of words where the context (surrounding words) is not considered. For example, previous analyses on politeness have focused on comparing the use of static words such as personal pronouns across (im)polite requests without taking the context of those words into account. Current word embeddings in NLP do capture context and thus can be leveraged to enrich linguistic analyses. In this work, we introduce a model which leverages the pre-trained BERT model to cluster contextualized representations of a word based on (1) the context in which the word appears and (2) the labels of items the word occurs in. Using politeness as case study, this model is able to automatically discover interpretable, fine-grained context patterns of words, some of which align with existing theories on politeness. Our model further discovers novel finer-grained patterns associated with (im)polite language. For example, the word please can occur in impolite contexts that are predictable from BERT clustering. The approach proposed here is validated by showing that features based on fine-grained patterns inferred from the clustering improve over politeness-word baselines."
2020.bionlp-1.2,Sequence-to-Set Semantic Tagging for Complex Query Reformulation and Automated Text Categorization in Biomedical {IR} using Self-Attention,2020,-1,-1,3,0,22226,manirupa das,Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing,0,"Novel contexts, comprising a set of terms referring to one or more concepts, may often arise in complex querying scenarios such as in evidence-based medicine (EBM) involving biomedical literature. These may not explicitly refer to entities or canonical concept forms occurring in a fact-based knowledge source, e.g. the UMLS ontology. Moreover, hidden associations between related concepts meaningful in the current context, may not exist within a single document, but across documents in the collection. Predicting semantic concept tags of documents can therefore serve to associate documents related in unseen contexts, or categorize them, in information filtering or retrieval scenarios. Thus, inspired by the success of sequence-to-sequence neural models, we develop a novel sequence-to-set framework with attention, for learning document representations in a unique unsupervised setting, using no human-annotated document labels or external knowledge resources and only corpus-derived term statistics to drive the training, that can effect term transfer within a corpus for semantically tagging a large collection of documents. Our sequence-to-set modeling approach to predict semantic tags, gives to the best of our knowledge, the state-of-the-art for both, an unsupervised query expansion (QE) task for the TREC CDS 2016 challenge dataset when evaluated on an Okapi BM25{--}based document retrieval system; and also over the MLTM system baseline baseline (Soleimani and Miller, 2016), for both supervised and semi-supervised multi-label prediction tasks on the del.icio.us and Ohsumed datasets. We make our code and data publicly available."
W19-2002,Characterizing the Impact of Geometric Properties of Word Embeddings on Task Performance,2019,47,0,5,0,24739,brendan whitaker,Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for {NLP},0,"Analysis of word embedding properties to inform their use in downstream NLP tasks has largely been studied by assessing nearest neighbors. However, geometric properties of the continuous feature space contribute directly to the use of embedding features in downstream models, and are largely unexplored. We consider four properties of word embedding geometry, namely: position relative to the origin, distribution of features in the vector space, global pairwise distances, and local pairwise distances. We define a sequence of transformations to generate new embeddings that expose subsets of these properties to downstream models and evaluate change in task performance to understand the contribution of each property to NLP models. We transform publicly available pretrained embeddings from three popular toolkits (word2vec, GloVe, and FastText) and evaluate on a variety of intrinsic tasks, which model linguistic information in the vector space, and extrinsic tasks, which use vectors as input to machine learning models. We find that intrinsic evaluations are highly sensitive to absolute position, while extrinsic tasks rely primarily on local similarity. Our findings suggest that future embedding models and post-processing techniques should focus primarily on similarity to nearby points in vector space."
D19-6218,Writing habits and telltale neighbors: analyzing clinical concept usage patterns with sublanguage embeddings,2019,23,0,2,1,804,denis newmangriffis,Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019),0,"Natural language processing techniques are being applied to increasingly diverse types of electronic health records, and can benefit from in-depth understanding of the distinguishing characteristics of medical document types. We present a method for characterizing the usage patterns of clinical concepts among different document types, in order to capture semantic differences beyond the lexical level. By training concept embeddings on clinical documents of different types and measuring the differences in their nearest neighborhood structures, we are able to measure divergences in concept usage while correcting for noise in embedding learning. Experiments on the MIMIC-III corpus demonstrate that our approach captures clinically-relevant differences in concept usage and provides an intuitive way to explore semantic characteristics of clinical document collections."
D19-3015,{HARE}: a Flexible Highlighting Annotator for Ranking and Exploration,2019,0,0,2,1,804,denis newmangriffis,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations,0,"Exploration and analysis of potential data sources is a significant challenge in the application of NLP techniques to novel information domains. We describe HARE, a system for highlighting relevant information in document collections to support ranking and triage, which provides tools for post-processing and qualitative analysis for model development and tuning. We apply HARE to the use case of narrative descriptions of mobility information in clinical data, and demonstrate its utility in comparing candidate embedding features. We provide a web-based interface for annotation visualization and document ranking, with a modular backend to support interoperability with existing annotation tools. Our system is available online at https://github.com/OSU-slatelab/HARE."
W18-3026,Jointly Embedding Entities and Text with Distant Supervision,2018,29,2,3,1,804,denis newmangriffis,Proceedings of The Third Workshop on Representation Learning for {NLP},0,"Learning representations for knowledge base entities and concepts is becoming increasingly important for NLP applications. However, recent entity embedding methods have relied on structured resources that are expensive to create for new domains and corpora. We present a distantly-supervised method for jointly learning embeddings of entities and text from an unnanotated corpus, using only a list of mappings between entities and surface forms. We learn embeddings from open-domain and biomedical corpora, and compare against prior methods that rely on human-annotated text or large knowledge graph structure. Our embeddings capture entity similarity and relatedness better than prior work, both in existing biomedical datasets and a new Wikipedia-based dataset that we release to the community. Results on analogy completion and entity sense disambiguation indicate that entities and words capture complementary information that can be effectively combined for downstream use."
W18-2313,{P}hrase2{V}ec{GLM}: Neural generalized language model{--}based semantic tagging for complex query reformulation in medical {IR},2018,0,2,2,0,22226,manirupa das,Proceedings of the {B}io{NLP} 2018 workshop,0,"In this work, we develop a novel, completely unsupervised, neural language model-based document ranking approach to semantic tagging of documents, using the document to be tagged as a query into the GLM to retrieve candidate phrases from top-ranked related documents, thus associating every document with novel related concepts extracted from the text. For this we extend the word embedding-based general language model due to Ganguly et al 2015, to employ phrasal embeddings, and use the semantic tags thus obtained for downstream query expansion, both directly and in feedback loop settings. Our method, evaluated using the TREC 2016 clinical decision support challenge dataset, shows statistically significant improvement not only over various baselines that use standard MeSH terms and UMLS concepts for query expansion, but also over baselines using human expert{--}assigned concept tags for the queries, run on top of a standard Okapi BM25{--}based document retrieval system."
W17-2303,Insights into Analogy Completion from the Biomedical Domain,2017,24,4,3,1,804,denis newmangriffis,{B}io{NLP} 2017,0,"Analogy completion has been a popular task in recent years for evaluating the semantic properties of word embeddings, but the standard methodology makes a number of assumptions about analogies that do not always hold, either in recent benchmark datasets or when expanding into other domains. Through an analysis of analogies in the biomedical domain, we identify three assumptions: that of a Single Answer for any given analogy, that the pairs involved describe the Same Relationship, and that each pair is Informative with respect to the other. We propose modifying the standard methodology to relax these assumptions by allowing for multiple correct answers, reporting MAP and MRR in addition to accuracy, and using multiple example pairs. We further present BMASS, a novel dataset for evaluating linguistic regularities in biomedical embeddings, and demonstrate that the relationships described in the dataset pose significant semantic challenges to current word embedding methods."
D17-1302,Cross-Lingual Transfer Learning for {POS} Tagging without Cross-Lingual Resources,2017,17,37,4,1,29309,jookyung kim,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Training a POS tagging model with crosslingual transfer learning usually requires linguistic knowledge and resources about the relation between the source language and the target language. In this paper, we introduce a cross-lingual transfer learning model for POS tagging without ancillary resources such as parallel corpora. The proposed cross-lingual model utilizes a common BLSTM that enables knowledge transfer from other languages, and private BLSTMs for language-specific representations. The cross-lingual model is trained with language-adversarial training and bidirectional language modeling as auxiliary objectives to better represent language-general information while not losing the information about a specific target language. Evaluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language."
W16-2903,"Identification, characterization, and grounding of gradable terms in clinical text",2016,0,4,3,1,12148,chaitanya shivade,Proceedings of the 15th Workshop on Biomedical Natural Language Processing,0,None
W16-1607,Adjusting Word Embeddings with Semantic Intensity Orders,2016,21,5,3,1,29309,jookyung kim,Proceedings of the 1st Workshop on Representation Learning for {NLP},0,None
W15-1520,Neural word embeddings with multiplicative feature interactions for tensor-based compositions,2015,19,1,3,1,29309,jookyung kim,Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,0,"Categorical compositional distributional models unify compositional formal semantic models and distributional models by composing phrases with tensor-based methods from vector representations. For the tensor-based compositions, Milajevs et al. (2014) showed that word vectors obtained from the continuous bag-of-words (CBOW) model are competitive with those from co-occurrence based models. However, because word vectors from the CBOW model are trained assuming additive interactions between context words, the word composition used for the training mismatches to the tensor-based methods used for evaluating the actual compositions including pointwise multiplication and tensor product of context vectors. In this work, we show whether the word embeddings from extended CBOW models using multiplication or tensor product between context words, reflecting the actual composition methods, can show better performance than those from the baseline CBOW model in actual tasks of compositions with multiplication or tensor-based methods."
W15-1305,Extending {N}eg{E}x with Kernel Methods for Negation Detection in Clinical Text,2015,21,7,3,1,12148,chaitanya shivade,Proceedings of the Second Workshop on Extra-Propositional Aspects of Meaning in Computational Semantics ({E}x{P}ro{M} 2015),0,"NegEx is a popular rule-based system used to identify negated concepts in clinical notes. This system has been reported to perform very well by numerous studies in the past. In this paper, we demonstrate the use of kernel methods to extend the performance of NegEx. A kernel leveraging the rules of NegEx and its output as features, performs as well as the rule-based system. An improvement in performance is achieved if this kernel is coupled with a bag of words kernel. Our experiments show that kernel methods outperform the rule-based system, when evaluated within and across two different open datasets. We also present the results of a semi-supervised approach to the problem, which improves performance on the data."
W15-0611,Interpreting Questions with a Log-Linear Ranking Model in a Virtual Patient Dialogue System,2015,23,4,4,0,7108,evan jaffe,Proceedings of the Tenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We present a log-linear ranking model for interpreting questions in a virtual patient dialogue system and demonstrate that it substantially outperforms a more typical multiclass classifier model using the same information. The full model makes use of weighted and concept-based matching features that together yield a 15% error reduction over a strong lexical overlap baseline. The accuracy of the ranking model approaches that of an extensively handcrafted pattern matching system, promising to reduce the authoring burden and make it possible to use confidence estimation in choosing dialogue acts; at the same time, the effectiveness of the concept-based features indicates that manual development resources can be productively employed with the approach in developing concept hierarchies."
N15-1051,Corpus-based discovery of semantic intensity scales,2015,21,1,3,1,12148,chaitanya shivade,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Gradable terms such as brief, lengthy and extended illustrate varying degrees of a scale and can therefore participate in comparative constructs. Knowing the set of words that can be compared on the same scale and the associated ordering between them (brief < lengthy < extended) is very useful for a variety of lexical semantic tasks. Current techniques to derive such an ordering rely on WordNet to determine which words belong on the same scale and are limited to adjectives. Here we describe an extension to recent work: we investigate a fully automated pipeline to extract gradable terms from a corpus, group them into clusters reflecting the same scale and establish an ordering among them. This methodology reduces the amount of required handcrafted knowledge, and can infer gradability of words independent of their part of speech. Our approach infers an ordering for adjectives with comparable performance to previous work, but also for adverbs with an accuracy of 71%. We find that the technique is useful for inferring such rankings among words across different domains, and present an example using biomedical text."
P14-1094,Cross-narrative Temporal Ordering of Medical Events,2014,35,10,2,1,12144,preethi raghavan,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Cross-narrative temporal ordering of medical events is essential to the task of generating a comprehensive timeline over a patientxe2x80x99s history. We address the problem of aligning multiple medical event sequences, corresponding to different clinical narratives, comparing the following approaches: (1) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding, and (2) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms. The cross-narrative coreference and temporal relation weights used in both these approaches are learned from a corpus of clinical narratives. We present results using both approaches and observe that the finite state transducer approach performs performs significantly better than the dynamic programming one by 6.8% for the problem of multiple-sequence alignment."
W12-2404,Temporal Classification of Medical Events,2012,14,11,2,1,12144,preethi raghavan,{B}io{NLP}: Proceedings of the 2012 Workshop on Biomedical Natural Language Processing,0,"We investigate the task of assigning medical events in clinical narratives to discrete time-bins. The time-bins are defined to capture when a medical event occurs relative to the hospital admission date in each clinical narrative. We model the problem as a sequence tagging task using Conditional Random Fields. We extract a combination of lexical, section-based and temporal features from medical events in each clinical narrative. The sequence tagging system outperforms a system that does not utilize any sequence information modeled using a Maximum Entropy classifier. We present results with both hand-tagged as well as automatically extracted features. We observe over 8% improvement in overall tagging accuracy with the inclusion of sequence information."
W12-2208,Comparing human versus automatic feature extraction for fine-grained elementary readability assessment,2012,12,3,3,0,30423,yi ma,Proceedings of the First Workshop on Predicting and Improving Text Readability for target reader populations,0,"Early primary children's literature poses some interesting challenges for automated readability assessment: for example, teachers often use fine-grained reading leveling systems for determining appropriate books for children to read (many current systems approach readability assessment at a coarser whole grade level). In previous work (Ma et al., 2012), we suggested that the fine-grained assessment task can be approached using a ranking methodology, and incorporating features that correspond to the visual layout of the page improves performance. However, the previous methodology for using found text (e.g., scanning in a book from the library) requires human annotation of the text regions and correction of the OCR text. In this work, we ask whether the annotation process can be automated, and also experiment with richer syntactic features found in the literature that can be automatically derived from either the human-corrected or raw OCR text. We find that automated visual and text feature extraction work reasonably well and can allow for scaling to larger datasets, but that in our particular experiments the use of syntactic features adds little to the performance of the system, contrary to previous findings."
P12-2014,Learning to Temporally Order Medical Events in Clinical Text,2012,16,11,3,1,12144,preethi raghavan,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We investigate the problem of ordering medical events in unstructured clinical narratives by learning to rank them based on their time of occurrence. We represent each medical event as a time duration, with a corresponding start and stop, and learn to rank the starts/stops based on their proximity to the admission date. Such a representation allows us to learn all of Allen's temporal relations between medical events. Interestingly, we observe that this methodology performs better than a classification-based approach for this domain, but worse on the relationships found in the Timebank corpus. This finding has important implications for styles of data representation and resources used for temporal relation learning: clinical narratives may have different language attributes corresponding to temporal ordering relative to Timebank, implying that the field may need to look at a wider range of domains to fully understand the nature of temporal ordering."
N12-1063,Ranking-based readability assessment for early primary children{'}s literature,2012,11,16,2,0,30423,yi ma,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Determining the reading level of children's literature is an important task for providing educators and parents with an appropriate reading trajectory through a curriculum. Automating this process has been a challenge addressed before in the computational linguistics literature, with most studies attempting to predict the particular grade level of a text. However, guided reading levels developed by educators operate at a more fine-grained level, with multiple levels corresponding to each grade. We find that ranking performs much better than classification at the fine-grained leveling task, and that features derived from the visual layout of a book are just as predictive as standard text features of level; including both sets of features, we find that we can predict the reading level up to 83% of the time on a small corpus of children's books."
N12-1091,Exploring Semi-Supervised Coreference Resolution of Medical Concepts using Semantic and Temporal Features,2012,26,6,2,1,12144,preethi raghavan,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We investigate the task of medical concept coreference resolution in clinical text using two semi-supervised methods, co-training and multi-view learning with posterior regularization. By extracting semantic and temporal features of medical concepts found in clinical text, we create conditionally independent data views; co-training MaxEnt classifiers on this data works almost as well as supervised learning for the task of pairwise coreference resolution of medical concepts. We also train MaxEnt models with expectation constraints, using posterior regularization, and find that posterior regularization performs comparably to or slightly better than co-training. We describe the process of semantic and temporal feature extraction and demonstrate our methods on a corpus of case reports from the New England Journal of Medicine and a corpus of patient narratives obtained from The Ohio State University Wexner Medical Center."
iosif-etal-2012-associative,Associative and Semantic Features Extracted From Web-Harvested Corpora,2012,28,2,3,0,30893,elias iosif,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We address the problem of automatic classification of associative and semantic relations between words, and particularly those that hold between nouns. Lexical relations such as synonymy, hypernymy/hyponymy, constitute the fundamental types of semantic relations. Associative relations are harder to define, since they include a long list of diverse relations, e.g., ''''''``Cause-Effect'''''''', ''''''``Instrument-Agency''''''''. Motivated by findings from the literature of psycholinguistics and corpus linguistics, we propose features that take advantage of general linguistic properties. For evaluation we merged three datasets assembled and validated by cognitive scientists. A proposed priming coefficient that measures the degree of asymmetry in the order of appearance of the words in text achieves the best classification results, followed by context-based similarity metrics. The web-based features achieve classification accuracy that exceeds 85{\%}."
N10-1110,Investigations into the Crandem Approach to Word Recognition,2010,10,2,5,0,45816,rohit prabhavalkar,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We suggest improvements to a previously proposed framework for integrating Conditional Random Fields and Hidden Markov Models, dubbed a Crandem system (2009). The previous authors' work suggested that local label posteriors derived from the CRF were too low-entropy for use in word-level automatic speech recognition. As an alternative to the log posterior representation used in their system, we explore frame-level representations derived from the CRF feature functions. We also describe a weight normalization transformation that leads to increased entropy of the CRF posteriors. We report significant gains over the previous Crandem system on the Wall Street Journal word recognition task."
W09-3304,Using the {W}iktionary Graph Structure for Synonym Detection,2009,12,19,3,0,46904,timothy weale,Proceedings of the 2009 Workshop on The People{'}s Web Meets {NLP}: Collaboratively Constructed Semantic Resources (People{'}s Web),0,"This paper presents our work on using the graph structure of Wiktionary for synonym detection. We implement semantic relatedness metrics using both a direct measure of information flow on the graph and a comparison of the list of vertices found to be close to a given vertex. Our algorithms, evaluated on ESL 50, TOEFL 80 and RDWP 300 data sets, perform better than or comparable to existing semantic relatedness measures."
W08-0205,Strategies for Teaching {``}Mixed{''} Computational Linguistics Classes,2008,7,0,1,1,4872,eric foslerlussier,Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics,0,"Many of the computational linguistics classes at Ohio State draw a diverse crowd of students, who bring different levels of preparation to the classroom. In the same classroom, we often get graduate and undergraduate students from Linguistics, Computer Science, Electrical Engineering and other departments; teaching the same material to all of these students presents an interesting challenge to the instructor. In this paper, I discuss some of the teaching strategies that I have employed to help integrate students in two classes on automatic speech recognition topics; strategies for a graduate seminar class and a standard lecture class are presented. Both courses make use of communal, online activities to facilitate interaction between students."
stoia-etal-2008-scare,{SCARE}: a Situated Corpus with Annotated Referring Expressions,2008,10,41,4,1,48007,laura stoia,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Even though a wealth of speech data is available for the dialog systems research community, the particular field of situated language has yet to find an appropriate free resource. The corpus required to answer research questions related to situated language should connect world information to the human language. In this paper we report on the release of a corpus of English spontaneous instruction giving situated dialogs. The corpus was collected using the Quake environment, a first-person virtual reality game, and consists of pairs of participants completing a direction giver- direction follower scenario. The corpus contains the collected audio and video, as well as word-aligned transcriptions and the positional/gaze information of the player. Referring expressions in the corpus are annotated with the IDs of their virtual world referents."
N07-2004,Joint Versus Independent Phonological Feature Models within {CRF} Phone Recognition,2007,13,3,3,0,49301,ilana bromberg,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"We compare the effect of joint modeling of phonological features to independent feature detectors in a Conditional Random Fields framework. Joint modeling of features is achieved by deriving phonological feature posteriors from the posterior probabilities of the phonemes. We find that joint modeling provides superior performance to the independent models on the TIMIT phone recognition task. We explore the effects of varying relationships between phonological features, and suggest that in an ASR system, phonological features should be handled as correlated, rather than independent."
W06-1412,Noun Phrase Generation for Situated Dialogs,2006,77,39,4,1,48007,laura stoia,Proceedings of the Fourth International Natural Language Generation Conference,0,"We report on a study examining the generation of noun phrases within a spoken dialog agent for a navigation domain. The task is to provide real-time instructions that direct the user to move between a series of destinations within a large interior space. A subtask within sentence planning is determining what form to choose for noun phrases. This choice is driven by both the discourse history and spatial context features derived from the direction-follower's position, e.g. his view angle, distance from the target referent and the number of similar items in view. The algorithm was developed as a decision tree and its output was evaluated by a group of human judges who rated 62.6% of the expressions generated by the system to be as good as or better than the language originally produced by human dialog partners."
N06-2040,Sentence Planning for Realtime Navigational Instruction,2006,10,11,4,1,48007,laura stoia,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"In the current work, we focus on systems that provide incremental directions and monitor the progress of mobile users following those directions. Such directions are based on dynamic quantities like the visibility of reference points and their distance from the user. An intelligent navigation assistant might take advantage of the user's mobility within the setting to achieve communicative goals, for example, by repositioning him to a point from which a description of the target is easier to produce. Calculating spatial variables over a corpus of human-human data developed for this study, we trained a classifier to detect contexts in which a target object can be felicitously described. Our algorithm matched the human subjects with 86% precision."
byron-fosler-lussier-2006-osu,The {OSU} Quake 2004 corpus of two-party situated problem-solving dialogs,2006,6,20,2,1,45130,donna byron,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This report describes the Ohio State University Quake 2004 corpus of English spontaneous task-oriented two-person situated dialog. The corpus was collected using a first-person display of an interior space (rooms, corridors, stairs) in which the partners collaborate on a treasure hunt task. The corpus contains exciting new features such as deictic and exophoric reference, language that is calibrated against the spatial arrangement of objects in the world, and partial-observability of the task world imposed by the perceptual limitations inherent in the physical arrangement of the world. The corpus differs from prior dialog collections which intentionally restricted the interacting subjects from sharing any perceptual context, and which allowed one subject (the direction-giver or system) to have total knowledge of the state of the task world. The corpus consists of audio/video recordings of each person's experience in the virtual world and orthographic transcriptions. The virtual world can also be used by other researchers who want to conduct additional studies using this stimulus."
W05-1529,Robust Extraction of Subcategorization Data from Spoken Language,2005,8,1,3,0,47912,jianguo li,Proceedings of the Ninth International Workshop on Parsing Technology,0,"Subcategorization data has been crucial for various NLP tasks. Current method for automatic SCF acquisition usually proceeds in two steps: first, generate all SCF cues from a corpus using a parser, and then filter out spurious SCF cues with statistical tests. Previous studies on SCF acquisition have worked mainly with written texts; spoken corpora have received little attention. Transcripts of spoken language pose two challenges absent in written texts: uncertainty about utterance segmentation and disfluency."
H05-1072,A Cost-Benefit Analysis of Hybrid Phone-Manner Representations for {ASR},2005,14,0,1,1,4872,eric foslerlussier,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"In the past decade, several researchers have started reinvestigating the use of sub-phonetic models for lexical representations within automatic speech recognition systems. Lest history repeat itself, it may be instructive to mine the further past for models of lexical representations in the lexical access literature. In this work, we re-evaluate the model of Briscoe (1989), in which a hybrid strategy of lexical representation between phones and manner classes is promoted. While many of Briscoe's assumptions do not match up with current ASR processing models, we show that his conclusions are essentially correct, and that reconsidering this structure for ASR lexica is an appropriate avenue for future ASR research."
P03-1071,Discourse Segmentation of Multi-Party Conversation,2003,27,254,3,0,4268,michel galley,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,We present a domain-independent topic segmentation algorithm for multi-party speech. Our feature-based algorithm combines knowledge about content using a text-based algorithm as a feature and about form using linguistic and acoustic cues about topic shifts extracted from speech. This segmentation algorithm uses automatically induced decision rules to combine the different features. The embedded text-based algorithm builds on lexical cohesion and has performance comparable to state-of-the-art algorithms based on lexical information. A significant error reduction is obtained by combining the two knowledge sources.
