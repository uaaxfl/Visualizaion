2020.aacl-main.84,O97-4005,0,0.072584,"s segmented using Stanford Parser (Manning et al., 2014) which fails to identify the word “PO主”, post owner and breaks it into two parts. The same type of error also occurs in other popular segmentation tools. Although Huang et al. (2007) proposed a radical method of word segmentation to meet the challenge, using a concept of classifying a string of character-boundaries into either word-boundaries or non-word-boundaries, their work did not address the cases of code-mixing words, whose word boundaries can also fall on foreigner alphabets. Some other methods mainly rely on unsupervised methods (Chang and Su, 1997) or simple statistical methods based on N-gram frequencies, with indices of collocation and co-occurrence (Chang 833 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 833–842 c December 4 - 7, 2020. 2020 Association for Computational Linguistics and Su, 1997; Chen and Ma, 2002; Dias, 2003). However, these works are mainly designed for new words of pure Chinese characters, which are not applicable to MAWs. In this paper, we address the issue of MAW ident"
2020.aacl-main.84,Y96-1018,1,0.623566,"dying the morpho-lexical status of MAWs (Lun, 2013; Riha and Baker, 2010; Riha, 2010). In the age of Internet and social media, the scale of MAWs, their extraction methods, and resources of MAWs have changed drastically since the last decade. For example, Zheng et al. (2005) extracted a small set of MAWs with manual validation from the corpus of People’s Daily (Year 2002). Jiang and Dang (2007) extracted 93 MAWs (out of 1,053 new domain-specific terms) using a statistical approach with rule-based validation. Recently, Huang and Liu (2017) extracted over 1,157 MAWs from both the Sinica Corpus (Chen et al., 1996) and the Chinese Gigaword Corpus (Huang, 2009) based on manually segmented MAWs in the corpora. Although they have extracted 60,000 tokens with alphabetical letters. However, the list mainly includes pure alphabets those are indeed switching codes of other languages. In our study, these pure code-switching words are excluded according to our definition. Their work has established a taxonomy of distributional patterns of alphabetical letters in MAWs and found that typical MAWs follow Chinese modifier-modified (head) morphological rule and the most frequent and productive pattern is alphabetical"
2020.aacl-main.84,C92-1019,0,0.776232,"y and orthography (e.g. “PK过"", player killed, past tense). Meanwhile, it also demonstrates some properties of the foreigner language (e.g. “维生素ing"", supplementing Vitamin, progressive)), providing a unique lexical resource for studying morphophonological idiosyncrasies of code-mixing words. Second, MAWs serve as an indispensable part of people’s daily vocabulary, especially under the rapid development of social media communication. Yet, being out-liars of the Chinese lexicon, they can cause problems to existing word segmentation/new word extraction tools that are trained on traditional words (Chen and Liu, 1992; Xue and Shen, 2003). Consider the following example: Mandarin Alphabetical Word (MAW) is one indispensable component of Modern Chinese that demonstrates unique code-mixing idiosyncrasies influenced by language exchanges. Yet, this interesting phenomenon has not been properly addressed and is mostly excluded from the Chinese language system. This paper addresses the core problem of MAW identification and proposes to construct a large collection of MAWs from Sina Weibo (SMAW) using an automatic web-based technique which includes rule-based identification, informaticsbased extraction, as well a"
2020.aacl-main.84,C02-1049,0,0.135039,"s, their work did not address the cases of code-mixing words, whose word boundaries can also fall on foreigner alphabets. Some other methods mainly rely on unsupervised methods (Chang and Su, 1997) or simple statistical methods based on N-gram frequencies, with indices of collocation and co-occurrence (Chang 833 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 833–842 c December 4 - 7, 2020. 2020 Association for Computational Linguistics and Su, 1997; Chen and Ma, 2002; Dias, 2003). However, these works are mainly designed for new words of pure Chinese characters, which are not applicable to MAWs. In this paper, we address the issue of MAW identification and present the construction of the Sina MAW lexicon (SMAW) (available at https://github.com/Christainx/SMAW) using a fully automatic information extraction technique. The quality of the MAWs (accurateness and inter-rater agreement) are rated by three experts for system evaluation. Compared to previous resources, this dataset provides an unprecedentedly large, balanced, and structured MAWs as well as a MAW"
2020.aacl-main.84,P14-5010,0,0.0136778,"inese lexical characteristics. It is noteworthy that MAWs shall be taken as a code-mixing phenomenon instead of code-switching as a MAW is still a Chinese word which is not switched into another language. Therefore, in this work, MAWS refer to the combined type which encodes both alphabet(s) and Chinese character(s) in one word, such as “A型”, A-type, “PO主”, post owner, and “γ线”, Gamma Ray. Seg: Golden Seg: PO主 主也不知道链接被吞了 (The post owner didn’t know that the link has been hacked off) PO/主 主/也/不/知道/链接/被/吞/了 PO主 主/也/不/知道/链接/被/吞/了 The sentence in E1 (example 1) is segmented using Stanford Parser (Manning et al., 2014) which fails to identify the word “PO主”, post owner and breaks it into two parts. The same type of error also occurs in other popular segmentation tools. Although Huang et al. (2007) proposed a radical method of word segmentation to meet the challenge, using a concept of classifying a string of character-boundaries into either word-boundaries or non-word-boundaries, their work did not address the cases of code-mixing words, whose word boundaries can also fall on foreigner alphabets. Some other methods mainly rely on unsupervised methods (Chang and Su, 1997) or simple statistical methods based"
2020.aacl-main.84,W03-1806,0,0.256838,"Missing"
2020.aacl-main.84,Y06-1024,1,0.736425,"Missing"
2020.aacl-main.84,P07-2018,1,0.473342,"to another language. Therefore, in this work, MAWS refer to the combined type which encodes both alphabet(s) and Chinese character(s) in one word, such as “A型”, A-type, “PO主”, post owner, and “γ线”, Gamma Ray. Seg: Golden Seg: PO主 主也不知道链接被吞了 (The post owner didn’t know that the link has been hacked off) PO/主 主/也/不/知道/链接/被/吞/了 PO主 主/也/不/知道/链接/被/吞/了 The sentence in E1 (example 1) is segmented using Stanford Parser (Manning et al., 2014) which fails to identify the word “PO主”, post owner and breaks it into two parts. The same type of error also occurs in other popular segmentation tools. Although Huang et al. (2007) proposed a radical method of word segmentation to meet the challenge, using a concept of classifying a string of character-boundaries into either word-boundaries or non-word-boundaries, their work did not address the cases of code-mixing words, whose word boundaries can also fall on foreigner alphabets. Some other methods mainly rely on unsupervised methods (Chang and Su, 1997) or simple statistical methods based on N-gram frequencies, with indices of collocation and co-occurrence (Chang 833 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Lin"
2020.aacl-main.84,W16-2013,0,0.0278025,"e system. This paper addresses the core problem of MAW identification and proposes to construct a large collection of MAWs from Sina Weibo (SMAW) using an automatic web-based technique which includes rule-based identification, informaticsbased extraction, as well as Baidu search engine validation. A collection of 16,207 qualified SMAWs are obtained using this technique along with an annotated corpus of more than 200,000 sentences for linguistic research and applicable inquiries. 1 Introduction E1: Mandarin Alphabetic Words (MAWs), also known as lettered words (Liu, 1994) or code-mixing words (Nguyen and Cornips, 2016), are usually formed by Latin, Greek, Arabic alphabets in combination with Chinese characters, e.g. “X-光/X射 线”, X-ray. Although pure alphabets (e.g. “NBA”) used in Chinese context have also been regarded as MAWs in some previous work (Liu, 1994; Huang and Liu, 2017), they are more like switching-codes that retain the orthography and linguistic behaviors of the original language, instead of showing typical Chinese lexical characteristics. It is noteworthy that MAWs shall be taken as a code-mixing phenomenon instead of code-switching as a MAW is still a Chinese word which is not switched into an"
2020.aacl-main.84,W03-1728,0,0.0204287,".g. “PK过"", player killed, past tense). Meanwhile, it also demonstrates some properties of the foreigner language (e.g. “维生素ing"", supplementing Vitamin, progressive)), providing a unique lexical resource for studying morphophonological idiosyncrasies of code-mixing words. Second, MAWs serve as an indispensable part of people’s daily vocabulary, especially under the rapid development of social media communication. Yet, being out-liars of the Chinese lexicon, they can cause problems to existing word segmentation/new word extraction tools that are trained on traditional words (Chen and Liu, 1992; Xue and Shen, 2003). Consider the following example: Mandarin Alphabetical Word (MAW) is one indispensable component of Modern Chinese that demonstrates unique code-mixing idiosyncrasies influenced by language exchanges. Yet, this interesting phenomenon has not been properly addressed and is mostly excluded from the Chinese language system. This paper addresses the core problem of MAW identification and proposes to construct a large collection of MAWs from Sina Weibo (SMAW) using an automatic web-based technique which includes rule-based identification, informaticsbased extraction, as well as Baidu search engine"
2020.aacl-main.84,W06-0127,0,0.0485724,"Missing"
2020.lrec-1.14,D16-1171,0,0.0324616,"Missing"
2020.lrec-1.14,D14-1080,0,0.0310631,"urce. This professional annotated lexicon are regarded as a highquality lexicon (Bainbridge et al., 1994) and it the main resource used in this work as the external affective resource. In 2010, a new release of this resource includes a collection of five thousand lexical items 1 (Heise, 2010). 2.2. Deep Neural Networks In recent years, neural network methods have greatly improved the performance of sentiment analysis. Commonly used models include Convolutional Neural Networks (CNN) (Socher et al., 2011), Recursive Neural Network ReNN (Socher et al., 2013), and Recurrent Neural Networks (RNN) (Irsoy and Cardie, 2014). Long-Short Term Memory model (LSTM), well known for text understanding, is introduced by Tang et al. (2015a) who added a gated mechanism to keep long-term memory. Attentionbased neural networks, mostly built from local context, are proposed to highlight semantically important words and sentences (Yang et al., 2016). Other methods build attention models using external knowledge, such as user/product information (Chen et al., 2016) and cognition grounded data (Long et al., 2019). 2.3. Use of Affective Knowledge Previous studies in combining lexicon-based methods and machine learning approach g"
2020.lrec-1.14,C16-1147,0,0.025131,"sifiers and linearly integrates them into one system. Andreevskaia and Bergler (2008), for instance, present an ensemble system of two classifiers with precision-based weighting. This method obtained significant gains in both accuracy and recall over corpus-based classifiers and lexicon-based systems. The second approach incorporates lexicon knowledge into learning algorithms. To name a few, Hutto and Gilbert (2014) design a rule-based approach to indicate sentiment scores. Wilson et al. (2005) and Melville et al. (2009) use a general-purpose sentiment dictionary to improve linear classifier. Jovanoski et al. (2016) also prove that sentiment lexicon can contribute to logistic regression models. In neural network models, a remarkable 1 113 http://www.indiana.edu/∼socpsy/public files/EnglishWords EPAs.xlsx 0.6 red: E blue: P orange: A 0.5 0.4 proportion work on utilizing sentiment lexicons is done by Teng et al. (2016). They treat the sentiment score of a sentence as a weighted sum of prior sentiment scores of negation words and sentiment words. Qian et al. (2016) propose to apply linguistic regularization to sentiment classification with three linguistically motivated structured regularizers based on pars"
2020.lrec-1.14,D14-1181,0,0.00506385,"echniques as well as emotion theories to identify sentiment expressions in a natural language context. Typical SA studies analyze subjective documents from the author’s perspective using high-frequency word representations and mapping the text (e.g., sentence or document) to categorical labels, e.g., sentiment polarity, with either a discrete label or a real number in a continuum. Recently, the rising use of neural network models has further elevated the performance of SA without involving laborious feature engineering. Typical neural network models such as Convolutional Neural Network (CNN) (Kim, 2014), Recursive auto-encoders (Socher et al., 2013), Long-Short Term Memory (LSTM) (Tang et al., 2015a) have shown promising results in a variety of sentiment analysis tasks. In spite of this, neural network models still face two main problems. First, neural network approaches lack direct mechanisms to highlight important components in a text. Second, external resources such as linguistic knowledge, cognition grounded data, and affective lexicons, are not fully employed in neural models. To tackle the first problem, cognition-based attention models have been adopted for sentiment classification us"
2020.lrec-1.14,P11-1015,0,0.23939,"Missing"
2020.lrec-1.14,D14-1162,0,0.083828,"Missing"
2020.lrec-1.14,D11-1014,0,0.0879513,"ough, there is no size indication, this EPA based lexicon is commonly used as a three dimensional affective resource. This professional annotated lexicon are regarded as a highquality lexicon (Bainbridge et al., 1994) and it the main resource used in this work as the external affective resource. In 2010, a new release of this resource includes a collection of five thousand lexical items 1 (Heise, 2010). 2.2. Deep Neural Networks In recent years, neural network methods have greatly improved the performance of sentiment analysis. Commonly used models include Convolutional Neural Networks (CNN) (Socher et al., 2011), Recursive Neural Network ReNN (Socher et al., 2013), and Recurrent Neural Networks (RNN) (Irsoy and Cardie, 2014). Long-Short Term Memory model (LSTM), well known for text understanding, is introduced by Tang et al. (2015a) who added a gated mechanism to keep long-term memory. Attentionbased neural networks, mostly built from local context, are proposed to highlight semantically important words and sentences (Yang et al., 2016). Other methods build attention models using external knowledge, such as user/product information (Chen et al., 2016) and cognition grounded data (Long et al., 2019)."
2020.lrec-1.14,D13-1170,0,0.0433099,"to identify sentiment expressions in a natural language context. Typical SA studies analyze subjective documents from the author’s perspective using high-frequency word representations and mapping the text (e.g., sentence or document) to categorical labels, e.g., sentiment polarity, with either a discrete label or a real number in a continuum. Recently, the rising use of neural network models has further elevated the performance of SA without involving laborious feature engineering. Typical neural network models such as Convolutional Neural Network (CNN) (Kim, 2014), Recursive auto-encoders (Socher et al., 2013), Long-Short Term Memory (LSTM) (Tang et al., 2015a) have shown promising results in a variety of sentiment analysis tasks. In spite of this, neural network models still face two main problems. First, neural network approaches lack direct mechanisms to highlight important components in a text. Second, external resources such as linguistic knowledge, cognition grounded data, and affective lexicons, are not fully employed in neural models. To tackle the first problem, cognition-based attention models have been adopted for sentiment classification using text-embedded information such as users, pr"
2020.lrec-1.14,D15-1167,0,0.235271,"age context. Typical SA studies analyze subjective documents from the author’s perspective using high-frequency word representations and mapping the text (e.g., sentence or document) to categorical labels, e.g., sentiment polarity, with either a discrete label or a real number in a continuum. Recently, the rising use of neural network models has further elevated the performance of SA without involving laborious feature engineering. Typical neural network models such as Convolutional Neural Network (CNN) (Kim, 2014), Recursive auto-encoders (Socher et al., 2013), Long-Short Term Memory (LSTM) (Tang et al., 2015a) have shown promising results in a variety of sentiment analysis tasks. In spite of this, neural network models still face two main problems. First, neural network approaches lack direct mechanisms to highlight important components in a text. Second, external resources such as linguistic knowledge, cognition grounded data, and affective lexicons, are not fully employed in neural models. To tackle the first problem, cognition-based attention models have been adopted for sentiment classification using text-embedded information such as users, products, and local context (Tang et al., 2015b; Yan"
2020.lrec-1.14,P15-1098,0,0.122464,"age context. Typical SA studies analyze subjective documents from the author’s perspective using high-frequency word representations and mapping the text (e.g., sentence or document) to categorical labels, e.g., sentiment polarity, with either a discrete label or a real number in a continuum. Recently, the rising use of neural network models has further elevated the performance of SA without involving laborious feature engineering. Typical neural network models such as Convolutional Neural Network (CNN) (Kim, 2014), Recursive auto-encoders (Socher et al., 2013), Long-Short Term Memory (LSTM) (Tang et al., 2015a) have shown promising results in a variety of sentiment analysis tasks. In spite of this, neural network models still face two main problems. First, neural network approaches lack direct mechanisms to highlight important components in a text. Second, external resources such as linguistic knowledge, cognition grounded data, and affective lexicons, are not fully employed in neural models. To tackle the first problem, cognition-based attention models have been adopted for sentiment classification using text-embedded information such as users, products, and local context (Tang et al., 2015b; Yan"
2020.lrec-1.14,D16-1169,0,0.0173378,"d approach incorporates lexicon knowledge into learning algorithms. To name a few, Hutto and Gilbert (2014) design a rule-based approach to indicate sentiment scores. Wilson et al. (2005) and Melville et al. (2009) use a general-purpose sentiment dictionary to improve linear classifier. Jovanoski et al. (2016) also prove that sentiment lexicon can contribute to logistic regression models. In neural network models, a remarkable 1 113 http://www.indiana.edu/∼socpsy/public files/EnglishWords EPAs.xlsx 0.6 red: E blue: P orange: A 0.5 0.4 proportion work on utilizing sentiment lexicons is done by Teng et al. (2016). They treat the sentiment score of a sentence as a weighted sum of prior sentiment scores of negation words and sentiment words. Qian et al. (2016) propose to apply linguistic regularization to sentiment classification with three linguistically motivated structured regularizers based on parse trees, topics, and hierarchical word clusters. Zou et al. (2018) adopt a mixed attention mechanism to further highlight the role of sentiment lexicon in the attention layer. Using sentiment polarity in a loss function is one way to employ attention mechanism. However, attention weights are normally obtai"
2020.lrec-1.14,H05-1044,0,0.158412,"on-based methods and machine learning approach generally diverge into two ways. The first approach uses two weighted classifiers and linearly integrates them into one system. Andreevskaia and Bergler (2008), for instance, present an ensemble system of two classifiers with precision-based weighting. This method obtained significant gains in both accuracy and recall over corpus-based classifiers and lexicon-based systems. The second approach incorporates lexicon knowledge into learning algorithms. To name a few, Hutto and Gilbert (2014) design a rule-based approach to indicate sentiment scores. Wilson et al. (2005) and Melville et al. (2009) use a general-purpose sentiment dictionary to improve linear classifier. Jovanoski et al. (2016) also prove that sentiment lexicon can contribute to logistic regression models. In neural network models, a remarkable 1 113 http://www.indiana.edu/∼socpsy/public files/EnglishWords EPAs.xlsx 0.6 red: E blue: P orange: A 0.5 0.4 proportion work on utilizing sentiment lexicons is done by Teng et al. (2016). They treat the sentiment score of a sentence as a weighted sum of prior sentiment scores of negation words and sentiment words. Qian et al. (2016) propose to apply lin"
2020.lrec-1.14,N16-1174,0,0.164662,"015a) have shown promising results in a variety of sentiment analysis tasks. In spite of this, neural network models still face two main problems. First, neural network approaches lack direct mechanisms to highlight important components in a text. Second, external resources such as linguistic knowledge, cognition grounded data, and affective lexicons, are not fully employed in neural models. To tackle the first problem, cognition-based attention models have been adopted for sentiment classification using text-embedded information such as users, products, and local context (Tang et al., 2015b; Yang et al., 2016; Chen et al., 2016; Long et al., 2019). For the second problem, Qian et al. (2016) proposed to add linguistic resources to deep learning models for further improvement. Yet, recent method of integration of additional lexical information are limited to matrix manipulation in attention layer due to the incompatibility of such representations with embedding ones, making it quite inefficient. In this paper, we attempt to address this problem by incorporating an affective lexicon as numerical influence values into affective neural network models through the framework of the Affect Control Theory ("
2020.lrec-1.14,C18-1074,0,0.0260178,"to logistic regression models. In neural network models, a remarkable 1 113 http://www.indiana.edu/∼socpsy/public files/EnglishWords EPAs.xlsx 0.6 red: E blue: P orange: A 0.5 0.4 proportion work on utilizing sentiment lexicons is done by Teng et al. (2016). They treat the sentiment score of a sentence as a weighted sum of prior sentiment scores of negation words and sentiment words. Qian et al. (2016) propose to apply linguistic regularization to sentiment classification with three linguistically motivated structured regularizers based on parse trees, topics, and hierarchical word clusters. Zou et al. (2018) adopt a mixed attention mechanism to further highlight the role of sentiment lexicon in the attention layer. Using sentiment polarity in a loss function is one way to employ attention mechanism. However, attention weights are normally obtained using local context information. The computational complexity of reweighing each word by attention requires matrix and softmax manipulation, which slows down the time for training and inference especially with long sentences. 0.3 0.2 0.1 0.0 −4 −2 0 affective value 2 4 Figure 1: Histogram of EPA values 3. Methodology We proposes a novel affection driven"
2020.lrec-1.701,W14-2608,0,0.0235638,"e are two main approaches in irony detection: rule-based approaches and machine learning approaches (Joshi et al., 2017). A rule-based approach in irony identification mainly relies on lexicons and syntactic patterns, while a machine learning approach combines different types of features and knowledge bases to detect irony (Maynard and Greenwood, 2014; Khattri et al., 2015). In addition to lexical features, some other types of features can contribute to detecting irony, depending on the text genre. These features included but not limited to sentiment words, punctuation (Carvalho et al., 2009; Buschmeier et al., 2014), emoticon (Buschmeier et al., 2014), emotional scenarios (Reyes and Rosso, 2014), as well as reversals (Li et al., 2019). Meanwhile, Support Vector Machine (SVM) and Logistic Regression (LR) remain the most popular models in classical machine learning approaches for irony detection (Ghosh et al., 2015; Li et al., 2019). Some recent works also tried to tackle irony identification using deep learning methods. Deep learning methods no longer need feature engineering and have shown superior abilities to complex word composition in text (Ghosh and Veale, 2016; Huang et al., 2017). A system based o"
2020.lrec-1.701,N19-1423,0,0.177849,"instances in Ciron are collected from Chinese 5714 microblogs in a grounded and more natural context. The annotation process ensures consistency and quality. The dataset is applied to several popular machine learning-based methods to demonstrate its effectiveness including Naive Bayes (NB), logistic regression (LR), support vector machine (SVM), convolutional neural networks(CNN) (Kim, 2014), long short-term memory networks(LSTM) (Tang et al., 2015), bidirectional LSTM with attention mechanism (BiLSTM-AT) (Zhang et al., 2018) and Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). The rest of this paper is organized as follows. Section 2 introduces related works on irony theories and illustrates typical examples of ironic sentences in Chinese. Section 3 provides an in-depth description of the data that we extracted from Weibo forums, together with the formalization of the irony features. The performance of the baseline models is evaluated in Section 4, highlighting its validity. Section 5 concludes this paper. 2. Related Works Based on several linguistic studies (Huang et al., 2017), we define ironic text as those expressions showing discrepancy/incongruity between th"
2020.lrec-1.701,W15-2915,0,0.0228667,"y detection has a large potential for various applications in the domain of text mining, especially those that require semantic analysis, such as author profiling, online harassment and hate speech detection, and perhaps, the most well-known task of affective analysis. Compared to other text analysis tasks, irony detection has received limited computational treatment (Barbieri and Saggion, 2014; Ghosh et al., 2019). Affective analysis works started to analyze and summarize linguistic features of irony from a sentiment shifting perspective in order to allow for its computational formalization (Ebert et al., 2015; Long et al., 2019). While theories based on English have provided a relatively comprehensive map of irony, there is still a lack of literature dealing with non-Indo-European languages. Even though irony is a pervasive linguistic phenomenon, some of its features vary in different cultures and in structural properties of a specific language (Xing and Xu, 2015). For example, Karoui and colleagues (2019) suggested capitalized words as a strong hint of irony in English. Yet, this hint does not work for Chinese as there is no capitalization or other obvious lexical variations in surface forms in C"
2020.lrec-1.701,W16-0425,0,0.0299825,"punctuation (Carvalho et al., 2009; Buschmeier et al., 2014), emoticon (Buschmeier et al., 2014), emotional scenarios (Reyes and Rosso, 2014), as well as reversals (Li et al., 2019). Meanwhile, Support Vector Machine (SVM) and Logistic Regression (LR) remain the most popular models in classical machine learning approaches for irony detection (Ghosh et al., 2015; Li et al., 2019). Some recent works also tried to tackle irony identification using deep learning methods. Deep learning methods no longer need feature engineering and have shown superior abilities to complex word composition in text (Ghosh and Veale, 2016; Huang et al., 2017). A system based on Long Short-Term Memory (LSTM) model with a multi-task learning strategy recently performed very well in the irony task (Wu et al., 2018). A large number of studies on irony detection have been conducted for English text. But, attempts on Chinese irony detection are still quite limited (Van Hee et al., 2018). Tang and Chen used a number of linguistic patterns to extract posts from Yahoo Blog as potential irony instances and then manually checked to identify irony instances for Traditional Chinese in Taiwan (2014). The linguistic patterns used in this wor"
2020.lrec-1.701,S15-2080,0,0.0314016,"to detect irony (Maynard and Greenwood, 2014; Khattri et al., 2015). In addition to lexical features, some other types of features can contribute to detecting irony, depending on the text genre. These features included but not limited to sentiment words, punctuation (Carvalho et al., 2009; Buschmeier et al., 2014), emoticon (Buschmeier et al., 2014), emotional scenarios (Reyes and Rosso, 2014), as well as reversals (Li et al., 2019). Meanwhile, Support Vector Machine (SVM) and Logistic Regression (LR) remain the most popular models in classical machine learning approaches for irony detection (Ghosh et al., 2015; Li et al., 2019). Some recent works also tried to tackle irony identification using deep learning methods. Deep learning methods no longer need feature engineering and have shown superior abilities to complex word composition in text (Ghosh and Veale, 2016; Huang et al., 2017). A system based on Long Short-Term Memory (LSTM) model with a multi-task learning strategy recently performed very well in the irony task (Wu et al., 2018). A large number of studies on irony detection have been conducted for English text. But, attempts on Chinese irony detection are still quite limited (Van Hee et al."
2020.lrec-1.701,W15-2905,0,0.0147946,"he contextual meaning of the words poses a serious challenge to text mining tasks such as affective analysis (Reyes and Rosso, 2014). In recent years, irony detection in NLP has become one of the most arduous and attractive research topics. There are two main approaches in irony detection: rule-based approaches and machine learning approaches (Joshi et al., 2017). A rule-based approach in irony identification mainly relies on lexicons and syntactic patterns, while a machine learning approach combines different types of features and knowledge bases to detect irony (Maynard and Greenwood, 2014; Khattri et al., 2015). In addition to lexical features, some other types of features can contribute to detecting irony, depending on the text genre. These features included but not limited to sentiment words, punctuation (Carvalho et al., 2009; Buschmeier et al., 2014), emoticon (Buschmeier et al., 2014), emotional scenarios (Reyes and Rosso, 2014), as well as reversals (Li et al., 2019). Meanwhile, Support Vector Machine (SVM) and Logistic Regression (LR) remain the most popular models in classical machine learning approaches for irony detection (Ghosh et al., 2015; Li et al., 2019). Some recent works also tried"
2020.lrec-1.701,D14-1181,0,0.00965356,"Missing"
2020.lrec-1.701,maynard-greenwood-2014-cares,0,0.0266114,"eversal from the literal to the contextual meaning of the words poses a serious challenge to text mining tasks such as affective analysis (Reyes and Rosso, 2014). In recent years, irony detection in NLP has become one of the most arduous and attractive research topics. There are two main approaches in irony detection: rule-based approaches and machine learning approaches (Joshi et al., 2017). A rule-based approach in irony identification mainly relies on lexicons and syntactic patterns, while a machine learning approach combines different types of features and knowledge bases to detect irony (Maynard and Greenwood, 2014; Khattri et al., 2015). In addition to lexical features, some other types of features can contribute to detecting irony, depending on the text genre. These features included but not limited to sentiment words, punctuation (Carvalho et al., 2009; Buschmeier et al., 2014), emoticon (Buschmeier et al., 2014), emotional scenarios (Reyes and Rosso, 2014), as well as reversals (Li et al., 2019). Meanwhile, Support Vector Machine (SVM) and Logistic Regression (LR) remain the most popular models in classical machine learning approaches for irony detection (Ghosh et al., 2015; Li et al., 2019). Some r"
2020.lrec-1.701,D14-1162,0,0.0980977,"methods and four deep learning methods on Ciron. Traditional methods include: Naive Bayes (NB), Logistic Regression (LR), and Support Vector Machine (SVM). Deep learning methods include: Convolutional Neural Network (CNN) (LeCun et al., 1998), Long Short-term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Bidirectional Long Short-term Memory network with attention mechanism (BiLSTM-AT) (Zhang et al., 2018) and the context aware Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). For the first three deep learning methods, pre-trained GloVe vectors (Pennington et al., 2014) are used as word embedding features. BERT, with the Chinese pre-trained model, is fine-tuned for Ciron. All models are tuned with the datasets. Detailed settings for the models are provided in Table 4. Accuracy and weighted F1 score are adopted as the metrics of performance. 4.2. Analysis of Result Empirical results are listed in Table 5. In general, traditional methods are outperformed by deep learning methods. NB is the the worst performer as expected. The performance of LR is worse than SVM with a narrow margin. SVM results in a competitive accuracy compared to deep learning methods. Among"
2020.lrec-1.701,C14-1120,0,0.0583209,"no capitalization or other obvious lexical variations in surface forms in Chinese text. Studies on irony detection should take into account the specific ways in which irony is expressed in a given culture and a given language. Otherwise, the capacity of automatic systems in modeling the notion of ”context” will always be limited (Van Hee, 2017). Chinese irony detection in social networks is more challenging because the language of social networks is mostly composed of short statements (Li and Huang, 2019). Few works to date have tried to investigate Chinese irony detection in social networks (Tang and Chen, 2014). However, existing resources are limited to linguistic studies of Chinese irony detection to describe certain lexical patterns as well as syntactic patterns which cannot be readily formulated for machine learning algorithms. The lack of training data for Chinese irony is still a bottleneck for developing computationally-intensive, broad-coverage Chinese irony detection models. To solve this problem, this work aims to first explore the characteristic features of irony of the Chinese language and also to provide a benchmark dataset that can be used for automatic irony detection. In this paper,"
2020.lrec-1.701,P15-1098,0,0.0119301,"e first Chinese resource for Chinese irony detection with such a volume of data and fine-grained annotation. In contrast to many NLP datasets that are crowdsourced, instances in Ciron are collected from Chinese 5714 microblogs in a grounded and more natural context. The annotation process ensures consistency and quality. The dataset is applied to several popular machine learning-based methods to demonstrate its effectiveness including Naive Bayes (NB), logistic regression (LR), support vector machine (SVM), convolutional neural networks(CNN) (Kim, 2014), long short-term memory networks(LSTM) (Tang et al., 2015), bidirectional LSTM with attention mechanism (BiLSTM-AT) (Zhang et al., 2018) and Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). The rest of this paper is organized as follows. Section 2 introduces related works on irony theories and illustrates typical examples of ironic sentences in Chinese. Section 3 provides an in-depth description of the data that we extracted from Weibo forums, together with the formalization of the irony features. The performance of the baseline models is evaluated in Section 4, highlighting its validity. Section 5 concludes this"
2020.lrec-1.701,S18-1005,0,0.0376319,"Missing"
2020.lrec-1.701,S18-1040,0,0.121314,"ta and fine-grained annotation. In contrast to many NLP datasets that are crowdsourced, instances in Ciron are collected from Chinese 5714 microblogs in a grounded and more natural context. The annotation process ensures consistency and quality. The dataset is applied to several popular machine learning-based methods to demonstrate its effectiveness including Naive Bayes (NB), logistic regression (LR), support vector machine (SVM), convolutional neural networks(CNN) (Kim, 2014), long short-term memory networks(LSTM) (Tang et al., 2015), bidirectional LSTM with attention mechanism (BiLSTM-AT) (Zhang et al., 2018) and Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). The rest of this paper is organized as follows. Section 2 introduces related works on irony theories and illustrates typical examples of ironic sentences in Chinese. Section 3 provides an in-depth description of the data that we extracted from Weibo forums, together with the formalization of the irony features. The performance of the baseline models is evaluated in Section 4, highlighting its validity. Section 5 concludes this paper. 2. Related Works Based on several linguistic studies (Huang et al., 201"
2020.starsem-1.4,E14-1049,0,0.0431734,"ingual embeddings, vector space models that represent words from multiple languages through some form of mapping from a monolingual to a multilingual space (Conneau et al., 2018; Ruder et al., 2019). A classical study by Mikolov et al. (2013b) learnt a linear projection to transform the space of a source language to the space of a target language by maximizing the similarity between the two spaces. Other approaches apply Canonical Correlation Analysis to simultaneously project words from two languages into a shared embedding space where the correlation between projected vectors are maximized (Faruqui and Dyer, 2014). Other works make use of the max-margin method such that, for embeddings projected from a source language, they maximize the margin between the correct translations and other candidates (Lazaridou et al., 2015; Joulin et al., 2018). For this study, we use the offthe-shelf crosslingual embeddings by Joulin et al. (2018) based on FastText (Bojanowski et al., 2017) 3 Our Proposed Approach In this work, we used regressors trained on crosslingual word embeddings to predict modality ratings in two different scenarios. In the monolingual scenario, we adopt a 5-fold cross-validation to predict the mo"
2020.starsem-1.4,D18-1330,0,0.0986365,"(2013b) learnt a linear projection to transform the space of a source language to the space of a target language by maximizing the similarity between the two spaces. Other approaches apply Canonical Correlation Analysis to simultaneously project words from two languages into a shared embedding space where the correlation between projected vectors are maximized (Faruqui and Dyer, 2014). Other works make use of the max-margin method such that, for embeddings projected from a source language, they maximize the margin between the correct translations and other candidates (Lazaridou et al., 2015; Joulin et al., 2018). For this study, we use the offthe-shelf crosslingual embeddings by Joulin et al. (2018) based on FastText (Bojanowski et al., 2017) 3 Our Proposed Approach In this work, we used regressors trained on crosslingual word embeddings to predict modality ratings in two different scenarios. In the monolingual scenario, we adopt a 5-fold cross-validation to predict the modality norms of an English dataset (Lynott and Connell, 2009, 2013). In the crosslingual scenario, a regressor is trained on a high-resource language, e.g. English, to predict the modality norms of an unseen language. 3.1 Datasets F"
2020.starsem-1.4,Q17-1010,0,0.279266,"nd Chinese norms. Results show that a) crosslingual embeddings perform similarly to or slightly better than monolingual embeddings in a monolingual setting; b) even after training only on English data, the regressor can predict norms in a totally unseen language with moderate-to-high correlations with human judgements. 2 Related Work Although word vectors have been a standard for word representations for almost two decades (Lenci, 2018), they became an essential ingredient for most NLP applications only after the introduction of word embeddings (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017). Differently from the first generation models using co-occurrence counting and weighting, word embeddings are estimated via neural network training with the objective of maximizing the probability of the contexts of a target word, and they gained popularity due to the availability of efficient and easy-touse tools (Mikolov et al., 2013a). The development of research on crosslingual transfer and the availability of new benchmarks for multilingual NLP has recently led to the introduction of the so-called crosslingual embeddings, vector space models that represent words from multiple languages t"
2020.starsem-1.4,P15-1027,0,0.030858,"study by Mikolov et al. (2013b) learnt a linear projection to transform the space of a source language to the space of a target language by maximizing the similarity between the two spaces. Other approaches apply Canonical Correlation Analysis to simultaneously project words from two languages into a shared embedding space where the correlation between projected vectors are maximized (Faruqui and Dyer, 2014). Other works make use of the max-margin method such that, for embeddings projected from a source language, they maximize the margin between the correct translations and other candidates (Lazaridou et al., 2015; Joulin et al., 2018). For this study, we use the offthe-shelf crosslingual embeddings by Joulin et al. (2018) based on FastText (Bojanowski et al., 2017) 3 Our Proposed Approach In this work, we used regressors trained on crosslingual word embeddings to predict modality ratings in two different scenarios. In the monolingual scenario, we adopt a 5-fold cross-validation to predict the modality norms of an English dataset (Lynott and Connell, 2009, 2013). In the crosslingual scenario, a regressor is trained on a high-resource language, e.g. English, to predict the modality norms of an unseen la"
2020.starsem-1.4,W17-2810,0,0.016163,"thus This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 32 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 32–38 Barcelona, Spain (Online), December 12–13, 2020 and trained on Wikipedia. 1 Despite the success of word embeddings, a common criticism is that they are not grounded in perception, as words are only defined in relation to each other and not to entities and actions in the physical world (Glenberg and Robertson, 2000; Fagarasan et al., 2015; Li and Gauthier, 2017). To address this issue, Fagarasan et al. (2015) used a regression method to map embeddings onto the conceptual properties of the McRae norms (McRae et al., 2005). A similar approach, using feedforward neural networks for predicting properties, was recently described by Li and Summers-Stay (2019). The work by Derby et al. (2019) goes in the opposite direction: instead of predicting norms from embeddings, they combined pretrained vectors and property vectors to inject conceptual knowledge into a new type of word representations. Their Feature2Vec system showed a strong performance in the predic"
2020.starsem-1.4,D19-1595,0,0.0105735,"1 Despite the success of word embeddings, a common criticism is that they are not grounded in perception, as words are only defined in relation to each other and not to entities and actions in the physical world (Glenberg and Robertson, 2000; Fagarasan et al., 2015; Li and Gauthier, 2017). To address this issue, Fagarasan et al. (2015) used a regression method to map embeddings onto the conceptual properties of the McRae norms (McRae et al., 2005). A similar approach, using feedforward neural networks for predicting properties, was recently described by Li and Summers-Stay (2019). The work by Derby et al. (2019) goes in the opposite direction: instead of predicting norms from embeddings, they combined pretrained vectors and property vectors to inject conceptual knowledge into a new type of word representations. Their Feature2Vec system showed a strong performance in the predicting norms of unseen words, compared to previous proposals. Finally, Utsumi (2018, 2020) proposed a similar mapping technique to exploit semantic feature norms by Binder et al. (2016) to analyze the semantic content of word embeddings in terms of neurobiologically-motivated features. Turton et al. (2020) also experimented with e"
2020.starsem-1.4,N19-1423,0,0.0107744,"the research on modality norms. In this first study we used a relatively simple methodology, but several refinements are possible for improving the prediction quality. Two possible directions would be, firstly, to exploit the presence of words that are direct translations from English to the other languages to apply retrofitting techniques (Faruqui et al., 2015; Mrkši´c et al., 2016, 2017; Vuli´c et al., 2018) to the crosslingual space, and secondly, to tackle the task by introducing more advanced neural architectures for the representation of words in context, e.g. multilingual transformers (Devlin et al., 2019; Pires et al., 2019). Table 5: Number and percentage of words that are translations of English items for each dataset. while multimodal concepts are typically associated with lower scores. Strongly multimodal concepts might be more difficult to predict, as their scores for the five modalities are generally closer than in the unimodal concepts. We tested this hypothesis by measuring the Spearman correlation between the word correlations and the modality exclusivity scores from the original dataset, but no strong evidence was found: the models showed no significant correlation for the Italian d"
2020.starsem-1.4,W15-0107,0,0.0288419,"red feature space. It is thus This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 32 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 32–38 Barcelona, Spain (Online), December 12–13, 2020 and trained on Wikipedia. 1 Despite the success of word embeddings, a common criticism is that they are not grounded in perception, as words are only defined in relation to each other and not to entities and actions in the physical world (Glenberg and Robertson, 2000; Fagarasan et al., 2015; Li and Gauthier, 2017). To address this issue, Fagarasan et al. (2015) used a regression method to map embeddings onto the conceptual properties of the McRae norms (McRae et al., 2005). A similar approach, using feedforward neural networks for predicting properties, was recently described by Li and Summers-Stay (2019). The work by Derby et al. (2019) goes in the opposite direction: instead of predicting norms from embeddings, they combined pretrained vectors and property vectors to inject conceptual knowledge into a new type of word representations. Their Feature2Vec system showed a strong p"
2020.starsem-1.4,N15-1184,0,0.0217214,"egressor on a highresource language (e.g. English) and to predict the norms for the low-resource one. In our experiments, we obtained moderate-to-high correlations even in the crosslingual setting. We think this is potentially a very useful application for the research on modality norms. In this first study we used a relatively simple methodology, but several refinements are possible for improving the prediction quality. Two possible directions would be, firstly, to exploit the presence of words that are direct translations from English to the other languages to apply retrofitting techniques (Faruqui et al., 2015; Mrkši´c et al., 2016, 2017; Vuli´c et al., 2018) to the crosslingual space, and secondly, to tackle the task by introducing more advanced neural architectures for the representation of words in context, e.g. multilingual transformers (Devlin et al., 2019; Pires et al., 2019). Table 5: Number and percentage of words that are translations of English items for each dataset. while multimodal concepts are typically associated with lower scores. Strongly multimodal concepts might be more difficult to predict, as their scores for the five modalities are generally closer than in the unimodal concept"
2020.starsem-1.4,S17-2008,0,0.0174212,"n this work, we used regressors trained on crosslingual word embeddings to predict modality ratings in two different scenarios. In the monolingual scenario, we adopt a 5-fold cross-validation to predict the modality norms of an English dataset (Lynott and Connell, 2009, 2013). In the crosslingual scenario, a regressor is trained on a high-resource language, e.g. English, to predict the modality norms of an unseen language. 3.1 Datasets Four modality norms datasets are used in this work: the English norms by Lynott and Connell (2009, 1 We ran experiments also with the Numberbatch embeddings by Speer and Lowry-Duda (2017), which are obtained by retrofitting different types of word embeddings with a subgraph of ConceptNet (Speer et al., 2017). However, while these vectors showed a strong performance in predicting the norms in the monolingual setting, they never achieved significant correlations with human judgements in the crosslingual prediction, and thus we omitted them from the Results section. 33 2013) (1002 words); the Italian norms by Vergallito et al. (2020) (1121 words); the Dutch norms by Speed and Majid (2017) (485 words); the Chinese norms by Chen et al. (2019) (291 words). The latter three datasets"
2020.starsem-1.4,2020.lincr-1.1,0,0.0276807,"-Stay (2019). The work by Derby et al. (2019) goes in the opposite direction: instead of predicting norms from embeddings, they combined pretrained vectors and property vectors to inject conceptual knowledge into a new type of word representations. Their Feature2Vec system showed a strong performance in the predicting norms of unseen words, compared to previous proposals. Finally, Utsumi (2018, 2020) proposed a similar mapping technique to exploit semantic feature norms by Binder et al. (2016) to analyze the semantic content of word embeddings in terms of neurobiologically-motivated features. Turton et al. (2020) also experimented with embeddings based on Binder features, showing that they can achieve performances comparable to Word2Vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) on similarity datasets. possible to train a regressor on a resource-rich language, e.g English, and predict modality ratings for words in an unseen language. We experimented with this crosslingual transfer method on Italian, Dutch and Chinese norms. Results show that a) crosslingual embeddings perform similarly to or slightly better than monolingual embeddings in a monolingual setting; b) even after training o"
2020.starsem-1.4,N16-1018,0,0.0540142,"Missing"
2020.starsem-1.4,Q17-1022,0,0.0292807,"Missing"
2020.starsem-1.4,N18-1048,0,0.0368776,"Missing"
2020.starsem-1.4,2020.figlang-1.16,1,0.5337,"rms with Crosslingual Word Embeddings Emmanuele Chersoni, Rong Xiang, Qin Lu, Chu-Ren Huang The Hong Kong Polytechnic University, 11 Yuk Choi Road, Hong Kong (China) {emmanuelechersoni, xiangrong0302}@gmail.com, csluqin@comp.polyu.edu.hk, churen.huang@polyu.edu.hk Abstract the perceptual strength of lexical items used in their experiments, motivating the publication of datasets in which words are rated according to their association with each of the five senses (see the example in Table 1). Moreover, such norms have also been shown to be useful for other NLP tasks, such as metaphor detection (Wan et al., 2020a,b). Normative studies on modality for English words are relatively common (Lynott and Connell, 2009; Juhasz et al., 2011; Lynott and Connell, 2013; Lynott et al., 2019), and similar norms have also been made available for other languages such as French (Bonin et al., 2015), Serbian (Ðurdevi´ ¯ c et al., 2016), Dutch (Speed and Majid, 2017), Russian (Miklashevsky, 2018), Chinese (Chen et al., 2019) and Italian (Vergallito et al., 2020). But in general, the number of languages for which they are available is still limited, and collecting modality norms is a time-consuming process, especially f"
2020.starsem-1.4,D14-1162,0,0.0833394,"ject conceptual knowledge into a new type of word representations. Their Feature2Vec system showed a strong performance in the predicting norms of unseen words, compared to previous proposals. Finally, Utsumi (2018, 2020) proposed a similar mapping technique to exploit semantic feature norms by Binder et al. (2016) to analyze the semantic content of word embeddings in terms of neurobiologically-motivated features. Turton et al. (2020) also experimented with embeddings based on Binder features, showing that they can achieve performances comparable to Word2Vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) on similarity datasets. possible to train a regressor on a resource-rich language, e.g English, and predict modality ratings for words in an unseen language. We experimented with this crosslingual transfer method on Italian, Dutch and Chinese norms. Results show that a) crosslingual embeddings perform similarly to or slightly better than monolingual embeddings in a monolingual setting; b) even after training only on English data, the regressor can predict norms in a totally unseen language with moderate-to-high correlations with human judgements. 2 Related Work Although word vectors have been"
2020.starsem-1.4,P19-1493,0,0.0223154,"ity norms. In this first study we used a relatively simple methodology, but several refinements are possible for improving the prediction quality. Two possible directions would be, firstly, to exploit the presence of words that are direct translations from English to the other languages to apply retrofitting techniques (Faruqui et al., 2015; Mrkši´c et al., 2016, 2017; Vuli´c et al., 2018) to the crosslingual space, and secondly, to tackle the task by introducing more advanced neural architectures for the representation of words in context, e.g. multilingual transformers (Devlin et al., 2019; Pires et al., 2019). Table 5: Number and percentage of words that are translations of English items for each dataset. while multimodal concepts are typically associated with lower scores. Strongly multimodal concepts might be more difficult to predict, as their scores for the five modalities are generally closer than in the unimodal concepts. We tested this hypothesis by measuring the Spearman correlation between the word correlations and the modality exclusivity scores from the original dataset, but no strong evidence was found: the models showed no significant correlation for the Italian data, while finding po"
2021.semeval-1.70,Q17-1010,0,0.0473349,"ery distinct path as a single feature. In total, we generated 267 dependency paths features with this mechanism. Another feature was based on Word Embedding similarity: first, we computed the sum of the embeddings for all the words preceding the target, as a sort of general representation of the sentence context 1 , and then we measured the cosine similarity with the embedding of the target word. If the target was a multiword expression, we summed the embeddings of the words composing it. As word embeddings, we used the publicly available FastText vectors, pre-trained on the Wikipedia corpus (Bojanowski et al., 2017). 2 We added one feature based on the BERT Transformer Model (Devlin et al., 2019) 3 by masking the target word in the original sentence and taking the probability value provided in output by the Softmax. For multiword expressions, we sequentially masked the words composing the target and took the average value. Similarly, we used the GPT-2 Transformer Model (Radford et al., 2019) 4 to obtain a probability score for the full sentence, computed as the product of the probabilities of the single tokens. The total number of extracted features is 300. Finally, we decided to generate polynomial feat"
2021.semeval-1.70,W16-4102,1,0.826572,"meanings. In linguistic typology, for example, complexity is generally studied as a property of the language system as a whole, it is conceived as the number of (morphological, syntactic, semantic etc.) distinctions that a speaker has to master, and it is assessed by comparing different languages (McWhorter, 2001; Parkvall, 2008). On the other hand, in the perspective of psycholinguistics and cognitive science, the notion of complexity can be described as the difficulty encountered by language users while processing concrete linguistic realizations (sentences, utterances etc.) (Blache, 2011; Chersoni et al., 2016, 2017, 2021; Iavarone et al., 2021; Sarti et al., 2021). Finally, in the Computational Linguistics community, the assessment of complexity at the lexical level is often related to readability applications (Shardlow et al., 2020), with the goal of determining if a word in a given text will be difficult to understand for the language users. Such applications are extremely useful for second language learners, for speakers with relatively low literacy and for people with reading disabilities, helping to tailor the difficult level of the texts to the needs of the target users. Task 1 of SemEval 20"
2021.semeval-1.70,S17-1021,1,0.866426,"Missing"
2021.semeval-1.70,N19-1423,0,0.0167303,"tures with this mechanism. Another feature was based on Word Embedding similarity: first, we computed the sum of the embeddings for all the words preceding the target, as a sort of general representation of the sentence context 1 , and then we measured the cosine similarity with the embedding of the target word. If the target was a multiword expression, we summed the embeddings of the words composing it. As word embeddings, we used the publicly available FastText vectors, pre-trained on the Wikipedia corpus (Bojanowski et al., 2017). 2 We added one feature based on the BERT Transformer Model (Devlin et al., 2019) 3 by masking the target word in the original sentence and taking the probability value provided in output by the Softmax. For multiword expressions, we sequentially masked the words composing the target and took the average value. Similarly, we used the GPT-2 Transformer Model (Radford et al., 2019) 4 to obtain a probability score for the full sentence, computed as the product of the probabilities of the single tokens. The total number of extracted features is 300. Finally, we decided to generate polynomial features from our set, in order to exploit potential interactions. We used the Polynom"
2021.semeval-1.70,2021.cmcl-1.23,0,0.0651991,"Missing"
2021.semeval-1.70,2005.mtsummit-papers.11,0,0.0360983,"ressions track. Examples of the instances are shown in Table 1. 4 Evaluation For both the single words and the multiword expressions track, we used the same set of features as input for a regression algorithm. In the multiword expressions track, we computed the value of the features for each of the two words in the target expression and then we took the average. 4.1 Datasets The datasets for the shared task are part of the CompLex corpus, which has been published and described by Shardlow et al. (2020). The annotated sentences were collected using three different corpora: the Europarl corpus (Koehn, 2005), which includes the proceedings of the European Parliament; the CRAFT biomedical corpus (Bada et al., 2012); and the Bible, in the modern version of the World English Bible translation (Christodouloupoulos and Steedman, 2015). The organizers selected targets as either single words (Sub-Task 1) or multiword expressions (SubTask 2), and the datasets include also multiple examples with the same target, as different contexts can determine different complexity values. As for the multiword expressions, they were identified via syntactic patterns, being either adjective-noun or noun-noun phrases. 20"
2021.semeval-1.70,P14-5010,0,0.00258051,"CompLex. In total, we obtained 6 features (4 frequency + 2 length features) for each instance. We also added two Boolean features for Capitalization: the first was equal to 1 if the first letter of the target word was upper case and 0 otherwise; the second one was equal to 1 if all the letters of the target word were upper case and 0 otherwise. The latter feature was added because we noticed that some of the target words in the dataset are acronyms. Apart from the lexical information, Syntactic Features were explored for both single words and 566 multiword expressions. The StanfordNLP tools (Manning et al., 2014) were first used to acquire both the part-of-speech (POS) tags and dependency trees. POS tags of target words were manipulated using one-hot encoding, for a total of 20 POS-based features. On the other hand, directed and path from the target word to the root were extracted as dependency features. We concatenated all dependency tags to the root, using one-hot encoding once again to encode every distinct path as a single feature. In total, we generated 267 dependency paths features with this mechanism. Another feature was based on Word Embedding similarity: first, we computed the sum of the embe"
2021.semeval-1.70,2021.cmcl-1.5,0,0.0626278,"Missing"
2021.semeval-1.70,2020.readi-1.9,0,0.14381,"ker has to master, and it is assessed by comparing different languages (McWhorter, 2001; Parkvall, 2008). On the other hand, in the perspective of psycholinguistics and cognitive science, the notion of complexity can be described as the difficulty encountered by language users while processing concrete linguistic realizations (sentences, utterances etc.) (Blache, 2011; Chersoni et al., 2016, 2017, 2021; Iavarone et al., 2021; Sarti et al., 2021). Finally, in the Computational Linguistics community, the assessment of complexity at the lexical level is often related to readability applications (Shardlow et al., 2020), with the goal of determining if a word in a given text will be difficult to understand for the language users. Such applications are extremely useful for second language learners, for speakers with relatively low literacy and for people with reading disabilities, helping to tailor the difficult level of the texts to the needs of the target users. Task 1 of SemEval 2021 (Shardlow et al., 2021) aims at the development of systems for the estimation of lexical complexity in context, both for single words and for multiword expressions. The organizers provided two datasets with the target words in"
2021.semeval-1.70,D18-1499,0,0.0547684,"Missing"
2021.semeval-1.70,W18-0507,0,0.0594112,"Missing"
2021.semeval-1.70,W17-5910,0,0.0184583,"variable: given a word in context, the word will be judged as complex or not. Of course, this was a simplifying assumption, since there might be many situations where the boundary is not a clear-cut one, and annotators would rather indicate a value in a continuous scale. Moreover, the ”complex” words in the data only needed to be categorized as such by just one 565 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 565–570 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics of the annotators. A further study by Zampieri et al. (2017) analyzed the output of the participating systems, showing that modeling complexity as binary actually hindered their performance. A second iteration of the shared task was organized in 2018 (Yimam et al., 2018), this time features two separate subtasks: the traditional binary classification task, where systems had to predict whether one word was complex or not, and a regression task, where systems had to estimate the probability that an annotator would have considered a given word as complex. Recently, Shardlow et al. (2020) have introduced CompLex, a new gold standard for the estimation of l"
C08-1062,W04-3247,0,0.249297,"(4) rank SB given that SA is provided. Among them, (4) is of most concern. It should be noting that both (2) and (4) need to consider the influence from the sentences in the same and different collections. In this study, we made an attempt to capture the intuition that “A sentence receives a positive influence from the sentences that correlate to it in the same collection, whereas a sentence receives a negative influence from the sentences that correlates to it in the different collection.” We represent the sentences in A or B as a text graph constructed using the same approach as was used in Erkan and Radev (2004a, 2004b). Different from the existing PageRank-like algorithms adopted in document summarization, we propose a novel sentence ranking algorithm, called PNR2 (Ranking with Positive and Negative Reinforcement). While PageRank models the positive mutual reinforcement among the sentences in the graph, PNR2 is capable of modeling both positive and negative reinforcement in the ranking process. The remainder of this paper is organized as follows. Section 2 introduces the background of the work presented in this paper, including existing graph-based summarization models, descriptions of update summa"
C08-1062,P06-1047,1,0.861538,"hich was then used as the criterion to rank and select summary sentences. Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year. Besides, they reported experimental comparison of three different graph-based sentence ranking algorithms obtained from Positional Power Function, HITS and PageRank (Mihalcea and Tarau, 2005). Both HITS and PageRank performed excellently. Likewise, the use of PageRank family was also very popular in event-based summarization approaches (Leskovec et al., 2004; Vanderwende et al., 2004; Yoshioka and Haraguchi, 2004; Li et al., 2006). In contrast to conventional sentencebased approaches, newly emerged event-based approaches took event terms, such as verbs and action nouns and their associated named entities as graph nodes, and connected nodes according to their co-occurrence information or semantic dependency relations. They were able to provide finer text representation and thus could be in favor of sentence compression which was targeted to include more informative contents in a fixed-length summary. Nevertheless, these advantages lied on appropriately defining and selecting event terms. All above-mentioned representati"
C08-1062,N03-1020,0,0.205294,"summary the highest ranked sentence of concern if it doesn’t significantly repeat the information already included in the summary until the word limitation is reached. Average number of documents Average number of sentences A 10 237.6 B 10 177.3 Table 1. Basic Statistics of DUC2007 Update Data Set As for the evaluation metric, it is difficult to come up with a universally accepted method that can measure the quality of machine-generated summaries accurately and effectively. Many literatures have addressed different methods for automatic evaluations other than human judges. Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. Given the fact that judgments by humans are timeconsuming and labor-intensive, and more important, ROUGE has been officially adopted for the DUC evaluations since 2005, like the other researchers, we also choose it as the evaluation criteria. In the following experiments, the sentences and the queries are all represented as the vectors of words. The relevance of a sentence to the query is calculated by cosine similarity. Notice that the word weights are normally measured by the document-level TF*IDF sche"
C08-1062,P04-3020,0,0.0697616,"Missing"
C08-1062,W04-3252,0,\N,Missing
C08-1062,H05-1115,0,\N,Missing
C08-1130,I05-3009,0,0.18405,"Missing"
C08-1130,J04-1004,0,0.338088,"Missing"
C08-1130,W02-1407,0,0.390796,"Missing"
C08-1130,C02-1125,0,0.511096,"Missing"
C08-1130,P07-2018,0,0.0627672,"Missing"
C08-1130,W03-1704,0,0.897926,"Missing"
C08-1130,W93-0104,0,0.0852044,"Missing"
C08-1130,W01-0513,0,0.115009,"gy, Harbin Institute of Technology, Harbin 150001, China tjzhao@mtlab.hit.edu .cn extraction is an essential task in domain knowledge acquisition which can be used for lexicon update, domain ontology construction, etc. Term extraction involves two steps. The first step extracts candidates by unithood calculation to qualify a string as a valid term. The second step verifies them through termhood measures (Kageura and Umino, 1996) to validate their domain specificity. Existing techniques extract term candidates mainly by two kinds of statistic based measures including internal association (e.g. Schone and Jurafsky, 2001) and context dependency (e.g. Sornlertlamvanich et al., 2000). These techniques are also used in Chinese term candidate extraction (e.g. Luo and Sun, 2003; Ji and Lu, 2007). Domain dependent features of domain terms are used in a weighted manner to identify term boundaries. However, these algorithms always face the dilemma that fewer features are not enough to identify terms from non-terms whereas more features lead to more conflicts among selected features in a specific instance. Most term verification techniques use features on the difference in distribution of a term occurred within a domai"
C08-1130,C00-2116,0,0.353401,"tjzhao@mtlab.hit.edu .cn extraction is an essential task in domain knowledge acquisition which can be used for lexicon update, domain ontology construction, etc. Term extraction involves two steps. The first step extracts candidates by unithood calculation to qualify a string as a valid term. The second step verifies them through termhood measures (Kageura and Umino, 1996) to validate their domain specificity. Existing techniques extract term candidates mainly by two kinds of statistic based measures including internal association (e.g. Schone and Jurafsky, 2001) and context dependency (e.g. Sornlertlamvanich et al., 2000). These techniques are also used in Chinese term candidate extraction (e.g. Luo and Sun, 2003; Ji and Lu, 2007). Domain dependent features of domain terms are used in a weighted manner to identify term boundaries. However, these algorithms always face the dilemma that fewer features are not enough to identify terms from non-terms whereas more features lead to more conflicts among selected features in a specific instance. Most term verification techniques use features on the difference in distribution of a term occurred within a domain and across domains, such as TF-IDF (Salton and McGill, 1983"
C10-2076,P98-1013,0,0.118461,"hieved by using the SVM classifier with both the basic features and the combined features. Experimental results on Chinese Proposition Bank (CPB) show that the method outperforms the traditional constituent-based or dependency-based SRL methods. 1 Introduction Semantic role labeling (SRL) is a major method in current semantic analysis which is important to NLP applications. The SRL task is to identify semantic roles (or arguments) of each predicate and then label them with their functional tags, such as 'Arg0' and 'ArgM' in PropBank (Palmer et al., 2005), or 'Agent' and 'Patient' in FrameNet (Baker et al., 1998). The significance of syntactic analysis in SRL has been proven by (Gildea and Palmer, 2002; Punyakanok et al., 2005), and syntactic parsing has been applied by almost all current studies. In terms of syntactic representations, the SRL approaches are mainly divided into three categories: constituent-based, chunkbased and dependency-based. Constituentbased SRL has been studied intensively with satisfactory results. Chunk-based SRL has been found to be less effective than the constituent-based by (Punyakanok et al., 2005). In recent years, the dependency-based SRL has been greatly promoted by th"
C10-2076,P09-1005,0,0.106569,"ed use of both constituentbased and dependency-based features in addition to using features of singular types of syntactic view. We propose a statistical method to select effective combined features using both constituent-based and dependency-based features to make full use of two syntactic views. 2 Related Work In recent years, many advances have been made on SRL using singular syntactic view, such as constituent (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Surdeanu et al., 2007), dependency (Hacioglu, 2004; Johansson and Nugues, 2008; Zhao et al., 2009), and CCG (Chen and Rambow, 2003; Boxwell et al, 2009). However, there are few studies on the use of multiple syntactic views. We briefly review the relevant studies of SRL using multiple syntactic views as follows. Pradhan et al. (2005) built three semantic role labelers using constituent, dependency and chunk syntactic views, and then heuristically combined them at the output level. The method was further improved in Pradhan et al. (2008) which trains two semantic role labelers for constituents and dependency separately, and then uses the output of the two systems as additional features in another labeler using chunk parsing. The result shows a"
C10-2076,W03-1006,0,0.0264139,"work, we exploit combined use of both constituentbased and dependency-based features in addition to using features of singular types of syntactic view. We propose a statistical method to select effective combined features using both constituent-based and dependency-based features to make full use of two syntactic views. 2 Related Work In recent years, many advances have been made on SRL using singular syntactic view, such as constituent (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Surdeanu et al., 2007), dependency (Hacioglu, 2004; Johansson and Nugues, 2008; Zhao et al., 2009), and CCG (Chen and Rambow, 2003; Boxwell et al, 2009). However, there are few studies on the use of multiple syntactic views. We briefly review the relevant studies of SRL using multiple syntactic views as follows. Pradhan et al. (2005) built three semantic role labelers using constituent, dependency and chunk syntactic views, and then heuristically combined them at the output level. The method was further improved in Pradhan et al. (2008) which trains two semantic role labelers for constituents and dependency separately, and then uses the output of the two systems as additional features in another labeler using chunk parsi"
C10-2076,D08-1034,0,0.035678,"Missing"
C10-2076,J02-3001,0,0.53152,"g several combined features each of which is composed by two single features (Xue and Palmer, 2004; Toutanova et al., 2005; Zhao et al., 2009). Thus, in this work, we exploit combined use of both constituentbased and dependency-based features in addition to using features of singular types of syntactic view. We propose a statistical method to select effective combined features using both constituent-based and dependency-based features to make full use of two syntactic views. 2 Related Work In recent years, many advances have been made on SRL using singular syntactic view, such as constituent (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Surdeanu et al., 2007), dependency (Hacioglu, 2004; Johansson and Nugues, 2008; Zhao et al., 2009), and CCG (Chen and Rambow, 2003; Boxwell et al, 2009). However, there are few studies on the use of multiple syntactic views. We briefly review the relevant studies of SRL using multiple syntactic views as follows. Pradhan et al. (2005) built three semantic role labelers using constituent, dependency and chunk syntactic views, and then heuristically combined them at the output level. The method was further improved in Pradhan et al. (2008) which trains two semantic role la"
C10-2076,P02-1031,0,0.0182034,"es. Experimental results on Chinese Proposition Bank (CPB) show that the method outperforms the traditional constituent-based or dependency-based SRL methods. 1 Introduction Semantic role labeling (SRL) is a major method in current semantic analysis which is important to NLP applications. The SRL task is to identify semantic roles (or arguments) of each predicate and then label them with their functional tags, such as 'Arg0' and 'ArgM' in PropBank (Palmer et al., 2005), or 'Agent' and 'Patient' in FrameNet (Baker et al., 1998). The significance of syntactic analysis in SRL has been proven by (Gildea and Palmer, 2002; Punyakanok et al., 2005), and syntactic parsing has been applied by almost all current studies. In terms of syntactic representations, the SRL approaches are mainly divided into three categories: constituent-based, chunkbased and dependency-based. Constituentbased SRL has been studied intensively with satisfactory results. Chunk-based SRL has been found to be less effective than the constituent-based by (Punyakanok et al., 2005). In recent years, the dependency-based SRL has been greatly promoted by the CoNLL shared tasks on semantic parsing (Hajic et al., 2009). However, there is not much r"
C10-2076,C04-1186,0,0.101237,"Palmer, 2004; Toutanova et al., 2005; Zhao et al., 2009). Thus, in this work, we exploit combined use of both constituentbased and dependency-based features in addition to using features of singular types of syntactic view. We propose a statistical method to select effective combined features using both constituent-based and dependency-based features to make full use of two syntactic views. 2 Related Work In recent years, many advances have been made on SRL using singular syntactic view, such as constituent (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Surdeanu et al., 2007), dependency (Hacioglu, 2004; Johansson and Nugues, 2008; Zhao et al., 2009), and CCG (Chen and Rambow, 2003; Boxwell et al, 2009). However, there are few studies on the use of multiple syntactic views. We briefly review the relevant studies of SRL using multiple syntactic views as follows. Pradhan et al. (2005) built three semantic role labelers using constituent, dependency and chunk syntactic views, and then heuristically combined them at the output level. The method was further improved in Pradhan et al. (2008) which trains two semantic role labelers for constituents and dependency separately, and then uses the outpu"
C10-2076,D08-1008,0,0.0809213,"outanova et al., 2005; Zhao et al., 2009). Thus, in this work, we exploit combined use of both constituentbased and dependency-based features in addition to using features of singular types of syntactic view. We propose a statistical method to select effective combined features using both constituent-based and dependency-based features to make full use of two syntactic views. 2 Related Work In recent years, many advances have been made on SRL using singular syntactic view, such as constituent (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Surdeanu et al., 2007), dependency (Hacioglu, 2004; Johansson and Nugues, 2008; Zhao et al., 2009), and CCG (Chen and Rambow, 2003; Boxwell et al, 2009). However, there are few studies on the use of multiple syntactic views. We briefly review the relevant studies of SRL using multiple syntactic views as follows. Pradhan et al. (2005) built three semantic role labelers using constituent, dependency and chunk syntactic views, and then heuristically combined them at the output level. The method was further improved in Pradhan et al. (2008) which trains two semantic role labelers for constituents and dependency separately, and then uses the output of the two systems as addi"
C10-2076,P03-1056,0,0.0401976,"ween the z-score of the combined (1) IntraDist f ( D pos , Dneg ) i i i i 668 all approach, in which six SVMs will be trained to separate each semantic type from the remaining types. We divide the corpus into three parts: the first 99 documents (chtb_001.fid to chtb_099.fid) serve as the test data, the last 32 documents (chtb_900.fid to chtb_931.fid) serve as the development data and the left 629 documents (chtb_100.fid to chtb_899.fid) serve as the training data. We use the SVM-Light Toolkit version 6.02 (Joachims, 1999) for the implementation of SVM, and use the Stanford Parser version 1.6 (Levy and Manning, 2003) as the constituent parser and the constituent-to-dependency converter. In classifications, we employ the linear kernel for SVM and set the regularization parameter to the default value which is the reciprocal of the average Euclidean norm of training data. The performance metrics are: accuracy (A), precision (P), recall (R) and F-score (F). feature and its two corresponding basic features as given in (9). I ( f ab ) = Z ( f ab ) − Max ( Z ( f a ), Z ( f b ) ) (9) Finally, the combined feature with a negative I ( f ab ) value is eliminated. Then, we will rank the combined features in terms of"
C10-2076,de-marneffe-etal-2006-generating,0,0.0151111,"Missing"
C10-2076,D09-1143,0,0.0228418,"is extremely high. Hacioglu (2004) proposed a SRL method to combine constituent and dependency syntactic views where the dependency parses are ob-tained through automatic mapping of constitu-ent parses. It uses the constituent parses to get candidates and then, the dependency parses to label them. Boxwell et al. (2009) proposed a SRL method using features of three syntactic views: 666 CCG, CFG and dependency. It primarily uses CCG-based features associated with 4 CFGbased and 2 dependency-based features. The combination of these syntactic views leads to a substantial performance improvement. Nguyen et al. (2009) proposed a composite kernel based on both constituent and dependency syntactic views and achieved a significant improvement in a relation extraction application. 3 Design Principle and Basic Features Compared to related work, the proposed method integrates the constituent and dependency views in a collaborative manner. First, we define a basic feature set containing features from constituent and dependency syntactic views. Then, to make better use of two syntactic views, we introduce a statistical method to select effective combined features from the basic feature set. Finally we use both the"
C10-2076,J05-1004,0,0.112907,"features which are composed by the basic features. SRL is achieved by using the SVM classifier with both the basic features and the combined features. Experimental results on Chinese Proposition Bank (CPB) show that the method outperforms the traditional constituent-based or dependency-based SRL methods. 1 Introduction Semantic role labeling (SRL) is a major method in current semantic analysis which is important to NLP applications. The SRL task is to identify semantic roles (or arguments) of each predicate and then label them with their functional tags, such as 'Arg0' and 'ArgM' in PropBank (Palmer et al., 2005), or 'Agent' and 'Patient' in FrameNet (Baker et al., 1998). The significance of syntactic analysis in SRL has been proven by (Gildea and Palmer, 2002; Punyakanok et al., 2005), and syntactic parsing has been applied by almost all current studies. In terms of syntactic representations, the SRL approaches are mainly divided into three categories: constituent-based, chunkbased and dependency-based. Constituentbased SRL has been studied intensively with satisfactory results. Chunk-based SRL has been found to be less effective than the constituent-based by (Punyakanok et al., 2005). In recent year"
C10-2076,P05-1072,0,0.256083,"Missing"
C10-2076,J08-2006,0,0.0192923,"ic view, such as constituent (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Surdeanu et al., 2007), dependency (Hacioglu, 2004; Johansson and Nugues, 2008; Zhao et al., 2009), and CCG (Chen and Rambow, 2003; Boxwell et al, 2009). However, there are few studies on the use of multiple syntactic views. We briefly review the relevant studies of SRL using multiple syntactic views as follows. Pradhan et al. (2005) built three semantic role labelers using constituent, dependency and chunk syntactic views, and then heuristically combined them at the output level. The method was further improved in Pradhan et al. (2008) which trains two semantic role labelers for constituents and dependency separately, and then uses the output of the two systems as additional features in another labeler using chunk parsing. The result shows an improvement to each labeler alone. A possible reason for the improvement is that the errors caused by different syntactic parsers are compensated. Yet, the features of different syntactic views can hardly complement each other in labeling. And the complexity of using multiple syntactic parsers is extremely high. Hacioglu (2004) proposed a SRL method to combine constituent and dependenc"
C10-2076,W05-0639,0,0.0461083,"Missing"
C10-2076,P05-1073,0,0.105942,"ng methods to predict the semantic labels. The method also involves two classification phases: semantic role identification (SRI) and semantic role classification (SRC). In addition, a heuristicbased pruning preprocessing (Xue and Palmer, 2004) is used to filter out a lot of apparently inappropriate constituents at the beginning. 665 Coling 2010: Poster Volume, pages 665–673, Beijing, August 2010 And it has been widely reported that, in feature-based SRL, the performance can be improved by adding several combined features each of which is composed by two single features (Xue and Palmer, 2004; Toutanova et al., 2005; Zhao et al., 2009). Thus, in this work, we exploit combined use of both constituentbased and dependency-based features in addition to using features of singular types of syntactic view. We propose a statistical method to select effective combined features using both constituent-based and dependency-based features to make full use of two syntactic views. 2 Related Work In recent years, many advances have been made on SRL using singular syntactic view, such as constituent (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Surdeanu et al., 2007), dependency (Hacioglu, 2004; Johansson and Nugues,"
C10-2076,W04-3212,0,0.414605,"ntroduces a novel method for Chinese SRL utilizing both constituent-based and dependency-based features. The method takes constituent as the basic unit of argument and adopts the labeling of PropBank. It follows the prevalent feature-based SRL methods to first turn predicate-argument pairs into flat structures by well-defined linguistic features, and then uses machine learning methods to predict the semantic labels. The method also involves two classification phases: semantic role identification (SRI) and semantic role classification (SRC). In addition, a heuristicbased pruning preprocessing (Xue and Palmer, 2004) is used to filter out a lot of apparently inappropriate constituents at the beginning. 665 Coling 2010: Poster Volume, pages 665–673, Beijing, August 2010 And it has been widely reported that, in feature-based SRL, the performance can be improved by adding several combined features each of which is composed by two single features (Xue and Palmer, 2004; Toutanova et al., 2005; Zhao et al., 2009). Thus, in this work, we exploit combined use of both constituentbased and dependency-based features in addition to using features of singular types of syntactic view. We propose a statistical method to"
C10-2076,J08-2004,0,0.0140941,"improve the feature-based SRL method. system (Liu et al. 2007) which uses 19 basic The complexity of the method will not increase features, 10 combined features and also the significantly compared to the method using ME classifier. Third, the 'Che' (Che, 2008) sys- one syntactic view as we use a constituent-totem use a hybrid convolution tree kernel to dependency conversion rather than additional directly measure the similarity between two dependency parsing. The effectiveness of the constituent structures. Fourth, the 'Xue2' sys- method has been proven by the experiments on tem described in (Xue, 2008), which is similar CPB using SVM classifier with linear kernel. to 'Xue1' on basic framework, but using a new feature set. The 'Xue2' system evaluates the Acknowledgments SRL of the verbal predicates and the nominal- This work is supported by the Key Program of ized predicates separately, and offers no con- National Natural Science Foundation of China solidated evaluation in (Xue, 2008). So in the under Grant No. 60736014, the Key Project of comparison, we refer to its performance on the the National High Technology Research and verbal predicates and the nominalized predi- Development Program"
C10-2076,D09-1004,0,0.179281,"Missing"
C10-2076,N07-1070,0,\N,Missing
C10-2076,C98-1013,0,\N,Missing
C10-2076,W09-1201,0,\N,Missing
C10-2076,N04-1030,0,\N,Missing
C10-2106,A97-1042,0,0.0961085,". The MEAD system performed very well in the generic multi-document summarization task of the DUC 2004 competition. Later, position information is also applied to more summarization tasks. For example, in queryfocused task, sentence position features are widely used in learning-based summarization systems as a component feature for calculating the composite sentence score (Ouyang et al, 2007; Toutanova et al, 2007). However, the effect of position features alone was not studied in these works. There were also studies aimed at analyzing and explaining the effectiveness of position information. Lin and Hovy (1997) provided an empirical validation on the sentence position hypothesis. For each position, the sentence position yield was defined as the average value of the significance of the sentences with the fixed position. It was observed that the average significance at earlier positions was indeed larger. Nenkova (2005) did a conclusive overview on the DUC 2001-2004 evaluation results. It was reported that position information is very effective in generic summarization. In generic single-document summarization, a leadbased baseline that simply takes the leading sentences as the summary can outperform"
C10-2106,P08-2052,0,0.0188063,"position. It was observed that the average significance at earlier positions was indeed larger. Nenkova (2005) did a conclusive overview on the DUC 2001-2004 evaluation results. It was reported that position information is very effective in generic summarization. In generic single-document summarization, a leadbased baseline that simply takes the leading sentences as the summary can outperform most submitted summarization system in DUC 2001 and 2002. As in multi-document summarization, the position-based baseline system is competitive in generating short summaries but not in longer summaries. Schilder and Kondadadi (2008) analyzed the effectiveness of the features that are used in their learning-based sentence scoring model for query-focused summarization. By comparing the ROUGE-2 results of each individual feature, it was reported that position-based features are less effective than frequency-based features. In (Gillick et al., 2009), the effect of position information in the update summarization task was studied. By using ROUGE to measure the density of valuable words at each sentence position, it was observed that the first sentence of newswire document was especially important for composing update summarie"
C10-2170,P05-1018,0,0.0472047,"ith directions to future work. 2 Related Work In MDS, information ordering is often realized on the sentence level and treated as a coherence enhancement task. A simple ordering criterion is the chronological order of the events represented in the sentences, which is often augmented with other ordering criteria such as lexical overlap (Conroy et al., 2006), lexical cohesion (Barzilay et al., 2002) or syntactic features (Lapata 2003). A different way to capture local coherence in sentence ordering is the Centering Theory (CT, Grosz et al. 1995)-inspired entity-transition approach, advocated by Barzilay and Lapata (2005, 2008). In their entity grid model, syntactic roles played by entities and transitions between these syntactic roles underlie the coherence patterns between sentences and in the 1489 Coling 2010: Poster Volume, pages 1489–1497, Beijing, August 2010 whole text. An entity-parsed corpus can be used to train a model that prefers the sentence orderings that comply with the optimal entity transition patterns. Another important clue to sentence ordering is the sentence positional information in a source document, or “precedence relation”, which is utilized by Okazaki et al. (2004) in combination wit"
C10-2170,P03-1069,0,0.427224,"luding layered clustering and cluster-based ordering. The performance of the event-enriched model will be extensively evaluated in section 6. Section 7 will conclude the work with directions to future work. 2 Related Work In MDS, information ordering is often realized on the sentence level and treated as a coherence enhancement task. A simple ordering criterion is the chronological order of the events represented in the sentences, which is often augmented with other ordering criteria such as lexical overlap (Conroy et al., 2006), lexical cohesion (Barzilay et al., 2002) or syntactic features (Lapata 2003). A different way to capture local coherence in sentence ordering is the Centering Theory (CT, Grosz et al. 1995)-inspired entity-transition approach, advocated by Barzilay and Lapata (2005, 2008). In their entity grid model, syntactic roles played by entities and transitions between these syntactic roles underlie the coherence patterns between sentences and in the 1489 Coling 2010: Poster Volume, pages 1489–1497, Beijing, August 2010 whole text. An entity-parsed corpus can be used to train a model that prefers the sentence orderings that comply with the optimal entity transition patterns. Ano"
C10-2170,P06-1047,1,0.90623,"ences by enhancing coherence since incoherence is the source of disorder. Recent researches in this direction mostly focus on local coherence by studying lexical cohesion (Conroy et al., 2006) or entity overlap and transition (Barzilay and Lapata, 2008). But global coherence, i.e., coherence between sentence groups with the whole text in view, is largely unaccounted for and few efforts are made at levels higher than entity or word in measuring sentence coherence. On the other hand, event as a high-level construct has proved useful in MDS content selection (Filatova and Hatzivassiloglou, 2004; Li et al., 2006). But the potential of event in summarization has not been fully gauged and few publications report using event in MDS information ordering. We will argue that event is instrumental for MDS information ordering, especially multi-document news summarization (MDNS). Ordering algorithms based on event and entity information outperform those based only on entity information. After related works are surveyed in section 2, we will discuss in section 3 the problem of semantic deficiency in IR-based text processing, which motivates building event information into sentence representation. The details o"
C10-2170,P07-2047,1,0.832055,"nt), but the extracted event-enriched representations help to alleviate the semantic deficiency problem in IR. 4.2 Event Relations The relations between two events include event term relation and event entity relation. Two events are similar if their event terms are similar and/or their event entities are similar. Such similarities are in turn defined on the word level. For event terms, we first find the root verbs of deverbal nouns and then measure verb similarity by using the fine-grained relations provided by VerbOcean (Chklovski and Pantel, 2004), which has proved useful in summarization (Liu et al., 2007). But unlike (Liu et al., 2007), we count in all the verb relations except antonymy because considering two antonymous verbs as similar is counterintuitive. The other four relations – similarity, strength, enablement, before – are all considered in our measurement of verb similarity. If we denote the normalized score of two verbs on relation i as VOi(V1, V2) with i = 1, 2, 3, 4 corresponding to the above four relations, the term similarity of two events ȝt(E1, E2) is defined as in Eq. 2, where İ is a small number to suppress zeroes. İ = 0.01 if VOi(V1, V2) = 1 and otherwise İ = 0. ȝt(E1, E2) ="
C10-2170,P06-1049,0,0.162047,"rsions, marked by * (p &lt; .05) and ** (p &lt; .01) on a two-tailed t-test. Peer Code A B C D E F G H I 200w Kendall’s Ĳ AC 0.014** 0.009** 0.387 0.151* 0.369* 0.128* 0.380 0.163 0.375* 0.156* 0.388 0.159* 0.385 0.158* 0.384 0.164 0.395 0.170 400w Kendall’s Ĳ AC -0.019** 0.004** 0.259** 0.151* 0.264* 0.156* 0.270* 0.158* 0.267* 0.157* 0.264* 0.157* 0.269* 0.162 0.292* 0.170 0.350 0.176 Table 3. Evaluation Result Table 2. Peer Orderings 6.3 where m is the number of inversions described above and n is the total number of sentences. The second metric we use is the Average Continuity (AC) developed by Bollegala et al. (2006), which captures the intuition that the ordering quality can be estimated by the number of correctly arranged continuous sentences. ଵ σ log(ܲ + ߝ)  = ܥܣexp( (Eq. 17) ିଵ ୀଶ where k is the maximum number of continuous sentences, ߝ is a small value in case Pn = 1. Pn, the proportion of continuous sentences of length n in an ordering, is defined as m/(N – n + 1) where m is the number of continuous sentences of length n in both the test and reference orderings and N is the total number of sentences. We set k = 4 and ߝ = 0.01. Metrics A popular metric used in sequence evaluation is Kendall’s Ĳ"
C10-2170,C04-1108,0,0.654885,", advocated by Barzilay and Lapata (2005, 2008). In their entity grid model, syntactic roles played by entities and transitions between these syntactic roles underlie the coherence patterns between sentences and in the 1489 Coling 2010: Poster Volume, pages 1489–1497, Beijing, August 2010 whole text. An entity-parsed corpus can be used to train a model that prefers the sentence orderings that comply with the optimal entity transition patterns. Another important clue to sentence ordering is the sentence positional information in a source document, or “precedence relation”, which is utilized by Okazaki et al. (2004) in combination with topical clustering. Those works are all relevant to the current work because we seek ordering clues from chronological order, lexical cohesion, entity transition, and sentence precedence. But we also add an important member to the panoply – event. Despite its intuitive and conceptual appeal, event is not as extensively used in summarization as term or entity. Filatova and Hatzivassiloglou (2004) use “atomic events” as conceptual representations in MDS content selection, followed by Li et al. (2006) who treat event terms and named entities as graph nodes in their PageRank a"
C10-2170,W04-3205,0,0.0169161,"admittedly coarse-grade (e.g., “storm” is missing from the “moving” event), but the extracted event-enriched representations help to alleviate the semantic deficiency problem in IR. 4.2 Event Relations The relations between two events include event term relation and event entity relation. Two events are similar if their event terms are similar and/or their event entities are similar. Such similarities are in turn defined on the word level. For event terms, we first find the root verbs of deverbal nouns and then measure verb similarity by using the fine-grained relations provided by VerbOcean (Chklovski and Pantel, 2004), which has proved useful in summarization (Liu et al., 2007). But unlike (Liu et al., 2007), we count in all the verb relations except antonymy because considering two antonymous verbs as similar is counterintuitive. The other four relations – similarity, strength, enablement, before – are all considered in our measurement of verb similarity. If we denote the normalized score of two verbs on relation i as VOi(V1, V2) with i = 1, 2, 3, 4 corresponding to the above four relations, the term similarity of two events ȝt(E1, E2) is defined as in Eq. 2, where İ is a small number to suppress zeroes."
C10-2170,W06-2407,0,0.0155647,"to introduce event. 4 Event-Enriched SentenceRepresentation In summarization, an event is an activity or episode associated with participants, time, place, and manner. Conceptually, event bridges sentence and term/entity and partially fills the semantic gap in the sentence representation. 4.1 Event Structure and Extraction Following (Li et al. 2006), we define an event E as a structured semantic unit consisting of one event term Term(E) and a set of event entities Entity(E). In the news domain, event terms are typically action verbs or deverbal nouns. Light verbs such as “take”, “give”, etc. (Tan et al., 2006) are removed. Event entities include named entities and high-frequency entities. Named entities denote people, locations, organizations, dates, etc. High-frequency entities are common nouns or NPs that frequently participate in news events. Filatova and Hatzivassiloglou (2004) take the top 10 most frequent entities and Li et al. (2006) take the entities with frequency &gt; 10. Rather than using a fixed threshold, we reformulate “high-frequency” as relative statistics based on (assumed) Gaussian distribution of the entities and consider those with z-score &gt; 1 as candidate event entities. Event ext"
C10-2170,W04-1017,0,0.179351,"A sensible solution is ordering sentences by enhancing coherence since incoherence is the source of disorder. Recent researches in this direction mostly focus on local coherence by studying lexical cohesion (Conroy et al., 2006) or entity overlap and transition (Barzilay and Lapata, 2008). But global coherence, i.e., coherence between sentence groups with the whole text in view, is largely unaccounted for and few efforts are made at levels higher than entity or word in measuring sentence coherence. On the other hand, event as a high-level construct has proved useful in MDS content selection (Filatova and Hatzivassiloglou, 2004; Li et al., 2006). But the potential of event in summarization has not been fully gauged and few publications report using event in MDS information ordering. We will argue that event is instrumental for MDS information ordering, especially multi-document news summarization (MDNS). Ordering algorithms based on event and entity information outperform those based only on entity information. After related works are surveyed in section 2, we will discuss in section 3 the problem of semantic deficiency in IR-based text processing, which motivates building event information into sentence representat"
C10-2170,J95-2003,0,0.057497,"xtensively evaluated in section 6. Section 7 will conclude the work with directions to future work. 2 Related Work In MDS, information ordering is often realized on the sentence level and treated as a coherence enhancement task. A simple ordering criterion is the chronological order of the events represented in the sentences, which is often augmented with other ordering criteria such as lexical overlap (Conroy et al., 2006), lexical cohesion (Barzilay et al., 2002) or syntactic features (Lapata 2003). A different way to capture local coherence in sentence ordering is the Centering Theory (CT, Grosz et al. 1995)-inspired entity-transition approach, advocated by Barzilay and Lapata (2005, 2008). In their entity grid model, syntactic roles played by entities and transitions between these syntactic roles underlie the coherence patterns between sentences and in the 1489 Coling 2010: Poster Volume, pages 1489–1497, Beijing, August 2010 whole text. An entity-parsed corpus can be used to train a model that prefers the sentence orderings that comply with the optimal entity transition patterns. Another important clue to sentence ordering is the sentence positional information in a source document, or “precede"
C10-2170,J08-1001,0,\N,Missing
chen-etal-2006-study,J90-1003,0,\N,Missing
chen-etal-2006-study,W03-1725,0,\N,Missing
chen-etal-2006-study,W02-1407,0,\N,Missing
chen-etal-2006-study,C02-1125,0,\N,Missing
chen-etal-2006-study,I05-3009,0,\N,Missing
chen-etal-2008-chinese,J04-2002,0,\N,Missing
chen-etal-2008-chinese,huang-etal-2004-sinica,0,\N,Missing
chen-etal-2008-chinese,I08-7003,1,\N,Missing
cui-etal-2008-corpus,P98-1013,0,\N,Missing
cui-etal-2008-corpus,C98-1013,0,\N,Missing
D16-1170,D14-1190,0,0.0228518,"ysis is to determine the taxonomy of emotions. Researchers have proposed a list of primary emotions(Plutchik, 1980; Ekman, 1984; Turner, 2000). In this study, we adopt Ekman’s emotion classification (Ekman, 1984), which identifies six primary emotions, namely happiness, sadness, fear, anger, disgust and surprise, known as the “Big6”1 scheme in the W3C Emotion Markup Language. This list is agreed upon by most previous works in Chinese emotion analysis. The second issue is how to do emotion classification and emotion information extraction. 1 http://www.w3.org/TR/emotion-voc/xml#big6 1640 Beck (Beck et al., 2014) proposed a Multi-task Gaussian-process based method for emotion classification. Xu (Xu et al., 2012) used a coarse to fine method to classify emotions in Chinese blog. Gao (Gao et al., 2013) proposed a joint model to cotrain a polarity classifier and an emotion classifier. Chang (Chang et al., 2015) used linguistic template to predict reader’s emotions. Das (Das and Bandyopadhyay, 2010) used an unsupervised method to extract emotion feelers from Bengali blog. There are other studies focused on joint learning with sentiment (Luo et al., 2015; Mohtarami et al., 2013), emotion in tweets or blog"
D16-1170,P15-2127,0,0.185522,"n Lu3 and Yu Zhou1 1. School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen Graduate School, Shenzhen, China 2. Guangdong Provincial Engineering Technology Research Center for Data Science 3. Department of Computing, the Hong Kong Polytechnic University, Hong Kong guilin.nlp@gmail.com;wudongyinhit@gmail.com;xuruifeng@hitsz.edu.cn; csluqin@comp.polyu.edu.hk;zhouyu.nlp@gmail.com Abstract tudies in emotion analysis focus on emotion classification including detection of emotions expressed by writers of text (Gao et al., 2013) as well as prediction of reader emotions (Chang et al., 2015). There are also some information extraction tasks in emotion analysis, such as extracting the feeler of emotion (Das and Bandyopadhyay, 2010). However, these methods need to observe emotion linked expressions. Sometimes, however, we care more about the stimuli, or the cause of an emotion. For instance, manufacturers want to know why people love, or hate a certain product. The White House may also prefer to know the cause of the emotional text “Let us hit the streets” rather than the distribution of different emotions. In this paper, we present our work in emotion cause extraction. Since there"
D16-1170,C10-1021,0,0.68951,"powerful medium anywhere and anytime. How to analyze the emotions of individuals through their writings becomes a new challenge for NLP. In recent years, s∗ corresponding author There are three main challenges in the study of emotion cause extraction. The first is that, up to now, there is no open dataset available for emotion cause extraction. This may explain why there are only few studies on emotion causes. The second is that, there is no formal definition about event in emotion cause extraction even though some researches claim that they extract events of emotion causes (Lee et al., 2010; Chen et al., 2010). The third is that, due to the complexity in annotation, the size of corpus for emotion cause extraction is usually very small. Due to this limitation, many machine learning methods are not suited for emotion cause detection. How to mine deep knowledge of a language for emotion causes is another thorny issue. In this paper, we first present an annotated dataset for emotion cause extraction to be released to the public. We then propose to use a 7-tuple to define emotion cause events. Based on this general defi1639 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Proc"
D16-1170,P02-1034,0,0.0357596,"ETs, emotion cause extraction becomes a classification problem. If an ET is an emotion cause, the label should be positive. Otherwise, the label should be negative. A binary classifier should be used. 4.2 Emotion Cause Extraction After the construction of ETs, we obtain positive and negative ET samples. Due to small amount of training samples, it is necessary to capture all features in the ETs. We choose convolution kernel based SVMs because it can search all possible syntactic features under a tree structure. Convolution kernel function The convolution kernel, also known as the tree kernel (Collins and Duffy, 2002), is widely used in many NLP tasks (Srivastava et al., 2013; Moschitti, 2006). For any two inputs T1 and T2 based on a tree structure , the kernel is defined as: K(T1 , T2 ) = X X δ(n1 , n2 ). (1) n1 ∈T1 n2 ∈T2 Here, n1 and n2 are tree nodes. δ is a function defined recursively: 1.δ(n1 , n2 ) = 0 if the productions of n1 and n2 are different; 2.Else, δ(n1 , n2 ) = 1 if n1 and n2 are matching in pre-terminals; 3.Otherwise, δ(n1 , n2 ) = Y (1 + δ(c(n1 , i), c(n2 , i))). i Here, c(n, i) is the i-th node of n. However, the above tree kernel definition does not consider terminals, which means that"
D16-1170,Y10-1071,0,0.176185,"China 2. Guangdong Provincial Engineering Technology Research Center for Data Science 3. Department of Computing, the Hong Kong Polytechnic University, Hong Kong guilin.nlp@gmail.com;wudongyinhit@gmail.com;xuruifeng@hitsz.edu.cn; csluqin@comp.polyu.edu.hk;zhouyu.nlp@gmail.com Abstract tudies in emotion analysis focus on emotion classification including detection of emotions expressed by writers of text (Gao et al., 2013) as well as prediction of reader emotions (Chang et al., 2015). There are also some information extraction tasks in emotion analysis, such as extracting the feeler of emotion (Das and Bandyopadhyay, 2010). However, these methods need to observe emotion linked expressions. Sometimes, however, we care more about the stimuli, or the cause of an emotion. For instance, manufacturers want to know why people love, or hate a certain product. The White House may also prefer to know the cause of the emotional text “Let us hit the streets” rather than the distribution of different emotions. In this paper, we present our work in emotion cause extraction. Since there is no open dataset available, the lack of annotated resources has limited the research in this area. Thus, we first present a dataset we buil"
D16-1170,P13-1095,0,0.012971,"proposed a Multi-task Gaussian-process based method for emotion classification. Xu (Xu et al., 2012) used a coarse to fine method to classify emotions in Chinese blog. Gao (Gao et al., 2013) proposed a joint model to cotrain a polarity classifier and an emotion classifier. Chang (Chang et al., 2015) used linguistic template to predict reader’s emotions. Das (Das and Bandyopadhyay, 2010) used an unsupervised method to extract emotion feelers from Bengali blog. There are other studies focused on joint learning with sentiment (Luo et al., 2015; Mohtarami et al., 2013), emotion in tweets or blog (Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014; Liu et al., 2013; Quan and Ren, 2009), and emotional lexicon construction (Yang et al., 2014; Staiano and Guerini, 2014; Mohammad and Turney, 2013). However, these related works all focused on analysis of emotion expressions rather than emotion causes.. Sophia M. Y. Lee first proposed a task on emotion cause extraction (Lee et al., 2010). They manually constructed a corpus from Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen and Lee (Chen et al., 2010) proposed a rule based method to detect emotion causes. The basic idea is to make"
D16-1170,W10-0206,0,0.696817,"ions through this powerful medium anywhere and anytime. How to analyze the emotions of individuals through their writings becomes a new challenge for NLP. In recent years, s∗ corresponding author There are three main challenges in the study of emotion cause extraction. The first is that, up to now, there is no open dataset available for emotion cause extraction. This may explain why there are only few studies on emotion causes. The second is that, there is no formal definition about event in emotion cause extraction even though some researches claim that they extract events of emotion causes (Lee et al., 2010; Chen et al., 2010). The third is that, due to the complexity in annotation, the size of corpus for emotion cause extraction is usually very small. Due to this limitation, many machine learning methods are not suited for emotion cause detection. How to mine deep knowledge of a language for emotion causes is another thorny issue. In this paper, we first present an annotated dataset for emotion cause extraction to be released to the public. We then propose to use a 7-tuple to define emotion cause events. Based on this general defi1639 Proceedings of the 2016 Conference on Empirical Methods in N"
D16-1170,P13-2091,0,0.0262073,"classification. Xu (Xu et al., 2012) used a coarse to fine method to classify emotions in Chinese blog. Gao (Gao et al., 2013) proposed a joint model to cotrain a polarity classifier and an emotion classifier. Chang (Chang et al., 2015) used linguistic template to predict reader’s emotions. Das (Das and Bandyopadhyay, 2010) used an unsupervised method to extract emotion feelers from Bengali blog. There are other studies focused on joint learning with sentiment (Luo et al., 2015; Mohtarami et al., 2013), emotion in tweets or blog (Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014; Liu et al., 2013; Quan and Ren, 2009), and emotional lexicon construction (Yang et al., 2014; Staiano and Guerini, 2014; Mohammad and Turney, 2013). However, these related works all focused on analysis of emotion expressions rather than emotion causes.. Sophia M. Y. Lee first proposed a task on emotion cause extraction (Lee et al., 2010). They manually constructed a corpus from Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen and Lee (Chen et al., 2010) proposed a rule based method to detect emotion causes. The basic idea is to make linguistic rules for cause extraction. Some studies (Gui e"
D16-1170,D15-1297,0,0.0275477,"tp://www.w3.org/TR/emotion-voc/xml#big6 1640 Beck (Beck et al., 2014) proposed a Multi-task Gaussian-process based method for emotion classification. Xu (Xu et al., 2012) used a coarse to fine method to classify emotions in Chinese blog. Gao (Gao et al., 2013) proposed a joint model to cotrain a polarity classifier and an emotion classifier. Chang (Chang et al., 2015) used linguistic template to predict reader’s emotions. Das (Das and Bandyopadhyay, 2010) used an unsupervised method to extract emotion feelers from Bengali blog. There are other studies focused on joint learning with sentiment (Luo et al., 2015; Mohtarami et al., 2013), emotion in tweets or blog (Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014; Liu et al., 2013; Quan and Ren, 2009), and emotional lexicon construction (Yang et al., 2014; Staiano and Guerini, 2014; Mohammad and Turney, 2013). However, these related works all focused on analysis of emotion expressions rather than emotion causes.. Sophia M. Y. Lee first proposed a task on emotion cause extraction (Lee et al., 2010). They manually constructed a corpus from Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen and Lee (Chen et al., 2010) propo"
D16-1170,P13-1097,0,0.0286999,"/emotion-voc/xml#big6 1640 Beck (Beck et al., 2014) proposed a Multi-task Gaussian-process based method for emotion classification. Xu (Xu et al., 2012) used a coarse to fine method to classify emotions in Chinese blog. Gao (Gao et al., 2013) proposed a joint model to cotrain a polarity classifier and an emotion classifier. Chang (Chang et al., 2015) used linguistic template to predict reader’s emotions. Das (Das and Bandyopadhyay, 2010) used an unsupervised method to extract emotion feelers from Bengali blog. There are other studies focused on joint learning with sentiment (Luo et al., 2015; Mohtarami et al., 2013), emotion in tweets or blog (Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014; Liu et al., 2013; Quan and Ren, 2009), and emotional lexicon construction (Yang et al., 2014; Staiano and Guerini, 2014; Mohammad and Turney, 2013). However, these related works all focused on analysis of emotion expressions rather than emotion causes.. Sophia M. Y. Lee first proposed a task on emotion cause extraction (Lee et al., 2010). They manually constructed a corpus from Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen and Lee (Chen et al., 2010) proposed a rule based method t"
D16-1170,D14-1123,0,0.0158183,"thod for emotion classification. Xu (Xu et al., 2012) used a coarse to fine method to classify emotions in Chinese blog. Gao (Gao et al., 2013) proposed a joint model to cotrain a polarity classifier and an emotion classifier. Chang (Chang et al., 2015) used linguistic template to predict reader’s emotions. Das (Das and Bandyopadhyay, 2010) used an unsupervised method to extract emotion feelers from Bengali blog. There are other studies focused on joint learning with sentiment (Luo et al., 2015; Mohtarami et al., 2013), emotion in tweets or blog (Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014; Liu et al., 2013; Quan and Ren, 2009), and emotional lexicon construction (Yang et al., 2014; Staiano and Guerini, 2014; Mohammad and Turney, 2013). However, these related works all focused on analysis of emotion expressions rather than emotion causes.. Sophia M. Y. Lee first proposed a task on emotion cause extraction (Lee et al., 2010). They manually constructed a corpus from Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen and Lee (Chen et al., 2010) proposed a rule based method to detect emotion causes. The basic idea is to make linguistic rules for cause extraction. S"
D16-1170,D14-1127,0,0.0127211,"aussian-process based method for emotion classification. Xu (Xu et al., 2012) used a coarse to fine method to classify emotions in Chinese blog. Gao (Gao et al., 2013) proposed a joint model to cotrain a polarity classifier and an emotion classifier. Chang (Chang et al., 2015) used linguistic template to predict reader’s emotions. Das (Das and Bandyopadhyay, 2010) used an unsupervised method to extract emotion feelers from Bengali blog. There are other studies focused on joint learning with sentiment (Luo et al., 2015; Mohtarami et al., 2013), emotion in tweets or blog (Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014; Liu et al., 2013; Quan and Ren, 2009), and emotional lexicon construction (Yang et al., 2014; Staiano and Guerini, 2014; Mohammad and Turney, 2013). However, these related works all focused on analysis of emotion expressions rather than emotion causes.. Sophia M. Y. Lee first proposed a task on emotion cause extraction (Lee et al., 2010). They manually constructed a corpus from Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen and Lee (Chen et al., 2010) proposed a rule based method to detect emotion causes. The basic idea is to make linguistic rules for ca"
D16-1170,D09-1150,0,0.0157741,"(Xu et al., 2012) used a coarse to fine method to classify emotions in Chinese blog. Gao (Gao et al., 2013) proposed a joint model to cotrain a polarity classifier and an emotion classifier. Chang (Chang et al., 2015) used linguistic template to predict reader’s emotions. Das (Das and Bandyopadhyay, 2010) used an unsupervised method to extract emotion feelers from Bengali blog. There are other studies focused on joint learning with sentiment (Luo et al., 2015; Mohtarami et al., 2013), emotion in tweets or blog (Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014; Liu et al., 2013; Quan and Ren, 2009), and emotional lexicon construction (Yang et al., 2014; Staiano and Guerini, 2014; Mohammad and Turney, 2013). However, these related works all focused on analysis of emotion expressions rather than emotion causes.. Sophia M. Y. Lee first proposed a task on emotion cause extraction (Lee et al., 2010). They manually constructed a corpus from Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen and Lee (Chen et al., 2010) proposed a rule based method to detect emotion causes. The basic idea is to make linguistic rules for cause extraction. Some studies (Gui et al., 2014; Li and X"
D16-1170,W11-1720,0,0.339925,"Missing"
D16-1170,D13-1144,0,0.0640194,"lem. If an ET is an emotion cause, the label should be positive. Otherwise, the label should be negative. A binary classifier should be used. 4.2 Emotion Cause Extraction After the construction of ETs, we obtain positive and negative ET samples. Due to small amount of training samples, it is necessary to capture all features in the ETs. We choose convolution kernel based SVMs because it can search all possible syntactic features under a tree structure. Convolution kernel function The convolution kernel, also known as the tree kernel (Collins and Duffy, 2002), is widely used in many NLP tasks (Srivastava et al., 2013; Moschitti, 2006). For any two inputs T1 and T2 based on a tree structure , the kernel is defined as: K(T1 , T2 ) = X X δ(n1 , n2 ). (1) n1 ∈T1 n2 ∈T2 Here, n1 and n2 are tree nodes. δ is a function defined recursively: 1.δ(n1 , n2 ) = 0 if the productions of n1 and n2 are different; 2.Else, δ(n1 , n2 ) = 1 if n1 and n2 are matching in pre-terminals; 3.Otherwise, δ(n1 , n2 ) = Y (1 + δ(c(n1 , i), c(n2 , i))). i Here, c(n, i) is the i-th node of n. However, the above tree kernel definition does not consider terminals, which means that the actual words in a sentence are ignored. As emotions cau"
D16-1170,P14-2070,0,0.0105484,"Missing"
D16-1170,P14-2069,0,0.0123206,"fy emotions in Chinese blog. Gao (Gao et al., 2013) proposed a joint model to cotrain a polarity classifier and an emotion classifier. Chang (Chang et al., 2015) used linguistic template to predict reader’s emotions. Das (Das and Bandyopadhyay, 2010) used an unsupervised method to extract emotion feelers from Bengali blog. There are other studies focused on joint learning with sentiment (Luo et al., 2015; Mohtarami et al., 2013), emotion in tweets or blog (Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014; Liu et al., 2013; Quan and Ren, 2009), and emotional lexicon construction (Yang et al., 2014; Staiano and Guerini, 2014; Mohammad and Turney, 2013). However, these related works all focused on analysis of emotion expressions rather than emotion causes.. Sophia M. Y. Lee first proposed a task on emotion cause extraction (Lee et al., 2010). They manually constructed a corpus from Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen and Lee (Chen et al., 2010) proposed a rule based method to detect emotion causes. The basic idea is to make linguistic rules for cause extraction. Some studies (Gui et al., 2014; Li and Xu, 2014; Gao et al., 2015) extended the rule based meth"
D17-1048,D14-1080,0,0.028703,"ness of cognition grounded data in building attention models. 2 Related works The basic task in sentiment analysis can be formulated as a classification problem. Class labels can either be binary (positive/negative) or polarity either as intensity by continuous values or as ratings in certain range such as 0 to 5 or 1 to 10, etc.. In recent years, deep learning based methods have greatly improved the performance of sentiment analysis. Commonly used models include Convolutional Neural Networks (Socher et al., 2011), Recursive Neural Network (Socher et al., 2013), and Recurrent Neural Networks (Irsoy and Cardie, 2014). RNN naturally benefits sentiment classification because of its ability to capture sequential information in text. However, standard RNN suffers from the gradient vanishing problem (Bengio et al., 1994) where gradients may grow or decay exponentially over long sequences. To address this problem, Long-Short Term Memory model (LSTM) is introduced by adding a gated mechanism to keep long term memory. Each LSTM layer is generally followed by mean pool463 Eye-Tracking Corpus), and Mishra et al. (Mishra et al., 2016b) are considered high-quality resources (Kennedy, 2003; Cop et al., 2016; Mishra et"
D17-1048,P14-2007,0,0.234147,"Missing"
D17-1048,N13-1088,0,0.22577,"Missing"
D17-1048,D16-1009,0,0.014538,"formance of sentiment analysis without the need for labor intensive feature engineering. 1 462 https://en.wikipedia.org/wiki/Eye-tracking Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 462–471 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ing and then feed into the next layer. Experiments in datasets which contain long documents and sentences demonstrate that the LSTM model outperforms the traditional RNN (Tang et al., 2015a,c). Not all words contribute equally to the semantics of a sentence (Hahn and Keller, 2016). Attention based neural networks are proposed to highlight their difference in contribution (Yang et al., 2016). In document level sentiment classification, both sentence level attention and document level attention are proposed. In the sentence level attention layer, an attention mechanism identifies words that are important. Those informative words are aggregated as attention weights to form sentence embedding representation. This method is generally called local context attention method. Similarly, some sentences can also be highlighted to indicate their importance in a document. Apart fro"
D17-1048,P16-2094,0,0.0714979,"Missing"
D17-1048,P11-1015,0,0.0615686,"re other aspects of attentions. Evaluations show the CBA based method outperforms the state-of-the-art local context based attention methods significantly. This brings insight to how cognition grounded data can be brought into NLP tasks. 1 Introduction Sentiment analysis is critical for many applications such as sentimental product recommendation (Dong et al., 2013), public opinion detection (Pang et al., 2008), and human-machine interaction (Clavel and Callejas, 2016), etc.Sentiment analysis has been well-explored (Pang et al., 2002; Vanzo et al., 2014; Tang et al., 2015a; Chen et al., 2016; Maas et al., 2011).Recently, deep learning based methods have further elevated the performance of sentiment analysis without the need for labor intensive feature engineering. 1 462 https://en.wikipedia.org/wiki/Eye-tracking Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 462–471 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ing and then feed into the next layer. Experiments in datasets which contain long documents and sentences demonstrate that the LSTM model outperforms the traditional RNN (Tang et al., 2015a,c). Not al"
D17-1048,D16-1171,0,0.438258,"reated equal. Some words are more important than others in conveying the message in a sentence. Similarly, some sentences are more important than others in a document. Although the overall reading time as a cognitive process may reflect the syntax and discourse complexity, reading time of individual words is also an indicator of their semantic importance in text (Roseman, 2001; Demberg and Keller, 2008). Previous attention models are built using information embedded in text including users, products and text in local context for sentiment classification (Tang et al., 2015b; Yang et al., 2016; Chen et al., 2016; Gui et al., 2016). However, attention models using local context based text through distributional similarity lack theoretical foundation to reflect the cognitive basis. But, the key in sentiment analysis lies in its cognitive basis. Thus, we envision that cognition grounded data obtained in text reading should be helpful in building an attention model. In this paper, we propose a novel cognition based attention(CBA) model for sentiment analysis learned from cognition grounded eye-tracking data. Eye-tracking is the process of measuring either the point of gaze or the motion of an eye relativ"
D17-1048,P14-5010,0,0.00351223,"Missing"
D17-1048,P13-2062,0,0.00655233,"iment annotation complexity based on eye-tracking data. Mishra (2014) presents a cognitive study of sentiment detection from the perspective of AI where readers are tested as sentiment readers. Mishra (Mishra et al., 2016b) recently proposes a model in sentiment analysis and sarcasm detection by using eye-tracking data as a feature in addition to text features using Naive-Bayes and SVM classifiers. In other NLP tasks, Joshi (2013) shows that Word-Sense-Disambiguation can make use of simultaneous eye-tracking. Eye-tracking data are also used to measure the difficulty in translation annotation (Mishra et al., 2013). Barrett (2016) finds that gaze patterns during reading are strongly influenced by the role a word plays in terms of syntax, semantic, and discourse. Among different available eye-tracking datasets, the Dundee corpus, GECO (the Ghent We first build a regression model to map syntax, and context features of a word to its reading time based on eye-tracking data. We then apply the model to sentiment analysis text to obtain the estimated reading time of words at the sentence level. The estimated reading time can then be used as the attention weights in its context to build the attention layer in a"
D17-1048,W14-2623,0,0.0545921,"Missing"
D17-1048,K16-1016,0,0.226636,"5). However, they are not suited for other genre of text as userproduct information are not generally available. Attention models can be built not only from local text or user/product information but also from cognitive grounded data, especially eye-tracking data (Rayner, 1998; Allopenna et al., 1998). Joshi (2014) proposes a novel metric called Sentiment Annotation Complexity for measuring sentiment annotation complexity based on eye-tracking data. Mishra (2014) presents a cognitive study of sentiment detection from the perspective of AI where readers are tested as sentiment readers. Mishra (Mishra et al., 2016b) recently proposes a model in sentiment analysis and sarcasm detection by using eye-tracking data as a feature in addition to text features using Naive-Bayes and SVM classifiers. In other NLP tasks, Joshi (2013) shows that Word-Sense-Disambiguation can make use of simultaneous eye-tracking. Eye-tracking data are also used to measure the difficulty in translation annotation (Mishra et al., 2013). Barrett (2016) finds that gaze patterns during reading are strongly influenced by the role a word plays in terms of syntax, semantic, and discourse. Among different available eye-tracking datasets, t"
D17-1048,P14-1146,0,0.0753487,"mplicated deep learning models suffer from serious over-fitting problem. And the result of Deep learning model with word embedding initialization partly supports the fact that the reading time are more depend on the micro level syntax and semantic feature for the word, such as number of letters in word and complexity score of the word instead of the deep level context features. 4.2 Type Num Bool Bool Bool Bool Bool Bool Num Num Num Num Num Num • AvgWordvec — A SVM classifier that takes the average of word embeddings in Word2Vec as document embedding. Here is a list of Group 2 methods: • SSWE (Tang et al., 2014) — A SVM classifier using sentiment specific word embedding. • RNTN+RNN (Socher et al., 2013) — A Recursive Neural Tensor Network(RNTN) to represent sentences and trained using RNN. • Paragraph vector (Le and Mikolov, 2014) — A SVM classifier using document embedding as features. Comparison of different sentiment classification methods • LSTM+LA (Chen et al., 2016) — State-ofthe-art LSTM using local context as attention mechanism in both sentence level and docu5 ment level. https : //en.wikipedia.org/wiki/Coef f iciento fd etermination Because the features used in our model are all text based,"
D17-1048,W02-1011,0,0.0193148,"in documents. Different attention mechanisms can also be incorporated to capture other aspects of attentions. Evaluations show the CBA based method outperforms the state-of-the-art local context based attention methods significantly. This brings insight to how cognition grounded data can be brought into NLP tasks. 1 Introduction Sentiment analysis is critical for many applications such as sentimental product recommendation (Dong et al., 2013), public opinion detection (Pang et al., 2008), and human-machine interaction (Clavel and Callejas, 2016), etc.Sentiment analysis has been well-explored (Pang et al., 2002; Vanzo et al., 2014; Tang et al., 2015a; Chen et al., 2016; Maas et al., 2011).Recently, deep learning based methods have further elevated the performance of sentiment analysis without the need for labor intensive feature engineering. 1 462 https://en.wikipedia.org/wiki/Eye-tracking Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 462–471 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ing and then feed into the next layer. Experiments in datasets which contain long documents and sentences demonstrate tha"
D17-1048,P10-1118,0,0.0148968,"ntains eye movement data from English and French newspapers (Kennedy, 2003). Measurements were taken while 10 participants read 20 newspaper articles. GECO is an English-Dutch bilingual corpus with eye-tracking data from 17 participants collected from reading the complete novel The Mysterious Affair at Styles. The corpus has 4,934 sentences, 774,015 tokens, and 9,876 words. The Mishra(Mishra et al., 2016a) dataset contains 994 text snippets with 383 positive and 611 negative examples from newspaper clippings, sampled from seven native speakers. To predict reading time using eye-tracking data, Tomanek et al. (2010) proposes a regression model using linguistic features related to syntax and semantics for calibration. Hahn (2016) proposes a novel approach to model both skipping and reading using unsupervised method which combines neural attention with auto-encoding trained on raw text using reinforcement learning. 3 Our proposed CBA model The basic idea of our method is to add a CBA model into a neural-network based LSTM sentiment classifier. Let D be a collection of documents. A document dk , dk ∈ D, has m number of sentences S1 , S2 , ...Sj , ..., Sm . A sentence Sj is formed by a sequence of words Sj ="
D17-1048,C14-1221,0,0.0609746,"Missing"
D17-1048,D14-1162,0,0.0969541,"Missing"
D17-1048,D16-1172,0,0.244785,"s the best performer. LSTM+LA (2016), which is the state-of-the-art method, uses local attention mechanism to improve performance significantly. Among our CBA based variations, using the GECO dataset gives the best result outperforming LSTM+LA in all three datasets. LSTM+CBAG has significant improvement over LSTM+LA with p values of p &lt; 0.016 on IMDB, p &lt; 0.0019 on Yelp 13, and p &lt; 0.00023 on Yelp 14. LSTM+CBAG has the best result compared to the other two variations because GECO has larger participant size. Its text genre is also closer to the review datasets for sentiment analysis. • CLSTM (Xu et al., 2016) — Cached LSTM to capture the overall semantic information in long text. The two variations include regular CLSTM and bi-directional B-CLSTM. • LSTM+UPA (Chen et al., 2016) — Stateof-the-art LSTM including LA as well as user/product as attention mechanism at both sentence level and document level. Our proposed CBA model has several variations as explained below. • LSTM+CBA — The LSTM classifier using only CBA model at sentence level and document level. Based on the three eye-tracking datasets(GECO, DUNDEE and Mishra’s) for reading time prediction, we label the same model by different training"
D17-1048,N16-1174,0,0.380427,"not all words are created equal. Some words are more important than others in conveying the message in a sentence. Similarly, some sentences are more important than others in a document. Although the overall reading time as a cognitive process may reflect the syntax and discourse complexity, reading time of individual words is also an indicator of their semantic importance in text (Roseman, 2001; Demberg and Keller, 2008). Previous attention models are built using information embedded in text including users, products and text in local context for sentiment classification (Tang et al., 2015b; Yang et al., 2016; Chen et al., 2016; Gui et al., 2016). However, attention models using local context based text through distributional similarity lack theoretical foundation to reflect the cognitive basis. But, the key in sentiment analysis lies in its cognitive basis. Thus, we envision that cognition grounded data obtained in text reading should be helpful in building an attention model. In this paper, we propose a novel cognition based attention(CBA) model for sentiment analysis learned from cognition grounded eye-tracking data. Eye-tracking is the process of measuring either the point of gaze or the motio"
D17-1048,D11-1014,0,0.106007,"ur method outperforms other state-of-the-art methods significantly. This work validates the effectiveness of cognition grounded data in building attention models. 2 Related works The basic task in sentiment analysis can be formulated as a classification problem. Class labels can either be binary (positive/negative) or polarity either as intensity by continuous values or as ratings in certain range such as 0 to 5 or 1 to 10, etc.. In recent years, deep learning based methods have greatly improved the performance of sentiment analysis. Commonly used models include Convolutional Neural Networks (Socher et al., 2011), Recursive Neural Network (Socher et al., 2013), and Recurrent Neural Networks (Irsoy and Cardie, 2014). RNN naturally benefits sentiment classification because of its ability to capture sequential information in text. However, standard RNN suffers from the gradient vanishing problem (Bengio et al., 1994) where gradients may grow or decay exponentially over long sequences. To address this problem, Long-Short Term Memory model (LSTM) is introduced by adding a gated mechanism to keep long term memory. Each LSTM layer is generally followed by mean pool463 Eye-Tracking Corpus), and Mishra et al."
D17-1048,D13-1170,0,0.0222159,"hods significantly. This work validates the effectiveness of cognition grounded data in building attention models. 2 Related works The basic task in sentiment analysis can be formulated as a classification problem. Class labels can either be binary (positive/negative) or polarity either as intensity by continuous values or as ratings in certain range such as 0 to 5 or 1 to 10, etc.. In recent years, deep learning based methods have greatly improved the performance of sentiment analysis. Commonly used models include Convolutional Neural Networks (Socher et al., 2011), Recursive Neural Network (Socher et al., 2013), and Recurrent Neural Networks (Irsoy and Cardie, 2014). RNN naturally benefits sentiment classification because of its ability to capture sequential information in text. However, standard RNN suffers from the gradient vanishing problem (Bengio et al., 1994) where gradients may grow or decay exponentially over long sequences. To address this problem, Long-Short Term Memory model (LSTM) is introduced by adding a gated mechanism to keep long term memory. Each LSTM layer is generally followed by mean pool463 Eye-Tracking Corpus), and Mishra et al. (Mishra et al., 2016b) are considered high-quali"
D17-1048,D15-1167,0,0.465263,"nt analysis because not all words are created equal. Some words are more important than others in conveying the message in a sentence. Similarly, some sentences are more important than others in a document. Although the overall reading time as a cognitive process may reflect the syntax and discourse complexity, reading time of individual words is also an indicator of their semantic importance in text (Roseman, 2001; Demberg and Keller, 2008). Previous attention models are built using information embedded in text including users, products and text in local context for sentiment classification (Tang et al., 2015b; Yang et al., 2016; Chen et al., 2016; Gui et al., 2016). However, attention models using local context based text through distributional similarity lack theoretical foundation to reflect the cognitive basis. But, the key in sentiment analysis lies in its cognitive basis. Thus, we envision that cognition grounded data obtained in text reading should be helpful in building an attention model. In this paper, we propose a novel cognition based attention(CBA) model for sentiment analysis learned from cognition grounded eye-tracking data. Eye-tracking is the process of measuring either the point"
D17-1048,P15-1098,0,0.122497,"nt analysis because not all words are created equal. Some words are more important than others in conveying the message in a sentence. Similarly, some sentences are more important than others in a document. Although the overall reading time as a cognitive process may reflect the syntax and discourse complexity, reading time of individual words is also an indicator of their semantic importance in text (Roseman, 2001; Demberg and Keller, 2008). Previous attention models are built using information embedded in text including users, products and text in local context for sentiment classification (Tang et al., 2015b; Yang et al., 2016; Chen et al., 2016; Gui et al., 2016). However, attention models using local context based text through distributional similarity lack theoretical foundation to reflect the cognitive basis. But, the key in sentiment analysis lies in its cognitive basis. Thus, we envision that cognition grounded data obtained in text reading should be helpful in building an attention model. In this paper, we propose a novel cognition based attention(CBA) model for sentiment analysis learned from cognition grounded eye-tracking data. Eye-tracking is the process of measuring either the point"
D17-1167,D14-1190,0,0.0625937,"Missing"
D17-1167,P15-2127,0,0.0423877,"the taxonomy of emotions. Researchers have proposed a list of primary emotions (Plutchik, 1980; Ekman, 1984; Turner, 2000). In this study, we Existing work in emotion analysis mostly focuses on emotion classification (Li et al., 2013; Zhou et al., 2016) and emotion information extraction (Balahur et al., 2013). Xu et al. (2012) used a coarse to fine method to classify emotions in Chinese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rat"
D17-1167,C10-1021,0,0.529812,"joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al., 2014; Li and Xu, 2014; Gao et al., 2015) extended the rule based method to informal text in Weibo text (Chinese tweets). Other than rule based methods, Russo et al. (2011) proposed a crowdsourcing method to construct a common-sense knowledge base which is related to emotion causes. But it is challenging to extend the common-sense knowledge base automatically. Ghazi et al. (2015) used Conditional Random Fields (CRFs) to extract emotion causes. However, it requires emotion"
D17-1167,Y10-1071,0,0.18817,"ure. 1 Introduction With the rapid growth of social network platforms, more and more people tend to share their experiences and emotions online. Emotion analysis of online text becomes a new challenge in Natural Language Processing (NLP). In recent years, studies in emotion analysis largely focus on emotion classification including detection of writers’ emotions (Gao et al., 2013) as well as readers’ emotions (Chang et al., 2015). There are also some information extraction tasks defined in emotion analysis (Chen et al., 2016; Balahur et al., 2011), such as extracting the feeler of an emotion (Das and Bandyopadhyay, 2010). These methods † Corresponding Author: xuruifeng@hit.edu.cn assume that emotion expressions are already observed. Sometimes, however, we care more about the stimuli, or the cause of an emotion. For instance, Samsung wants to know why people love or hate Note 7 rather than the distribution of different emotions. Ex.1 我的手机昨天丢了，我现在很难过。 Ex.1 Because I lost my phone yesterday, I feel sad now. In an example shown above, “sad” is an emotion word, and the cause of “sad” is “I lost my phone”. The emotion cause extraction task aims to identify the reason behind an emotion expression. It is a more diffi"
D17-1167,P13-1095,0,0.0353223,"classify emotions in Chinese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al., 2014; Li and Xu, 2014;"
D17-1167,D14-1181,0,0.00503362,"t by an attention mechanism. Based on the learned attention result, the network maps the text into a low dimensional vector space. This vector is then used to generate an answer. Existing memory network based approaches to QA use weighted sum of attentions to jointly consider short text segments stored in memory. However, they do not explicitly model sequential information in the context. In this paper, we propose a new deep memory network architecture to model the context of each word simultaneously by multiple memory slots which capture sequential information using convolutional operations (Kim, 2014), and achieves the state-of-the-art performance compared to existing methods which use manual rules, common sense knowledge bases or other machine learning models. The rest of the paper is organized as follows. Section 2 gives a review of related works on emotion analysis. Section 3 presents our proposed deep memory network based model for emotion cause extraction. Section 4 discusses evaluation results. Finally, Section 5 concludes the work and outlines the future directions. 2 Related Work Identifying emotion categories in text is one of the key tasks in NLP (Liu, 2015). Going one step furth"
D17-1167,W10-0206,0,0.743034,"s to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al., 2014; Li and Xu, 2014; Gao et al., 2015) extended the rule based method to informal text in Weibo text (Chinese tweets). Other than rule based methods, Russo et al. (2011) proposed a crowdsourcing method to construct a common-sense knowledge base which is related to emotion causes. But it is challe"
D17-1167,P13-2091,0,0.0388326,"to fine method to classify emotions in Chinese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al.,"
D17-1167,D15-1297,0,0.0226715,"16) and emotion information extraction (Balahur et al., 2013). Xu et al. (2012) used a coarse to fine method to classify emotions in Chinese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method"
D17-1167,D14-1123,0,0.0222424,"(2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al., 2014; Li and Xu, 2014; Gao et al., 2015) extended the rule based"
D17-1167,D14-1127,0,0.0455892,"inese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al., 2014; Li and Xu, 2014; Gao et al., 2015) exten"
D17-1167,D09-1150,0,0.0406229,"2012) used a coarse to fine method to classify emotions in Chinese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some st"
D17-1167,W11-1720,0,0.272973,"Missing"
D17-1167,P14-2070,0,0.0182461,"al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al., 2014; Li and Xu, 2014; Gao et al., 2015) extended the rule based method to informal text in Weibo text (Chinese tweets). Other than rule based methods, Russo et al. (2011)"
D17-1167,D16-1021,0,0.0120791,"use extraction requires an understanding of a given piece of text in order to correctly identify the relation between the description of an event which causes an emotion and the expression of that emotion, it can essentially be considered as a QA task. In our work, we choose the memory network, which is designed to model the relation between a story and a query for QA systems (Weston et al., 2014; Sukhbaatar et al., 2015). Apart from its application in QA, memory network has also achieved great successes in other NLP tasks, such as machine translation (Luong et al., 2015), sentiment analysis (Tang et al., 2016) or summarization (M. Rush et al., 2015). To the best of our knowledge, this is the first work which uses memory network for emotion cause extraction. 3 Our Approach In this section, we will first define our task. Then, a brief introduction of memory network will be given, including its basic learning structure of memory network and deep architecture. Last, our modified deep memory network for emotion cause extraction will be presented. 3.1 Task Definition The formal definition of emotion cause extraction is given in (Gui et al., 2016). In this task, a given document, which is a passage about"
D17-1167,D15-1166,0,0.00787256,"in their model learning. Since emotion cause extraction requires an understanding of a given piece of text in order to correctly identify the relation between the description of an event which causes an emotion and the expression of that emotion, it can essentially be considered as a QA task. In our work, we choose the memory network, which is designed to model the relation between a story and a query for QA systems (Weston et al., 2014; Sukhbaatar et al., 2015). Apart from its application in QA, memory network has also achieved great successes in other NLP tasks, such as machine translation (Luong et al., 2015), sentiment analysis (Tang et al., 2016) or summarization (M. Rush et al., 2015). To the best of our knowledge, this is the first work which uses memory network for emotion cause extraction. 3 Our Approach In this section, we will first define our task. Then, a brief introduction of memory network will be given, including its basic learning structure of memory network and deep architecture. Last, our modified deep memory network for emotion cause extraction will be presented. 3.1 Task Definition The formal definition of emotion cause extraction is given in (Gui et al., 2016). In this task, a g"
D17-1167,D15-1044,0,0.0332273,"of a given piece of text in order to correctly identify the relation between the description of an event which causes an emotion and the expression of that emotion, it can essentially be considered as a QA task. In our work, we choose the memory network, which is designed to model the relation between a story and a query for QA systems (Weston et al., 2014; Sukhbaatar et al., 2015). Apart from its application in QA, memory network has also achieved great successes in other NLP tasks, such as machine translation (Luong et al., 2015), sentiment analysis (Tang et al., 2016) or summarization (M. Rush et al., 2015). To the best of our knowledge, this is the first work which uses memory network for emotion cause extraction. 3 Our Approach In this section, we will first define our task. Then, a brief introduction of memory network will be given, including its basic learning structure of memory network and deep architecture. Last, our modified deep memory network for emotion cause extraction will be presented. 3.1 Task Definition The formal definition of emotion cause extraction is given in (Gui et al., 2016). In this task, a given document, which is a passage about an emotion event, contains an emotion wo"
D17-1167,P13-1097,0,0.0280841,"formation extraction (Balahur et al., 2013). Xu et al. (2012) used a coarse to fine method to classify emotions in Chinese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes"
D17-1167,P14-2069,0,0.014941,"classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al., 2014; Li and Xu, 2014; Gao et al., 2015) extended the rule based method to informal text in Weibo text (Chinese tweets). Other than rule based m"
D17-1167,D16-1061,0,0.0378978,"categories in text is one of the key tasks in NLP (Liu, 2015). Going one step further, emotion cause extraction can reveal important information about what causes a certain emotion and why there is an emotion change. In this section, we introduce related work on emotion analysis including emotion cause extraction. In emotion analysis, we first need to determine the taxonomy of emotions. Researchers have proposed a list of primary emotions (Plutchik, 1980; Ekman, 1984; Turner, 2000). In this study, we Existing work in emotion analysis mostly focuses on emotion classification (Li et al., 2013; Zhou et al., 2016) and emotion information extraction (Balahur et al., 2013). Xu et al. (2012) used a coarse to fine method to classify emotions in Chinese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo"
D19-5541,S18-1037,0,0.11352,"Missing"
D19-5541,S17-2126,0,0.272521,"sly, every domain has its own characteristics which deserve special attention. A typical Table 1: Example of an real-world irregular expression preprocessed by methods at different levels. ‘##’ is a sign of word pieces, and ‘&lt;&gt;’ is a special mark produced by a Twitter-specific preprocessor. these domain-specific expressions are strong indicators for affective analysis in Twitter, and these characteristics are worthy of special consideration. Simply neglecting them would lose a lot of useful information. Such information can be formulated as domain knowledge by using Twitter preprocessor like (Baziotis et al., 2017). After Twitterspecific preprocessing, these expressions are annotated automatically and we can find informa316 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 316–321 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics training predicts the masked and unseen words within the remaining context. However, by corrupting inputs with masks, BERT neglects dependency between masked positions. XLNet (Yang et al., 2019) proposes to maximizes the likelihood over all permutations of the factorization order of conditional probability"
D19-5541,S17-2094,0,0.0131948,"ctional structure to learn context from both directions. As a consequence of its bidirectionality, BERT is not trained by predicting words in sequence either from left-to-right or from right-toleft. After masking a part of words in a sentence, 317 3.2 2012) look for tweets with a target set of hashtags such as #happy and #sad to collect an emotionlinked training data. Due to the abundance of these naturally labeled training data, deep neural networks has proven its dominance in recent competitions by means of the framework of transfer learning (Severyn and Moschitti, 2015; Deriu et al., 2016; Cliche, 2017). They pre-train models on naturally labeled data to get a better starting point and fine-tune their models on the target task. 3 After Twitter-specific annotation using a preprocessing tool, some input words are annotated and stand out conspicuously. In this step, we identify informative token patterns for emotion classification. A simple convolution network is used to examine tokens within a fixed-length window to detect token patterns. The network structure is a 1D convolution layer followed by temporal max-pooling, similar to that of (Kim, 2014). But we only use a token window of size 3 to"
D19-5541,S16-1173,0,0.0436158,"Missing"
D19-5541,S18-1039,0,0.0367274,"Missing"
D19-5541,N18-1202,0,0.0388904,"ce of research study on affective analysis of people towards a topic. N-grams and negative indicators are widely used in affective analysis of Twitter (Mohammad et al., 2013; Miura et al., 2014). Affect-based lexicons are also included to provide general sentiment or emotion information (Hagen et al., 2015). (Go et al.) use :) and :( emoticons as natural labels and collect a pseudo-labeled training data to increase their ngram classifier. Similarly, Wang et al (Wang et al., In contrast to n-gram LMs and early neural models for learning word embeddings, recent LMs have deeper structures. ELMo (Peters et al., 2018) use a stack of bi-directional LSTM to encode word context either from left-to-right or from right-toleft. BERT (Devlin et al., 2019) has a bidirectional structure to learn context from both directions. As a consequence of its bidirectionality, BERT is not trained by predicting words in sequence either from left-to-right or from right-toleft. After masking a part of words in a sentence, 317 3.2 2012) look for tweets with a target set of hashtags such as #happy and #sad to collect an emotionlinked training data. Due to the abundance of these naturally labeled training data, deep neural networks"
D19-5541,S15-2097,0,0.0535852,"Missing"
D19-5541,S15-2079,0,0.0467707,"Missing"
D19-5541,N19-1419,0,0.0199945,"ve’, ‘you’, ‘lots’, ‘&lt;/hashtag&gt;’, ‘love’, ‘&lt;hashtag&gt;’ Introduction Deep language models (LM) have been very successful in recent years. In pre-training, a deep LM learns to predict unseen words in the context at hand in an unsupervised way, which enables the LM to make use of very large amount of unlabeled data. By using deep structures and large amount of training data, these deep LMs can learn useful linguistic knowledge common to many natural language processing tasks. For example, BERT (Devlin et al., 2019) has the ability to encode grammatical knowledge in context in its representations (Hewitt and Manning, 2019). Deep LMs provide general knowledge of text to benefit downstream tasks. To be adaptive to a target domain, they do need to be fine-tuned by data of the target domain. Obviously, every domain has its own characteristics which deserve special attention. A typical Table 1: Example of an real-world irregular expression preprocessed by methods at different levels. ‘##’ is a sign of word pieces, and ‘&lt;&gt;’ is a special mark produced by a Twitter-specific preprocessor. these domain-specific expressions are strong indicators for affective analysis in Twitter, and these characteristics are worthy of sp"
D19-5541,D14-1181,0,0.00274652,"and Moschitti, 2015; Deriu et al., 2016; Cliche, 2017). They pre-train models on naturally labeled data to get a better starting point and fine-tune their models on the target task. 3 After Twitter-specific annotation using a preprocessing tool, some input words are annotated and stand out conspicuously. In this step, we identify informative token patterns for emotion classification. A simple convolution network is used to examine tokens within a fixed-length window to detect token patterns. The network structure is a 1D convolution layer followed by temporal max-pooling, similar to that of (Kim, 2014). But we only use a token window of size 3 to simply observe trigrams. The three-token range should cover most of potential token patterns for our work. Given a convolution kernel, it serves as a detector to check whether a particular token pattern appears in a sentence measured by a matching score si according to the following formula 1, Methodology The basic idea of our work is to use a Twitterspecific preprocessor to decode Twitter-related expressions. A token pattern detector is then trained to identify affect-bearing token patterns. Finally, a two-step training process is introduced to in"
D19-5541,2021.ccl-1.108,0,0.0891839,"Missing"
D19-5541,S18-1043,0,0.0947942,"Missing"
D19-5541,S14-2111,0,0.0236424,"overall representation of this sentence for sentencelevel tasks such as entailment or sentiment analysis. Related Work Related works include both deep LMs especially BERT, a representative deep learning based LM and works on Twitter classification. 2.2 2.1 Deep Language Models Twitter Affective Analysis As a platform to express everyday thoughts, Twitter has huge amount of affect-related text. Thus Twitter is a good source of research study on affective analysis of people towards a topic. N-grams and negative indicators are widely used in affective analysis of Twitter (Mohammad et al., 2013; Miura et al., 2014). Affect-based lexicons are also included to provide general sentiment or emotion information (Hagen et al., 2015). (Go et al.) use :) and :( emoticons as natural labels and collect a pseudo-labeled training data to increase their ngram classifier. Similarly, Wang et al (Wang et al., In contrast to n-gram LMs and early neural models for learning word embeddings, recent LMs have deeper structures. ELMo (Peters et al., 2018) use a stack of bi-directional LSTM to encode word context either from left-to-right or from right-toleft. BERT (Devlin et al., 2019) has a bidirectional structure to learn c"
D19-5541,S13-2053,0,0.0328771,"esponding output as the overall representation of this sentence for sentencelevel tasks such as entailment or sentiment analysis. Related Work Related works include both deep LMs especially BERT, a representative deep learning based LM and works on Twitter classification. 2.2 2.1 Deep Language Models Twitter Affective Analysis As a platform to express everyday thoughts, Twitter has huge amount of affect-related text. Thus Twitter is a good source of research study on affective analysis of people towards a topic. N-grams and negative indicators are widely used in affective analysis of Twitter (Mohammad et al., 2013; Miura et al., 2014). Affect-based lexicons are also included to provide general sentiment or emotion information (Hagen et al., 2015). (Go et al.) use :) and :( emoticons as natural labels and collect a pseudo-labeled training data to increase their ngram classifier. Similarly, Wang et al (Wang et al., In contrast to n-gram LMs and early neural models for learning word embeddings, recent LMs have deeper structures. ELMo (Peters et al., 2018) use a stack of bi-directional LSTM to encode word context either from left-to-right or from right-toleft. BERT (Devlin et al., 2019) has a bidirectional"
D19-5541,S18-1001,0,0.100006,"Missing"
E14-4008,J10-4006,1,0.410132,"c Models (DSMs) have gained much attention in computational linguistics as unsupervised methods to build lexical semantic representations from corpus-derived co-occurrences encoded as distributional vectors (Sahlgren, 2006; Turney and Pantel, 2010). DSMs rely on the Distributional Hypothesis (Harris, 1954) and model lexical semantic similarity as a function of distributional similarity, which is most commonly measured with the vector cosine (Turney and Pantel, 2010). DSMs have achieved impressive results in tasks such as synonym detection, semantic categorization, etc. (Padó and Lapata, 2007; Baroni and Lenci, 2010). 38 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 38–42, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics In this paper, we introduce SLQS, a new entropy-based distributional measure that aims to identify hypernyms by providing a distributional characterization of their semantic generality. We assess it with two tasks: (i.) the identification of the broader term in hyponym-hypernym pairs (directionality task); (ii.) the discrimination of hypernymy among other semantic relations (detectio"
E14-4008,J07-2002,0,0.0337174,"Distributional Semantic Models (DSMs) have gained much attention in computational linguistics as unsupervised methods to build lexical semantic representations from corpus-derived co-occurrences encoded as distributional vectors (Sahlgren, 2006; Turney and Pantel, 2010). DSMs rely on the Distributional Hypothesis (Harris, 1954) and model lexical semantic similarity as a function of distributional similarity, which is most commonly measured with the vector cosine (Turney and Pantel, 2010). DSMs have achieved impressive results in tasks such as synonym detection, semantic categorization, etc. (Padó and Lapata, 2007; Baroni and Lenci, 2010). 38 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 38–42, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics In this paper, we introduce SLQS, a new entropy-based distributional measure that aims to identify hypernyms by providing a distributional characterization of their semantic generality. We assess it with two tasks: (i.) the identification of the broader term in hyponym-hypernym pairs (directionality task); (ii.) the discrimination of hypernymy among other sem"
E14-4008,W09-0215,0,0.851838,"Missing"
E14-4008,P05-1014,0,0.914436,"Missing"
E14-4008,W03-1011,0,0.848579,"we propose SLQS, which measures the semantic generality of a word by the entropy of its statistically most prominent contexts. Related work The problem of identifying asymmetric relations like hypernymy has so far been addressed in distributional semantics only in a limited way (Kotlerman et al., 2010) or treated through semisupervised approaches, such as pattern-based approaches (Hearst, 1992). The few works that have attempted a completely unsupervised approach to the identification of hypernymy in corpora have mostly relied on some versions of the Distributional Inclusion Hypothesis (DIH; Weeds and Weir, 2003; Weeds et al., 2004), according to which the contexts of a narrow term are also shared by the broad term. One of the first proposed measures formalizing the DIH is WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), which quantifies the weights of the features f of a narrow term u that are included into the set of features of a broad term v: , = SLQS: A new entropy-based measure For every term wi we identify the N most associated contexts c (where N is a parameter empirically set to 50)1. The association strength has been calculated with Local Mutual Information (LMI; Evert, 2005). For each"
E14-4008,C04-1146,0,0.70813,"h measures the semantic generality of a word by the entropy of its statistically most prominent contexts. Related work The problem of identifying asymmetric relations like hypernymy has so far been addressed in distributional semantics only in a limited way (Kotlerman et al., 2010) or treated through semisupervised approaches, such as pattern-based approaches (Hearst, 1992). The few works that have attempted a completely unsupervised approach to the identification of hypernymy in corpora have mostly relied on some versions of the Distributional Inclusion Hypothesis (DIH; Weeds and Weir, 2003; Weeds et al., 2004), according to which the contexts of a narrow term are also shared by the broad term. One of the first proposed measures formalizing the DIH is WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), which quantifies the weights of the features f of a narrow term u that are included into the set of features of a broad term v: , = SLQS: A new entropy-based measure For every term wi we identify the N most associated contexts c (where N is a parameter empirically set to 50)1. The association strength has been calculated with Local Mutual Information (LMI; Evert, 2005). For each selected context c,"
E14-4008,S12-1012,1,0.739596,"Missing"
E14-4008,C92-2082,0,\N,Missing
E14-4008,W11-2501,1,\N,Missing
E14-4008,W09-0200,0,\N,Missing
I05-1037,C02-1150,0,0.0766252,"Missing"
I05-1037,A00-1023,1,\N,Missing
I05-1037,W01-1203,0,\N,Missing
I05-1037,N01-1005,0,\N,Missing
I05-1037,W03-1208,0,\N,Missing
I05-1037,P04-1072,0,\N,Missing
I05-1037,H01-1069,0,\N,Missing
I05-1037,A00-1041,0,\N,Missing
I05-1061,W01-1313,0,0.0207662,"ed by the potential applications, temporal information processing has absorbed more attention recently than ever, such as ACL 2001 workshop on temporal and spatial information processing, LREC 2002 and TERN 2004 [14]. Mani [10] gives a good review about the recent trend. Research works in this area can be classified into four types: designing annotation scheme for temporal information representation [4, 6, 12]; developing temporal ontology which covers temporal objects and their relationships between each other [2, 7]; Identifying time-stamps of events or temporal relationships between events [5, 9]; Identifying and normalizing temporal expressions from different languages [1, 3, 8, 11, 13, 15]. Temporal annotation, temporal ontology and temporal reasoning are not the focuses in this paper. Among the research works on temporal expression extraction and normalization, most of them are based on hand-written rules or machine-learnt rules. Mani and Wilson [11] resolve temporal expressions by hand-crafted and machinelearnt rules. Their focus is resolving temporal expressions, especially indexical expressions, which designate times that are dependent on the speaker and some reference time. We"
I05-1061,W01-1305,1,0.823148,"ed by the potential applications, temporal information processing has absorbed more attention recently than ever, such as ACL 2001 workshop on temporal and spatial information processing, LREC 2002 and TERN 2004 [14]. Mani [10] gives a good review about the recent trend. Research works in this area can be classified into four types: designing annotation scheme for temporal information representation [4, 6, 12]; developing temporal ontology which covers temporal objects and their relationships between each other [2, 7]; Identifying time-stamps of events or temporal relationships between events [5, 9]; Identifying and normalizing temporal expressions from different languages [1, 3, 8, 11, 13, 15]. Temporal annotation, temporal ontology and temporal reasoning are not the focuses in this paper. Among the research works on temporal expression extraction and normalization, most of them are based on hand-written rules or machine-learnt rules. Mani and Wilson [11] resolve temporal expressions by hand-crafted and machinelearnt rules. Their focus is resolving temporal expressions, especially indexical expressions, which designate times that are dependent on the speaker and some reference time. We"
I05-1061,P00-1010,0,0.0532695,"e attention recently than ever, such as ACL 2001 workshop on temporal and spatial information processing, LREC 2002 and TERN 2004 [14]. Mani [10] gives a good review about the recent trend. Research works in this area can be classified into four types: designing annotation scheme for temporal information representation [4, 6, 12]; developing temporal ontology which covers temporal objects and their relationships between each other [2, 7]; Identifying time-stamps of events or temporal relationships between events [5, 9]; Identifying and normalizing temporal expressions from different languages [1, 3, 8, 11, 13, 15]. Temporal annotation, temporal ontology and temporal reasoning are not the focuses in this paper. Among the research works on temporal expression extraction and normalization, most of them are based on hand-written rules or machine-learnt rules. Mani and Wilson [11] resolve temporal expressions by hand-crafted and machinelearnt rules. Their focus is resolving temporal expressions, especially indexical expressions, which designate times that are dependent on the speaker and some reference time. We concentrate on the procedure of extraction and normalization, and try to cover more temporal expr"
I05-1061,W01-1309,0,0.0850976,"e attention recently than ever, such as ACL 2001 workshop on temporal and spatial information processing, LREC 2002 and TERN 2004 [14]. Mani [10] gives a good review about the recent trend. Research works in this area can be classified into four types: designing annotation scheme for temporal information representation [4, 6, 12]; developing temporal ontology which covers temporal objects and their relationships between each other [2, 7]; Identifying time-stamps of events or temporal relationships between events [5, 9]; Identifying and normalizing temporal expressions from different languages [1, 3, 8, 11, 13, 15]. Temporal annotation, temporal ontology and temporal reasoning are not the focuses in this paper. Among the research works on temporal expression extraction and normalization, most of them are based on hand-written rules or machine-learnt rules. Mani and Wilson [11] resolve temporal expressions by hand-crafted and machinelearnt rules. Their focus is resolving temporal expressions, especially indexical expressions, which designate times that are dependent on the speaker and some reference time. We concentrate on the procedure of extraction and normalization, and try to cover more temporal expr"
I05-1061,W01-1314,0,0.162426,"e attention recently than ever, such as ACL 2001 workshop on temporal and spatial information processing, LREC 2002 and TERN 2004 [14]. Mani [10] gives a good review about the recent trend. Research works in this area can be classified into four types: designing annotation scheme for temporal information representation [4, 6, 12]; developing temporal ontology which covers temporal objects and their relationships between each other [2, 7]; Identifying time-stamps of events or temporal relationships between events [5, 9]; Identifying and normalizing temporal expressions from different languages [1, 3, 8, 11, 13, 15]. Temporal annotation, temporal ontology and temporal reasoning are not the focuses in this paper. Among the research works on temporal expression extraction and normalization, most of them are based on hand-written rules or machine-learnt rules. Mani and Wilson [11] resolve temporal expressions by hand-crafted and machinelearnt rules. Their focus is resolving temporal expressions, especially indexical expressions, which designate times that are dependent on the speaker and some reference time. We concentrate on the procedure of extraction and normalization, and try to cover more temporal expr"
I05-3012,C02-1097,0,0.0266704,"-based approaches, and hybrid approaches. Among these approaches, the supervised corpus-based approach had been applied and discussed by many researches ([2, 3, 4, 5, 6, 7, 8]). According to [1], the corpusbased supervised machine learning methods are the most successful approaches to WSD where contextual features have been used mainly to distinguish ambiguous words in these methods. However, word occurrences in the context are too diverse to capture the right pattern, which means that the dimension of contextual words will be very large when all words in the training samples are used for WSD [14]. Certain uninformative features will weaken the discriminative power of a classifier resulting in a lower precision rate. To narrow down the context, we propose to use collocations as contextual information as defined in Section 3.1.2. It is generally understood that the sense of an ambiguous word is unique in a given collocation [19]. For example, “ࣙ㺅” means “burden” but not “baggage” when it appears in the collocation “ᗱᛇࣙ㺅” (“ burden of thought”). In this paper, we apply a classifier to combine the local features of collocations which contain the target word with other contextual features"
I05-3012,C96-1005,0,0.139751,"Missing"
I05-3012,P91-1019,0,0.115845,"Missing"
I05-3012,W03-1302,0,0.0457119,"to distinguish ambiguous words in these methods. However, word occurrences in the context are too diverse to capture the right pattern, which means that the dimension of contextual words will be very large when all words in the training samples are used for WSD [14]. Certain uninformative features will weaken the discriminative power of a classifier resulting in a lower precision rate. To narrow down the context, we propose to use collocations as contextual information as defined in Section 3.1.2. It is generally understood that the sense of an ambiguous word is unique in a given collocation [19]. For example, “ࣙ㺅” means “burden” but not “baggage” when it appears in the collocation “ᗱᛇࣙ㺅” (“ burden of thought”). In this paper, we apply a classifier to combine the local features of collocations which contain the target word with other contextual features to discriminate the ambiguous words. The intuition is that when the target context captures a collocation, the influence of other dimensions of Abstract The selection of features is critical in providing discriminative information for classifiers in Word Sense Disambiguation (WSD). Uninformative features will degrade the performance of"
I05-3012,W97-0201,0,0.116005,"Missing"
I05-3012,P03-1058,0,0.0738081,"lyu.ed u.hk du.hk du.hk Through Time”, or the Chinese word “ഄᮍ” in “ഄᮍᬓᑰ”(“local government”) and “Ҫгᇍ ⱘഄᮍ”(“He is also partly right”). WSD tries to automatically assign an appropriate sense to an occurrence of a word in a given context. Various approaches have been proposed to deal with the word sense disambiguation problem including rule-based approaches, knowledge or dictionary based approaches, corpus-based approaches, and hybrid approaches. Among these approaches, the supervised corpus-based approach had been applied and discussed by many researches ([2, 3, 4, 5, 6, 7, 8]). According to [1], the corpusbased supervised machine learning methods are the most successful approaches to WSD where contextual features have been used mainly to distinguish ambiguous words in these methods. However, word occurrences in the context are too diverse to capture the right pattern, which means that the dimension of contextual words will be very large when all words in the training samples are used for WSD [14]. Certain uninformative features will weaken the discriminative power of a classifier resulting in a lower precision rate. To narrow down the context, we propose to use collocations as conte"
I05-3012,H93-1051,0,0.117697,"Missing"
I05-3012,J98-1006,0,0.0972472,"SENSEVAL-3 Chinese training data. Even though in both approaches, statistically significant bi-gram co-occurrence information is used, they are not necessarily true collocations. For example, in the express “ᥠᦵⲥ㾚ᴀᎲ ഄᮄ㒇㊍ߚᄤⱘ⌏ᚙ”މ, the bi-grams in their system are (ᥠᦵ , ⲥ㾚 , ⲥ㾚ᴀ Related Work Automating word sense disambiguation tasks based on annotated corpora have been proposed. Examples of supervised learning methods for WSD appear in [2, 3, 4], [7, 8]. The learning algorithms applied including: decision tree, decisionlist [15], neural networks [7], naïve Bayesian learning ([5],[11]) and maximum entropy [10]. Among these leaning methods, the most important issue is what features will be used to construct the classifier. It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ([6], [16, 17, 18]). It is generally true that when Ꮂ , ᴀᎲഄ , ഄᮄ㒇㊍ , ᮄ㒇㊍ⱘ  ⱘ⌏ , ⌏ᚙ މSome bi-grams such as ⌏   ᚙ   މmay have higher frequency but may introduce noise when considering it as features in disambiguating the sense “human|Ҏ” and “symbol|ヺো” like in the example case of “∈ߚᄤ⌏ᚙ”މ. In our system,"
I05-3012,J98-1004,0,0.170264,"ᥠᦵ , ⲥ㾚 , ⲥ㾚ᴀ Related Work Automating word sense disambiguation tasks based on annotated corpora have been proposed. Examples of supervised learning methods for WSD appear in [2, 3, 4], [7, 8]. The learning algorithms applied including: decision tree, decisionlist [15], neural networks [7], naïve Bayesian learning ([5],[11]) and maximum entropy [10]. Among these leaning methods, the most important issue is what features will be used to construct the classifier. It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ([6], [16, 17, 18]). It is generally true that when Ꮂ , ᴀᎲഄ , ഄᮄ㒇㊍ , ᮄ㒇㊍ⱘ  ⱘ⌏ , ⌏ᚙ މSome bi-grams such as ⌏   ᚙ   މmay have higher frequency but may introduce noise when considering it as features in disambiguating the sense “human|Ҏ” and “symbol|ヺো” like in the example case of “∈ߚᄤ⌏ᚙ”މ. In our system, we do not rely on co-occurrence information. Instead, we utilize true collocation information (ᮄ㒇㊍, ߚᄤ) which fall in the window size of (-5, +5) as fea88 the contextual window size as 10 in our system. Each of the Chinese words except the stop words inside the window range wi"
I05-3012,J98-1005,0,0.0690024,"n 5 is the conclusion. 2 words are used in the same sense, they have similar context and co-occurrence information [13]. It is also generally true that the nearby context words of an ambiguous word give more effective patterns and features values than those far from it [12]. The existing methods consider features selection for context representation including both local and topic features where local features refer to the information pertained only to the given context and topical features are statistically obtained from a training corpus. Most of the recent works for English corpus including [7] and [8], which combine both local and topical information in order to improve their performance. An interesting study on feature selection for Chinese [10] has considered topical features as well as local collocational, syntactic, and semantic features using the maximum entropy model. In Dang’s [10] work, collocational features refer to the local PoS information and bi-gram co-occurrences of words within 2 positions of the ambiguous word. A useful result from this work based on (about one million words) the tagged People’s Daily News shows that adding more features from richer levels of lingu"
I05-3012,P94-1013,0,0.167045,"Missing"
I05-3012,P95-1026,0,0.359684,"Missing"
I05-3012,C02-1143,0,0.240681,"ata. Even though in both approaches, statistically significant bi-gram co-occurrence information is used, they are not necessarily true collocations. For example, in the express “ᥠᦵⲥ㾚ᴀᎲ ഄᮄ㒇㊍ߚᄤⱘ⌏ᚙ”މ, the bi-grams in their system are (ᥠᦵ , ⲥ㾚 , ⲥ㾚ᴀ Related Work Automating word sense disambiguation tasks based on annotated corpora have been proposed. Examples of supervised learning methods for WSD appear in [2, 3, 4], [7, 8]. The learning algorithms applied including: decision tree, decisionlist [15], neural networks [7], naïve Bayesian learning ([5],[11]) and maximum entropy [10]. Among these leaning methods, the most important issue is what features will be used to construct the classifier. It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ([6], [16, 17, 18]). It is generally true that when Ꮂ , ᴀᎲഄ , ഄᮄ㒇㊍ , ᮄ㒇㊍ⱘ  ⱘ⌏ , ⌏ᚙ މSome bi-grams such as ⌏   ᚙ   މmay have higher frequency but may introduce noise when considering it as features in disambiguating the sense “human|Ҏ” and “symbol|ヺো” like in the example case of “∈ߚᄤ⌏ᚙ”މ. In our system, we do not rely on co-occurrence"
I05-3012,W04-0847,0,0.276723,"as fully fixed collocations, fixed collocations, strong collocations and loose collocations. Fixed collocations means the appearance of one word implies the co-occurrence of another one such as “ग़ࣙ㺅” (“burden of history”), while strong collocations allows very limited substitution of the components, for example, “ഄᮍ䰶” (“local college”), or ” ഄᮍ ᄺ” (“local university”). The sense of ambiguous words can be uniquely determined in these two types of collocations, therefore are the collocations applied in our system. The sources of the collocations will be explained in Section 4.1. In both Niu [11] and Dang’s [10] work, topical features as well as the so called collocational features were used. However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features. However, bi-gram co-occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations. Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD. The local features in our system"
I05-3012,P97-1007,0,\N,Missing
I08-7003,N03-1033,0,0.0108382,"eriments show that this method is quite effective in giving good precision and minimal computing time. The remaining of this paper is organized as follows. Section 2 reviews the related work. Section 3 gives the observations to the task and corresponding corpus, then presents our method for TPOS tagging. Section 4 gives the evaluation details and discussions on the proposed method and reference methods. Section 5 concludes this paper. 2 Some existing methods are based on the analysis of word morphology. They exploited more features besides morphology or took morphology as supplementary means (Toutanova et al., 2003; Huihsin Tseng et al., 2005; Samuelsson, Christer, 1993). Toutanova et al. demonstrated the use of both preceding and following tag contexts via a dependency network representation and made use of some additional features such as lexical features including jointly conditioning on multiple consecutive words and other fine-grained modeling of word features (Toutanova et al., 2003). Huihisin et al. proposed a variety of morphological word features, such as the tag sequence features from both left and right side of the current word for POS tagging and implemented them in a Maximum Entropy Markov"
I08-7003,I05-3005,0,0.0135145,"s quite effective in giving good precision and minimal computing time. The remaining of this paper is organized as follows. Section 2 reviews the related work. Section 3 gives the observations to the task and corresponding corpus, then presents our method for TPOS tagging. Section 4 gives the evaluation details and discussions on the proposed method and reference methods. Section 5 concludes this paper. 2 Some existing methods are based on the analysis of word morphology. They exploited more features besides morphology or took morphology as supplementary means (Toutanova et al., 2003; Huihsin Tseng et al., 2005; Samuelsson, Christer, 1993). Toutanova et al. demonstrated the use of both preceding and following tag contexts via a dependency network representation and made use of some additional features such as lexical features including jointly conditioning on multiple consecutive words and other fine-grained modeling of word features (Toutanova et al., 2003). Huihisin et al. proposed a variety of morphological word features, such as the tag sequence features from both left and right side of the current word for POS tagging and implemented them in a Maximum Entropy Markov model (Huihsin Tseng et al.,"
I08-7003,O05-4006,1,0.881949,"Missing"
I08-7003,A00-1031,0,0.0173915,"Missing"
I08-7003,E99-1018,0,0.090738,"Missing"
I08-7003,A97-1018,0,0.0617779,"Missing"
I08-7003,W03-1730,0,0.0254164,"Missing"
I08-7003,J97-3003,0,\N,Missing
I08-7003,P96-1043,0,\N,Missing
I17-2025,D14-1162,0,0.0874904,"extended ANEW (Warriner et al., 2013), and CVAW (Yu et al., 2016). Although multi-dimensional affective lexicons are theoretically sound, there are mainly two issues. The first one is how to obtain good coverage for affective lexicons. The second one is how to infer the representation of larger text units using word information in the affective lexicons. A previous work uses the average value of the component words as the final representation of larger texts (Yu et al., 2016). Word embedding has recently been used to represent word semantics, such as word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014). Word embedding represents a word as a dense vector, which can be used to measure semantic similarity of words more accurately. To infer the representation of larger text units based on word embedding, different composition models are proposed, such as weighted addition and multiplication (Mitchell and Lapata, 2008), tensor product (Zhao et al., 2015), recursive neural network (Socher et al., 2013), recurrent neural network (Irsoy and Cardie, 2014), and convelutional neural network (Kim, 2014). Attempts have also been made to infer the affective labels In this paper, we investigate the effect"
I17-2025,S15-2078,0,0.0202601,"es clearly show that word embedding using unsupervised distributional method outperforms manually prepared lexicons no matter what affective models are used in the lexicons. Our conclusion is that although different affective lexicons are cognitively backed by theories, they do not show any advantage over the automatically obtained word embedding. 1 Introduction Sentiment analysis aims to infer the polarity expressed in a text, which has important applications for data analysis, such as product review (Pang et al., 2008), stock market performance (Nguyen and Shirai, 2015), and crowd opinions (Rosenthal et al., 2015). Sentiment lexicons play a critical role in sentiment analysis (Hutto and Gilbert, 2014). A sentiment lexicon contains a list of words with sentiment polarity (positive or negative) or polarity intensity, such as the NRC Hashtag Lexicon (Mohammad et al., 2013) and VADER sentiment lexicon (Hutto and Gilbert, 2014). However, sentiment lexicons may fail for compositional methods to obtain sentiment of larger text units, such as phrases and sentences. For example, the phrase avoid imprisonment expresses positive sentiment. However, when we use sentiment lexicon, it is hard to classify this phrase"
I17-2025,D13-1170,0,0.0344334,"ponent words as the final representation of larger texts (Yu et al., 2016). Word embedding has recently been used to represent word semantics, such as word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014). Word embedding represents a word as a dense vector, which can be used to measure semantic similarity of words more accurately. To infer the representation of larger text units based on word embedding, different composition models are proposed, such as weighted addition and multiplication (Mitchell and Lapata, 2008), tensor product (Zhao et al., 2015), recursive neural network (Socher et al., 2013), recurrent neural network (Irsoy and Cardie, 2014), and convelutional neural network (Kim, 2014). Attempts have also been made to infer the affective labels In this paper, we investigate the effectiveness of different affective lexicons through sentiment analysis of phrases. We examine how phrases can be represented through manually prepared lexicons, extended lexicons using computational methods, or word embedding. Comparative studies clearly show that word embedding using unsupervised distributional method outperforms manually prepared lexicons no matter what affective models are used in th"
I17-2025,D14-1181,0,0.0107118,"Missing"
I17-2025,C14-1018,0,0.023173,") of size 54,129, constructed automatically based on hashtags (Mohammad et al., 2013). Related Work To apply a sentiment lexicon in sentiment analysis, the simpliest way is to take word present in a lexicon as a simple feature (Pang et al., 2008). For intensity-based sentiment lexicons, the sentiment value can be aggregated by addition of every sentiment linked word in a sentence (Hutto and Gilbert, 2014; Vo and Zhang, 2016). Another method is to use sentiment related features, such as total count of sentiment tokens, total sentiment score, maximal sentiment score, etc.(Mohammad et al., 2013; Tang et al., 2014). Many efforts have been made to construct multi-dimensional affective lexicons, such as ANEW for English (Bradley and Lang, 1999; Warriner et al., 2013), CVAW for Chinese (Yu et al., 2016), and other languages (Montefinese et al., 2014; Imbir, 2015). However, only few works use multi-dimensional affective lexicons for affective analysis. The work by Yu et al. (2016) uses the average VAD values of individual words as the VAD value of a sentence. In (Palogiannidi et al., 2016), affective representation of phrases is obtained through matrix-vector multiplication, where modifier words are represe"
I17-2025,P16-2036,0,0.0205651,"y. 1. The VADER sentiment lexicon of size 7,502, annotated through crowdsourcing (Hutto and Gilbert, 2014). Its value range is [-4, 4]. 2. The NRC Hasntag sentiment lexicon (denoted as HSenti) of size 54,129, constructed automatically based on hashtags (Mohammad et al., 2013). Related Work To apply a sentiment lexicon in sentiment analysis, the simpliest way is to take word present in a lexicon as a simple feature (Pang et al., 2008). For intensity-based sentiment lexicons, the sentiment value can be aggregated by addition of every sentiment linked word in a sentence (Hutto and Gilbert, 2014; Vo and Zhang, 2016). Another method is to use sentiment related features, such as total count of sentiment tokens, total sentiment score, maximal sentiment score, etc.(Mohammad et al., 2013; Tang et al., 2014). Many efforts have been made to construct multi-dimensional affective lexicons, such as ANEW for English (Bradley and Lang, 1999; Warriner et al., 2013), CVAW for Chinese (Yu et al., 2016), and other languages (Montefinese et al., 2014; Imbir, 2015). However, only few works use multi-dimensional affective lexicons for affective analysis. The work by Yu et al. (2016) uses the average VAD values of individua"
I17-2025,P08-1028,0,0.716367,"d information in the affective lexicons. A previous work uses the average value of the component words as the final representation of larger texts (Yu et al., 2016). Word embedding has recently been used to represent word semantics, such as word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014). Word embedding represents a word as a dense vector, which can be used to measure semantic similarity of words more accurately. To infer the representation of larger text units based on word embedding, different composition models are proposed, such as weighted addition and multiplication (Mitchell and Lapata, 2008), tensor product (Zhao et al., 2015), recursive neural network (Socher et al., 2013), recurrent neural network (Irsoy and Cardie, 2014), and convelutional neural network (Kim, 2014). Attempts have also been made to infer the affective labels In this paper, we investigate the effectiveness of different affective lexicons through sentiment analysis of phrases. We examine how phrases can be represented through manually prepared lexicons, extended lexicons using computational methods, or word embedding. Comparative studies clearly show that word embedding using unsupervised distributional method o"
I17-2025,N16-1066,0,0.163421,"ch can be considered as one-dimensional affective lexicons, different multi-dimensional affect models are also proposed to represent affective information of words, such as the evaluationpotency-activity (EPA) model (Osgood, 1952) and the valence-arousal-dominance (VAD) model (Ressel, 1980). Sentiment can be seen as one of the dimensions under these affective models, such as the evaluation dimension of EPA, and the valence dimension of VAD. Aside from the EPA based lexicon (Heise, 2010), VAD based lexicons include ANEW (Bradley and Lang, 1999), extended ANEW (Warriner et al., 2013), and CVAW (Yu et al., 2016). Although multi-dimensional affective lexicons are theoretically sound, there are mainly two issues. The first one is how to obtain good coverage for affective lexicons. The second one is how to infer the representation of larger text units using word information in the affective lexicons. A previous work uses the average value of the component words as the final representation of larger texts (Yu et al., 2016). Word embedding has recently been used to represent word semantics, such as word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014). Word embedding represents a word as a d"
I17-2025,S13-2053,0,0.205428,"heories, they do not show any advantage over the automatically obtained word embedding. 1 Introduction Sentiment analysis aims to infer the polarity expressed in a text, which has important applications for data analysis, such as product review (Pang et al., 2008), stock market performance (Nguyen and Shirai, 2015), and crowd opinions (Rosenthal et al., 2015). Sentiment lexicons play a critical role in sentiment analysis (Hutto and Gilbert, 2014). A sentiment lexicon contains a list of words with sentiment polarity (positive or negative) or polarity intensity, such as the NRC Hashtag Lexicon (Mohammad et al., 2013) and VADER sentiment lexicon (Hutto and Gilbert, 2014). However, sentiment lexicons may fail for compositional methods to obtain sentiment of larger text units, such as phrases and sentences. For example, the phrase avoid imprisonment expresses positive sentiment. However, when we use sentiment lexicon, it is hard to classify this phrase because both avoid and imprisonment are nega146 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 146–150, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP to infer the representation of a sentence, such"
I17-2025,P15-1131,0,\N,Missing
I17-2025,W16-0424,0,\N,Missing
I17-2043,D15-1167,0,0.0596751,"Missing"
I17-2043,C08-1006,0,0.0298992,"Missing"
I17-2043,P17-2067,0,0.208105,"ltime/2011/03/17/fearingradiation-chinese-rush-to-buy-table-salt/ 2 252 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 252–256, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP dicate the credibility of a piece of news. For example, a conservative might neglect the impact of climate change, while a progressive might exaggerate inequality. On some occasions, it is hard to make fake claims like congressional inquiry. But in other cases, the speaker tend to exaggerate facts like the campaign rally. For the study use profile information, Wang (2017) proposes a hybrid CNN model to detect fake news, which uses speaker profiles as a part of the input data. Wang (2017) also made the first large scale fake news detection benchmark dataset with speaker information such as party affiliation, location of speech, job title, credit history as well as topic. The Long-Short memory network (LSTM), as a neural network model, is proven to work better for long sentences (Tang et al., 2015). Attention models are also proposed to weigh the importance of different words in their context. Current attention models are either based on local semantic attention"
I17-2043,N16-1174,0,0.0150442,"oft-max weight for each label. where W 3 254 http://www.politifact.com/truth-o-meter/ The performance of four baseline models are shown in Table 2 including a simple majority model, the LSTM model without using profile information, the hybrid CNN model proposed by Wang (2017) without profile information(CNNWang), and the hybrid CNN model by Wang with profile information(CNN-WangP). Note that firstly, LSTM without profile does not perform better than CNN-Wang. However, other studies show that when attention model is incorporated, LSTM generally outperforms that of CNN model (Chen et al., 2016; Yang et al., 2016) which will be shown later. Secondly, CNNWangP, which uses speaker profiles has the best performance. For word representation, we train the skip-gram word embedding (Mikolov et al., 2013) on each dataset separately to initialize the word vectors. All embedding sizes on the model are set to N = 200, a commonly used size. In speaker profiles, there are four basic attributes: party affiliation(Pa), location of speech(La), job title of speaker(Ti), and credit history(Ch) which counts the inaccurate statements for speaker in past speeches. Note that credit history is not a commonly available data."
I17-2043,D16-1171,0,0.0136344,"models ~ l is the soft-max weight for each label. where W 3 254 http://www.politifact.com/truth-o-meter/ The performance of four baseline models are shown in Table 2 including a simple majority model, the LSTM model without using profile information, the hybrid CNN model proposed by Wang (2017) without profile information(CNNWang), and the hybrid CNN model by Wang with profile information(CNN-WangP). Note that firstly, LSTM without profile does not perform better than CNN-Wang. However, other studies show that when attention model is incorporated, LSTM generally outperforms that of CNN model (Chen et al., 2016; Yang et al., 2016) which will be shown later. Secondly, CNNWangP, which uses speaker profiles has the best performance. For word representation, we train the skip-gram word embedding (Mikolov et al., 2013) on each dataset separately to initialize the word vectors. All embedding sizes on the model are set to N = 200, a commonly used size. In speaker profiles, there are four basic attributes: party affiliation(Pa), location of speech(La), job title of speaker(Ti), and credit history(Ch) which counts the inaccurate statements for speaker in past speeches. Note that credit history is not a commo"
I17-2043,I13-1039,0,0.182189,"Missing"
J14-3004,P07-1056,0,0.0481432,"Missing"
J14-3004,W02-1001,0,0.609391,"g rate or so-called decaying rate, and Lstoch (zzi , w t ) is the stochastic loss function based on a training sample z i . (More details of SGD are described in Bottou [1998], Tsuruoka, Tsujii, and Ananiadou [2009], and Sun et al. [2013].) Following the most recent work of SGD, the exponential decaying rate works the best for natural language processing tasks, and it is adopted in our implementation of the SGD (Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al. 2013). Other well-known on-line training methods include perceptron training (Freund and Schapire 1999), averaged perceptron training (Collins 2002), more recent development/extensions of stochastic gradient descent (e.g., the second-order stochastic gradient descent training methods like stochastic meta descent) (Vishwanathan et al. 2006; Hsu et al. 2009), and so on. However, the second-order stochastic gradient descent method requires the computation or approximation of the inverse of the Hessian matrix of the objective function, which is typically slow, especially for heavily structured classification models. Usually the convergence speed based on number of training iterations is moderately faster, but the time cost per iteration is sl"
J14-3004,W04-1217,0,0.0112629,"tion (Bio-NER) task is from the BIONLP-2004 shared task. The task is to recognize five kinds of biomedical named entities, including DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential labeling task with the BIO encoding. This data set consists of 20,546 training samples (from 2,000 MEDLINE article abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data are summarized in Table 1. State-of-the-art systems for this task include Settles (2004), Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al. (2009), and Tsuruoka, Tsujii, and Ananiadou (2009). Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki, et al. 2009), we use word token–based features, part-of-speech (POS) based features, and orthography pattern–based features (prefix, uppercase/lowercase, etc.), as listed in Table 2. With the traditional implementation of CRF systems (e.g., the HCRF package), the edges features usually contain only the information of yi−1 and yi , and ignore the 572 Sun et al. Feature-Frequency–Adaptive On-line"
J14-3004,P07-1104,0,0.0145497,"e word segmentation aims to automatically segment character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at positions i − 2, i − 1, i and i + 1. Whether xj and xj+1 are identical, for"
J14-3004,W04-1213,0,0.0135585,"also perform experiments on a nonstructured binary classification task: sentiment-based text classification. For the nonstructured classification task, the ADF training is based on the maximum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996). 4.1 Biomedical Named Entity Recognition (Structured Classification) The biomedical named entity recognition (Bio-NER) task is from the BIONLP-2004 shared task. The task is to recognize five kinds of biomedical named entities, including DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential labeling task with the BIO encoding. This data set consists of 20,546 training samples (from 2,000 MEDLINE article abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data are summarized in Table 1. State-of-the-art systems for this task include Settles (2004), Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al. (2009), and Tsuruoka, Tsujii, and Ananiadou (2009). Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki, et al. 2009), we u"
J14-3004,N01-1025,0,0.136481,"ssification) In the phrase chunking task, the non-recursive cores of noun phrases, called base NPs, are identified. The phrase chunking data is extracted from the data of the CoNLL-2000 shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. We use the feature templates based on word n-grams and part-of-speech n-grams, and feature templates are shown in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8 × 105 features in total. State-of-the-art systems for this task include Kudo and Matsumoto (2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al. (2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior studies, the evaluation metric for this task is the balanced F-score. 4.4 Sentiment Classification (Non-Structured Classification) To demonstrate that the proposed method is not limited to structured classification, we select a well-known sentiment classification task for evaluating the proposed method on non-structured classification. Table 3 Feature templates used for the phrase chunking task. wi , ti , and yi are defined as befor"
J14-3004,H05-1124,0,0.245783,"Missing"
J14-3004,P06-1059,0,0.0295941,"from the BIONLP-2004 shared task. The task is to recognize five kinds of biomedical named entities, including DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential labeling task with the BIO encoding. This data set consists of 20,546 training samples (from 2,000 MEDLINE article abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data are summarized in Table 1. State-of-the-art systems for this task include Settles (2004), Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al. (2009), and Tsuruoka, Tsujii, and Ananiadou (2009). Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki, et al. 2009), we use word token–based features, part-of-speech (POS) based features, and orthography pattern–based features (prefix, uppercase/lowercase, etc.), as listed in Table 2. With the traditional implementation of CRF systems (e.g., the HCRF package), the edges features usually contain only the information of yi−1 and yi , and ignore the 572 Sun et al. Feature-Frequency–Adaptive On-line Training for Natural Lang"
J14-3004,W96-0213,0,0.366727,"xisting gold-standard systems, which are complicated and use extra resources. 2. Related Work Our main focus is on structured classification models with high dimensional features. For structured classification, the conditional random fields model is widely used. To illustrate that the proposed method is a general-purpose training method not limited to a specific classification task or model, we also evaluate the proposal for non-structured classification tasks like binary classification. For non-structured classification, the maximum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996) is widely used. Here, we review the conditional random fields model and the related work of on-line training methods. 2.1 Conditional Random Fields The conditional random field (CRF) model is a representative structured classification model and it is well known for its high accuracy in real-world applications. The CRF model is proposed for structured classification by solving “the label bias problem” (Lafferty, McCallum, and Pereira 2001). Assuming a feature function that maps a pair of observation sequence x and label sequence y to a global feature vector f, the probability of a label sequen"
J14-3004,W00-0726,0,0.0354294,"nstraints on j and k. All feature templates are instantiated with values that occurred in training samples. The extracted feature set is large, and there are 2.4 × 107 features in total. Our evaluation is based on a closed test, and we do not use extra resources. Following prior studies, the evaluation metric for this task is the balanced F-score. 4.3 Phrase Chunking (Structured Classification) In the phrase chunking task, the non-recursive cores of noun phrases, called base NPs, are identified. The phrase chunking data is extracted from the data of the CoNLL-2000 shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. We use the feature templates based on word n-grams and part-of-speech n-grams, and feature templates are shown in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8 × 105 features in total. State-of-the-art systems for this task include Kudo and Matsumoto (2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al. (2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior studies, the evaluation metric for this task is t"
J14-3004,W04-1221,0,0.0458102,"Missing"
J14-3004,C10-2139,0,0.0102007,"ent character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at positions i − 2, i − 1, i and i + 1. Whether xj and xj+1 are identical, for j = i − 2, . . . , i + 1. Whether xj an"
J14-3004,C08-1106,1,0.654936,"fied. The phrase chunking data is extracted from the data of the CoNLL-2000 shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. We use the feature templates based on word n-grams and part-of-speech n-grams, and feature templates are shown in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8 × 105 features in total. State-of-the-art systems for this task include Kudo and Matsumoto (2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al. (2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior studies, the evaluation metric for this task is the balanced F-score. 4.4 Sentiment Classification (Non-Structured Classification) To demonstrate that the proposed method is not limited to structured classification, we select a well-known sentiment classification task for evaluating the proposed method on non-structured classification. Table 3 Feature templates used for the phrase chunking task. wi , ti , and yi are defined as before. Word-Token–based Features: {wi−2 , wi−1 , wi , wi+1 , wi+2 , wi−1 wi , wi wi+1 } ×{yi , yi−1 yi } P"
J14-3004,P12-1027,1,0.678739,"Missing"
J14-3004,N09-1007,1,0.809977,"Missing"
J14-3004,I05-3027,0,0.0133462,"R is recall. 4.2 Chinese Word Segmentation (Structured Classification) Chinese word segmentation aims to automatically segment character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at"
J14-3004,P09-1054,0,0.0266947,"Missing"
J14-3004,N06-2049,0,0.0453143,"Missing"
J14-3004,P07-1106,0,0.0122276,"d Classification) Chinese word segmentation aims to automatically segment character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at positions i − 2, i − 1, i and i + 1. Whether xj and xj+1"
J14-3004,J96-1002,0,\N,Missing
K17-1006,2014.lilt-9.5,0,0.0215713,"nize metaphoric phrases. Zhou et al. (2011) use the Maximum Entrophy model to detect the metaphoric reading of verb phrases based on collocation with noun phrases, and point out that there is no mature syntactic and semantic tool for metaphor analysis in Chinese. Our study will close the gap by building a model of metaphor detection based on syntactic conditions. Regarding metaphor detection, most papers emphasize on distinguishing metaphoric senses from literal senses in a polysemy network. Disambiguation of senses has been handled by DSMs based on the availability of contextual information (Baroni et al., 2014; Boleda et al., 2012; Erk and Padó, 2010; Kartsaklis and Sadrzadeh 2013). When more contextual information is incorporated, disambiguation would be more successful. It should be noted that the senses of one form have different degrees of transparency to be traced in semantics. The senses of a form which can be chained together via overlapping semantics, as in the case of polysemy (cut a new window in the wall vs. the ball broke a window), are more likely to be traced. On the contrary, when the senses of a linguistic form are discrete as in the case of homonymy (e.g. piano keys vs. key point),"
K17-1006,W06-3506,0,0.0492086,"d Li, 2009), topic modeling (Li et al., 2010; Heintz et al., 2013), and compositional distributional semantic models (CDSMs) (Gutiérrez et al. 2016). Feature-based classification, in particular, attracts most attention since a wide array of contextual information is included (Sporleder and Li, 2009; Dunn., 2013; Hovy et al., 2011; Mohler et al., 2013; Neuman et al., 2013; Tsvetkov et al., 2013; Tsvetkov et al., 2014). Since the studies regarding metaphor identification have primarily focused on English, there are more available datasets in English in both manually-tagged linguistic resources (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Broadwell et al., 2013) and corpus-based approach (Birke and Sarker, 2007; Shutova et al., 2013; Neuman et al., 2013; Hovy et al., 2013). Metaphor detection in Chinese is at the incipient stage. Fu et al., (2016) uses hierarchical clustering https://en.wikipedia.org/wiki/Baidu_Baike 37 for Chinese noun phrases according to their contextual information to recognize metaphoric phrases. Zhou et al. (2011) use the Maximum Entrophy model to detect the metaphoric reading of verb phrases based on collocation with noun phrases, and point out that there is no mature synt"
K17-1006,P14-1023,0,0.00966636,"nize metaphoric phrases. Zhou et al. (2011) use the Maximum Entrophy model to detect the metaphoric reading of verb phrases based on collocation with noun phrases, and point out that there is no mature syntactic and semantic tool for metaphor analysis in Chinese. Our study will close the gap by building a model of metaphor detection based on syntactic conditions. Regarding metaphor detection, most papers emphasize on distinguishing metaphoric senses from literal senses in a polysemy network. Disambiguation of senses has been handled by DSMs based on the availability of contextual information (Baroni et al., 2014; Boleda et al., 2012; Erk and Padó, 2010; Kartsaklis and Sadrzadeh 2013). When more contextual information is incorporated, disambiguation would be more successful. It should be noted that the senses of one form have different degrees of transparency to be traced in semantics. The senses of a form which can be chained together via overlapping semantics, as in the case of polysemy (cut a new window in the wall vs. the ball broke a window), are more likely to be traced. On the contrary, when the senses of a linguistic form are discrete as in the case of homonymy (e.g. piano keys vs. key point),"
K17-1006,P16-1018,0,0.061043,"Missing"
K17-1006,W13-0908,0,0.0157053,"ormation by radicals increases both the precision and the recall in metaphor detection. Although this approach is especially effective for Chinese because of the information embedded in radicals, broader implications include the possibility of leveraging eventive information from different sources in other languages. 2 Related Work The task of metaphor detection has been handled in a wide variety of approaches including clustering models (Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010), semantic similarity graphs (Sporleder and Li, 2009), topic modeling (Li et al., 2010; Heintz et al., 2013), and compositional distributional semantic models (CDSMs) (Gutiérrez et al. 2016). Feature-based classification, in particular, attracts most attention since a wide array of contextual information is included (Sporleder and Li, 2009; Dunn., 2013; Hovy et al., 2011; Mohler et al., 2013; Neuman et al., 2013; Tsvetkov et al., 2013; Tsvetkov et al., 2014). Since the studies regarding metaphor identification have primarily focused on English, there are more available datasets in English in both manually-tagged linguistic resources (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Broadwell et"
K17-1006,W13-0907,0,0.0665575,"rimarily rely on contextual information. This study provides a novel approach to detect and classify metaphors by analyzing eventive information. Concepts can be classified into a wide array of event types according to ontology, the organization of knowledge (Huang et al., 2007). Eventive information thus can be applied to the classification of metaphors, which concern mappings of conceptual structures from a source domain to a target domain. The classification of metaphoric and literal senses has been approached by different methods such as vector-space models with distributional statistics (Hovy et al., 2013; Tsvetkov et al., 2014) and compositional distributional semantic models (CDSMs) (Kartsaklis and Sadrzadeh, 2013a). Most of the studies regarding metaphoric detection have been done in English, while the task in Chinese is at the incipient stage. The relevant studies such as clustering models and similarity computation in context (Fu et al., 2016; Wang, 2010) mainly focus on the metaphoric sense of each individual noun or adjectival phrase because the analyses are highly dependent on contextual information. However, metaphoric senses of verbs are less touched because it is difficult to define"
K17-1006,D12-1112,0,0.0149991,"es. Zhou et al. (2011) use the Maximum Entrophy model to detect the metaphoric reading of verb phrases based on collocation with noun phrases, and point out that there is no mature syntactic and semantic tool for metaphor analysis in Chinese. Our study will close the gap by building a model of metaphor detection based on syntactic conditions. Regarding metaphor detection, most papers emphasize on distinguishing metaphoric senses from literal senses in a polysemy network. Disambiguation of senses has been handled by DSMs based on the availability of contextual information (Baroni et al., 2014; Boleda et al., 2012; Erk and Padó, 2010; Kartsaklis and Sadrzadeh 2013). When more contextual information is incorporated, disambiguation would be more successful. It should be noted that the senses of one form have different degrees of transparency to be traced in semantics. The senses of a form which can be chained together via overlapping semantics, as in the case of polysemy (cut a new window in the wall vs. the ball broke a window), are more likely to be traced. On the contrary, when the senses of a linguistic form are discrete as in the case of homonymy (e.g. piano keys vs. key point), they may be problema"
K17-1006,chou-huang-2006-hantology,1,0.791552,"Missing"
K17-1006,P10-2017,0,0.0116245,") use the Maximum Entrophy model to detect the metaphoric reading of verb phrases based on collocation with noun phrases, and point out that there is no mature syntactic and semantic tool for metaphor analysis in Chinese. Our study will close the gap by building a model of metaphor detection based on syntactic conditions. Regarding metaphor detection, most papers emphasize on distinguishing metaphoric senses from literal senses in a polysemy network. Disambiguation of senses has been handled by DSMs based on the availability of contextual information (Baroni et al., 2014; Boleda et al., 2012; Erk and Padó, 2010; Kartsaklis and Sadrzadeh 2013). When more contextual information is incorporated, disambiguation would be more successful. It should be noted that the senses of one form have different degrees of transparency to be traced in semantics. The senses of a form which can be chained together via overlapping semantics, as in the case of polysemy (cut a new window in the wall vs. the ball broke a window), are more likely to be traced. On the contrary, when the senses of a linguistic form are discrete as in the case of homonymy (e.g. piano keys vs. key point), they may be problematic to DSM (Baroni e"
K17-1006,D13-1166,0,0.0368647,"Missing"
K17-1006,W13-3513,0,0.0224189,"Missing"
K17-1006,P14-1024,0,0.19485,"ntextual information. This study provides a novel approach to detect and classify metaphors by analyzing eventive information. Concepts can be classified into a wide array of event types according to ontology, the organization of knowledge (Huang et al., 2007). Eventive information thus can be applied to the classification of metaphors, which concern mappings of conceptual structures from a source domain to a target domain. The classification of metaphoric and literal senses has been approached by different methods such as vector-space models with distributional statistics (Hovy et al., 2013; Tsvetkov et al., 2014) and compositional distributional semantic models (CDSMs) (Kartsaklis and Sadrzadeh, 2013a). Most of the studies regarding metaphoric detection have been done in English, while the task in Chinese is at the incipient stage. The relevant studies such as clustering models and similarity computation in context (Fu et al., 2016; Wang, 2010) mainly focus on the metaphoric sense of each individual noun or adjectival phrase because the analyses are highly dependent on contextual information. However, metaphoric senses of verbs are less touched because it is difficult to define regularities of their c"
K17-1006,P10-1071,0,0.057361,"Missing"
K17-1006,C10-1113,0,0.0622662,"Missing"
K17-1006,E09-1086,0,0.024469,"cting metaphors. The approach of leveraging event type information by radicals increases both the precision and the recall in metaphor detection. Although this approach is especially effective for Chinese because of the information embedded in radicals, broader implications include the possibility of leveraging eventive information from different sources in other languages. 2 Related Work The task of metaphor detection has been handled in a wide variety of approaches including clustering models (Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010), semantic similarity graphs (Sporleder and Li, 2009), topic modeling (Li et al., 2010; Heintz et al., 2013), and compositional distributional semantic models (CDSMs) (Gutiérrez et al. 2016). Feature-based classification, in particular, attracts most attention since a wide array of contextual information is included (Sporleder and Li, 2009; Dunn., 2013; Hovy et al., 2011; Mohler et al., 2013; Neuman et al., 2013; Tsvetkov et al., 2013; Tsvetkov et al., 2014). Since the studies regarding metaphor identification have primarily focused on English, there are more available datasets in English in both manually-tagged linguistic resources (Gedigian et"
K17-1006,Q15-1016,0,0.0524555,"Missing"
K17-1006,W13-0909,0,0.02709,"Missing"
K17-1006,P10-1116,0,0.0126172,"ng event type information by radicals increases both the precision and the recall in metaphor detection. Although this approach is especially effective for Chinese because of the information embedded in radicals, broader implications include the possibility of leveraging eventive information from different sources in other languages. 2 Related Work The task of metaphor detection has been handled in a wide variety of approaches including clustering models (Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010), semantic similarity graphs (Sporleder and Li, 2009), topic modeling (Li et al., 2010; Heintz et al., 2013), and compositional distributional semantic models (CDSMs) (Gutiérrez et al. 2016). Feature-based classification, in particular, attracts most attention since a wide array of contextual information is included (Sporleder and Li, 2009; Dunn., 2013; Hovy et al., 2011; Mohler et al., 2013; Neuman et al., 2013; Tsvetkov et al., 2013; Tsvetkov et al., 2014). Since the studies regarding metaphor identification have primarily focused on English, there are more available datasets in English in both manually-tagged linguistic resources (Gedigian et al., 2006; Krishnakumaran and Zh"
K17-1006,W13-0906,0,0.0184013,"Work The task of metaphor detection has been handled in a wide variety of approaches including clustering models (Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010), semantic similarity graphs (Sporleder and Li, 2009), topic modeling (Li et al., 2010; Heintz et al., 2013), and compositional distributional semantic models (CDSMs) (Gutiérrez et al. 2016). Feature-based classification, in particular, attracts most attention since a wide array of contextual information is included (Sporleder and Li, 2009; Dunn., 2013; Hovy et al., 2011; Mohler et al., 2013; Neuman et al., 2013; Tsvetkov et al., 2013; Tsvetkov et al., 2014). Since the studies regarding metaphor identification have primarily focused on English, there are more available datasets in English in both manually-tagged linguistic resources (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Broadwell et al., 2013) and corpus-based approach (Birke and Sarker, 2007; Shutova et al., 2013; Neuman et al., 2013; Hovy et al., 2013). Metaphor detection in Chinese is at the incipient stage. Fu et al., (2016) uses hierarchical clustering https://en.wikipedia.org/wiki/Baidu_Baike 37 for Chinese noun phrases according to their contextual i"
K17-1006,N10-1039,0,0.0144066,"ove the effectiveness of eventive information in detecting metaphors. The approach of leveraging event type information by radicals increases both the precision and the recall in metaphor detection. Although this approach is especially effective for Chinese because of the information embedded in radicals, broader implications include the possibility of leveraging eventive information from different sources in other languages. 2 Related Work The task of metaphor detection has been handled in a wide variety of approaches including clustering models (Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010), semantic similarity graphs (Sporleder and Li, 2009), topic modeling (Li et al., 2010; Heintz et al., 2013), and compositional distributional semantic models (CDSMs) (Gutiérrez et al. 2016). Feature-based classification, in particular, attracts most attention since a wide array of contextual information is included (Sporleder and Li, 2009; Dunn., 2013; Hovy et al., 2011; Mohler et al., 2013; Neuman et al., 2013; Tsvetkov et al., 2013; Tsvetkov et al., 2014). Since the studies regarding metaphor identification have primarily focused on English, there are more available datasets in English in b"
K17-1006,W13-0904,0,0.0160985,"ent sources in other languages. 2 Related Work The task of metaphor detection has been handled in a wide variety of approaches including clustering models (Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010), semantic similarity graphs (Sporleder and Li, 2009), topic modeling (Li et al., 2010; Heintz et al., 2013), and compositional distributional semantic models (CDSMs) (Gutiérrez et al. 2016). Feature-based classification, in particular, attracts most attention since a wide array of contextual information is included (Sporleder and Li, 2009; Dunn., 2013; Hovy et al., 2011; Mohler et al., 2013; Neuman et al., 2013; Tsvetkov et al., 2013; Tsvetkov et al., 2014). Since the studies regarding metaphor identification have primarily focused on English, there are more available datasets in English in both manually-tagged linguistic resources (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Broadwell et al., 2013) and corpus-based approach (Birke and Sarker, 2007; Shutova et al., 2013; Neuman et al., 2013; Hovy et al., 2013). Metaphor detection in Chinese is at the incipient stage. Fu et al., (2016) uses hierarchical clustering https://en.wikipedia.org/wiki/Baidu_Baike 37 for Chinese"
K17-1006,W07-0103,0,\N,Missing
K17-1006,E06-1042,0,\N,Missing
L16-1722,W11-2501,1,0.38469,"Missing"
L16-1722,E12-1004,0,0.22534,"Missing"
L16-1722,W09-0215,0,0.0557322,"Missing"
L16-1722,C92-2082,0,0.209128,"Missing"
L16-1722,P13-2078,0,0.00781986,"Missing"
L16-1722,N15-1098,0,0.476188,"Missing"
L16-1722,E14-1054,0,0.0390832,"Missing"
L16-1722,C14-1097,0,0.715404,"Missing"
L16-1722,E14-4008,1,0.754471,"close hypernym, which are therefore attributionally similar (Weeds et al., 2014). The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The fe"
L16-1722,Y14-1018,1,0.121991,"ntiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution (Santus et al., 2015) 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011). The ablation test has show"
L16-1722,W15-4208,1,0.746309,"s et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution (Santus et al., 2015) 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011). The ablation test has shown that four out of thirteen feat"
L16-1722,C08-1107,0,0.029533,"Missing"
L16-1722,W03-1011,0,0.807271,"ting hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly"
L16-1722,C14-1212,0,0.148115,"ymy in fact represents a key organization principle of semantic memory (Murphy, 2002), the backbone of taxonomies and ontologies, and one of the crucial semantic relations supporting lexical entailment (Geffet and Dagan, 2005). Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar (Weeds et al., 2014). The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have cl"
L16-1722,Y15-1021,1,0.87055,"Missing"
L16-1723,W11-2501,1,0.863897,"Missing"
L16-1723,J90-1003,0,0.489356,"ty measures play a fundamental role in tasks such as Information Retrieval (IR), Text Classification (TC), Text Summarization (TS), Question Answering (QA), Sentiment Analysis (SA), and so on (Terra and Clarke, 2003; Tungthamthiti et al., 2015). They can be either knowledge-based or corpus-based (Gomaa and Fahmy, 2013). The former rely on lexicons or semantic networks, such as WordNet (Fellbaum, 1998), measuring the distance between the nodes in the network. The latter, instead, compute the similarity between words relying on statistical information about their distributions in large corpora (Church and Hanks, 1990). Knowledge based approaches generally exploit hand-crafted resources. While being hand-crafted ensures high quality, it also entails arbitrariness and high development and update costs. This is the main reason why these resources are known for their limited coverage (Santus et al., 2015b). Such limitation has often prompted researchers to pursue hybrid approaches (Turney, 2001). A key assumption of corpus-based approaches is that similarity between words can be measured by looking at words co-occurrences. In particular, following the Distributional Hypothesis (Harris, 1954; Firth, 1957), thes"
L16-1723,C92-2082,0,0.177797,"approaches generally exploit the Distributional Hypothesis, according to which words that occur in similar contexts also have similar meanings (Harris, 1954). Although these approaches extract statistics from large corpora, they vary in the way they define what has to be considered context (i.e. lexical context, syntactic context, documents, etc.), how the association with such context is measured (e.g. frequency of co-occurrence, association measures like Pointwise Mutual Information, etc.), and how the association with the contexts is used to identify the similarity (Terra and Clarke, 2003; Hearst, 1992; Santus et al., 2014a; Santus et al., 2014b; Santus et al., 2016a). A common way to represent word meaning in NLP is by using vectors to encode the association between the target words and their contexts. The resulting vector space is generally referred as Vector Space Model (VSM) or, more specifically, as Distributional Semantic Model (DSM). In such vector space, word similarity can be calculated by using the Vector Cosine, which measures the angle between the vectors (Turney and Pantel, 2010). Other measures – such as Manhattan Distance, Dice’s Coefficient, Euclidean Distance, Jaccard Simil"
L16-1723,Q15-1016,0,0.605479,"ctor Cosine is generally considered to be the optimal choice (Bullinaria and Levy 2007). Another common way to represent word meaning is using word embeddings, which are vector-space word representations that are implicitly learned by the input-layer weights of neural networks. These models have shown a strong ability to capture synonymy and analogies (such as in the famous “King - Man + Woman = Queen” example, where Mikolov et al. (2013) subtract the vector of “Man” from the one of “King”, and then add the vector of “Woman”, obtaining a very similar vector to the one of “Queen”), even though Levy et al. (2015) have claimed that traditional count-based DSMs can achieve the same results if their hyperparameters are properly optimized. A well-known problem with the distributional approaches is that they rely on a very loose definition of similarity. In fact, vectors have as nearest neighbours not only synonyms, but also hypernyms, co-hyponyms, antonyms, as well as a wide range of other semantically related items (Santus et al., 2015). For this reason, several datasets have been proposed by the NLP community to test distributional similarity measures. Among the most common ones, there are the English a"
L16-1723,J07-2002,0,0.0420041,"Missing"
L16-1723,2003.mtsummit-papers.42,0,0.263036,"Missing"
L16-1723,W15-4208,1,0.935386,"Missing"
L16-1723,E14-4008,1,0.92777,"Missing"
L16-1723,N03-1032,0,0.173981,"Missing"
L16-1723,Y15-1021,1,0.830439,"Missing"
L16-1723,C08-1114,0,0.186322,"Missing"
li-etal-2006-mining,I05-1037,1,\N,Missing
li-etal-2006-mining,hutchinson-2004-mining,0,\N,Missing
O05-2006,P97-1008,0,0.0324378,"Missing"
O05-2006,P97-1009,0,0.229488,"Missing"
O05-2006,M98-1006,0,0.0571489,"Missing"
O05-2006,J93-1007,0,0.287471,"Missing"
O05-2006,P03-1016,0,0.126057,"Missing"
O05-2006,W03-1708,0,0.0411392,"Missing"
O05-2006,J90-1003,0,\N,Missing
O05-2006,W00-1209,0,\N,Missing
O05-4006,J90-1003,0,0.0287936,"Missing"
O05-4006,W99-0707,0,0.0516261,"Missing"
O05-4006,J93-2004,0,0.0264824,"Missing"
O05-4006,xia-etal-2000-developing,0,0.0976994,"Missing"
O05-4006,C02-1145,0,0.0465314,"Missing"
P06-1047,W04-1017,0,0.225201,"Missing"
P06-1047,W03-0502,0,0.0620901,"Missing"
P06-1047,P05-3013,0,0.00625203,"ir term vectors was used to generate links and define link strength. The same idea was followed and investigated exten370 occurrences. They roughly relate to “did What”. One or more associated named entities are considered as what are denoted by linguists as event arguments. Four types of named entities are currently under the consideration. These are &lt;Person&gt;, &lt;Organization&gt;, &lt;Location&gt; and &lt;Date&gt;. They convey the information of “Who”, “Whom”, “When” and “Where”. A verb or an action noun is deemed as an event term only when it presents itself at least once between two named entities. sively (Mihalcea, 2005). Yoshioka and Haraguchi (2004) went one step further toward eventbased summarization. Two sentences were linked if they shared similar events. When tested on TSC-3, the approach favoured longer summaries. In contrast, the importance of the verbs and nouns constructing events was evaluated with PageRank as individual nodes aligned by their dependence relations (Vanderwende, 2004; Leskovec, 2004). Although we agree that the fabric of event constitutions constructed by their syntactic relations can help dig out the important events, we have two comments. First, not all verbs denote event happeni"
P06-1047,P05-1018,0,0.0200939,"Missing"
P06-1047,N04-3012,0,0.0634733,"Missing"
P06-1047,N03-1020,0,\N,Missing
P07-2047,W04-1017,0,0.0566291,"Missing"
P07-2047,W04-3205,0,0.223325,"Missing"
P07-2047,P06-1047,1,0.858338,"Science and Technology mfliu_china@hotmail.com Most existing event-based summarization approaches rely on the statistical features derived from documents and generally associated with Event-based summarization extracts and single events, but they neglect the relations among organizes summary sentences in terms of events. However, events are commonly related the events that the sentences describe. In with one another especially when the documents to this work, we focus on semantic relations be summarized are about the same or very similar among event terms. By connecting terms topics. Li et al (2006) report that the improved with relations, we build up event term performance can be achieved by taking into graph, upon which relevant terms are account of event distributional similarities, but it grouped into clusters. We assume that each does not benefit much from semantic similarities. cluster represents a topic of documents. This motivated us to further investigate whether Then two summarization strategies are event-based summarization can take advantage of investigated, i.e. selecting one term as the the semantic relations of event terms, and most representative of each topic so as to co"
P07-2047,N03-1020,0,\N,Missing
P08-2023,H05-1091,0,0.0999416,"ck of necessary co-referenced mentions might be the main reason. 2 Related Work Many approaches have been proposed in the literature of relation extraction. Among them, feature-based and kernel-based approaches are most popular. Kernel-based approaches exploit the structure of the tree that connects two entities. Zelenko et al (2003) proposed a kernel over two parse trees, which recursively matched nodes from roots to leaves in a top-down manner. Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. The above two work was further advanced by Bunescu and Mooney (2005) who argued that the information to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph. Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. In the feature-based framework, Kambhatla (2004) employed ME models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention level, overlap, dependency and parse tr"
P08-2023,P04-1054,0,0.1814,"less effort than applying deep natural language processing. But unfortunately, entity co-reference does not help as much as we have expected. The lack of necessary co-referenced mentions might be the main reason. 2 Related Work Many approaches have been proposed in the literature of relation extraction. Among them, feature-based and kernel-based approaches are most popular. Kernel-based approaches exploit the structure of the tree that connects two entities. Zelenko et al (2003) proposed a kernel over two parse trees, which recursively matched nodes from roots to leaves in a top-down manner. Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. The above two work was further advanced by Bunescu and Mooney (2005) who argued that the information to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph. Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. In the feature-based framework, Kambhatla (2004) employed ME"
P08-2023,N07-1015,0,0.0958831,"y determining context-sensitive tree spans. In the feature-based framework, Kambhatla (2004) employed ME models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention level, overlap, dependency and parse tree. Based on his work, Zhou et al (2005) 89 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 89–92, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list. Jiang and Zhai (2007) then systematically explored a large space of features and evaluated the effectiveness of different feature subspaces corresponding to sequence, syntactic parse tree and dependency parse tree. Their experiments showed that using only the basic unit features within each feature subspace can already achieve state-of-art performance, while over-inclusion of complex features might hurt the performance. Previous approaches mainly focused on English relations. Most of them were evaluated on the ACE 2004 data set (or a sub set of it) which defined 7 relation types and 23 subtypes. Although Chinese p"
P08-2023,P05-1053,0,0.169738,"formation to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph. Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. In the feature-based framework, Kambhatla (2004) employed ME models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention level, overlap, dependency and parse tree. Based on his work, Zhou et al (2005) 89 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 89–92, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list. Jiang and Zhai (2007) then systematically explored a large space of features and evaluated the effectiveness of different feature subspaces corresponding to sequence, syntactic parse tree and dependency parse tree. Their experiments showed that using only the basic unit features within each"
P08-2023,D07-1076,0,0.0581352,"ects two entities. Zelenko et al (2003) proposed a kernel over two parse trees, which recursively matched nodes from roots to leaves in a top-down manner. Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. The above two work was further advanced by Bunescu and Mooney (2005) who argued that the information to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph. Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. In the feature-based framework, Kambhatla (2004) employed ME models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention level, overlap, dependency and parse tree. Based on his work, Zhou et al (2005) 89 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 89–92, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collecte"
P08-2023,I05-2023,0,0.101769,"Missing"
P08-2023,P06-1104,0,\N,Missing
P09-2029,N03-1020,0,0.0990589,"For the words in the hierarchical tree, set the initial states of the top n words3 as “activated” and the states of other words as “inactivated”. 2: For all the sentences in the document set, 3 4 Experiment Experiments are conducted on the DUC 2007 data set which contains 45 document sets. Each document set consists of 25 documents and a topic description as the query. In the task definition, the length of the summary is limited to 250 words. In our summarization system, preprocessing includes stop-word removal and word stemming (conducted by GATE4). One of the DUC evaluation methods, ROUGE (Lin and Hovy, 2003), is used to evaluate the content of the generated summaries. ROUGE is a state-of-the-art automatic evaluation method based on N-gram matching between system summaries and human summaries. In the experiment, our system is compared to the top systems in DUC 2007. Moreover, a baseline system which considers only the frequencies of words but ignores the relations between words is included for comparison. Table 1 below shows the average recalls of ROUGE-1, ROUGE-2 and ROUGE-SU4 over the 45 DUC 2007document sets. In the experiment, the proposed summarization system outperforms the baseline system,"
P09-2054,I05-3009,0,0.374292,"Missing"
P09-2054,C08-1130,1,0.621394,"Missing"
P14-2139,W06-1615,0,0.0434173,"Related works TTL has been widely used before the formal concept and definition of TTL was given in (Arnold, 2007). Wan introduced the co-training method into cross-lingual opinion analysis (Wan, 2009; Zhou et al., 2011), and Aue et al. introduced transfer learning into cross domain analysis (Aue, 2005) which solves similar problems. In this paper, we will use the terms source language and target language to refer to all cross lingual/domain analysis. Traditionally, transfer learning methods focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of ___________________ 860 *Corresponding author Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing"
P14-2139,P13-2084,0,0.0282056,"two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s cut edge weight statistic method in self-training (Li and Zhou, 2005) and co-training (Li and Zhou, 2011). Chao used Li’s method in tri-training (Chao et al, 2008). (Fukumoto et al, 2013) used the support vectors to detect class noise in semi-supervised learning. In TTL, however, training and testing samples cannot be assumed to have the same distributions. Thus, noise detection methods used in semisupervised learning are not directly suited in TTL. Y. Cheng has tried to use semi-supervised method (Jiang and Zhou, 2004) in transfer learning (Cheng and Li, 2009). His experiment showed that their approach would work when the source domain and the target domain share similar distributions. How to reduce negative transfers is still a problem in transfer learning. 3 Our Approach In"
P14-2139,P09-1027,0,0.379913,"are used beating the performance degradation curse of most transfer learning methods when training data reaches certain size. The rest of the paper is organized as follows. Section 2 introduces related works in transfer learning, cross lingual opinion analysis, and class noise detection technology. Section 3 presents our algorithm. Section 4 gives performance evaluation. Section 5 concludes this paper. 2 Related works TTL has been widely used before the formal concept and definition of TTL was given in (Arnold, 2007). Wan introduced the co-training method into cross-lingual opinion analysis (Wan, 2009; Zhou et al., 2011), and Aue et al. introduced transfer learning into cross domain analysis (Aue, 2005) which solves similar problems. In this paper, we will use the terms source language and target language to refer to all cross lingual/domain analysis. Traditionally, transfer learning methods focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of __"
P14-2139,W03-1730,0,0.0705095,"Missing"
P14-2139,P11-1033,0,0.0361894,"TTL was given in (Arnold, 2007). Wan introduced the co-training method into cross-lingual opinion analysis (Wan, 2009; Zhou et al., 2011), and Aue et al. introduced transfer learning into cross domain analysis (Aue, 2005) which solves similar problems. In this paper, we will use the terms source language and target language to refer to all cross lingual/domain analysis. Traditionally, transfer learning methods focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of ___________________ 860 *Corresponding author Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are two basic methods"
P14-2139,P12-1060,0,0.431442,"focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of ___________________ 860 *Corresponding author Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s c"
S10-1067,P03-1056,0,0.0468786,"Missing"
S10-1067,W05-0639,0,0.0565545,"Missing"
S10-1067,H93-1052,0,0.0736784,"Missing"
S10-1067,S10-1015,0,0.0418449,"Missing"
S12-1075,S12-1051,0,0.138087,"Missing"
S12-1075,P04-1077,0,0.0441386,"tic vector is used and the semantic relatedness between words is derived from two sources: WordNet and Wikipedia. Because WordNet is limited in its coverage, Wikipedia is used as a candidate for determining word similarity. Word order, however, is not considered in semantic vector. As semantic information are coded in sentences according to its order of writing, and in our systems, content words may not be adjacent to each other, we proposed to use skip bigrams to represent the structure of sentences. Skip bigrams, generally speaking, are pairs of words in a sentence order with arbitrary gap (Lin and Och, 2004a). Different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (Lin and Och, 2004a), the skip bigrams we used are weighted by a decaying factor of the skipping gap in a sentence, giving higher scores to closer occurrences of skip bigrams. It is reasonable to assume that similar sentences should have more overlapping skip bigrams, and the gaps in their shared skip bigrams should also be similar. The rest of this paper is organized as followed. Section 2 describes sentence similarity using semantic vectors and the order-sensitive skip"
S12-1075,W99-0625,0,0.74648,"Missing"
S13-1012,S12-1075,1,0.777484,"in}@comp.polyu.edu.hk Abstract (2006) proposed to incorporate the semantic vector and word order to calculate sentence similarity. Biemann et al. (2012) applied the log-linear regression model by combining the simple string based measures, for example, word ngrams and semantic similarity measures, for example, textual entailment. Similarly, Saric et al. (2012) used a support vector regression model which incorporates features computed from sentence pairs. The features are knowledge- and corpus-based word similarity, ngram overlaps, WordNet augmented word overlap, syntactic features and so on. Xu et al. (2012) combined semantic vectors with skip bigrams to determine sentence similarity, whereas the skip bigrams take into the sequential order between words. The Semantic Textual Similarity (STS) task aims to exam the degree of semantic equivalence between sentences (Agirre et al., 2012). This paper presents the work of the Hong Kong Polytechnic University (PolyUCOMP) team which has participated in the STS core and typed tasks of SemEval2013. For the STS core task, the PolyUCOMP system disambiguates words senses using contexts and then determine sentence similarity by counting the number of senses the"
S13-1012,W99-0625,0,\N,Missing
S13-1012,S12-1059,0,\N,Missing
S13-1012,S12-1060,0,\N,Missing
S13-1012,S12-1051,0,\N,Missing
W04-1113,J93-1007,0,0.254135,"Missing"
W04-1113,J90-1003,0,\N,Missing
W04-1113,P03-1016,0,\N,Missing
W04-1113,P97-1009,0,\N,Missing
W04-1114,C02-1145,0,0.0905706,"Missing"
W04-1114,J90-1003,0,\N,Missing
W04-1114,J93-2004,0,\N,Missing
W07-1511,W03-2405,0,0.024716,"lysis. However, the performances of automatic collocation extraction systems are not satisfactory (Pecina 2005). A problem is that collocations are word combinations that co-occur within a short context, but not all such co-occurrences are true collocations. Further examinations is needed to filter out pseudo-collocations once co-occurred word pairs are identified. A collocation bank with true collocations annotated is naturally an indispensable resource for collocation research. (Kosho et al. 2000) presented their works of collocation annotation on Japanese text. Also, the Turkish treebank, (Bedin 2003) included collocation annotation as one step in its annotation. These two collocation banks provided collocation identification and co-occurrence verification information. (Tutin 2005) used shallow analysis based on finite state transducers and lexicon-grammar to identify and annotate collocations in a French corpus. This collocation bank further provided the lexical functions of the collocations. However to this day, there is no reported Chinese collocation bank available. 61 Proceedings of the Linguistic Annotation Workshop, pages 61–68, c Prague, June 2007. 2007 Association for Computationa"
W07-1511,shudo-etal-2000-collocations,0,0.0790794,"Missing"
W07-1511,J93-1007,0,\N,Missing
W07-1511,P05-2003,0,\N,Missing
W12-6326,P98-1012,0,0.287534,"Missing"
W12-6326,S07-1058,0,0.0588558,"Missing"
W12-6326,C98-1012,0,\N,Missing
W18-6214,D17-1048,1,0.843637,"y a separate CNN model. Emotions in the minor text are further highlighted through an attention mechanism before emotion classification. Performance evaluation shows that incorporating WSCs features using deep learning models can improve performance measured by F1-scores compared to the state-of-the-art model. 1 Introduction Emotion analysis has been studied using different NLP methods from a variety of linguistic perspectives such as semantic, syntactic, and cognitive properties (Barbosa and Feng, 2010; Balamurali et al., 2011; Liu and Zhang, 2012; Wilson et al., 2013; Joshi and Itkat, 2014; Long et al., 2017). In many areas, such as Hong Kong and the Chinese Mainland, social media text is often written in mixed text with major text written in Chinese characters, an ideograph-based writing system. The minor text can be written in En1 2 https://en.wikipedia.org/wiki/Pinyin https://en.wikipedia.org/wiki/Code-switching 91 Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 91–96 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 the fuck up). People also use WSCs to ex"
W18-6214,D11-1100,0,0.0170015,"ion of the major text is learned through an LSTM model whereas the minor text is learned by a separate CNN model. Emotions in the minor text are further highlighted through an attention mechanism before emotion classification. Performance evaluation shows that incorporating WSCs features using deep learning models can improve performance measured by F1-scores compared to the state-of-the-art model. 1 Introduction Emotion analysis has been studied using different NLP methods from a variety of linguistic perspectives such as semantic, syntactic, and cognitive properties (Barbosa and Feng, 2010; Balamurali et al., 2011; Liu and Zhang, 2012; Wilson et al., 2013; Joshi and Itkat, 2014; Long et al., 2017). In many areas, such as Hong Kong and the Chinese Mainland, social media text is often written in mixed text with major text written in Chinese characters, an ideograph-based writing system. The minor text can be written in En1 2 https://en.wikipedia.org/wiki/Pinyin https://en.wikipedia.org/wiki/Code-switching 91 Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 91–96 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational"
W18-6214,C10-2005,0,0.0360372,"points. Then representation of the major text is learned through an LSTM model whereas the minor text is learned by a separate CNN model. Emotions in the minor text are further highlighted through an attention mechanism before emotion classification. Performance evaluation shows that incorporating WSCs features using deep learning models can improve performance measured by F1-scores compared to the state-of-the-art model. 1 Introduction Emotion analysis has been studied using different NLP methods from a variety of linguistic perspectives such as semantic, syntactic, and cognitive properties (Barbosa and Feng, 2010; Balamurali et al., 2011; Liu and Zhang, 2012; Wilson et al., 2013; Joshi and Itkat, 2014; Long et al., 2017). In many areas, such as Hong Kong and the Chinese Mainland, social media text is often written in mixed text with major text written in Chinese characters, an ideograph-based writing system. The minor text can be written in En1 2 https://en.wikipedia.org/wiki/Pinyin https://en.wikipedia.org/wiki/Code-switching 91 Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 91–96 c Brussels, Belgium, October 31, 2018. 2018 Asso"
W18-6214,W04-3253,0,0.256029,"Missing"
W18-6214,S17-2088,0,0.0652001,"Missing"
W18-6214,C14-1008,0,0.0239622,"democracy. This paper studies WSCs related textual features from the orthography perspective to explore their effectiveness as emotion indicators. Previous studies in emotion analysis mostly rely on emotion lexicon, context information, or semantic knowledge to improve sentence level classification tasks. This linguistic knowledge is often used to transform raw data into feature vector, called feature engineering (Kanter and Veeramachaneni, 2015). However, WSCs can break the syntax of the major text and the switched minor text also lacks linguistic cues in this type of social media data (Dos Santos and Gatti, 2014). This makes feature engineering-based methods difficult to work. Neologism in the Internet forums increases the difficulty for both syntactic and semantic analysis. In particular, newly coined phrases tend to contain different types of symbols. Despite the challenges, this type of datasets is rich in shifts of writing systems orthographically. This characteristic offers reliable clues for emotion classification. Since WSCs is relatively common in realtime on-line platforms like microblog in China 3 . This work adopts a broader scope of WSCs to include switching between two languages, and chan"
W18-6214,C16-1153,0,0.0436569,"Missing"
W18-6214,N16-1174,0,0.119974,"Missing"
W18-6214,W15-3116,0,0.0276817,"ods, the word representation in di = w ~ 1 , ..., w ~ m , di , is learned using two networks. To distinguish the WSC units, they are given designed switch labels w~js (w~js ⊂di , j = 1...k) and are extracted to be fed into the CNN as an extra feature. di is fed into LSTM to generate the hidden vector ~h1 , ~h2 ...~hm from di . In Chinese social media, WSCs segments are generally dispersed sporadically. So, for di with k WSC segments, the convolution is calculated using a sliding window of size 2n + 1: k+n X − −−→p = conv w ~ ps , A Chinese microblog dataset is used for performance evaluation (Lee and Wang, 2015). We first present the dataset with some analysis first and then proceed to make performance comparison to baseline systems on emotion classification. 3.1 ~ wsc = R Pk (1) The major text is written in Chinese characters. The WSC segments contain English words and Pinyin scripts, acronyms of Pinyin or other scripts. The annotation of emotions in each instance allows more than one class label. Each instance is labeled independently by the five emotion classes, happiness, sadness, anger, fear and surprise, based on the Ekman model (1992) except for Disgust. The emotion label can be contributed by"
W18-6220,D17-1054,0,0.279419,"rd-level preference is considered rather than information at the semantic level (Chen et al., 2016). Gui et al. (2016) introduce an inter-subjectivity network to link users to the terms they used as well as the polarities of the terms. The network aims to learn writer embeddings which are subsequently incorporated into a CNN network for sentiment analysis. Chen et al. (2016) propose a model to incorporate user and product information into an LSTM with attention mechanism. This model is reported to produce the state-of-the-art results in the three benchmark datasets (IMDB, Yelp13, and Yelp14). Dou (2017) also proposes a deep memory network to integrate user profile and product information in a unified model. However, the model only achieves a comparable result to the state-of-the-art attention based LSTM (Chen et al., 2016). 3 3.3 Inspired by the successful use of memory networks in language modeling, question answering, and sentiment analysis (Sukhbaatar et al., 2015; Tang et al., 2016; Dou, 2017), we propose our DUPMN by extending a single memory network model to two memory networks to reflect different influences from users’ perspective and products’ perspective. The structure of the model"
W18-6220,D14-1080,0,0.0308845,"uct information into a CNN network for document level sentiment classification. User ids and product names are included as features in a unified document vector using the vector space model such that document vectors capture important global clues include individual preferences and product information. Neural Network Models In recent years, deep learning has greatly improved the performance of sentiment analysis. Commonly used models include Convolutional Neural Networks (CNNs) (Socher et al., 2011), Recursive Neural Network (ReNNs) (Socher et al., 2013), and Recurrent Neural Networks (RNNs) (Irsoy and Cardie, 2014). RNN naturally benefits sentiment classification because of its ability to 141 bedding representation of documents. The first LSTM layer is used to obtain sentence representation by the hidden state of an LSTM network. The same mechanism is also used for document level representation with sentence-level representation as input. User and product attentions are included in the network so that all salient features are included in document representation. For document ~ d~ is a vector repd, its embedding is denoted as d. resentation with dimension size n. In principle, the embedding representatio"
W18-6220,N16-2016,0,0.0569405,"Missing"
W18-6220,D14-1181,0,0.0217797,"Missing"
W18-6220,I17-2043,1,0.936773,"e gradient vanishing problem. An LSTM model provides a gated mechanism to keep the long-term memory. Each LSTM layer is generally followed by mean pooling and the output is fed into the next layer. Experiments in datasets which contain sentences and long documents demonstrate that LSTM model outperforms the traditional RNNs (Tang et al., 2015a,c). Attention mechanism is also added to LSTM models to highlight important segments at both sentence level and document level. Attention models can be built from text in local context (Yang et al., 2016), user/production information (Chen et al., 2016; Long et al., 2017a) and other information such as cognition grounded eye tracking data (Long et al., 2017b). LSTM models with attention mechanism are currently the state-of-theart models in document sentiment analysis tasks (Chen et al., 2016; Long et al., 2017b). Memory networks are designed to handle larger context for a collection of documents. Memory networks introduce inference components combined with a so called long-term memory component (Weston et al., 2014). The long-term memory component is a large external memory to represent data as a collection. This collective information can contain local conte"
W18-6220,D17-1048,1,0.797194,"e gradient vanishing problem. An LSTM model provides a gated mechanism to keep the long-term memory. Each LSTM layer is generally followed by mean pooling and the output is fed into the next layer. Experiments in datasets which contain sentences and long documents demonstrate that LSTM model outperforms the traditional RNNs (Tang et al., 2015a,c). Attention mechanism is also added to LSTM models to highlight important segments at both sentence level and document level. Attention models can be built from text in local context (Yang et al., 2016), user/production information (Chen et al., 2016; Long et al., 2017a) and other information such as cognition grounded eye tracking data (Long et al., 2017b). LSTM models with attention mechanism are currently the state-of-theart models in document sentiment analysis tasks (Chen et al., 2016; Long et al., 2017b). Memory networks are designed to handle larger context for a collection of documents. Memory networks introduce inference components combined with a so called long-term memory component (Weston et al., 2014). The long-term memory component is a large external memory to represent data as a collection. This collective information can contain local conte"
W18-6220,P14-5010,0,0.00443626,"configuration by the development dataset is used for the test set to obtain the final result. Experiment and Result Analysis Performance evaluations are conducted on three datasets and DUPMN is compared with a set of commonly used baseline methods including the state-of-the-art LSTM based method (Chen et al., 2016; Wu et al., 2018). 4.1 IMDB 10 84,919 1,310 1,635 24.56 64.82 51.93 1,223 318 72 22 Datasets The three benchmarking datasets include movie reviews from IMDB, restaurant reviews from Yelp13 and Yelp14 developed by Tang (2015a). All datasets are tokenized using the Stanford NLP tool (Manning et al., 2014). Table 1 lists statistics of the datasets including the number of classes, number of documents, average length of sentences, the average number of documents per user, and the average number of documents per product. 4.2 Baseline Methods In order to make a systematic comparison, three groups of baselines are used in the evaluation. Group 1 includes all commonly used feature sets mentioned in Chen et al. (2016) including Majority, Trigram, Text features (TextFeatures), and AveWordvec. All feature sets in Group 1 except 143 G1 G2 G3 New Model Majority Trigram TextFeature AvgWordvec SSWE RNTN+RNN"
W18-6220,K16-1016,0,0.063936,"Missing"
W18-6220,D16-1172,0,0.0212771,"hus K=1 is used. Majority use the SVM classifier. Group 2 methods include the recently published sentiment analysis models which only use context information, including: • SSWE (Tang et al., 2014) — An SVM model using sentiment specific word embedding. • InterSub (Gui et al., 2016) — A CNN model making use of user and product information. • RNTN+RNN (Socher et al., 2013) — A Recursive Neural Tensor Network (RNTN) to represent sentences. • LSTM+UPA (Chen et al., 2016) — The state-of-the-art LSTM including both local context based attentions and user/product in the attention mechanism. • CLSTM (Xu et al., 2016) — A Cached LSTM model to capture overall semantic information in long text. For the DUPMN model, we also include two variations which use only one memory network. The first variation only includes user profiles in the memory network, denoted as DUPMN-U. The second variation only uses product information, denoted as DUPMN-P. • LSTM+LA (Chen et al., 2016) — A state-ofthe-art LSTM using local context as attention mechanism at both sentence level and document level. 4.3 • LSTM+CBA (Long et al., 2017b)— A state-of-the-art LSTM model using cognition based data to build attention mechanism. Performa"
W18-6220,D11-1014,0,0.138511,"on have crucial effects on sentiment polarities. Tang et al. (2015b) proposed a model by incorporating user and product information into a CNN network for document level sentiment classification. User ids and product names are included as features in a unified document vector using the vector space model such that document vectors capture important global clues include individual preferences and product information. Neural Network Models In recent years, deep learning has greatly improved the performance of sentiment analysis. Commonly used models include Convolutional Neural Networks (CNNs) (Socher et al., 2011), Recursive Neural Network (ReNNs) (Socher et al., 2013), and Recurrent Neural Networks (RNNs) (Irsoy and Cardie, 2014). RNN naturally benefits sentiment classification because of its ability to 141 bedding representation of documents. The first LSTM layer is used to obtain sentence representation by the hidden state of an LSTM network. The same mechanism is also used for document level representation with sentence-level representation as input. User and product attentions are included in the network so that all salient features are included in document representation. For document ~ d~ is a v"
W18-6220,N16-1174,0,0.18836,"Missing"
W18-6220,D13-1170,0,0.163024,"Written text is often meant to express sentiments of individuals. Recognizing the underlying sentiment expressed in the text is essential to understand the full meaning of the text. The SA community is increasingly interested in using natural language processing (NLP) techniques as well as sentiment theories to identify sentiment expressions in the text. Recently, deep learning based methods have taken over feature engineering approaches to gain further performance improvement in SA. Typical neural network models include Convolutional Neural Network (CNN) (Kim, 2014), Recursive auto-encoders (Socher et al., 2013), Long-Short Term Memory (LSTM) (Tang et al., 2015a), and many more. Attention-based models are introduced to highlight important words and sentences in a piece 140 Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 140–148 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 capture sequential information in text. However, standard RNNs suffer from the so-called gradient vanishing problem (Bengio et al., 1994) where gradients may grow or decay exponentially ove"
W18-6220,D15-1167,0,0.200971,"individuals. Recognizing the underlying sentiment expressed in the text is essential to understand the full meaning of the text. The SA community is increasingly interested in using natural language processing (NLP) techniques as well as sentiment theories to identify sentiment expressions in the text. Recently, deep learning based methods have taken over feature engineering approaches to gain further performance improvement in SA. Typical neural network models include Convolutional Neural Network (CNN) (Kim, 2014), Recursive auto-encoders (Socher et al., 2013), Long-Short Term Memory (LSTM) (Tang et al., 2015a), and many more. Attention-based models are introduced to highlight important words and sentences in a piece 140 Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 140–148 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 capture sequential information in text. However, standard RNNs suffer from the so-called gradient vanishing problem (Bengio et al., 1994) where gradients may grow or decay exponentially over long sequences. LSTM models are adopted to solve"
W18-6220,P15-1098,0,0.676968,"individuals. Recognizing the underlying sentiment expressed in the text is essential to understand the full meaning of the text. The SA community is increasingly interested in using natural language processing (NLP) techniques as well as sentiment theories to identify sentiment expressions in the text. Recently, deep learning based methods have taken over feature engineering approaches to gain further performance improvement in SA. Typical neural network models include Convolutional Neural Network (CNN) (Kim, 2014), Recursive auto-encoders (Socher et al., 2013), Long-Short Term Memory (LSTM) (Tang et al., 2015a), and many more. Attention-based models are introduced to highlight important words and sentences in a piece 140 Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 140–148 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 capture sequential information in text. However, standard RNNs suffer from the so-called gradient vanishing problem (Bengio et al., 1994) where gradients may grow or decay exponentially over long sequences. LSTM models are adopted to solve"
W18-6220,D16-1021,0,0.0651268,"ly the state-of-theart models in document sentiment analysis tasks (Chen et al., 2016; Long et al., 2017b). Memory networks are designed to handle larger context for a collection of documents. Memory networks introduce inference components combined with a so called long-term memory component (Weston et al., 2014). The long-term memory component is a large external memory to represent data as a collection. This collective information can contain local context (Das et al., 2017) or external knowledge base (Jain, 2016). It can also be used to represent the context of users and products globally (Tang et al., 2016). Dou uses (2017) a memory network model in document level sentiment analysis and makes comparable result to the state-of-the-art model (Chen et al., 2016). product should give dual consideration to individual users as well as all reviews as a collection. In this paper, we address the aforementioned issue by proposing to learn user profiles and product review information separately before making a joint prediction on sentiment classification. In the proposed Dual User and Product Memory Network (DUPMN) model, we first build a hierarchical LSTM (Hochreiter and Schmidhuber, 1997) model to genera"
W18-6220,P14-1146,0,0.0395321,"686 0.686 0.678 0.784 0.662 0.690 0.654 0.639 MAE 0.744 0.513 0.520 0.568 N/A N/A N/A N/A N/A 0.464 0.369 N/A N/A 0.351 Table 2: Evaluation of different methods; best result/group in accuracy is marked in bold; second best is underlined. • UPDMN (Dou, 2017) — A deep memory network for document level sentiment classification by including user and product information in a unified model. Hop 1 gives the best result, and thus K=1 is used. Majority use the SVM classifier. Group 2 methods include the recently published sentiment analysis models which only use context information, including: • SSWE (Tang et al., 2014) — An SVM model using sentiment specific word embedding. • InterSub (Gui et al., 2016) — A CNN model making use of user and product information. • RNTN+RNN (Socher et al., 2013) — A Recursive Neural Tensor Network (RNTN) to represent sentences. • LSTM+UPA (Chen et al., 2016) — The state-of-the-art LSTM including both local context based attentions and user/product in the attention mechanism. • CLSTM (Xu et al., 2016) — A Cached LSTM model to capture overall semantic information in long text. For the DUPMN model, we also include two variations which use only one memory network. The first variat"
xu-etal-2012-grammar,Y06-1024,1,\N,Missing
xu-etal-2012-grammar,Y96-1018,1,\N,Missing
Y06-1014,P97-1065,0,0.0367446,"Missing"
Y06-1014,W03-1704,0,0.0611892,"Missing"
Y06-1014,W01-0513,0,\N,Missing
Y06-1014,W03-1026,0,\N,Missing
Y06-1014,C96-1009,0,\N,Missing
Y06-1015,kis-etal-2004-new,0,0.0245865,"languages. Thus, the hybrid syntactic-statistical approaches seem to be viable in collocation extraction, and indeed, such approaches to collocation extraction have been applied in a number of projects using European language corpora. These will be described in the next section. 109 In this paper, we present the TCtract system, a syntactic-based Chinese collocation extraction system enhanced by the use of additional statistical measures. Central to the design is that the extracted candidate collocations should be a pair having a defined grammatical relationship. A recent study on collocation [2] expressed a similar idea, saying that “collocation research is especially valuable if it aims at finding typed collocations, that is, collocations selected on the basis of some morpho-syntactic properties, as opposed to the extraction of typeless ones”. At this stage, TCtract extracts one kind of typed collocation, noun phrase collocations. These collocations are represented in a set of phrase rules validated by using a test corpus and manual checking. It will be easy to extend the rule-based representation in the future to include other linguistic knowledge such as grammar, syntax, phrase an"
Y06-1015,W03-1717,0,0.641604,"Missing"
Y06-1015,O04-1027,0,0.340191,"Missing"
Y06-1015,J93-1003,0,0.16112,"Missing"
Y06-1015,J00-3001,0,0.0378141,"Missing"
Y06-1015,J93-1001,0,0.0616031,"m+n&gt;; &lt;r+n&gt;; &lt;b+n&gt;; &lt;i+n&gt;; &lt;l+n&gt;; &lt;p+n&gt;; &lt;s+n&gt;; &lt;q+n&gt;; &lt;j+n&gt;. 3. χ2-test The χ2-test is formulated below: χ2 = N ( fx ⋅ O x y − Ox y ⋅ Ox y ) 2 (6) f x ⋅ f y ⋅ (Oxy + Oxy ) ⋅ (Oxy + Oxy ) Where N : of the total instances of BNP; O : of the total instances of pair (x;y) fy: of the total instances of y fx: of the total instances of x; O : of pairs do no contain x and y simultaneously xy Oxy : of pairs contain y but not x; Oxy : of pairs contain x but not y The statistical measures given in formulas (2) to (4) assume that the data are normally distributed which is proven to be untrue for English [16]. It is also been proven untrue for Chinese corpus [20]. Therefore, the χ2-test is used in this work because it does not assume normal distribution probabilities. 3.3.3 Rejection Rules After applied the association measures in Stage two, there are still some pseudo-collocations remained because of their high co-occurrence frequency which makes the AMs identify them as collocations. For example “很多/m 机遇/n”, “亿万/m 资产/n”, “某些/r 单位/n”, “各项/r 资金/n”, “某种/r 道德/n” etc. These pseudo-collocations will be weeded out by rejection rules such as [/r /n] and [/m /n] when they are identified by these rejectio"
Y06-1015,O05-4006,1,0.399335,", t-score. As there are rare attempts made on typed collocation extraction in Chinese, one of aims of this work is to evaluate the usefulness of different association measures for extracting typed collocations from Chinese corpus. 110 3 TCtract Collocations are defined as a recurrent and conventional expression of words which holds semantic relations and fits predefined syntactic structures. This paper focuses on the bi-gram noun phrase collocations extraction. 3.1 Resources and Evaluation Methods Two corpuses are used in the experiments: one is a one million small data corpus, namely corpusS [18], tokenized by linguists with chunking information as well as PoS tagging. Another is a larger corpus with half a year People’s Daily newspaper prepared by Peking University [20], called corpusL which contains 11 million data with PoS tag information only. The BNP patterns are extracted from corpusS first. Then, a set of candidate lexeme pairs matched the types predefined are extracted from corpusL and further passed to the association measures procedure for further evaluation. As stated early, different association measures may be effective in the exaction of different types of collocation. I"
Y06-1015,seretan-etal-2004-using,0,0.165527,"stical approaches would appear to be insuperable. For this reason it would appear that it may be useful to introduce other identifying features as hybrids to improve the precision of collocation extraction. On the face of it, linguistic knowledge is one area that would seem to offer an abundance of potentially useful text-related features such as semantic knowledge, morphological knowledge and, as will be discussed in this paper, syntactic knowledge, Studies that have made use of syntactic knowledge mainly fall into two broad categories. The first category makes use of syntactic filters ([1], [21]), which will not be considered in this paper. The second category makes use of collocation patterns ([3]), such as Adjective-Noun, Noun-Noun, Verb-Noun, Verb-Object, Subject-Verb, etc. However, the syntactic knowledge, such as phrase rules in this work, is mainly used for identifying occurrences of particular phenomena of certain given rules. It is not likely to apply the phrase rules individually because both inter-conflicts and the precision of the rules tend to introduce noise if no further strategies are used. Furthermore, there exist statistical regularities in natural languages. Thus, t"
Y06-1015,J96-1001,0,\N,Missing
Y06-1015,J93-1007,0,\N,Missing
Y11-1045,J93-1003,0,0.284544,"Missing"
Y11-1045,C10-1014,0,0.0600026,"Missing"
Y11-1045,O04-1027,0,0.0447803,"Missing"
Y11-1045,O05-2006,1,0.879104,"Missing"
Y11-1045,Y06-1015,1,0.753902,"s also existed. For example “重要/位置” and “主要/位置” are one synonym collocation pair because “重要” and “主要” are synonyms and “重要/位置” and “主要/位置” co-occurs in Chinese corpus. “重要/位置” and “重要/地位” is another synonym collocation pair because “位置” and “地位” are synonyms. Based on the  Acknowledgments “The work is partially supported by a grant from The Chiang Ching-kuo Foundation for International Scholarly Exchange under the project RG013-D-09” . Copyright 2011 by Wanyin Li, Qin Lu 25th Pacific Asia Conference on Language, Information and Computation, pages 430–439 430 previous work (Li et. al., 2005; Li and Lu, 2006), this work proposes a hybrid approach to employ the syntactic, semantic and statistical information for the extraction of syntactically bound synony bi-gram collocations. This paper focuses on collocation extraction in noun/verb phrases. A sub-model of HowNet based similarity calculation is proposed to identify bi-gram synonymous collocations derived from a base noun/verb phrase structure, say in the distance of [-5, +5], especially the ones in low frequency from a monolingual corpora. Our pattern generation process from the actual training data is similar to the works in (Seretan, 2005). The"
Y11-1045,P97-1009,0,0.0570286,"Missing"
Y11-1045,W03-1810,0,0.10219,"Missing"
Y11-1045,J96-1001,0,0.233409,"Missing"
Y11-1045,seretan-etal-2004-using,0,0.0723617,"Missing"
Y11-1045,P03-1016,0,0.174347,"e mainly based on the similarity calculation. Lin (Lin, 1997) proposed a distributional hypothesis that if two words have similar sets of collocations, they are considered similar. According to (Miller, 1992), two expressions are synonymous in a context C if the substitution of one for the other in C does not change the truth-value of a sentence in which the substitution is made. Liu Qun (Liu et al., 2002) defined word similarity as two words that can substitute for each other in a context and keep the sentence consistent in syntax and semantic structure. Researchers (Lin, 1997; Pearce, 2001; Wu and Zhou, 2003) have applied similarity-based calculation for collocation identifications. Pearce identified collocations by relying on a mapping from one word to its synonyms for each of its senses. (Wu and Zhou, 2003) are the first researches to extract synonymous collocations by mapping synonyms relationships between two different languages to automatically acquire English synonymous collocations. However, this method needs a parallel corpus which is difficult to be obtained in real case. 3. System Design Figure 1 shows the system framework consisted of two cascaded modules. Module I, labeled as BNP/BVP b"
Y11-1045,W03-1717,0,0.0283035,"Missing"
Y11-1045,O05-4006,1,0.826334,"Missing"
Y14-1018,J10-4006,1,0.804961,"of the contrasting pairs in GRE closest-to-opposite questions are not listed as opposites in WordNet”. Copyright 2014 by Enrico Santus, Qin Lu, Alessandro Lenci and Chu-Ren Huang 28th Pacific Asia Conference on Language, Information and Computation pages 135–144 !135 PACLIC 28 The automatic identification of semantic relations is a core task in computational semantics. Distributional Semantic Models (DSMs) have often been exploited for their well known ability to identify semantically similar lexemes using corpus-derived co-occurrences encoded as distributional vectors (Santus et al., 2014a; Baroni and Lenci, 2010; Turney and Pantel, 2010; Padó and Lapata, 2007; Sahlgren, 2006). These models are based on the Distributional Hypothesis (Harris, 1954) and represent lexical semantic similarity in function of distributional similarity, which can be measured by vector cosine (Turney and Pantel, 2010). However, these models are characterized by a major shortcoming. That is, they are not able to discriminate among different kinds of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponym"
Y14-1018,P10-1018,0,0.0497849,"Missing"
Y14-1018,2020.inlg-1.14,0,0.0238601,"Missing"
Y14-1018,C92-2082,0,0.398782,"gether with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a distributional interpretation of the so-called paradox of simultaneous similarity and difference between the antonyms (Cruse, 1986). According to this paradox, antonyms are similar to synonyms"
Y14-1018,J91-1001,0,0.690848,"l need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a distributional interpretation of the so-called paradox of simultaneous similarity and difference between the antonyms (Cruse, 1986). According to this paradox, antonyms are similar to synonyms in every dimension of meaning except one. Our hypothesis is that the different dimension of meaning is a salient one and it can be identified with DSMs and exploited for discriminating antonyms from synonyms. The rest of the paper is organized as follows. Section 2 gives the definition and illustrates the various types of antonyms. Section 3 gives a brief o"
Y14-1018,D13-1169,0,0.0689646,"Missing"
Y14-1018,S12-1012,1,0.925115,"are characterized by a major shortcoming. That is, they are not able to discriminate among different kinds of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponyms like house, meronyms like brick, antonyms like shack, together with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Ch"
Y14-1018,W11-2128,0,0.0412439,"Missing"
Y14-1018,H05-1067,0,0.0744627,"Missing"
Y14-1018,D08-1103,0,0.428385,"makes use of Average Precision to estimate the extent and salience of the intersection among the most descriptive contexts of two target words. Evaluation shows that the proposed method is able to distinguish antonyms and synonyms with high accuracy across different parts of speech, including nouns, adjectives and verbs. APAnt outperforms the vector cosine and a baseline model implementing the cooccurrence hypothesis. 1 Introduction Antonymy is one of the fundamental relations shaping the organization of the semantic lexicon and its identification is very challenging for computational models (Mohammad et al., 2008; Deese, 1965; Deese, 1964). Yet, antonymy is essential for many Natural Language Processing (NLP) applications, such as Information Retrieval (IR), Ontology Learning (OL), Machine Translation (MT), Sentiment Analysis (SA) and Dialogue Systems (Roth and Schulte im Walde, 2014; Mohammad et al., 2013). In particular, the automatic identification of semantic opposition is a crucial component for the detection and generation of paraphrases (Marton et al., 2011), the understanding of contradictions (de Marneffe et al., 2008) and the detection of humor (Mihalcea and Strapparava, 2005). Several exist"
Y14-1018,J07-2002,0,0.0433936,"ite questions are not listed as opposites in WordNet”. Copyright 2014 by Enrico Santus, Qin Lu, Alessandro Lenci and Chu-Ren Huang 28th Pacific Asia Conference on Language, Information and Computation pages 135–144 !135 PACLIC 28 The automatic identification of semantic relations is a core task in computational semantics. Distributional Semantic Models (DSMs) have often been exploited for their well known ability to identify semantically similar lexemes using corpus-derived co-occurrences encoded as distributional vectors (Santus et al., 2014a; Baroni and Lenci, 2010; Turney and Pantel, 2010; Padó and Lapata, 2007; Sahlgren, 2006). These models are based on the Distributional Hypothesis (Harris, 1954) and represent lexical semantic similarity in function of distributional similarity, which can be measured by vector cosine (Turney and Pantel, 2010). However, these models are characterized by a major shortcoming. That is, they are not able to discriminate among different kinds of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponyms like house, meronyms like brick, antonyms like"
Y14-1018,P06-1015,0,0.714471,"e brick, antonyms like shack, together with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a distributional interpretation of the so-called paradox of simultaneous similarity and difference between the antonyms (Cruse, 1986). According to this paradox, antonyms are simi"
Y14-1018,P14-2086,0,0.361997,"Missing"
Y14-1018,W14-5814,0,0.121769,"Missing"
Y14-1018,C02-1061,0,0.103734,"Missing"
Y14-1018,C08-1114,0,0.435435,"r lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponyms like house, meronyms like brick, antonyms like shack, together with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a dis"
Y14-1018,E14-4008,1,0.918306,"wo lexemes are antonyms or synonyms by looking at the extent and salience of this intersection: the broader and more salient the intersection, the higher the probability that the lexemes are synonyms; vice versa the narrower and less salient the intersection, the higher the probability that the lexemes are antonyms. To verify this hypothesis, we select the N most salient contexts of the two target words (N=1001). We define the salience of a context for a specific target word by ranking the contexts through Local Mutual Information (LMI; Evert, 2005) and picking the first N, as already done by Santus et al. (2014a). Once the N most salient contexts for the two target words have been identified, we verify the extent and the salience of the contexts shared by both the target words. We predict that synonyms share a significantly higher number of salient contexts than antonyms. To estimate the extent and the salience of the shared contexts, we adapt the Average Precision measure (AP; Voorhees and Harman, 1999), a common Information Retrieval (IR) evaluation metric already used by Kotlerman et al. (2010) to identify lexical entailment. In IR systems, this measure is used to evaluate the ranked documents re"
Y14-1018,J13-3004,0,\N,Missing
Y14-1018,P08-1118,0,\N,Missing
Y16-2013,S14-1002,0,0.0195299,"for news headlines (Strapparava and Mihalcea, 2007), RenCECps for blogs in both sentence and document levels (Quan and Ren, 2009), and NLP&CC 2013 for Chinese microblogs. Because of the rapid development of social networks, many studies also try to automatically construct emotion corpus from the web using reader added emotion tags as labels. Lin crawled a news article corpus based on the emotion related tags by readers from Yahoo!s news (Lin et al., 2007). Hashtags, emoticons, and emoji characters are also used as naturally annotated labels to construct large emotion corpus from social media (Bandhakavi et al., 2014; Mohammad et al., 2013; Wang et al., 2012). However, these naturally annotated labels often contain noise. A recent trend is to make use of crowdsourcing to obtain annotated data. Crowdsourcing can be reliable if some control strategies are properly used. Example of resources obtained by crowdsourcing include lexicons constructed by Hutto (2013) and Mohammad (2013). Emotion classiﬁcation can be categorized into 1) rule based methods and machine learning based methods. As an example of rule based systems, the work by Chaumartin (2007) uses a set of hand crafted rules based on common knowledge"
Y16-2013,W11-3706,0,0.0801885,"Missing"
Y16-2013,S07-1094,0,0.0398999,"to construct large emotion corpus from social media (Bandhakavi et al., 2014; Mohammad et al., 2013; Wang et al., 2012). However, these naturally annotated labels often contain noise. A recent trend is to make use of crowdsourcing to obtain annotated data. Crowdsourcing can be reliable if some control strategies are properly used. Example of resources obtained by crowdsourcing include lexicons constructed by Hutto (2013) and Mohammad (2013). Emotion classiﬁcation can be categorized into 1) rule based methods and machine learning based methods. As an example of rule based systems, the work by Chaumartin (2007) uses a set of hand crafted rules based on common knowledge to analyze the emotions of news headlines. In another 154 PACLIC 30 Proceedings system, Strapparava (2008) represents each emotion and text using latent semantic analysis (LSA) and analyzes the corresponding emotion based on the similarity between the text and the corresponding emotions. Machine learning based methods, on the other hand, heavily rely on the availability of training data as well as good feature selection methods. Mohammad shows that the combined using of emotion lexicon and N-gram features is more effective than N-gram"
Y16-2013,P14-1119,0,0.0731175,"Missing"
Y16-2013,lee-etal-2014-annotating,0,0.0221342,"Missing"
Y16-2013,S13-2053,0,0.0288837,"parava and Mihalcea, 2007), RenCECps for blogs in both sentence and document levels (Quan and Ren, 2009), and NLP&CC 2013 for Chinese microblogs. Because of the rapid development of social networks, many studies also try to automatically construct emotion corpus from the web using reader added emotion tags as labels. Lin crawled a news article corpus based on the emotion related tags by readers from Yahoo!s news (Lin et al., 2007). Hashtags, emoticons, and emoji characters are also used as naturally annotated labels to construct large emotion corpus from social media (Bandhakavi et al., 2014; Mohammad et al., 2013; Wang et al., 2012). However, these naturally annotated labels often contain noise. A recent trend is to make use of crowdsourcing to obtain annotated data. Crowdsourcing can be reliable if some control strategies are properly used. Example of resources obtained by crowdsourcing include lexicons constructed by Hutto (2013) and Mohammad (2013). Emotion classiﬁcation can be categorized into 1) rule based methods and machine learning based methods. As an example of rule based systems, the work by Chaumartin (2007) uses a set of hand crafted rules based on common knowledge to analyze the emotions"
Y16-2013,N12-1071,0,0.0827493,"hand crafted rules based on common knowledge to analyze the emotions of news headlines. In another 154 PACLIC 30 Proceedings system, Strapparava (2008) represents each emotion and text using latent semantic analysis (LSA) and analyzes the corresponding emotion based on the similarity between the text and the corresponding emotions. Machine learning based methods, on the other hand, heavily rely on the availability of training data as well as good feature selection methods. Mohammad shows that the combined using of emotion lexicon and N-gram features is more effective than N-gram feature only (Mohammad, 2012). Quan makes use of emotional words based features and tries to apply them to different classiﬁers such as SVM, Naive Bayes, and decision trees for sentence level blog emotion classiﬁcation (Quan and Ren, 2009). Based on word embedding, Chen uses a sentence vector in combination with ML-KNN for microblog data (Chen et al., 2014). Inspired by Poria (2014) that uses dependency features and CRF model, a segment-based method is proposed to extract sentence segments using dependency trees and the semi-CRF model is used to label emotions of all the segments, and then the log linear model is used to"
Y16-2013,W02-1011,0,0.0281353,"Missing"
Y16-2013,D09-1150,0,0.183341,"to predict the values in the two axis (Wu et al., 2013). When using discrete models, emotion prediction becomes a multi-class classiﬁcation problem, which is the most commonly used methods in literature. This two representation methods are well compared in (Calvo and Mac Kim, 2013). One of the problems that hinder emotion analysis is the lack of training data. Annotated emotion corpus is relative scarce compared to other NLP tasks. Manually labeled emotion corpora include SemEval 2007 for news headlines (Strapparava and Mihalcea, 2007), RenCECps for blogs in both sentence and document levels (Quan and Ren, 2009), and NLP&CC 2013 for Chinese microblogs. Because of the rapid development of social networks, many studies also try to automatically construct emotion corpus from the web using reader added emotion tags as labels. Lin crawled a news article corpus based on the emotion related tags by readers from Yahoo!s news (Lin et al., 2007). Hashtags, emoticons, and emoji characters are also used as naturally annotated labels to construct large emotion corpus from social media (Bandhakavi et al., 2014; Mohammad et al., 2013; Wang et al., 2012). However, these naturally annotated labels often contain noise"
Y16-2013,P14-2070,0,0.0434082,"Missing"
Y16-2013,S07-1013,0,0.194921,"ation. When using dimension based models, emotion prediction becomes a regression problem to predict the values in the two axis (Wu et al., 2013). When using discrete models, emotion prediction becomes a multi-class classiﬁcation problem, which is the most commonly used methods in literature. This two representation methods are well compared in (Calvo and Mac Kim, 2013). One of the problems that hinder emotion analysis is the lack of training data. Annotated emotion corpus is relative scarce compared to other NLP tasks. Manually labeled emotion corpora include SemEval 2007 for news headlines (Strapparava and Mihalcea, 2007), RenCECps for blogs in both sentence and document levels (Quan and Ren, 2009), and NLP&CC 2013 for Chinese microblogs. Because of the rapid development of social networks, many studies also try to automatically construct emotion corpus from the web using reader added emotion tags as labels. Lin crawled a news article corpus based on the emotion related tags by readers from Yahoo!s news (Lin et al., 2007). Hashtags, emoticons, and emoji characters are also used as naturally annotated labels to construct large emotion corpus from social media (Bandhakavi et al., 2014; Mohammad et al., 2013; Wan"
Y16-2013,strapparava-valitutti-2004-wordnet,0,0.258374,"Missing"
Y16-2013,E14-4025,0,0.0170562,"the data mining technique class sequential rules (CSR) mining is used to analyze the emotion of the whole microblog containing several sentences. Based on psychological studies that events can trigger emotions (Cacioppo and Gardner, 1999), many studies propose event based emotion prediction methods. Tokuhisa extracts events that can trigger emotions from the web based on emotion words and uses k-NN to predict new text for dialogs (Tokuhisa et al., 2008). Extending from Tokuhisa (2008), Vu constructs an event corpus by ﬁrst deﬁning a set of seed events and then extends it using boot-strapping (Vu et al., 2014). Lee builds an emotion linked event corpus from Chinese stories (Lee et al., 2014). Li proposes a system to detect and extract the cause event in microblogs, and uses these events as features to train a classiﬁer for emotion prediction in microblogs (Li et al., 2014). 3 Corpus Construction In order to serve the objective of our work for event based emotion prediction on news articles, we need to ﬁrst prepare an appropriate training data which is currently not available. With consideration of resources, we choose to use the crowsourcing platform to annotate the data. 3.1 Data Source The raw da"
Y16-2013,W14-6809,0,0.0396705,"ures and tries to apply them to different classiﬁers such as SVM, Naive Bayes, and decision trees for sentence level blog emotion classiﬁcation (Quan and Ren, 2009). Based on word embedding, Chen uses a sentence vector in combination with ML-KNN for microblog data (Chen et al., 2014). Inspired by Poria (2014) that uses dependency features and CRF model, a segment-based method is proposed to extract sentence segments using dependency trees and the semi-CRF model is used to label emotions of all the segments, and then the log linear model is used to infer the ﬁnal emotion of the whole sentence (Wang, 2014). Similar idea is adopted by Wen (2014) where the data mining technique class sequential rules (CSR) mining is used to analyze the emotion of the whole microblog containing several sentences. Based on psychological studies that events can trigger emotions (Cacioppo and Gardner, 1999), many studies propose event based emotion prediction methods. Tokuhisa extracts events that can trigger emotions from the web based on emotion words and uses k-NN to predict new text for dialogs (Tokuhisa et al., 2008). Extending from Tokuhisa (2008), Vu constructs an event corpus by ﬁrst deﬁning a set of seed eve"
Y18-2004,D17-1048,1,0.792398,"are represented by a dense vector learnt from embedding models such as word2vec or Doc2Vec. One of the ”linguistic” knowledge that can potentially improve the accuracy of sentiment analysis are sentiment lexicons or more complex emotion lexicons. They are integrated into the machine learning pipeline, e.g. (S.K.Rastogi et al., 2014). Sentiment lexicons are important resources for sentiment analysis. These lexicons consist of predetermined list of words assigned to sentiment labels or values, which is baseline for many machine learning based methods(Liu and Zhang, 2012; Tabak and Evrim, 2016; Long et al., 2017). Depending on sentiment models, there are two mainstream labeling schemas. The first schema is representing affective meanings of words by discrete sentiment labels, such as as positive, negative, etc (Ekman et al., 1983). The second schema is to represent affective meanings by means of more comprehensive multi-dimensional representation models, like the valence-arousal dominance model (VAD) (Russell, 1980) and the evaluationpotency-activity model (EPA) (Heise, 1965). Sentiment lexicons are heavily investigated in English (Gilbert, 2014; Li et al., 2017) and Chinese (Wang and Ku, 2016). Howev"
Y18-2004,N13-1090,0,0.00452476,"nlabeled documentsreviews (the reviews are not labeled). After preprocessing the reviews (cleaning, tokenization), we trained the model on the openrice data using Doc2Vec implementation. We choose the vector size of 100, so that each review is encoded into the array of this size. After training the model, we substituted the reviews in the corpus with the respective vector according to the model and thus the text data became numerical and could had served as input for standard classifiers. The side-effect of the trained model is its capability to represent relations between words or sentences (Mikolov et al., 2013). We tested on the following 3 The scripts with a small sample of data are available at: https://github.com/polyu-llt/openrice_ annotations Figure 2: Lexical relations obtained from the model using the function most similar 3.2 Preliminary Experiments In our preliminary experiments, we trained three classifiers using sklearn python library: • Logistic regression (logreg) • Support Vector Machines (SVM) • k-Nearest Neighbours We conducted the following experiments; results are summarized in Table 3: • Multi-class classification. For this, we used all the reviews that were ranked on the scale fr"
Y18-2004,L16-1428,0,0.0271889,"2016; Long et al., 2017). Depending on sentiment models, there are two mainstream labeling schemas. The first schema is representing affective meanings of words by discrete sentiment labels, such as as positive, negative, etc (Ekman et al., 1983). The second schema is to represent affective meanings by means of more comprehensive multi-dimensional representation models, like the valence-arousal dominance model (VAD) (Russell, 1980) and the evaluationpotency-activity model (EPA) (Heise, 1965). Sentiment lexicons are heavily investigated in English (Gilbert, 2014; Li et al., 2017) and Chinese (Wang and Ku, 2016). However, building sentiment lexicon for low-resources languages or dialects is not an easy task. That is why we decided to collect and investigate the text in Cantonese. Cantonese is a variety of Chinese spoken in the city of Guangzhou (historically known as Canton) and its surrounding area in southeastern China, including HK SAR and Macao SAR 1 . Compared to the studies on Man1 https://en.wikipedia.org/wiki/Cantonese 882 32nd Pacific Asia Conference on Language, Information and Computation The 25th Joint Workshop on Linguistics and Language Processing Hong Kong, 1-3 December 2018 Copyright"
yang-etal-2008-chinese-term,W01-0513,0,\N,Missing
yang-etal-2008-chinese-term,C00-2116,0,\N,Missing
yang-etal-2008-chinese-term,P07-2018,0,\N,Missing
yang-etal-2008-chinese-term,J04-1004,0,\N,Missing
zhang-etal-2008-exploiting,N07-1015,0,\N,Missing
zhang-etal-2008-exploiting,P05-1053,0,\N,Missing
zhang-etal-2008-exploiting,P06-2060,0,\N,Missing
zhang-etal-2008-exploiting,P04-1054,0,\N,Missing
zhang-etal-2008-exploiting,N06-1038,0,\N,Missing
zhang-etal-2008-exploiting,D07-1076,0,\N,Missing
zhang-etal-2008-exploiting,H05-1091,0,\N,Missing
zhang-etal-2008-exploiting,P06-1104,0,\N,Missing
