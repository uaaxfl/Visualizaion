2021.iwslt-1.9,The {N}iu{T}rans End-to-End Speech Translation System for {IWSLT} 2021 Offline Task,2021,-1,-1,7,1,5747,chen xu,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"This paper describes the submission of the NiuTrans end-to-end speech translation system for the IWSLT 2021 offline task, which translates from the English audio to German text directly without intermediate transcription. We use the Transformer-based model architecture and enhance it by Conformer, relative position encoding, and stacked acoustic and textual encoding. To augment the training data, the English transcriptions are translated to German translations. Finally, we employ ensemble decoding to integrate the predictions from several models trained with the different datasets. Combining these techniques, we achieve 33.84 BLEU points on the MuST-C En-De test set, which shows the enormous potential of the end-to-end model."
2021.findings-emnlp.357,Bag of Tricks for Optimizing Transformer Efficiency,2021,-1,-1,4,1,7297,ye lin,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Improving Transformer efficiency has become increasingly attractive recently. A wide range of methods has been proposed, e.g., pruning, quantization, new architectures and etc. But these methods are either sophisticated in implementation or dependent on hardware. In this paper, we show that the efficiency of Transformer can be improved by combining some simple and hardware-agnostic methods, including tuning hyper-parameters, better design choices and training strategies. On the WMT news translation tasks, we improve the inference efficiency of a strong Transformer system by 3.80x on CPU and 2.52x on GPU."
2021.emnlp-main.191,{R}ank{NAS}: Efficient Neural Architecture Search by Pairwise Ranking,2021,-1,-1,7,1,9034,chi hu,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"This paper addresses the efficiency challenge of Neural Architecture Search (NAS) by formulating the task as a ranking problem. Previous methods require numerous training examples to estimate the accurate performance of architectures, although the actual goal is to find the distinction between {``}good{''} and {``}bad{''} candidates. Here we do not resort to performance predictors. Instead, we propose a performance ranking method (RankNAS) via pairwise ranking. It enables efficient architecture search using much fewer training examples. Moreover, we develop an architecture selection method to prune the search space and concentrate on more promising candidates. Extensive experiments on machine translation and language modeling tasks show that RankNAS can design high-performance architectures while being orders of magnitude faster than state-of-the-art NAS systems."
2021.ccl-1.30,å©ç¨å¾åæè¿°ä¸ç¥è¯å¾è°±å¢å¼ºè¡¨ç¤ºçè§è§é®ç­(Exploiting Image Captions and External Knowledge as Representation Enhancement for Visual Question Answering),2021,-1,-1,6,0,11744,gechao wang,Proceedings of the 20th Chinese National Conference on Computational Linguistics,0,"{``}è§è§é®ç­ä½ä¸ºå¤æ¨¡æä»»å¡,éè¦æ·±åº¦çè§£å¾ååææ¬é®é¢ä»èæ¨çåºç­æ¡ãç¶èå¨è®¸å¤æ
åµä¸,ä»
å¨å¾ååé®é¢ä¸è¿è¡ç®åæ¨çé¾ä»¥å¾å°æ­£ç¡®çç­æ¡,äºå®ä¸è¿æå
¶å®ææçä¿¡æ¯å¯ä»¥è¢«å©ç¨,ä¾å¦å¾åæè¿°ãå¤é¨ç¥è¯ç­ãéå¯¹ä»¥ä¸é®é¢,æ¬ææåºäºå©ç¨å¾åæè¿°åå¤é¨ç¥è¯å¢å¼ºè¡¨ç¤ºçè§è§é®ç­æ¨¡åãè¯¥æ¨¡åä»¥é®é¢ä¸ºå¯¼å,åºäºååæ³¨æåæºå¶åå«å¨å¾ååå
¶æè¿°ä¸è¿è¡ç¼ç ,å¹¶ä¸å©ç¨ç¥è¯å¾è°±åµå
¥,å°å¤é¨ç¥è¯ç¼ç å°æ¨¡åå½ä¸­,ä¸°å¯äºæ¨¡åçç¹å¾è¡¨ç¤º,å¢å¼ºæ¨¡åçæ¨çè½åãå¨OKVQAæ°æ®éä¸çå®éªç»æè¡¨ææ¬ææ¹æ³ç¸æ¯åºçº¿ç³»ç»æ1.71{\%}çåç¡®çæå,ä¸å
åå·¥ä½ä¸­çä¸»æµæ¨¡åç¸æ¯ä¹æ1.88{\%}çåç¡®çæå,è¯æäºæ¬ææ¹æ³çæææ§ã{''}"
2021.acl-long.162,Weight Distillation: Transferring the Knowledge in Neural Network Parameters,2021,-1,-1,7,1,7297,ye lin,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Knowledge distillation has been proven to be effective in model acceleration and compression. It transfers knowledge from a large neural network to a small one by using the large neural network predictions as targets of the small neural network. But this way ignores the knowledge inside the large neural networks, e.g., parameters. Our preliminary study as well as the recent success in pre-training suggests that transferring parameters are more effective in distilling knowledge. In this paper, we propose Weight Distillation to transfer the knowledge in parameters of a large neural network to a small neural network through a parameter generator. On the WMT16 En-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks, our experiments show that weight distillation learns a small network that is 1.88 2.94x faster than the large network but with competitive BLEU performance. When fixing the size of small networks, weight distillation outperforms knowledge distillation by 0.51 1.82 BLEU points."
2021.acl-long.204,Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders,2021,-1,-1,8,1,5747,chen xu,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Encoder pre-training is promising in end-to-end Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find that ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for speech translation. Our encoder begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence. In this way, it is straightforward to incorporate the pre-trained models into the system. Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge. Experimental results on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our method achieves state-of-the-art BLEU scores of 18.3 and 25.2. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available."
2020.wmt-1.37,The {N}iu{T}rans Machine Translation Systems for {WMT}20,2020,-1,-1,18,0,4467,yuhao zhang,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes NiuTrans neural machine translation systems of the WMT20 news translation tasks. We participated in Japanese{\textless}-{\textgreater}English, English-{\textgreater}Chinese, Inuktitut-{\textgreater}English and Tamil-{\textgreater}English total five tasks and rank first in Japanese{\textless}-{\textgreater}English both sides. We mainly utilized iterative back-translation, different depth and widen model architectures, iterative knowledge distillation and iterative fine-tuning. And we find that adequately widened and deepened the model simultaneously, the performance will significantly improve. Also, iterative fine-tuning strategy we implemented is effective during adapting domain. For Inuktitut-{\textgreater}English and Tamil-{\textgreater}English tasks, we built multilingual models separately and employed pretraining word embedding to obtain better performance."
2020.wmt-1.117,The {N}iu{T}rans System for the {WMT}20 Quality Estimation Shared Task,2020,-1,-1,12,1,9034,chi hu,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes the submissions of the NiuTrans Team to the WMT 2020 Quality Estimation Shared Task. We participated in all tasks and all language pairs. We explored the combination of transfer learning, multi-task learning and model ensemble. Results on multiple tasks show that deep transformer machine translation models and multilingual pretraining methods significantly improve translation quality estimation performance. Our system achieved remarkable results in multiple level tasks, e.g., our submissions obtained the best results on all tracks in the sentence-level Direct Assessment task."
2020.ngt-1.24,The {N}iu{T}rans System for {WNGT} 2020 Efficiency Task,2020,-1,-1,8,1,9034,chi hu,Proceedings of the Fourth Workshop on Neural Generation and Translation,0,"This paper describes the submissions of the NiuTrans Team to the WNGT 2020 Efficiency Shared Task. We focus on the efficient implementation of deep Transformer models (Wang et al., 2019; Li et al., 2019) using NiuTensor, a flexible toolkit for NLP tasks. We explored the combination of deep encoder and shallow decoder in Transformer models via model compression and knowledge distillation. The neural machine translation decoding also benefits from FP16 inference, attention caching, dynamic batching, and batch pruning. Our systems achieve promising results in both translation quality and efficiency, e.g., our fastest system can translate more than 40,000 tokens per second with an RTX 2080 Ti while maintaining 42.9 BLEU on newstest2018."
2020.findings-emnlp.385,Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation,2020,-1,-1,3,1,19917,qiang wang,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditions on different terminal devices (e.g., mobile phones) may vary greatly. Such individual training leads to increased model maintenance costs and slower model iterations, especially for the industry. In this work, we propose to use multi-task learning to train a flexible depth model that can adapt to different depth configurations during inference. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training method{---}{---}LayerDrop."
2020.emnlp-main.72,Shallow-to-Deep Training for Neural Machine Translation,2020,-1,-1,8,1,12925,bei li,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT{'}16 English-German and WMT{'}14 English-French translation tasks show that it is 1:4 faster than training from scratch, and achieves a BLEU score of 30:33 and 43:29 on two tasks. The code is publicly available at \url{https://github.com/libeineu/SDT-Training}."
2020.coling-main.352,Dynamic Curriculum Learning for Low-Resource Neural Machine Translation,2020,-1,-1,9,1,5747,chen xu,Proceedings of the 28th International Conference on Computational Linguistics,0,"Large amounts of data has made neural machine translation (NMT) a big success in recent years. But it is still a challenge if we train these models on small-scale corpora. In this case, the way of using data appears to be more important. Here, we investigate the effective use of training data for low-resource NMT. In particular, we propose a dynamic curriculum learning (DCL) method to reorder training samples in training. Unlike previous work, we do not use a static scoring function for reordering. Instead, the order of training samples is dynamically determined in two ways - loss decline and model competence. This eases training by highlighting easy samples that the current model has enough competence to learn. We test our DCL method in a Transformer-based system. Experimental results show that DCL outperforms several strong baselines on three low-resource machine translation benchmarks and different sized data of WMT{'}16 En-De."
2020.coling-main.377,Layer-Wise Multi-View Learning for Neural Machine Translation,2020,-1,-1,5,1,19917,qiang wang,Proceedings of the 28th International Conference on Computational Linguistics,0,"Traditional neural machine translation is limited to the topmost encoder layer{'}s context representation and cannot directly perceive the lower encoder layers. Existing solutions usually rely on the adjustment of network architecture, making the calculation more complicated or introducing additional structural restrictions. In this work, we propose layer-wise multi-view learning to solve this problem, circumventing the necessity to change the model structure. We regard each encoder layer{'}s off-the-shelf output, a by-product in layer-by-layer encoding, as the redundant view for the input sentence. In this way, in addition to the topmost encoder layer (referred to as the primary view), we also incorporate an intermediate encoder layer as the auxiliary view. We feed the two views to a partially shared decoder to maintain independent prediction. Consistency regularization based on KL divergence is used to encourage the two views to learn from each other. Extensive experimental results on five translation tasks show that our approach yields stable improvements over multiple strong baselines. As another bonus, our method is agnostic to network architectures and can maintain the same inference speed as the original model."
2020.coling-main.526,A Simple and Effective Approach to Robust Unsupervised Bilingual Dictionary Induction,2020,-1,-1,8,1,7298,yanyang li,Proceedings of the 28th International Conference on Computational Linguistics,0,"Unsupervised Bilingual Dictionary Induction methods based on the initialization and the self-learning have achieved great success in similar language pairs, e.g., English-Spanish. But they still fail and have an accuracy of 0{\%} in many distant language pairs, e.g., English-Japanese. In this work, we show that this failure results from the gap between the actual initialization performance and the minimum initialization performance for the self-learning to succeed. We propose Iterative Dimension Reduction to bridge this gap. Our experiments show that this simple method does not hamper the performance of similar language pairs and achieves an accuracy of 13.64 55.53{\%} between English and four distant languages, i.e., Chinese, Japanese, Vietnamese and Thai."
2020.acl-main.322,Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation,2020,22,0,6,1,12925,bei li,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"In encoder-decoder neural models, multiple encoders are in general used to represent the contextual information in addition to the individual sentence. In this paper, we investigate multi-encoder approaches in document-level neural machine translation (NMT). Surprisingly, we find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator. This makes us rethink the real benefits of multi-encoder in context-aware translation - some of the improvements come from robust training. We compare several methods that introduce noise and/or well-tuned dropout setup into the training of these encoders. Experimental results show that noisy training plays an important role in multi-encoder-based NMT, especially when the training data is small. Also, we establish a new state-of-the-art on IWSLT Fr-En task by careful use of noise generation and dropout methods."
2020.acl-main.592,Learning Architectures from an Extended Search Space for Language Modeling,2020,-1,-1,7,1,9038,yinqiao li,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Neural architecture search (NAS) has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell. In this paper, we extend the search space of NAS. In particular, we present a general approach to learn both intra-cell and inter-cell architectures (call it ESS). For a better search result, we design a joint learning method to perform intra-cell and inter-cell NAS simultaneously. We implement our model in a differentiable architecture search system. For recurrent neural language modeling, it outperforms a strong baseline significantly on the PTB and WikiText data, with a new state-of-the-art on PTB. Moreover, the learned architectures show good transferability to other systems. E.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity recognition (NER) tasks and CoNLL chunking task, indicating a promising line of research on large-scale pre-learned architectures."
W19-5325,The {N}iu{T}rans Machine Translation Systems for {WMT}19,2019,0,1,17,1,12925,bei li,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"This paper described NiuTrans neural machine translation systems for the WMT 2019 news translation tasks. We participated in 13 translation directions, including 11 supervised tasks, namely ENâ{ZH, DE, RU, KK, LT}, GUâEN and the unsupervised DEâCS sub-track. Our systems were built on Deep Transformer and several back-translation methods. Iterative knowledge distillation and ensemble+reranking were also employed to obtain stronger models. Our unsupervised submissions were based on NMT enhanced by SMT. As a result, we achieved the highest BLEU scores in {KKâEN, GUâEN} directions, ranking 2nd in {RUâEN, DEâCS} and 3rd in {ZHâEN, LTâEN, ENâRU, ENâDE} among all constrained submissions."
P19-1176,Learning Deep Transformer Models for Machine Translation,2019,36,11,4,1,19917,qiang wang,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de facto standard for development of the Transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. Here, we continue the line of research on the latter. We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. On WMT{'}16 English-German and NIST OpenMT{'}12 Chinese-English tasks, our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base baseline (6-layer encoder) by 0.4-2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big."
P19-1352,Shared-Private Bilingual Word Embeddings for Neural Machine Translation,2019,35,1,6,0,7025,xuebo liu,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Word embedding is central to neural machine translation (NMT), which has attracted intensive research interest in recent years. In NMT, the source embedding plays the role of the entrance while the target embedding acts as the terminal. These layers occupy most of the model parameters for representation learning. Furthermore, they indirectly interface via a soft-attention mechanism, which makes them comparatively isolated. In this paper, we propose shared-private bilingual word embeddings, which give a closer relationship between the source and target embeddings, and which also reduce the number of model parameters. For similar source and target words, their embeddings tend to share a part of the features and they cooperatively learn these common representation units. Experiments on 5 language pairs belonging to 6 different language families and written in 5 different alphabets demonstrate that the proposed model provides a significant performance boost over the strong baselines with dramatically fewer model parameters."
D19-1367,Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition,2019,0,3,5,0,9174,yufan jiang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"In this paper, we study differentiable neural architecture search (NAS) methods for natural language processing. In particular, we improve differentiable architecture search by removing the softmax-local constraint. Also, we apply differentiable NAS to named entity recognition (NER). It is the first time that differentiable NAS methods are adopted in NLP tasks other than language modeling. On both the PTB language modeling and CoNLL-2003 English NER data, our method outperforms strong baselines. It achieves a new state-of-the-art on the NER task."
Y18-1010,Detecting Free Translation in Parallel Corpora from Attention Scores,2018,0,0,3,0,15542,qi chen,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
W18-6430,The {N}iu{T}rans Machine Translation System for {WMT}18,2018,0,3,9,1,19917,qiang wang,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This paper describes the submission of the NiuTrans neural machine translation system for the WMT 2018 Chinese â English news translation tasks. Our baseline systems are based on the Transformer architecture. We further improve the translation performance 2.4-2.6 BLEU points from four aspects, including architectural improvements, diverse ensemble decoding, reranking, and post-processing. Among constrained submissions, we rank 2nd out of 16 submitted systems on Chinese â English task and 3rd out of 16 on English â Chinese task, respectively."
P18-2047,A Simple and Effective Approach to Coverage-Aware Neural Machine Translation,2018,0,3,6,1,7298,yanyang li,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We offer a simple and effective method to seek a better balance between model confidence and length preference for Neural Machine Translation (NMT). Unlike the popular length normalization and coverage models, our model does not require training nor reranking the limited n-best outputs. Moreover, it is robust to large beam sizes, which is not well studied in previous work. On the Chinese-English and English-German translation tasks, our approach yields +0.4 1.5 BLEU improvements over the state-of-the-art baselines."
C18-1255,Multi-layer Representation Fusion for Neural Machine Translation,2018,0,13,6,1,19917,qiang wang,Proceedings of the 27th International Conference on Computational Linguistics,0,"Neural machine translation systems require a number of stacked layers for deep models. But the prediction depends on the sentence representation of the top-most layer with no access to low-level representations. This makes it more difficult to train the model and poses a risk of information loss to prediction. In this paper, we propose a multi-layer representation fusion (MLRF) approach to fusing stacked layers. In particular, we design three fusion functions to learn a better representation from the stack. Experimental results show that our approach yields improvements of 0.92 and 0.56 BLEU points over the strong Transformer baseline on IWSLT German-English and NIST Chinese-English MT tasks respectively. The result is new state-of-the-art in German-English translation."
D17-1150,Towards Bidirectional Hierarchical Representations for Attention-based Neural Machine Translation,2017,15,3,5,0,4173,baosong yang,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes a hierarchical attentional neural translation model which focuses on enhancing source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder. To maximize the predictive likelihood of target words, a weighted variant of an attention mechanism is used to balance the attentive information between lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem. Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks."
P15-4025,{N}iu{P}arser: A {C}hinese Syntactic and Semantic Parsing Toolkit,2015,10,8,1,1,5752,jingbo zhu,Proceedings of {ACL}-{IJCNLP} 2015 System Demonstrations,0,"We present a new toolkit NiuParser for Chinese syntactic and semantic analysis. It can handle a wide range of Natural Language Processing (NLP) tasks in Chinese, including word segmentation, partof-speech tagging, named entity recognition, chunking, constituent parsing, dependency parsing, and semantic role labeling. The NiuParser system runs fast and shows state-of-the-art performance on several benchmarks. Moreover, it is very easy to use for both research and industrial purposes. Advanced features include the Software Development Kit (SDK) interfaces and a multi-thread implementation for system speed-up."
P14-2092,A Hybrid Approach to Skeleton-based Translation,2014,23,4,2,1,4608,tong xiao,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper we explicitly consider sentence skeleton information for Machine Translation (MT). The basic idea is that we translate the key elements of the input sentence using a skeleton translation model, and then cover the remain segments using a full translation model. We apply our approach to a state-of-the-art phrase-based system and demonstrate very promising BLEU improvements and TER reductions on the NIST Chinese-English MT evaluation data."
P14-2128,Punctuation Processing for Projective Dependency Parsing,2014,15,10,3,1,9696,ji ma,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Modern statistical dependency parsers as- sign lexical heads to punctuations as well as words. Punctuation parsing errors lead to low parsing accuracy on words. In this work, we propose an alternative approach to addressing punctuation in dependency parsing. Rather than assigning lexical heads to punctuations, we treat punctu- ations as properties of their neighbour- ing words, used as features to guide the parser to build the dependency graph. In- tegrating our method with an arc-standard parser yields a 93.06% unlabelled attach- ment score, which is the best accuracy by a single-model transition-based parser re- ported so far."
P14-1014,Tagging The Web: Building A Robust Web Tagger with Neural Network,2014,34,17,3,1,9696,ji ma,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper, we address the problem of web-domain POS tagging using a twophase approach. The first phase learns representations that capture regularities underlying web text. The representation is integrated as features into a neural network that serves as a scorer for an easy-first POS tagger. Parameters of the neural network are trained using guided learning in the second phase. Experiment on the SANCL 2012 shared task show that our approach achieves 93.15% average tagging accuracy, which is the best accuracy reported so far on this data set, higher than those given by ensembled syntactic parsers."
D14-1021,Syntactic {SMT} Using a Discriminative Text Generation Model,2014,36,5,4,0.210495,884,yue zhang,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"We study a novel architecture for syntactic SMT. In contrast to the dominant approach in the literature, the system does not rely on translation rules, but treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target languages. Target syntax features and bilingual translation features are trained consistently in a discriminative model. Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems."
C14-1195,Effective Incorporation of Source Syntax into Hierarchical Phrase-based Translation,2014,34,4,3,1,4608,tong xiao,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,In this paper we explicitly consider source language syntactic information in both rule extraction and decoding for hierarchical phrase-based translation. We obtain tree-to-string rules by the GHKM method and use them to complement Hiero-style rules. All these rules are then employed to decode new sentences with source language parse trees. We experiment with our approach in a state-of-the-art Chinese-English system and demonstrate 1.2 and 0.8 BLEU improvements on the NIST newswire and web evaluation data of MT08 and MT12.
P13-2020,Easy-First {POS} Tagging and Dependency Parsing with Beam Search,2013,16,11,2,1,9696,ji ma,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of xe2x80x9cearly-updatexe2x80x9d to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance."
P13-1043,Fast and Accurate Shift-Reduce Constituent Parsing,2013,32,82,5,1,11745,muhua zhu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser."
W12-6336,{NEU} Systems in {SIGHAN} Bakeoff 2012,2012,8,0,5,1,9696,ji ma,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"This paper describes the methods used for the parsing the Sinica Treebank for the bakeoff task of SigHan 2012. Based on the statistics of the training data and the experimental results, we show that the major difficulties in parsing the Sinica Treebank comes from both the data sparse problem caused by the fine-grained annotation and the tagging ambiguity."
P12-3004,{N}iu{T}rans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation,2012,19,49,2,1,4608,tong xiao,Proceedings of the {ACL} 2012 System Demonstrations,0,"We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntax-based models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning."
P12-2055,Learning Better Rule Extraction with Translation Span Alignment,2012,21,1,1,1,5752,jingbo zhu,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation.
C12-1106,Easy-First {C}hinese {POS} Tagging and Dependency Parsing,2012,16,10,3,1,9696,ji ma,Proceedings of {COLING} 2012,0,"The easy-first non-directional dependency parser has demonstrated its ad vantage over transition based dependency parsers which parse sentences from left to right. This work investigates easy-first method on Chinese POS tagging, dependency parsing and j oint tagging and dependency parsing. In particular, we generalize the easy-first dependency parsing algorithm to a general framework and apply this framework to Chinese POS tagging and dependency parsing. We then propose the first joint tagging and dependency parsing algorithm under the easy-first framework. We train the joint model with both supervised objective an d additional loss which only relates to one of the individual tasks (either tagging or parsing). In this way, we can bias the joint model towards the preferred task. Experimental results show that bo th the tagger and the parser achieve state-of-the-art accuracy and runs fast . And our joint model achieves tagging accuracy of 94.27 which is the best result reported so far."
C12-1194,Exploiting Lexical Dependencies from Large-Scale Data for Better Shift-Reduce Constituency Parsing,2012,32,6,2,1,11745,muhua zhu,Proceedings of {COLING} 2012,0,"This paper proposes a method to improve shift-reduce constituency parsing by using lexical dependencies. The lexical dependency information is obtained from a large amount of auto-parsed data that is generated by a baseline shift-reduce parser on unlabeled data. We then incorporate a set of novel features defined on this information into the shift-reduce parsing model. The features can help to disambiguate action conflicts during decoding. Experimental results show that the new features achieve absolute improvements over a strong baseline by 0.9% and 1.1% on English and Chinese respectively. Moreover, the improved parser outperforms all previously reported shift-reduce constituency parsers. Title and Abstract in Chinese xe5x88xa9xe7x94xa8xe5xa4xa7xe8xa7x84xe6xa8xa1xe6x95xb0xe6x8dxaexe8xafx8dxe6xb1x87xe4xbex9dxe5xadx98xe5x85xb3xe7xb3xbbxe6x94xb9xe8xbfx9bxe7xa7xbbxe8xbfx9b-xe5xbdx92xe7xbaxa6xe6x88x90xe5x88x86xe5x8fxa5xe6xb3x95xe5x88x86xe6x9ex90 xe6x9cxacxe6x96x87xe6x8fx90xe5x87xbaxe4xbax86xe4xb8x80xe7xa7x8dxe5x88xa9xe7x94xa8xe8xafx8dxe6xb1x87xe4xbex9dxe5xadx98xe5x85xb3xe7xb3xbbxe6x94xb9xe8xbfx9bxe7xa7xbbxe8xbfx9b-xe5xbdx92xe7xbaxa6xe6x88x90xe5x88x86xe5x8fxa5xe6xb3x95xe5x88x86xe6x9ex90xe7x9ax84xe6x96xb9xe6xb3x95xe3x80x82xe9xa6x96xe5x85x88,xe6x88x91xe4xbbxacxe5x88xa9xe7x94xa8 xe5x9fxbaxe5x87x86xe7xb3xbbxe7xbbx9fxe5x9cxa8xe5xa4xa7xe8xa7x84xe6xa8xa1xe6x97xa0xe6xa0x87xe6xb3xa8xe6x95xb0xe6x8dxaexe4xb8x8axe8xbfx9bxe8xa1x8cxe8x87xaaxe5x8axa8xe5x8fxa5xe6xb3x95xe5x88x86xe6x9ex90xe5xb9xb6xe4xbbx8exe5x88x86xe6x9ex90xe7xbbx93xe6x9ex9cxe4xb8xadxe6x8axbdxe5x8fx96xe8xafx8dxe6xb1x87xe4xbex9dxe5xadx98xe5x85xb3xe7xb3xbbxe3x80x82xe5x85xb6 xe5x90x8e,xe6x88x91xe4xbbxacxe5x9cxa8xe8xafx8dxe6xb1x87xe4xbex9dxe5xadx98xe4xbfxa1xe6x81xafxe7x9ax84xe5x9fxbaxe7xa1x80xe4xb8x8axe5xaex9axe4xb9x89xe4xbax86xe4xb8x80xe7xbbx84xe6x96xb0xe7x89xb9xe5xbex81xe5xb9xb6xe5xb0x86xe8xbfx99xe4xbax9bxe7x89xb9xe5xbex81xe6x95xb4xe5x90x88xe5x88xb0xe7xa7xbbxe8xbfx9b-xe5xbdx92xe7xbaxa6xe5x8fxa5xe6xb3x95 xe5x88x86xe6x9ex90xe6xa8xa1xe5x9ex8b xe4xb8xadxe3x80x82xe6x96xb0xe7x89xb9xe5xbex81xe7x94xa8xe4xbax8exe5xb8xaexe5x8axa9xe6xb6x88xe9x99xa4xe7xa7xbbxe8xbfx9b-xe5xbdx92xe7xbaxa6xe8xbfx87xe7xa8x8bxe4xb8xadxe7x9ax84xe5x8axa8xe4xbdx9cxe6xadxa7xe4xb9x89xe3x80x82xe5xaex9exe9xaax8cxe7xbbx93xe6x9ex9cxe8xa1xa8xe6x98x8e,xe6x96xb0xe7x89xb9xe5xbex81 xe5x9cxa8xe8x8bxb1xe6x96x87xe5x92x8cxe4xb8xadxe6x96x87xe6x95xb0xe6x8dxaexe4xb8x8axe5x88x86xe5x88xabxe5x8fx96xe5xbex97xe4xbax860.9% xe5x92x8c1.1%xe7x9ax84xe6x80xa7xe8x83xbdxe6x94xb9xe8xbfx9bxe3x80x82xe6x9cx80xe7xbbx88xe5xbex97xe5x88xb0xe7x9ax84xe5x8fxa5xe6xb3x95xe5x88x86xe6x9ex90xe5x99xa8xe7x9ax84xe6x80xa7xe8x83xbd xe4xbcx98xe4xbax8exe7x9bxb8xe5x85xb3xe7xa0x94xe7xa9xb6xe5xb7xa5xe4xbdx9cxe4xb8xadxe6x89x80xe6x8axa5xe5x91x8axe7x9ax84xe7xa7xbbxe8xbfx9b-xe5xbdx92xe7xbaxa6xe5x8fxa5xe6xb3x95xe5x88x86xe6x9ex90xe5x99xa8xe7x9ax84xe6x80xa7xe8x83xbdxe3x80x82"
P11-2073,Improving Decoding Generalization for Tree-to-String Translation,2011,21,2,1,1,5752,jingbo zhu,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"To address the parse error issue for tree-to-string translation, this paper proposes a similarity-based decoding generation (SDG) solution by reconstructing similar source parse trees for decoding at the decoding time instead of taking multiple source parse trees as input for decoding. Experiments on Chinese-English translation demonstrated that our approach can achieve a significant improvement over the standard method, and has little impact on decoding speed in practice. Our approach is very easy to implement, and can be applied to other paradigms such as tree-to-tree models."
P11-2126,Better Automatic Treebank Conversion Using A Feature-Based Approach,2011,20,5,2,1,11745,muhua zhu,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"For the task of automatic treebank conversion, this paper presents a feature-based approach which encodes bracketing structures in a tree-bank into features to guide the conversion of this treebank to a different standard. Experiments on two Chinese treebanks show that our approach improves conversion accuracy by 1.31% over a strong baseline."
2011.mtsummit-papers.13,Document-level Consistency Verification in Machine Translation,2011,-1,-1,2,1,4608,tong xiao,Proceedings of Machine Translation Summit XIII: Papers,0,None
W10-4110,Mining Large-scale Parallel Corpora from Multilingual Patents: An {E}nglish-{C}hinese example and its application to {SMT},2010,16,14,5,0.666667,44655,bin lu,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
W10-4135,High {OOV}-Recall {C}hinese Word Segmenter,2010,4,2,4,0,45171,xiaoming xu,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
W10-4143,{C}hinese Syntactic Parsing Evaluation,2010,14,6,2,0,22015,qiang zhou,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
W10-4153,A Multi-stage Clustering Framework for {C}hinese Personal Name Disambiguation,2010,4,0,6,0.784314,11746,huizhen wang,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
W10-4168,{NEUNLPL}ab {C}hinese Word Sense Induction System for {SIGHAN} Bakeoff 2010,2010,7,0,3,0,7671,hao zhang,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
P10-1076,Boosting-Based System Combination for Machine Translation,2010,33,9,2,1,4608,tong xiao,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrase-based system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems."
C10-2154,An Empirical Study of Translation Rule Extraction with Multiple Parsers,2010,21,3,2,1,4608,tong xiao,Coling 2010: Posters,0,"Translation rule extraction is an important issue in syntax-based Statistical Machine Translation (SMT). Recent studies show that rule coverage is one of the key factors affecting the success of syntax-based systems. In this paper, we first present a simple and effective method to improve rule coverage by using multiple parsers in translation rule extraction, and then empirically investigate the effectiveness of our method on Chinese-English translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora."
C10-2176,Automatic Treebank Conversion via Informed Decoding,2010,14,2,2,1,11745,muhua zhu,Coling 2010: Posters,0,"In this paper, we focus on the challenge of automatically converting a constituency treebank (source treebank) to fit the standard of another constituency treebank (target treebank). We formalize the conversion problem as an informed decoding procedure: information from original annotations in a source treebank is incorporated into the decoding phase of a parser trained on a target treebank during the parser assigning parse trees to sentences in the source treebank. Experiments on two Chinese treebanks show significant improvements in conversion accuracy over baseline systems, especially when training data used for building the parser is small in size."
C10-1151,Heterogeneous Parsing via Collaborative Decoding,2010,14,3,2,1,11745,muhua zhu,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"There often exist multiple corpora for the same natural language processing (NLP) tasks. However, such corpora are generally used independently due to distinctions in annotation standards. For the purpose of full use of readily available human annotations, it is significant to simultaneously utilize multiple corpora of different annotation standards. In this paper, we focus on the challenge of constituent syntactic parsing with treebanks of different annotations and propose a collaborative decoding (or co-decoding) approach to improve parsing accuracy by leveraging bracket structure consensus between multiple parsing decoders trained on individual treebanks. Experimental results show the effectiveness of the proposed approach, which outperforms state-of-the-art baselines, especially on long sentences."
W09-3532,{C}hinese-{E}nglish Organization Name Translation Based on Correlative Expansion,2009,19,7,4,0.833333,8981,feiliang ren,Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration ({NEWS} 2009),0,"This paper presents an approach to translating Chinese organization names into English based on correlative expansion. Firstly, some candidate translations are generated by using statistical translation method. And several correlative named entities for the input are retrieved from a correlative named entity list. Secondly, three kinds of expansion methods are used to generate some expanded queries. Finally, these queries are submitted to a search engine, and the refined translation results are mined and re-ranked by using the returned web pages. Experimental results show that this approach outperforms the compared system in overall translation accuracy."
D09-1038,Better Synchronous Binarization for Machine Translation,2009,17,10,4,1,4608,tong xiao,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Binarization of Synchronous Context Free Grammars (SCFG) is essential for achieving polynomial time complexity of decoding for SCFG parsing based machine translation systems. In this paper, we first investigate the excess edge competition issue caused by a left-heavy binary SCFG derived with the method of Zhang et al. (2006). Then we propose a new binarization method to mitigate the problem by exploring other alternative equivalent binary SCFGs. We present an algorithm that iteratively improves the resulting binary SCFG, and empirically show that our method can improve a string-to-tree statistical machine translations system based on the synchronous binarization method in Zhang et al. (2006) on the NIST machine translation evaluation tasks."
2009.mtsummit-wpt.3,The Construction of a {C}hinese-{E}nglish Patent Parallel Corpus,2009,20,8,3,0.666667,44655,bin lu,Proceedings of the Third Workshop on Patent Translation,0,"In this paper, we describe the construction of a parallel Chinese-English patent sentence corpus which is created from noisy parallel patents. First, we use a publicly available sentence aligner to find parallel sentence candidates in the noisy parallel data. Then we compare and evaluate three individual measures and different ensemble techniques to sort the parallel sentence candidates according to the confidence score and filter out those with low scores as the noisy data. The experiment shows that the combination of measures outperforms the individual measures, and that filtering out low-quality sentence pairs is readily justified as it can improve SMT performance. Finally, we arrive at the final corpus consisting of 160K sentence pairs in which about 90% are correct or partially correct alignments."
I08-4004,An Effective Hybrid Machine Learning Approach for Coreference Resolution,2008,14,3,2,0.833333,8981,feiliang ren,Proceedings of the Sixth {SIGHAN} Workshop on {C}hinese Language Processing,0,"We present a hybrid machine learning approach for coreference resolution. In our method, we use CRFs as basic training model, use active learning method to generate combined features so as to make existed features used more effectively; at last, we proposed a novel clustering algorithm which used both the linguistics knowledge and the statistical knowledge. We built a coreference resolution system based on the proposed method and evaluate its performance from three aspects: the contributions of active learning; the effects of different clustering algorithms; and the resolution performance of different kinds of NPs. Experimental results show that additional performance gain can be obtained by using active learning method; clustering algorithm has a great effect on coreference resolutionxe2x80x99s performance and our clustering algorithm is very effective; and the key of coreference resolution is to improve the performance of the normal nounxe2x80x99s resolution, especially the pronounxe2x80x99s resolution."
I08-4009,Which Performs Better on In-Vocabulary Word Segmentation: Based on Word or Character?,2008,11,1,3,1,48568,zhenxing wang,Proceedings of the Sixth {SIGHAN} Workshop on {C}hinese Language Processing,0,"Since the first Chinese Word Segmentation (CWS) Bakeoff on 2003, CWS has experienced a prominent flourish because Bakeoff provides a platform for the participants, which helps them recognize the merits and drawbacks of their segmenters. However, the evaluation metric of bakeoff is not sufficient enough to measure the performance thoroughly, sometimes even misleading. One typical example caused by this insufficiency is that there is a popular belief existing in the research field that segmentation based on word can yield a better result than character-based tagging (CT) on in-vocabulary (IV) word segmentation even within closed tests of Bakeoff. Many efforts were paid to balance the performance on IV and out-ofvocabulary (OOV) words by combining these two methods according to this belief. In this paper, we provide a more detailed evaluation metric of IV and OOV words than Bakeoff to analyze CT method and combination method, which is a typical way to seek such balance. Our evaluation metric shows that CT outperforms dictionary-based (or so called word-based in general) segmentation on both IV and OOV words within Bakeoff * The work is done when the first author is working in MSRA as an intern. closed tests. Furthermore, our analysis shows that using confidence measure to combine the two segmentation results should be under certain limitation."
I08-4015,The Character-based {CRF} Segmenter of {MSRA}{\\&}{NEU} for the 4th Bakeoff,2008,10,6,3,1,48568,zhenxing wang,Proceedings of the Sixth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper describes the Chinese Word Segmenter for the fourth International Chinese Language Processing Bakeoff. Base on Conditional Random Field (CRF) model, a basic segmenter is designed as a problem of character-based tagging. To further improve the performance of our segmenter, we employ a word-based approach to increase the in-vocabulary (IV) word recall and a post-processing to increase the out-of-vocabulary (OOV) word recall. We participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora."
I08-2124,Towards Automated Semantic Analysis on Biomedical Research Articles,2008,24,3,3,0,46907,donghui feng,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"In this paper, we present an empirical study on adapting Conditional Random Fields (CRF) models to conduct semantic analysis on biomedical articles using active learning. We explore uncertaintybased active learning with the CRF model to dynamically select the most informative training examples. This abridges the power of the supervised methods and expensive human annotation cost."
I08-1048,Learning a Stopping Criterion for Active Learning for Word Sense Disambiguation and Text Classification,2008,19,32,1,1,5752,jingbo zhu,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"In this paper, we address the problem of knowing when to stop the process of active learning. We propose a new statistical learning approach, called minimum expected error strategy, to defining a stopping criterion through estimation of the classifierxe2x80x99s expected error on future unlabeled examples in the active learning process. In experiments on active learning for word sense disambiguation and text classification tasks, experimental results show that the new proposed stopping criterion can reduce approximately 50% human labeling costs in word sense disambiguation with degradation of 0.5% average accuracy, and approximately 90% costs in text classification with degradation of 2% average accuracy."
C08-1142,Multi-Criteria-Based Strategy to Stop Active Learning for Data Annotation,2008,21,33,1,1,5752,jingbo zhu,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"In this paper, we address the issue of deciding when to stop active learning for building a labeled training corpus. Firstly, this paper presents a new stopping criterion, classification-change, which considers the potential ability of each unlabeled example on changing decision boundaries. Secondly, a multi-criteria-based combination strategy is proposed to solve the problem of predefining an appropriate threshold for each confidence-based stopping criterion, such as max-confidence, min-error, and overall-uncertainty. Finally, we examine the effectiveness of these stopping criteria on uncertainty sampling and heterogeneous uncertainty sampling for active learning. Experimental results show that these stopping criteria work well on evaluation data sets, and the combination strategies outperform individual criteria."
C08-1143,Active Learning with Sampling by Uncertainty and Density for Word Sense Disambiguation and Text Classification,2008,21,87,1,1,5752,jingbo zhu,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper addresses two issues of active learning. Firstly, to solve a problem of uncertainty sampling that it often fails by selecting outliers, this paper presents a new selective sampling technique, sampling by uncertainty and density (SUD), in which a k-Nearest-Neighbor-based density measure is adopted to determine whether an unlabeled example is an outlier. Secondly, a technique of sampling by clustering (SBC) is applied to build a representative initial training data set for active learning. Finally, we implement a new algorithm of active learning with SUD and SBC techniques. The experimental results from three real-world data sets show that our method outperforms competing methods, particularly at the early stages of active learning."
D07-1082,Active Learning for Word Sense Disambiguation with Methods for Addressing the Class Imbalance Problem,2007,28,129,1,1,5752,jingbo zhu,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"In this paper, we analyze the effect of resampling techniques, including undersampling and over-sampling used in active learning for word sense disambiguation (WSD). Experimental results show that under-sampling causes negative effects on active learning, but over-sampling is a relatively good choice. To alleviate the withinclass imbalance problem of over-sampling, we propose a bootstrap-based oversampling (BootOS) method that works better than ordinary over-sampling in active learning for WSD. Finally, we investigate when to stop active learning, and adopt two strategies, max-confidence and min-error, as stopping conditions for active learning. According to experimental results, we suggest a prediction solution by considering max-confidence as the upper bound and min-error as the lower bound for stopping conditions."
W06-0141,Designing Special Post-Processing Rules for {SVM}-Based {C}hinese Word Segmentation,2006,4,6,5,1,11745,muhua zhu,Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing,0,"We participated in the Third International Chinese Word Segmentation Bakeoff. Specifically, we evaluated our Chinese word segmenter NEUCipSeg in the close track, on all four corpora, namely Academis Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSRA), and University of Pennsylvania/University of Colorado (UPENN). Based on Support Vector Machines (SVMs), a basic segmenter is designed regarding Chinese word segmentation as a problem of character-based tagging. Moreover, we proposed postprocessing rules specially taking into account the properties of results brought out by the basic segmenter. Our system achieved good ranks in all four corpora."
I05-3015,Some Studies on {C}hinese Domain Knowledge Dictionary and Its Application to Text Classification,2005,8,3,1,1,5752,jingbo zhu,Proceedings of the Fourth {SIGHAN} Workshop on {C}hinese Language Processing,0,"In this paper, we study some issues on Chinese domain knowledge dictionary and its application to text classification task. First a domain knowledge hierarchy description framework and our Chinese domain knowledge dictionary named NEUKD are introduced. Second, to alleviate the cost of construction of domain knowledge dictionary by hand, we use a boostrapping-based algorithm to learn new domain associated terms from a large amount of unlabeled data. Third, we propose two models (BOTW and BOF) which use domain knowledge as textual features for text categorization. But due to limitation of size of domain knowledge dictionary, we further study machine learning technique to solve the problem, and propose a BOL model which could be considered as the extended version of BOF model. Naive Bayes classifier based on BOW model is used as baseline system in the comparison experiments. Experimental results show that domain knowledge is very useful for text categorization, and BOL model performs better than other three models, including BOW, BOTW and BOF models."
I05-1026,Using Multiple Discriminant Analysis Approach for Linear Text Segmentation,2005,23,4,1,1,5752,jingbo zhu,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Research on linear text segmentation has been an on-going focus in NLP for the last decade, and it has great potential for a wide range of applications such as document summarization, information retrieval and text understanding. However, for linear text segmentation, there are two critical problems involving automatic boundary detection and automatic determination of the number of segments in a document. In this paper, we propose a new domain-independent statistical model for linear text segmentation. In our model, Multiple Discriminant Analysis (MDA) criterion function is used to achieve global optimization in finding the best segmentation by means of the largest word similarity within a segment and the smallest word similarity between segments. To alleviate the high computational complexity problem introduced by the model, genetic algorithms (GAs) are used. Comparative experimental results show that our method based on MDA criterion functions has achieved higher Pk measure (Beeferman) than that of the baseline system using TextTiling algorithm."
W02-1820,A Knowledge-based Approach to Text Classification,2002,10,8,1,1,5752,jingbo zhu,{COLING}-02: The First {SIGHAN} Workshop on {C}hinese Language Processing,0,The paper presents a simple and effective knowledge-based approach for the task of text classification. The approach uses topic identification algorithm named FIFA to text classification. In this paper the basic process of text classification task and FIFA algorithm are described in detail. At last some results of experiment and evaluations are discussed.
