2020.emnlp-main.651,Conversational Semantic Parsing for Dialog State Tracking,2020,-1,-1,3,0,20632,jianpeng cheng,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"We consider a new perspective on dialog state tracking (DST), the task of estimating a user{'}s goal through the course of a dialog. By formulating DST as a semantic parsing task over hierarchical representations, we can incorporate semantic compositionality, cross-domain knowledge sharing and co-reference. We present TreeDST, a dataset of 27k conversations annotated with tree-structured dialog states and system acts. We describe an encoder-decoder framework for DST with hierarchical representations, which leads to {\textasciitilde}20{\%} improvement over state-of-the-art DST approaches that operate on a flat meaning space of slot-value pairs."
W18-0523,"Grotoco@{SLAM}: Second Language Acquisition Modeling with Simple Features, Learners and Task-wise Models",2018,0,2,2,0.769231,23672,sigrid klerke,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We present our submission to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM). We focus on evaluating a range of features for the task, including user-derived measures, while examining how far we can get with a simple linear classifier. Our analysis reveals that errors differ per exercise format, which motivates our final and best-performing system: a task-wise (per exercise-format) model."
L18-1586,Automatic Annotation of Semantic Term Types in the Complete {ACL} {A}nthology Reference Corpus,2018,0,0,2,0,17028,annekathrin schumann,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1718,Cheating a Parser to Death: Data-driven Cross-Treebank Annotation Transfer,2018,0,0,4,0,167,djame seddah,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"We present an efficient and accurate method for transferring annotations between two different treebanks of the same language. This method led to the creation of a new instance of the French Treebank (Abeille et al., 2003), which follows the Universal Dependency annotation scheme and which was proposed to the participants of the CoNLL 2017 Universal Dependency parsing shared task (Zeman et al., 2017). Strong results from an evaluation on our gold standard (94.75% of LAS, 99.40% UAS on the test set) demonstrate the quality of this new annotated data set and validate our approach."
W17-6304,Improving neural tagging with lexical information,2017,0,4,2,0,250,benoit sagot,Proceedings of the 15th International Conference on Parsing Technologies,0,"Neural part-of-speech tagging has achieved competitive results with the incorporation of character-based and pre-trained word embeddings. In this paper, we show that a state-of-the-art bi-LSTM tagger can benefit from using information from morphosyntactic lexicons as additional input. The tagger, trained on several dozen languages, shows a consistent, average improvement when using lexical information, even when also using character-based embeddings, thus showing the complementarity of the different sources of lexical information. The improvements are particularly important for the smaller datasets."
W17-1725,Benchmarking Joint Lexical and Syntactic Analysis on Multiword-Rich Data,2017,8,0,2,0,23668,matthieu constant,Proceedings of the 13th Workshop on Multiword Expressions ({MWE} 2017),0,"This article evaluates the extension of a dependency parser that performs joint syntactic analysis and multiword expression identification. We show that, given sufficient training data, the parser benefits from explicit multiword information and improves overall labeled accuracy score in eight of the ten evaluation cases."
W17-0805,Annotating omission in statement pairs,2017,11,0,1,1,20634,hector alonso,Proceedings of the 11th Linguistic Annotation Workshop,0,"We focus on the identification of omission in statement pairs. We compare three annotation schemes, namely two different crowdsourcing schemes and manual expert annotation. We show that the simplest of the two crowdsourcing approaches yields a better annotation quality than the more complex one. We use a dedicated classifier to assess whether the annotators{'} behavior can be explained by straightforward linguistic features. The classifier benefits from a modeling that uses lexical information beyond length and overlap measures. However, for our task, we argue that expert and not crowdsourcing-based annotation is the best compromise between annotation cost and quality."
K17-3001,{C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to {U}niversal {D}ependencies,2017,28,32,35,0,5828,daniel zeman,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"The Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets. In 2017, the task was devoted to learning dependency parsers for a large number of languages, in a real-world setting without any gold-standard annotation on input. All test sets followed a unified annotation scheme, namely that of Universal Dependencies. In this paper, we define the task and evaluation methodology, describe how the data sets were prepared, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems."
E17-1005,When is multitask learning effective? Semantic sequence prediction under varying data conditions,2017,0,56,1,1,20634,hector alonso,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"Multitask learning has been applied successfully to a range of tasks, mostly morphosyntactic. However, little is known on \textit{when} MTL works and whether there are data characteristics that help to determine the success of MTL. In this paper we evaluate a range of semantic sequence labeling tasks in a MTL setup. We examine different auxiliary task configurations, amongst which a novel setup, and correlate their impact to data-dependent conditions. Our results show that MTL is not always effective, because significant improvements are obtained only for 1 out of 5 tasks. When successful, auxiliary tasks with compact and more uniform label distributions are preferable."
E17-1022,Parsing {U}niversal {D}ependencies without training,2017,15,3,1,1,20634,hector alonso,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"We present UDP, the first training-free parser for Universal Dependencies (UD). Our algorithm is based on PageRank and a small set of specific dependency head rules. UDP features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD. The parser has very few parameters and distinctly robust to domain change across languages."
W16-3905,From Noisy Questions to {M}inecraft Texts: Annotation Challenges in Extreme Syntax Scenario,2016,24,0,1,1,20634,hector alonso,Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT}),0,"User-generated content presents many challenges for its automatic processing. While many of them do come from out-of-vocabulary effects, others spawn from different linguistic phenomena such as unusual syntax. In this work we present a French three-domain data set made up of question headlines from a cooking forum, game chat logs and associated forums from two popular online games (MINECRAFT {\&} LEAGUE OF LEGENDS). We chose these domains because they encompass different degrees of lexical and syntactic compliance with canonical language. We conduct an automatic and manual evaluation of the difficulties of processing these domains for part-of-speech prediction, and introduce a pilot study to determine whether dependency analysis lends itself well to annotate these data. We also discuss the development cost of our data set."
W16-1801,Learning Paraphrasing for Multiword Expressions,2016,31,1,2,0,282,seid yimam,Proceedings of the 12th Workshop on Multiword Expressions,0,"In this paper, we investigate the impact of context for the paraphrase ranking task, comparing and quantifying results forn multi-word expressions and single words. We focus on systematic integration of existing paraphrase resources to producen paraphrase candidates and later ask human annotators to judge paraphrasability in context.n We first conduct a paraphrase-scoring annotation task with and without context for targets that are i) single- and multi-wordn expressions ii) verbs and nouns. We quantify how differently annotators score paraphrases when context information is provided.n Furthermore, we report on experiments with automatic paraphrase ranking. If we regard the problem as a binary classificationn task, we obtain an F1xe2x80x93score of 81.56% and 79.87% for multi-word expressions and single words resp. using kNN classifier. Approaching the problem as a learning-to-rank task, we attain MAP scores up to 87.14% and 91.58% for multiword expressions and single words resp. using LambdaMART, thus yielding highquality contextualized paraphrased selection. Further, we provide the first dataset with paraphrase judgments for multi-word targets in context."
W16-1706,Supersense tagging with inter-annotator disagreement,2016,18,3,1,1,20634,hector alonso,Proceedings of the 10th Linguistic Annotation Workshop held in conjunction with {ACL} 2016 ({LAW}-X 2016),0,"Linguistic annotation underlies many successful approaches in Natural Language Processing (NLP), where the annotated corpora are used for training and evaluating supervised learners. The consistency of annotation limits the performance of supervised models, and thus a lot of effort is put into obtaining high-agreement annotated datasets. Recent research has shown that annotation disagreement is not random noise, but carries a systematic signal that can be used for improving the supervised learner. However, prior work was limited in scope, focusing only on part-of-speech tagging in a single language. In this paper we broaden the experiments to a semantic task (supersense tagging) using multiple languages. In particular, we analyse how systematic disagreement is for sense annotation, and we present a preliminary study of whether patterns of disagreements transfer across languages."
S16-1160,{C}oastal{CPH} at {S}em{E}val-2016 Task 11: The importance of designing your Neural Networks right,2016,0,0,3,0,24953,joachim bingel,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
S16-1209,{MS}ejr{K}u at {S}em{E}val-2016 Task 14: Taxonomy Enrichment by Evidence Ranking,2016,15,2,2,0,8554,michael schlichtkrull,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"Automatic enrichment of semantic taxonomies with novel data is a relatively unexplored task with potential benefits in a broad array of natural language processing problems. Task 14 of SemEval 2016 poses the challenge of designing systems for this task. In this paper, we describe and evaluate several machine learning systems constructed for our participation in the competition. We demonstrate an f1-score of 0.680 for our submitted systems xe2x80x94 a small improvement over the 0.679 produced by the hard baseline."
Q16-1022,Multilingual Projection for Parsing Truly Low-Resource Languages,2016,28,38,4,0.275081,21438,vzeljko agic,Transactions of the Association for Computational Linguistics,0,"We propose a novel approach to cross-lingual part-of-speech tagging and dependency parsing for truly low-resource languages. Our annotation projection-based approach yields tagging and parsing models for over 100 languages. All that is needed are freely available parallel texts, and taggers and parsers for resource-rich languages. The empirical evaluation across 30 test languages shows that our method consistently provides top-level accuracies, close to established upper bounds, and outperforms several competitive baselines."
L16-1136,The {S}em{D}a{X} Corpus â Sense Annotations with Scalable Sense Inventories,2016,18,2,4,0,6194,bolette pedersen,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We launch the SemDaX corpus which is a recently completed Danish human-annotated corpus available through a CLARIN academic license. The corpus includes approx. 90,000 words, comprises six textual domains, and is annotated with sense inventories of different granularity. The aim of the developed corpus is twofold: i) to assess the reliability of the different sense annotation schemes for Danish measured by qualitative analyses and annotation agreement scores, and ii) to serve as training and test data for machine learning algorithms with the practical purpose of developing sense taggers for Danish. To these aims, we take a new approach to human-annotated corpus resources by double annotating a much larger part of the corpus than what is normally seen: for the all-words task we double annotated 60{\%} of the material and for the lexical sample task 100{\%}. We include in the corpus not only the adjucated files, but also the diverging annotations. In other words, we consider not all disagreement to be noise, but rather to contain valuable linguistic information that can help us improve our annotation schemes and our learning algorithms."
2016.jeptalnrecital-poster.5,Approximate unsupervised summary optimisation for selections of {ROUGE},2016,-1,-1,2,0.404531,10219,natalie schluter,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters),0,"Approximate summary optimisation for selections of ROUGE It is standard to measure automatic summariser performance using the ROUGE metric. Unfortunately, ROUGE is not appropriate for unsupervised summarisation approaches. On the other hand, we show that it is possible to optimise approximately for ROUGE-n by using a document-weighted ROUGE objective. Doing so results in state-of-the-art summariser performance for single and multiple document summaries for both English and French. This is despite a non-correlation of the documentweighted ROUGE metric with human judgments, unlike the original ROUGE metric. These findings suggest a theoretical approximation link between the two metrics."
2016.gwc-1.30,An empirically grounded expansion of the supersense inventory,2016,0,3,1,1,20634,hector alonso,Proceedings of the 8th Global WordNet Conference (GWC),0,"In this article we present an expansion of the supersense inventory. All new super-senses are extensions of members of the current inventory, which we postulate by identifying semantically coherent groups of synsets. We cover the expansion of the already-established supernsense inventory for nouns and verbs, the addition of coarse supersenses for adjectives in absence of a canonical supersense inventory, and super-senses for verbal satellites. We evaluate the viability of the new senses examining the annotation agreement, frequency and co-ocurrence patterns."
W15-2711,Predicting word sense annotation agreement,2015,20,2,1,1,20634,hector alonso,"Proceedings of the First Workshop on Linking Computational Models of Lexical, Sentential and Discourse-level Semantics",0,"High agreement is a common objective when annotating data for word senses. However, a number of factors make perfect agreement impossible, e.g. the limitations of sense inventories, the difficulty of the examples or the interpretation preferences of the annotators. Estimating potential agreement is thus a relevant task to supplement the evaluation of sense annotations. In this article we propose two methods to predict agreement on wordannotation instances. We experiment with a continuous representation and a threeway discretization of observed agreement. In spite of the difficulty of the task, we find that different levels of agreement can be identifiedxe2x80x94in particular, low-agreement examples are easier to identify."
W15-2005,Coarse-grained sense annotation of {D}anish across textual domains,2015,10,2,3,0,6195,sussi olsen,Proceedings of the workshop on Semantic resources and semantic annotation for Natural Language Processing and the Digital Humanities at {NODALIDA} 2015,0,"We present the results of a coarse-grained sense annotation task on verbs, nouns and adjectives across six textual domains in Danish. We present the domain-wise differences in intercoder agreement and discuss how the applicability and validity of the sense inventory vary depending on domain. We find that domain-wise agreement is not higher in very canonical or edited text. In fact, newswire text and parliament speeches have lower agreement than blogs and chats, probably because the language of these text types is more complex and uses more abstract concepts. We further observe that domains differ in their sense distribution. For instance, newswire and magazines stand out as having a high focus on persons, and discussion fora typically include a restricted number of senses dependent on specialized topics. We anticipate that these findings can be exploited in automatic sense tagging when dealing with domain shift."
W15-1806,Supersense tagging for {D}anish,2015,13,11,1,1,20634,hector alonso,Proceedings of the 20th Nordic Conference of Computational Linguistics ({NODALIDA} 2015),0,"We describe the creation of a new Danish resource for automated coarse-grained word sense disambiguation of running text (supersense tagging, SST). Based on corpus evidence we expand the sense inventory to incorporate new lexical classes. We add tags for verbal satellites like collocates, particles and reflexive pronouns, to give account for the satellite-framing properties of Danish. Finally, we evaluate the quality of our expanded sense inventory in terms of variation in F1 on a stateof-the-art SST system. The SST systems uses type constraints and achieves performance just under the upper bound of interannotator agreement. The initial release is a 1,500-sentence corpus covering six genres, made available under an open-source license. 1"
W15-1814,Looking hard: Eye tracking for detecting grammaticality of automatically compressed sentences,2015,21,3,2,0.769231,23672,sigrid klerke,Proceedings of the 20th Nordic Conference of Computational Linguistics ({NODALIDA} 2015),0,"Natural language processing (NLP) tools are often developed with the intention of easing human processing, a goal which is hard to measure. Eye movements in reading are known to reflect aspects of the cognitive processing of text (Rayner et al., 2013). We explore how eye movements reflect aspects of reading that are of relevance to NLP system evaluation and development. This becomes increasingly relevant as eye tracking is becoming available in consumer products. In this paper we present an analysis of the differences between reading automatic sentence compressions and manually simplified newswire using eye-tracking experiments and readersxe2x80x99 evaluations. We show that both manual simplification and automatic sentence compression provide texts that are easier to process than standard newswire, and that the main source of difficulty in processing machine-compressed text is ungrammaticality. Especially the proportion of regressions to previously read text is found to be sensitive to the differences in human- and computer-induced complexity. This finding is relevant for evaluation of automatic summarization, simplification and translation systems designed with the intention of facilitating human reading."
W15-1831,Active learning for sense annotation,2015,9,2,1,1,20634,hector alonso,Proceedings of the 20th Nordic Conference of Computational Linguistics ({NODALIDA} 2015),0,None
W15-1617,Non-canonical language is not harder to annotate than canonical language,2015,5,3,2,0.194729,106,barbara plank,Proceedings of The 9th Linguistic Annotation Workshop,0,"As researchers developing robust NLP for a wide range of text types, we are often confronted with the prejudice that annotation of non-canonical language (whatever that means) is somehow more arbitrary than annotation of canonical language. To investigate this, we present a small annotation study where annotators were asked, with minimal guidelines, to identify main predicates and arguments in sentences across five different domains, ranging from newswire to Twitter. Our study indicates that (at least such) annotation of non-canonical language is not harder. However, we also observe that agreements in social media domains correlate less with model confidence, suggesting that maybe annotators disagree for different reasons when annotating social media data."
S15-2118,{CPH}: Sentiment analysis of Figurative Language on {T}witter {\\#}easypeasy {\\#}not,2015,8,2,2,0,37281,sarah mcgillion,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes the details of our system submitted to the SemEval 2015 shared task on sentiment analysis of figurative language on Twitter. We tackle the problem as regression task and combine several base systems using stacked generalization (Wolpert, 1992). An initial analysis revealed that the data is heavily biased, and a general sentiment analysis system (GSA) performs poorly on it. However, GSA proved helpful on the test data, which contains an estimated 25% nonfigurative tweets. Our best system, a stacking system with backoff to GSA, ranked 4th on the final test data (Cosine 0.661, MSE 3.404). 1"
P15-1165,Inverted indexing for cross-lingual {NLP},2015,24,47,3,0.0102894,143,anders sogaard,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We present a novel, count-based approach to obtaining inter-lingual word representations based on inverted indexing of Wikipedia. We present experiments applying these representations to 17 datasets in document classification, POS tagging, dependency parsing, and word alignment. Our approach has the advantage that it is simple, computationally efficient and almost parameter-free, and, more importantly, it enables multi-source crosslingual learning. In 14/17 cases, we improve over using state-of-the-art bilingual embeddings."
N15-1135,Mining for unambiguous instances to adapt part-of-speech taggers to new domains,2015,24,7,3,0.131729,417,dirk hovy,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present a simple, yet effective approach to adapt part-of-speech (POS) taggers to new domains. Our approach only requires a dictionary and large amounts of unlabeled target data. The idea is to use the dictionary to mine the unlabeled target data for unambiguous word sequences, thus effectively collecting labeled target data. We add the mined instances to available labeled newswire data to train a POS tagger for the target domain. The induced models significantly improve tagging accuracy on held-out test sets across three domains (Twitter, spoken language, and search queries). We also present results for Dutch, Spanish and Portuguese Twitter data, and provide two novel manually-annotated test sets."
N15-1152,Learning to parse with {IAA}-weighted loss,2015,14,2,1,1,20634,hector alonso,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Natural language processing (NLP) annotation projects employ guidelines to maximize inter-annotator agreement (IAA), and models are estimated assuming that there is one single ground truth. However, not all disagreement is noise, and in fact some of it may contain valuable linguistic information. We integrate such information in the training of a cost-sensitive dependency parser. We introduce five different factorizations of IAA and the corresponding loss functions, and evaluate these across six different languages. We obtain robust improvements across the board using a factorization that considers dependency labels and directionality. The best method-dataset combination reaches an average overall error reduction of 6.4% in labeled attachment score."
K15-1033,Do dependency parsing metrics correlate with human judgments?,2015,20,2,2,0.194729,106,barbara plank,Proceedings of the Nineteenth Conference on Computational Natural Language Learning,0,"Using automatic measures such as labeled and unlabeled attachment scores is common practice in dependency parser evaluation. In this paper, we examine whether these measures correlate with human judgments of overall parse quality. We ask linguists with experience in dependency annotation to judge system outputs. We measure the correlation between their judgments and a range of parse evaluation metrics across five languages. The humanmetric correlation is lower for dependency parsing than for other NLP tasks. Also, inter-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech."
D15-1245,Any-language frame-semantic parsing,2015,10,9,2,0.78125,20640,anders johannsen,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We present a multilingual corpus of Wikipedia and Twitter texts annotated with FRAMENET 1.5 semantic frames in nine different languages, as well as a novel technique for weakly supervised cross-lingual frame-semantic parsing. Our approach only assumes the existence of linked, comparable source and target language corpora (e.g., Wikipedia) and a bilingual dictionary (e.g., Wiktionary or BABELNET). Our approach uses a truly interlingual representation, enabling us to use the same model across all nine languages. We present average error reductions over running a state-of-the-art parser on word-to-word translations of 46% for target identification, 37% for frame identification, and 14% for argument identification."
W14-1601,What{'}s in a p-value in {NLP}?,2014,39,17,5,0.0102894,143,anders sogaard,Proceedings of the Eighteenth Conference on Computational Natural Language Learning,0,"In NLP, we need to document that our proposed methods perform significantly better with respect to standard metrics than previous approaches, typically by reporting p-values obtained by rank- or randomization-based tests. We show that significance results following current research standards are unreliable and, in addition, very sensitive to sample size, covariates such as sentence length, as well as to the existence of multiple metrics. We estimate that under the assumption of perfect metrics and unbiased data, we need a significance cut-off at xe2x87xa00.0025 to reduce the risk of false positive results to <5%. Since in practice we often have considerable selection bias and poor metrics, this, however, will not do alone."
S14-2034,"Copenhagen-Malm{\\\o}: Tree Approximations of Semantic Parsing Problems""",2014,7,3,6,0.404531,10219,natalie schluter,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"In this shared task paper for SemEval2014 Task 8, we show that most semantic structures can be approximated by trees through a series of almost bijective graph transformations. We transform input graphs, apply off-the-shelf methods from syntactic parsing on the resulting trees, and retrieve output graphs. Using tree approximations, we obtain good results across three semantic formalisms, with a 15.9% error reduction over a stateof-the-art semantic role labeling system on development data. Our system came in 3/6 in the shared task closed track."
S14-1001,More or less supervised supersense tagging of {T}witter,2014,29,19,3,0.78125,20640,anders johannsen,Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*{SEM} 2014),0,"We present two Twitter datasets annotated with coarse-grained word senses (supersenses), as well as a series of experiments with three learning scenarios for supersense tagging: weakly supervised learning, as well as unsupervised and supervised domain adaptation. We show that (a) off-the-shelf tools perform poorly on Twitter, (b) models augmented with embeddings learned from Twitter data perform much better, and (c) errors can be reduced using type-constrained inference with distant supervision from WordNet."
alonso-romeo-2014-crowdsourcing,Crowdsourcing as a preprocessing for complex semantic annotation tasks,2014,19,1,1,1,20634,hector alonso,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This article outlines a methodology that uses crowdsourcing to reduce the workload of experts for complex semantic tasks. We split turker-annotated datasets into a high-agreement block, which is not modified, and a low-agreement block, which is re-annotated by experts. The resulting annotations have higher observed agreement. We identify different biases in the annotation for both turkers and experts."
W13-5411,Class-based Word Sense Induction for dot-type nominals,2013,18,1,2,0,39664,lauren romeo,Proceedings of the 6th International Conference on Generative Approaches to the Lexicon ({GL}2013),0,"Comunicacio presentada a: 6th International Conference on Generative Approaches to the Lexicon, celebrada a Pisa, Italia, del 24 al 25 de setembre del 2013."
P13-2127,Annotation of regular polysemy and underspecification,2013,17,11,1,1,20634,hector alonso,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Comunicacio presentada a: 51st Annual Meeting of the Association for Computational Linguistics, celebrat a Sofia, Bulgaria, del 4 al 9 d'agost de 2013."
N13-1070,Down-stream effects of tree-to-dependency conversions,2013,28,22,5,0.627305,38644,jakob elming,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Dependency analysis relies on morphosyntactic evidence, as well as semantic evidence. In some cases, however, morphosyntactic evidence seems to be in conflict with semantic evidence. For this reason dependency grammar theories, annotation guidelines and tree-to-dependency conversion schemes often differ in how they analyze various syntactic constructions. Most experiments for which constituent-based treebanks such as the Penn Treebank are converted into dependency treebanks rely blindly on one of four-five widely used tree-to-dependency conversion schemes. This paper evaluates the down-stream effect of choice of conversion scheme, showing that it has dramatic impact on end results."
alonso-etal-2012-voting,A voting scheme to detect semantic underspecification,2012,29,2,1,1,20634,hector alonso,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The following work describes a voting system to automatically classify the sense selection of the complex types Location/Organization and Container/Content, which depend on regular polysemy, as described by the Generative Lexicon (Pustejovsky, 1995) . This kind of sense alternations very often presents semantic underspecificacion between its two possible selected senses. This kind of underspecification is not traditionally contemplated in word sense disambiguation systems, as disambiguation systems are still coping with the need of a representation and recognition of underspecification (Pustejovsky, 2009) The data are characterized by the morphosyntactic and lexical enviroment of the headwords and provided as input for a classifier. The baseline decision tree classifier is compared against an eight-member voting scheme obtained from variants of the training data generated by modifications on the class representation and from two different classification algorithms, namely decision trees and k-nearest neighbors. The voting system improves the accuracy for the non-underspecified senses, but the underspecified sense remains difficult to identify"
W11-4604,Identification of sense selection in regular polysemy using shallow features,2011,17,2,1,1,20634,hector alonso,Proceedings of the 18th Nordic Conference of Computational Linguistics ({NODALIDA} 2011),0,"The following work describes a method to automatically classify the sense selection of the complex type Location/Organization xe2x80x93which depends on regular polysemyxe2x80x93 using shallow features, as well as a way to increase the volume of sense-selection gold standards by using monosemous data as filler. The classifier results show that grammatical features are the most relevant cues for the identification of sense selection in this instance of regular polysemy."
