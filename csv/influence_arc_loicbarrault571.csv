2009.iwslt-evaluation.10,2006.iwslt-evaluation.15,0,0.0321155,"and Dev3 corpora. The target language model was trained on the English side of the those corpora. No additional texts were used (constrained condition). We report results on Dev6 (development data) and Dev7 (internal test set). All BLEU scores are case-sensitive and include punctuations. For some systems, the Dev6 corpus was added to the training material after optimizing the system and the full system was retrained, keeping all settings unmodified. By these means we hope to lower the OOV rate on the official test set. This idea was already successfully proposed in previous IWSLT evaluations [1]. The statistical phrase-based system is based on the Moses SMT toolkit [2] and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted. Both steps use the default settings of the Moses SMT toolkit. A 4-gram back-off target language model (LM) is constructed on all available English data. The translation itself is performed in two passes: first, Moses is run and a 1000-best list is generated for each sentence. In our system fourteen features functions were used, namely phrase and lexical translation prob"
2009.iwslt-evaluation.10,P07-2045,0,0.00776614,"e of the those corpora. No additional texts were used (constrained condition). We report results on Dev6 (development data) and Dev7 (internal test set). All BLEU scores are case-sensitive and include punctuations. For some systems, the Dev6 corpus was added to the training material after optimizing the system and the full system was retrained, keeping all settings unmodified. By these means we hope to lower the OOV rate on the official test set. This idea was already successfully proposed in previous IWSLT evaluations [1]. The statistical phrase-based system is based on the Moses SMT toolkit [2] and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted. Both steps use the default settings of the Moses SMT toolkit. A 4-gram back-off target language model (LM) is constructed on all available English data. The translation itself is performed in two passes: first, Moses is run and a 1000-best list is generated for each sentence. In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion"
2009.iwslt-evaluation.10,W07-0732,0,0.0227542,"us 376k words En Btec human translations Dev1−3 Giza++ Phrase extraction SRILM CSLM Phrase table 4g LM 4g CSLM phrase table SMT system 4−gram LM Moses Ar/En Src Moses 1000 bests Trg Src LM rescoring phrase table Trg SYSTRAN decode optimized with MERT λi Moses En’/En Condor BLEU SPE system 2nd pass optimisation Figure 2: Comparison of SMT and SPE systems. Figure 1: Architecture of the SMT system. 3. SPE System In the last years, there is increasing interest in the interaction between rule-based and statistical machine translation. A popular and successful idea is statistical post editing (SPE) [6, 7]. The principle idea is to train an SMT system to correct the outputs of a rule-based translation system. This is shown in figure 2. The operation performed by the rulebased translation system could also be seen as a very good tokenization or preprocessing, that actually performs many of the translation steps. Therefore, the task of the SMT system itself is very simplified. Accordingly, we argue that an SMT and SPE system are only two extreme cases of the interaction between tokenization/preprocessing and translation itself. An interesting question is whether both systems can be combined since"
2009.iwslt-evaluation.10,J07-2003,0,0.131844,"Missing"
2009.iwslt-evaluation.10,W09-0424,0,0.0287796,"Missing"
2009.iwslt-evaluation.10,2008.iwslt-evaluation.6,0,0.0423786,"Missing"
2009.iwslt-evaluation.10,P07-1040,0,0.0322066,"he development set (Dev6) using the provided Z-MERT procedure. The grammar rules extraction tools and Z-MERT are provided in the Joshua toolkit. Figure 3 summarizes the architecture of the Joshua translation system. The decoder is based on the token pass decoding algorithm. The scores used to evaluate the hypotheses are the following: 5. System combination • the system score : this replaces the score of the translation model. Until now, the words given by all systems 1 have the same probability which is . M The system combination approach is based on confusion network decoding as described in [11, 12] and shown in Figure 4. The protocol can be decomposed into three steps : • the language model (LM) probability. The 4-gram LM used for the combination is the same than the one used by each single system. 1. 1-best hypotheses from all M systems are aligned and confusion networks are built. It is obvious that this combination framework is not optimal, but as we can see in the results section, this simple architecture can already achieve improvements when combining only two systems. 2. All confusion networks are connected into a single lattice. 6. Experimental Evaluation 3. A 4-gram language mod"
2009.iwslt-evaluation.10,2006.amta-papers.25,0,0.0173726,"ter optimize our hierarchical systems built with Joshua. Rescoring the n-best lists with the continuous space LM achieved an improvement of 1.2 BLEU on the internal test set for the Arabic/English SMT system, and 0.6 BLEU for the SPE system. Due to time constraints, the continuous space LM was not applied on the hierarchical system. The improvements obtained by the CSLM are generally smaller 5.1. Hypotheses alignment and confusion network generation For each segment, the best hypotheses of M − 1 systems are aligned against the last one used as backbone. The alignment is done with the TER tool [13], without any tuning performed at this step (default edit costs are used). M confusion networks are generated in this way. Then all the confusion networks are connected into a single lattice by adding a first and last node. The probability of the first arcs must reflect how well such system provides a well structured hypothesis (good order). In our experiments, no tuning was done at this step, and we chose equal prior probabilities for all systems. - 67 - Proceedings of IWSLT 2009, Tokyo - Japan Approach: Train bitexts Arabic/English: Btec+Dev123 Btec+Dev1236 LM SMT Moses Dev Test Hierarchical"
2009.iwslt-evaluation.10,W07-0728,0,\N,Missing
2009.iwslt-evaluation.10,2008.iwslt-evaluation.10,0,\N,Missing
2010.iwslt-evaluation.14,P07-2045,0,0.00435578,"each using only a subset of corpora, optimizing them on the development corpus and comparing the resulting BLEU scores. The Table 3 shows the BLEU scores obtained on different corpus combinations. Regarding these experiments, the best combination we could determine was to select the TED corpus, along with the news-commentary and the Europarl ones, for a total amount of data of about forty-seven millions of tokens. corpus combination All corpora TED + NC + Eparl TED + NC + UN200x All except Gigaword Parallel corpus Giza++ Our statistical phrase-based systems are based on the Moses SMT toolkit [4] and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted. Both steps use the default settings of the Moses SMT toolkit. In our systems, fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model. The coefficients of these feature functions are tuned on the development corpus with the cMERT tool using 100best lists. The Figure 1 shows the basi"
2010.iwslt-evaluation.14,P08-1115,0,0.0234514,"ench in ASR condition as a full-fledged language, consequently our recaser can be regarded as a “French ASR-to-French” SMT system. This system was then optimized on the CRR development corpus, which is nothing more than the ASR manual transcription, except that it contains case and punctuation. The Figure 2 presents the global architecture of our SMT system for ASR condition. 3. Translating ASR outputs 3.1. Handling ASR word lattices For this evaluation campaign, word lattices, n-best lists and 1-best hypotheses were available. The use of word lattices as input of Moses system is described in [5]. Alternatively, we decided to generate another kind of input, namely confusion networks [6], computed from the word lattices. The word lattices were provided by the organizers under SLF format, which is the file format used by the HTK tools. In practice, word lattices provided by the organizers were very large, too large to be reasonably managed “as is” by the Moses decoder. This large size can be mainly explained because the word lattice topology strictly represents the history constraints applied to words in these word lattices, making their language model scores consistent with their histo"
2010.iwslt-evaluation.14,2005.iwslt-1.19,0,\N,Missing
2010.iwslt-evaluation.14,W09-0424,0,\N,Missing
2010.iwslt-evaluation.14,P05-1056,0,\N,Missing
2010.iwslt-evaluation.14,2009.iwslt-evaluation.5,0,\N,Missing
2010.iwslt-evaluation.14,2005.eamt-1.19,0,\N,Missing
2010.iwslt-evaluation.14,zhang-etal-2004-interpreting,0,\N,Missing
2010.iwslt-evaluation.14,2010.iwslt-papers.5,0,\N,Missing
2012.amta-papers.21,D11-1033,0,0.0501955,"M on the achieved by extending equation 2 as follows: source (or target) side of the bitexts, independently C for each corpus. There is a well known EM proceX ˜ wc Countc (˜ si , tij ) dure to linearly interpolate these individual LMs c=1 to minimize the perplexity on some development P (t˜ij |˜ si ) = C (3) X X data. The resulting corpus coefficients can be diwc Countc (˜ si , t˜ik ) rectly used to weight the parallel corpora. c=1 k Perplexity can also be used to weight each individual sentence. This was used to select a releThe equation 3 is identical as given in (Matvant subset of LM data (Axelrod et al., 2011) or soukas et al., 2009), where wc represents the bitexts (Moore and Lewis, 2010). In our case, we features-mapped to a weight calculated for each build a LM on the source side of the in-domain sentence by neural network. However in our case corpus and use this model to calculate the perplexit represents the direct weight for each corpus. If ity of each sentence in all the other corpora. Since all corpus weights are identical, equation 3 simlower perplexity represents “better” sentences, we plifies to the original formulation in equation 2. set q(si , ti ) to the inverse of the perplexity. It"
2012.amta-papers.21,W11-2138,0,0.0201059,"domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. The feature vecto"
2012.amta-papers.21,P08-2040,0,0.0322483,"f the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discriminative function on a g"
2012.amta-papers.21,W07-0722,0,0.0195778,"e genre of the data may be also different, for instance, scientific text is translated with the models trained mainly on news data. Similarly, the quality of the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation qualit"
2012.amta-papers.21,W07-0717,0,0.225857,"for instance, scientific text is translated with the models trained mainly on news data. Similarly, the quality of the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a techn"
2012.amta-papers.21,D10-1044,0,0.0928776,"scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. The feature vectors were mapped to scalar weights (0, 1) which are then used to estimate probability with weighted counts. Foster et al. (2010) proposed an extended approach by an instant weighting scheme which learns weights on individual phrase pairs instead of sentences and incorporated the instanceweighting model into a linear combination. Phillips and Brown (2011) trained the models with a second-order Taylor approximation of weighted translation instances and discount models on the basis of this approximation. Zhao et al. (2004b) rescore phrase translation pairs for statistical machine translation using tf.idf to encode the weights in phrase translation pairs. The translation probability is then modeled by similarity functions"
2012.amta-papers.21,2010.amta-papers.21,0,0.0305351,"the goodness scores at the important to note that our approach is a generalizasentence level, we will get: tion of data selection approaches: instead of doing a hard decision which data to keep to discard, we ( ) C S X Y keep all the sentences and attach a weight to each s wc Countc (˜ si , t˜ij ) · hγc,s (˜ si , t˜ij ) one (this weight could be zero in an extreme case). c=1 s=1 P (t˜ij |˜ si ) = C ( ) S X X Y It was also observed that parallel sentences s wc Countc (˜ si , t˜ik ) · hγc,s (˜ si , t˜ik ) which are closer to the test set period are more imc=1 s=1 k (4) portant than older ones (Hardt and Elming, 2010; where γs is an additional parameter to weight Levenberg et al., 2010; Shah et al., 2011), in parthe different sentence goodness scores among each ticular when translating texts in the news domain. other. We implemented phrase probability calculaFollowing (Shah et al., 2011), we use an exponention according to equation 4 in the the memscore tial decay function: tool of Moses. q(si , ti ) = e−α·∆t i (5) 2.3 Calculation of the corpus weights and where α is the decay factor and ∆t is the dissentence goodness scores cretized time distance (0 for most recent part, 1 Our theoretical framework and i"
2012.amta-papers.21,C10-1056,0,0.0151975,"ach by an instant weighting scheme which learns weights on individual phrase pairs instead of sentences and incorporated the instanceweighting model into a linear combination. Phillips and Brown (2011) trained the models with a second-order Taylor approximation of weighted translation instances and discount models on the basis of this approximation. Zhao et al. (2004b) rescore phrase translation pairs for statistical machine translation using tf.idf to encode the weights in phrase translation pairs. The translation probability is then modeled by similarity functions defined in a vector space. Huang and Xiang (2010) proposed a rescoring algorithm in which phrase pair features are combined with linear regression model and neural network to predict the quality score of the phrase translation pair. These phrase scores are used to boost good phrase translations and bad translations are discarded. Shah et al. (2010) proposed a technique to weight heterogeneous data by weighted resampling of the alignments. In an extended work, the same authors proposed to consider meta-weights for each part of the training data (Shah et al., 2011). The work proposed in this paper is an extension and generalization of several"
2012.amta-papers.21,W07-0733,0,0.0577815,"c text is translated with the models trained mainly on news data. Similarly, the quality of the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted"
2012.amta-papers.21,N03-1017,0,0.00993485,"urteen feature functions: the above mentioned four scores for the phrases, a phrase and word penalty, six scores for the lexicalized distortion model, a language model score and a distance based reordering model. The phrase-table itself is created by the following procedure 1. collect parallel training data 2. eventually discard sentence pairs that are too long or which have a large length difference 3. run Giza++ on this data in both directions (source-to-target and target-to-source) 4. use some heuristics to symmetrize the alignments in both directions, e.g. the so-called grow-diagonal-... (Koehn et al., 2003) and extract a list of phrases 5. calculate the lexical probabilities 6. calculate the phrase probabilities P (t˜|˜ s) and ˜ P (˜ s|t). 7. create the phrase table by merging the forward and backward probabilities In our approach we only modify the way how the phrase translations probabilities P (t˜|˜ s) and P (˜ s|t˜) are calculated. The goal is to increase the probability of phrase pairs which we believe to be more important for the considered task, to be more reliable, etc; and consequently, to down weight those which should be used less often. It is important to point out that our phrase ta"
2012.amta-papers.21,P07-2045,0,0.00815239,"odness scores file. Then, phrases are extracted and the goodness score q(si , ti ) is synchronized with the phrases. In the case that one phrase occurs in multiple sentences (this actually happens quite often), we use the arithmetic mean of the goodness scores in our experiments. The maximum or some other interpolation functions could En tokens 2.0 2.8 50.6 232.5 287.9 Fr tokens 2.2 3.3 56.2 272.6 334.3 Table 1: Size of parallel corpora (in millions) to build baseline systems for WMT and IWSLT Tasks. 3 Experimental evaluation We have built several phrase-based systems using the Moses toolkit (Koehn et al., 2007). The scoring framework is implemented by extending the memory based scoring tool called memscore (Hardmeier, 2010) available in the Moses toolkit. In our system, fourteen feature functions are used. These feature functions include phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and phrase penalty, and the target language model. The MERT tool (Och, 2003) is used to tune the coefficients of these feature functions. The experiments are performed on two well-known evaluation tasks i.e. the 2011 WMT and IWSLT English/Fren"
2012.amta-papers.21,W11-2132,1,0.860059,"Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the gi"
2012.amta-papers.21,N10-1062,0,0.0172727,"eneralizasentence level, we will get: tion of data selection approaches: instead of doing a hard decision which data to keep to discard, we ( ) C S X Y keep all the sentences and attach a weight to each s wc Countc (˜ si , t˜ij ) · hγc,s (˜ si , t˜ij ) one (this weight could be zero in an extreme case). c=1 s=1 P (t˜ij |˜ si ) = C ( ) S X X Y It was also observed that parallel sentences s wc Countc (˜ si , t˜ik ) · hγc,s (˜ si , t˜ik ) which are closer to the test set period are more imc=1 s=1 k (4) portant than older ones (Hardt and Elming, 2010; where γs is an additional parameter to weight Levenberg et al., 2010; Shah et al., 2011), in parthe different sentence goodness scores among each ticular when translating texts in the news domain. other. We implemented phrase probability calculaFollowing (Shah et al., 2011), we use an exponention according to equation 4 in the the memscore tial decay function: tool of Moses. q(si , ti ) = e−α·∆t i (5) 2.3 Calculation of the corpus weights and where α is the decay factor and ∆t is the dissentence goodness scores cretized time distance (0 for most recent part, 1 Our theoretical framework and implementation is generic and does not depend on the exact calculation"
2012.amta-papers.21,D09-1074,0,0.38334,"hao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. The feature vectors were mapped to scalar weights (0, 1) which are then used to estimate probability with weighted counts. Foster et al. (2010) proposed an extended approach by an instant weighting scheme which learns weights on individual phrase pairs instead of sentences and incorporated the instanceweighting model into a linear combination. Phillips a"
2012.amta-papers.21,P10-2041,0,0.0432961,"the bitexts, independently C for each corpus. There is a well known EM proceX ˜ wc Countc (˜ si , tij ) dure to linearly interpolate these individual LMs c=1 to minimize the perplexity on some development P (t˜ij |˜ si ) = C (3) X X data. The resulting corpus coefficients can be diwc Countc (˜ si , t˜ik ) rectly used to weight the parallel corpora. c=1 k Perplexity can also be used to weight each individual sentence. This was used to select a releThe equation 3 is identical as given in (Matvant subset of LM data (Axelrod et al., 2011) or soukas et al., 2009), where wc represents the bitexts (Moore and Lewis, 2010). In our case, we features-mapped to a weight calculated for each build a LM on the source side of the in-domain sentence by neural network. However in our case corpus and use this model to calculate the perplexit represents the direct weight for each corpus. If ity of each sentence in all the other corpora. Since all corpus weights are identical, equation 3 simlower perplexity represents “better” sentences, we plifies to the original formulation in equation 2. set q(si , ti ) to the inverse of the perplexity. It is Considering in addition the goodness scores at the important to note that our"
2012.amta-papers.21,P03-1021,0,0.0201975,"in millions) to build baseline systems for WMT and IWSLT Tasks. 3 Experimental evaluation We have built several phrase-based systems using the Moses toolkit (Koehn et al., 2007). The scoring framework is implemented by extending the memory based scoring tool called memscore (Hardmeier, 2010) available in the Moses toolkit. In our system, fourteen feature functions are used. These feature functions include phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and phrase penalty, and the target language model. The MERT tool (Och, 2003) is used to tune the coefficients of these feature functions. The experiments are performed on two well-known evaluation tasks i.e. the 2011 WMT and IWSLT English/French evaluations. The corpora and their sizes used to build the systems for both these tasks are given in table 1. 3.1 Experiments on the WMT task For the WMT task we used the official development sets of the 2011 WMT translation tasks, i.e news-test09 as development corpus and newstest10 as test corpus. We built English-French systems by using the time-stamped Europarl and 1 with CONDOR (Berghen and Bersini, 2005) WMT Task Baselin"
2012.amta-papers.21,2011.mtsummit-papers.2,0,0.0141079,"al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. The feature vectors were mapped to scalar weights (0, 1) which are then used to estimate probability with weighted counts. Foster et al. (2010) proposed an extended approach by an instant weighting scheme which learns weights on individual phrase pairs instead of sentences and incorporated the instanceweighting model into a linear combination. Phillips and Brown (2011) trained the models with a second-order Taylor approximation of weighted translation instances and discount models on the basis of this approximation. Zhao et al. (2004b) rescore phrase translation pairs for statistical machine translation using tf.idf to encode the weights in phrase translation pairs. The translation probability is then modeled by similarity functions defined in a vector space. Huang and Xiang (2010) proposed a rescoring algorithm in which phrase pair features are combined with linear regression model and neural network to predict the quality score of the phrase translation p"
2012.amta-papers.21,2008.iwslt-papers.6,1,0.85596,"sus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discriminative function on a given tuning set. Sentence level features were extracted"
2012.amta-papers.21,W10-1759,1,0.927706,"unt models on the basis of this approximation. Zhao et al. (2004b) rescore phrase translation pairs for statistical machine translation using tf.idf to encode the weights in phrase translation pairs. The translation probability is then modeled by similarity functions defined in a vector space. Huang and Xiang (2010) proposed a rescoring algorithm in which phrase pair features are combined with linear regression model and neural network to predict the quality score of the phrase translation pair. These phrase scores are used to boost good phrase translations and bad translations are discarded. Shah et al. (2010) proposed a technique to weight heterogeneous data by weighted resampling of the alignments. In an extended work, the same authors proposed to consider meta-weights for each part of the training data (Shah et al., 2011). The work proposed in this paper is an extension and generalization of several ideas proposed in previous works such as weighted counts with goodness scores. However our proposed framework gives the flexibility to inject the goodness scores in a unified formulation calculated at various levels. It is based on the following principles: • the use of a set of “quality measures” at"
2012.amta-papers.21,I11-1148,1,0.909749,"bability is then modeled by similarity functions defined in a vector space. Huang and Xiang (2010) proposed a rescoring algorithm in which phrase pair features are combined with linear regression model and neural network to predict the quality score of the phrase translation pair. These phrase scores are used to boost good phrase translations and bad translations are discarded. Shah et al. (2010) proposed a technique to weight heterogeneous data by weighted resampling of the alignments. In an extended work, the same authors proposed to consider meta-weights for each part of the training data (Shah et al., 2011). The work proposed in this paper is an extension and generalization of several ideas proposed in previous works such as weighted counts with goodness scores. However our proposed framework gives the flexibility to inject the goodness scores in a unified formulation calculated at various levels. It is based on the following principles: • the use of a set of “quality measures” at different levels: weights for each corpus (or data source) and for each individual sentence in the bitexts. • no additional feature functions to express the quality or appropriateness of certain phrase pairs, but we mo"
2012.amta-papers.21,2006.iwslt-papers.3,0,0.0463553,"data. Similarly, the quality of the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimi"
2012.amta-papers.21,P07-1004,0,0.0253161,", the quality of the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discrimina"
2012.amta-papers.21,C04-1059,0,0.23709,"be also different, for instance, scientific text is translated with the models trained mainly on news data. Similarly, the quality of the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al."
2012.amta-papers.21,W04-3227,0,0.19264,"be also different, for instance, scientific text is translated with the models trained mainly on news data. Similarly, the quality of the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al."
2012.iwslt-papers.6,W10-2407,0,0.186067,"Missing"
2012.iwslt-papers.6,P06-1142,0,0.0732457,"Missing"
2012.iwslt-papers.6,W11-3208,0,0.139735,"Missing"
2012.iwslt-papers.6,D11-1128,0,0.150284,"Missing"
2012.iwslt-papers.6,P12-1049,0,0.111056,"Missing"
2012.iwslt-papers.6,W09-3504,0,0.198118,"Missing"
2012.iwslt-papers.6,P11-1044,0,0.116572,"Missing"
2012.iwslt-papers.6,N03-1033,0,0.0297077,"literation system on TPs and evaluate the system performance which is correlated with the transliteration mining quality.                        3.1. TM algorithm for parallel corpora The algorithm as shown in Figure 1 is designed to compare two aligned words and detect the words which are transliteration of each other, with respect to the observations in section 3.3. We developed the following TM algorithm: (1) First, the parallel corpus is tagged using a part-ofspeech (POS) tagger. We used Stanford POS tagger [11] for English and Mada/Tokan [12] for Arabic POS tagging. (2) Then, we align the tagged bitext using Giza++ [13], using the source/target alignment ﬁle, remove all aligned word pairs with POS tags other than noun (NN) or proper noun (PNN) tags and remove all English words starting with lower-case letters. Words which have most lowest align     Figure 2: Calculating the three levels of similarity scores As shown in Figure 2, we developed a three normalization functions which can be used to normalize the Arabic transliterated word and English word to be more comparable to each other"
2012.iwslt-papers.6,J03-1002,0,0.00569818,"g quality.                        3.1. TM algorithm for parallel corpora The algorithm as shown in Figure 1 is designed to compare two aligned words and detect the words which are transliteration of each other, with respect to the observations in section 3.3. We developed the following TM algorithm: (1) First, the parallel corpus is tagged using a part-ofspeech (POS) tagger. We used Stanford POS tagger [11] for English and Mada/Tokan [12] for Arabic POS tagging. (2) Then, we align the tagged bitext using Giza++ [13], using the source/target alignment ﬁle, remove all aligned word pairs with POS tags other than noun (NN) or proper noun (PNN) tags and remove all English words starting with lower-case letters. Words which have most lowest align     Figure 2: Calculating the three levels of similarity scores As shown in Figure 2, we developed a three normalization functions which can be used to normalize the Arabic transliterated word and English word to be more comparable to each other phonically. These normalized forms are used to 187 The 9th International Workshop on Spoken Language Translati"
2012.iwslt-papers.6,P07-2045,0,0.00388265,"2,3 T LSi = Levenshtein(N ormi (At ), N ormi (E)) |N ormi (E)| (1) In this formula, Levenshtein function is the edit distance between the two words, which is the number of singlecharacter edits required to change the ﬁrst word into the second one. 3.3. Customized English pronunciation similarity comparison for Arabic-English transliteration Our TM algorithm is based on the following pronunciation (and hence transliteration) observations in the English language considering the transliteration task from Arabic language characteristics: The transliteration system is built using the moses toolkit [14]. We train a letter-based SMT system on the list of TPs extracted using our TM algorithm explained in section 3.1. The distortion limit is set to 0 to disable any reordering. The transliteration system should be able to learn the proper letter mapping using the alignment of the letters, and hence be able to generate the possible transliterations of a name written in the source language script using the learned mapping rules into a name written in the target language script. This research focuses on the following points: • Evaluate the performance of TM the algorithm by using the TPs to build a"
2012.iwslt-papers.6,P05-1045,0,0.00376052,"didate in the candidate list produced by a transliteration system. • F-Score= Fuzziness in Top-1. The mean F-score measures how different, on average, the top transliteration candidate is from its closest reference. • MRR=Mean Reciprocal Rank measures traditional MRR for any right answer produced by the system, among the candidates. • M APref tightly measures the precision in the n-best candidates for the i-th source name, for which reference transliterations are available. (using only XIN, AFP and NYT parts) by extracting a list of proper names using the Stanford name entity recognizer (NER) [16]. The second resource (LM2) is the English part of the extracted TPs. The Table 1 below compares the results of using LM1 vs. LM2. These results show that the target part (i.e. LM2) of the extracted TPs gives better ACC score while it has some impact on the mean F-score. We decided to use LM2 in all other experiments that measure other variables. System LM1 LM2 ACC 0.43750 0.44159 Mean F-Score 0.88160 0.87860 MRR 0.54787 0.54862 M APref 0.43750 0.44160 Table 1: LM1 vs. LM2 3.5.4. Three levels similarity scores thresholds selections Several systems were trained to evaluate the best thresholds t"
2012.iwslt-papers.6,W02-0505,0,\N,Missing
2012.iwslt-papers.6,W12-4402,0,\N,Missing
2013.mtsummit-wptp.13,D11-1033,0,0.0228773,"o adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram LM. However, this is in general true only if"
2013.mtsummit-wptp.13,N09-2038,0,0.0242009,"the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is performed on the MT outputs. On standard machine translation tasks, Niehues and Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in impr"
2013.mtsummit-wptp.13,W12-3155,1,0.851259,"ently homogeneous, the language is sufficiently complex, and there is sufficient multilingual data available to train and tune MT models. The paper is organized as follows. Section 2 lists some of the related works. Section 3 introduces methods used for project adaptation. Section 4 briefly describes the conduct of the field test. Section 5 and Section 6, respectively, introduce the set-up and results of experiments. Section 7 concludes the paper with a discussion on the overall results. 2 Related Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (e"
2013.mtsummit-wptp.13,2011.iwslt-evaluation.18,1,0.827144,"elated Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is pe"
2013.mtsummit-wptp.13,P11-2031,0,0.0146555,"Dntgt , n=0,1) or the concatenation of the source side of both the development and test set (D01src ); we name FGtgt and FGsrc the selected corpus and the models trained on it in the two cases. The table also provides the percentage of data selected, computed with respect to the target side. The optimal splitting was performed by minimizing the perplexity of the target side of the development set. 6 Experiments Lab test experiments have been performed on data sets described in Section 5. Performance are provided in terms of BLEU and TER, computed by means of the MultEval script implemented by Clark et al. (2011), and of GTM.4 For statistical significance, p-values were calculated via approximate randomization for adapted systems with respect to the baselines and are reported in Tables 5 and 6 whenever not larger than 0.10. The SMT systems have been built upon the open-source MT toolkit Moses (Koehn et al., 2007). The translation and the lexicalized reordering models are trained on the available parallel training data (Table 1); 5-gram LMs smoothed through the improved Kneser-Ney tech4 http://nlp.cs.nyu.edu/GTM nique (Chen and Goodman, 1999) are estimated on the target side via the IRSTLM toolkit (Fed"
2013.mtsummit-wptp.13,2012.amta-papers.22,1,0.534373,"MT suggestions for the second half of the document came from a system adapted to the text of the first day by means of one of the adaptation methods tested in our experiments (Section 6). Translators post-edited machine-generated translations for correcting mistakes and making them stylistically appropriate. The document was selected such that the size of its halves corresponds approximately to the daily productivity of professional translators, that is three to five thousand words. A report on the field test including an analysis of the productivity of translators has already been published (Federico et al., 2012). Moreover, we performed a preliminary measure of the performance of MT outputs versus the post-edition of each translator. In both cases, pretty large inter-translator differences were observed. Since the limited number of subjects would have led to scores with large variances, we decided to choose segments en→de up, called backoff, in which the indicator feature is discarded. Again, the backoff method proposed by Niehues and Waibel (2012) differs slightly in the way the scores of the phrase pairs stemming from the background phrase table are computed. Language model: As concerns the LM adapt"
2013.mtsummit-wptp.13,P02-1023,0,0.0361356,"Missing"
2013.mtsummit-wptp.13,W07-0733,0,0.0352986,"t-up and results of experiments. Section 7 concludes the paper with a discussion on the overall results. 2 Related Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performan"
2013.mtsummit-wptp.13,P07-2045,1,0.00690725,"plitting was performed by minimizing the perplexity of the target side of the development set. 6 Experiments Lab test experiments have been performed on data sets described in Section 5. Performance are provided in terms of BLEU and TER, computed by means of the MultEval script implemented by Clark et al. (2011), and of GTM.4 For statistical significance, p-values were calculated via approximate randomization for adapted systems with respect to the baselines and are reported in Tables 5 and 6 whenever not larger than 0.10. The SMT systems have been built upon the open-source MT toolkit Moses (Koehn et al., 2007). The translation and the lexicalized reordering models are trained on the available parallel training data (Table 1); 5-gram LMs smoothed through the improved Kneser-Ney tech4 http://nlp.cs.nyu.edu/GTM nique (Chen and Goodman, 1999) are estimated on the target side via the IRSTLM toolkit (Federico et al., 2008). The weights of the log-linear interpolation model have been optimized by means of the Margin Infused Relaxed Algorithm (MIRA) process (Hasler et al., 2011) provided within the Moses toolkit. Various models have been built by means of the methods described in Section 3. Here the list o"
2013.mtsummit-wptp.13,D12-1037,0,0.0226802,"and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is performed on the MT outputs. On standard machine transla"
2013.mtsummit-wptp.13,D09-1074,0,0.0233102,"Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram"
2013.mtsummit-wptp.13,P10-2041,0,0.0161319,"of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram LM. However, this is in general true only if the new data is enough relevant to the task at hand, a condition"
2013.mtsummit-wptp.13,W08-0320,0,0.0387515,"Missing"
2013.mtsummit-wptp.13,2012.amta-papers.19,0,0.127366,"rien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is performed on the MT outputs. On standard machine translation tasks, Niehues and Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et a"
2013.mtsummit-wptp.13,W12-3147,1,0.900227,"Missing"
2013.mtsummit-wptp.13,steinberger-etal-2006-jrc,0,0.0170217,"Missing"
2013.mtsummit-wptp.13,I08-2088,0,0.0276276,"on tasks, Niehues and Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistic"
2013.mtsummit-wptp.13,W07-0717,0,0.0387197,"a discussion on the overall results. 2 Related Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the M"
2013.mtsummit-wptp.13,D10-1044,0,0.0217311,"ifferent approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram LM. However, this is"
2014.iwslt-evaluation.14,rousseau-etal-2014-enhancing,1,0.817939,"ing (NLP) systems like the ones we are going to present here can often be enhanced using various methods, which can occur before, during or after the actual system processing. Among these, one of the most efficient pre-processing method is data selection, i.e. the fact to determine which data will be injected into the system we are going to build. For this campaign, many data selection processing was done, both in ASR and SLT tasks. 2.1. Selection for the ASR task 2.1.1. Acoustic models training data selection For our acoustic modeling we used as a primary source the TED-LIUM corpus release 2 [1], removing from it all talks recorded after December 31st, 2010. In order to strengthen this base, we first added data from the Euronews corpora [2] distributed by the campaign organizers and from the 1997 English Broadcast News Speech (HUB4) [3]. Then, from the MediaEval 2014 evaluation campaign Search and Hyperlinking Task data transcripts (BBC recordings from 2008 which were decoded by the LIUM) [4], we applied a threshold on our confidence measures to select the best possible segments for our system within a limit of 50 hours of speech. Table 1 summarizes the characteristics of the data in"
2014.iwslt-evaluation.14,gretter-2014-euronews,0,0.147438,"e actual system processing. Among these, one of the most efficient pre-processing method is data selection, i.e. the fact to determine which data will be injected into the system we are going to build. For this campaign, many data selection processing was done, both in ASR and SLT tasks. 2.1. Selection for the ASR task 2.1.1. Acoustic models training data selection For our acoustic modeling we used as a primary source the TED-LIUM corpus release 2 [1], removing from it all talks recorded after December 31st, 2010. In order to strengthen this base, we first added data from the Euronews corpora [2] distributed by the campaign organizers and from the 1997 English Broadcast News Speech (HUB4) [3]. Then, from the MediaEval 2014 evaluation campaign Search and Hyperlinking Task data transcripts (BBC recordings from 2008 which were decoded by the LIUM) [4], we applied a threshold on our confidence measures to select the best possible segments for our system within a limit of 50 hours of speech. Table 1 summarizes the characteristics of the data included in our ASR system acoustic models. Corpus TED-LIUM Euronews 1997 HUB4 MedialEval 14 Total Duration 130.1h 68.2h 75.0h 50.0h 323.3h Segments 6"
2014.iwslt-evaluation.14,W08-0509,0,0.0139487,"d. 4.1. Architecture of the LIUM SLT system The SMT system is based on the Moses toolkit [11]. The standard 14 feature functions were used (i.e phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, word and phrase penalty and target language model (LM) probability). In addition to these, an Operation Sequence Model (OSM) [12] have been trained and included in the system. 4.1.1. Translation model The translation models have been trained with the standard procedure. First, the bitexts are word aligned in both directions with GIZA++ [13]. Then the phrase pairs are extracted and the lexical and phrase probabilities are computed. The weights have been optimized with MERT using two versions of the development data. For some systems, the provided transcriptions were used, and for others, the outputs of our ASR system was used. This was performed for the sake of comparing the impact of ASR systems improvement (observed during the last few years). 4.1.2. Language modeling Table 3: Interpolation coefficients and perplexities for the bigram, trigram, quadrigram and CSLM language models used in the LIUM ASR system. The language model"
2014.iwslt-evaluation.14,P07-2045,0,0.0036987,"Missing"
2014.iwslt-evaluation.14,P11-1105,0,\N,Missing
2015.iwslt-evaluation.7,P11-1105,0,\N,Missing
2015.iwslt-evaluation.7,P07-2045,0,\N,Missing
2015.iwslt-evaluation.7,gretter-2014-euronews,0,\N,Missing
2015.iwslt-evaluation.7,W08-0509,0,\N,Missing
2015.iwslt-papers.5,J93-2004,0,\N,Missing
2015.iwslt-papers.5,D11-1033,0,\N,Missing
2015.iwslt-papers.5,P02-1040,0,\N,Missing
2015.iwslt-papers.5,P06-2093,1,\N,Missing
2015.iwslt-papers.5,P10-2041,0,\N,Missing
2015.iwslt-papers.5,P07-2045,0,\N,Missing
2015.iwslt-papers.5,E12-1055,0,\N,Missing
2015.iwslt-papers.5,J03-1002,0,\N,Missing
2015.iwslt-papers.5,C12-2104,1,\N,Missing
2015.iwslt-papers.5,P14-1023,0,\N,Missing
2015.iwslt-papers.5,W11-2123,0,\N,Missing
2019.jeptalnrecital-court.13,D14-1179,1,0.0456706,"Missing"
2019.jeptalnrecital-court.13,Y17-1038,0,0.0362267,"Missing"
2019.jeptalnrecital-court.13,D18-1398,0,0.0384699,"Missing"
2019.jeptalnrecital-court.13,W18-6325,0,0.0298165,"Missing"
2019.jeptalnrecital-court.13,W17-3204,0,0.069773,"Missing"
2019.jeptalnrecital-court.13,D18-2012,0,0.0696866,"Missing"
2019.jeptalnrecital-court.13,I17-2050,0,0.0327831,"Missing"
2019.jeptalnrecital-court.13,D16-1163,0,0.0461713,"Missing"
2020.emnlp-main.184,D18-1337,0,0.199294,"trade-off between the quality of the translation and the latency incurred in producing it. Previous work has considered rulebased strategies that rely on waiting until some constraint is satisfied, which includes approaches based on syntactic constraints (Bub et al., 1997; Ryu et al., 2006), segment/chunk/alignment information (Bangalore et al., 2012) heuristic-based conditions during decoding (Cho and Esipova, 2016) or deterministic policies with pre-determined latency constraints (Ma et al., 2019). An alternative line of research focuses on learning the decision policy: Gu et al. (2017) and Alinejad et al. (2018) frame SiMT as learning to generate READ/WRITE actions and employ reinforcement learning (RL) to formulate the problem as a policy agent interacting with its environment (i.e. a pre-trained MT model). Recent work has also explored supervised learning of the policy, by using oracle action sequences predicted by a pre-trained MT using confidence-based heuristics (Zheng et al., 2019) or external word aligners (Arthur et al., 2020) (details in §2). Thus far, all prior research has focused on unimodal interpretation1 . In this paper, we explore SiMT for multimodal machine translation (MMT) (Specia"
2020.emnlp-main.184,P19-1126,0,0.0613006,"Missing"
2020.emnlp-main.184,2020.iwslt-1.27,0,0.04374,"al. (2018) pointed out a potential mismatch between the training and decoding regimens of such approaches and proposed fine-tuning the models using chunked data or prefix pairs. Ma et al. (2019) proposed an end-to-end, fixed-latency framework called ‘waitk’ which allows prefix-to-prefix training using a deterministic policy: the agent starts by reading a specified number of source tokens (k), followed by alternating WRITE and READ actions. Arivazhagan et al. (2019) extended the wait-k framework using an advanced attention mechanism and optimising a differential latency metric (DAL). Recently, Arivazhagan et al. (2020) explored a radically different approach which enriches full-sentence training with prefix pairs (Niehues et al., 2018) and allows re-translation of previously committed target tokens to increase the translation quality. Another line of research focuses on learning adaptive policies in a supervised way by using oracle READ/WRITE actions generated with heuristic or alignment-based approaches. Zheng et al. (2019) extracted action sequences from a pre-trained NMT model with a confidence-based heuristic and used them to train a separate policy network while Arthur et al. (2020) explored jointly tr"
2020.emnlp-main.184,2021.eacl-main.233,0,0.930166,"Missing"
2020.emnlp-main.184,N12-1048,0,0.0230046,"nslation reliably, and how long the listener has to wait for the translation. In contrast to consecutive machine translation where source sentences are available in their entirety before translation, the challenge in SiMT is thus the design of a strategy to find a good trade-off between the quality of the translation and the latency incurred in producing it. Previous work has considered rulebased strategies that rely on waiting until some constraint is satisfied, which includes approaches based on syntactic constraints (Bub et al., 1997; Ryu et al., 2006), segment/chunk/alignment information (Bangalore et al., 2012) heuristic-based conditions during decoding (Cho and Esipova, 2016) or deterministic policies with pre-determined latency constraints (Ma et al., 2019). An alternative line of research focuses on learning the decision policy: Gu et al. (2017) and Alinejad et al. (2018) frame SiMT as learning to generate READ/WRITE actions and employ reinforcement learning (RL) to formulate the problem as a policy agent interacting with its environment (i.e. a pre-trained MT model). Recent work has also explored supervised learning of the policy, by using oracle action sequences predicted by a pre-trained MT us"
2020.emnlp-main.184,W18-6402,1,0.869037,"− C ∗ ) + 1] + βbDt − D∗ c+ where Ct denotes the CW metric introduced here to avoid long consecutive waits and Dt refers to AP (see § 3.4.1 for metrics). D∗ and C ∗ are hyperparameters that determine the expected/target values for AP and CW, respectively. The optimal quality-latency trade-off is achieved by balancing the two reward terms. 4 Experimental Setup 4.1 Dataset We use the Multi30k dataset (Elliott et al., 2016)5 which has been the primary corpus for MMT research across the three shared tasks of the “Conference on Machine Translation (WMT)” (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Multi30k extends the Flickr30k image captioning dataset (Young et al., 2014) to provide caption translations in German, French and Czech. In this work, we focus on the English→German and English→French (Elliott et al., 2017) language directions (Table 1). We use flickr2016 (2016), flickr2017 (2017) and coco2017 (COCO) for model evaluation. The latter test set is explicitly designed (Elliott et al., 2017) to contain at least 5 Preprocessing. We use Moses scripts (Koehn et al., 2007) to lowercase, punctuation-normalise and tokenise the sentences with hyphen splitting. We then create word vocab"
2020.emnlp-main.184,W17-4746,1,0.865441,"Missing"
2020.emnlp-main.184,W16-2358,1,0.892582,"Missing"
2020.emnlp-main.184,N19-1422,1,0.913351,"formance (Table 2). We observe that the decoder-attention using object detection features (DEC-OD) performs better than other variants. We also see that the improvements on flickr2017 (⇑ 0.5) and coco2017 (⇑ 1.03) test sets are higher than flickr2016 (⇑ 0.1) on average. A possible explanation is that flickr2017 and coco2017 are more distant from the training set distribution (higher OOV count, see Table 1) and thus there is more room for improvement with the visual cues. In summary, unlike previous conclusions in MMT where improvements were not found to be substantial (Gr¨onroos et al., 2018; Caglayan et al., 2019), we observe that the benefit of the visual modality is more pronounced here. We believe that this is due to (i) the encoder being now unidirectional different from state-of-the-art NMT, (ii) the modality representations being passed through layer normalisation (Ba et al., 2016), and (iii) the representational power of OD features. 5.2 Unimodal SiMT baselines We now compare unimodal SiMT approaches to get an initial understanding of how they perform on Multi30k. Figure 2 contrasts AL and BLEU for three trained wait-k systems, wait-if-diff (WID) decoding with k ∈ {1, 2} and δ=1, reinforcement l"
2020.emnlp-main.184,W16-2359,0,0.467061,"using auxiliary sources of information (Sulubacak et al., 2020). The most typical framework explored in previous work makes use of the images when translating their descriptions between languages, with the hypothesis that visual grounding could provide contextual cues to resolve linguistic phenomena such as word-sense disambiguation or gender marking. Existing work often rely on the use of visual features extracted from state-of-the-art CNN models pre-trained on large-scale visual tasks. The methods can be grouped into two branches depending on the feature type used: (i) multimodal attention (Calixto et al., 2016; Caglayan et al., 2016; Libovick´y and Helcl, 2017; Delbrouck and Dupont, 2017) which implements a soft attention (Bahdanau 2 During our initial experiments we also explored the RLbased SiMT policy (Gu et al., 2017) but could not find good hyper-parameter settings, especially settings which were stable across two language pairs. Therefore, we did not proceed with RL for multimodal SiMT. et al., 2014) over spatial feature maps, and (ii) multimodal interaction between a pooled visual feature vector and linguistic representations (Calixto and Liu, 2017; Caglayan et al., 2017a; Elliott and K´ad´a"
2020.emnlp-main.184,D17-1105,0,0.103036,"feature type used: (i) multimodal attention (Calixto et al., 2016; Caglayan et al., 2016; Libovick´y and Helcl, 2017; Delbrouck and Dupont, 2017) which implements a soft attention (Bahdanau 2 During our initial experiments we also explored the RLbased SiMT policy (Gu et al., 2017) but could not find good hyper-parameter settings, especially settings which were stable across two language pairs. Therefore, we did not proceed with RL for multimodal SiMT. et al., 2014) over spatial feature maps, and (ii) multimodal interaction between a pooled visual feature vector and linguistic representations (Calixto and Liu, 2017; Caglayan et al., 2017a; Elliott and K´ad´ar, 2017; Gr¨onroos et al., 2018). 2.2 Simultaneous Neural MT Simultaneous NMT was first explored by Cho and Esipova (2016) in a greedy decoding framework where heuristic waiting criteria are used to decide whether the model should read more source words or emit a target word. Gu et al. (2017) instead utilised a pre-trained NMT model in conjunction with a reinforcement learning agent whose goal is to learn a READ/WRITE policy by maximising quality and minimising latency. Alinejad et al. (2018) further extended the latter approach by adding a PREDICT a"
2020.emnlp-main.184,D14-1179,0,0.0521052,"Missing"
2020.emnlp-main.184,N18-2079,0,0.0385922,"edy decoding framework where heuristic waiting criteria are used to decide whether the model should read more source words or emit a target word. Gu et al. (2017) instead utilised a pre-trained NMT model in conjunction with a reinforcement learning agent whose goal is to learn a READ/WRITE policy by maximising quality and minimising latency. Alinejad et al. (2018) further extended the latter approach by adding a PREDICT action whose purpose is to anticipate the next source word. A common property of the above approaches is their reliance on consecutive NMT models pretrained on full-sentences. Dalvi et al. (2018) pointed out a potential mismatch between the training and decoding regimens of such approaches and proposed fine-tuning the models using chunked data or prefix pairs. Ma et al. (2019) proposed an end-to-end, fixed-latency framework called ‘waitk’ which allows prefix-to-prefix training using a deterministic policy: the agent starts by reading a specified number of source tokens (k), followed by alternating WRITE and READ actions. Arivazhagan et al. (2019) extended the wait-k framework using an advanced attention mechanism and optimising a differential latency metric (DAL). Recently, Arivazhaga"
2020.emnlp-main.184,W17-4718,1,0.88393,"d as: rtD = α [sgn(Ct − C ∗ ) + 1] + βbDt − D∗ c+ where Ct denotes the CW metric introduced here to avoid long consecutive waits and Dt refers to AP (see § 3.4.1 for metrics). D∗ and C ∗ are hyperparameters that determine the expected/target values for AP and CW, respectively. The optimal quality-latency trade-off is achieved by balancing the two reward terms. 4 Experimental Setup 4.1 Dataset We use the Multi30k dataset (Elliott et al., 2016)5 which has been the primary corpus for MMT research across the three shared tasks of the “Conference on Machine Translation (WMT)” (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Multi30k extends the Flickr30k image captioning dataset (Young et al., 2014) to provide caption translations in German, French and Czech. In this work, we focus on the English→German and English→French (Elliott et al., 2017) language directions (Table 1). We use flickr2016 (2016), flickr2017 (2017) and coco2017 (COCO) for model evaluation. The latter test set is explicitly designed (Elliott et al., 2017) to contain at least 5 Preprocessing. We use Moses scripts (Koehn et al., 2007) to lowercase, punctuation-normalise and tokenise the sentences with hyphen splitting. W"
2020.emnlp-main.184,W16-3210,1,0.891154,"Missing"
2020.emnlp-main.184,I17-1014,0,0.0922802,"Missing"
2020.emnlp-main.184,W18-6439,0,0.0844784,"Missing"
2020.emnlp-main.184,E17-1099,0,0.29126,"ategy to find a good trade-off between the quality of the translation and the latency incurred in producing it. Previous work has considered rulebased strategies that rely on waiting until some constraint is satisfied, which includes approaches based on syntactic constraints (Bub et al., 1997; Ryu et al., 2006), segment/chunk/alignment information (Bangalore et al., 2012) heuristic-based conditions during decoding (Cho and Esipova, 2016) or deterministic policies with pre-determined latency constraints (Ma et al., 2019). An alternative line of research focuses on learning the decision policy: Gu et al. (2017) and Alinejad et al. (2018) frame SiMT as learning to generate READ/WRITE actions and employ reinforcement learning (RL) to formulate the problem as a policy agent interacting with its environment (i.e. a pre-trained MT model). Recent work has also explored supervised learning of the policy, by using oracle action sequences predicted by a pre-trained MT using confidence-based heuristics (Zheng et al., 2019) or external word aligners (Arthur et al., 2020) (details in §2). Thus far, all prior research has focused on unimodal interpretation1 . In this paper, we explore SiMT for multimodal machine"
2020.emnlp-main.184,2020.wmt-1.70,0,0.328621,"ons and employ reinforcement learning (RL) to formulate the problem as a policy agent interacting with its environment (i.e. a pre-trained MT model). Recent work has also explored supervised learning of the policy, by using oracle action sequences predicted by a pre-trained MT using confidence-based heuristics (Zheng et al., 2019) or external word aligners (Arthur et al., 2020) (details in §2). Thus far, all prior research has focused on unimodal interpretation1 . In this paper, we explore SiMT for multimodal machine translation (MMT) (Specia et al., 2016), where in addition to 1 We note that Imankulova et al. (2020) also attempted to explore multimodality in SiMT. However, their paper overestimates the impact of visual cues, and in personal correspondence with the authors about the mismatch in the findings, they discovered critical bugs in their implementation. 2350 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2350–2361, c November 16–20, 2020. 2020 Association for Computational Linguistics the source sentence, we have access to visual information in the form of an image. We believe that having access to a complementary context should help the models antic"
2020.emnlp-main.184,P02-1040,0,0.106801,"nd mini-batch size to 0.0004 and 6, respectively. For each sentence pair in a batch, ten trajectories are sampled. For inference, greedy sampling is used to pick action sequences. We set the hyperparameters C ∗ =2, D∗ =0.3, α=0.025 and β= − 1. To encourage exploration, the negative entropy policy term is weighed empirically with 0.1 and 0.3 for En→Fr and En→De directions, respectively. Training. We use nmtpytorch (Caglayan et al., 2017b) with PyTorch (Paszke et al., 2019) v1.4 for our experiments7 . We train each model for a maximum of 50 epochs and early stop the training if validation BLEU (Papineni et al., 2002) does not improve for 10 epochs. We also halve the learning rate if no improvement is obtained for two epochs. On a single NVIDIA RTX2080-Ti GPU, it takes around 35 minutes for the unimodal and multimodal encoder variants to complete train6 7 https://github.com/multi30k/dataset 2354 https://github.com/nyu-dl/dl4mt-simul-trans https://github.com/ImperialNLP/pysimt English→German 2016 2017 COCO English→French 2016 2017 COCO NMT 34.6 26.4 22.1 57.8 50.3 41.4 – ENC-OD ⇓ 0.6 ⇑ 0.3 ⇑ 0.8 ⇑ 0.3 ⇑ 0.1 ⇑ 1.1 ⇑ 0.33 DEC-OC ⇑ 0.4 ⇑ 0.8 ⇑ 0.7 ⇓ 0.3 ⇑ 0.2 ⇑ 0.7 ⇑ 0.52 DEC-OD ⇑ 0.7 ⇑ 1.0 ⇑ 1.7 ⇑ 0.1 ⇑ 0.6 ⇑"
2020.emnlp-main.184,P07-2045,0,0.00695164,"ee shared tasks of the “Conference on Machine Translation (WMT)” (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Multi30k extends the Flickr30k image captioning dataset (Young et al., 2014) to provide caption translations in German, French and Czech. In this work, we focus on the English→German and English→French (Elliott et al., 2017) language directions (Table 1). We use flickr2016 (2016), flickr2017 (2017) and coco2017 (COCO) for model evaluation. The latter test set is explicitly designed (Elliott et al., 2017) to contain at least 5 Preprocessing. We use Moses scripts (Koehn et al., 2007) to lowercase, punctuation-normalise and tokenise the sentences with hyphen splitting. We then create word vocabularies on the training subset of the dataset. We did not use subword segmentation to avoid its potential side effects on SiMT and to be able to analyse the grounding capability of the models better. The resulting English, French and German vocabularies contain 9.8K, 11K and 18K tokens, respectively. 4.2 Reproducibility Hyperparameters. The dimensions of embeddings and GRU hidden states are set to 200 and 320, respectively. The decoder’s input and output embeddings are shared (Press"
2020.emnlp-main.184,W18-6319,0,0.0164835,") on the validation set with patience set to ten epochs. The number of learnable parameters is around 6M. 4.3 5.1 2 3 4 ... 13.1 1 WAIT1 WAIT2 2 3 4 ... 11.4 RL Consecutive WAIT3 Figure 2: AL vs BLEU comparison across unimodal SiMT approaches: wait-k systems are “trained”. Evaluation To mitigate variance in results due to different initialisations, we repeat each experiment three times, with random seeds. Following previous work, we decode translations with greedy search, using the checkpoint that achieved the lowest perplexity. We report average BLEU scores across three runs using sacreBLEU (Post, 2018), which is also used for computing sentence-level scores for the oracle experiments. 5 1 Results Consecutive baselines We first present the impact of the visual integration approaches on consecutive NMT performance (Table 2). We observe that the decoder-attention using object detection features (DEC-OD) performs better than other variants. We also see that the improvements on flickr2017 (⇑ 0.5) and coco2017 (⇑ 1.03) test sets are higher than flickr2016 (⇑ 0.1) on average. A possible explanation is that flickr2017 and coco2017 are more distant from the training set distribution (higher OOV coun"
2020.emnlp-main.184,P17-2031,0,0.121743,"Missing"
2020.emnlp-main.184,E17-2025,0,0.0166106,"2007) to lowercase, punctuation-normalise and tokenise the sentences with hyphen splitting. We then create word vocabularies on the training subset of the dataset. We did not use subword segmentation to avoid its potential side effects on SiMT and to be able to analyse the grounding capability of the models better. The resulting English, French and German vocabularies contain 9.8K, 11K and 18K tokens, respectively. 4.2 Reproducibility Hyperparameters. The dimensions of embeddings and GRU hidden states are set to 200 and 320, respectively. The decoder’s input and output embeddings are shared (Press and Wolf, 2017). We use ADAM (Kingma and Ba, 2014) as the optimiser and set the learning rate and mini-batch size to 0.0004 and 64, respectively. A weight decay of 1e−5 is applied for regularisation. We clip the gradients if the norm of the full parameter vector exceeds 1 (Pascanu et al., 2013). For the RL baseline, we closely follow (Gu et al., 2017)6 . The agent is implemented by a 320-dimensional GRU followed by a softmax layer and the baseline network – used for variance reduction of policy gradient – is similar to the agent except with a scalar output layer. We use ADAM as the optimiser and set the lear"
2020.emnlp-main.184,P06-2088,0,0.396269,"e between how much context is needed to generate the translation reliably, and how long the listener has to wait for the translation. In contrast to consecutive machine translation where source sentences are available in their entirety before translation, the challenge in SiMT is thus the design of a strategy to find a good trade-off between the quality of the translation and the latency incurred in producing it. Previous work has considered rulebased strategies that rely on waiting until some constraint is satisfied, which includes approaches based on syntactic constraints (Bub et al., 1997; Ryu et al., 2006), segment/chunk/alignment information (Bangalore et al., 2012) heuristic-based conditions during decoding (Cho and Esipova, 2016) or deterministic policies with pre-determined latency constraints (Ma et al., 2019). An alternative line of research focuses on learning the decision policy: Gu et al. (2017) and Alinejad et al. (2018) frame SiMT as learning to generate READ/WRITE actions and employ reinforcement learning (RL) to formulate the problem as a policy agent interacting with its environment (i.e. a pre-trained MT model). Recent work has also explored supervised learning of the policy, by"
2020.emnlp-main.184,E17-3017,0,0.0321164,"Missing"
2020.emnlp-main.184,W16-2346,1,0.910315,"Missing"
2020.emnlp-main.184,Q14-1006,0,0.0477757,"id long consecutive waits and Dt refers to AP (see § 3.4.1 for metrics). D∗ and C ∗ are hyperparameters that determine the expected/target values for AP and CW, respectively. The optimal quality-latency trade-off is achieved by balancing the two reward terms. 4 Experimental Setup 4.1 Dataset We use the Multi30k dataset (Elliott et al., 2016)5 which has been the primary corpus for MMT research across the three shared tasks of the “Conference on Machine Translation (WMT)” (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Multi30k extends the Flickr30k image captioning dataset (Young et al., 2014) to provide caption translations in German, French and Czech. In this work, we focus on the English→German and English→French (Elliott et al., 2017) language directions (Table 1). We use flickr2016 (2016), flickr2017 (2017) and coco2017 (COCO) for model evaluation. The latter test set is explicitly designed (Elliott et al., 2017) to contain at least 5 Preprocessing. We use Moses scripts (Koehn et al., 2007) to lowercase, punctuation-normalise and tokenise the sentences with hyphen splitting. We then create word vocabularies on the training subset of the dataset. We did not use subword segmenta"
2020.emnlp-main.184,D19-1137,0,0.513174,"g decoding (Cho and Esipova, 2016) or deterministic policies with pre-determined latency constraints (Ma et al., 2019). An alternative line of research focuses on learning the decision policy: Gu et al. (2017) and Alinejad et al. (2018) frame SiMT as learning to generate READ/WRITE actions and employ reinforcement learning (RL) to formulate the problem as a policy agent interacting with its environment (i.e. a pre-trained MT model). Recent work has also explored supervised learning of the policy, by using oracle action sequences predicted by a pre-trained MT using confidence-based heuristics (Zheng et al., 2019) or external word aligners (Arthur et al., 2020) (details in §2). Thus far, all prior research has focused on unimodal interpretation1 . In this paper, we explore SiMT for multimodal machine translation (MMT) (Specia et al., 2016), where in addition to 1 We note that Imankulova et al. (2020) also attempted to explore multimodality in SiMT. However, their paper overestimates the impact of visual cues, and in personal correspondence with the authors about the mismatch in the findings, they discovered critical bugs in their implementation. 2350 Proceedings of the 2020 Conference on Empirical Meth"
2020.jeptalnrecital-jep.58,L18-1238,1,0.83364,"Missing"
2020.jeptalnrecital-jep.58,L16-1039,0,0.0375101,"Missing"
2020.jeptalnrecital-jep.58,P02-1040,0,0.115471,"Missing"
2020.jeptalnrecital-jep.58,2020.lrec-1.226,1,0.286204,"Missing"
2020.jeptalnrecital-jep.58,D18-1318,0,0.0365269,"Missing"
2020.jeptalnrecital-taln.20,N19-1389,0,0.0128082,"ollmann et al., 2011), la traduction Volume 2 : Traitement Automatique des Langues Naturelles, pages 213–222. hal : hal-02784770. Cette œuvre est mise à disposition sous licence Attribution 4.0 International. automatique statistique (Statistical Machine Translation, désormais SMT) (Pettersson et al., 2013 ; Sánchez-Martínez et al., 2013 ; Scherrer & Erjavec, 2013) ou, plus récemment, la traduction automatique neuronale (Neural Machine Translation, désormais NMT)(Bollmann & Søgaard, 2016). De nombreuses évaluations ont montré l’efficacité des deux dernières solutions (Pettersson et al., 2014 ; Bollmann, 2019), dont le défaut est cependant bien connu : la quantité de données nécessaires pour l’entraînement d’un modèle. De telles données existent pour de nombreuses langues (anglais, allemand, hongrois, islandais, portugais, slovène, espagnol ou suédois), mais pas pour le français. Nous nous proposons donc de reprendre l’approche comparatiste de Bollmann & Søgaard (2016) mais, contrairement à ces derniers, nous travaillons sur la langue française classique, et surtout nous n’opérons pas sur des mots isolés pris dans un dictionnaire mais sur des phrases entières. Il nous est ainsi possible de traiter"
2020.jeptalnrecital-taln.20,W11-4106,0,0.0368693,"isons. D’une part, même si on peut le déplorer (Gabay, 2014), les lecteurs ont pris l’habitude de lire la langue de Molière avec une orthographe contemporaine. D’autre part, la variation graphique complique l’approche computationnelle de la langue : elle altère le rapport token/type (désormais TTR) pour les études stylométriques (Kestemont, 2012 ; Pinche et al., 2019), complique la lemmatisation (Manjavacas et al., 2019) et l’extraction d’information (Pettersson, 2016)… Pour toutes ces raisons, plusieurs solutions ont été testées afin de normaliser les données : les systèmes à base de règles (Bollmann et al., 2011), la traduction Volume 2 : Traitement Automatique des Langues Naturelles, pages 213–222. hal : hal-02784770. Cette œuvre est mise à disposition sous licence Attribution 4.0 International. automatique statistique (Statistical Machine Translation, désormais SMT) (Pettersson et al., 2013 ; Sánchez-Martínez et al., 2013 ; Scherrer & Erjavec, 2013) ou, plus récemment, la traduction automatique neuronale (Neural Machine Translation, désormais NMT)(Bollmann & Søgaard, 2016). De nombreuses évaluations ont montré l’efficacité des deux dernières solutions (Pettersson et al., 2014 ; Bollmann, 2019), dont"
2020.jeptalnrecital-taln.20,C16-1013,0,0.0208074,"2016)… Pour toutes ces raisons, plusieurs solutions ont été testées afin de normaliser les données : les systèmes à base de règles (Bollmann et al., 2011), la traduction Volume 2 : Traitement Automatique des Langues Naturelles, pages 213–222. hal : hal-02784770. Cette œuvre est mise à disposition sous licence Attribution 4.0 International. automatique statistique (Statistical Machine Translation, désormais SMT) (Pettersson et al., 2013 ; Sánchez-Martínez et al., 2013 ; Scherrer & Erjavec, 2013) ou, plus récemment, la traduction automatique neuronale (Neural Machine Translation, désormais NMT)(Bollmann & Søgaard, 2016). De nombreuses évaluations ont montré l’efficacité des deux dernières solutions (Pettersson et al., 2014 ; Bollmann, 2019), dont le défaut est cependant bien connu : la quantité de données nécessaires pour l’entraînement d’un modèle. De telles données existent pour de nombreuses langues (anglais, allemand, hongrois, islandais, portugais, slovène, espagnol ou suédois), mais pas pour le français. Nous nous proposons donc de reprendre l’approche comparatiste de Bollmann & Søgaard (2016) mais, contrairement à ces derniers, nous travaillons sur la langue française classique, et surtout nous n’opér"
2020.jeptalnrecital-taln.20,D14-1179,0,0.029156,"Missing"
2020.jeptalnrecital-taln.20,W14-3348,0,0.056641,"Missing"
2020.jeptalnrecital-taln.20,C14-2028,1,0.871732,"Missing"
2020.jeptalnrecital-taln.20,N19-1153,0,0.0154086,": Normalisation, 17th c. French, Neural Machine Translation (NMT), Statistical Machine Translation (SMT), digital humanities. 1 Introduction Le français pré-orthographique est un problème pour deux raisons. D’une part, même si on peut le déplorer (Gabay, 2014), les lecteurs ont pris l’habitude de lire la langue de Molière avec une orthographe contemporaine. D’autre part, la variation graphique complique l’approche computationnelle de la langue : elle altère le rapport token/type (désormais TTR) pour les études stylométriques (Kestemont, 2012 ; Pinche et al., 2019), complique la lemmatisation (Manjavacas et al., 2019) et l’extraction d’information (Pettersson, 2016)… Pour toutes ces raisons, plusieurs solutions ont été testées afin de normaliser les données : les systèmes à base de règles (Bollmann et al., 2011), la traduction Volume 2 : Traitement Automatique des Langues Naturelles, pages 213–222. hal : hal-02784770. Cette œuvre est mise à disposition sous licence Attribution 4.0 International. automatique statistique (Statistical Machine Translation, désormais SMT) (Pettersson et al., 2013 ; Sánchez-Martínez et al., 2013 ; Scherrer & Erjavec, 2013) ou, plus récemment, la traduction automatique neuronale"
2020.jeptalnrecital-taln.20,P02-1040,0,0.110877,"Missing"
2020.jeptalnrecital-taln.20,W13-5617,0,0.0242338,"(désormais TTR) pour les études stylométriques (Kestemont, 2012 ; Pinche et al., 2019), complique la lemmatisation (Manjavacas et al., 2019) et l’extraction d’information (Pettersson, 2016)… Pour toutes ces raisons, plusieurs solutions ont été testées afin de normaliser les données : les systèmes à base de règles (Bollmann et al., 2011), la traduction Volume 2 : Traitement Automatique des Langues Naturelles, pages 213–222. hal : hal-02784770. Cette œuvre est mise à disposition sous licence Attribution 4.0 International. automatique statistique (Statistical Machine Translation, désormais SMT) (Pettersson et al., 2013 ; Sánchez-Martínez et al., 2013 ; Scherrer & Erjavec, 2013) ou, plus récemment, la traduction automatique neuronale (Neural Machine Translation, désormais NMT)(Bollmann & Søgaard, 2016). De nombreuses évaluations ont montré l’efficacité des deux dernières solutions (Pettersson et al., 2014 ; Bollmann, 2019), dont le défaut est cependant bien connu : la quantité de données nécessaires pour l’entraînement d’un modèle. De telles données existent pour de nombreuses langues (anglais, allemand, hongrois, islandais, portugais, slovène, espagnol ou suédois), mais pas pour le français. Nous nous propo"
2020.jeptalnrecital-taln.20,W14-0605,0,0.0176879,"stèmes à base de règles (Bollmann et al., 2011), la traduction Volume 2 : Traitement Automatique des Langues Naturelles, pages 213–222. hal : hal-02784770. Cette œuvre est mise à disposition sous licence Attribution 4.0 International. automatique statistique (Statistical Machine Translation, désormais SMT) (Pettersson et al., 2013 ; Sánchez-Martínez et al., 2013 ; Scherrer & Erjavec, 2013) ou, plus récemment, la traduction automatique neuronale (Neural Machine Translation, désormais NMT)(Bollmann & Søgaard, 2016). De nombreuses évaluations ont montré l’efficacité des deux dernières solutions (Pettersson et al., 2014 ; Bollmann, 2019), dont le défaut est cependant bien connu : la quantité de données nécessaires pour l’entraînement d’un modèle. De telles données existent pour de nombreuses langues (anglais, allemand, hongrois, islandais, portugais, slovène, espagnol ou suédois), mais pas pour le français. Nous nous proposons donc de reprendre l’approche comparatiste de Bollmann & Søgaard (2016) mais, contrairement à ces derniers, nous travaillons sur la langue française classique, et surtout nous n’opérons pas sur des mots isolés pris dans un dictionnaire mais sur des phrases entières. Il nous est ainsi po"
2020.jeptalnrecital-taln.20,W13-2409,0,0.0268789,"2012 ; Pinche et al., 2019), complique la lemmatisation (Manjavacas et al., 2019) et l’extraction d’information (Pettersson, 2016)… Pour toutes ces raisons, plusieurs solutions ont été testées afin de normaliser les données : les systèmes à base de règles (Bollmann et al., 2011), la traduction Volume 2 : Traitement Automatique des Langues Naturelles, pages 213–222. hal : hal-02784770. Cette œuvre est mise à disposition sous licence Attribution 4.0 International. automatique statistique (Statistical Machine Translation, désormais SMT) (Pettersson et al., 2013 ; Sánchez-Martínez et al., 2013 ; Scherrer & Erjavec, 2013) ou, plus récemment, la traduction automatique neuronale (Neural Machine Translation, désormais NMT)(Bollmann & Søgaard, 2016). De nombreuses évaluations ont montré l’efficacité des deux dernières solutions (Pettersson et al., 2014 ; Bollmann, 2019), dont le défaut est cependant bien connu : la quantité de données nécessaires pour l’entraînement d’un modèle. De telles données existent pour de nombreuses langues (anglais, allemand, hongrois, islandais, portugais, slovène, espagnol ou suédois), mais pas pour le français. Nous nous proposons donc de reprendre l’approche comparatiste de Bollmann &"
2020.jeptalnrecital-taln.20,E17-3017,0,0.0317665,"d. inv.+ML BLEU (4-grammes) 77,667 77,108 76,680 75,766 METEOR 87,891 87,308 87,024 86,257 wAcc 86.68496 86.4022 86.06121 85.57052 Table 3 – Evaluation des modèles avec cSMT Les résultats sont bons, mais il semble que les différentes techniques utilisées pour améliorer les scores aient un effet neutre, voire négatif sur le score final. 4.2 NMT En ce qui concerne la traduction automatique neuronale, nous avons décidé d’utiliser NMTPyTorch (Caglayan et al., 2017). Le modèle de base est composé d’un encodeur GRU bidirectionnel (Cho et al., 2014) et d’un décodeur (GRU conditionnel à deux couches (Sennrich et al., 2017)) avec mécanisme d’attention de type perceptron multicouche (Bahdanau et al., 2015). L’encodeur et le décodeur ont tous les deux 400 unités cachées et leur état initial caché est initialisé à zéro. Les plongements lexicaux ont une taille fixée à 200. Trois versions du système ont été entraînées : une première au niveau mot, une seconde avec des unités byte pair encoding ou BPE (Sennrich et al., 2015) opérant au niveau du sous-mot, et une troisième au niveau du caractère. Le tableau 4 montre le résultats de tels pré-traitements. Chaque système a été entraîné quatre fois avec une initialisation"
2020.lrec-1.226,D18-1318,0,0.0742405,"evaluate online HAL In order to guarantee the fairness of evaluation, a human domain expert simulator is used in order to allow reproducibility of both offline and online adaptation processes. 2.1. Evaluating offline human-assisted learning The efficiency of human assisted learning lies in its ability to improve the performance of the system while minimizing the cost of human interactions. In the literature, many documents advise to measure the quality of this adaptation process by considering the dependence between cost of interactions and performance of the system (Krogh and Vedelsby, 1995; Siddhant and Lipton, 2018; Drugman et al., 2019; Beluch et al., 2018; P´erezDattari et al., 2018; Celemin and Ruiz-del Solar, 2019). This approach is popular and we do not propose any new proposal, but only report here this measure as a way to evaluate offline human assisted learning. Given an initial model, an iterative questions/answer interaction is initiated by the system with the goal of reducing the error rate on a given dataset. The system can potentially ask questions related to any document it has access to: the adaptation set, and its performance is evaluated on a dataset it is not aware of and that can be d"
2020.lrec-1.226,2006.amta-papers.25,0,0.04896,"nformation provided by the human expert to other parts of the data. However, the generality of this measure does not allow to express the cost of human interaction in terms of concrete units such as time spent by the operator, onerousness, strenuousness, etc.. For some specific cases, the computation of impaired scores rises questions related to the definition of wrong hypothesis. For instance, for the case of a binary classification task, the impaired hypothesis consists of allocating the wrong label to an element. The case of machine translation evaluated by the Translation Edit Rate (TER, (Snover et al., 2006)) is problematic. Because it is always possible to worsen the score by inserting words, then the worst translation cannot be identified. A simple way to deal with such a case is to bound the maximum error to the sequence length, which sounds a reasonable compromise. In appendix, we illustrate the penalised evaluation for the cases of another machine translation metric (BLEU) and a speaker diarization metric (Diarization Error Rate). The proposed computation of penalisation assumes that the answer provided by the human expert (or the correction in case of interactive learning) can be directly a"
2020.lrec-1.226,L18-1238,1,0.764452,"Missing"
2020.lrec-1.226,L16-1039,0,0.569331,"Missing"
2020.lrec-1.226,P02-1040,0,0.113577,"Missing"
2020.wmt-1.1,2020.nlpcovid19-2.5,1,0.796341,"tails of the evaluation. 4.1.1 Covid Test Suite TICO-19 The TICO-19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19. Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation, as described by the World Health Organization. The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here. The submitted systems were evaluated using the test set from the recently-released TICO-19 dataset (Anastasopoulos et al., 2020). The dataset provides manually created translations of COVID19 related data. The test set consists of PubMed articles (678 sentences from 5 scientific articles), patient-medical professional conversations (104 sentences), as well as related Wikipedia articles (411 sentences), announcements (98 sentences from Wikisource), and news items (67 sentences from Wikinews), for a total of 2100 sentences. Table 15 outlines the BLEU scores by each submitted system in the English-to-X directions, also breaking down the results per domain. The analysis shows that some systems are significantly more prepar"
2020.wmt-1.1,2020.wmt-1.6,0,0.0647231,"AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua Universit"
2020.wmt-1.1,2020.wmt-1.38,0,0.0746945,"Missing"
2020.wmt-1.1,2020.wmt-1.54,1,0.802974,"Missing"
2020.wmt-1.1,W07-0718,1,0.671054,"Missing"
2020.wmt-1.1,W08-0309,1,0.762341,"Missing"
2020.wmt-1.1,W12-3102,1,0.500805,"Missing"
2020.wmt-1.1,2020.lrec-1.461,0,0.0795779,"Missing"
2020.wmt-1.1,2012.eamt-1.60,0,0.124643,"tted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS"
2020.wmt-1.1,2020.wmt-1.3,0,0.0731913,"on Machine Translation (WMT20)1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation. This conference built on 14 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; CallisonBurch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • automatic post-editing (Chatterjee et al., 2020) • biomedical translation (Bawden et al., 2020b) • chat translation (Farajian et al., 2020) • lifelong learning (Barrault et al., 2020) 1 Makoto Morishita NTT Santanu Pal WIPRO AI Abstract 1 Philipp Koehn JHU http://www.statmt.org/wmt20/ 1 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1–55 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics as “direct assessment”) that we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems. The primary objectives o"
2020.wmt-1.1,2020.wmt-1.8,0,0.0898111,"D D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Know"
2020.wmt-1.1,2009.freeopmt-1.3,0,0.088081,"ve (CONTRASTIVE) or primary (PRIMARY), and the BLEU, RIBES and TER results. The scores are sorted by BLEU. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. This year we recived major number of participants for the case of Indo-Aryan language group NUST-FJWU NUST-FJWU system is an extension of state-of-the-art Transformer model with hierarchical attention networks to incorporate contextual information. During training the model used back-translation. Prompsit This team is participating with a rulebased system based on Apertium (Forcada et al., 2009-11). Apertium is a free/open-source platform for developing rule-based machine translation systems and language technology that was first released in 2005. Apertium is hosted in Github where both language data and code are licensed under the GNU GPL. It is a research and business platform with a very active community that loves small languages. Language pairs are at a very different level of development and output quality in the platform, depending on two main variables: how much funded or in-kind effort has 32 5.4 i.e. Hindi–Marathi (in both directions). We received 22 submissions from 14 te"
2020.wmt-1.1,2020.wmt-1.80,0,0.0933589,"airs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new for this year. Furthermore, English to and from Khmer and Pashto were included, using the same test sets as in the corpus filtering task. Th"
2020.wmt-1.1,W19-5204,0,0.0543621,"Missing"
2020.wmt-1.1,2020.emnlp-main.5,0,0.0410594,"luation of out-ofEnglish translations, HITs were generated using the same method as described for the SR+DC evaluation of into-English translations in Section 3.2.1 with minor modifications. Source-based DA allows to include human references in the evaluation as another system to provide an estimate of human performance. Human references were added to the pull of system outputs prior to sampling documents for tasks generation. If multiple references are available, which is the case for English→German (3 alternative reference translations, including 1 generated using the paraphrasing method of Freitag et al. (2020)) and English→Chinese (2 translations), each reference is assessed individually. Since the annotations are made by researchers and professional translators who ensure a betTable 11: Amount of data collected in the WMT20 manual document- and segment-level evaluation campaigns for bilingual/source-based evaluation out of English and nonEnglish pairs. et al., 2020; Laubli et al., 2020). It differs from SR+DC DA introduced in WMT19 (Bojar et al., 2019), and still used in into-English human evaluation this year, where a single segment from a document is provided on a screen at a time, followed by s"
2020.wmt-1.1,W18-3931,1,0.874637,"ese improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art translation systems on trans"
2020.wmt-1.1,2020.wmt-1.18,0,0.0913945,"2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al., 2020) Baseline System from Biomedical Task (Bawden et al., 2020b) American University of Beirut (no associated paper) Zoho Corporation (no associated paper) Table 6: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion"
2020.wmt-1.1,2020.wmt-1.43,0,0.0835067,"Missing"
2020.wmt-1.1,2020.wmt-1.9,0,0.0939415,"Missing"
2020.wmt-1.1,2020.wmt-1.19,0,0.0674131,"Missing"
2020.wmt-1.1,2009.mtsummit-btm.6,0,0.103443,"Missing"
2020.wmt-1.1,W13-2305,1,0.929934,"work which can be well applied to different translation. directions. Techniques used in the submitted systems include optional multilingual pre-training (mRASP) for low resource languages, very deep Transformer or dynamic convolution models up to 50 encoder layers, iterative backtranslation, knowledge distillation, model ensemble and development set fine-tuning. The key ingredient of the process seems the strong focus on diversification of the (synthetic) training data, using multiple scalings of the Transformer model 3.1 Direct Assessment Since running a comparison of direct assessments (DA, Graham et al., 2013, 2014, 2016) and relative ranking in 2016 (Bojar et al., 2016) and verifying a high correlation of system rankings for the two methods, as well as the advantages of DA, such as quality controlled crowd-sourcing and linear growth relative to numbers of submissions, we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100"
2020.wmt-1.1,E14-1047,1,0.888167,"Missing"
2020.wmt-1.1,2020.lrec-1.312,1,0.804196,"A screenshot of OCELoT is shown in Figure 5. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, Engli"
2020.wmt-1.1,2020.emnlp-main.6,1,0.839606,"shown in Table 3, where the first and second are simple merges or splits, whereas the third is a rare case of more complex reordering. We leave a detailed analysis of the translators’ treatment of paragraph-split data for future work. development set is provided, it is a mixture of both “source-original” and “target-original” texts, in order to maximise its size, although the original language is always marked in the sgm file, except for Inuktitut↔English. The consequences of directionality in test sets has been discussed recently in the literature (Freitag et al., 2019; Laubli et al., 2020; Graham et al., 2020), and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation. We use “source-original” parallel sentences wherever possible, on the basis that it is the more realistic scenario for practical MT usage. Exception: the test sets for the two Inuktitut↔English translation directions contain the same data, without regard to original direction. For most news text in the test and development sets, English was the original language and Inuktitut the translation, while the parliamentary data mixes the two directions. The origins of the news test documents"
2020.wmt-1.1,2020.wmt-1.11,0,0.0940191,"N GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) S"
2020.wmt-1.1,D19-1632,1,0.881933,"ent and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS (software localization, Tatoeba, Global Voices), the Bible, and specialprepared corpora from TED Talks and the Jehova Witness web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics ab"
2020.wmt-1.1,2020.wmt-1.12,1,0.754946,"Missing"
2020.wmt-1.1,2020.wmt-1.13,0,0.0737827,"2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al"
2020.wmt-1.1,2020.wmt-1.20,0,0.057602,"Missing"
2020.wmt-1.1,2020.wmt-1.14,1,0.820019,"set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Mari"
2020.wmt-1.1,2020.wmt-1.39,1,0.812433,"kables was collected in the first phase of the annotation, which amounted to 4k assessments across the systems. The second annotation phase with 6.5k assessments compared markable translations, always checking outputs of all the 13 competing MT systems but still considering the document-level context of each of them. Among other things, the observations indicate that the better the system, the lower the variance in manual scores. Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely, 4.1.3 Gender Coreference and Bias (Kocmi et al., 2020) The test suite by Kocmi et al. (2020) focuses on the gender bias in professions (e.g. physician, teacher, secretary) for the translation from English into Czech, German, Polish and Russian. These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages. The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence. Once disambiguated, the gender needs to be preserved in translation. To correctly translate the given noun, the translation system thus has to corr"
2020.wmt-1.1,2020.wmt-1.53,0,0.089538,"Missing"
2020.wmt-1.1,2020.wmt-1.78,1,0.815194,"rence on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new fo"
2020.wmt-1.1,W17-1208,0,0.0524248,"ttribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art tr"
2020.wmt-1.1,2020.wmt-1.21,0,0.0791885,"Missing"
2020.wmt-1.1,2020.wmt-1.23,0,0.0607352,"020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2"
2020.wmt-1.1,2020.wmt-1.77,1,0.84299,"slation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inu"
2020.wmt-1.1,2020.wmt-1.24,0,0.0435945,"Missing"
2020.wmt-1.1,D18-1512,0,0.05415,"Missing"
2020.wmt-1.1,2020.wmt-1.47,1,0.740067,"Missing"
2020.wmt-1.1,W18-3601,1,0.891679,", we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100 rating scale.5 No sentence or document length restriction is applied during manual evaluation. Direct Assessment is also employed for evaluation of video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019) and multilingual surface realisation (Mille et al., 2018, 2019). 3.1.1 tion 2, most of our test sets do not include reversecreated sentence pairs, except when there were resource constraints on the creation of the test sets. 3.1.3 Prior to WMT19, the issue of including document context was raised within the community (Läubli et al., 2018; Toral et al., 2018) and at WMT19 a range of DA styles were subsequently tested that included document context. In WMT19, two options were run, firstly, an evaluation that included the document context “+DC” (with document context), and secondly, a variation that omitted document context “−DC” (without document con"
2020.wmt-1.1,2020.wmt-1.27,0,0.247738,"he original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans"
2020.wmt-1.1,D19-6301,1,0.888512,"Missing"
2020.wmt-1.1,W18-6424,0,0.0431841,"(Kocmi, 2020) combines transfer learning from a high-resource language pair Czech–English into the low-resource Inuktitut-English with an additional backtranslation step. Surprising behaviour is noticed when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. The system is the Transformer model in a constrained submission. 2.3.3 Charles University (CUNI) CUNI-D OC T RANSFORMER (Popel, 2020) is similar to the sentence-level version (CUNI-T2T2018, CUBBITT), but trained on sequences with multiple sentences of up to 3000 characters. CUNI-T2T-2018 (Popel, 2018), also called CUBBITT, is exactly the same system as in WMT2018. It is the Transformer model trained according to Popel and Bojar (2018) plus a novel concat-regime backtranslation with checkpoint averaging (Popel et al., 2020), tuned separately for CZ-domain and non CZ-domain articles, possibly handling also translation-direction (“translationese”) issues. For cs→en also a coreference preprocessing was used adding the female-gender CUNI-T RANSFORMER (Popel, 2020) is similar to the WMT2018 version of CUBBITT, but with 12 encoder layers instead of 6 and trained on CzEng 2.0 instead of CzEng 1.7."
2020.wmt-1.1,2020.wmt-1.25,0,0.094349,"Missing"
2020.wmt-1.1,2020.wmt-1.28,0,0.0792624,"cument in the test set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 20"
2020.wmt-1.1,2020.lrec-1.443,1,0.79707,"er their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained t"
2020.wmt-1.1,2020.wmt-1.48,0,0.090424,"Missing"
2020.wmt-1.1,2020.wmt-1.49,0,0.0519886,"Missing"
2020.wmt-1.1,W19-6712,0,0.0573476,"tly crawled multilingual parallel corpora from Indian government websites (Haddow and Kirefu, 2020; Siripragada et al., 2020), the Tanzil corpus (Tiedemann, 2009), the Pavlick dicParagraph-split Test Sets For the language pairs English↔Czech, English↔German and English→Chinese, we provided the translators with paragraph-split texts, instead of sentence-split texts. We did this in order to provide the translators with greater freedom and, hopefully, to improve the quality of the translation. Allowing translators to merge and split sentences removes one of the “translation shifts” identified by Popovic (2019), which can make translations create solely for MT evaluation different from translations produced for other purposes. We first show some descriptive statistics of the source texts, for Czech, English and German, in 3 Europarl Parallel Corpus Czech ↔ English German ↔ English Polish↔ English German ↔ French Sentences 645,241 1,825,745 632,435 1,801,076 Words 14,948,900 17,380,340 48,125,573 50,506,059 14,691,199 16,995,232 47,517,102 55,366,136 Distinct words 172,452 63,289 371,748 113,960 170,271 62,694 368,585 134,762 News Commentary Parallel Corpus Czech ↔ English 248,927 5,570,734 6,156,063"
2020.wmt-1.1,2020.wmt-1.26,0,0.0845151,"Missing"
2020.wmt-1.1,2020.vardial-1.10,0,0.0933804,"Missing"
2020.wmt-1.1,W18-6301,0,0.038239,"Missing"
2020.wmt-1.1,2020.wmt-1.51,0,0.0917045,"Missing"
2020.wmt-1.1,2020.wmt-1.50,1,0.78567,"Missing"
2020.wmt-1.1,2020.wmt-1.52,0,0.0485419,"Missing"
2020.wmt-1.1,P19-1164,0,0.0581517,"26 26.37 25.51 24.82 28.33 23.33 21.13 21.96 20.43 22.90 22.58 21.90 22.17 22.17 20.53 19.40 20.01 40.44 32.39 30.39 37.04 32.27 27.54 25.97 26.09 46.38 37.30 36.05 35.96 33.76 33.07 27.20 27.07 Table 15: TICO-19 test suite results on the English-to-X WMT20 translation directions. 26 4.1.5 antecedent (a less common direction of information flow), and then correctly express the noun in the target language. The success of the MT system in this test can be established automatically, whenever the gender of the target word can be automatically identified. Kocmi et al. (2020) build upon the WinoMT (Stanovsky et al., 2019) test set, which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously (for the human eye) refers to it based the situation described. When extending WinMT with Czech and Polish, Stanovsky et al. have to disregard some test patterns but the principle remains. The results indicate that all MT systems fail in this test, following gender bias (stereotypical patterns attributing the masculine gender to some professions and feminine gender to others) rather than the coreference link. Word Sense Disambiguation (Scherrer et"
2020.wmt-1.1,2020.wmt-1.31,0,0.0881792,"morphological segmentation of the polysynthetic Inuktitut, testing rule-based, supervised, semi-supervised as well as unsupervised word segmentation methods, (2) whether or not adding data from a related language (Greenlandic) helps, and (3) whether contextual word embeddings (XLM) improve translation. G RONINGEN - ENIU use Transformer implemented in Marian with the default setting, improving the performance also with tagged backtranslation, domain-specific data, ensembling and finetuning. 2.3.7 DONG - NMT (no associated paper) No description provided. 2.3.8 ENMT (Kim et al., 2020) Kim et al. (2020) base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. The model is then fine-tuned with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, final results are generated with an ensemble model and re-ranked with averaged models and language models. G RONINGEN - ENTAM (Dhar et al., 2020) study the effects of various techniques such as linguistically motivated segmenta"
2020.wmt-1.1,2020.wmt-1.32,0,0.0839317,"- NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ub"
2020.wmt-1.1,2020.wmt-1.33,0,0.080166,"Missing"
2020.wmt-1.1,2020.wmt-1.34,0,0.0803867,"Missing"
2020.wmt-1.1,2020.wmt-1.35,0,0.0951745,"ss web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics about the training and test materials are given in Figures 1, 2, 3 and 4. 4 8 ARIEL XV https://github.com/AppraiseDev/OCELoT English I English II English III Chinese Czech German Inuktitut Japanese Polish Russian Tamil ABC News (2), All Africa (5), Brisbane Times (1), CBS LA (1), CBS News (1), CNBC (3), CNN (2), Daily Express (1), Daily Mail (2), Fox News (1), Gateway (1), Guardian (3), Huffington Post (2), London Evening Standard (2), Metro (2), NDTV (7), RTE (7), Reuters (4), STV (2), S"
2020.wmt-1.1,2020.wmt-1.55,0,0.0877526,"Missing"
2020.wmt-1.1,P17-4012,0,0.0273892,"o SJTU-NICT using large XLM model to improve NMT but the exact relation is unclear. 2.3.14 H UAWEI TSC (Wei et al., 2020a) H UAWEI TSC use Transformer-big with a further increased model size, focussing on standard techniques of careful pre-processing and filtering, back-translation and forward translation, including self-training, i.e. translating one of the sides of the original parallel data. Ensembling of individual training runs is used in the forward as well as backward translation, and single models are created from the ensembles using knowledge distillation. The submission uses THUNMT (Zhang et al., 2017) open-source engine. 2.3.19 N IU T RANS (Zhang et al., 2020) N IU T RANS gain their performance from focussed attention to six areas: (1) careful data preprocessing and filtering, (2) iterative back-translation to generate additional training data, (3) using different model architectures, such as wider and/or deeper models, relative position representation and relative length, to enhance the diversity of translations, (4) iterative knowledge distillation by in-domain monolingual data, (5) iterative finetuning for domain adaptation using small training batches, (6) rule-based post-processing of"
2020.wmt-1.1,P98-2238,0,0.590812,"and punctuation, and we tend to attribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate th"
2020.wmt-1.1,2020.wmt-1.41,1,0.814291,"Missing"
2020.wmt-1.2,2020.lrec-1.226,1,0.794186,"= GRU(yt−1 , d0t−1 ) ct = Attention(A, query ← dt ) d0t Spen = Sadapt + (Simp − Scor ) (1) 0 = GRU (ct , dt ) ot = tanh(Wc ct + Wd d0t + Wy yt−1 ) lt = Wo (Wb ot + bb ) + bo with Sadapt being the score of the adapted system and Simp and Scor are the scores of this system where all sentences requested to the user simulation are considered entirely wrong and correct, respectively. Note that in the case of BLEU, the brevity penalty is not impacted by this calculation, only the correct n-gram counts will be decreased proportionally to the sentence requested for translation. For more details, see (Prokopalo et al., 2020). P (yt |X, Y<t ) = softmax(lt ) For a single training sample, we then maximise the joint likelihood of source and target sentences: L(X, Y ) = log (P (yt |X, Y<t )) (2) t=1 5 4 T X Baseline systems Adaptation techniques The first adaptation technique used is rather simple. It consists of selecting N sentences from training data that are the closest to the sentences in the document. The chosen similarity metric is the cosine between sentence embeddings obtained by a simple average of word embeddings, as described in (Arora et al., 2017). This data is then used to finetune the initial model for"
2020.wmt-1.2,E17-3017,0,0.0743157,"Missing"
2020.wmt-1.2,P16-1162,0,0.262589,"ings and GRU hidden states are set to 128 and 256, respectively. The embeddings are shared in the decoder (Press and Wolf, 2017). We use ADAM (Kingma and Ba, 2014) as the optimiser and set the learning rate and mini-batch size to 0.0004 and 64, respectively. Regularisation is done by means of a weight decay of 1e−5 and the use of dropout on the embeddings, the source context and the output (set at 0.4) (Srivastava et al., 2014). We clip the gradients if the norm of the full parameter vector exceeds 1 (Pascanu et al., 2013). The data is processed by a joint BPE model with 30k merge operations (Sennrich et al., 2016a). This leads to respectively 20.7k and 25.1k units for English and French and 17.2k and 26.5k units for English and German, respectively. We train each model for a maximum of 100 epochs and early stop the training if validation BLEU (Papineni et al., 2002) does not improve for 10 epochs (Figure 2). We also halve the learning rate if no improvement is obtained for three epochs. The number of learnable parameters is around 8.7M for En-Fr and 8.5M for En-De. Results show that a simple data selection method along with finetuning can provide a small improvement of the system’s performance for Eng"
2021.acl-short.60,N19-1423,0,0.0292784,"mposition which models interactions between visual (from CNN) and textual (from RNN) representations. Those systems exhibit rather low performance compared to those obtained on standard VQA, demonstrating that the corpus requires external knowledge to be solved correctly. Recent work has introduced methods to incorporate visual information to create Vision+Language BERT models through joint multimodal embeddings (Chen et al., 2020; Su et al., 2019; Lu et al., 2019). First, image and text are embedded into the same space, and then Transformer networks are applied as in the standard BERT model (Devlin et al., 2019). Our work is most similar to that of Shah et al. (2019) since the same preprocessing pipeline is used. However, our system does not use a memory network, and instead relies on on a BERT-based model (UNITER, see section 3) to model the relationship between question, facts, and image with self-attention layers. 3 Methodology To answer KVQA with Neural models, we first take the V+L BERT model UNITER (Chen et al., 2020) with the highest score on the commonsense VQA task, VCR (Zellers et al., 2019). In order to allow UNITER to accept external KG facts, we cast these facts to a textual form ‘Entity"
2021.acl-short.60,D15-1075,0,0.113647,"Missing"
2021.acl-short.60,K19-1063,0,0.0517608,"Missing"
2021.acl-short.60,W19-4828,0,0.0566364,"Missing"
2021.acl-short.60,2020.conll-1.45,0,0.0786985,"Missing"
2021.acl-short.60,2020.acl-main.469,0,0.0993039,"Missing"
2021.acl-short.60,P19-1487,0,0.0397214,"Missing"
2021.acl-short.60,P18-1238,0,0.0689464,"Missing"
2021.acl-short.60,2020.acl-main.357,0,0.0383506,"Missing"
2021.emnlp-main.51,I17-2052,0,0.0285663,"ties compared to its neighbors in the training set (Eq. 1, 2), suggesting that it may lie near the model’s decision boundary. To this end, our acquisition function selects the top b examples from the pool that have the highest score sxp (cf. line 8), that form the acquired batch Q. 3 3.1 Experimental Setup Tasks & Datasets We conduct experiments on sentiment analysis, topic classification, natural language inference and paraphrase detection tasks. We provide details for the datasets in Table 1. We follow Yuan et al. (2020) and use IMDB (Maas et al., 2011), SST-2 (Socher et al., 2013), PUBMED (Dernoncourt and Lee, 2017) and AGNEWS from Zhang et al. (2015) where we also acquired DBPEDIA. We experiment with tasks requiring pairs of input sequences, using QQP and QNLI from GLUE (Wang et al., 2019). To evaluate robustness on out-of-distribution (OOD) data, we follow Hendrycks et al. (2020) and use SST-2 as OOD dataset for IMDB and vice versa. We finally use TWITTERPPDB (Lan et al., 2017) as OOD data for QQP as in Desai and Durrett (2020). 3.2 Baselines uncertainty sampling, by computing gradient embeddings gx for every candidate data point x in Dpool and then using clustering to select a batch. Each gx is comput"
2021.emnlp-main.51,2020.emnlp-main.21,0,0.0203091,"paraphrase detection tasks. We provide details for the datasets in Table 1. We follow Yuan et al. (2020) and use IMDB (Maas et al., 2011), SST-2 (Socher et al., 2013), PUBMED (Dernoncourt and Lee, 2017) and AGNEWS from Zhang et al. (2015) where we also acquired DBPEDIA. We experiment with tasks requiring pairs of input sequences, using QQP and QNLI from GLUE (Wang et al., 2019). To evaluate robustness on out-of-distribution (OOD) data, we follow Hendrycks et al. (2020) and use SST-2 as OOD dataset for IMDB and vice versa. We finally use TWITTERPPDB (Lan et al., 2017) as OOD data for QQP as in Desai and Durrett (2020). 3.2 Baselines uncertainty sampling, by computing gradient embeddings gx for every candidate data point x in Dpool and then using clustering to select a batch. Each gx is computed as the gradient of the crossentropy loss with respect to the parameters of the model’s last layer, aiming to be the component that incorporates uncertainty in the acquisition function 7 . We also evaluate a recently introduced coldstart acquisition function called A LPS (Yuan et al., 2020) that uses the masked language model (MLM) loss of B ERT as a proxy for model uncertainty in the downstream classification task."
2021.emnlp-main.51,N19-1423,0,0.103054,"ing Loop Assuming a multi-class classification problem with C classes, labeled data for training Dlab and a pool of unlabeled data Dpool , we perform AL for T iterations. At each iteration, we train a model on Dlab and then use our proposed acquisition function, C AL (Algorithm 1), to acquire a batch Q consisting of b examples from Dpool . The acquired examples are then labeled4 , they are removed from the pool Dpool and added to the labeled dataset Dlab , which will serve as the training set for training a model in the next AL iteration. In our experiments, we use a pretrained B ERT model M (Devlin et al., 2019), which we fine-tune at each AL iteration using the current Dlab . We begin the AL loop by training a model M using an initial labeled dataset Dlab 5 . 4 We simulate AL, so we already have the labels of the examples of Dpool (but still treat it as an unlabeled dataset). 5 We acquire the first examples that form the initial training set Dlab by applying random stratified sampling (i.e. keeping the initial label distribution). Find Nearest Neighbors for Unlabeled Candidates The first step of our contrastive acquisition function (cf. line 2) is to find examples that are similar in the model featu"
2021.emnlp-main.51,2020.emnlp-main.638,0,0.171095,"d sentences Q and tokens from the rest of the data pool U . Following Yuan et al. (2020), we compute D IV.-I as the Jaccard similarity between the set of tokens from the sampled sentences Q, VQ , and the set of tokens from the unsampled sentences UQ, VQ0 , |VQ ∩VQ0 | J (VQ , VQ0 ) = |VQ ∪VQ0 |. A high D IV.-I value indicates high diversity because the sampled and unsampled sentences have many tokens in common. Diversity in feature space (D IV.-F) We next evaluate diversity in the (model) feature space, using the [CLS] representations of a trained B ERT model 11 . Following Zhdanov (2019) and Ein-Dor et al. (2020), we compute D IV.-F of a set Q as  −1 1 P min d(Φ(x ), Φ(x )) , where Φ(xi ) i j |U | xi ∈U xj ∈Q denotes the [CLS] output token of example xi obtained by the model which was trained using L, and d(Φ(xi ), Φ(xj )) denotes the Euclidean distance between xi and xj in the feature space. Uncertainty (U NC .) To measure uncertainty, we use the model Mf trained on the entire training dataset (Figure 2 - Full supervision). As in Yuan et al. (2020), we use the logits from the fully trained model to estimate the uncertainty of an example, as it is a reliable estimate due to its high performance afte"
2021.emnlp-main.51,D17-1063,0,0.0225474,", might be beneficial. Especially, if similar behavior appears in other NLP tasks too. Another interesting future direction for C AL, related to interpretability, would be to evaluate whether acquiring contrastive examples for the task (Kaushik et al., 2020; Gardner et al., 2020) is more beneficial than contrastive examples for the model, as we do in C AL. Hybrid There are several existing approaches that combine representative and uncertainty sampling. Such approaches include active learning algorithms that use meta-learning (Baram et al., 2004; Hsu and Lin, 2015) and reinforcement learning (Fang et al., 2017; Liu et al., 2018), aiming to learn a policy for switching between a diversitybased or an uncertainty-based criterion at each iteration. Recently, Ash et al. (2020) propose Batch Active learning by Diverse Gradient Embeddings (BADGE) and Yuan et al. (2020) propose Active Learning by Processing Surprisal (A LPS), a coldstart acquisition function specific for pretrained Acknowledgments language models. Both methods construct representations for the unlabeled data based on uncertainty, KM and NA are supported by Amazon through the and then use them for clustering; hence combining Alexa Fellowshi"
2021.emnlp-main.51,P04-1075,0,0.136048,"l, both approaches have core limitations that may lead to acquiring redundant data points. Algorithms based on uncertainty may end up choosing uncertain yet uninformative repetitive data, while diversity-based methods may tend to select diverse yet easy examples for the model (Roy and McCallum, 2001). The two approaches are orthogonal to each other, since uncertainty sampling is usually based on the model’s output, while diversity exploits information from the input (i.e. feature) space. Hybrid data acquisition functions that combine uncertainty and diversity sampling have also been proposed (Shen et al., 2004; Zhu et al., 2008; Ducoffe and Precioso, 2018; Ash et al., 2020; Yuan et al., 2020; Ru et al., 2020). Active learning (AL) is a machine learning paradigm for efficiently acquiring data for annotation from a (typically large) pool of unlabeled data (Lewis and Catlett, 1994; Cohn et al., 1996; Settles, 2009). Its goal is to concentrate the human labeling effort on the most informative data points In this work, we aim to leverage characteristics that will benefit model performance the most and from hybrid data acquisition. We hypothesize that thus reducing data annotation cost. data points that"
2021.emnlp-main.51,W17-2630,0,0.0261208,"e predictive entropy. Houlsby et al. (2011) 657 propose Bayesian Active Learning by Disagreement (BALD), a method that chooses data points that maximize the mutual information between predictions and model’s posterior probabilities. Gal et al. (2017) applied BALD for deep neural models using Monte Carlo dropout (Gal and Ghahramani, 2016) to acquire multiple uncertainty estimates for each candidate example. Least confidence, entropy and BALD acquisition functions have been applied in a variety of text classification and sequence labeling tasks, showing to substantially improve data efficiency (Shen et al., 2017; Siddhant and Lipton, 2018; Lowell and Lipton, 2019; Kirsch et al., 2019; Shelmanov et al., 2021; Margatina et al., 2021). Diversity Sampling On the other hand, diversity or representative sampling is based on selecting batches of unlabeled examples that are representative of the unlabeled pool, based on the intuition that a representative set of examples once labeled, can act as a surrogate for the full data available. In the context of deep learning, Geifman and El-Yaniv (2017) and Sener and Savarese (2018) select representative examples based on core-set construction, a fundamental problem"
2021.emnlp-main.51,D18-1318,0,0.0145939,"y. Houlsby et al. (2011) 657 propose Bayesian Active Learning by Disagreement (BALD), a method that chooses data points that maximize the mutual information between predictions and model’s posterior probabilities. Gal et al. (2017) applied BALD for deep neural models using Monte Carlo dropout (Gal and Ghahramani, 2016) to acquire multiple uncertainty estimates for each candidate example. Least confidence, entropy and BALD acquisition functions have been applied in a variety of text classification and sequence labeling tasks, showing to substantially improve data efficiency (Shen et al., 2017; Siddhant and Lipton, 2018; Lowell and Lipton, 2019; Kirsch et al., 2019; Shelmanov et al., 2021; Margatina et al., 2021). Diversity Sampling On the other hand, diversity or representative sampling is based on selecting batches of unlabeled examples that are representative of the unlabeled pool, based on the intuition that a representative set of examples once labeled, can act as a surrogate for the full data available. In the context of deep learning, Geifman and El-Yaniv (2017) and Sener and Savarese (2018) select representative examples based on core-set construction, a fundamental problem in computational geometry."
2021.emnlp-main.51,D13-1170,0,0.00365068,"e in model predicted probabilities compared to its neighbors in the training set (Eq. 1, 2), suggesting that it may lie near the model’s decision boundary. To this end, our acquisition function selects the top b examples from the pool that have the highest score sxp (cf. line 8), that form the acquired batch Q. 3 3.1 Experimental Setup Tasks & Datasets We conduct experiments on sentiment analysis, topic classification, natural language inference and paraphrase detection tasks. We provide details for the datasets in Table 1. We follow Yuan et al. (2020) and use IMDB (Maas et al., 2011), SST-2 (Socher et al., 2013), PUBMED (Dernoncourt and Lee, 2017) and AGNEWS from Zhang et al. (2015) where we also acquired DBPEDIA. We experiment with tasks requiring pairs of input sequences, using QQP and QNLI from GLUE (Wang et al., 2019). To evaluate robustness on out-of-distribution (OOD) data, we follow Hendrycks et al. (2020) and use SST-2 as OOD dataset for IMDB and vice versa. We finally use TWITTERPPDB (Lan et al., 2017) as OOD data for QQP as in Desai and Durrett (2020). 3.2 Baselines uncertainty sampling, by computing gradient embeddings gx for every candidate data point x in Dpool and then using clustering"
2021.emnlp-main.51,C08-1143,0,0.0542557,"have core limitations that may lead to acquiring redundant data points. Algorithms based on uncertainty may end up choosing uncertain yet uninformative repetitive data, while diversity-based methods may tend to select diverse yet easy examples for the model (Roy and McCallum, 2001). The two approaches are orthogonal to each other, since uncertainty sampling is usually based on the model’s output, while diversity exploits information from the input (i.e. feature) space. Hybrid data acquisition functions that combine uncertainty and diversity sampling have also been proposed (Shen et al., 2004; Zhu et al., 2008; Ducoffe and Precioso, 2018; Ash et al., 2020; Yuan et al., 2020; Ru et al., 2020). Active learning (AL) is a machine learning paradigm for efficiently acquiring data for annotation from a (typically large) pool of unlabeled data (Lewis and Catlett, 1994; Cohn et al., 1996; Settles, 2009). Its goal is to concentrate the human labeling effort on the most informative data points In this work, we aim to leverage characteristics that will benefit model performance the most and from hybrid data acquisition. We hypothesize that thus reducing data annotation cost. data points that are close in the m"
2021.emnlp-main.51,2020.emnlp-main.746,0,0.0834467,"Missing"
2021.emnlp-main.51,2020.emnlp-main.637,0,0.117787,"points. Algorithms based on uncertainty may end up choosing uncertain yet uninformative repetitive data, while diversity-based methods may tend to select diverse yet easy examples for the model (Roy and McCallum, 2001). The two approaches are orthogonal to each other, since uncertainty sampling is usually based on the model’s output, while diversity exploits information from the input (i.e. feature) space. Hybrid data acquisition functions that combine uncertainty and diversity sampling have also been proposed (Shen et al., 2004; Zhu et al., 2008; Ducoffe and Precioso, 2018; Ash et al., 2020; Yuan et al., 2020; Ru et al., 2020). Active learning (AL) is a machine learning paradigm for efficiently acquiring data for annotation from a (typically large) pool of unlabeled data (Lewis and Catlett, 1994; Cohn et al., 1996; Settles, 2009). Its goal is to concentrate the human labeling effort on the most informative data points In this work, we aim to leverage characteristics that will benefit model performance the most and from hybrid data acquisition. We hypothesize that thus reducing data annotation cost. data points that are close in the model feature space The most widely used approaches to acquiring ("
C14-2028,2013.mtsummit-papers.5,1,0.880457,"Missing"
C14-2028,2013.mtsummit-wptp.13,1,0.900099,"Missing"
C14-2028,2012.amta-papers.22,1,0.865102,"Missing"
C14-2028,2013.mtsummit-wptp.10,0,0.135035,"Missing"
C14-2028,W13-2231,1,0.678754,"Missing"
C14-2028,P14-1067,1,0.806908,"Missing"
C14-2028,2013.mtsummit-wptp.7,1,\N,Missing
D17-1070,S14-2010,0,0.0512224,"ion x and the image y to the same embedding space. We use a margin α = 0.2 and 30 contrastive terms. We use the same splits as in (Karpathy and Fei-Fei, 2015), i.e., we use 113k images from the COCO dataset (each containing 5 captions) for training, 5k images for validation and 5k images for test. For evaluation, we split the 5k images in 5 random sets of 1k images on which we compute Recall@K, with K ∈ {1, 5, 10} and STS14 - Semantic Textual Similarity While semantic relatedness is supervised in the case of SICK-R, we also evaluate our embeddings on the 6 unsupervised SemEval tasks of STS14 (Agirre et al., 2014). This dataset includes subsets of news articles, forum discussions, image descriptions and headlines from news articles containing pairs of sentences (lower-cased), labeled with 3 https://www.github.com/ facebookresearch/SentEval 674 name SNLI task NLI N 560k SICK-E NLI 10k SICK-R STS 10k STS14 STS 4.5k premise ”Two women are embracing while holding to go packages.” A man is typing on a machine used for stenography ”A man is singing a song and playing the guitar” ”Liquid ammonia leak kills 15 in Shanghai” hypothesis ”Two woman are holding packages.” label entailment The man isn’t operating a"
D17-1070,D15-1075,0,0.924908,"vel understanding task that involves reasoning about the semantic relationships within sentences. Unlike in computer vision, where convolutional neural networks are predominant, there are multiple ways to encode a sentence using neural networks. Hence, we investigate the impact of the sentence encoding architecture on representational transferability, and compare convolutional, recurrent and even simpler word composition schemes. Our experiments show that an encoder based on a bi-directional LSTM architecture with max pooling, trained on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), yields state-of-the-art sentence embeddings comMany modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently ou"
D17-1070,N13-1092,0,0.0224448,"Missing"
D17-1070,N16-1162,0,0.72628,"Missing"
D17-1070,marelli-etal-2014-sick,0,0.201032,"arch/InferSent 670 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics machine translation data (using the WMT’14 English/French and English/German pairs), dictionary definitions and image captioning data from the COCO dataset (Lin et al., 2014). These models obtained significantly lower results compared to the unsupervised Skip-Thought approach. Recent work has explored training sentence encoders on the SNLI corpus and applying them on the SICK corpus (Marelli et al., 2014), either using multi-task learning or pretraining (Mou et al., 2016; Bowman et al., 2015). The results were inconclusive and did not reach the same level as simpler approaches that directly learn a classifier on top of unsupervised sentence embeddings instead (Arora et al., 2017). To our knowledge, this work is the first attempt to fully exploit the SNLI corpus for building generic sentence encoders. As we show in our experiments, we are able to consistently outperform unsupervised approaches, even if our models are trained on much less (but humanannotated) data. pared to all existing alternat"
D17-1070,D13-1090,0,0.0314371,"8.7 88.2 68.4/76.8 72.7/80.9 75.1/82.3 76.2/83.1 0.849 0.863 0.885 0.884 83.1 83.1 86.3 86.3 .46/.42 .67/.70 .43/.42 .71/ .55/.54 .68/.65 .70/.67 92.4 - 80.4/85.9 - 0.868 84.5 - - Table 4: Transfer test results for various architectures trained in different ways. Underlined are best results for transfer learning approaches, in bold are best results among the models trained in the same way. † indicates methods that we trained, other transfer models have been extracted from (Hill et al., 2016). For best published supervised methods (no transfer), we consider AdaSent (Zhao et al., 2015), TF-KLD (Ji and Eisenstein, 2013), Tree-LSTM (Tai et al., 2015) and Illinois-LH system (Lai and Hockenmaier, 2014). (*) Our model trained on SST obtained 83.4 for MR and 86.0 for SST (MR and SST come from the same source), which we do not put in the tables for fair comparison with transfer methods. regard to the embedding size. 5.2 Since it is easier to linearly separate in high dimension, especially with logistic regression, it is not surprising that increased embedding sizes lead to increased performance for almost all models. However, this is particularly true for some models (BiLSTM-Max, HConvNet, inner-att), which demons"
D17-1070,D16-1046,0,0.0556965,"ds in Natural Language Processing, pages 670–680 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics machine translation data (using the WMT’14 English/French and English/German pairs), dictionary definitions and image captioning data from the COCO dataset (Lin et al., 2014). These models obtained significantly lower results compared to the unsupervised Skip-Thought approach. Recent work has explored training sentence encoders on the SNLI corpus and applying them on the SICK corpus (Marelli et al., 2014), either using multi-task learning or pretraining (Mou et al., 2016; Bowman et al., 2015). The results were inconclusive and did not reach the same level as simpler approaches that directly learn a classifier on top of unsupervised sentence embeddings instead (Arora et al., 2017). To our knowledge, this work is the first attempt to fully exploit the SNLI corpus for building generic sentence encoders. As we show in our experiments, we are able to consistently outperform unsupervised approaches, even if our models are trained on much less (but humanannotated) data. pared to all existing alternative unsupervised approaches like SkipThought or FastSent, while bei"
D17-1070,D14-1162,0,0.115818,"erence datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available1 . 1 Holger Schwenk Facebook AI Research schwenk@fb.com Introduction Distributed representations of words (or word embeddings) (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) have shown to provide useful features for various tasks in natural language processing and computer vision. While there seems to be a consensus concerning the usefulness of word embeddings and how to learn them, this is not yet clear with regard to representations that carry the meaning of a full sentence. That is, how to capture the 1 https://www.github.com/ facebookresearch/InferSent 670 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics machine tran"
D17-1070,S14-2055,0,0.0154055,"3.1 86.3 86.3 .46/.42 .67/.70 .43/.42 .71/ .55/.54 .68/.65 .70/.67 92.4 - 80.4/85.9 - 0.868 84.5 - - Table 4: Transfer test results for various architectures trained in different ways. Underlined are best results for transfer learning approaches, in bold are best results among the models trained in the same way. † indicates methods that we trained, other transfer models have been extracted from (Hill et al., 2016). For best published supervised methods (no transfer), we consider AdaSent (Zhao et al., 2015), TF-KLD (Ji and Eisenstein, 2013), Tree-LSTM (Tai et al., 2015) and Illinois-LH system (Lai and Hockenmaier, 2014). (*) Our model trained on SST obtained 83.4 for MR and 86.0 for SST (MR and SST come from the same source), which we do not put in the tables for fair comparison with transfer methods. regard to the embedding size. 5.2 Since it is easier to linearly separate in high dimension, especially with logistic regression, it is not surprising that increased embedding sizes lead to increased performance for almost all models. However, this is particularly true for some models (BiLSTM-Max, HConvNet, inner-att), which demonstrate unequal abilities to incorporate more information as the size grows. We hyp"
D17-1070,D16-1157,0,0.0885981,"information or semantics of the input data by specializing too much on these biases. Learning models on large unsupervised task makes it harder for the model to specialize. Littwin and Wolf (2016) showed that co-adaptation of encoders and classifiers, when trained end-to-end, can negatively impact the generalization power of image features generated by an encoder. They propose a loss that incorporates multiple orthogonal classifiers to counteract this effect. Recent work on generating sentence embeddings range from models that compose word embeddings (Le and Mikolov, 2014; Arora et al., 2017; Wieting et al., 2016b) to more complex neural network architectures. SkipThought vectors (Kiros et al., 2015) propose an objective function that adapts the skip-gram model for words (Mikolov et al., 2013) to the sentence level. By encoding a sentence to predict the sentences around it, and using the features in a linear model, they were able to demonstrate good performance on 8 transfer tasks. They further obtained better results using layer-norm regularization of their model in (Ba et al., 2016). Hill et al. (2016) showed that the task on which sentence embeddings are trained significantly impacts their quality."
D17-1070,P15-1150,0,\N,Missing
D17-1070,W14-4012,0,\N,Missing
D17-1070,L18-1269,1,\N,Missing
E17-1104,C14-1008,0,0.0841201,"ngth of the sentence and the position of this layer in the network. (Zhang et al., 2015) were the first to perform sentiment analysis entirely at the character level. Their systems use up to six convolutional layers, followed by three fully connected classification layers. Convolutional kernels of size 3 and 7 are used, as well as simple max-pooling layers. Another interesting aspect of this paper is the introduction of several large-scale data sets for text classification. We use the same experimental setting (see section 4.1). The use of character level information was also proposed by (Dos Santos and Gatti, 2014): all the character embeddings of one word are combined by a max operation and they are then jointly used with the word embedding information in a shallow architecture. In parallel to our work, (Yang et al., 2016) proposed a based hierarchical attention network for document classification that perform an attention first on the sentences in the document, and on the words in the sentence. Their architecture performs very well on datasets whose samples contain multiple sentences. In the computer vision community, the combination of recurrent and convolutional networks in one architecture has also"
E17-1104,P14-1062,0,0.410003,"h are shared for all nodes (Socher et al., 2011). The state of the top node is fed to the classifier. A recurrent neural net1108 work (RNN) could be considered as a special case of a recursive NN: the combination is performed sequentially, usually from left to right. The last state of the RNN is used as fixed-sized representation of the sentence, or eventually a combination of all the hidden states. First works using convolutional neural networks for NLP appeared in (Collobert and Weston, 2008; Collobert et al., 2011). They have been subsequently applied to sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015). We will discuss these techniques in more detail below. If not otherwise stated, all approaches operate on words which are projected into a high-dimensional space. A rather shallow neural net was proposed in (Kim, 2014): one convolutional layer (using multiple widths and filters) followed by a max pooling layer over time. The final classifier uses one fully connected layer with drop-out. Results are reported on six data sets, in particular Stanford Sentiment Treebank (SST). A similar system was proposed in (Kalchbrenner et al., 2014), but using five convolutional layers."
E17-1104,D14-1181,0,0.0559411,"eights which are shared for all nodes (Socher et al., 2011). The state of the top node is fed to the classifier. A recurrent neural net1108 work (RNN) could be considered as a special case of a recursive NN: the combination is performed sequentially, usually from left to right. The last state of the RNN is used as fixed-sized representation of the sentence, or eventually a combination of all the hidden states. First works using convolutional neural networks for NLP appeared in (Collobert and Weston, 2008; Collobert et al., 2011). They have been subsequently applied to sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015). We will discuss these techniques in more detail below. If not otherwise stated, all approaches operate on words which are projected into a high-dimensional space. A rather shallow neural net was proposed in (Kim, 2014): one convolutional layer (using multiple widths and filters) followed by a max pooling layer over time. The final classifier uses one fully connected layer with drop-out. Results are reported on six data sets, in particular Stanford Sentiment Treebank (SST). A similar system was proposed in (Kalchbrenner et al., 2014), but using"
E17-1104,P16-1162,0,0.0127549,"ve been applied to text processing. 1 Introduction The goal of natural language processing (NLP) is to process text with computers in order to analyze it, to extract information and eventually to represent the same information differently. We may want to associate categories to parts of the text (e.g. POS tagging or sentiment analysis), structure text differently (e.g. parsing), or convert it to some other form which preserves all or part of the content (e.g. machine translation, summarization). The level of granularity of this processing can range from individual characters to subword units (Sennrich et al., 2016) or words up to whole sentences or even paragraphs. After a couple of pioneer works (Bengio et al. (2001), Collobert and Weston (2008), Collobert et al. (2011) among others), the use of neural networks for NLP applications is attracting huge interest in the research community and they are systematically applied to all NLP tasks. However, while the use of (deep) neural networks in NLP has shown very good results for many tasks, it seems that they have not yet reached the level to outperform the state-of-the-art by a large margin, as it was observed in computer vision and speech recognition. Con"
E17-1104,D11-1014,0,0.0202546,"een projected into a low-dimensional space, and these embeddings are combined to obtain a fixed size representation of the input sentence, which then serves as input for the classifier. The simplest combination is the element-wise mean. This usually performs badly since all notion of token order is disregarded. Another class of approaches are recursive neural networks. The main idea is to use an external tool, namely a parser, which specifies the order in which the word embeddings are combined. At each node, the left and right context are combined using weights which are shared for all nodes (Socher et al., 2011). The state of the top node is fed to the classifier. A recurrent neural net1108 work (RNN) could be considered as a special case of a recursive NN: the combination is performed sequentially, usually from left to right. The last state of the RNN is used as fixed-sized representation of the sentence, or eventually a combination of all the hidden states. First works using convolutional neural networks for NLP appeared in (Collobert and Weston, 2008; Collobert et al., 2011). They have been subsequently applied to sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015)."
E17-1104,N16-1174,0,0.221121,"ollowed by three fully connected classification layers. Convolutional kernels of size 3 and 7 are used, as well as simple max-pooling layers. Another interesting aspect of this paper is the introduction of several large-scale data sets for text classification. We use the same experimental setting (see section 4.1). The use of character level information was also proposed by (Dos Santos and Gatti, 2014): all the character embeddings of one word are combined by a max operation and they are then jointly used with the word embedding information in a shallow architecture. In parallel to our work, (Yang et al., 2016) proposed a based hierarchical attention network for document classification that perform an attention first on the sentences in the document, and on the words in the sentence. Their architecture performs very well on datasets whose samples contain multiple sentences. In the computer vision community, the combination of recurrent and convolutional networks in one architecture has also been investigated, with the goal to “get the best of both worlds”, e.g. (Pinheiro and Collobert, 2014). The same idea was recently applied to sentence classification (Xiao and Cho, 2016). A convolutional network"
F12-2039,2010.jeptalnrecital-long.28,0,0.0713715,"Missing"
F12-2039,C04-1151,0,0.0788859,"Missing"
F12-2039,W11-1209,0,0.0222291,"Missing"
F12-2039,P07-2045,0,0.00730689,"Missing"
F12-2039,J05-4003,0,0.0937007,"Missing"
F12-2039,P02-1040,0,0.0932168,"Missing"
F12-2039,J03-3002,0,0.0988077,"Missing"
F12-2039,2011.iwslt-evaluation.10,1,0.896882,"Missing"
F12-2039,P03-1010,0,0.0426264,"Missing"
I11-1148,D10-1044,0,0.0511251,"data often comes from different sources, e.g. Europarl, UN, in-domain data in limited amounts, data crawled from the Internet or even bitexts automatically extracted from comparable corpora. It seems obvious that the appropriateness and the usefulness of this parallel data for There have been several attempts in the literature to address some of these problems. Matsoukas et al. (2009) proposed to weight each sentence in the training bitexts by optimizing a discriminative function on a tuning set. Sentencelevel features are extracted to estimate the weights that are relevant to the given task. Foster et al. (2010) proposed an extended approach by an instant weighting scheme which learns weights on individual phrase pairs instead of sentences and incorporated the instance-weighting model into a linear combination of feature functions. The technique presented in this paper is related to these previous works as it concerns the weighting of corpora or sentences. However, it does not 1323 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1323–1331, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Alignments obtained from time-stamped training parallel text"
I11-1148,2010.amta-papers.21,0,0.597558,"to extract and score the phrase pairs by simple relative frequency. Doing this, the parallel data is (wrongly) considered as one homogeneous pool of knowledge. We argue that the parallel data is quite inhomogeneous in many practical applications with respect to several factors: During the last years there is increasing interest in methods that perform some kind of weighting of heterogeneous parallel training data when building a statistical machine translation system. It was for instance observed that training data that is close to the period of the test data is more valuable than older data (Hardt and Elming, 2010; Levenberg et al., 2010). In this paper we obtain such a weighting by resampling alignments using weights that decrease with the temporal distance of bitexts to the test set. By these means, we can use all the available bitexts and still put an emphasis on the most recent one. The main idea of our approach is to use a parametric form or meta-weights for the weighting of the different parts of the bitexts. This ensures that our approach has only few parameters to optimize. We report experimental results on the Europarl corpus, translating from French to English and further verified it on the o"
I11-1148,P07-2045,0,0.00393663,"be used to achieve best weights. Only one weight is given to each time span, consequently the span size will have an impact on the alignment selection process. Using smaller spans results in a more fine grained weighting scheme. We have tested different settings with different time spans to see whether the impact of weighting changes with the { { 500k Dev=2.5k Test=2.5k Our first experiments are based on the FrenchEnglish portion of the freely available timestamped Europarl data (Koehn, 2005) from April 1996 to December 2010. We have built several phrase-based systems using the Moses toolkit (Koehn et al., 2007), though our approach is equally applicable to any other approach based on alignments and could be used for any language pairs. In our system, fourteen feature functions are used. These feature functions include phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and phrase penalty, and the target language model. The coefficients of these feature functions are optimized by minimum error training. In the first experiments, the whole Europarl corpus was split into train, development and test as shown in Figure 3. The most r"
I11-1148,2005.mtsummit-papers.11,0,0.0293589,"iament. As an example Figure 4 shows the histogram of the data per year. One can ask which time granularity should be used to achieve best weights. Only one weight is given to each time span, consequently the span size will have an impact on the alignment selection process. Using smaller spans results in a more fine grained weighting scheme. We have tested different settings with different time spans to see whether the impact of weighting changes with the { { 500k Dev=2.5k Test=2.5k Our first experiments are based on the FrenchEnglish portion of the freely available timestamped Europarl data (Koehn, 2005) from April 1996 to December 2010. We have built several phrase-based systems using the Moses toolkit (Koehn et al., 2007), though our approach is equally applicable to any other approach based on alignments and could be used for any language pairs. In our system, fourteen feature functions are used. These feature functions include phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and phrase penalty, and the target language model. The coefficients of these feature functions are optimized by minimum error training. In th"
I11-1148,N10-1062,0,0.476804,"phrase pairs by simple relative frequency. Doing this, the parallel data is (wrongly) considered as one homogeneous pool of knowledge. We argue that the parallel data is quite inhomogeneous in many practical applications with respect to several factors: During the last years there is increasing interest in methods that perform some kind of weighting of heterogeneous parallel training data when building a statistical machine translation system. It was for instance observed that training data that is close to the period of the test data is more valuable than older data (Hardt and Elming, 2010; Levenberg et al., 2010). In this paper we obtain such a weighting by resampling alignments using weights that decrease with the temporal distance of bitexts to the test set. By these means, we can use all the available bitexts and still put an emphasis on the most recent one. The main idea of our approach is to use a parametric form or meta-weights for the weighting of the different parts of the bitexts. This ensures that our approach has only few parameters to optimize. We report experimental results on the Europarl corpus, translating from French to English and further verified it on the official WMT’11 task, tran"
I11-1148,D09-1074,0,0.0584299,"ntroduction Statistical machine translation (SMT) systems are based on two types of resources: monolingual data to build a language model (LM) and bilingual data – also called bitexts – to train the translation model (TM). The parallel data often comes from different sources, e.g. Europarl, UN, in-domain data in limited amounts, data crawled from the Internet or even bitexts automatically extracted from comparable corpora. It seems obvious that the appropriateness and the usefulness of this parallel data for There have been several attempts in the literature to address some of these problems. Matsoukas et al. (2009) proposed to weight each sentence in the training bitexts by optimizing a discriminative function on a tuning set. Sentencelevel features are extracted to estimate the weights that are relevant to the given task. Foster et al. (2010) proposed an extended approach by an instant weighting scheme which learns weights on individual phrase pairs instead of sentences and incorporated the instance-weighting model into a linear combination of feature functions. The technique presented in this paper is related to these previous works as it concerns the weighting of corpora or sentences. However, it doe"
I11-1148,P00-1056,0,0.242428,"Missing"
I11-1148,W11-2158,1,0.838473,"e extracted from different parallel sentences coming from different time spans. Furthermore, weighting the alignments with their scores has shown improvements in the BLEU score as presented in Table 3, but considering the alignment score at the phrase level is not straight forward. 6 Experiments on the WMT task To further verify whether our results are robust beyond the narrow experimental conditions, we considered a task where the development and test data do not come from the same source than the bitexts. We took the official test sets of the 2011 WMT translation tasks as dev and test sets (Schwenk et al., 2011) i.e news-test09 and news-test10 respectively. We built English-French systems by using the Europarl and News-Commentary (NC) corpora, both contain news data over a long time period. For this set-up, there are three coefficients to optimize: the decay factor for Europarl λ1 , the decay factor for the news-commentary texts λ2 and a coefficient for the alignments α. The Europarl corpus was divided into time span according to years and NC corpus was assumed to be sorted over time since time-stamp information was not available for the NC corpus. Remaining settings are kept same as mentioned in pre"
I11-1148,W10-1759,1,0.764199,"45 0.4 !t Normalized weights 0.05 0.09 .................................. 0.17 0.20 Resampling with replacement 1996 1997 .......................... 2009 2010 Resampled alignments used to build Phrase Table Figure 1: Overview of the weighting scheme. The alignments are weighted by an exponential decay function, parameterized by λ. Resampling with replacement is used to create a new corpus (parts with higher weight will appear more often). The phrase table is built from this corpus using the standard procedure. require the calculation of additional sentence-level features. In our previous work Shah et al. (2010) we proposed a technique to weight heterogeneous data by weighted resampling of the alignments. The weights were numerically optimized on development data. Hardt and Elming (2010) has shown recency effect in terms of file-context and concluded that the data within the same file is of greater importance than the rest. Levenberg et al. (2010) proposed an incremental training procedure to deal with a continuous stream of parallel text. Word alignment was performed by the stepwise online EM algorithm and the phrase table was represented with suffix arrays. The authors showed that it is better to u"
I13-1033,C10-1073,0,0.0933475,"arallel data than the sentence extraction method for each TER threshold, and this difference of quantities improves results of MT system until the TER threshold of 80 is reached. However, we can see in Table 4 that the quality of only 39.35k (TER 80) extracted by SentExtract can have exactly the same impact of 25.3M extracted by our new technique. That is why we intend to investigate in the filtering module of our system. 4 Related Work Research on exploiting comparable corpora goes back to more than 15 years ago (Fung and Yee, 1998; Koehn and Knight, 2000; Vogel, 2003; Gaussier et al., 2004; Li and Gaussier, 2010). A lot of studies on data acquisition from comparable corpora for machine translation have been reported (Su and Babych, 2012; Hewavitharana and Vogel, 2011; Riesa and Marcu, 2012). To the best of our knowledge (Munteanu and 289 TER 0 10 20 30 40 50 60 70 80 90 100 Baseline BLEU score SentExtract 22.86 22.97 23.06 22.95 22.92 23.26 23.10 23.29 23.40 23.39 23.34 22.93 BLEU score PhrExtract 23.39 23.35 23.53 23.39 23.45 23.54 23.70 23.41 23.40 23.18 23.26 - # tokens (fr) SentExtract 55 313 1.7k 6.9k 23.5k 62.4k 13.82k 25.15k 39.35k 57.54k 83.60k 60.1M # tokens (fr) PhrExtract 1.06M 1.4M 2.5M 4."
I13-1033,C04-1151,0,0.263326,"nk Universit´e du Maine, Avenue Olivier Messiaen F-72085 - LE MANS, France FirstName.LastName@lium.univ-lemans.fr Abstract beneficial in order to overcome the lack of parallel data. The ability to detect these parallel data enables the automatic creation of large parallel corpora. Most of existing studies dealing with comparable corpora look for parallel data at the sentence level (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; AbdulRauf and Schwenk, 2011). However, the degree of parallelism can vary considerably, from noisy parallel texts, to quasi parallel texts (Fung and Cheung, 2004). Corpora from the last category contain none or few good parallel sentence pairs. However, there could have parallel phrases in comparable sentences that can prove to be helpful for SMT (Munteanu and Marcu, 2006). As an example, consider Figure 1, which presents two news articles with their video from the English and French editions of the Euronews website1 . The articles report on the same event with different sentences that contain some parallel translations at the phrase level. These two documents contain in particular no exact sentence pairs, so techniques for extracting parallel sentence"
I13-1033,J05-4003,0,0.105594,"ignificant improvements over a state-ofthe-art baseline. 1 Introduction The development of a statistical machine translation (SMT) system requires one or more parallel corpora called bitexts for training the translation model and monolingual data to build the target language model. Unfortunately, parallel texts are a limited resource and they are often not available for some specific domains and language pairs. That is why, recently, there has been a huge interest in the automatic creation of parallel data. Since comparable corpora exist in large quantities and are much more easily available (Munteanu and Marcu, 2005), the ability to exploit them is highly 1 www.euronews.com/ 286 International Joint Conference on Natural Language Processing, pages 286–292, Nagoya, Japan, 14-18 October 2013. Multimodal Comparable Corpora Audio L1 ASR Comparable audio Sentences L1 Split Manual Phrases L1 Transcription Texts L2 Manual Transcription SMT Comparable texts Parallel Data Split Translations L2 IR Phrases L2 Figure 1: Example of multimodal comparable corpora from the Euronews website. Phrases L2 Filter Figure 3: Principle of the parallel phrase extraction system from multimodal comparable corpora. 2.2 System Archite"
I13-1033,P98-1069,0,0.15657,"Table 4. The benefit of our method is that it can generates more quantities of parallel data than the sentence extraction method for each TER threshold, and this difference of quantities improves results of MT system until the TER threshold of 80 is reached. However, we can see in Table 4 that the quality of only 39.35k (TER 80) extracted by SentExtract can have exactly the same impact of 25.3M extracted by our new technique. That is why we intend to investigate in the filtering module of our system. 4 Related Work Research on exploiting comparable corpora goes back to more than 15 years ago (Fung and Yee, 1998; Koehn and Knight, 2000; Vogel, 2003; Gaussier et al., 2004; Li and Gaussier, 2010). A lot of studies on data acquisition from comparable corpora for machine translation have been reported (Su and Babych, 2012; Hewavitharana and Vogel, 2011; Riesa and Marcu, 2012). To the best of our knowledge (Munteanu and 289 TER 0 10 20 30 40 50 60 70 80 90 100 Baseline BLEU score SentExtract 22.86 22.97 23.06 22.95 22.92 23.26 23.10 23.29 23.40 23.39 23.34 22.93 BLEU score PhrExtract 23.39 23.35 23.53 23.39 23.45 23.54 23.70 23.41 23.40 23.18 23.26 - # tokens (fr) SentExtract 55 313 1.7k 6.9k 23.5k 62.4k"
I13-1033,P06-1011,0,0.117389,"parallel data enables the automatic creation of large parallel corpora. Most of existing studies dealing with comparable corpora look for parallel data at the sentence level (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; AbdulRauf and Schwenk, 2011). However, the degree of parallelism can vary considerably, from noisy parallel texts, to quasi parallel texts (Fung and Cheung, 2004). Corpora from the last category contain none or few good parallel sentence pairs. However, there could have parallel phrases in comparable sentences that can prove to be helpful for SMT (Munteanu and Marcu, 2006). As an example, consider Figure 1, which presents two news articles with their video from the English and French editions of the Euronews website1 . The articles report on the same event with different sentences that contain some parallel translations at the phrase level. These two documents contain in particular no exact sentence pairs, so techniques for extracting parallel sentences will not give good results. We need a method to extract parallel phrases which exist at the sub-sentential level. For some languages, text comparable corpora may not cover all topics in some specific domains and"
I13-1033,W08-0509,0,0.0370378,"Missing"
I13-1033,P03-1021,0,0.00575989,"(Koehn et al., 2007). The standard fourteen feature functions are used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model. It is constructed as follows. First, word alignments in both directions are calculated. We used the multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. The parameters of our system were tuned on a development corpus, using the MERT tool (Och, 2003). We use the Lemur IR toolkit (Ogilvie and Callan, 2001) for the phrases extraction procedure. We first index all the French text (after splitting it into segments) into a database using Indri Index. This feature enable us to index our text documents in such a way we can use the translated phrases as queries to run information retrieval in the database, with the specialized Indri Query Language. By these means we can retrieve the best matching phrases from the French side of the comparable corpus. For each candidate phrases pair, we need to decide whether the two phrases are mutual translation"
I13-1033,P04-1067,0,0.105618,"Missing"
I13-1033,P02-1040,0,0.088425,"Missing"
I13-1033,2007.mtsummit-papers.50,0,0.0186078,"n automatic speech recognition system, a statistical machine translation system and information retrieval system. We showed by experiments conducted on English-French data, that parallel phrases extracted with this method improves significantly SMT performance. Our approach can be improved in several aspects. The automatic splitting is very simple; more advanced phrases generation might work better, and eliminate redundancy. Trying other method on filtering can also improve the precision of the method. The second type of approach in extracting parallel phrases is the alignment-based approach (Quirk et al., 2007; Riesa and Marcu, 2012). These methods are promising, but since the proposed method in (Quirk et al., 2007) do not improve significantly MT performance and model in (Riesa and Marcu, 2012) is designed for parallel data, it’s hard to say that this approach is actually effective for comparable data. 6 Acknowledgments This work has been partially funded by the French Government under the project DEPART. References This work is similar to the work by (Afli et al., 2012) where the extraction is done at the phrase level instead of the sentence level. Our methodology is the first effort aimed at det"
I13-1033,W11-1209,0,0.0213336,"threshold of 80 is reached. However, we can see in Table 4 that the quality of only 39.35k (TER 80) extracted by SentExtract can have exactly the same impact of 25.3M extracted by our new technique. That is why we intend to investigate in the filtering module of our system. 4 Related Work Research on exploiting comparable corpora goes back to more than 15 years ago (Fung and Yee, 1998; Koehn and Knight, 2000; Vogel, 2003; Gaussier et al., 2004; Li and Gaussier, 2010). A lot of studies on data acquisition from comparable corpora for machine translation have been reported (Su and Babych, 2012; Hewavitharana and Vogel, 2011; Riesa and Marcu, 2012). To the best of our knowledge (Munteanu and 289 TER 0 10 20 30 40 50 60 70 80 90 100 Baseline BLEU score SentExtract 22.86 22.97 23.06 22.95 22.92 23.26 23.10 23.29 23.40 23.39 23.34 22.93 BLEU score PhrExtract 23.39 23.35 23.53 23.39 23.45 23.54 23.70 23.41 23.40 23.18 23.26 - # tokens (fr) SentExtract 55 313 1.7k 6.9k 23.5k 62.4k 13.82k 25.15k 39.35k 57.54k 83.60k 60.1M # tokens (fr) PhrExtract 1.06M 1.4M 2.5M 4.3M 7.02M 11.4M 13.8M 18.04M 25.3M 35.9M 45.3M - Table 4: Number of tokens extracted and BLEU scores on DevTED obtained with PhrExtract and SentExtract method"
I13-1033,N12-1061,0,0.0996885,"wever, we can see in Table 4 that the quality of only 39.35k (TER 80) extracted by SentExtract can have exactly the same impact of 25.3M extracted by our new technique. That is why we intend to investigate in the filtering module of our system. 4 Related Work Research on exploiting comparable corpora goes back to more than 15 years ago (Fung and Yee, 1998; Koehn and Knight, 2000; Vogel, 2003; Gaussier et al., 2004; Li and Gaussier, 2010). A lot of studies on data acquisition from comparable corpora for machine translation have been reported (Su and Babych, 2012; Hewavitharana and Vogel, 2011; Riesa and Marcu, 2012). To the best of our knowledge (Munteanu and 289 TER 0 10 20 30 40 50 60 70 80 90 100 Baseline BLEU score SentExtract 22.86 22.97 23.06 22.95 22.92 23.26 23.10 23.29 23.40 23.39 23.34 22.93 BLEU score PhrExtract 23.39 23.35 23.53 23.39 23.45 23.54 23.70 23.41 23.40 23.18 23.26 - # tokens (fr) SentExtract 55 313 1.7k 6.9k 23.5k 62.4k 13.82k 25.15k 39.35k 57.54k 83.60k 60.1M # tokens (fr) PhrExtract 1.06M 1.4M 2.5M 4.3M 7.02M 11.4M 13.8M 18.04M 25.3M 35.9M 45.3M - Table 4: Number of tokens extracted and BLEU scores on DevTED obtained with PhrExtract and SentExtract methods for each TER threshold"
I13-1033,2011.iwslt-evaluation.10,1,0.897826,"Missing"
I13-1033,N03-1017,0,0.0129603,"Missing"
I13-1033,2006.amta-papers.25,0,0.016165,"f multimodal comparable data coming from the TED website 2 . We have an audio source of a talk in English and its text translation in French. We think that we can extract parallel data from this corpora, at the sentence and the sub-sentential level. In this work we seek to adapt and to improve machine translation systems that suffer from resource deficiency by automatically extracting parallel data in specific domains. 2 Our technique is similar to that of (Munteanu and Marcu, 2006), but we bypass the need of the Log-Likelihood-Ratio lexicon by using a baseline SMT system and the TER measure (Snover et al., 2006) for filtering. We also report an extension of the work of (Afli et al., 2012) by splitting transcribed sentences and the text parts of the multimodal corpus into phrases with length between two to ten tokens. We extract from each sentence on the corpus all combinations of two to ten sequential words. http://www.ted.com/ 287 2.3 Schwenk, 2011),4 i.e. between automatic translation, and the phrases selected by IR. Baseline systems Our ASR system is a five-pass system based on the open-source CMU Sphinx toolkit 3 (version 3 and 4), similar to the LIUM’08 French ASR system described in (Del´eglise"
I13-1033,P07-2045,0,0.0128362,"Missing"
I13-1033,W12-0102,0,0.0147084,"system until the TER threshold of 80 is reached. However, we can see in Table 4 that the quality of only 39.35k (TER 80) extracted by SentExtract can have exactly the same impact of 25.3M extracted by our new technique. That is why we intend to investigate in the filtering module of our system. 4 Related Work Research on exploiting comparable corpora goes back to more than 15 years ago (Fung and Yee, 1998; Koehn and Knight, 2000; Vogel, 2003; Gaussier et al., 2004; Li and Gaussier, 2010). A lot of studies on data acquisition from comparable corpora for machine translation have been reported (Su and Babych, 2012; Hewavitharana and Vogel, 2011; Riesa and Marcu, 2012). To the best of our knowledge (Munteanu and 289 TER 0 10 20 30 40 50 60 70 80 90 100 Baseline BLEU score SentExtract 22.86 22.97 23.06 22.95 22.92 23.26 23.10 23.29 23.40 23.39 23.34 22.93 BLEU score PhrExtract 23.39 23.35 23.53 23.39 23.45 23.54 23.70 23.41 23.40 23.18 23.26 - # tokens (fr) SentExtract 55 313 1.7k 6.9k 23.5k 62.4k 13.82k 25.15k 39.35k 57.54k 83.60k 60.1M # tokens (fr) PhrExtract 1.06M 1.4M 2.5M 4.3M 7.02M 11.4M 13.8M 18.04M 25.3M 35.9M 45.3M - Table 4: Number of tokens extracted and BLEU scores on DevTED obtained with Ph"
I13-1033,P03-1010,0,0.304405,"Missing"
I13-1033,E03-1050,0,0.0417892,"t can generates more quantities of parallel data than the sentence extraction method for each TER threshold, and this difference of quantities improves results of MT system until the TER threshold of 80 is reached. However, we can see in Table 4 that the quality of only 39.35k (TER 80) extracted by SentExtract can have exactly the same impact of 25.3M extracted by our new technique. That is why we intend to investigate in the filtering module of our system. 4 Related Work Research on exploiting comparable corpora goes back to more than 15 years ago (Fung and Yee, 1998; Koehn and Knight, 2000; Vogel, 2003; Gaussier et al., 2004; Li and Gaussier, 2010). A lot of studies on data acquisition from comparable corpora for machine translation have been reported (Su and Babych, 2012; Hewavitharana and Vogel, 2011; Riesa and Marcu, 2012). To the best of our knowledge (Munteanu and 289 TER 0 10 20 30 40 50 60 70 80 90 100 Baseline BLEU score SentExtract 22.86 22.97 23.06 22.95 22.92 23.26 23.10 23.29 23.40 23.39 23.34 22.93 BLEU score PhrExtract 23.39 23.35 23.53 23.39 23.45 23.54 23.70 23.41 23.40 23.18 23.26 - # tokens (fr) SentExtract 55 313 1.7k 6.9k 23.5k 62.4k 13.82k 25.15k 39.35k 57.54k 83.60k 60"
I13-1033,C98-1066,0,\N,Missing
N15-1103,D11-1033,0,0.0329288,"ng the translators to post-edit translations of a Legal document from English into French (about 15k words) over five days (i.e. about 3k words/day). An in-domain adapted (DA) system was used as baseline system for the first day, before project adapted (PA) systems have taken over. 4.1 Domain adapted system Before the human translator starts working, our DA system is trained using an extracted subset of bilingual training data that is mostly relevant to our specific domain. The extraction process, widely known as data selection, is applied using cross-entropy difference algorithm proposed by (Axelrod et al., 2011)2 . In order to augment the amount of training data3 (about 22M words) we also select a bilingual subset from Europarl, JRC-Acquis, news commentary, software manuals of the OPUS corpus, translation memories and the United Nations corpus. About 700M additional newspaper monolingual data selected from WMT evaluation campaign are also used for language modeling. 4.2 Project adapted system Our project-adaptation scenario, which is repeated iteratively during the lifetime of the translation 2 3 We used the XenC tool for data selection DGT+ECB corpora (see http://opus.lingfil.uu.se) project, is achi"
N15-1103,P07-2045,0,0.00512288,"of data is translated every day by each translator. The systems are then adapted, individually for each translator, using previous days of work, and used by the translators during the next day, and so on. 3 4 Evaluation Protocol We defined an adaptation protocol with the goal to assess the same task with and without adaptation procedure. Like in (Guerberof, 2009; Plitt and Masselot, 2010), three professional translators were involved in a two parts experiment: during the first part, translators receive MT suggestions from a state-of-the-art domain-adapted engine built with the Moses toolkit (Koehn et al., 2007), without being adapted with the data generated during the translation of the project.For the second part, the MT suggestions are provided by a MT system which was previously adapted to the current project using the human translations of prior working days. Since we asked the same translators to post-edit the same document twice (i.e. with and without MT adaptation), the second run was launched after a sufficient delay: the human memory impact is reduced since translators worked on other projects in between. To measure the user productivity, we considered two performance indicators: (i) the po"
N15-1103,P02-1040,0,0.0934212,"th and without MT adaptation), the second run was launched after a sufficient delay: the human memory impact is reduced since translators worked on other projects in between. To measure the user productivity, we considered two performance indicators: (i) the post-editing effort measured with TER (Snover et al., 2006) which corresponds to the number of edits made individually by each translator, (ii) the time-to-edit rate expressed in number of translated words per hour. In addition to these two key indicators, we evaluated the translation quality using an automatic measure, namely BLEU score (Papineni et al., 2002). This measure is used to make sure that no regression in the translation quality is observed after several days 1002 Experimental framework We ran contrastive experiments by asking the translators to post-edit translations of a Legal document from English into French (about 15k words) over five days (i.e. about 3k words/day). An in-domain adapted (DA) system was used as baseline system for the first day, before project adapted (PA) systems have taken over. 4.1 Domain adapted system Before the human translator starts working, our DA system is trained using an extracted subset of bilingual trai"
N15-1103,2006.amta-papers.25,0,0.060854,"generated during the translation of the project.For the second part, the MT suggestions are provided by a MT system which was previously adapted to the current project using the human translations of prior working days. Since we asked the same translators to post-edit the same document twice (i.e. with and without MT adaptation), the second run was launched after a sufficient delay: the human memory impact is reduced since translators worked on other projects in between. To measure the user productivity, we considered two performance indicators: (i) the post-editing effort measured with TER (Snover et al., 2006) which corresponds to the number of edits made individually by each translator, (ii) the time-to-edit rate expressed in number of translated words per hour. In addition to these two key indicators, we evaluated the translation quality using an automatic measure, namely BLEU score (Papineni et al., 2002). This measure is used to make sure that no regression in the translation quality is observed after several days 1002 Experimental framework We ran contrastive experiments by asking the translators to post-edit translations of a Legal document from English into French (about 15k words) over five"
N15-1103,W09-0441,0,0.0199861,"mber of edits performed by the translator in order to obtain a suitable translation. The first column indicates the day of the experiment. The second column represents three SMT systems, namely: the baseline system adapted to the domain (DA), the same system with a CSLM (DA+CSLM) and the project adapted system (all models were updated, including the CSLM) noted “PA+CSLM-adapt”. The third, fourth and fifth columns represent respectively the TER scores for the three translators. The first score is calculated using the reference produced by the translator himself. It could be considered as HTER (Snover et al., 2009). The second score (in parenthesis) is calculated using the three references produced by the translators. The third score (in brackets) is calculated according to an official “generic” reference provided by the European Commission. By these additional results, we aim to assess whether their is a tendency of the systems to adapt strongly to the particular style of one translator, or whether they still perform well with respect to independent references. On day 1, only the DA and DA+CSLM systems are presented since the project adaptation can only start after the first working day. First of all,"
N15-1103,2009.mtsummit-btm.7,0,\N,Missing
N15-1103,W15-4006,1,\N,Missing
N19-1422,W18-6402,1,0.82762,"d into the model. 1 Pranava Madhyastha Imperial College London pranava@imperial.ac.uk Introduction Multimodal Machine Translation (MMT) aims at designing better translation systems which take into account auxiliary inputs such as images. Initially organized as a shared task within the First Conference on Machine Translation (WMT16) (Specia et al., 2016), MMT has so far been studied using the Multi30K dataset (Elliott et al., 2016), a multilingual extension of Flickr30K (Young et al., 2014) with translations of the English image descriptions into German, French and Czech (Elliott et al., 2017; Barrault et al., 2018). The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y and Helcl, 2017; Helcl et al., 2018) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016; Gr¨onroos et al., 2018). Nevertheless, the conclusion about the contributio"
N19-1422,W17-4746,1,0.83507,"Missing"
N19-1422,W18-6438,1,0.873621,"Missing"
N19-1422,W16-2359,0,0.254545,"ts such as images. Initially organized as a shared task within the First Conference on Machine Translation (WMT16) (Specia et al., 2016), MMT has so far been studied using the Multi30K dataset (Elliott et al., 2016), a multilingual extension of Flickr30K (Young et al., 2014) with translations of the English image descriptions into German, French and Czech (Elliott et al., 2017; Barrault et al., 2018). The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y and Helcl, 2017; Helcl et al., 2018) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016; Gr¨onroos et al., 2018). Nevertheless, the conclusion about the contribution of the visual modality is still unclear: Gr¨onroos et al. (2018) consider their multimodal gains “modest” and attribute the largest gain to the usage of external parallel corpora. Lala et al. (2018) observe that the"
N19-1422,D17-1105,0,0.67641,"T has so far been studied using the Multi30K dataset (Elliott et al., 2016), a multilingual extension of Flickr30K (Young et al., 2014) with translations of the English image descriptions into German, French and Czech (Elliott et al., 2017; Barrault et al., 2018). The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y and Helcl, 2017; Helcl et al., 2018) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016; Gr¨onroos et al., 2018). Nevertheless, the conclusion about the contribution of the visual modality is still unclear: Gr¨onroos et al. (2018) consider their multimodal gains “modest” and attribute the largest gain to the usage of external parallel corpora. Lala et al. (2018) observe that their multimodal word-sense disambiguation approach is not significantly different than the monomodal counterpart. The organizers of the latest"
N19-1422,D14-1179,0,0.0654888,"Missing"
N19-1422,P11-2031,0,0.0378663,"encoder/decoder outputs, respectively (Srivastava et al., 2014). The weights are decayed with a factor of 1e−5. We use ADAM (Kingma and Ba, 2014) with a learning rate of 4e−4 and mini-batches of 64 samples. The gradients are clipped if the total norm exceeds 1 (Pascanu et al., 2013). The training is early-stopped if dev set METEOR (Denkowski and Lavie, 2014) does not improve for ten epochs. All experiments are conducted with nmtpytorch1 (Caglayan et al., 2017b). 4 We train all systems three times each with different random initialization in order to perform significance testing with multeval (Clark et al., 2011). Throughout the section, we always report the mean over three runs (and the standard deviation) of the considered metrics. We decode the translations with a beam size of 12. github.com/lium-lst/nmtpytorch We first present test2017 METEOR scores for the baseline NMT and MMT systems, when trained on the full dataset D (Table 2). The first column indicates that, although MMT models perform slightly better on average, they are not significantly better than the baseline NMT. We now introduce and discuss the results obtained under the proposed degradation schemes. Please refer to Table 5 and the ap"
N19-1422,W14-3348,0,0.0692332,"encoder and decoder GRUs have 400 hidden units and are initialized with 0 except the multimodal INIT system. All embeddings are 200-dimensional and the decoder embeddings are tied (Press and Wolf, 2016). A dropout of 0.4 and 0.5 is applied on source embeddings and encoder/decoder outputs, respectively (Srivastava et al., 2014). The weights are decayed with a factor of 1e−5. We use ADAM (Kingma and Ba, 2014) with a learning rate of 4e−4 and mini-batches of 64 samples. The gradients are clipped if the total norm exceeds 1 (Pascanu et al., 2013). The training is early-stopped if dev set METEOR (Denkowski and Lavie, 2014) does not improve for ten epochs. All experiments are conducted with nmtpytorch1 (Caglayan et al., 2017b). 4 We train all systems three times each with different random initialization in order to perform significance testing with multeval (Clark et al., 2011). Throughout the section, we always report the mean over three runs (and the standard deviation) of the considered metrics. We decode the translations with a beam size of 12. github.com/lium-lst/nmtpytorch We first present test2017 METEOR scores for the baseline NMT and MMT systems, when trained on the full dataset D (Table 2). The first c"
N19-1422,D18-1329,0,0.450816,"ion about the contribution of the visual modality is still unclear: Gr¨onroos et al. (2018) consider their multimodal gains “modest” and attribute the largest gain to the usage of external parallel corpora. Lala et al. (2018) observe that their multimodal word-sense disambiguation approach is not significantly different than the monomodal counterpart. The organizers of the latest edition of the shared task concluded that the multimodal integration schemes explored so far resulted in marginal changes in terms of automatic metrics and human evaluation (Barrault et al., 2018). In a similar vein, Elliott (2018) demonstrated that MMT models can translate without significant performance losses even in the presence of features from unrelated images. These empirical findings seem to indicate that images are ignored by the models and hint at the fact that this is due to representation or modeling limitations. We conjecture that the most plausible reason for the linguistic dominance is that – at least in Multi30K – the source text is sufficient to perform the translation, eventually preventing the visual information from intervening in the learning process. To investigate this hypothesis, we introduce sev"
N19-1422,W17-4718,1,0.847898,"way they are integrated into the model. 1 Pranava Madhyastha Imperial College London pranava@imperial.ac.uk Introduction Multimodal Machine Translation (MMT) aims at designing better translation systems which take into account auxiliary inputs such as images. Initially organized as a shared task within the First Conference on Machine Translation (WMT16) (Specia et al., 2016), MMT has so far been studied using the Multi30K dataset (Elliott et al., 2016), a multilingual extension of Flickr30K (Young et al., 2014) with translations of the English image descriptions into German, French and Czech (Elliott et al., 2017; Barrault et al., 2018). The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y and Helcl, 2017; Helcl et al., 2018) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016; Gr¨onroos et al., 2018). Nevertheless, the conclusi"
N19-1422,W16-3210,1,0.827773,"Missing"
N19-1422,W18-6439,0,0.123297,"Missing"
N19-1422,W18-6441,0,0.340492,"Missing"
N19-1422,W16-2360,0,0.228592,"image descriptions into German, French and Czech (Elliott et al., 2017; Barrault et al., 2018). The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y and Helcl, 2017; Helcl et al., 2018) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016; Gr¨onroos et al., 2018). Nevertheless, the conclusion about the contribution of the visual modality is still unclear: Gr¨onroos et al. (2018) consider their multimodal gains “modest” and attribute the largest gain to the usage of external parallel corpora. Lala et al. (2018) observe that their multimodal word-sense disambiguation approach is not significantly different than the monomodal counterpart. The organizers of the latest edition of the shared task concluded that the multimodal integration schemes explored so far resulted in marginal changes in terms of automatic metrics and human eva"
N19-1422,P07-2045,0,0.00557821,"rating the visual modality would likely deteriorate in terms of metrics. 3 Experimental Setup Dataset. We conduct experiments on the English→French part of Multi30K. The models are trained on the concatenation of the train and val sets (30K sentences) whereas test2016 (dev) and test2017 (test) are used for early-stopping and model evaluation, respectively. For entity masking, we revert to the default Flickr30K splits and perform the model evaluation on test2016, since test2017 is not annotated for entities. We use word-level vocabularies of 9,951 English and 11,216 French words. We use Moses (Koehn et al., 2007) scripts to lowercase, normalize and tokenize the sentences with hyphen splitting. The hyphens are stitched back prior to evaluation. Visual Features. We use a ResNet-50 CNN (He et al., 2016) trained on ImageNet (Deng et al., 2009) as image encoder. Prior to feature extraction, we center and standardize the images using ImageNet statistics, resize the shortest edge to 256 pixels and take a center crop of size 256x256. We extract spatial features of size 2048x8x8 from the final convolutional layer and apply L2 normalization along the depth dimension (Caglayan et al., 2018). For the non-attentiv"
N19-1422,W18-6442,1,0.870079,"et al., 2016; Calixto et al., 2016; Libovick´y and Helcl, 2017; Helcl et al., 2018) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016; Gr¨onroos et al., 2018). Nevertheless, the conclusion about the contribution of the visual modality is still unclear: Gr¨onroos et al. (2018) consider their multimodal gains “modest” and attribute the largest gain to the usage of external parallel corpora. Lala et al. (2018) observe that their multimodal word-sense disambiguation approach is not significantly different than the monomodal counterpart. The organizers of the latest edition of the shared task concluded that the multimodal integration schemes explored so far resulted in marginal changes in terms of automatic metrics and human evaluation (Barrault et al., 2018). In a similar vein, Elliott (2018) demonstrated that MMT models can translate without significant performance losses even in the presence of features from unrelated images. These empirical findings seem to indicate that images are ignored by the"
N19-1422,P17-2031,0,0.3724,"Missing"
N19-1422,W17-4751,0,0.0226414,"ed using the Multi30K dataset (Elliott et al., 2016), a multilingual extension of Flickr30K (Young et al., 2014) with translations of the English image descriptions into German, French and Czech (Elliott et al., 2017; Barrault et al., 2018). The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features (Caglayan et al., 2016; Calixto et al., 2016; Libovick´y and Helcl, 2017; Helcl et al., 2018) (ii) cross-modal interactions with spatially-unaware global features (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017) and (iii) the integration of regional features from object detection networks (Huang et al., 2016; Gr¨onroos et al., 2018). Nevertheless, the conclusion about the contribution of the visual modality is still unclear: Gr¨onroos et al. (2018) consider their multimodal gains “modest” and attribute the largest gain to the usage of external parallel corpora. Lala et al. (2018) observe that their multimodal word-sense disambiguation approach is not significantly different than the monomodal counterpart. The organizers of the latest edition of the sh"
N19-1422,Q14-1006,0,\N,Missing
N19-1422,W16-2346,1,\N,Missing
N19-1422,W17-4752,1,\N,Missing
P18-1198,P17-1080,0,0.270596,"Tense and TopConst. It is not clear that they controlled for the same factors we considered (in particular, lexical overlap and 2133 Figure 2: Spearman correlation matrix between probing and downstream tasks. Correlations based on all sentence embeddings we investigated (more than 40). Cells in gray denote task pairs that are not significantly correlated (after correcting for multiple comparisons). sentence length), and they use much smaller training sets, limiting classifier-based evaluation to logistic regression. Moreover, they test a smaller set of models, focusing on machine translation. Belinkov et al. (2017a), Belinkov et al. (2017b) and Dalvi et al. (2017) are also interested in understanding the type of linguistic knowledge encoded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders. Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to assess how they handle various linguistic phenomena. Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-w"
P18-1198,I17-1001,0,0.0335861,"Tense and TopConst. It is not clear that they controlled for the same factors we considered (in particular, lexical overlap and 2133 Figure 2: Spearman correlation matrix between probing and downstream tasks. Correlations based on all sentence embeddings we investigated (more than 40). Cells in gray denote task pairs that are not significantly correlated (after correcting for multiple comparisons). sentence length), and they use much smaller training sets, limiting classifier-based evaluation to logistic regression. Moreover, they test a smaller set of models, focusing on machine translation. Belinkov et al. (2017a), Belinkov et al. (2017b) and Dalvi et al. (2017) are also interested in understanding the type of linguistic knowledge encoded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders. Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to assess how they handle various linguistic phenomena. Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-w"
P18-1198,D15-1075,0,0.013006,"sentences. Following Vinyals et al. (2015), we train a seq2seq architecture to generate linearized grammatical parse trees (see Table 1) from source sentences (Seq2Tree). We use the Stanford parser to generate trees for Europarl source English sentences. We train SkipThought vectors (Kiros et al., 2015) by predicting the next sentence given the current one (Tang et al., 2017), on 30M sentences from the Toronto Book Corpus, excluding those in the probing sets. Finally, following Conneau et al. (2017), we train sentence encoders on Natural Language Inference using the concatenation of the SNLI (Bowman et al., 2015) and MultiNLI (Bowman et al., 2015) data sets (about 1M sentence pairs). In this task, a sentence encoder is trained to encode two sentences, which are fed to a classifier and whose role is to distinguish whether the sentences are contradictory, neutral or entailed. Finally, as in Conneau et al. (2017), we also include Untrained encoders with random weights, which act as random projections of pre-trained word embeddings. 3.3 Training details BiLSTM encoders use 2 layers of 512 hidden units (∼4M parameters), Gated ConvNet has 8 convolutional layers of 512 hidden units, kernel size 3 (∼12M param"
P18-1198,D17-1070,1,0.640675,"=1,...,T , a bidirectional LSTM computes a set of T vectors {ht }t . For t ∈ [1, . . . , T ], ht is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions. We experiment with two ways of combining the varying number of (h1 , . . . , hT ) to form a fixed-size vector, either by selecting the last hidden state of hT or by selecting the maximum value over each dimension of the hidden units. The choice of these models are motivated by their demonstrated efficiency in seq2seq (Sutskever et al., 2014) and universal sentence representation learning (Conneau et al., 2017), respectively.2 Gated ConvNet We also consider the nonrecurrent convolutional equivalent of LSTMs, based on stacked gated temporal convolutions. Gated convolutional networks were shown to perform well as neural machine translation encoders (Gehring et al., 2017) and language modeling decoders (Dauphin et al., 2017). The encoder is composed of an input word embedding table that is augmented with positional encodings (Sukhbaatar et al., 2015), followed by a stack of temporal convolutions with small kernel size. The output of each convolutional layer is filtered by a gating mechanism, similar to"
P18-1198,I17-1015,0,0.0114727,"led for the same factors we considered (in particular, lexical overlap and 2133 Figure 2: Spearman correlation matrix between probing and downstream tasks. Correlations based on all sentence embeddings we investigated (more than 40). Cells in gray denote task pairs that are not significantly correlated (after correcting for multiple comparisons). sentence length), and they use much smaller training sets, limiting classifier-based evaluation to logistic regression. Moreover, they test a smaller set of models, focusing on machine translation. Belinkov et al. (2017a), Belinkov et al. (2017b) and Dalvi et al. (2017) are also interested in understanding the type of linguistic knowledge encoded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders. Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to assess how they handle various linguistic phenomena. Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-word behaviour of the network (e.g., Nagamine et al."
P18-1198,P03-1054,0,0.0358665,"y contain to their hierarchical structure to subtle facets of semantic acceptability. We think the current task set is reasonably representative of different linguistic domains, but we are not claiming that it is exhaustive. We expect future work to extend it. The sentences for all our tasks are extracted from the Toronto Book Corpus (Zhu et al., 2015), more specifically from the random pre-processed portion made available by Paperno et al. (2016). We only sample sentences in the 5-to-28 word range. We parse them with the Stanford Parser (2017-06-09 version), using the pre-trained PCFG model (Klein and Manning, 2003), and we rely on the part-of-speech, constituency and dependency parsing information provided by this tool where needed. For each task, we construct training sets containing 100k sentences, and 10k-sentence val1 https://github.com/facebookresearch/ SentEval/tree/master/data/probing idation and test sets. All sets are balanced, having an equal number of instances of each target class. Surface information These tasks test the extent to which sentence embeddings are preserving surface properties of the sentences they encode. One can solve the surface tasks by simply looking at tokens in the input"
P18-1198,2005.mtsummit-papers.11,0,0.00574583,"irectional LSTM, with consistently poorer results. max-pooling along the temporal dimension is performed on the output feature maps of the last convolution (Collobert and Weston, 2008). 3.2 Training tasks Seq2seq systems have shown strong results in machine translation (Zhou et al., 2016). They consist of an encoder that encodes a source sentence into a fixed-size representation, and a decoder which acts as a conditional language model and that generates the target sentence. We train Neural Machine Translation systems on three language pairs using about 2M sentences from the Europarl corpora (Koehn, 2005). We pick English-French, which involves two similar languages, English-German, involving larger syntactic differences, and English-Finnish, a distant pair. We also train with an AutoEncoder objective (Socher et al., 2011) on Europarl source English sentences. Following Vinyals et al. (2015), we train a seq2seq architecture to generate linearized grammatical parse trees (see Table 1) from source sentences (Seq2Tree). We use the Stanford parser to generate trees for Europarl source English sentences. We train SkipThought vectors (Kiros et al., 2015) by predicting the next sentence given the cur"
P18-1198,P07-2045,0,0.0125781,"Missing"
P18-1198,S14-2055,0,0.00993521,"s require complex forms of inference, making it difficult to pinpoint the information a model is relying upon. Impressive as it might be that a system can tell that the sentence “A movie that doesn’t aim too high, but it doesn’t need to” (Pang and Lee, 2004) expresses a subjective viewpoint, it is Guillaume Lample Facebook AI Research Sorbonne Universités glample@fb.com Marco Baroni Facebook AI Research mbaroni@fb.com hard to tell how the system (or even a human) comes to this conclusion. Complex tasks can also carry hidden biases that models might lock onto (Jabri et al., 2016). For example, Lai and Hockenmaier (2014) show that the simple heuristic of checking for explicit negation words leads to good accuracy in the SICK sentence entailment task. Model introspection techniques have been applied to sentence encoders in order to gain a better understanding of which properties of the input sentences their embeddings retain (see Section 5). However, these techniques often depend on the specifics of an encoder architecture, and consequently cannot be used to compare different methods. Shi et al. (2016) and Adi et al. (2017) introduced a more general approach, relying on the notion of what we will call probing"
P18-1198,N16-1082,0,0.017319,"standing the type of linguistic knowledge encoded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders. Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to assess how they handle various linguistic phenomena. Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-word behaviour of the network (e.g., Nagamine et al., 2015; Hupkes et al., 2017; Li et al., 2016; Linzen et al., 2016; Kàdàr et al., 2017; Li et al., 2017). These methods, complementary to ours, are not agnostic to encoder architecture, and cannot be used for general-purpose cross-model evaluation. Finally, Conneau et al. (2017) propose a largescale, multi-task evaluation of sentence embeddings, focusing entirely on downstream tasks. 6 Conclusion We introduced a set of tasks probing the linguistic knowledge of sentence embedding methods. Their purpose is not to encourage the development of ad-hoc models that attain top performance on them, but to help exploring what information is captur"
P18-1198,Q16-1037,0,0.140678,"of linguistic knowledge encoded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders. Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to assess how they handle various linguistic phenomena. Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-word behaviour of the network (e.g., Nagamine et al., 2015; Hupkes et al., 2017; Li et al., 2016; Linzen et al., 2016; Kàdàr et al., 2017; Li et al., 2017). These methods, complementary to ours, are not agnostic to encoder architecture, and cannot be used for general-purpose cross-model evaluation. Finally, Conneau et al. (2017) propose a largescale, multi-task evaluation of sentence embeddings, focusing entirely on downstream tasks. 6 Conclusion We introduced a set of tasks probing the linguistic knowledge of sentence embedding methods. Their purpose is not to encourage the development of ad-hoc models that attain top performance on them, but to help exploring what information is captured by different pre-t"
P18-1198,marelli-etal-2014-sick,1,0.538324,"downstream tasks after WC. We observe intriguing asymmetries: SOMO correlates with the SICK-E sentence entailment test, but not with SICK-R, which is about modeling sentence relatedness intuitions. Indeed, logical entailment requires deeper semantic analysis than modeling similarity judgments. TopConst and the number tasks negatively correlate with various similarity and sentiment data sets (SST, STS, SICK-R). This might expose biases in these tasks: SICK-R, for example, deliberately contains sentence pairs with opposite voice, that will have different constituent structure but equal meaning (Marelli et al., 2014). It might also mirrors genuine factors affecting similarity judgments (e.g., two sentences differing only in object number are very similar). Remarkably, TREC question type classification is the downstream task correlating with most probing tasks. Question classification is certainly an outlier among our downstream tasks, but we must leave a full understanding of this behaviour to future work (this is exactly the sort of analysis our probing tasks should stimulate). 5 Related work Adi et al. (2017) introduced SentLen, WC and a word order test, focusing on a bag-of-vectors baseline, an autoenc"
P18-1198,L18-1008,0,0.0218728,"pairs). In this task, a sentence encoder is trained to encode two sentences, which are fed to a classifier and whose role is to distinguish whether the sentences are contradictory, neutral or entailed. Finally, as in Conneau et al. (2017), we also include Untrained encoders with random weights, which act as random projections of pre-trained word embeddings. 3.3 Training details BiLSTM encoders use 2 layers of 512 hidden units (∼4M parameters), Gated ConvNet has 8 convolutional layers of 512 hidden units, kernel size 3 (∼12M parameters). We use pre-trained fastText word embeddings of size 300 (Mikolov et al., 2018) without fine-tuning, to isolate the impact of encoder architectures and to handle words outside the training sets. Training task performance and further details are in Appendix. 2129 task source I myself was out on an island in the AutoEncoder Swedish archipelago , at Sandhamn . I myself was out on an island in the NMT En-Fr Swedish archipelago , at Sandhamn . We really need to up our particular conNMT En-De tribution in that regard . It is too early to see one system as a uniNMT En-Fi versal panacea and dismiss another . SkipThought the old sami was gone , and he was a different person now ."
P18-1198,N13-1090,0,0.18687,"Missing"
P18-1198,P04-1035,0,0.0348809,"Missing"
P18-1198,P16-1144,1,0.804551,"up with a set of tasks that, while respecting the previous constraints, probe a wide range of phenomena, from superficial properties of sentences such as which words they contain to their hierarchical structure to subtle facets of semantic acceptability. We think the current task set is reasonably representative of different linguistic domains, but we are not claiming that it is exhaustive. We expect future work to extend it. The sentences for all our tasks are extracted from the Toronto Book Corpus (Zhu et al., 2015), more specifically from the random pre-processed portion made available by Paperno et al. (2016). We only sample sentences in the 5-to-28 word range. We parse them with the Stanford Parser (2017-06-09 version), using the pre-trained PCFG model (Klein and Manning, 2003), and we rely on the part-of-speech, constituency and dependency parsing information provided by this tool where needed. For each task, we construct training sets containing 100k sentences, and 10k-sentence val1 https://github.com/facebookresearch/ SentEval/tree/master/data/probing idation and test sets. All sets are balanced, having an equal number of instances of each target class. Surface information These tasks test the"
P18-1198,N18-1101,0,0.0165034,"Missing"
P18-1198,D14-1162,0,0.123956,"Missing"
P18-1198,P15-1094,1,0.909464,"Missing"
P18-1198,Q16-1027,0,0.0181797,"composed of an input word embedding table that is augmented with positional encodings (Sukhbaatar et al., 2015), followed by a stack of temporal convolutions with small kernel size. The output of each convolutional layer is filtered by a gating mechanism, similar to the one of LSTMs. Finally, 2 We also experimented with a unidirectional LSTM, with consistently poorer results. max-pooling along the temporal dimension is performed on the output feature maps of the last convolution (Collobert and Weston, 2008). 3.2 Training tasks Seq2seq systems have shown strong results in machine translation (Zhou et al., 2016). They consist of an encoder that encodes a source sentence into a fixed-size representation, and a decoder which acts as a conditional language model and that generates the target sentence. We train Neural Machine Translation systems on three language pairs using about 2M sentences from the Europarl corpora (Koehn, 2005). We pick English-French, which involves two similar languages, English-German, involving larger syntactic differences, and English-Finnish, a distant pair. We also train with an AutoEncoder objective (Socher et al., 2011) on Europarl source English sentences. Following Vinyal"
P18-1198,E17-2060,0,0.00815249,"denote task pairs that are not significantly correlated (after correcting for multiple comparisons). sentence length), and they use much smaller training sets, limiting classifier-based evaluation to logistic regression. Moreover, they test a smaller set of models, focusing on machine translation. Belinkov et al. (2017a), Belinkov et al. (2017b) and Dalvi et al. (2017) are also interested in understanding the type of linguistic knowledge encoded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders. Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to assess how they handle various linguistic phenomena. Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-word behaviour of the network (e.g., Nagamine et al., 2015; Hupkes et al., 2017; Li et al., 2016; Linzen et al., 2016; Kàdàr et al., 2017; Li et al., 2017). These methods, complementary to ours, are not agnostic to encoder architecture, and cannot be used for general-purpose cross-model evaluat"
P18-1198,D16-1159,0,0.18153,"ex tasks can also carry hidden biases that models might lock onto (Jabri et al., 2016). For example, Lai and Hockenmaier (2014) show that the simple heuristic of checking for explicit negation words leads to good accuracy in the SICK sentence entailment task. Model introspection techniques have been applied to sentence encoders in order to gain a better understanding of which properties of the input sentences their embeddings retain (see Section 5). However, these techniques often depend on the specifics of an encoder architecture, and consequently cannot be used to compare different methods. Shi et al. (2016) and Adi et al. (2017) introduced a more general approach, relying on the notion of what we will call probing tasks. A probing task is a classification problem that focuses on simple linguistic properties of sentences. For example, one such task might require to categorize sentences by the tense of their main verb. Given an encoder (e.g., an LSTM) pre-trained on a certain task (e.g., machine translation), we use the sentence embeddings it produces to train the tense classifier (without further embedding tuning). If the classifier succeeds, it means that the pre-trained encoder is storing reada"
P18-1198,J17-4003,0,\N,Missing
P18-1198,L18-1269,1,\N,Missing
W09-0423,D07-1090,0,0.0203199,"adding this data improves the overall system and they were not used in the final system, in order to keep the phrase-table small. We also performed experiments with the provided so-called bilingual French/English Gigaword corpus (575M English words in release 3). Again, we were not able to achieve any improvement by adding this data to the training material of the translation model. These findings are somehow surprising since it was eventually believed by the community that adding large amounts of bitexts should improve the translation model, as it is usually observed for the language model (Brants et al., 2007). In addition to these human generated bitexts, we also integrated a high quality bilingual dictionary from SYSTRAN. The entries of the dictionary were directly added to the bitexts. This technique has the potential advantage that the dictionary words could improve the alignments of these words when they also appear in the other bitexts. However, it is not guaranteed that multi-word expressions will be correctly aligned by GIZA++ and that only meaningful translations will actually appear in the phrase-table. A typical example is fire engine – camion de pompiers, for which the individual consti"
W09-0423,H93-1039,0,0.895104,"Missing"
W09-0423,W07-0732,1,0.833621,"rg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 5 Architecture of the SPE system During the last years statistical post-editing systems have shown to achieve very competitive performance (Simard et al., 2007; Dugast et al., 2007). The main idea of this techniques is to use 2 The source is available at http://www.cs.cmu. edu/˜qing/ 132 Corpus SMT system Eparl+NC Eparl+NC+dict Eparl+NC+dict+AFP SPE system SYSTRAN Eparl+NC Eparl+NC+AFP # En words Dev09a Dev09b Test09 41.6M 44.0M 51.7M 21.89 22.28 22.21 21.78 22.35# 21.43 23.80 24.13 23.88 44.2M 53.3M 18.68 23.03 22.95 18.84 23.15 23.15∗ 20.29 24.36 24.62 Table 3: Case sensitive NIST BLEU scores for the English-French systems. “NC” denotes the newscommentary bitexts, “dict” denotes SYSTRAN’s bilingual dictionary and “AFP” the automatically aligned news texts (∗ =primary,"
W09-0423,W08-0509,0,0.0200328,"ngual dictionary and “AFP” the automatically aligned news texts (∗ =primary, # =contrastive system) are given in Table 2. Adding the new news-train08 monolingual data had an important impact on the quality of the LM, even when the Gigaword data is already included. Data Vocabulary size Eparl+news + LDC Gigaword + Hansard and UN news-train08 alone all French 407k 248.8 142.2 137.5 165.0 120.6 The system is based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008).2 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. This previously caused problems when adding the entries of the bilingual dictionary to the bitexts. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. The parameters of Moses are tuned on news-dev2009a, using the cmert tool. The basic architecture of the system is identical to the one used in the 2008 WMT evaluation (Schwenk et al., 2008), but we did not use two pass decoding and n-best list rescoring with a continuous space language model. The results of t"
W09-0423,2003.mtsummit-tttt.3,0,0.0883033,"tems that include the additional AFP texts exhibit a bad generalisation behavior. We provide also the performance of the different systems on the official test set, calculated after the evaluation. In most of the cases, the observed improvements carry over on the test set. English 299k 416.7 194.9 187.5 245.9 174.8 Table 2: Perplexities on the development data of various language models. 4 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 5 Architecture"
W09-0423,P07-2045,0,0.0198073,"Missing"
W09-0423,P02-1038,0,0.0848608,"e development data of various language models. 4 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 5 Architecture of the SPE system During the last years statistical post-editing systems have shown to achieve very competitive performance (Simard et al., 2007; Dugast et al., 2007). The main idea of this techniques is to use 2 The source is available at http://www.cs.cmu. edu/˜qing/ 132 Corpus SMT system Eparl+NC Eparl+NC+dict Eparl+NC+dict+AFP SPE system"
W09-0423,J03-1002,0,0.0119318,"e additional AFP texts exhibit a bad generalisation behavior. We provide also the performance of the different systems on the official test set, calculated after the evaluation. In most of the cases, the observed improvements carry over on the test set. English 299k 416.7 194.9 187.5 245.9 174.8 Table 2: Perplexities on the development data of various language models. 4 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 5 Architecture of the SPE system D"
W09-0423,E09-1003,1,0.863217,"ines 580934–581316 and 599839–600662. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 130–134, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 130 French Gigaword English translations used as queries per day articles parallel sentences with extra words at ends candidate sentence pairs parallel sentences SMT FR EN length comparison tail removal + number / table removing 174M words WER 133M words 10.3M words 9.3M words +−5 day articles from English Gigaword Figure 1: Architecture of the parallel sentence extraction system (Rauf and Schwenk, 2009). (Brown et al., 1993). In comparison to our previous work (Schwenk et al., 2008), we also included all verbs in the French subjonctif and pass´e simple tense. In fact, those tenses seem to be frequently used in news material. In total about 10,000 verbs, 1,500 adjectives/adverbs and more than 100,000 noun forms were added. 2.2 word error rate is described in detail in (Rauf and Schwenk, 2009). 2.3 Monolingual data The French and English target language models were trained on all provided monolingual data. We realized that the news-train08 corpora contained some foreign texts, in particular in"
W09-0423,W08-0313,1,0.883458,"m based on the Moses decoder and a statistical post-editing system using SYSTRAN’s rule-based system. We also investigated techniques to automatically extract additional bilingual texts from comparable corpora. 1 Introduction This paper describes the machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2009 WMT shared task evaluation. This work was performed in cooperation with the company SYSTRAN. We only consider the translation between French and English (in both directions). The main differences to the previous year’s system (Schwenk et al., 2008) are as follows: better usage of SYSTRAN’s bilingual dictionary in the statistical system, less bilingual training data, additional language model training data (news-train08 as distributed by the organizers), usage of comparable corpora to improve the translation model, and development of a statistical post-editing system (SPE). These different components are described in the following. 2 Used Resources In the frame work of the 2009 WMT shared translation task many resources were made available. The following sections describe how they were used to train the translation and language models of"
W09-0423,2008.iwslt-papers.6,1,0.811009,"human evaluation. With respect to the SMT system, we were not able to improve the translation model by adding large amounts of bitexts, although different 133 sources were available (Canadian Hansard, UN or WEB data). Eventually these corpora are too noisy or out-of-domain. On the other hand, the integration of a high quality bilingual dictionary was helpful, as well as the automatic alignment of news texts from comparable corpora. Future work will concentrate on the integration of previously successful techniques, in particular continuous space language models and lightlysupervised training (Schwenk, 2008). We also believe that the tokenization could be improved, in particular for the French sources texts. Numbers, dates and other numerical expressions could be translated by a rule-based system. System combination has recently shown to provide important improvements of translation quality. We are currently working on a combination of the SMT and SPE system. It may be also interesting to add a third (hierarchical) MT system. 7 Acknowledgments This work has been partially funded by the French Government under the project I NSTAR (ANR JCJC06 143038) and the by the Higher Education Commission, Paki"
W09-0423,W07-0728,0,0.0556224,"arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 5 Architecture of the SPE system During the last years statistical post-editing systems have shown to achieve very competitive performance (Simard et al., 2007; Dugast et al., 2007). The main idea of this techniques is to use 2 The source is available at http://www.cs.cmu. edu/˜qing/ 132 Corpus SMT system Eparl+NC Eparl+NC+dict Eparl+NC+dict+AFP SPE system SYSTRAN Eparl+NC Eparl+NC+AFP # En words Dev09a Dev09b Test09 41.6M 44.0M 51.7M 21.89 22.28 22.21 21.78 22.35# 21.43 23.80 24.13 23.88 44.2M 53.3M 18.68 23.03 22.95 18.84 23.15 23.15∗ 20.29 24.36 24.62 Table 3: Case sensitive NIST BLEU scores for the English-French systems. “NC” denotes the newscommentary bitexts, “dict” denotes SYSTRAN’s bilingual dictionary and “AFP” the automatically aligned ne"
W09-0423,N03-1017,0,\N,Missing
W10-1739,P08-2021,0,0.0144983,"data are presented in section 4 along results of tuning on newssyscombtune2010. LIUM participated in the System Combination task of the Fifth Workshop on Statistical Machine Translation (WMT 2010). Hypotheses from 5 French/English MT systems were combined with MANY, an open source system combination software based on confusion networks currently developed at LIUM. 2 MANY is a system combination software (Barrault, 2010) based on the decoding of a lattice made of several Confusion Networks (CN). This is a widespread approach in MT system combination (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008). MANY can be decomposed in two main modules. The first one is the alignment module which actually is a modified version of TERp (Snover et al., 2009). Its role is to incrementally align the hypotheses against a backbone in order to create a confusion network. Those confusion networks are then connected together to create a lattice. This module uses different costs (which corresponds to a match, an insertion, a deletion, a substitution, a shift, a synonym and a stem) to compute the best alignment and incrementally build a confusion network. In the case of confusion network, the match (substitu"
W10-1739,W09-0407,0,0.0425512,"Missing"
W10-1739,P07-1040,0,0.0293732,"an be found in section 2. Results on WMT’09 data are presented in section 4 along results of tuning on newssyscombtune2010. LIUM participated in the System Combination task of the Fifth Workshop on Statistical Machine Translation (WMT 2010). Hypotheses from 5 French/English MT systems were combined with MANY, an open source system combination software based on confusion networks currently developed at LIUM. 2 MANY is a system combination software (Barrault, 2010) based on the decoding of a lattice made of several Confusion Networks (CN). This is a widespread approach in MT system combination (Rosti et al., 2007); (Shen et al., 2008); (Karakos et al., 2008). MANY can be decomposed in two main modules. The first one is the alignment module which actually is a modified version of TERp (Snover et al., 2009). Its role is to incrementally align the hypotheses against a backbone in order to create a confusion network. Those confusion networks are then connected together to create a lattice. This module uses different costs (which corresponds to a match, an insertion, a deletion, a substitution, a shift, a synonym and a stem) to compute the best alignment and incrementally build a confusion network. In the c"
W10-1759,W07-0733,0,0.107222,"e used to generate topic-dependent alignments by extension of the HMM alignment model and derivation of Viterbi alignments. (Zhao et al., 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. (Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. The training corpus was divided into different components, a model was trained on each part and then weighted appropriately for the given context. (Koehn and Schroeder, 2007) used two language models and two translation models: one in-domain and other out-of-domain to adapt the system. Two decoding paths were used to translate the text. Comparable corpora are exploited to find additional parallel texts. Information retrieval techniques are used to identify candidate sentences (Hildebrand et al., 2005). (Snover et al., 2008) used cross-lingual information retrieval to find texts in the target language that are related to the domain of the source texts. A self-enhancing approach was applied by (Ueffing, 2006) to filter the translations of the test set with the help"
W10-1759,D09-1074,0,0.091401,"table. (Ueffing, 2007) further refined this approach by using transductive semi-supervised methods for effective use of monolingual data from the source text. (Chen et al., 2008) performed domain adaptation simultaneously for the translation, language and reordering model by learning posterior knowledge from N-best hypothesis. A related approach was investigated in (Schwenk, 2008) and (Schwenk and Senellart, 2009) in which lightly supervised training was used. An SMT system was used to translate large collections of monolingual texts, which were then filtered and added to the training data. (Matsoukas et al., 2009) propose to weight each sentence in the training bitext by optimizing a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. Then certain parts of the training bitexts were downweighted to optimize an objective function on the development data. This can lead to parameter over-fitting if the function that maps sentence features to weights is complex. The technique proposed in this paper is somehow related to the above approach of weighting the texts. Our method does not require an explicit specificatio"
W10-1759,P08-2040,0,0.0861323,"ate texts of all kinds. Therefore, it is the domain of the training resources that influences the translations that are selected among several choices. While monolingual 392 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 392–399, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics phrase table. This additional table was used with the existing generic phrase table. (Ueffing, 2007) further refined this approach by using transductive semi-supervised methods for effective use of monolingual data from the source text. (Chen et al., 2008) performed domain adaptation simultaneously for the translation, language and reordering model by learning posterior knowledge from N-best hypothesis. A related approach was investigated in (Schwenk, 2008) and (Schwenk and Senellart, 2009) in which lightly supervised training was used. An SMT system was used to translate large collections of monolingual texts, which were then filtered and added to the training data. (Matsoukas et al., 2009) propose to weight each sentence in the training bitext by optimizing a discriminative function on a given tuning set. Sentence level features were extracte"
W10-1759,W07-0722,0,0.104862,"organized as follows. The next section describes related work on weighting the corpora and model adaptation. Section 3 describes the architecture allowing to resample and to weight the bitexts. Experimental results are presented in section 4 and the paper concludes with a discussion. 2 Related Work Adaptation of SMT systems is a topic of increasing interest since few years. In previous work, adaptation is done by using mixture models, by exploiting comparable corpora and by selfenhancement of translation models. Mixture models were used to optimize the coefficients to the adaptation domain. (Civera and Juan, 2007) proposed a model that can be used to generate topic-dependent alignments by extension of the HMM alignment model and derivation of Viterbi alignments. (Zhao et al., 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. (Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. The training corpus was divided into different components, a model was trained on each part and then weighted appropriat"
W10-1759,2009.mtsummit-posters.17,1,0.610293,"chine Translation and MetricsMATR, pages 392–399, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics phrase table. This additional table was used with the existing generic phrase table. (Ueffing, 2007) further refined this approach by using transductive semi-supervised methods for effective use of monolingual data from the source text. (Chen et al., 2008) performed domain adaptation simultaneously for the translation, language and reordering model by learning posterior knowledge from N-best hypothesis. A related approach was investigated in (Schwenk, 2008) and (Schwenk and Senellart, 2009) in which lightly supervised training was used. An SMT system was used to translate large collections of monolingual texts, which were then filtered and added to the training data. (Matsoukas et al., 2009) propose to weight each sentence in the training bitext by optimizing a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. Then certain parts of the training bitexts were downweighted to optimize an objective function on the development data. This can lead to parameter over-fitting if the function"
W10-1759,W07-0717,0,0.0686785,"increasing interest since few years. In previous work, adaptation is done by using mixture models, by exploiting comparable corpora and by selfenhancement of translation models. Mixture models were used to optimize the coefficients to the adaptation domain. (Civera and Juan, 2007) proposed a model that can be used to generate topic-dependent alignments by extension of the HMM alignment model and derivation of Viterbi alignments. (Zhao et al., 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. (Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. The training corpus was divided into different components, a model was trained on each part and then weighted appropriately for the given context. (Koehn and Schroeder, 2007) used two language models and two translation models: one in-domain and other out-of-domain to adapt the system. Two decoding paths were used to translate the text. Comparable corpora are exploited to find additional parallel texts. Information retrieval techniques are used to identify"
W10-1759,2008.iwslt-papers.6,1,0.88566,"op on Statistical Machine Translation and MetricsMATR, pages 392–399, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics phrase table. This additional table was used with the existing generic phrase table. (Ueffing, 2007) further refined this approach by using transductive semi-supervised methods for effective use of monolingual data from the source text. (Chen et al., 2008) performed domain adaptation simultaneously for the translation, language and reordering model by learning posterior knowledge from N-best hypothesis. A related approach was investigated in (Schwenk, 2008) and (Schwenk and Senellart, 2009) in which lightly supervised training was used. An SMT system was used to translate large collections of monolingual texts, which were then filtered and added to the training data. (Matsoukas et al., 2009) propose to weight each sentence in the training bitext by optimizing a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. Then certain parts of the training bitexts were downweighted to optimize an objective function on the development data. This can lead to param"
W10-1759,2006.iwslt-papers.3,0,0.207015,"eighted appropriately for the given context. (Koehn and Schroeder, 2007) used two language models and two translation models: one in-domain and other out-of-domain to adapt the system. Two decoding paths were used to translate the text. Comparable corpora are exploited to find additional parallel texts. Information retrieval techniques are used to identify candidate sentences (Hildebrand et al., 2005). (Snover et al., 2008) used cross-lingual information retrieval to find texts in the target language that are related to the domain of the source texts. A self-enhancing approach was applied by (Ueffing, 2006) to filter the translations of the test set with the help of a confidence score and to use reliable alignments to train an additional 3 Description of the algorithm The architecture of the algorithm is summarized in figure 1. The starting point is an (arbitrary) number of parallel corpora. We first concatenate these bitexts and perform word alignments in both directions using GIZA++. This is done on the concatenated bitexts since GIZA++ may perform badly if some of the individual bitexts are rather small. Next, the alignments are separated in parts corre393 be used by the standard phrase extra"
W10-1759,P07-1004,0,0.100745,"ls depends of course on the quality and quantity of the available resources. Today, most SMT systems are generic, i.e. the same system is used to translate texts of all kinds. Therefore, it is the domain of the training resources that influences the translations that are selected among several choices. While monolingual 392 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 392–399, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics phrase table. This additional table was used with the existing generic phrase table. (Ueffing, 2007) further refined this approach by using transductive semi-supervised methods for effective use of monolingual data from the source text. (Chen et al., 2008) performed domain adaptation simultaneously for the translation, language and reordering model by learning posterior knowledge from N-best hypothesis. A related approach was investigated in (Schwenk, 2008) and (Schwenk and Senellart, 2009) in which lightly supervised training was used. An SMT system was used to translate large collections of monolingual texts, which were then filtered and added to the training data. (Matsoukas et al., 2009)"
W10-1759,C04-1059,0,0.054191,"weight the bitexts. Experimental results are presented in section 4 and the paper concludes with a discussion. 2 Related Work Adaptation of SMT systems is a topic of increasing interest since few years. In previous work, adaptation is done by using mixture models, by exploiting comparable corpora and by selfenhancement of translation models. Mixture models were used to optimize the coefficients to the adaptation domain. (Civera and Juan, 2007) proposed a model that can be used to generate topic-dependent alignments by extension of the HMM alignment model and derivation of Viterbi alignments. (Zhao et al., 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. (Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. The training corpus was divided into different components, a model was trained on each part and then weighted appropriately for the given context. (Koehn and Schroeder, 2007) used two language models and two translation models: one in-domain and other out-of-domain to adapt the system. Two"
W10-1759,D08-1090,0,\N,Missing
W10-1759,P07-2045,0,\N,Missing
W10-1759,2005.eamt-1.19,0,\N,Missing
W11-2115,P08-2021,0,0.306086,"Missing"
W11-2115,P03-1021,0,0.0114527,"Missing"
W11-2115,P07-1040,0,0.250576,"Missing"
W11-2115,W09-0409,0,0.560198,"Missing"
W11-2158,E09-1003,1,0.897701,"Missing"
W11-2158,J93-2003,0,0.0301933,"Missing"
W11-2158,J07-2003,0,0.0504628,"010 was used as internal test set. The default Moses tokenization was used. However, we added abbreviations for the French tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the tool multi-bleu.perl and are case sensitive. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our main system is a phrase-based system (Koehn et al., 2003; Och and Ney, 2003), but we have also performed some experiments with a hierarchical system (Chiang, 2007). Both use a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) = arg max{exp( e X λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The hierarchical system"
W11-2158,W08-0509,0,0.0820888,"Missing"
W11-2158,P08-2015,0,0.0489218,"Missing"
W11-2158,D07-1103,0,0.0170723,"nal bitext added to Eparl+NC+1092 , we observe no improvement in French to English, and a very small improvement in English to French. However, added to the base468 line system (Eparl+NC+1092 ) adapted with the News data, the IR additional bitexts yield a small (0.2 BLEU) improvement in both translation directions. Final System In both translation directions our best system was the one trained on Eparl+NC+1092 +News+IR. We further achieved small improvements by pruning the phrase-table and by increasing the beam size. To prune the phrase-table, we used the ‘sigtest-filter’ available in Moses (Johnson et al., 2007), more precisely the α −  filter3 . We also build hierarchical systems on the various human translated corpora, using up to 323M words (corpora Eparl+NC+1092 ). The systems yielded similar results than the phrase-based approach, but required much more computational resources, in particular large amounts of main memory to perform the translations. Running the decoder was actually only possible with binarized rule-tables. Therefore, the hierarchical system was not used in the evaluation system. 3 The p-value of two-by-two contingency tables (describing the degree of association between a source"
W11-2158,2003.mtsummit-tttt.3,0,0.0945186,"oved from the Gigaword collections. 2.4 Development data All development was done on newstest2009, and newstest2010 was used as internal test set. The default Moses tokenization was used. However, we added abbreviations for the French tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the tool multi-bleu.perl and are case sensitive. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our main system is a phrase-based system (Koehn et al., 2003; Och and Ney, 2003), but we have also performed some experiments with a hierarchical system (Chiang, 2007). Both use a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) = arg max{exp( e X λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicali"
W11-2158,P07-2045,0,0.0249647,"Missing"
W11-2158,W10-1716,1,0.404823,"o performed initial experiments with hierarchical systems. Additional, new features this year include improved translation model adaptation using monolingual data, a continuous space language model and the treatment of unknown words. 1 Resources Used 2.1 Introduction This paper describes the statistical machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2011 WMT shared task evaluation. We only considered the translation between French and English (in both directions). The main differences with respect to previous year’s system (Lambert et al., 2010) are as follows: use of more training data as provided by the organizers, improved translation model adaptation by unsupervised training, a continuous space language model for the translation into French, some attempts to automatically induce translations of unknown words and first experiments with hierarchical systems. These different points are described in the rest of the paper, together with a summary of the experimental results showing the impact of each component. Bilingual data Our system was developed in two stages. First, a baseline system was built to generate automatic translations"
W11-2158,W11-2132,1,0.92181,"contains new combinations (phrases) of known words, and reinforces the probability of some phrase pairs (Schwenk, 2008). This year, we improved this method in the following way. In the original approach, the automatic translations are added to the human translated bitexts and a complete new system is build, including time consuming word alignment with GIZA++. For WMT’11, we directly used the word-to-word alignments produced by the decoder at the output instead of GIZA’s alignments. This speeds-up the procedure and yields the same results in our experiments. A detailed comparison is given in (Lambert et al., 2011). Second, as in last year’s evaluation, we automatically extracted and aligned parallel sentences from comparable in-domain corpora. We used the AFP and APW news texts since there are available in the French and English LDC Gigaword corpora. The general architecture of our parallel sentence extraction system is described in detail by Abdul-Rauf and Schwenk (2009). We first translated 91M words from French into English using our first stage SMT system. These English sentences were then used to search for translations in the English AFP and APW texts of the Gigaword corpus using information retr"
W11-2158,P02-1038,0,0.26782,"Missing"
W11-2158,J03-1002,0,0.0108264,"rd collections. 2.4 Development data All development was done on newstest2009, and newstest2010 was used as internal test set. The default Moses tokenization was used. However, we added abbreviations for the French tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the tool multi-bleu.perl and are case sensitive. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our main system is a phrase-based system (Koehn et al., 2003; Och and Ney, 2003), but we have also performed some experiments with a hierarchical system (Chiang, 2007). Both use a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) = arg max{exp( e X λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model"
W11-2158,2008.iwslt-papers.6,1,0.92273,"Missing"
W11-2158,N03-1017,0,\N,Missing
W12-3147,E09-1003,1,0.887484,"Missing"
W12-3147,2011.mtsummit-papers.1,0,0.0352814,"were no differences when more training data is available. Due to time constraints, this procedure was not used in the submitted system. 4 Results and Discussion The results of our SMT systems are summarized in Table 2. The MT metric scores for the development set are the average of three optimisations performed with different seeds (see Section 3). For the test set, they are the average of four values: the three values corresponding to these different optimisations, plus a fourth value obtained by taking as weight for each model, the average of the weights obtained in the three optimisations (Cettolo et al., 2011). The numbers in parentheses are the standard deviation of these three or four values. The standard deviation gives a lower bound of the significance of the difference between two systems. If the difference between two average scores is less than the sum of the standard deviations, we can say that this difference is not significant. The reverse is not true. The results of Table 2 show that adding several adapted corpora (the filtered 109 corpus, the synBitext Translation : En→Fr Eparl+NC Eparl+NC+ntsXX Eparl+NC+ntsXX+109f Eparl+NC+ntsXX+109f +IR Eparl+NC+ntsXX+109f +news+IR Translation : Fr→En"
W12-3147,W08-0509,0,0.0287729,"Missing"
W12-3147,2003.mtsummit-tttt.3,0,0.0303475,". We had time to apply the domain-based data selection only for French. Thus all data were used for English. We used this method to filter the French–English 109 parallel corpus as well, based on the difference between in-domain cross-entropy and out-ofdomain cross-entropy calculated for each sentence of the English side of the corpus. We kept 49 million words (in the English side) to train our models, called 109f . 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . We have build phrase-based systems (Koehn et al., 2003; Och and Ney, 2003), using the standard log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) = arg max{exp( e X i λi hi (e, f ))} (1) The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och, 2003). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The"
W12-3147,P07-2045,0,0.0187376,"Missing"
W12-3147,W11-2132,1,0.887796,"Missing"
W12-3147,P10-2041,0,0.0502836,"cribes the statistical machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2012 WMT shared task evaluation. We only considered the translation between French and English (in both directions). The main differences with respect to previous year’s system (Schwenk et al., 2011) are as follows: (i) use of more training data as provided by the organizers and (ii) better selection of the monolingual and parallel data according to the domain, using the cross-entropy difference with respect to in-domain and out-of-domain language models (Moore and Lewis, 2010). We kept some previous features: the improvement of the translation model adaptation by unsupervised training, a parallel corpus retrieved by Information Retrieval (IR) techniques and finally, the rescoring with a continuous space target language model for the translation into French. These different points are described in the rest of the paper, together with a summary of the experimental results showing the impact of each component. Bilingual data The latest version of the News-Commentary (NC) corpus and of the Europarl (Eparl) corpus (version 7) were used. We also took as training data a s"
W12-3147,J03-1002,0,0.0037201,"ly the domain-based data selection only for French. Thus all data were used for English. We used this method to filter the French–English 109 parallel corpus as well, based on the difference between in-domain cross-entropy and out-ofdomain cross-entropy calculated for each sentence of the English side of the corpus. We kept 49 million words (in the English side) to train our models, called 109f . 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . We have build phrase-based systems (Koehn et al., 2003; Och and Ney, 2003), using the standard log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) = arg max{exp( e X i λi hi (e, f ))} (1) The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och, 2003). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on"
W12-3147,P03-1021,0,0.0190644,"Missing"
W12-3147,W11-2158,1,0.664778,"ase-based systems based on the Moses decoder, trained on the provided data only. Additionally, new features this year included improved language and translation model adaptation using the cross-entropy score for the corpus selection. 1 Resources Used 2.1 Introduction This paper describes the statistical machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2012 WMT shared task evaluation. We only considered the translation between French and English (in both directions). The main differences with respect to previous year’s system (Schwenk et al., 2011) are as follows: (i) use of more training data as provided by the organizers and (ii) better selection of the monolingual and parallel data according to the domain, using the cross-entropy difference with respect to in-domain and out-of-domain language models (Moore and Lewis, 2010). We kept some previous features: the improvement of the translation model adaptation by unsupervised training, a parallel corpus retrieved by Information Retrieval (IR) techniques and finally, the rescoring with a continuous space target language model for the translation into French. These different points are des"
W12-3147,2008.iwslt-papers.6,1,0.898559,"Missing"
W12-3147,N03-1017,0,\N,Missing
W14-1002,2008.amta-srw.3,0,0.0378468,"ypotheses. Some of these new n-grams are ungrammatical, despite the presence of a language model. These novel ngrams are due to errors in hypothesis alignment and the confusion network structure. In section 3 we present two methods used to boost n-grams present in input hypotheses. Currently, decisions taken by the decoder mainly depend on the language model score, which is deemed insufficient to precisely evaluate the hypotheses. In consequence, it is interesting to estimate a score for better judging their quality. The challenge of our work is to exploit certain parameters defined by (Almut Siljaand and Vogel, 2008) to calculate word confidence score. These features are detailed in section 4. The approach is 2 Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 2–6, c Gothenburg, Sweden, April 27, 2014. 2014 Association for Computational Linguistics Language models log(Pw ) = X αi log(hi (t)) Three 4-gram language models named LM-Web, LM-Tune and LM-Test, are used to interpolate the adapted LM. They were trained respectively on the English web Corpus and the system outputs : development and test sets (except their references) involved in system combination, usin"
W14-1002,P07-1040,0,0.0773394,"NY has been updated in order to use word confidence score and to boost n-grams occurring in input hypotheses. In this paper we propose either to use an adapted language model or adding some additional features in the decoder to boost certain n-grams probabilities. Experimental results show that the updates yielded significant improvements in terms of BLEU score. 1 2 System description MANY is a system combination software (Barrault, 2010) based on the decoding of a lattice made of several Confusion Networks (CN). This is a widespread approach in MT system combination, see e.g. (Antti-Veikko I.Rosti and Schwartz, 2007; Damianos Karakos and Dreyer, 2008; Shen et al., 2008; Antti-Veikko I. Rosti and Schw, 2009). MANY can be decomposed in two main modules. The first one is the alignment module which is a modified version of TERp (Matthew G. Snover and Schwartz, 2009). Its role is to incrementally align the hypotheses against a backbone in order to create a confusion network. 1-best hypotheses from all M systems are aligned in order to build M confusion networks (one for each system considered as backbone). These confusion networks are then connected together to create a lattice. This module uses different cos"
W14-1002,W11-2115,1,0.646303,"ILM Toolkit (Stolcke, 2002). The resulting model from the interpolation of LM-Tune and LM-Test is interpolated linearly with the LM-Web to build the adapted LM. These models are tuned to minimize the perplexity on the tune reference. (1) i where t is the hypothesis, the αi are the weights of the feature functions hi . The following features are considered for decoding: • The language model probability: the probability given by a 4-gram language model. • The word penalty: penalty depending on the size (in words) of the hypothesis. 4 The best hypothesis selection relies on several features. In (Barrault, 2011) decisions taken by the decoder depend mainly on a n-gram language model, but it is sometimes insufficient to evaluate correctly the quality of the hypotheses. In order to improve these decisions, some additional information should be used. Several researches presented some studies of confidence scores at word and sentence level, such as (Almut Siljaand and Vogel, 2008) and (Ueffing and Ney, 2007). A large set of confidence scores were calculated over the n-best list. (Almut Siljaand and Vogel, 2008) defines several features extracted from n-best lists (at the sentence level) to select the bes"
W14-1002,P08-2021,0,0.359279,"Missing"
W14-1002,W09-0409,0,\N,Missing
W14-1002,2008.iwslt-evaluation.10,0,\N,Missing
W15-4006,D11-1033,0,0.0884987,"Missing"
W15-4006,N09-2038,0,0.025546,"one, the interpolation coefficients being estimated to minimize perplexity on an in-domain development corpus. This is known as linear mixture models. We can also integrate the various corpusspecific LMs as separate feature functions in the usual log-linear model of an SMT system. Data selection aims at extracting the most relevant subset of all the available LM training data. The approach proposed in (Moore and Lewis, 2010) has turned out to be the most effective one in many settings. Adaptation of the LM of an SMT models in an CAT environment was also investigated in several studies, e.g. (Bach et al., 2009; Bertoldi et al., 2012; Cettolo et al., 2014). Adaptation to new data was also investigated in the neural network community, usually by some type of incremental training on a (subset) of the 2 3 3 Statistical Machine Translation In the statistical approach to machine translation, all models are automatically estimated from examples. Let us assume that we want to translate a sentence in the source language s to a sentence in the target language t. Then, the fundamental equation of SMT is, applying Bayes rule: t∗ = arg max P (t|s) = arg max P (s|t)P (t) t t (1) The translation model P (s|t) is"
W15-4006,W12-3155,0,0.0124636,"tion coefficients being estimated to minimize perplexity on an in-domain development corpus. This is known as linear mixture models. We can also integrate the various corpusspecific LMs as separate feature functions in the usual log-linear model of an SMT system. Data selection aims at extracting the most relevant subset of all the available LM training data. The approach proposed in (Moore and Lewis, 2010) has turned out to be the most effective one in many settings. Adaptation of the LM of an SMT models in an CAT environment was also investigated in several studies, e.g. (Bach et al., 2009; Bertoldi et al., 2012; Cettolo et al., 2014). Adaptation to new data was also investigated in the neural network community, usually by some type of incremental training on a (subset) of the 2 3 3 Statistical Machine Translation In the statistical approach to machine translation, all models are automatically estimated from examples. Let us assume that we want to translate a sentence in the source language s to a sentence in the target language t. Then, the fundamental equation of SMT is, applying Bayes rule: t∗ = arg max P (t|s) = arg max P (s|t)P (t) t t (1) The translation model P (s|t) is estimated from bitexts,"
W15-4006,P03-1021,0,0.00901973,"his idea was previously proposed in framework of a speech recognition system (Park et al., 2010). We build on this work and explore different variants of this technique. An interesting alternative is to keep the original architecture of the NN and to only modify one layer, e.g. the weights between two tanh layers in Figure 1. This variant will be explored in future work. Ney, 2003). The translation probabilities of these phrase pairs are usually estimated by simple relative frequency. The LM is normally a 4-gram back-off model. The log-linear approach is commonly used to consider more models (Och, 2003), instead of just a translation and language model: t∗ = arg max t M X λm hm (s, t), (2) m=1 where hm (s, t) are so-called feature functions. The weights λm are optimized during the tuning stage. In the Moses system, fourteen feature functions are usually used. Automatic evaluation of an SMT system remains an open question and many metrics have been proposed. In this study we use the BLEU score which measures the n-gram precision between the translation and a human reference translation (Papineni et al., 2002). Higher values mean better translation quality. 4 Adaptation schemes Continuous Spac"
W15-4006,P02-1040,0,0.0944133,"mally a 4-gram back-off model. The log-linear approach is commonly used to consider more models (Och, 2003), instead of just a translation and language model: t∗ = arg max t M X λm hm (s, t), (2) m=1 where hm (s, t) are so-called feature functions. The weights λm are optimized during the tuning stage. In the Moses system, fourteen feature functions are usually used. Automatic evaluation of an SMT system remains an open question and many metrics have been proposed. In this study we use the BLEU score which measures the n-gram precision between the translation and a human reference translation (Papineni et al., 2002). Higher values mean better translation quality. 4 Adaptation schemes Continuous Space Language Model The basic architecture of an CSLM is shown in Figure 1. The words are first projected onto a continuous representation, the remaining part of the network estimates the probabilities. Usually one tanh hidden and a softmax output layer are used, but recent studies have shown that deeper architecture perform better (Schwenk et al., 2014). We will use three tanh hidden and a softmax output layer as depicted in Figure 1. This type of architecture is now well known and the reader is referred to the"
W15-4006,P13-2119,0,0.0199103,"tion of a phoneme classifier based on TRAPS (Trmal et al., 2010). There are also a few publications which investigate adaptation of neural network language models, most of them very recent. The insertion of an additional adaption layer to perform speaker adaptation was proposed by Park et al. (Park et al., 2010). Earlier this idea was explored in (Yao et al., 2012) for speech recognition through an affine transform of the output layer. Adaptation through data selection was studied in (Jalalvand, 2013) (selection of sentences in out-of-domain corpora based on similarity between sentences) and (Duh et al., 2013) (training of three models: n-gram, RNN and interpolated LM on two SMT systems: in-domain data only and all-domain). Several variants of curriculum learning are explored by Shi et al. to adapt a recurrent LM to a sub-domain, again in the area of speech recognition (Shia et al., 2014). Finally, one of the early applications of RNN was in (Kombrink et al., 2011): it was used to rescore the n-best list, speed-up the rescoring process, adapt an LM and estimate the influence of history. Related work Popular approaches to adapt the LM in an SMT system are mixture models, e.g. (Foster and Kuhn, 2007;"
W15-4006,W07-0717,0,0.0256608,"and (Duh et al., 2013) (training of three models: n-gram, RNN and interpolated LM on two SMT systems: in-domain data only and all-domain). Several variants of curriculum learning are explored by Shi et al. to adapt a recurrent LM to a sub-domain, again in the area of speech recognition (Shia et al., 2014). Finally, one of the early applications of RNN was in (Kombrink et al., 2011): it was used to rescore the n-best list, speed-up the rescoring process, adapt an LM and estimate the influence of history. Related work Popular approaches to adapt the LM in an SMT system are mixture models, e.g. (Foster and Kuhn, 2007; Koehn and Schroeder, 2007) and data selection. In the former case, separate LMs are trained on the available corpora and are then merged into one, the interpolation coefficients being estimated to minimize perplexity on an in-domain development corpus. This is known as linear mixture models. We can also integrate the various corpusspecific LMs as separate feature functions in the usual log-linear model of an SMT system. Data selection aims at extracting the most relevant subset of all the available LM training data. The approach proposed in (Moore and Lewis, 2010) has turned out to be the mo"
W15-4006,R13-2013,0,0.021625,"to transfer features in convolutional networks (Yosinski et al., 2014), or research to perform speaker adaptation of a phoneme classifier based on TRAPS (Trmal et al., 2010). There are also a few publications which investigate adaptation of neural network language models, most of them very recent. The insertion of an additional adaption layer to perform speaker adaptation was proposed by Park et al. (Park et al., 2010). Earlier this idea was explored in (Yao et al., 2012) for speech recognition through an affine transform of the output layer. Adaptation through data selection was studied in (Jalalvand, 2013) (selection of sentences in out-of-domain corpora based on similarity between sentences) and (Duh et al., 2013) (training of three models: n-gram, RNN and interpolated LM on two SMT systems: in-domain data only and all-domain). Several variants of curriculum learning are explored by Shi et al. to adapt a recurrent LM to a sub-domain, again in the area of speech recognition (Shia et al., 2014). Finally, one of the early applications of RNN was in (Kombrink et al., 2011): it was used to rescore the n-best list, speed-up the rescoring process, adapt an LM and estimate the influence of history. Re"
W15-4006,W07-0733,0,0.0342231,"(training of three models: n-gram, RNN and interpolated LM on two SMT systems: in-domain data only and all-domain). Several variants of curriculum learning are explored by Shi et al. to adapt a recurrent LM to a sub-domain, again in the area of speech recognition (Shia et al., 2014). Finally, one of the early applications of RNN was in (Kombrink et al., 2011): it was used to rescore the n-best list, speed-up the rescoring process, adapt an LM and estimate the influence of history. Related work Popular approaches to adapt the LM in an SMT system are mixture models, e.g. (Foster and Kuhn, 2007; Koehn and Schroeder, 2007) and data selection. In the former case, separate LMs are trained on the available corpora and are then merged into one, the interpolation coefficients being estimated to minimize perplexity on an in-domain development corpus. This is known as linear mixture models. We can also integrate the various corpusspecific LMs as separate feature functions in the usual log-linear model of an SMT system. Data selection aims at extracting the most relevant subset of all the available LM training data. The approach proposed in (Moore and Lewis, 2010) has turned out to be the most effective one in many set"
W15-4006,C12-2104,1,0.88246,"Missing"
W15-4006,N03-1017,0,0.0157558,"tion In the statistical approach to machine translation, all models are automatically estimated from examples. Let us assume that we want to translate a sentence in the source language s to a sentence in the target language t. Then, the fundamental equation of SMT is, applying Bayes rule: t∗ = arg max P (t|s) = arg max P (s|t)P (t) t t (1) The translation model P (s|t) is estimated from bitexts, bilingual sentence aligned data, and the language model P (t) from monolingual data in the target language. A popular approach are phrasebased models which translate short sequences of words together (Koehn et al., 2003; Och and https://www.matecat.com/ http://www.statmt.org/moses/ 49 All our experiments were performed with the open-source CSLM toolkit4 (Schwenk, 2013), which was extended for our purposes. A major challenge for neural network LMs is how to handle the words at the output layer since a the softmax normalization would be very costly for large vocabularies. Various solutions have been proposed: short-lists (Schwenk, 2007), a class decomposition (Mikolov et al., 2011) or an hierarchical decomposition (Le et al., 2011). In this work, we use short-lists, but our adaptation scheme could be equally a"
W15-4006,P07-2045,0,0.0463206,"Missing"
W15-4006,N12-1005,0,0.0311623,"Missing"
W15-4006,P10-2041,0,0.161083,"em are mixture models, e.g. (Foster and Kuhn, 2007; Koehn and Schroeder, 2007) and data selection. In the former case, separate LMs are trained on the available corpora and are then merged into one, the interpolation coefficients being estimated to minimize perplexity on an in-domain development corpus. This is known as linear mixture models. We can also integrate the various corpusspecific LMs as separate feature functions in the usual log-linear model of an SMT system. Data selection aims at extracting the most relevant subset of all the available LM training data. The approach proposed in (Moore and Lewis, 2010) has turned out to be the most effective one in many settings. Adaptation of the LM of an SMT models in an CAT environment was also investigated in several studies, e.g. (Bach et al., 2009; Bertoldi et al., 2012; Cettolo et al., 2014). Adaptation to new data was also investigated in the neural network community, usually by some type of incremental training on a (subset) of the 2 3 3 Statistical Machine Translation In the statistical approach to machine translation, all models are automatically estimated from examples. Let us assume that we want to translate a sentence in the source language s"
W15-4006,J03-1002,0,\N,Missing
W15-4006,D14-1179,1,\N,Missing
W16-2358,2015.iwslt-papers.5,1,0.814071,"LM (Stolcke, 2002), KenLM (Heafield, 2011), and GIZA++ (Och and Ney, 2003). This system is trained using the data provided by the organizers and tuned using MERT (Och, 2003) to maximize B LEU (Papineni et al., 2002) and M ETEOR (Lavie and Agarwal, 2007) scores on the validation set. 627 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 627–633, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics We also used Continuous Space Language Model1 (CSLM) (Schwenk, 2010) with the auxiliary features support as proposed by (Aransa et al., 2015). This CSLM architecture allows us to use sentence-level features for each line in the training data (i.e. all n-grams in the same sentence will have the same auxiliary features). By this means, better context specific LM estimations can be obtained. We used four additional scores to rerank 1000best outputs of our baseline system: The first two scores are obtained from two separate CSLM(s) trained on the target side (i.e. German) of the parallel training corpus and each one of the following auxiliary features: We define by X and Y , a source sentence of length N and a target sentence of length"
W16-2358,W16-3210,0,0.344228,"Missing"
W16-2358,N16-1101,0,0.0157052,"2015). 0 (7) zj 0 (11) 0 (12) = (1 − zj ) sj + zj sj = tanh(W cj + rj (U sj )) 0 (13) 0 (14) = σ(Wr cj + Ur sj ) = σ(Wz cj + Uz sj ) 0 where sj is the hidden state, rj and zj are the reset and update gate activations. W , Ur , Wr , Ur , Wz and Uz are the parameters to be learned. Finally, in order to compute the target word, the following formulations are applied: 0 where E is the target word embedding, sj is the 0 0 hidden state, rj and zj are the reset and update 0 0 0 0 0 0 gate activations. W , Ur , Wr , Ur , Wz and Uz are the parameters to be learned. A shared attention layer similar to (Firat et al., 2016) that consists of a fully-connected feedforward network is used to compute a set of modality specific attention coefficients emod at ij oj = Lo tanh(E[yj−1 ] + Ls sj + Lc cj ) (15) P (yj |yj−1 , sj , cj ) = Softmax(oj ) (16) where Lo , Ls and Lc are trained parameters. 630 (N, 1, 2000) Phrase Context Vectors (N, 1, 1000) RNN Forward x1 x2 x3 Φ (Σ) xN ... c1 xN ... x3 (N, 1, 1000) RNN Backward *LO Softmax Predicted Target Word Target Embeddings cN x1 x2 GRU GRU *LS ResNet-50 CNN 14 x 14 x 1024 Feature Maps *Watt * Wimg= (196, 1, 1024) Image Context Vectors (196, 1, 2000) Image Context Vectors Φ"
W16-2358,J03-1002,0,0.0142823,"o participated in the two proposed tasks for the WMT 2016 Multimodal Machine Translation evaluation campaign: Multimodal machine translation (Task 1) and multimodal image description (Task 2). 2 Multimodal Machine Translation This task consists in translating an English sentence that describes an image into German, given the English sentence itself and the image that it describes. 2.1 Phrase-based System Our baseline system for task 1 is developed following the standard phrase-based Moses pipeline as described in (Koehn et al., 2007), SRILM (Stolcke, 2002), KenLM (Heafield, 2011), and GIZA++ (Och and Ney, 2003). This system is trained using the data provided by the organizers and tuned using MERT (Och, 2003) to maximize B LEU (Papineni et al., 2002) and M ETEOR (Lavie and Agarwal, 2007) scores on the validation set. 627 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 627–633, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics We also used Continuous Space Language Model1 (CSLM) (Schwenk, 2010) with the auxiliary features support as proposed by (Aransa et al., 2015). This CSLM architecture allows us to use sentence-lev"
W16-2358,P03-1021,0,0.0588944,"n: Multimodal machine translation (Task 1) and multimodal image description (Task 2). 2 Multimodal Machine Translation This task consists in translating an English sentence that describes an image into German, given the English sentence itself and the image that it describes. 2.1 Phrase-based System Our baseline system for task 1 is developed following the standard phrase-based Moses pipeline as described in (Koehn et al., 2007), SRILM (Stolcke, 2002), KenLM (Heafield, 2011), and GIZA++ (Och and Ney, 2003). This system is trained using the data provided by the organizers and tuned using MERT (Och, 2003) to maximize B LEU (Papineni et al., 2002) and M ETEOR (Lavie and Agarwal, 2007) scores on the validation set. 627 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 627–633, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics We also used Continuous Space Language Model1 (CSLM) (Schwenk, 2010) with the auxiliary features support as proposed by (Aransa et al., 2015). This CSLM architecture allows us to use sentence-level features for each line in the training data (i.e. all n-grams in the same sentence will have the"
W16-2358,P02-1040,0,0.114819,"ion (Task 1) and multimodal image description (Task 2). 2 Multimodal Machine Translation This task consists in translating an English sentence that describes an image into German, given the English sentence itself and the image that it describes. 2.1 Phrase-based System Our baseline system for task 1 is developed following the standard phrase-based Moses pipeline as described in (Koehn et al., 2007), SRILM (Stolcke, 2002), KenLM (Heafield, 2011), and GIZA++ (Och and Ney, 2003). This system is trained using the data provided by the organizers and tuned using MERT (Och, 2003) to maximize B LEU (Papineni et al., 2002) and M ETEOR (Lavie and Agarwal, 2007) scores on the validation set. 627 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 627–633, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics We also used Continuous Space Language Model1 (CSLM) (Schwenk, 2010) with the auxiliary features support as proposed by (Aransa et al., 2015). This CSLM architecture allows us to use sentence-level features for each line in the training data (i.e. all n-grams in the same sentence will have the same auxiliary features). By this means,"
W16-2358,W11-2123,0,0.00801008,"developed by LIUM and CVC who participated in the two proposed tasks for the WMT 2016 Multimodal Machine Translation evaluation campaign: Multimodal machine translation (Task 1) and multimodal image description (Task 2). 2 Multimodal Machine Translation This task consists in translating an English sentence that describes an image into German, given the English sentence itself and the image that it describes. 2.1 Phrase-based System Our baseline system for task 1 is developed following the standard phrase-based Moses pipeline as described in (Koehn et al., 2007), SRILM (Stolcke, 2002), KenLM (Heafield, 2011), and GIZA++ (Och and Ney, 2003). This system is trained using the data provided by the organizers and tuned using MERT (Och, 2003) to maximize B LEU (Papineni et al., 2002) and M ETEOR (Lavie and Agarwal, 2007) scores on the validation set. 627 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 627–633, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics We also used Continuous Space Language Model1 (CSLM) (Schwenk, 2010) with the auxiliary features support as proposed by (Aransa et al., 2015). This CSLM architectu"
W16-2358,D13-1176,0,0.0105018,"methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR. 1 Introduction Recently, deep learning has greatly impacted the natural language processing field as well as computer vision. Machine translation (MT) with deep neural networks (DNN), proposed by (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) and (Bahdanau et al., 2014) competed successfully in the last year’s WMT evaluation campaign (Bojar et al., 2015). In the same trend, generating descriptions from images using DNNs has been proposed by (Elliott et al., 2015). Several attempts have been made to incorporate features from different modalities in order to help the automatic system to better model the task at hand (Elliott et al., 2015; Kiros et al., 2014b; Kiros et al., 2014a). This paper describes the systems developed by LIUM and CVC who participated in the two proposed tasks for the WMT 2016 Multimodal"
W16-2358,D15-1248,0,0.0197979,"enk/cslm-toolkit github.com/nyu-dl/dl4mt-tutorial 628 2.3 Data 3.1 Phrase-based and NMT systems for Task 1 are trained using the dataset provided by the organizers and described in Table 1. This dataset consists of 29K parallel sentences (direct translations of image descriptions from English to German) for training, 1014 for validation and finally 1000 for the test set. We preprocessed the dataset using the punctuation normalization, tokenization and lowercasing scripts from Moses. In order to generalize better over the compound structs in German, we trained and applied a compound splitter3 (Sennrich and Haddow, 2015) over the German vocabulary of training and validation sets. This reduces the target vocabulary from 18670 to 15820 unique tokens. During translation generation, the splitted compounds are stitched back together. Side English German Vocabulary Words 10211 15820 377K 369K To describe the image content we make use of Convolutional Neural Networks (CNN). In a breakthrough work, Krizhevsky et al. (Krizhevsky et al., 2012) convincingly show that CNNs yield a far superior image representation compared to previously used hand-crafted image features. Based on this success an intensified research effor"
W16-2358,P07-2045,0,0.0142624,"os et al., 2014a). This paper describes the systems developed by LIUM and CVC who participated in the two proposed tasks for the WMT 2016 Multimodal Machine Translation evaluation campaign: Multimodal machine translation (Task 1) and multimodal image description (Task 2). 2 Multimodal Machine Translation This task consists in translating an English sentence that describes an image into German, given the English sentence itself and the image that it describes. 2.1 Phrase-based System Our baseline system for task 1 is developed following the standard phrase-based Moses pipeline as described in (Koehn et al., 2007), SRILM (Stolcke, 2002), KenLM (Heafield, 2011), and GIZA++ (Och and Ney, 2003). This system is trained using the data provided by the organizers and tuned using MERT (Och, 2003) to maximize B LEU (Papineni et al., 2002) and M ETEOR (Lavie and Agarwal, 2007) scores on the validation set. 627 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 627–633, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics We also used Continuous Space Language Model1 (CSLM) (Schwenk, 2010) with the auxiliary features support as proposed"
W16-2358,W07-0734,0,0.047943,"scription (Task 2). 2 Multimodal Machine Translation This task consists in translating an English sentence that describes an image into German, given the English sentence itself and the image that it describes. 2.1 Phrase-based System Our baseline system for task 1 is developed following the standard phrase-based Moses pipeline as described in (Koehn et al., 2007), SRILM (Stolcke, 2002), KenLM (Heafield, 2011), and GIZA++ (Och and Ney, 2003). This system is trained using the data provided by the organizers and tuned using MERT (Och, 2003) to maximize B LEU (Papineni et al., 2002) and M ETEOR (Lavie and Agarwal, 2007) scores on the validation set. 627 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 627–633, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics We also used Continuous Space Language Model1 (CSLM) (Schwenk, 2010) with the auxiliary features support as proposed by (Aransa et al., 2015). This CSLM architecture allows us to use sentence-level features for each line in the training data (i.e. all n-grams in the same sentence will have the same auxiliary features). By this means, better context specific LM estimations"
W16-2358,W15-3001,0,\N,Missing
W16-2392,P10-2041,0,0.0146331,"on output embeddings. A Gated Recurrent Unit (GRU) (Chung et al., 2014) is used for the encoder and decoder. They have 1000 hidden units each, leading to an annotation vector ht ∈ R2000 . The attention mechanism, implemented as a simple fully-connected feed-forward neural network, accepts the hidden state ht of the decoder’s recurrent layer and one input annotation at a time, Table 1, reports detailed statistics on the monolingual data used to train the back-off LM and CSLM. The training dataset consists of WMT16 translation task monolingual corpora with the Moore-Lewis data selection method (Moore and Lewis, 2010) to select the CSLM training data with respect to the task’s development set. The 1 Train 84G 79G 2 http://www-lium.univ-lemans.fr/cslm/ 839 github.com/kyunghyuncho/dl4mt-material • NMTll : A log likelihood feature for each source and target sentence using NMT as described in Section 3. to produce the attention coefficients. A softmax activation is applied on those attention coefficients to obtain the attention weights used to generate the weighted annotation vector for time t. Both NMT systems are trained with WMT16 Quality Estimation English-German datasets (we used post-editions on the Germ"
W16-2392,H05-1026,0,0.0519421,"ed from segment pairs in isolation, ignoring contextual clues from other segments in the text. The focus of our contributions this year is to explore a new set of features which are language-independent, require minimal resources, and can be extracted in unsupervised ways with the use of neural networks. Word embeddings have shown their potential in modelling long distance dependencies in data, including syntactic and semantic information. For instance, neural network language models (Bengio et al., 2003) have been successfully explored in many problems including Automatic Speech Recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and Machine Translation (Schwenk, 2012). In this paper, we extend our previous work (Shah et al., 2015a; Shah et al., 2015b) to investigate the use of sentence embeddings extracted from a neural network language model along with cross entropy scores as features for QE. We also investigate the use of a neural machine translation model to extract the log likelihood of sentences as QE features. The features extracted from such resources are used in isolation or combined with hand-crafted features from QuEst to learn prediction models. This paper describes our systems for Task 1 o"
W16-2392,C12-2104,0,0.19648,"other segments in the text. The focus of our contributions this year is to explore a new set of features which are language-independent, require minimal resources, and can be extracted in unsupervised ways with the use of neural networks. Word embeddings have shown their potential in modelling long distance dependencies in data, including syntactic and semantic information. For instance, neural network language models (Bengio et al., 2003) have been successfully explored in many problems including Automatic Speech Recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and Machine Translation (Schwenk, 2012). In this paper, we extend our previous work (Shah et al., 2015a; Shah et al., 2015b) to investigate the use of sentence embeddings extracted from a neural network language model along with cross entropy scores as features for QE. We also investigate the use of a neural machine translation model to extract the log likelihood of sentences as QE features. The features extracted from such resources are used in isolation or combined with hand-crafted features from QuEst to learn prediction models. This paper describes our systems for Task 1 of the WMT16 Shared Task on Quality Estimation. Our submi"
W16-2392,2013.mtsummit-papers.21,1,0.535938,"ll -CSLMboth−emb • SHEF-SVM-QuEst-CSLMce -NMTll -CSLMboth−emb These systems contain all of our CSLM and NMT features either with or without QuEst: 719 and 644 features in total, respectively. We named them SVM-NN-both-emb and SVM-NNboth-emb-QuEst in the official submissions. The official results are shown in Table 4. Our systems show promising performance across all of the metrics used for evaluation in both scoring and ranking task variants. Our best system was ranked: Features We extracted the following features: • QuEst: 79 black-box features using the QuEst framework (Specia et al., 2013; Shah et al., 2013a) as described in Shah et al. (2013b). The full set of features can be found on http: //www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox. • Third place in the scoring task variant according to Pearson r (official scoring metric), and second place according MAE and RMSE. • CSLMce : A cross-entropy feature for each source and target sentence using CSLM as described in Section 2. • Second place in the ranking task variant according to Spearman ρ (official ranking metric) and first place according to DeltaAvg. 840 System. Baseline (SVM) SHEF-SVM-QuEst SHEF-SVM-QuEst-CSLMce -NMTll SHEF-SVM-"
W16-2392,W15-3041,1,0.67056,"his year is to explore a new set of features which are language-independent, require minimal resources, and can be extracted in unsupervised ways with the use of neural networks. Word embeddings have shown their potential in modelling long distance dependencies in data, including syntactic and semantic information. For instance, neural network language models (Bengio et al., 2003) have been successfully explored in many problems including Automatic Speech Recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and Machine Translation (Schwenk, 2012). In this paper, we extend our previous work (Shah et al., 2015a; Shah et al., 2015b) to investigate the use of sentence embeddings extracted from a neural network language model along with cross entropy scores as features for QE. We also investigate the use of a neural machine translation model to extract the log likelihood of sentences as QE features. The features extracted from such resources are used in isolation or combined with hand-crafted features from QuEst to learn prediction models. This paper describes our systems for Task 1 of the WMT16 Shared Task on Quality Estimation. Our submissions use (i) a continuous space language model (CSLM) to extr"
W16-2392,D15-1125,1,0.906714,"his year is to explore a new set of features which are language-independent, require minimal resources, and can be extracted in unsupervised ways with the use of neural networks. Word embeddings have shown their potential in modelling long distance dependencies in data, including syntactic and semantic information. For instance, neural network language models (Bengio et al., 2003) have been successfully explored in many problems including Automatic Speech Recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and Machine Translation (Schwenk, 2012). In this paper, we extend our previous work (Shah et al., 2015a; Shah et al., 2015b) to investigate the use of sentence embeddings extracted from a neural network language model along with cross entropy scores as features for QE. We also investigate the use of a neural machine translation model to extract the log likelihood of sentences as QE features. The features extracted from such resources are used in isolation or combined with hand-crafted features from QuEst to learn prediction models. This paper describes our systems for Task 1 of the WMT16 Shared Task on Quality Estimation. Our submissions use (i) a continuous space language model (CSLM) to extr"
W16-2392,P13-4014,1,0.903998,"Missing"
W17-4703,P16-2058,0,0.0535605,"Missing"
W17-4703,N06-2001,0,0.0221876,"vocabularies partially mitigates these issues, yet may cause serious instability (when computing embeddings of rare or unseen words) and complexity issues (when dealing with large softmax layers). Several proposals have been put forward to address these problems, which are particularly harmful when one language is a morphologically rich ∗ Both authors have contributed equally to this work. 20 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 20–31 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al., 2016; Alexandrescu and Kirchhoff, 2006; Wu et al., 2012), and more recently in a neural machine translation architecture as input features (Sennrich and Haddow, 2016) and in the output by separating the lemma and morphological factors (Garc´ıa-Mart´ınez et al., 2016). One contribution of the current paper is the investigation of new variants of the latter architecture. There have been other attempts with dual training objectives in NMT. In (Chen et al., 2016), a guided alignment training using topic information of the sentence as a second objective helps the decoder to improve the translation. Multi-task and multilingual learning"
W17-4703,W08-0310,1,0.904814,"Missing"
W17-4703,W17-3203,0,0.0172074,"e helps the decoder to improve the translation. Multi-task and multilingual learning in NMT have also been considered in several papers (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016), where training batches have to carefully balance tasks and language pairs. In contrast to these approaches, our factored NMT (FNMT) system produces several outputs simultaneously. cal unit and the morphological information (§ 3). These components are assessed separately and in conjunction using translation from English into two MRLs: Czech and Latvian. Our experiments show improvement over a strong (Denkowski and Neubig, 2017) BPE-to-BPE baseline, incorporating ensemble of models and backtranslated data (§ 5). Overall, they suggest that BPE representations, which loosely simulates concatenative morphological processes, is complementary to feature-based morphological representations. 2 Related Work Translating from and into MRLs has recently attracted some attention from the research community, as these languages compound a number of difficulties for automatic translation, such as the need to analyze or generate word forms unseen in training, or to handle variation in word order. To mitigate the unknown word problem"
W17-4703,W07-0735,0,0.207772,"ting from and into MRLs has recently attracted some attention from the research community, as these languages compound a number of difficulties for automatic translation, such as the need to analyze or generate word forms unseen in training, or to handle variation in word order. To mitigate the unknown word problem, a first approach consists in translating into target stems (Minkov et al., 2007; Toutanova et al., 2008); the right form is then selected from the full paradigms in a second step using a classifier. Target words may also be represented as lemmas complemented with side information. Bojar (2007); Bojar and Kos (2010); Bojar et al. (2012) use such a representation for two statistical MT systems: the first one translates from English into Czech lemmas decorated with source-side information and the second one performs a monotone translation into fully inflected Czech. Fraser et al. (2012) propose a target morphology normalization for German words represented as lemmas followed by a sequence of morphological tags and introduce a linguistically motivated selection of these when translating from English. The selection step consists in predicting the tags that have been removed during norma"
W17-4703,W16-2302,0,0.135293,"Missing"
W17-4703,P15-1166,0,0.028189,"chitecture as input features (Sennrich and Haddow, 2016) and in the output by separating the lemma and morphological factors (Garc´ıa-Mart´ınez et al., 2016). One contribution of the current paper is the investigation of new variants of the latter architecture. There have been other attempts with dual training objectives in NMT. In (Chen et al., 2016), a guided alignment training using topic information of the sentence as a second objective helps the decoder to improve the translation. Multi-task and multilingual learning in NMT have also been considered in several papers (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016), where training batches have to carefully balance tasks and language pairs. In contrast to these approaches, our factored NMT (FNMT) system produces several outputs simultaneously. cal unit and the morphological information (§ 3). These components are assessed separately and in conjunction using translation from English into two MRLs: Czech and Latvian. Our experiments show improvement over a strong (Denkowski and Neubig, 2017) BPE-to-BPE baseline, incorporating ensemble of models and backtranslated data (§ 5). Overall, they suggest that BPE representations, which loosely"
W17-4703,W12-1514,0,0.0397072,"Missing"
W17-4703,W12-3130,0,0.0137968,"attracted some attention from the research community, as these languages compound a number of difficulties for automatic translation, such as the need to analyze or generate word forms unseen in training, or to handle variation in word order. To mitigate the unknown word problem, a first approach consists in translating into target stems (Minkov et al., 2007; Toutanova et al., 2008); the right form is then selected from the full paradigms in a second step using a classifier. Target words may also be represented as lemmas complemented with side information. Bojar (2007); Bojar and Kos (2010); Bojar et al. (2012) use such a representation for two statistical MT systems: the first one translates from English into Czech lemmas decorated with source-side information and the second one performs a monotone translation into fully inflected Czech. Fraser et al. (2012) propose a target morphology normalization for German words represented as lemmas followed by a sequence of morphological tags and introduce a linguistically motivated selection of these when translating from English. The selection step consists in predicting the tags that have been removed during normalization, using a specific Conditional Rand"
W17-4703,2012.eamt-1.6,0,0.0469218,"Missing"
W17-4703,W10-1705,0,0.0619956,"into MRLs has recently attracted some attention from the research community, as these languages compound a number of difficulties for automatic translation, such as the need to analyze or generate word forms unseen in training, or to handle variation in word order. To mitigate the unknown word problem, a first approach consists in translating into target stems (Minkov et al., 2007; Toutanova et al., 2008); the right form is then selected from the full paradigms in a second step using a classifier. Target words may also be represented as lemmas complemented with side information. Bojar (2007); Bojar and Kos (2010); Bojar et al. (2012) use such a representation for two statistical MT systems: the first one translates from English into Czech lemmas decorated with source-side information and the second one performs a monotone translation into fully inflected Czech. Fraser et al. (2012) propose a target morphology normalization for German words represented as lemmas followed by a sequence of morphological tags and introduce a linguistically motivated selection of these when translating from English. The selection step consists in predicting the tags that have been removed during normalization, using a spec"
W17-4703,N16-1101,0,0.0286375,"features (Sennrich and Haddow, 2016) and in the output by separating the lemma and morphological factors (Garc´ıa-Mart´ınez et al., 2016). One contribution of the current paper is the investigation of new variants of the latter architecture. There have been other attempts with dual training objectives in NMT. In (Chen et al., 2016), a guided alignment training using topic information of the sentence as a second objective helps the decoder to improve the translation. Multi-task and multilingual learning in NMT have also been considered in several papers (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016), where training batches have to carefully balance tasks and language pairs. In contrast to these approaches, our factored NMT (FNMT) system produces several outputs simultaneously. cal unit and the morphological information (§ 3). These components are assessed separately and in conjunction using translation from English into two MRLs: Czech and Latvian. Our experiments show improvement over a strong (Denkowski and Neubig, 2017) BPE-to-BPE baseline, incorporating ensemble of models and backtranslated data (§ 5). Overall, they suggest that BPE representations, which loosely simulates concatenat"
W17-4703,W17-4705,1,0.915106,"acks many of the morphological contrasts that exist in the MRL. Normalization is needed to reduce the morphological variability on the MRL side so as to limit the number of types in the target, and to mitigate sparsity issues. This strategy is used for instance by Burlot et al. (2016) who remove the case mark from Czech nouns, which is not predictable from their English counterpart(s). Normalization is usually performed using handcrafted rules and requires expert knowledge for each language pair. In this paper, normalized words are obtained with an automatic data-driven method1 introduced in (Burlot and Yvon, 2017b). In a nutshell, this method performs a clustering of the MRL vocabulary by grouping together words that tend to share the same translation(s) in English. This translational similarity is based on the conditional entropy of lexical translation models estimated, for each MRL word form, using automatic word alignments. The clustering procedure merges two words whenever the resulting cluster does not increase the conditional entropy, which ensures a minimal loss of information during the whole process. Figure 1: Factored NMT system. The encoder and the attention mechanism of the Factored NMT ar"
W17-4703,E12-1068,0,0.0214011,"ate the unknown word problem, a first approach consists in translating into target stems (Minkov et al., 2007; Toutanova et al., 2008); the right form is then selected from the full paradigms in a second step using a classifier. Target words may also be represented as lemmas complemented with side information. Bojar (2007); Bojar and Kos (2010); Bojar et al. (2012) use such a representation for two statistical MT systems: the first one translates from English into Czech lemmas decorated with source-side information and the second one performs a monotone translation into fully inflected Czech. Fraser et al. (2012) propose a target morphology normalization for German words represented as lemmas followed by a sequence of morphological tags and introduce a linguistically motivated selection of these when translating from English. The selection step consists in predicting the tags that have been removed during normalization, using a specific Conditional Random Field (CRF) model for each morphological attribute to predict. Finally, word forms are produced via look-up in a morphological dictionary. This approach is extended by Weller et al. (2013), who takes verbal subcategorization frames into account, thus"
W17-4703,2016.amta-researchers.10,0,0.021389,"anslation (WMT), Volume 1: Research Papers, pages 20–31 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al., 2016; Alexandrescu and Kirchhoff, 2006; Wu et al., 2012), and more recently in a neural machine translation architecture as input features (Sennrich and Haddow, 2016) and in the output by separating the lemma and morphological factors (Garc´ıa-Mart´ınez et al., 2016). One contribution of the current paper is the investigation of new variants of the latter architecture. There have been other attempts with dual training objectives in NMT. In (Chen et al., 2016), a guided alignment training using topic information of the sentence as a second objective helps the decoder to improve the translation. Multi-task and multilingual learning in NMT have also been considered in several papers (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016), where training batches have to carefully balance tasks and language pairs. In contrast to these approaches, our factored NMT (FNMT) system produces several outputs simultaneously. cal unit and the morphological information (§ 3). These components are assessed separately and in conjunction using translation from"
W17-4703,P15-1001,0,0.0627746,"Missing"
W17-4703,W14-4012,0,0.101439,"Missing"
W17-4703,P16-1009,0,0.515486,"procedures: for instance by using a structured output layer (Mnih and Hinton, 2008) or by altering the training or decoding criteria (Jean et al., 2015). An alternative approach is to work with representations designed to remove some variations via source-side or target-side normalization procedures; or more radically to consider character-based representations (Ling et al., 2015; Luong and Manning, 2016; Costa-juss`a and Fonollosa, 2016), which are however much more costly to train, and make long distance dependencies even longer. None has however been as successful as the recent proposal of Sennrich et al. (2016b) which seems to achieve a right balance between a limited vocabulary size and an ability to translate a fully open vocabulary. In a nutshell, this approach decomposes source and target tokens into smaller units of variable length (using what is now termed as a “Byte Pair Encoding” or BPE in short): this means that (a) all source tokens can be represented as a sequence of such units, which crucially are all seen in training; (b) all possible target words can also be generated; (c) the size of the output layer can be set to remain within tractable limits; (d) most frequent words are kept as BP"
W17-4703,P07-2045,0,0.00736517,"r architecture, since the former (the cluster ID) constraints the set of possible values of the latter (the fine-grained PoS), which is notably used in our constrained decoding procedure (§ 5.4). 4.2 5 Experiments We introduce here the experimental setup for all the reported systems translating from English into Czech and Latvian. 5.1 Data and Preprocessing Our experimental setting follows the guidelines of the WMT’172 news translation task. The preprocessing of English data relies on in-house tools (D´echelotte et al., 2008). All the Czech data were tokenized and truecased the Moses toolkit (Koehn et al., 2007). PoS-tagging was performed with Morphodita (Strakov´a et al., 2014). The pre-processing of Latvian was provided by Tilde.3 Latvian PoS-tags were obtained with the LU MII Tagger (Paikens et al., 2013). For English-to-Czech, the parallel data used consisted in nearly 20M sentences from a subset of WMT data relevant to the news domain: Newscommentary, Europarl and specific categories of the Czeng corpus (news, paraweb, EU, fiction). Newstest-2015 was used for validation and the systems are tested on Newstest-2016 and 2017. The normalization of the Czech data was trained on the parallel data used"
W17-4703,P16-1162,0,0.820115,"procedures: for instance by using a structured output layer (Mnih and Hinton, 2008) or by altering the training or decoding criteria (Jean et al., 2015). An alternative approach is to work with representations designed to remove some variations via source-side or target-side normalization procedures; or more radically to consider character-based representations (Ling et al., 2015; Luong and Manning, 2016; Costa-juss`a and Fonollosa, 2016), which are however much more costly to train, and make long distance dependencies even longer. None has however been as successful as the recent proposal of Sennrich et al. (2016b) which seems to achieve a right balance between a limited vocabulary size and an ability to translate a fully open vocabulary. In a nutshell, this approach decomposes source and target tokens into smaller units of variable length (using what is now termed as a “Byte Pair Encoding” or BPE in short): this means that (a) all source tokens can be represented as a sequence of such units, which crucially are all seen in training; (b) all possible target words can also be generated; (c) the size of the output layer can be set to remain within tractable limits; (d) most frequent words are kept as BP"
W17-4703,D14-1025,0,0.150007,"Missing"
W17-4703,P14-5003,0,0.124455,"Missing"
W17-4703,P08-1059,0,0.100124,"). Overall, they suggest that BPE representations, which loosely simulates concatenative morphological processes, is complementary to feature-based morphological representations. 2 Related Work Translating from and into MRLs has recently attracted some attention from the research community, as these languages compound a number of difficulties for automatic translation, such as the need to analyze or generate word forms unseen in training, or to handle variation in word order. To mitigate the unknown word problem, a first approach consists in translating into target stems (Minkov et al., 2007; Toutanova et al., 2008); the right form is then selected from the full paradigms in a second step using a classifier. Target words may also be represented as lemmas complemented with side information. Bojar (2007); Bojar and Kos (2010); Bojar et al. (2012) use such a representation for two statistical MT systems: the first one translates from English into Czech lemmas decorated with source-side information and the second one performs a monotone translation into fully inflected Czech. Fraser et al. (2012) propose a target morphology normalization for German words represented as lemmas followed by a sequence of morpho"
W17-4703,P16-1100,0,0.0305342,"es LIUM, University of Le Mans Franc¸ois Yvon LIMSI, CNRS, Universit´e Paris Saclay Abstract language (MRL), exhibiting larger token/type ratio than is observed for English. One strategy is to improve NMT’s internal procedures: for instance by using a structured output layer (Mnih and Hinton, 2008) or by altering the training or decoding criteria (Jean et al., 2015). An alternative approach is to work with representations designed to remove some variations via source-side or target-side normalization procedures; or more radically to consider character-based representations (Ling et al., 2015; Luong and Manning, 2016; Costa-juss`a and Fonollosa, 2016), which are however much more costly to train, and make long distance dependencies even longer. None has however been as successful as the recent proposal of Sennrich et al. (2016b) which seems to achieve a right balance between a limited vocabulary size and an ability to translate a fully open vocabulary. In a nutshell, this approach decomposes source and target tokens into smaller units of variable length (using what is now termed as a “Byte Pair Encoding” or BPE in short): this means that (a) all source tokens can be represented as a sequence of such units"
W17-4703,W16-2342,0,0.0356469,"tor (Strakov´a et al., 2014). Since we had no such tool for Latvian, all monolingual data available at WMT’17 were automatically tagged using the LU MII Tagger (Paikens et al., 2013) and we gathered the result in a look-up table. As one could expect, we obtained a large table (nearly 2.5M forms) in which we observed a lot of noise. 5.4 Automatic Evaluation Results are reported using the following automatic metrics: BLEU (Papineni et al., 2002), BEER (Stanojevi´c and Sima’an, 2014) which tunes a large number of features to maximize the human ranking correlation at sentence level and CharacTER (Wang et al., 2016), a character-level version of TER which has shown a high correlation with human rankings (Bojar et al., 2016). Each score on fully inflected word systems is averaged from two independent runs (for both single and ensembled models). 6.1 Experiments with Bitext The results using the bitext provided at the WMT’17 the evaluation campaign are presented in Table 2 for English-to-Czech 8 and in Table 3 for English-to-Latvian. We can observe that using the constrained decoding consistently improves the results, except when using split clusters. In this last case, the system is forced to predict a PoS"
W17-4703,P07-1017,0,0.0250328,"ktranslated data (§ 5). Overall, they suggest that BPE representations, which loosely simulates concatenative morphological processes, is complementary to feature-based morphological representations. 2 Related Work Translating from and into MRLs has recently attracted some attention from the research community, as these languages compound a number of difficulties for automatic translation, such as the need to analyze or generate word forms unseen in training, or to handle variation in word order. To mitigate the unknown word problem, a first approach consists in translating into target stems (Minkov et al., 2007; Toutanova et al., 2008); the right form is then selected from the full paradigms in a second step using a classifier. Target words may also be represented as lemmas complemented with side information. Bojar (2007); Bojar and Kos (2010); Bojar et al. (2012) use such a representation for two statistical MT systems: the first one translates from English into Czech lemmas decorated with source-side information and the second one performs a monotone translation into fully inflected Czech. Fraser et al. (2012) propose a target morphology normalization for German words represented as lemmas followe"
W17-4703,P13-1058,0,0.0126976,"e performs a monotone translation into fully inflected Czech. Fraser et al. (2012) propose a target morphology normalization for German words represented as lemmas followed by a sequence of morphological tags and introduce a linguistically motivated selection of these when translating from English. The selection step consists in predicting the tags that have been removed during normalization, using a specific Conditional Random Field (CRF) model for each morphological attribute to predict. Finally, word forms are produced via look-up in a morphological dictionary. This approach is extended by Weller et al. (2013), who takes verbal subcategorization frames into account, thus enabling the CRFs to make better predictions. Note that Burlot et al. (2016) and El Kholy and Habash (2012b,a) propose related approaches respectively for translating into Czech and Arabic. Factored word representations have also been considered in neural language models (Niehues 3 Model Architectures The baseline NMT system used in this paper is an implementation of a standard NMT model with attention mechanism (Bahdanau et al., 2015). It consists of a sequence to sequence encoderdecoder of two recurrent neural networks (RNN), one"
W17-4703,2012.iwslt-papers.11,0,0.0257771,"hese issues, yet may cause serious instability (when computing embeddings of rare or unseen words) and complexity issues (when dealing with large softmax layers). Several proposals have been put forward to address these problems, which are particularly harmful when one language is a morphologically rich ∗ Both authors have contributed equally to this work. 20 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 20–31 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al., 2016; Alexandrescu and Kirchhoff, 2006; Wu et al., 2012), and more recently in a neural machine translation architecture as input features (Sennrich and Haddow, 2016) and in the output by separating the lemma and morphological factors (Garc´ıa-Mart´ınez et al., 2016). One contribution of the current paper is the investigation of new variants of the latter architecture. There have been other attempts with dual training objectives in NMT. In (Chen et al., 2016), a guided alignment training using topic information of the sentence as a second objective helps the decoder to improve the translation. Multi-task and multilingual learning in NMT have also b"
W17-4703,W16-2208,0,0.0566494,"Missing"
W17-4703,W13-5624,0,0.411502,"2 5 Experiments We introduce here the experimental setup for all the reported systems translating from English into Czech and Latvian. 5.1 Data and Preprocessing Our experimental setting follows the guidelines of the WMT’172 news translation task. The preprocessing of English data relies on in-house tools (D´echelotte et al., 2008). All the Czech data were tokenized and truecased the Moses toolkit (Koehn et al., 2007). PoS-tagging was performed with Morphodita (Strakov´a et al., 2014). The pre-processing of Latvian was provided by Tilde.3 Latvian PoS-tags were obtained with the LU MII Tagger (Paikens et al., 2013). For English-to-Czech, the parallel data used consisted in nearly 20M sentences from a subset of WMT data relevant to the news domain: Newscommentary, Europarl and specific categories of the Czeng corpus (news, paraweb, EU, fiction). Newstest-2015 was used for validation and the systems are tested on Newstest-2016 and 2017. The normalization of the Czech data was trained on the parallel data used to train the MT systems, except Czeng fiction and paraweb subcorpora, which amounts to over 10M sentences. A part of these systems was also trained on synthetic parallel data (Sennrich et al., 2016a)"
W17-4703,P02-1040,0,0.0994593,"ugh each word and keeping at each step the k-best reinflection hypotheses according to the unigram model mentioned above. For Czech reinflection, we used the Morphodita generator (Strakov´a et al., 2014). Since we had no such tool for Latvian, all monolingual data available at WMT’17 were automatically tagged using the LU MII Tagger (Paikens et al., 2013) and we gathered the result in a look-up table. As one could expect, we obtained a large table (nearly 2.5M forms) in which we observed a lot of noise. 5.4 Automatic Evaluation Results are reported using the following automatic metrics: BLEU (Papineni et al., 2002), BEER (Stanojevi´c and Sima’an, 2014) which tunes a large number of features to maximize the human ranking correlation at sentence level and CharacTER (Wang et al., 2016), a character-level version of TER which has shown a high correlation with human rankings (Bojar et al., 2016). Each score on fully inflected word systems is averaged from two independent runs (for both single and ensembled models). 6.1 Experiments with Bitext The results using the bitext provided at the WMT’17 the evaluation campaign are presented in Table 2 for English-to-Czech 8 and in Table 3 for English-to-Latvian. We ca"
W17-4703,W16-2209,0,0.0441737,"nd complexity issues (when dealing with large softmax layers). Several proposals have been put forward to address these problems, which are particularly harmful when one language is a morphologically rich ∗ Both authors have contributed equally to this work. 20 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 20–31 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al., 2016; Alexandrescu and Kirchhoff, 2006; Wu et al., 2012), and more recently in a neural machine translation architecture as input features (Sennrich and Haddow, 2016) and in the output by separating the lemma and morphological factors (Garc´ıa-Mart´ınez et al., 2016). One contribution of the current paper is the investigation of new variants of the latter architecture. There have been other attempts with dual training objectives in NMT. In (Chen et al., 2016), a guided alignment training using topic information of the sentence as a second objective helps the decoder to improve the translation. Multi-task and multilingual learning in NMT have also been considered in several papers (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016), where training b"
W17-4718,W17-4746,1,0.668714,"Missing"
W17-4718,D17-1105,0,0.401049,"Missing"
W17-4718,W17-4749,0,0.112115,"Missing"
W17-4718,W14-3348,0,0.128767,"yer from a ResNet-50 (He et al., 2016) convolutional neural network trained on the ImageNet dataset (Russakovsky et al., 2015). Those feature maps provide spatial information on which the model focuses through the attention mechanism. 4 Text-similarity Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.3 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and 220 3 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 4.1.1 Task 1: English → German Multi30K 2017 test data Table 4 shows the resu"
W17-4718,W17-4748,0,0.0838327,"Missing"
W17-4718,1983.tc-1.13,0,0.273609,"Missing"
W17-4718,P14-2074,1,0.832004,"Missing"
W17-4718,W16-3210,1,0.583063,"tion at test time. The training data, however, consists of images with independent descriptions in both source and target languages. Introduction The Shared Task on Multimodal Translation and Multilingual Image Description tackles the problem of generating descriptions of images for languages other than English. The vast majority of image description research has focused on Englishlanguage description due to the abundance of crowdsourced resources (Bernardi et al., 2016). However, there has been a significant amount of recent work on creating multilingual image description datasets in German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Turkish (Unal et al., 2016), Chinese (Li et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), and Dutch (van Miltenburg et al., 2017). Progress on this problem will be useful for native-language image search, multilingual ecommerce, and audio-described video for visually impaired viewers. The first empirical results for multimodal translation showed the potential for visual context to The translation task has been extended to include a new language, French. This extension means the Multi30K dataset (Elliott et al., 201"
W17-4718,N16-1022,0,0.0283339,"authors selected 1,000 images from the collection to form the dataset for the Multimodal Translation task based on a manual inspection of the English descriptions. Professional German translations were collected for those 1,000 English-described images. The remaining 1,071 images were used for the Multilingual Image Description task. We collected five ad2 http://www.crowdflower.com As a secondary evaluation dataset for the Multimodal Translation task, we collected and translated a set of image descriptions that potentially contain ambiguous verbs. We based our selection on the VerSe dataset (Gella et al., 2016), which annotates a subset of the COCO (Lin et al., 2014) and TUHOI (Le et al., 2014) images with OntoNotes senses for 90 verbs which are ambiguous, e.g. play. Their goals were to test the feasibility of annotating images with the word sense of a given verb (rather than verbs themselves) and to provide a gold-labelled dataset for evaluating automatic visual sense disambiguation methods. Altogether, the VerSe dataset contains 3,518 images, but we limited ourselves to its COCO section, since for our purposes we also need the image descriptions, which are not available in TUHOI. The COCO portion"
W17-4718,P16-1227,0,0.0406008,"training data, however, consists of images with independent descriptions in both source and target languages. Introduction The Shared Task on Multimodal Translation and Multilingual Image Description tackles the problem of generating descriptions of images for languages other than English. The vast majority of image description research has focused on Englishlanguage description due to the abundance of crowdsourced resources (Bernardi et al., 2016). However, there has been a significant amount of recent work on creating multilingual image description datasets in German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Turkish (Unal et al., 2016), Chinese (Li et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), and Dutch (van Miltenburg et al., 2017). Progress on this problem will be useful for native-language image search, multilingual ecommerce, and audio-described video for visually impaired viewers. The first empirical results for multimodal translation showed the potential for visual context to The translation task has been extended to include a new language, French. This extension means the Multi30K dataset (Elliott et al., 2016) is now triple aligned"
W17-4718,W17-4750,0,0.0641413,"Missing"
W17-4718,P16-4010,0,0.0212154,"Missing"
W17-4718,P07-2045,0,0.0170989,"etail in (Calixto et al., 2017b). They are model IMGW , in which image features are used as words in the source-language encoder; model IMGE , where image features are used to initialise the hidden states of the forward and backward encoder RNNs; and model IMGD , where the image features are used as additional signals to initialise the decoder hidden state. Each image has one corresponding feature vector, obtained from the activations of the NICT (Task 1) These are constrained submissions for both language pairs. First, a hierarchical phrase-based (HPB) translation system s built using Moses (Koehn et al., 2007) with standard features. Then, an attentional encoder-decoder network (Bahdanau et al., 2015) is trained and used as an additional feature to rerank the n-best output of the HPB system. A unimodal NMT model is also trained to integrate visual information. Instead of integrating visual features into the NMT model directly, image retrieval methods are employed to obtain target language descriptions of images that are similar to the image described by the source sentence, and this target description information is integrated into the NMT model. A multimodal 219 NMT model is also used to rerank th"
W17-4718,W14-5403,0,0.0178273,"l Translation task based on a manual inspection of the English descriptions. Professional German translations were collected for those 1,000 English-described images. The remaining 1,071 images were used for the Multilingual Image Description task. We collected five ad2 http://www.crowdflower.com As a secondary evaluation dataset for the Multimodal Translation task, we collected and translated a set of image descriptions that potentially contain ambiguous verbs. We based our selection on the VerSe dataset (Gella et al., 2016), which annotates a subset of the COCO (Lin et al., 2014) and TUHOI (Le et al., 2014) images with OntoNotes senses for 90 verbs which are ambiguous, e.g. play. Their goals were to test the feasibility of annotating images with the word sense of a given verb (rather than verbs themselves) and to provide a gold-labelled dataset for evaluating automatic visual sense disambiguation methods. Altogether, the VerSe dataset contains 3,518 images, but we limited ourselves to its COCO section, since for our purposes we also need the image descriptions, which are not available in TUHOI. The COCO portion covers 82 verbs; we further discarded verbs that are unambiguous in the dataset, i.e."
W17-4718,P17-2031,0,0.221406,"Missing"
W17-4718,D15-1166,0,0.00458431,"models - as measured by BLEU on the Multi30K development set - was used for both the constrained and unconstrained submissions. SHEF (Task 1) The SHEF systems utilize the predicted posterior probability distribution over the image object classes as image features. To do so, they make use of the pre-trained ResNet-152 (He et al., 2016), a deep CNN based image network that is trained over the 1,000 object categories on the Imagenet dataset (Deng et al., 2009) to obtain the posterior distribution. The model follows a standard encoder-decoder NMT approach using softdot attention as described in (Luong et al., 2015). It explores image information in three ways: a) to initialize the encoder; b) to initialize the decoder; c) to condition each source word with the image class posteriors. In all these three ways, non-linear affine transformations over the posteriors are used as image features. Baseline — Task 1 The baseline system for the multimodal translation task is a text-only neural machine translation system built with the Nematus toolkit (Sennrich et al., 2017). Most settings and hyperparameters were kept as default, with a few exceptions: batch size of 40 (instead of 80 due to memory constraints) and"
W17-4718,W17-4751,0,0.0653545,"Missing"
W17-4718,2006.amta-papers.25,0,0.119578,"ation on which the model focuses through the attention mechanism. 4 Text-similarity Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.3 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and 220 3 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 4.1.1 Task 1: English → German Multi30K 2017 test data Table 4 shows the results on the Multi30K 2017 test data with a German target language. It interesting to note that the metrics do not fully agree on the ranking of systems, although th"
W17-4718,W17-4752,1,0.893716,"Missing"
W17-4718,W16-2346,1,0.498931,"Missing"
W17-4718,P16-1168,0,0.0742711,"Missing"
W17-4718,P03-1021,0,0.033669,"re to rerank the n-best output of the HPB system. A unimodal NMT model is also trained to integrate visual information. Instead of integrating visual features into the NMT model directly, image retrieval methods are employed to obtain target language descriptions of images that are similar to the image described by the source sentence, and this target description information is integrated into the NMT model. A multimodal 219 NMT model is also used to rerank the HPB output. All feature weights (including the standard features, the NMT feature and the multimodal NMT feature) were tuned by MERT (Och, 2003). On the development set, the NMT feature improved the HPB system significantly. However, the multimodal NMT feature did not further improve the HPB system that had integrated the NMT feature. OREGONSTATE (Task 1) The OREGONSTATE system uses a very simple but effective model which feeds the image information to both encoder and decoder. On the encoder side, the image representation was used as an initialization information to generate the source words’ representations. This step strengthens the relatedness between image’s and source words’ representations. Additionally, the decoder uses alignm"
W17-4718,tiedemann-2012-parallel,0,0.032402,"and generates more accurate target side sentence. UvA-TiCC (Task 1) The submitted systems are Imagination models (Elliott and K´ad´ar, 2017), which are trained to perform two tasks in a multitask learning framework: a) produce the target sentence, and b) predict the visual feature vector of the corresponding image. The constrained models are trained over only the 29,000 training examples in the Multi30K dataset with a source-side vocabulary of 10,214 types and a target-side vocabulary of 16,022 types. The unconstrained models are trained over a concatenation of the Multi30K, News Commentary (Tiedemann, 2012) parallel texts, and MS COCO (Chen et al., 2015) dataset with a joint source-target vocabulary of 17,597 word pieces (Schuster and Nakajima, 2012). In both constrained and unconstrained submissions, the models were trained to predict the 2048D GoogleLeNetV3 feature vector (Szegedy et al., 2015) of an image associated with a source language sentence. The output of an ensemble of the three best randomly initialized models - as measured by BLEU on the Multi30K development set - was used for both the constrained and unconstrained submissions. SHEF (Task 1) The SHEF systems utilize the predicted po"
W17-4718,P02-1040,0,0.113412,"trained on the ImageNet dataset (Russakovsky et al., 2015). Those feature maps provide spatial information on which the model focuses through the attention mechanism. 4 Text-similarity Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.3 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and 220 3 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 4.1.1 Task 1: English → German Multi30K 2017 test data Table 4 shows the results on the Multi30K 2017 test data with a German target languag"
W17-4718,W17-3503,1,0.863632,"Missing"
W17-4718,P17-2066,0,0.0342394,"Multimodal Translation and Multilingual Image Description tackles the problem of generating descriptions of images for languages other than English. The vast majority of image description research has focused on Englishlanguage description due to the abundance of crowdsourced resources (Bernardi et al., 2016). However, there has been a significant amount of recent work on creating multilingual image description datasets in German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Turkish (Unal et al., 2016), Chinese (Li et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), and Dutch (van Miltenburg et al., 2017). Progress on this problem will be useful for native-language image search, multilingual ecommerce, and audio-described video for visually impaired viewers. The first empirical results for multimodal translation showed the potential for visual context to The translation task has been extended to include a new language, French. This extension means the Multi30K dataset (Elliott et al., 2016) is now triple aligned, with English descriptions translated into both German and French. The description generation task has substantially changed since last year. T"
W17-4718,Q14-1006,0,0.339227,". In last year’s Crosslingual Image Description task, the aim was to produce a single target language description, given five source language descriptions and the image. In this year’s Multilingual Image Description task, participants received only an unseen image at test time, without source language descriptions. 2.2 En: A group of people are eating noddles. De: Eine Gruppe von Leuten isst Nudeln. Fr: Un groupe de gens mangent des nouilles. Datasets The Multi30K dataset (Elliott et al., 2016) is the primary dataset for the shared task. It contains 31K images originally described in English (Young et al., 2014) with two types of multilingual data: a collection of professionally translated German sentences, and a collection of independently crowdsourced German descriptions. This year the Multi30K dataset has been extended with new evaluation data for the Translation and Image Description tasks, and an additional language for the Translation task. In addition, we released a new evaluation dataset featuring ambiguities that we expected would benefit from visual context. Table 1 presents an overview of the new evaluation datasets. Figure 1 shows an example of an image with an aligned English-German-Fren"
W17-4718,W16-2323,0,0.0161023,"urce word with the image class posteriors. In all these three ways, non-linear affine transformations over the posteriors are used as image features. Baseline — Task 1 The baseline system for the multimodal translation task is a text-only neural machine translation system built with the Nematus toolkit (Sennrich et al., 2017). Most settings and hyperparameters were kept as default, with a few exceptions: batch size of 40 (instead of 80 due to memory constraints) and ADAM as optimizer. In order to handle rare and OOV words, we used the Byte Pair Encoding Compression Algorithm to segment words (Sennrich et al., 2016b). The merge operations for word segmentation were learned using training data in both source and target languages. These were then applied to all training, validation and test sets in both source and target languages. In post-processing, the original words were restored by concatenating the subwords. Baseline — Task 2 The baseline for the multilingual image description task is an attention-based image description system trained over only the German image descriptions (Caglayan et al., 2017b). The visual representation are extracted from the so-called res4f relu layer from a ResNet-50 (He et"
W17-4718,W17-4753,0,0.0938137,"sh component of the model is ignored and only German captions are ID AFRL-OHIOSTATE Participating team Air Force Research Laboratory & Ohio State University (Duselis et al., 2017) CMU Carnegie Melon University (Jaffe, 2017) CUNI Univerzita Karlova v Praze (Helcl and Libovick´y, 2017) DCU-ADAPT Dublin City University (Calixto et al., 2017a) LIUMCVC Laboratoire d’Informatique de l’Universit´e du Maine & Universitat Autonoma de Barcelona Computer Vision Center (Caglayan et al., 2017a) NICT National Institute of Information and Communications Technology & Nara Institute of Science and Technology (Zhang et al., 2017) OREGONSTATE SHEF UvA-TiCC Oregon State University (Ma et al., 2017) University of Sheffield (Madhyastha et al., 2017) Universiteit van Amsterdam & Tilburg University (Elliott and K´ad´ar, 2017) Table 3: Participants in the WMT17 multimodal machine translation shared task. generated. FC7 layer of the VGG19 network, and consist of a 4096D real-valued vector that encode information about the entire image. CUNI (Tasks 1 and 2) For Task 1, the submissions employ the standard neural MT (NMT) scheme enriched with another attentive encoder for the input image. It uses a hierarchical attention combina"
W17-4718,P16-1162,0,0.00319904,"urce word with the image class posteriors. In all these three ways, non-linear affine transformations over the posteriors are used as image features. Baseline — Task 1 The baseline system for the multimodal translation task is a text-only neural machine translation system built with the Nematus toolkit (Sennrich et al., 2017). Most settings and hyperparameters were kept as default, with a few exceptions: batch size of 40 (instead of 80 due to memory constraints) and ADAM as optimizer. In order to handle rare and OOV words, we used the Byte Pair Encoding Compression Algorithm to segment words (Sennrich et al., 2016b). The merge operations for word segmentation were learned using training data in both source and target languages. These were then applied to all training, validation and test sets in both source and target languages. In post-processing, the original words were restored by concatenating the subwords. Baseline — Task 2 The baseline for the multilingual image description task is an attention-based image description system trained over only the German image descriptions (Caglayan et al., 2017b). The visual representation are extracted from the so-called res4f relu layer from a ResNet-50 (He et"
W17-4718,E17-3017,0,0.0333329,"Missing"
W17-4718,P11-2031,0,\N,Missing
W17-4718,W17-4747,0,\N,Missing
W17-4718,I17-1014,1,\N,Missing
W17-4718,N16-1021,0,\N,Missing
W17-4726,P02-1040,0,\N,Missing
W17-4726,P06-2093,0,\N,Missing
W17-4726,W13-5624,0,\N,Missing
W17-4726,W17-4705,0,\N,Missing
W17-4726,P16-1162,0,\N,Missing
W17-4726,P16-1009,0,\N,Missing
W17-4726,E17-2025,0,\N,Missing
W17-4746,P11-2031,0,0.132834,"Missing"
W17-4746,W16-3210,0,0.109813,"Missing"
W17-4746,W16-2358,1,0.858448,"Missing"
W17-4746,I17-1014,0,0.162356,"Missing"
W17-4746,J82-2005,0,0.668952,"Missing"
W17-4746,W16-2360,0,0.111793,"ities in the context of CIC (Caglayan et al., 2016b) and MMT (Calixto et al., 2017a). Besides experimenting with multimodal attention, Calixto et al. (2017a) and Libovick´y and Helcl (2017) also proposed a gating extension inspired from Xu et al. (2015) which is believed to allow the decoder to learn when to attend to a particular modality although Libovick´y and Helcl (2017) report no improvement over baseline NMT. There have also been attempts to benefit from different types of visual information instead of relying on features extracted from a CNN pretrained on ImageNet. One such study from Huang et al. (2016) extended the sequence of source embeddings consumed by the RNN with several regional features extracted from a region-proposal 432 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 432–439 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics network (Ren et al., 2015). The architecture thus predicts a single attention distribution over a sequence of mixed-modality representations leading to significant improvement over their NMT baseline. More recently, a radically different multi-task architecture called Imag"
W17-4746,Q17-1024,0,0.0577553,"Missing"
W17-4746,P16-1162,0,0.0598791,"ward and backward directions. Their hidden states are concatenated to form a set of source annotations S where each element si is a vector of dimension C = 2 × R:   #» GRUForw (X) M ×C  S= »# ∈ R GRUBack (X) Data We use the Multi30k (Elliott et al., 2016) dataset provided by the organizers which contains 29000, 1014 and 1000 English→{German,French} image-caption pairs respectively for training, validation and Test2016 (the official evaluation set of WMT16 campaign) set. Following task rules we normalized punctuations, applied tokenization and lowercasing. A Byte Pair Encoding (BPE) model (Sennrich et al., 2016) with 10K merge operations is learned for each language pair resulting in 5234→7052 tokens for English→German and 5945→6547 tokens for English→French respectively. We report results on Flickr Test2017 set containing 1000 image-caption pairs and the additional ambiguous MSCOCO test set (Elliott et al., 2017) of 461 image-caption pairs. Both encoders are equipped with layer normalization (Ba et al., 2016) where each hidden unit adaptively normalizes its incoming activations with a learnable gain and bias. Decoder A decoder block namely CGRU (two stacked GRUs where the hidden state of the first G"
W17-4746,W07-0734,0,0.0518108,"5) than the baseline using the approximate randomization test of multeval for 5 runs. (D6) is the official submission of LIUM-CVC. En→Fr.) An L2 regularization term with a factor of 1e−5 is also applied to avoid overfitting unless otherwise stated. Finally, we set E=128 and R=256 (Section 3) respectively for embedding and GRU dimensions. All models are implemented and trained with the nmtpy framework2 (Caglayan et al., 2017) using Theano v0.9 (Theano Development Team, 2016). Each experiment is repeated with 5 different seeds to mitigate the variance of BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) and to benefit from ensembling. The training is early stopped if validation set METEOR does not improve for 10 validations performed per 1000 updates. A beam-search with a beam size of 12 is used for translation decoding. 5 ble scores which are better than the baseline NMT in contrast to fusion-conv which fails to improve over it. Our submitted system (D6) achieves an ensembling score of 60.4 METEOR which is 1.2 better than NMT. Although the improvements are smaller, (D6) is still the best system on Test2017 in terms of ensembling/mean METEOR scores. One interesting point to be stressed at th"
W17-4746,P17-2031,0,0.252766,"Missing"
W17-4746,W16-2346,0,0.245952,"Missing"
W17-4746,P02-1040,0,0.0979394,"nificantly different (p-value ≤ 0.05) than the baseline using the approximate randomization test of multeval for 5 runs. (D6) is the official submission of LIUM-CVC. En→Fr.) An L2 regularization term with a factor of 1e−5 is also applied to avoid overfitting unless otherwise stated. Finally, we set E=128 and R=256 (Section 3) respectively for embedding and GRU dimensions. All models are implemented and trained with the nmtpy framework2 (Caglayan et al., 2017) using Theano v0.9 (Theano Development Team, 2016). Each experiment is repeated with 5 different seeds to mitigate the variance of BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) and to benefit from ensembling. The training is early stopped if validation set METEOR does not improve for 10 validations performed per 1000 updates. A beam-search with a beam size of 12 is used for translation decoding. 5 ble scores which are better than the baseline NMT in contrast to fusion-conv which fails to improve over it. Our submitted system (D6) achieves an ensembling score of 60.4 METEOR which is 1.2 better than NMT. Although the improvements are smaller, (D6) is still the best system on Test2017 in terms of ensembling/mean METEOR scores. One i"
W18-6402,W18-6438,1,0.861948,"Missing"
W18-6402,P11-2031,0,0.0245715,"5e−5 and a batch size of 64. The input embedding dimensionality was set to 128 and the remainder of the hyperparameters were kept as default. Bite-pair encoding with 10,000 merge operations was used for all language pairs. For Task 1b, only the English-Czech portion of the training corpus is used. 4 Automatic Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.5 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 Task 1: English → German Table 6 shows the results on the Test 2018 dataset with"
W18-6402,W18-6439,0,0.0587398,"Missing"
W18-6402,W14-3348,0,0.193691,"ality was set to 128 and the remainder of the hyperparameters were kept as default. Bite-pair encoding with 10,000 merge operations was used for all language pairs. For Task 1b, only the English-Czech portion of the training corpus is used. 4 Automatic Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.5 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 Task 1: English → German Table 6 shows the results on the Test 2018 dataset with a German target language. The first observation is that the best-p"
W18-6402,N13-1073,0,0.0242655,"t is the set of correct lexical translations of aw in the target language that conform to the context i. A word is said to be ambiguous in the source language if it has multiple translations (as given in the Multi30K corpus) with different meanings. We prepared the evaluation dataset following the procedure described in Lala and Specia (2018), with some additional steps. First, the parallel text in the Multi30K training and the validation sets are decompounded with SECOS (Riedl and Biemann, 2016) (for German only) and lemmatised3 . Second, we perform automatic word alignment using fast align (Dyer et al., 2013) to identify the English words that are aligned to two or more different words in the target language. This step results in a dictionary of {key : val} pairs, where key is a potentially ambiguous English word, and val is the set of words in the target language that align to key. This dictionary is then filtered by humans, students of translation studies who are fluent in both the source and target languages, to remove incorrect/noisy alignments and unambiguous instances, resulting in a cleaned dictionary containing {aw : lt} pairs, where aw is an ambiguous English word, and lt is the set of le"
W18-6402,W17-4718,1,0.509969,"using the image itself and its English description. This task can be addressed as either a pure translation task from the source English descriptions (ignoring the corresponding image), or as a multimodal translation task where the translation process is guided by the image in addition to the source description. Initial results in this area showed the potential for visual context to improve translation quality (Elliott et al., 2015; Hitschler et al., 2016). This was followed by a wide range of work in the first two editions of this shared task at the WMT in 2016 and 2017 (Specia et al., 2016; Elliott et al., 2017). This year we challenged participants to target the task of multimodal translation, with two variants: Task 1 is identical to previous editions of the shared task, however, it now includes an additional Czech target language. Therefore, participants can submit translations to any of the following languages: German, French and Czech. This extension means the Multi30K dataset (Elliott et al., 2016) is now 5-way aligned, with images described in English, which are translated into German, French and Czech.1 Task 1b is similar to Task 1; the main difference is that multiple source languages can be"
W18-6402,W16-3210,1,0.812877,"ation quality (Elliott et al., 2015; Hitschler et al., 2016). This was followed by a wide range of work in the first two editions of this shared task at the WMT in 2016 and 2017 (Specia et al., 2016; Elliott et al., 2017). This year we challenged participants to target the task of multimodal translation, with two variants: Task 1 is identical to previous editions of the shared task, however, it now includes an additional Czech target language. Therefore, participants can submit translations to any of the following languages: German, French and Czech. This extension means the Multi30K dataset (Elliott et al., 2016) is now 5-way aligned, with images described in English, which are translated into German, French and Czech.1 Task 1b is similar to Task 1; the main difference is that multiple source languages can be used (simultaneously) and Czech is the only target language. We introduce two new evaluation sets that extend the existing Multi30K dataset: a set of 1071 English sentences and their corresponding images and translations for Task 1, and 1,000 translations for the 2017 test set into Czech for Task 1b. Another new feature of this year’s shared task is the introduction of a new evaluation metric: Le"
W18-6402,I17-1014,1,0.823606,"Missing"
W18-6402,W18-6440,0,0.0330665,"Missing"
W18-6402,W18-6441,0,0.117332,"Missing"
W18-6402,P16-1227,0,0.120596,"ges, all parallel. Introduction The Shared Task on Multimodal Machine Translation tackles the problem of generating a description of an image in a target language using the image itself and its English description. This task can be addressed as either a pure translation task from the source English descriptions (ignoring the corresponding image), or as a multimodal translation task where the translation process is guided by the image in addition to the source description. Initial results in this area showed the potential for visual context to improve translation quality (Elliott et al., 2015; Hitschler et al., 2016). This was followed by a wide range of work in the first two editions of this shared task at the WMT in 2016 and 2017 (Specia et al., 2016; Elliott et al., 2017). This year we challenged participants to target the task of multimodal translation, with two variants: Task 1 is identical to previous editions of the shared task, however, it now includes an additional Czech target language. Therefore, participants can submit translations to any of the following languages: German, French and Czech. This extension means the Multi30K dataset (Elliott et al., 2016) is now 5-way aligned, with images desc"
W18-6402,P18-4020,0,0.0218896,"Missing"
W18-6402,P07-2045,0,0.00543567,"le 4: Statistics of dataset used for the LTA evaluation after human filtering. and their submission identifiers. AFRL-OHIO-STATE (Task 1) The AFRL-OHIO-STATE team builds on their previous year Visual Machine Translation (VMT) submission by combining it with text-only translation models. Two types of models were submitted: AFRL-OHIO-STATE 1 2IMPROVE U is a system combination of the VMT system and an instantiation of a Marian NMT model (Junczys-Dowmunt et al., 2018), and AFRL-OHIOSTATE 1 4COMBO U is a systems combination of the VMT system along with instantiations of Marian, OpenNMT, and Moses (Koehn et al., 2007). CUNI (Task 1) The CUNI submissions use two architectures based on the self-attentive Transformer model (Vaswani et al., 2017). For German and Czech, a language model is used to extract pseudo-in307 ID AFRL-OHIOSTATE CUNI LIUMCVC MeMAD OSU-BAIDU SHEF UMONS Participating team Air Force Research Laboratory & Ohio State University (Gwinnup et al., 2018) Univerzita Karlova v Praze (Helcl et al., 2018) Laboratoire d’Informatique de l’Universit´e du Maine & Universitat Autonoma de Barcelona Computer Vision Center (Caglayan et al., 2018) Aalto University, Helsinki University & EURECOM (Gr¨onroos et"
W18-6402,W18-6442,1,0.871581,"Missing"
W18-6402,L18-1602,1,0.908032,"ment and 2018 test datasets. The figures correspond to tuples with an image and parallel sentences in four languages: English, German, French and Czech. Dataset for LTA representing an instance in the test set, aw is an ambiguous word in English found in that instance i, and clt is the set of correct lexical translations of aw in the target language that conform to the context i. A word is said to be ambiguous in the source language if it has multiple translations (as given in the Multi30K corpus) with different meanings. We prepared the evaluation dataset following the procedure described in Lala and Specia (2018), with some additional steps. First, the parallel text in the Multi30K training and the validation sets are decompounded with SECOS (Riedl and Biemann, 2016) (for German only) and lemmatised3 . Second, we perform automatic word alignment using fast align (Dyer et al., 2013) to identify the English words that are aligned to two or more different words in the target language. This step results in a dictionary of {key : val} pairs, where key is a potentially ambiguous English word, and val is the set of words in the target language that align to key. This dictionary is then filtered by humans, st"
W18-6402,L16-1147,0,0.0341753,"nput image size for convolutional feature extraction process and found that multimodal attention without L2 normalisation performs significantly worse than baseline NMT. MeMAD (Task 1) The MeMAD team adapts the Transformer neural machine translation architecture to a multimodal setting. They use global image features extracted from Detectron (Girshick et al., 2018), a pre-trained object detection and localisation neural network, and two additional training corpora: MS-COCO (Lin et al., 2014) (an English multimodal dataset, which they extend with synthetic multilingual data) and OpenSubtitles (Lison and Tiedemann, 2016) (a multilingual, text-only dataset). Their experiments show that the effect of the visual features in the system is small; the largest differences in quality amongst the systems tested is attributed to the quality of the underlying text-only neural MT system. OSU-BAIDU (Tasks 1 and 1b) For Task 1, the OREGONSTATE system ensembles models including some neural machine translation models which only consider text information and multimodal machine translation models which also consider image information. Both types of models use global attention mechanism to align source to target words. For the"
W18-6402,P02-1040,0,0.10187,"kept as default. Bite-pair encoding with 10,000 merge operations was used for all language pairs. For Task 1b, only the English-Czech portion of the training corpus is used. 4 Automatic Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.5 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 Task 1: English → German Table 6 shows the results on the Test 2018 dataset with a German target language. The first observation is that the best-performing system, MeMAD 1 FLICKR DE MeMAD-OpenNMTmmod U, is sub"
W18-6402,D17-1120,0,0.0202937,"ates are re-ranked using word sense disambiguation (WSD) approaches: (i) most frequency sense (MFS), (ii) lexical translation (LT) and, (iii) multimodal lexical translation (MLT). Models (i) and (ii) are baselines, whilst MLT is a novel multimodal cross-lingual WSD model. The main idea is to have the cross-lingual WSD model select the translation candidate which correctly disambiguates ambiguous words in the source sentence and the intuition is that the image could help in the disambiguation process. The re-ranking cross-lingual WSD models are based on neural sequence learning models for WSD (Raganato et al., 2017; Yuan et al., 2016) trained on the Multimodal Lexical Translation Dataset (Lala and Specia, 2018). More specifically, they train LSTMs as taggers to disambiguate/translate every word in the source sentence. For Task 1b, the SHEF team explores three approaches. The first approach takes the concatenation of the 10-best translation candidates of German-Czech, French-Czech and English-Czech neural MT systems and then re-ranks them using the same multimodal cross-lingual WSD model as in Task 1. The second approach explores consensus between the different 10-best lists. The best hypothesis is selec"
W18-6402,N16-1075,0,0.024076,"taset for LTA representing an instance in the test set, aw is an ambiguous word in English found in that instance i, and clt is the set of correct lexical translations of aw in the target language that conform to the context i. A word is said to be ambiguous in the source language if it has multiple translations (as given in the Multi30K corpus) with different meanings. We prepared the evaluation dataset following the procedure described in Lala and Specia (2018), with some additional steps. First, the parallel text in the Multi30K training and the validation sets are decompounded with SECOS (Riedl and Biemann, 2016) (for German only) and lemmatised3 . Second, we perform automatic word alignment using fast align (Dyer et al., 2013) to identify the English words that are aligned to two or more different words in the target language. This step results in a dictionary of {key : val} pairs, where key is a potentially ambiguous English word, and val is the set of words in the target language that align to key. This dictionary is then filtered by humans, students of translation studies who are fluent in both the source and target languages, to remove incorrect/noisy alignments and unambiguous instances, resulti"
W18-6402,2006.amta-papers.25,0,0.106552,"ing with 10,000 merge operations was used for all language pairs. For Task 1b, only the English-Czech portion of the training corpus is used. 4 Automatic Metric Results The submissions were evaluated against either professional or crowd-sourced references. All submissions and references were pre-processed to lowercase, normalise punctuation, and tokenise the sentences using the Moses scripts.5 The evaluation was performed using MultEval (Clark et al., 2011) with the primary metric of Meteor 1.5 (Denkowski and Lavie, 2014). We also report the results using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics. The winning submissions are indicated by •. These are the topscoring submissions and those that are not significantly different (based on Meteor scores) according the approximate randomisation test (with p-value ≤ 0.05) provided by MultEval. Submissions marked with * are not significantly different from the Baseline according to the same test. 4.1 Task 1: English → German Table 6 shows the results on the Test 2018 dataset with a German target language. The first observation is that the best-performing system, MeMAD 1 FLICKR DE MeMAD-OpenNMTmmod U, is substantially better than other s"
W18-6402,W16-2346,1,0.724118,"Missing"
W18-6402,P14-5003,0,0.0511421,"Missing"
W18-6402,Q14-1006,0,0.277424,"rs, pages 304–323 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64029 the accuracy of a system at translating correctly a subset of ambiguous source language words. Participants could submit both constrained (shared task data only) and unconstrained (any data) systems for both tasks, with a limit of two systems per task variant and language pair per team. 2 Datasets The Multi30K dataset (Elliott et al., 2016) is the primary resource for the shared task. It contains 31K images originally described in English (Young et al., 2014) with two types of multilingual data: a collection of professionally translated German sentences, and a collection of independently crowdsourced German descriptions. Over the two last years, we have extended the Multi30K dataset with 2,071 new images and two additional languages for the translation task: French and Czech. Table 1 presents an overview of the new evaluation datasets. Figure 1 shows an example of an image with an aligned EnglishGerman-French-Czech description. This year we also released a new version of the evaluation datasets featuring a subset of sentences that contain ambiguou"
W18-6402,W18-6443,0,0.0405294,"Missing"
W18-6438,W17-4746,1,0.725041,"Missing"
W18-6438,W07-0734,0,0.0601817,"ty FirstName.LastName@univ-lemans.fr Kai Wang, Marc Masana, Luis Herranz and Joost van de Weijer CVC, Universitat Autonoma de Barcelona {kwang,mmasana,lherranz,joost}@cvc.uab.es Abstract refine the visual features by learning an encoderguided early spatial attention. In overall, we find that normalizing feature maps is crucial for the multimodal attention to obtain a comparable performance to monomodal NMT while the impact of the input image size remains unclear. Finally, with the help of the refined attention, we obtain modest improvements in terms of BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). The paper is organized as follows: data preprocessing, model details and training hyperparameters are detailed respectively in section 2 and section 3. The results based on automatic evaluation metrics are reported in section 4. Finally the paper ends with a conclusion in section 5. This paper describes the multimodal Neural Machine Translation systems developed by LIUM and CVC for WMT18 Shared Task on Multimodal Translation. This year we propose several modifications to our previous multimodal attention architecture in order to better integrate convolutional features and refine them using e"
W18-6438,D14-1179,1,0.0731185,"Missing"
W18-6438,P02-1040,0,0.102235,"¨ıc Barrault LIUM, Le Mans University FirstName.LastName@univ-lemans.fr Kai Wang, Marc Masana, Luis Herranz and Joost van de Weijer CVC, Universitat Autonoma de Barcelona {kwang,mmasana,lherranz,joost}@cvc.uab.es Abstract refine the visual features by learning an encoderguided early spatial attention. In overall, we find that normalizing feature maps is crucial for the multimodal attention to obtain a comparable performance to monomodal NMT while the impact of the input image size remains unclear. Finally, with the help of the refined attention, we obtain modest improvements in terms of BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). The paper is organized as follows: data preprocessing, model details and training hyperparameters are detailed respectively in section 2 and section 3. The results based on automatic evaluation metrics are reported in section 4. Finally the paper ends with a conclusion in section 5. This paper describes the multimodal Neural Machine Translation systems developed by LIUM and CVC for WMT18 Shared Task on Multimodal Translation. This year we propose several modifications to our previous multimodal attention architecture in order to better integrate convoluti"
W18-6438,P11-2031,0,0.038866,"ge width on the performance of multimodal attention variants. ConvAtt block is inspired from previous works in visual question answering (VQA) (Yang et al., 2016; Kazemi and Elqursh, 2017). It basically computes a spatial attention distribution β pre which we further use to mask the actual convolue replaces V in the tional features V. The filtered V equation 7 instead of being pooled into a single visual embedding in contrast to VQA models. EN→DE test2017 BLEU Results We train each model 4 times using different seeds and report mean and standard deviation for the final results using multeval (Clark et al., 2011) 3 599 www.statmt.org/wmt18/multimodal-task.html English→German # Params Baseline NMT 4.6M Multimodal Attention (MA) Filtered Attention (FA) 10.0M 11.3M test2017 (µ ± σ) BLEU METEOR TER 31.0 ± 0.3 30.8 ± 0.5 31.6 ± 0.5 52.1 ± 0.4 52.0 ± 0.2 52.5 ± 0.4 51.2 ± 0.5 51.1 ± 0.7 50.5 ± 0.5 Table 3: EN→DE results: Filtered attention is statistically different than the NMT (p ≤ 0.02). English→French # Params Baseline NMT 4.6M Multimodal Attention (MA) Filtered Attention (FA) 10.0M 11.3M test2017 (µ ± σ) BLEU METEOR TER 53.1 ± 0.3 52.6 ± 0.3 52.8 ± 0.2 69.9 ± 0.2 69.6 ± 0.3 69.6 ± 0.1 31.9 ± 0.8 31.9 ±"
W18-6438,W16-3210,0,0.279711,"Missing"
W18-6438,I17-1014,0,0.149957,"Missing"
W18-6438,E17-3017,0,0.0342971,"nd crop the images to 224x224 and 448x448. Features are then extracted from the final convolutional layer (res5c relu) of a pretrained ResNet50 (He et al., 2016) CNN.1 This led to feature maps V ∈ R2048×w×w where the spatial dimensionality w is 7 or 14. 2.1.1 3.1 Let us denote the length of the source sentence {x1 , . . . , xS } and the target sentence {y1 , . . . , yT } by S and T respectively. The source sentence is first encoded with a 2-layer bidirectional GRU to obtain the set of hidden states: Henc ← Enc({x1 , . . . , xS }), Henc ∈ RS×512 The decoder is a 2-layer conditional GRU (CGRU) (Sennrich et al., 2017) with tied embeddings (Press and Wolf, 2016). CGRU is a stacked 2-layer recurrence block with the attention mechanism in the middle. We use feed-forward attention (Bahdanau et al., 2014) which encapsulates a learnable layer. The first decoder (which is initialized with a zero vector) receives the previous target embeddings as inputs (equation 1). At each timestep of the decoding stage, the attention mechanisms produces a context vector ctxt t (equation 2) that becomes the input to the second GRU (equation 3). Finally, the probability over the target vocabulary is conditioned over a transformat"
W18-6438,P16-1162,0,0.0293368,"we propose to 2 Data We use Multi30k (Elliott et al., 2016) dataset provided by the organizers which contains 29000, 1014, 1000 and 1000 English→{German,French} sentence pairs respectively for train, dev, test2016 and test2017. A new training split of 30014 pairs is formed by concatenating the train and dev splits. Early-stopping is performed based on METEOR computed over the test2016 set and the final model selection is done over test2017. Punctuation normalization, lowercasing and aggressive hyphen splitting were applied to all sentences prior to training. A Byte Pair Encoding (BPE) model (Sennrich et al., 2016) with 10K merge operations is jointly learned on EnglishGerman and English-French resulting in vocabularies of 5189-7090 and 5830-6608 subwords respectively. 2.1 Visual Features Since Multi30k images involve much more complex region-level relationships and scene compositions compared to ImageNet (Russakovsky et al., 597 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 597–602 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64065 Tile Softmax Text Encoder 1x1x"
W18-6438,W17-4749,0,0.0436902,"Missing"
W18-6438,W16-2346,0,0.148204,"Missing"
W19-5301,W19-5424,1,0.858444,"Missing"
W19-5301,W19-5306,0,0.248769,"al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated"
W19-5301,D18-1549,0,0.116951,"s are available for this system. 2.5.7 BASELINE - RE - RERANK (no associated CUNI-T RANSFORMER -T2T2018 (Popel, 2018) is the exact same system as used last year. paper) BASELINE - RE - RERANK is a standard Transformer, with corpus filtering, pre-processing, postprocessing, averaging and ensembling as well as n-best list reranking. 2.5.8 CUNI-T RANSFORMER -M ARIAN (Popel et al., 2019) is a “reimplementation” of the last year’s system (Popel, 2018) in Marian (JunczysDowmunt et al., 2018). CA I RE (Liu et al., 2019) CUNI-U NSUPERVISED -NER- POST (Kvapilíková et al., 2019) follows the strategy of Artetxe et al. (2018), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel corpus. The synthetic corpus is produced by the seed phrase-based MT system or by a such a model refined through iterative back-translation. CUNI-U NSUPERVISED -NER- POST further focuses on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffer most. CA I RE is a hybrid system that took part only in the unsupervised track."
W19-5301,D18-1332,0,0.0215805,"et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔English translation task are that character-level model on the Chinese side can be used when translating into Chinese to improve the BLEU score. The same does not hold when transl"
W19-5301,W19-5351,0,0.0505622,"Missing"
W19-5301,W19-5423,0,0.0419015,"Missing"
W19-5301,W18-6412,1,0.856623,"Missing"
W19-5301,W19-5305,0,0.0496912,"Missing"
W19-5301,W12-3102,1,0.474924,"Missing"
W19-5301,W19-5310,0,0.0432396,"Missing"
W19-5301,W19-5425,0,0.0236032,"ys-Dowmunt et al., 2018) and Phrase-based machine translation system (implemented with Moses) and for the Spanish-Portuguese task. The system combination included features formerly presented in (Marie and Fujita, 2018), including scores left-to-right and right-to-left, sentence level translation probabilities and language model scores. Also authors provide contrastive results with an unsupervised phrase-based MT system which achieves quite close results to their primary system. Authors associate high performance of the unsupervised system to the language similarity. Incomslav: Team INCOMSLAV (Chen and Avgustinova, 2019) by Saarlad University participated in the Czech to Polish translation task only. The team’s primary submission builds on a transformer-based NMT baseline with back translation which has been submitted one of their contrastive submission. Incomslav’s primary system is a phoneme-based system re-scored using their NMT baseline. A second contrastive submission builds our phrase-based SMT system combined with a joint BPE model. NITS-CNLP: The NITS-CNLP team (Laskar et al., 2019) by the National Institute of Technology Silchar in India submitted results to the HI-NE translation task in both directi"
W19-5301,W07-0718,1,0.530103,"ojar Charles University Yvette Graham Barry Haddow Dublin City University University of Edinburgh Philipp Koehn JHU / University of Edinburgh Mathias Müller University of Zurich Marta R. Costa-jussà Christian Federmann UPC Microsoft Cloud + AI Shervin Malmasi Harvard Medical School Santanu Pal Saarland University Matt Post JHU Abstract Introduction The Fourth Conference on Machine Translation (WMT) held at ACL 20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to tra"
W19-5301,P16-2058,1,0.819865,"ipated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transformer (implemented with Fairseq (Ott et al., 2019)) for the Czechto-Polish task and a Phrase-based system (implemented with Moses (Koehn et al., 2007)) for Spanish-to-Portuguese. They tested adding monolingual data to the NMT system by copying the same data on the source and target sides, with negative results. Also, their system combination based on sentence-level BLEU in back-translation 5.4 Conclusion of Simi"
W19-5301,W08-0309,1,0.659809,"Missing"
W19-5301,W18-3931,1,0.820211,"or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language pairs: Spanish - Portuguese (Romance languages), Czech - Polis"
W19-5301,W19-5312,0,0.0773587,"Missing"
W19-5301,W19-5313,0,0.0931699,"University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of"
W19-5301,W19-5314,0,0.0200465,"Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 2.5.6 BTRANS only the middle sentence was considered for the final translation hypothesis, otherwise shorter context of two sentences or just a single sentence was used. Unfortunately, no details are available for this system. 2.5.7 BASELINE - RE - RERANK (no associ"
W19-5301,D18-1045,0,0.0285693,"xt was morphologically segmented with Apertium. The UEDIN systems are supervised NMT systems based on the transformer architecture and trained using Marian (Junczys-Dowmunt et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔Engl"
W19-5301,W18-6410,0,0.0193718,"ormance can be found in Hindi-Nepali (both directions), where the best performing system is around 50 BLEU (53 for Hindi-to-Nepali and 49.1 for Nepali-toHindi), and the lowest entry is 1.4 for Hindi-toNepali and 0 for Nepali-to-Hindi. The lowest variance is for Polish-to-Czech and it may be because only two teams participated. UHelsinki: The University of Helsinki team (Scherrer et al., 2019) participated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transform"
W19-5301,W19-5317,0,0.114089,"ed paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 201"
W19-5301,W19-5315,0,0.0266353,"Missing"
W19-5301,W19-5318,0,0.198338,"ance of the systems when translating from French to German seems to heavily depend on the 7 http://data.statmt.org/wmt19/ translation-task/dev.tgz 6 Systems MSRA.MADL eTranslation LIUM MLLP-UPV onlineA TartuNLP onlineB onlineY onlineG onlineX FULL 47.3 45.4 43.7 41.5 40.8 39.2 39.1 39.0 38.5 38.1 source FR 38.3 37.4 37.5 36.4 35.4 34.8 35.3 34.7 34.6 35.6 source DE 50.0 47.8 45.5 43.0 42.3 40.5 40.2 40.2 39.7 38.8 evaluations. In the rest of this sub-section, we provide brief details of the submitted systems, for those in cases where the authors provided such details. 2.5.1 AFRL - SYSCOMB 19 (Gwinnup et al., 2019) is a system combination of a Marian ensemble system, two distinct OpenNMT systems, a Sockeyebased Elastic Weight Consolidation system, and one Moses phrase-based system. Table 3: French→German Meteor scores. Systems MSRA.MADL LinguaCustodia MLLP_UPV Kyoto_University_T2T LIUM onlineY onlineB TartuNLP onlineA onlineX onlineG FULL 52.0 51.3 49.5 48.8 48.3 47.5 46.4 46.3 45.3 42.7 41.7 source FR 51.9 52.5 49.9 49.7 46.5 43.7 43.7 45.0 43.7 41.6 40.9 source DE 52.0 51.0 49.4 48.6 48.7 48.4 47.0 46.7 45.8 42.9 41.9 AFRL- EWC (Gwinnup et al., 2019) is a Sockeye Transformer system trained with the de"
W19-5301,W19-5316,0,0.109691,"boratory (Gwinnup et al., 2019) Apertium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of"
W19-5301,E14-1047,1,0.904335,"Missing"
W19-5301,W19-5427,0,0.0467733,"Missing"
W19-5301,W19-5322,1,0.807321,"Missing"
W19-5301,W19-5302,1,0.715203,"20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We 1 Christof Monz University of Amsterdam Marcos Zampieri University of Wolverhampton held 18 translation tasks this year, between English and each of Chinese, Czech (into Czech only), German, Finnish, Lithuanian, and Russian. New this year were Gujarati↔English and Kazakh↔English. B"
W19-5301,W19-5333,0,0.0923169,"Missing"
W19-5301,W19-5353,0,0.0658382,"Missing"
W19-5301,W19-5430,1,0.873847,"Missing"
W19-5301,W19-5431,0,0.0199622,"Universitat Politècnica de València (UPV) participated with a Transformer (implemented with FairSeq (Ott et al., 2019)) and a finetuning strategy for domain adaptaion in the task of Spanish-Portuguese. Fine-tunning on the development data provide improvements of almost 12 BLEU points, which may explain their clear best performance in the task for this language pair. As a contrastive system authors provided only for the Portuguese-to-Spanish a novel 2D alternating RNN model which did not respond so well when fine-tunning. UBC-NLP: Team UBC-NLP from the University of British Columbia in Canada (Przystupa and Abdul-Mageed, 2019) compared the performance of the LSTM plus attention (Bahdanau et al., 2015) and Transformer (Vaswani et al., 2017) (implemented in OpenNMT toolkit22 ) perform for the three tasks at hand. Authors use backtranslation to introduce monolingual data in their systems. LSTM plus attention outperformed Transformer for Hindi-Nepali, and viceversa for the other two tasks. As reported by the authors, Hindi-Nepali task provides much more shorter sentences than KYOTOUNIVERSITY: Kyoto University’s submission, listed simply as KYOTO in Table 25 for PT → ES task is based on transformer NMT system. They used"
W19-5301,P02-1040,0,0.11337,"ation of the source (CS), and a second encoder to encode sub-word (byte-pair-encoding) information of the source (CS). The results obtained by their system in translating from Czech→Polish and comment on the impact of out-of-domain test data in the performance of their system. UDSDFKI ranked second among ten teams in Czech– Polish translation. 5.3 Results We present results for the three language pairs, each of them in the two possible directions. For this first edition of the Similar Translation Task and differently from News task, evaluation was only performed on automatic basis using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) measures. Each language direction is reported in one different table which contain information of the team; type of system, either contrastive (C) or primary (P), and the BLEU and TER results. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. Even if we are presenting 3 pairs of languages each pair belonging to the same family, translation quality in terms of BLEU varies signficantly. While the best systems for Spanish-Portuguese are above 64 BLEU and below 21 TER (see Tables 26 and 27), best syste"
W19-5301,W19-5354,0,0.0611791,"Missing"
W19-5301,W18-6486,0,0.0189853,"the agglutinative nature of Kazakh, (ii) data from an additional language (Russian), given the scarcity of English–Kazakh data and (iii) synthetic data for the source language filtered using language-independent sentence similarity. RUG _ KKEN _ MORFESSOR Tilde developed both constrained and unconstrained NMT systems for English-Lithuanian and Lithuanian-English using the Marian toolkit. All systems feature ensembles of four to five transformer models that were trained using the quasi-hyperbolic Adam optimiser (Ma and Yarats, 2018). Data for the systems were prepared using TildeMT filtering (Pinnis, 2018) and preprocessing (Pinnis et al., 2018) methods. For unconstrained systems, data were additionally filtered using dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018a). All systems were trained using iterative back-translation (Rikters, 2018) and feature synthetic data that allows training NMT systems to support handling of unknown phenomena (Pinnis et al., 2017). During translation, automatic named entity and nontranslatable phrase post-editing were performed. For constrained systems, named entities and nontranslatable phrase lists were extracted from the parallel training data."
W19-5301,W19-5335,0,0.0408704,"Missing"
W19-5301,W19-5344,1,0.904781,"n Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas e"
W19-5301,W19-5346,0,0.197908,"rtium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communicatio"
W19-5301,W19-5341,0,0.0172601,"A,B,G,X,Y. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human AYLIEN _ MULTILINGUAL (Hokamp et al., 2019) The Aylien research team built a Multilingual NMT system which is trained on all WMT2019 language pairs in all directions, then fine-tuned for a small number of iterations on Gujarati-English data only, including some self-backtranslated data. 2.5.5 BAIDU (Sun et al., 2019) Baidu systems are based on the Transformer architecture with several improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. 7 Team AFRL A PERTIUM - FIN - ENG A PPRENTICE - C AYLIEN _ MULTILINGUAL BAIDU BTRANS BASELINE - RE - RERANK CA I RE CUNI DBMS-KU DFKI - NMT E T RANSLATION FACEBOOK FAIR GTCOM H ELSINKI NLP IIITH-MT IITP JHU JUMT JU_S AARLAND KSAI K YOTO U NIVERSITY L INGUA C USTODIA LIUM LMU-NMT MLLP-UPV MS T RANSLATOR MSRA N IU T RANS NICT NRC PARFDA"
W19-5301,P16-1162,1,0.310296,"ssible, 2.5.13 E T RANSLATION (Oravecz et al., 2019) E T RANSLATION En-De E T RANSLATION ’s EnDe system is an ensemble of 3 base Transformers and a Transformer-type language model, trained from all available parallel data (cleaned up and filtered with dual conditional cross-entropy filtering) and with additional back-translated data generated 9 2.5.17 from monolingual news. Each Transformer model is fine tuned on previous years’ test sets. H ELSINKI NLP is a Transformer (Vaswani et al., 2017) style model implemented in OpenNMTpy using a variety of corpus filtering techniques, truecasing, BPE (Sennrich et al., 2016), backtranslation, ensembling and fine-tuning for domain adaptation. E T RANSLATION Fr-De The Fr-De system is an ensemble of 2 big Transformers (with size 8192 FFN layers). Back-translation data was selected using topic modelling techniques to tune the model towards the domain defined in the task. 2.5.18 En-Lt The En-Lt system is an ensemble of 2 big Transformers (as for Fr-De) and a Transformer type language model. The training data contains the Rapid corpus and the news domain back-translated data sets 2 times oversampled. E T RANSLATION 2.5.19 FACEBOOK FAIR (Ng et al., 2019) 2.5.20 JHU (Mar"
W19-5301,W19-5339,0,0.0767002,"Missing"
W19-5301,W19-5347,0,0.0328257,"Missing"
W19-5301,W19-5342,1,0.887858,"encia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and"
W19-5301,W19-5355,1,0.869964,"Missing"
W19-5301,W19-5350,0,0.0441915,"Missing"
W19-5301,P98-2238,0,0.38957,"n trained to translate texts from and to English or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language"
W19-5307,P07-2045,0,0.020011,"Missing"
W19-5307,P10-2041,0,0.0214214,"to the small transformer model, we also used 2 This was carried out by LinguaCustodia dev and test sets can be downloaded from https:// github.com/lium-lst/euelections 3 130 europarl-v7 Common Crawl ParaCrawl dev08-14 #lines 1.7M 585k 6.7M 18k #token FR 45.9M 13M 107M 417.1k de → fr 1. Small Transformer (x1) +Ensemble (x2) +Ensemble (x5) 2. Big Transformer (x1) +Ensemble (x2) +Ensemble (x5)* #token DE 40.9 11M 95M 369.5k Table 3: Training corpora statistics for FR↔DE systems after the cleaning process. Table 4: BLEU results for DE→FR NMT systems using all training data but ParaCrawl corpus. (Moore and Lewis, 2010). Data selection was performed with the europarl corpus as in-domain data and using the XenC Toolkit (Rousseau, 2013). By doing this, we were able to extract 3.4M German sentences out of the 38.6M sentences of the monolingual German 2018 News Crawl corpus. Similarly, 3.3M sentences were extracted out of the 8.2M monolingual French 2018 News Crawl. 4 for the big model compared to the results reported in table 4 (without ParaCrawl). de → fr 1. Small Transformer (x1) +Ensemble (x2) +Ensemble (x5) 2. Big Transformer (x1) +Ensemble (x2) +Ensemble (x5) Experiments and Results In this section, we fir"
W19-5307,N19-4009,0,0.0157442,"l data, we have used monolingual News Crawl articles as additional synthetic bilingual data. We used only news 2018 from which we selected a subpart based on cross-entropy data selection method LIUM Submissions All our systems are constrained as we only used the supplied parallel data (described in table 1) with additional back-translations created from a subset of the monolingual news data made available by the shared task organizers. 3.1 Data Preparation Model Description For our submissions we used the Transformer (Vaswani et al., 2017) sequence-to-sequence model as implemented in fairseq (Ott et al., 2019). Transformer is the state of the art NMT model which rely on a multi-headed attention applied as self-attention to source and target sentences. Our models are based on both small and big Transformer configurations. All experiments with the big transformer are models with 6 blocks in the encoder and decoder networks following the configuration described in (Ott et al., 2018). With respect to the small transformer model, we also used 2 This was carried out by LinguaCustodia dev and test sets can be downloaded from https:// github.com/lium-lst/euelections 3 130 europarl-v7 Common Crawl ParaCrawl"
W19-5307,W18-6301,0,0.0156395,"ingual news data made available by the shared task organizers. 3.1 Data Preparation Model Description For our submissions we used the Transformer (Vaswani et al., 2017) sequence-to-sequence model as implemented in fairseq (Ott et al., 2019). Transformer is the state of the art NMT model which rely on a multi-headed attention applied as self-attention to source and target sentences. Our models are based on both small and big Transformer configurations. All experiments with the big transformer are models with 6 blocks in the encoder and decoder networks following the configuration described in (Ott et al., 2018). With respect to the small transformer model, we also used 2 This was carried out by LinguaCustodia dev and test sets can be downloaded from https:// github.com/lium-lst/euelections 3 130 europarl-v7 Common Crawl ParaCrawl dev08-14 #lines 1.7M 585k 6.7M 18k #token FR 45.9M 13M 107M 417.1k de → fr 1. Small Transformer (x1) +Ensemble (x2) +Ensemble (x5) 2. Big Transformer (x1) +Ensemble (x2) +Ensemble (x5)* #token DE 40.9 11M 95M 369.5k Table 3: Training corpora statistics for FR↔DE systems after the cleaning process. Table 4: BLEU results for DE→FR NMT systems using all training data but ParaC"
W19-5307,P02-1040,0,0.105891,"Missing"
W19-5307,P16-1009,0,0.0327264,"7M/51.3M) 31.8M (467M/502M) – – FR-DE 1.7M (46M / 41M) 622k (14M/12.2M) 7.2M (110.6M/99.6M) 18k (417.1k/369.5k) Table 1: Training corpora statistics (number of sentences) for FR↔DE News translation shared task. The second line of each cell corresponds to the number of tokens in French followed by the number of tokens in German. a 6 blocks encoder and decoder network with an embedding layer of size 512, a feed-forward layer with an inner dimension of 1024, and a multiheaded attention with 4 attention heads. We use a vocabulary of 35K units based on a joint source and target byte pair encoding (Sennrich et al., 2016). We set the batch size to 2048 tokens and maximum sentence length to 150 BPE units, in order to fit the big Transformer configuration to our GPUs (NVIDIA GeForce GTX 1080 Ti with 11 GB RAM). been followed for the test set creation: 335 of the 1701 test sentences have been produced from French documents and the 1366 remaining pairs from German documents. We note that 756 out of the German 1366 German sentences in the test set have been translated into French by professional translators2 . The dev and test sets are freely distributed and available for download3 . dev2019 test2019 #lines 1512 17"
