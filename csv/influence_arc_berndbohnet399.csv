2007.mtsummit-ucnlg.6,P05-1067,0,0.0187425,"n the surface realization step. The input to the surface realizer is defined by the standard architecture RAGS for NLG systems as “syntactic representations” which is “based on some notion of abstract syntactic structure which does not encode surface constituency or word order”, cf. page 17 (Mellish et al., 2006). These input structures are usually dependency structures. In MT, statistical n-gram approaches are rather successful and therefore mostly used. Nevertheless, some systems use dependency trees in order to improve the results for instance Lavoie et al. (2000), ˇ Cmejrek et al. (2003), Ding and Palmer (2005) and 38 John wird LK nach Berlin reisen RK . ’John will to Berlin. travel’lit. weil LK es ihm gef¨allt RK . ’because it he likes. ’lit. The syntax determines the constituents of the brackets. In relation to the brackets, three positions are possible: before the left bracket (pre-field), in the middle, i.e., between the left and the right brackets (middle field), and after the right bracket (postfield). Nach Berlin ’to Berlin VF ist is LK er um 5 John at 5am MF abgereist RK . departed’lit. This kind of approach has been developed only with regard to German and a few other languages, such as anc"
2007.mtsummit-ucnlg.6,P01-1024,0,0.0213712,"these, we used to learn linearization grammars the Catalan corpus, the English Penn Treebank, the German Tiger corpus, and the Italian corpus . 2 The two Elements of Topological Models The two elements of topological models can be discovered by analyzing various topological models, that have been developed mainly for text generation. Br¨oker (1997) describes word order by domains and explicit precedence relations. The domains do not allow the words from outside of a domain to go between words contained in a domain. A topology is derived with rules based on modal logic from a dependency tree. Duchier and Debusmann (2001) use a tree to describe the linear precedence (LP). The tree is projective and partially ordered. The edges are labelled with the names of topological fields. The LP tree is derived from a dependency tree called immediate dominance tree (ID). The LP tree is computed from the ID tree by a constrained based approach using lexical constraints and conditions for the claiming of nodes. The topological model of Gerdes (2002) consist of topological fields and boxes. Boxes contain fields which contain recursively boxes and boxes again contain fields. The boxes and fields are supplied in form of a list"
2007.mtsummit-ucnlg.6,P95-1024,0,0.0703472,"conditions for the claiming of nodes. The topological model of Gerdes (2002) consist of topological fields and boxes. Boxes contain fields which contain recursively boxes and boxes again contain fields. The boxes and fields are supplied in form of a list. Both behave like domains. A topology is derived by traversing the dependency tree top down. Each time a word is placed in a box or field, depending on its subcatframe, new boxes or fields are created within the box or field. Word order domains have been used also for the linearization of phrase structures, cf. (Reape, 1993), (Rambow, 1994), (Kathol and Pollard, 1995). Reape describes word order in terms of the containers which are associated with phrases. A container can include recursively other containers and words. For the mapping from phrase structures to such a container structure, the continuous phrases are associated with a container and the word of discontinuous phrases are included in the parent container. The order between the words is kept during the mapping. The topological models above define word order either in terms of lists or sets. The sets need additional precedence relations for ordering the elements. A model using sets and precedence"
2007.mtsummit-ucnlg.6,H01-1014,0,0.0251005,"ase structures fulfil already the requirements for a topological model with regard to English as well as with regard to quite a few other languages. Phrase structures describe the syntax by inclusion of constituent in other constituents and the position within the constituents. In the following example on the left, the and kitchen are constituents within the constituent c. This can occur recursively, as shown in the example below. thea kitchenb c ina the kitchen b c Figure 1 shows a dependency tree. The words of the dependency tree are not ordered. Therefore, the word order has to be derived. Xia and Palmer (2001) describe a method for converting dependency structures to phrase structures. The conversion grammar is trained on corpora annotated with dependency structures and phrase structures. In order to learn word order rules, this method can be simplified, so that only dependency structures and the word order is needed. has SBJ VC John read OBJ book NMOD a ADV about PMOD Berlin Figure 1: Dependency Tree. Bohnet and Seniv (2004) introduced a method to map dependency structures to unordered phrase structures for languages with free word order. Since the constituents are not ordered, additional rules ar"
2020.acl-main.173,P19-1483,0,0.41514,"ata-to-text generation (Lebret et al., 2016; Wiseman et al., 2017) which are not open-ended, require models to be factual and/or faithful to the source text. Despite recent improvements in conditional text generation, most summarization systems are trained to maximize the log-likelihood of the reference summary at the word-level, which does not necessarily reward models for being faithful. Moreover, models are usually agnostic to the noises or artifacts of the training data, such as reference divergence, making them vulnerable to hallucinations (Kryscinski et al., 2019a; Wiseman et al., 2017; Dhingra et al., 2019). Thus, models can generate texts that are not consistent with the input, yet would likely have reasonable model log-likelihood. 2.1 Intrinsic and Extrinsic Hallucinations Given a document D and its abstractive summary S, we try to identify all hallucinations in S with respect to the content of D, regardless of the quality of the summary. In this work, we define a summary as being hallucinated if it has a span(s) wi . . . wi+j , j ≥ i, that is not supported by the input document. To distinguish hallucinations further in the context of a document and a summary, we categorize hallucinations by t"
2020.acl-main.173,P16-1154,0,0.0367126,"ot only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.1 1 Introduction Current state of the art conditional text generation models accomplish a high level of fluency and coherence, mostly thanks to advances in sequenceto-sequence architectures with attention and copy (Sutskever et al., 2014; Bahdanau et al., 2015; Gu et al., 2016), fully attention-based Transformer architectures (Vaswani et al., 2017; Dai et al., 2019) and more recently pretrained language modeling for natural language understanding (Devlin et al., 2019; Radford et al., 2018; Yang et al., 2019; Liu et al., 2019). There has been a growing interest in ∗ The first two authors contributed equally. Our human annotated summaries for faithfulness and factuality will be released at https://github.com/google-researchdatasets/xsum hallucination annotations. 1 understanding how maximum likelihood training and approximate beam-search decoding in these models lead"
2020.acl-main.173,2021.ccl-1.108,0,0.226531,"Missing"
2020.acl-main.173,W01-0100,0,0.502282,"tions. 1 understanding how maximum likelihood training and approximate beam-search decoding in these models lead to less human-like text in open-ended text generation such as language modeling and story generation (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In this paper we investigate how these models are prone to generate hallucinated text in conditional text generation, specifically, extreme abstractive document summarization (Narayan et al., 2018a). Document summarization — the task of producing a shorter version of a document while preserving its information content (Mani, 2001; Nenkova and McKeown, 2011) — requires models to generate text that is not only human-like but also faithful and/or factual given the document. The example in Figure 1 illustrates that the faithfulness and factuality are yet to be conquered by conditional text generators. The article describes an event of “Conservative MP Zac Smith winning the primary for 2016 London mayoral election”, but summaries often forge entities (e.g., “Nigel Goldsmith” or “Zac Goldwin”) or information (e.g., “UKIP leader Nigel Goldsmith”, “Nigel Goldsmith winning the mayoral election”, “Sadiq Khan being the former Lo"
2020.acl-main.173,D18-1206,1,0.740796,"uman annotated summaries for faithfulness and factuality will be released at https://github.com/google-researchdatasets/xsum hallucination annotations. 1 understanding how maximum likelihood training and approximate beam-search decoding in these models lead to less human-like text in open-ended text generation such as language modeling and story generation (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In this paper we investigate how these models are prone to generate hallucinated text in conditional text generation, specifically, extreme abstractive document summarization (Narayan et al., 2018a). Document summarization — the task of producing a shorter version of a document while preserving its information content (Mani, 2001; Nenkova and McKeown, 2011) — requires models to generate text that is not only human-like but also faithful and/or factual given the document. The example in Figure 1 illustrates that the faithfulness and factuality are yet to be conquered by conditional text generators. The article describes an event of “Conservative MP Zac Smith winning the primary for 2016 London mayoral election”, but summaries often forge entities (e.g., “Nigel Goldsmith” or “Zac Goldwin"
2020.acl-main.173,N18-1158,1,0.86231,"uman annotated summaries for faithfulness and factuality will be released at https://github.com/google-researchdatasets/xsum hallucination annotations. 1 understanding how maximum likelihood training and approximate beam-search decoding in these models lead to less human-like text in open-ended text generation such as language modeling and story generation (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In this paper we investigate how these models are prone to generate hallucinated text in conditional text generation, specifically, extreme abstractive document summarization (Narayan et al., 2018a). Document summarization — the task of producing a shorter version of a document while preserving its information content (Mani, 2001; Nenkova and McKeown, 2011) — requires models to generate text that is not only human-like but also faithful and/or factual given the document. The example in Figure 1 illustrates that the faithfulness and factuality are yet to be conquered by conditional text generators. The article describes an event of “Conservative MP Zac Smith winning the primary for 2016 London mayoral election”, but summaries often forge entities (e.g., “Nigel Goldsmith” or “Zac Goldwin"
2020.acl-main.173,N04-1019,0,0.253075,"quality of summaries (Nenkova, 2005). Most popular among them is the automatic metric ROUGE (Lin and Hovy, 2003) that measures the unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a proxy for assessing informativeness and the longest common subsequence (ROUGE-L), for fluency. ROUGE, however, can be misleading when used as the only means to assess the informativeness of summaries (Schluter, 2017). Hence, the ROUGE score is often complemented with subjective human assessment of summaries. More objective measures have been proposed to improve agreement among human annotators. Pyramid method (Nenkova and Passonneau, 2004) requires summaries to be annotated by experts for salient information. Narayan et al. (2018a,b) used a questionanswering based approach where a summary is used as context to answer questions which were written based on its reference summary. Hardy et al. (2019) proposed a reference-less approach where a summary is assessed against the source document, highlighted with its pertinent content. There has not been much work on evaluating faithfulness and truthfulness of abstractive summaries. The automatic evaluation such as ROUGE and the human evaluation of saliency and linguistic quality of summ"
2020.acl-main.173,P19-1459,0,0.0611913,"Missing"
2020.acl-main.173,N18-2102,0,0.222406,"ly sampled 500 articles from the test set to facilitate our study. Using the full test set was unfeasible given its size and the cost of human judgments. We have trained annotators (fluent in English) specifically for our assessment. Our annotators went through two pilot studies to have a better understanding of intrinsic and extrinsic hallucinations, and factuality of summaries. Documents used in the pilot studies were not used in the final annotation. We also report on ROUGE (Lin and Hovy, 2003) scores, BERTScore (Zhang et al., 2020) and semantic inference metric such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019; Falke et al., 2019; Kryscinski et al., 2019b) and question answering (Arumae and Liu, 2019; Wang et al., 2020). 5.1 Automatic Evaluation of Summaries ROUGE (Lin and Hovy, 2003) provides a means to quickly assess a model’s ability to generate summaries closer to human authored summaries. We report on ROUGE-1 and ROUGE-2 for informativeness and ROUGE-L, for fluency. Like ROUGE, BERTScore (Zhang et al., 2020) computes a similarity score for each token in the candidate summary with each token in the reference summary. However, instead of exact matches, it computes token sim"
2020.acl-main.173,2020.tacl-1.18,1,0.943458,"tion (e.g., “UKIP leader Nigel Goldsmith”, “Nigel Goldsmith winning the mayoral election”, “Sadiq Khan being the former London mayor” or “Zac Goldwin being the Labour’s candidate”) that are not supported by the document or are factually wrong. Interestingly, all summaries are topical and fluent, and perform well in terms of ROUGE scores (Lin and Hovy, 2003). We conducted a large-scale human evaluation of hallucinated content in systems that use Recurrent Neural Network (RNN) (See et al., 2017), Convolutional Neural Network (CNN) (Narayan et al., 2018a), and Transformers (Radford et al., 2019; Rothe et al., 2020), as well as human written summaries for the recently introduced eXtreme S UMmarization task (XS UM, Narayan et al., 2018a). We seek to answer the following questions: (i) How frequently do abstractive summarizers hallucinate content?; (ii) Do models hal1906 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919 c July 5 - 10, 2020. 2020 Association for Computational Linguistics G OLD Zac Goldsmith will contest the 2016 London mayoral election for the Conservatives, it has been announced. D OCUMENT: The Richmond Park and North Kingston MP said"
2020.acl-main.173,E17-2007,0,0.0221606,"e a good balance between ROUGE and better faithfulness. 6 Related Work Following the Document Understanding Conference (DUC; Dang, 2005), a majority of work has focused on evaluating the content and the linguistic quality of summaries (Nenkova, 2005). Most popular among them is the automatic metric ROUGE (Lin and Hovy, 2003) that measures the unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a proxy for assessing informativeness and the longest common subsequence (ROUGE-L), for fluency. ROUGE, however, can be misleading when used as the only means to assess the informativeness of summaries (Schluter, 2017). Hence, the ROUGE score is often complemented with subjective human assessment of summaries. More objective measures have been proposed to improve agreement among human annotators. Pyramid method (Nenkova and Passonneau, 2004) requires summaries to be annotated by experts for salient information. Narayan et al. (2018a,b) used a questionanswering based approach where a summary is used as context to answer questions which were written based on its reference summary. Hardy et al. (2019) proposed a reference-less approach where a summary is assessed against the source document, highlighted with i"
2020.acl-main.173,P17-1099,0,0.649396,"2016 London mayoral election”, but summaries often forge entities (e.g., “Nigel Goldsmith” or “Zac Goldwin”) or information (e.g., “UKIP leader Nigel Goldsmith”, “Nigel Goldsmith winning the mayoral election”, “Sadiq Khan being the former London mayor” or “Zac Goldwin being the Labour’s candidate”) that are not supported by the document or are factually wrong. Interestingly, all summaries are topical and fluent, and perform well in terms of ROUGE scores (Lin and Hovy, 2003). We conducted a large-scale human evaluation of hallucinated content in systems that use Recurrent Neural Network (RNN) (See et al., 2017), Convolutional Neural Network (CNN) (Narayan et al., 2018a), and Transformers (Radford et al., 2019; Rothe et al., 2020), as well as human written summaries for the recently introduced eXtreme S UMmarization task (XS UM, Narayan et al., 2018a). We seek to answer the following questions: (i) How frequently do abstractive summarizers hallucinate content?; (ii) Do models hal1906 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919 c July 5 - 10, 2020. 2020 Association for Computational Linguistics G OLD Zac Goldsmith will contest the 2016 Londo"
2020.acl-main.173,K19-1079,0,0.10635,"al language understanding (Devlin et al., 2019; Radford et al., 2018; Yang et al., 2019; Liu et al., 2019). There has been a growing interest in ∗ The first two authors contributed equally. Our human annotated summaries for faithfulness and factuality will be released at https://github.com/google-researchdatasets/xsum hallucination annotations. 1 understanding how maximum likelihood training and approximate beam-search decoding in these models lead to less human-like text in open-ended text generation such as language modeling and story generation (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In this paper we investigate how these models are prone to generate hallucinated text in conditional text generation, specifically, extreme abstractive document summarization (Narayan et al., 2018a). Document summarization — the task of producing a shorter version of a document while preserving its information content (Mani, 2001; Nenkova and McKeown, 2011) — requires models to generate text that is not only human-like but also faithful and/or factual given the document. The example in Figure 1 illustrates that the faithfulness and factuality are yet to be conquered by conditional text gener"
2020.acl-main.173,P19-1363,0,0.0636683,"m the test set to facilitate our study. Using the full test set was unfeasible given its size and the cost of human judgments. We have trained annotators (fluent in English) specifically for our assessment. Our annotators went through two pilot studies to have a better understanding of intrinsic and extrinsic hallucinations, and factuality of summaries. Documents used in the pilot studies were not used in the final annotation. We also report on ROUGE (Lin and Hovy, 2003) scores, BERTScore (Zhang et al., 2020) and semantic inference metric such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019; Falke et al., 2019; Kryscinski et al., 2019b) and question answering (Arumae and Liu, 2019; Wang et al., 2020). 5.1 Automatic Evaluation of Summaries ROUGE (Lin and Hovy, 2003) provides a means to quickly assess a model’s ability to generate summaries closer to human authored summaries. We report on ROUGE-1 and ROUGE-2 for informativeness and ROUGE-L, for fluency. Like ROUGE, BERTScore (Zhang et al., 2020) computes a similarity score for each token in the candidate summary with each token in the reference summary. However, instead of exact matches, it computes token similarity using contextu"
2020.acl-main.173,N18-1101,0,0.0295043,"D. For factuality, the differences between P T G EN, TC ONV S2S, and T RAN S2S were insignificant. 5.4 Automatic Measures for Hallucinations Summaries are a proxy for their source documents under the assumption that they highlight the most important content. With this assumption, we further studied the extent to which the hallucinated content can be measured by semantic inference related measures, such as textual entailment and question answering. Textual Entailment. We trained an entailment classifier by finetuning a BERT-Large pretrained model (Devlin et al., 2019) on the Multi-NLI dataset (Williams et al., 2018). We calculated the entailment probability score between the document and its abstractive summaries. Note that this entailment classifier is not optimal for the BBC article-summary pairs; the Multi-NLI dataset contains sentence-sentence pairs. Ideally a summary should entail the document or perhaps be neutral to the document, but never contradict the document. As can be seen in Table 3, the B ERT S2S abstracts showed the least number of 5 See Appendix for full results. Models P T G EN TC ONV S2S T RAN S2S B ERT S2S G OLD Textual Entailment entail. neut. cont. 38.4 34.4 27.2 29.6 37.4 33.0 34.6"
2020.acl-main.173,2020.acl-main.450,0,0.364319,"n judgments. We have trained annotators (fluent in English) specifically for our assessment. Our annotators went through two pilot studies to have a better understanding of intrinsic and extrinsic hallucinations, and factuality of summaries. Documents used in the pilot studies were not used in the final annotation. We also report on ROUGE (Lin and Hovy, 2003) scores, BERTScore (Zhang et al., 2020) and semantic inference metric such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019; Falke et al., 2019; Kryscinski et al., 2019b) and question answering (Arumae and Liu, 2019; Wang et al., 2020). 5.1 Automatic Evaluation of Summaries ROUGE (Lin and Hovy, 2003) provides a means to quickly assess a model’s ability to generate summaries closer to human authored summaries. We report on ROUGE-1 and ROUGE-2 for informativeness and ROUGE-L, for fluency. Like ROUGE, BERTScore (Zhang et al., 2020) computes a similarity score for each token in the candidate summary with each token in the reference summary. However, instead of exact matches, it computes token similarity using contextual embeddings. Results are presented in Table 1. For both cases, the pretrained encoder-decoder architecture B E"
2020.acl-main.173,N03-1020,0,\N,Missing
2020.acl-main.173,D17-1239,0,\N,Missing
2020.acl-main.173,D18-2012,0,\N,Missing
2020.acl-main.173,D18-1443,0,\N,Missing
2020.acl-main.173,Q19-1026,0,\N,Missing
2020.acl-main.173,N19-1264,0,\N,Missing
2020.acl-main.173,P19-1330,1,\N,Missing
2020.acl-main.173,P19-1620,0,\N,Missing
2020.acl-main.173,P19-1213,0,\N,Missing
2020.acl-main.173,N19-1423,0,\N,Missing
2020.acl-main.173,P19-1285,0,\N,Missing
2020.acl-main.577,N19-1078,0,0.067223,"Missing"
2020.acl-main.577,C18-1139,0,0.128229,"Missing"
2020.acl-main.577,W07-1009,0,0.0601862,"and its siblings provided better language models that turned again into higher scores for NER. Lample et al. (2016) cast NER as transitionbased dependency parsing using a Stack-LSTM. They compare with a LSTM-CRF model which turns out to be a very strong baseline. Their transition-based system uses two transitions (shift and reduce) to mark the named entities and handles flat NER while our system has been designed to handle both nested and flat entities. Nested Named Entity Recognition. Early work on nested NER, motivated particularly by the GENIA corpus, includes (Shen et al., 2003; Beatrice Alex and Grover, 2007; Finkel and Manning, 2009). Finkel and Manning (2009) also proposed a constituency parsing-based approach. In the last years, we saw an increasing number of neural models targeting nested NER as well. Ju et al. (2018) suggested a LSTM-CRF model to predict nested named entities. Their algorithm iteratively continues until no further entities are predicted. Lin et al. (2019) tackle the problem in two steps: they first detect the entity head, and then they infer the entity boundaries as well as the category of the named entity. Strakov´a et al. (2019) tag the nested named entity by a sequence-to"
2020.acl-main.577,N18-1131,0,0.264782,"h turns out to be a very strong baseline. Their transition-based system uses two transitions (shift and reduce) to mark the named entities and handles flat NER while our system has been designed to handle both nested and flat entities. Nested Named Entity Recognition. Early work on nested NER, motivated particularly by the GENIA corpus, includes (Shen et al., 2003; Beatrice Alex and Grover, 2007; Finkel and Manning, 2009). Finkel and Manning (2009) also proposed a constituency parsing-based approach. In the last years, we saw an increasing number of neural models targeting nested NER as well. Ju et al. (2018) suggested a LSTM-CRF model to predict nested named entities. Their algorithm iteratively continues until no further entities are predicted. Lin et al. (2019) tackle the problem in two steps: they first detect the entity head, and then they infer the entity boundaries as well as the category of the named entity. Strakov´a et al. (2019) tag the nested named entity by a sequence-to-sequence model exploring combinations of context-based embeddings such as ELMo, BERT, and Flair. Zheng et al. (2019) use a boundary aware network to solve the nested NER. Similar to our work, Sohrab and Miwa (2018) Me"
2020.acl-main.577,P19-1066,0,0.0290515,"l exploring combinations of context-based embeddings such as ELMo, BERT, and Flair. Zheng et al. (2019) use a boundary aware network to solve the nested NER. Similar to our work, Sohrab and Miwa (2018) Methods Our model is inspired by the dependency parsing model of Dozat and Manning (2017). We use both word embeddings and character embeddings as input, and feed the output into a BiLSTM and finally to a biaffine classifier. Figure 1 shows an overview of the architecture. To encode words, we use both BERTLarge and fastText embeddings (Bojanowski et al., 2016). For BERT we follow the recipe of (Kantor and Globerson, 2019) to obtain the context dependent embeddings for a target token with 64 surrounding tokens each side. For the character-based word embeddings, we use a CNN to encode the characters of the tokens. The concatenation of the word and character-based word embeddings is feed into a BiLSTM to obtain the word representations (x). After obtaining the word representations from the BiLSTM, we apply two separate FFNNs to create different representations (hs /he ) for the start/end of the spans. Using different representations for the start/end of the spans allow the system to learn to identify the start/en"
2020.acl-main.577,N18-1079,0,0.112789,"Missing"
2020.acl-main.577,D18-1217,0,0.0320911,"SoTA. We provide the code as open source1 . 2 Related Work Flat Named Entity Recognition. The majority of flat NER models are based on a sequence labelling approach. Collobert et al. (2011) introduced a neural NER model that uses CNNs to encode tokens combined with a CRF layer for the classification. Many other neural systems followed this approach but used instead LSTMs to encode the input and a CRF for the prediction (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). These latter models were later extended to use contextdependent embeddings such as ELMo (Peters et al., 2018). Clark et al. (2018) quite successfully used cross-view training (CVT) paired with multi-task learning. This method yields impressive gains for 1 The code is available at https://github.com/ juntaoy/biaffine-ner 6470 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6470–6476 c July 5 - 10, 2020. 2020 Association for Computational Linguistics enumerate exhaustively all possible spans up to a defined length by concatenating the LSTMs outputs for the start and end position and then using this to calculate a score for each span. Apart from the different network and word e"
2020.acl-main.577,P19-1511,0,0.382875,"ain difference between their model and ours is there for the use of biaffine model. Due to the biaffine model, we get a global view of the sentence while Sohrab and Miwa (2018) concatenates the output of the LSTMs of possible start and end positions up to a distinct length. Dozat and Manning (2017) demonstrated that the biaffine mapping performs significantly better than just the concatenation of pairs of LSTM outputs. Biaffine Classifier FFNN_Start FFNN_End BiLSTM BERT, fastText & Char Embeddings 3 Figure 1: The network architectures of our system. a number of NLP applications including NER. Devlin et al. (2019) invented BERT, a bidirectional transformer architecture for the training of language models. BERT and its siblings provided better language models that turned again into higher scores for NER. Lample et al. (2016) cast NER as transitionbased dependency parsing using a Stack-LSTM. They compare with a LSTM-CRF model which turns out to be a very strong baseline. Their transition-based system uses two transitions (shift and reduce) to mark the named entities and handles flat NER while our system has been designed to handle both nested and flat entities. Nested Named Entity Recognition. Early work"
2020.acl-main.577,D18-1124,0,0.0488677,"Missing"
2020.acl-main.577,N18-1202,0,0.180717,"Missing"
2020.acl-main.577,W03-1307,0,0.125749,"ing of language models. BERT and its siblings provided better language models that turned again into higher scores for NER. Lample et al. (2016) cast NER as transitionbased dependency parsing using a Stack-LSTM. They compare with a LSTM-CRF model which turns out to be a very strong baseline. Their transition-based system uses two transitions (shift and reduce) to mark the named entities and handles flat NER while our system has been designed to handle both nested and flat entities. Nested Named Entity Recognition. Early work on nested NER, motivated particularly by the GENIA corpus, includes (Shen et al., 2003; Beatrice Alex and Grover, 2007; Finkel and Manning, 2009). Finkel and Manning (2009) also proposed a constituency parsing-based approach. In the last years, we saw an increasing number of neural models targeting nested NER as well. Ju et al. (2018) suggested a LSTM-CRF model to predict nested named entities. Their algorithm iteratively continues until no further entities are predicted. Lin et al. (2019) tackle the problem in two steps: they first detect the entity head, and then they infer the entity boundaries as well as the category of the named entity. Strakov´a et al. (2019) tag the nest"
2020.acl-main.577,D18-1309,0,0.192973,"Missing"
2020.acl-main.577,P19-1527,0,0.235149,"Missing"
2020.acl-main.577,D17-1283,0,0.0938625,"Missing"
2020.acl-main.577,W02-2024,0,0.690669,"Missing"
2020.acl-main.577,D09-1015,0,\N,Missing
2020.acl-main.577,W03-0419,0,\N,Missing
2020.acl-main.577,W12-4501,0,\N,Missing
2020.acl-main.577,D15-1102,0,\N,Missing
2020.acl-main.577,N16-1030,0,\N,Missing
2020.acl-main.577,D17-1276,0,\N,Missing
2020.acl-main.577,N19-1308,0,\N,Missing
2020.acl-main.577,N19-1423,0,\N,Missing
2020.acl-main.577,D18-1019,0,\N,Missing
2020.acl-main.577,Q16-1026,0,\N,Missing
2020.acl-main.577,D19-1034,0,\N,Missing
2020.lrec-1.1,W07-1009,0,0.0671979,"al., 2016); thus, the quality of mention detection affects both the performance of models for such applications and the quality of annotated data used to train them (Chamberlain et al., 2016; Poesio et al., 2019). Much mention detection research for NER has concentrated on a simplified version of MD that focuses on proper names only (i.e., it doesn’t consider as mentions nominals such as the protein or pronouns such as it), and ignores the fact that mentions may nest (e.g., noun phrases such as [[CCITA] mRNA] in the GENIA corpus are mentions of two separate entities, CCITA and CCITA and mRNA (Alex et al., 2007)). However such simplified view of mentions is not sufficient for NER in domains such as biomedical, or for coreference, that requires full mention detection. Another limitation of typical mention detection systems is that they only predict mentions in a HIGH F 1 fashion, whereas in coreference, for instance, mentions are usually predicted in a HIGH RE CALL setting, since further pruning will be carried out at the coreference system (Clark and Manning, 2016b; Clark and Manning, 2016a; Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019). There are only very few recent studies that a"
2020.lrec-1.1,P14-1005,0,0.171029,"Missing"
2020.lrec-1.1,L16-1323,1,0.891442,"Missing"
2020.lrec-1.1,Q16-1026,0,0.073381,"Missing"
2020.lrec-1.1,P15-1136,0,0.351811,"ition, Deep Neural Networks 1. Introduction tract mentions for an annotation project, or by other coreference systems. Thus the only standalone mention detectors that can be used as preprocessing for a coreference annotation are ones that do not take advantage of these advances and still heavily rely on parsing to identify all NPs as candidate mentions (Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016) or ones that use the rulebased mention detector from the Stanford deterministic system (Lee et al., 2013) to extract mentions from NPs, named entity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version of the mention detection part of the Lee et al. (2018) system. The second system employs"
2020.lrec-1.1,D16-1245,0,0.366373,"ks 1. Introduction tract mentions for an annotation project, or by other coreference systems. Thus the only standalone mention detectors that can be used as preprocessing for a coreference annotation are ones that do not take advantage of these advances and still heavily rely on parsing to identify all NPs as candidate mentions (Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016) or ones that use the rulebased mention detector from the Stanford deterministic system (Lee et al., 2013) to extract mentions from NPs, named entity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version of the mention detection part of the Lee et al. (2018) system. The second system employs a bi-directional LSTM on"
2020.lrec-1.1,P16-1061,0,0.276917,"ks 1. Introduction tract mentions for an annotation project, or by other coreference systems. Thus the only standalone mention detectors that can be used as preprocessing for a coreference annotation are ones that do not take advantage of these advances and still heavily rely on parsing to identify all NPs as candidate mentions (Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016) or ones that use the rulebased mention detector from the Stanford deterministic system (Lee et al., 2013) to extract mentions from NPs, named entity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version of the mention detection part of the Lee et al. (2018) system. The second system employs a bi-directional LSTM on"
2020.lrec-1.1,N19-1423,0,0.491022,"et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version of the mention detection part of the Lee et al. (2018) system. The second system employs a bi-directional LSTM on the sentence level and uses biaffine attention (Dozat and Manning, 2017) over the LSTM outputs to predict the mentions. The third system takes the outputs from BERT (Devlin et al., 2019) and feeds them into a feed-forward neural network to classify candidates into mentions and non mentions. All three systems have the options to output mentions in HIGH RECALL or HIGH F 1 settings; the former is well suited for the coreference task, whereas the latter can be used as a standard mention detector for tasks like nested named entity recognition. We evaluate our models on the CONLL and the CRAC data sets for coreference mention detection, and on GENIA corpora for nested NER. The contributions of this paper are therefore as follows. First, we show that mention detection performance im"
2020.lrec-1.1,D09-1015,0,0.0189104,"ion (the PEAR stories). This corpus is more appropriate for studying mention detection as all mentions are annotated. As done in the CRAC shared task, we used the RST portion of the corpora, consisting of news texts (1/3 of the PENN Treebank). Since none of the state-of-the-art coreference systems predict singleton mentions, a version of the CRAC dataset with singleton mentions excluded was created for the coreference task evaluation. The GENIA corpora is one of the main resources for studying nested NER. We use the GENIA v3.0.2 corpus and preprocess the dataset following the same settings of Finkel and Manning (2009) and Lu and Roth (2015). Historically, the dataset has been split into two different ways: the first approach splits the data into two sets (train and test) by 90:10 (GENIA 90), whereas the second approach further creates a development set by splitting the data into 81:9:10 (GENIA 81). We evaluate our model on both approaches to make the fair comparisons with previous work. For evaluation on GENIA 90, since we do not have a development set, we train our model for 40K steps (20 epochs) and take evaluate on the final model. 4.2. Value L EE, B IA L EE, B IA L EE, B IA B ER B ER B ER L EE, B IA, B"
2020.lrec-1.1,N18-1131,0,0.332814,"not be directly applied in tasks that require nested mentions, such as NER in the biomedical domain or coreference. The first neural network based NER model was introduced by Collobert et al. (2011), who used a CNN to encode the tokens and applied a CRF layer on top. After that, many other network architectures for NER MD have also been proposed, such as LSTM - CRF (Lample et al., 2016; Chiu and Nichols, 2016), LSTM-CRF + ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019). Recently, a number of NER systems based on neural network architectures have been introduced to solve nested NER . Ju et al. (2018) introduce a stacked LSTM - CRF approach to solve nested NER in multi-steps. Sohrab and Miwa (2018) use an exhaustive region classification model. Lin et al. (2019) solve the problem in two steps: they first detect the entity head, and then infer the entity boundaries and classes in the second step. Strakov´a et al. (2019) infer the nested NER by a sequence-to-sequence model. Zheng et al. (2019) introduce a boundary aware network to train the boundary detection and the entity classification models in a multi-task learning setting. However, none of those systems can be directly used for corefer"
2020.lrec-1.1,P19-1066,0,0.171584,"ons of two separate entities, CCITA and CCITA and mRNA (Alex et al., 2007)). However such simplified view of mentions is not sufficient for NER in domains such as biomedical, or for coreference, that requires full mention detection. Another limitation of typical mention detection systems is that they only predict mentions in a HIGH F 1 fashion, whereas in coreference, for instance, mentions are usually predicted in a HIGH RE CALL setting, since further pruning will be carried out at the coreference system (Clark and Manning, 2016b; Clark and Manning, 2016a; Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019). There are only very few recent studies that attempt to apply neural network approaches to develop a standalone mention detector. Neural network approaches using contextsensitive embeddings such as ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) have resulted in substantial improvements for mention detectors in the NER benchmark CONLL 2003 data set. However, most coreference systems that appeared after Lee et al., (2017; 2018) carry out mention detection as a part of their end-to-end coreference system. Such systems do not output intermediate mentions, hence the mention detector can"
2020.lrec-1.1,N16-1030,0,0.220661,"rence annotation are ones that do not take advantage of these advances and still heavily rely on parsing to identify all NPs as candidate mentions (Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016) or ones that use the rulebased mention detector from the Stanford deterministic system (Lee et al., 2013) to extract mentions from NPs, named entity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version of the mention detection part of the Lee et al. (2018) system. The second system employs a bi-directional LSTM on the sentence level and uses biaffine attention (Dozat and Manning, 2017) over the LSTM outputs to predict the mentions. The third system takes the outputs from BERT (Devlin et al.,"
2020.lrec-1.1,J13-4004,0,0.272961,"ask. Keywords: Mention Detection, Coreference Resolution, Nested Named Entity Recognition, Deep Neural Networks 1. Introduction tract mentions for an annotation project, or by other coreference systems. Thus the only standalone mention detectors that can be used as preprocessing for a coreference annotation are ones that do not take advantage of these advances and still heavily rely on parsing to identify all NPs as candidate mentions (Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016) or ones that use the rulebased mention detector from the Stanford deterministic system (Lee et al., 2013) to extract mentions from NPs, named entity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version"
2020.lrec-1.1,D17-1018,0,0.0677949,"mRNA] in the GENIA corpus are mentions of two separate entities, CCITA and CCITA and mRNA (Alex et al., 2007)). However such simplified view of mentions is not sufficient for NER in domains such as biomedical, or for coreference, that requires full mention detection. Another limitation of typical mention detection systems is that they only predict mentions in a HIGH F 1 fashion, whereas in coreference, for instance, mentions are usually predicted in a HIGH RE CALL setting, since further pruning will be carried out at the coreference system (Clark and Manning, 2016b; Clark and Manning, 2016a; Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019). There are only very few recent studies that attempt to apply neural network approaches to develop a standalone mention detector. Neural network approaches using contextsensitive embeddings such as ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) have resulted in substantial improvements for mention detectors in the NER benchmark CONLL 2003 data set. However, most coreference systems that appeared after Lee et al., (2017; 2018) carry out mention detection as a part of their end-to-end coreference system. Such systems do not output interm"
2020.lrec-1.1,N18-2108,0,0.126431,"ntity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version of the mention detection part of the Lee et al. (2018) system. The second system employs a bi-directional LSTM on the sentence level and uses biaffine attention (Dozat and Manning, 2017) over the LSTM outputs to predict the mentions. The third system takes the outputs from BERT (Devlin et al., 2019) and feeds them into a feed-forward neural network to classify candidates into mentions and non mentions. All three systems have the options to output mentions in HIGH RECALL or HIGH F 1 settings; the former is well suited for the coreference task, whereas the latter can be used as a standard mention detector for tasks like nested named entity recognit"
2020.lrec-1.1,P19-1511,0,0.125014,"troduced by Collobert et al. (2011), who used a CNN to encode the tokens and applied a CRF layer on top. After that, many other network architectures for NER MD have also been proposed, such as LSTM - CRF (Lample et al., 2016; Chiu and Nichols, 2016), LSTM-CRF + ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019). Recently, a number of NER systems based on neural network architectures have been introduced to solve nested NER . Ju et al. (2018) introduce a stacked LSTM - CRF approach to solve nested NER in multi-steps. Sohrab and Miwa (2018) use an exhaustive region classification model. Lin et al. (2019) solve the problem in two steps: they first detect the entity head, and then infer the entity boundaries and classes in the second step. Strakov´a et al. (2019) infer the nested NER by a sequence-to-sequence model. Zheng et al. (2019) introduce a boundary aware network to train the boundary detection and the entity classification models in a multi-task learning setting. However, none of those systems can be directly used for coreference, due to the large difference between the settings used in NER and in coreference (e.g. for coreference the mention need to be predicted in a HIGH RECALL fashio"
2020.lrec-1.1,D15-1102,0,0.0248292,"orpus is more appropriate for studying mention detection as all mentions are annotated. As done in the CRAC shared task, we used the RST portion of the corpora, consisting of news texts (1/3 of the PENN Treebank). Since none of the state-of-the-art coreference systems predict singleton mentions, a version of the CRAC dataset with singleton mentions excluded was created for the coreference task evaluation. The GENIA corpora is one of the main resources for studying nested NER. We use the GENIA v3.0.2 corpus and preprocess the dataset following the same settings of Finkel and Manning (2009) and Lu and Roth (2015). Historically, the dataset has been split into two different ways: the first approach splits the data into two sets (train and test) by 90:10 (GENIA 90), whereas the second approach further creates a development set by splitting the data into 81:9:10 (GENIA 81). We evaluate our model on both approaches to make the fair comparisons with previous work. For evaluation on GENIA 90, since we do not have a development set, we train our model for 40K steps (20 epochs) and take evaluate on the final model. 4.2. Value L EE, B IA L EE, B IA L EE, B IA B ER B ER B ER L EE, B IA, B ER L EE, B IA, B ER L"
2020.lrec-1.1,D14-1162,0,0.0829822,"raw candidate scores (rm ). The raw scores are then used to create the probabilities (pm ) by applying a sigmoid function to the rm : L EE MD Our first system is based on the mention detection part of the Lee et al. (2018) system. The system represents a candidate span with the outputs of a bi-directional LSTM. The sentences of a document are encoded bidirectional via the LSTM s to obtain forward/backward representations for each token in the sentence. The bi-directional LSTM takes as input the concatenated embeddings ((xt )Tt=1 ) of both word and character levels. For word embeddings, GloVe (Pennington et al., 2014) and ELMO (Peters et al., 2018) embeddings are used. Character embeddings are learned from convolution neural networks (CNN) during training. The tokens are represented by concatenated outputs from the forward and the backward LSTMs. The token representations (x∗t )Tt=1 are used together with head representations (h∗i ) to represent candidate spans (Ni∗ ). The h∗i of a span is obtained by applying an attention over its token representations ({x∗si , ..., x∗ei }), where si and ei are the indices of the start and the end of the span respectively. Formally, we compute h∗i , Ni∗ as follows: rm (i)"
2020.lrec-1.1,N18-1202,0,0.402252,"Another limitation of typical mention detection systems is that they only predict mentions in a HIGH F 1 fashion, whereas in coreference, for instance, mentions are usually predicted in a HIGH RE CALL setting, since further pruning will be carried out at the coreference system (Clark and Manning, 2016b; Clark and Manning, 2016a; Lee et al., 2017; Lee et al., 2018; Kantor and Globerson, 2019). There are only very few recent studies that attempt to apply neural network approaches to develop a standalone mention detector. Neural network approaches using contextsensitive embeddings such as ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) have resulted in substantial improvements for mention detectors in the NER benchmark CONLL 2003 data set. However, most coreference systems that appeared after Lee et al., (2017; 2018) carry out mention detection as a part of their end-to-end coreference system. Such systems do not output intermediate mentions, hence the mention detector cannot be directly used to ex1 1 This performance difference is measured on mention recall, by training the mention detector alone. Second, our best system achieves improvements of 5.3 and 6.2 percentage points when compared wit"
2020.lrec-1.1,W18-0702,1,0.93671,"ct, or by other coreference systems. Thus the only standalone mention detectors that can be used as preprocessing for a coreference annotation are ones that do not take advantage of these advances and still heavily rely on parsing to identify all NPs as candidate mentions (Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016) or ones that use the rulebased mention detector from the Stanford deterministic system (Lee et al., 2013) to extract mentions from NPs, named entity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper, we compare three neural architectures for standalone MD. The first system is a slightly modified version of the mention detection part of the Lee et al. (2018) system. The second system employs a bi-directional LSTM on the sentence level and uses biaffine attention (Dozat"
2020.lrec-1.1,N19-1176,1,0.858045,"he contributions of this paper are therefore as follows. First, we show that mention detection performance improved by up to 1.5 percentage points1 can be achieved Mention detection (MD) is the task of identifying mentions of entities in text. It is an important preprocessing step for downstream applications such as nested named entity recognition (Zheng et al., 2019) or coreference resolution (Poesio et al., 2016); thus, the quality of mention detection affects both the performance of models for such applications and the quality of annotated data used to train them (Chamberlain et al., 2016; Poesio et al., 2019). Much mention detection research for NER has concentrated on a simplified version of MD that focuses on proper names only (i.e., it doesn’t consider as mentions nominals such as the protein or pronouns such as it), and ignores the fact that mentions may nest (e.g., noun phrases such as [[CCITA] mRNA] in the GENIA corpus are mentions of two separate entities, CCITA and CCITA and mRNA (Alex et al., 2007)). However such simplified view of mentions is not sufficient for NER in domains such as biomedical, or for coreference, that requires full mention detection. Another limitation of typical menti"
2020.lrec-1.1,W12-4501,0,0.135911,"effects of our model on coreference: i.e., we integrate the mentions extracted from our best system into state-of-the-art coreference systems (both end-to-end and the pipeline system). The third series of experiments focuses on the nested NER task. We evaluate our systems both on boundary detection and on the full NER tasks. The rest of this section introduces our experimental settings in detail. 4.1. Data Set We evaluate our models on two different corpora for both the mention detection and the coreference tasks and one additional corpora for nested NER task, the CONLL 2012 English corpora (Pradhan et al., 2012), the CRAC 2018 corpora (Poesio et al., 2018) and the GENIA (Kim et al., 2003) corpora. The CONLL data set is the standard reference corpora for coreference resolution. The English subset consists of 2802, 342, and 348 documents for the train, development and test sets respectively. The CONLL data set is not however ideal for mention detection, since not all mentions are annotated, but only mentions involved in coreference Ni∗ = [x∗si , x∗ei ] rm (i) = FFNNm (Ni∗ ) 1 pm (i) = 1 + e−rm (i) 4 chains of length &gt; 1. This has a negative impact on learning since singleton mentions will always receiv"
2020.lrec-1.1,D18-1309,0,0.227154,"domain or coreference. The first neural network based NER model was introduced by Collobert et al. (2011), who used a CNN to encode the tokens and applied a CRF layer on top. After that, many other network architectures for NER MD have also been proposed, such as LSTM - CRF (Lample et al., 2016; Chiu and Nichols, 2016), LSTM-CRF + ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019). Recently, a number of NER systems based on neural network architectures have been introduced to solve nested NER . Ju et al. (2018) introduce a stacked LSTM - CRF approach to solve nested NER in multi-steps. Sohrab and Miwa (2018) use an exhaustive region classification model. Lin et al. (2019) solve the problem in two steps: they first detect the entity head, and then infer the entity boundaries and classes in the second step. Strakov´a et al. (2019) infer the nested NER by a sequence-to-sequence model. Zheng et al. (2019) introduce a boundary aware network to train the boundary detection and the entity classification models in a multi-task learning setting. However, none of those systems can be directly used for coreference, due to the large difference between the settings used in NER and in coreference (e.g. for cor"
2020.lrec-1.1,P19-1527,0,0.0322637,"Missing"
2020.lrec-1.1,I13-1012,0,0.0613488,"Missing"
2020.lrec-1.1,P15-1137,0,0.427361,"Missing"
2020.lrec-1.1,N16-1114,0,0.262682,"at our model matches or outperforms state-of-the-art models despite not being specifically designed for this task. Keywords: Mention Detection, Coreference Resolution, Nested Named Entity Recognition, Deep Neural Networks 1. Introduction tract mentions for an annotation project, or by other coreference systems. Thus the only standalone mention detectors that can be used as preprocessing for a coreference annotation are ones that do not take advantage of these advances and still heavily rely on parsing to identify all NPs as candidate mentions (Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Wiseman et al., 2016) or ones that use the rulebased mention detector from the Stanford deterministic system (Lee et al., 2013) to extract mentions from NPs, named entity mentions and pronouns (Clark and Manning, 2015; Clark and Manning, 2016b). To the best of our knowledge, Poesio et al. (2018) introduced the only standalone neural mention detector. By using a modified version of the NER system of Lample et al. (2016), they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system. In this paper"
2020.lrec-1.1,P18-2017,0,0.0380079,"Missing"
2020.lrec-1.1,D19-1034,0,0.0334868,"Missing"
2020.lrec-1.634,P18-1246,1,0.880272,"Missing"
2020.lrec-1.634,D17-1304,0,0.0208857,"of the annotated word count. It is also the first large Turkish dependency treebank that has a dedicated Wikipedia section. We present the tagsets and the methodology that are used in annotating the treebank and also the results of the baseline experiments on Turkish dependency parsing with this treebank. Keywords: treebank, dependency parsing, Turkish 1. Introduction Dependency parsing is an important building block in improving the performance of downstream NLP tasks such as semantic role labeling (Marcheggiani et al., 2017), relation extraction (Zhang et al., 2018) or machine translation (Chen et al., 2017). Treebanks are invaluable resources for developing and training accurate dependency parsers in supervised settings and more and more research has been invested in developing high quality treebanks for various languages over the years. Compared to other well studied languages in NLP, the publicly available treebanks for Turkish has remained meagre in size and domain variation until very recently. The METU Sabancı Treebank (MST) (Atalay et al., 2003; Oflazer et al., 2003) had been the only treebank for Turkish for over a decade. Even though the cumulative size of morphosyntactically annotated d"
2020.lrec-1.634,K17-3002,0,0.0599702,"Missing"
2020.lrec-1.634,E06-1012,0,0.135793,"Missing"
2020.lrec-1.634,K17-1041,0,0.0181163,"is the largest publicly available human-annotated morpho-syntactic Turkish treebank in terms of the annotated word count. It is also the first large Turkish dependency treebank that has a dedicated Wikipedia section. We present the tagsets and the methodology that are used in annotating the treebank and also the results of the baseline experiments on Turkish dependency parsing with this treebank. Keywords: treebank, dependency parsing, Turkish 1. Introduction Dependency parsing is an important building block in improving the performance of downstream NLP tasks such as semantic role labeling (Marcheggiani et al., 2017), relation extraction (Zhang et al., 2018) or machine translation (Chen et al., 2017). Treebanks are invaluable resources for developing and training accurate dependency parsers in supervised settings and more and more research has been invested in developing high quality treebanks for various languages over the years. Compared to other well studied languages in NLP, the publicly available treebanks for Turkish has remained meagre in size and domain variation until very recently. The METU Sabancı Treebank (MST) (Atalay et al., 2003; Oflazer et al., 2003) had been the only treebank for Turkish"
2020.lrec-1.634,W19-3110,1,0.88054,"Missing"
2020.lrec-1.634,W15-1610,0,0.0330599,"Missing"
2020.lrec-1.634,K18-2016,0,0.025019,"Missing"
2020.lrec-1.634,C16-1325,0,0.0471164,"Missing"
2020.lrec-1.634,K17-3001,1,0.855588,"Missing"
2020.lrec-1.634,K18-2001,0,0.0305622,"Missing"
2020.lrec-1.634,D18-1244,0,0.0148065,"d morpho-syntactic Turkish treebank in terms of the annotated word count. It is also the first large Turkish dependency treebank that has a dedicated Wikipedia section. We present the tagsets and the methodology that are used in annotating the treebank and also the results of the baseline experiments on Turkish dependency parsing with this treebank. Keywords: treebank, dependency parsing, Turkish 1. Introduction Dependency parsing is an important building block in improving the performance of downstream NLP tasks such as semantic role labeling (Marcheggiani et al., 2017), relation extraction (Zhang et al., 2018) or machine translation (Chen et al., 2017). Treebanks are invaluable resources for developing and training accurate dependency parsers in supervised settings and more and more research has been invested in developing high quality treebanks for various languages over the years. Compared to other well studied languages in NLP, the publicly available treebanks for Turkish has remained meagre in size and domain variation until very recently. The METU Sabancı Treebank (MST) (Atalay et al., 2003; Oflazer et al., 2003) had been the only treebank for Turkish for over a decade. Even though the cumulat"
2020.msr-1.1,W13-3520,0,0.0265768,". Restricted-resources subtrack (same as SR’19 Track 1): Teams built models trained on the provided T1 dataset(s), but use of external task-specific data was not permitted. However, teams were allowed to use external generic resources. For example, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material. Also permitted was the use of generic publicly available off-the-shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013). Alternatively, BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). b. Open subtrack: In this track, teams built models trained on the provided T1 dataset(s), also using any additional resources, without restrictions. Teams could even use the SR conversion tool to produce data with the exact same specifications as the data provided in the track, by applying the converter to a parsed output (see Section 4.2). T2 Deep Track: Inputs in this track are UD structures as in T1 from"
2020.msr-1.1,P11-2040,0,0.0129219,"R’18 (Mille et al., 2018) and SR’19 (Mille et al., 2019). The evaluation method is Direct Assessment (DA) (Graham et al., 2016), as used by the WMT competitions to produce the official ranking of machine translation systems (Barrault et al., 2020) and video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019). We ran the evaluation on Mechanical Turk,6 assessing two quality criteria, in separate evaluation experiments, but using the same method: Readability and Meaning Similarity. We used continuous sliders as rating tools, the evidence being that raters tend to prefer them (Belz and Kow, 2011). Slider positions were mapped to values from 0 to 100 (best). Raters were given brief instructions, including the direction to ignore formatting errors, superfluous whitespace, capitalisation issues, and poor hyphenation. The statement to be assessed in the Readability evaluation was: The text reads well and is free from grammatical errors and awkward constructions. The corresponding statement in the Meaning Similarity evaluation, in which system outputs (‘the black text’) were compared to reference sentences (‘the gray text’), was:7,8 The meaning of the gray text is adequately expressed by t"
2020.msr-1.1,W11-2832,0,0.0178935,"of the SR’20 tracks, data and evaluation methods, as well as brief summaries of the participating systems. Full descriptions of the participating systems can be found in separate system reports elsewhere in this volume. 1 Introduction SR’20 is the fourth in a line of shared tasks focused on surface realisation, the name originally given to the last stage in the first-generation (pre-statistical and pre-neural) Natural Language Generation (NLG) pipeline, mapping from semantic representations to fully realised surface word strings. When we ran the first Surface Realisation Shared Task in 2011 (Belz et al., 2011), it was to address a situation where there were many different approaches to SR but none of them were comparable. We developed a commonground input representation that different approaches could map their normal inputs to, making results directly comparable for the first time. Most SR’11 systems (and all top performing ones) were statistical dependency realisers that did not make use of an explicit, pre-existing grammar. However, the question of how inputs to the realisers were going to be provided in an embedding system was left open. By the time we proposed the second SR Task (Mille et al.,"
2020.msr-1.1,K17-3005,0,0.0265528,"tense, verbal finiteness, etc.). The test data sets can be grouped into three types: (i) in-domain test data, in the same domains as the training and development data; (ii) Out-of-domain, which are test sets of parallel sentences in different languages in domains not covered by the training and development data; and (iii) silver standard data, which consists of automatically parsed sentences. The in-domain and out-of-domain data is provided in the UD release V2.3.1 The silver standard data was processed using the best CoNLL’18 parsers for the chosen datasets: the Harbin HIT-SCIR (HIT) parser (Che et al., 2017) for English_ewt, Hindi_hdtb, Korean_kaist and Spanish_ancora; the LATTICE (LAT) parser (Lim et al., 2018) for English_pud and the Stanford (STF) parser (Qi et al., 2019) for Portuguese_bosque.2 A detailed description of all SR’19 datasets and how they were processed can be found in the SR’19 report paper (Mille et al., 2019). 4.2 SR’20 new test sets To obtain new test sets,3 we selected sentences from Wikipedia in six out of the eleven SR’19 languages for which it was possible to get a good quantity of clean texts on the same topics. The used articles contain mostly landmarks and some histori"
2020.msr-1.1,W19-8652,0,0.0338279,"map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisation stage. Nevertheless, the community enthusiastically participated in SR’18 and SR’19 (Mille et al., 2018; Mille et al., 2019) as we expanded tracks to 11 languages. This year, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are increasingly proposed to address such issues (Hua and Wang, 2019; Zhai et al., 2019; Zhao et al., 2020), and are beginning to look somewhat like the old NLG pipeline. In this context, surface realisation is very much back o"
2020.msr-1.1,W18-3604,1,0.862544,"redicts a sequence of edit operations to convert lemmas to word forms character by character; the contraction model predicts BIO tags to group words to be contracted, and then generates the contracted word form of each group with a seq2seq model. The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises texts by first preprocessing the dependency tree into a preordered linearized form, which is then converted into its textual counterpart using a rule-based approach together with a statistical machine translation (SMT) model. A singular version of the model was trained for each language considered in the experiment. 4 4.1 Data Sets T1 and T2 training and test sets (same as in SR’19) There are 42 datasets in 11 languages, 29 datasets for T1, and 13 for T2 (for a summary overview, see Table 2, top 3 sections of the table). The datasets were selected from the available collection of 4"
2020.msr-1.1,D19-1055,0,0.0142412,"ear, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are increasingly proposed to address such issues (Hua and Wang, 2019; Zhai et al., 2019; Zhao et al., 2020), and are beginning to look somewhat like the old NLG pipeline. In this context, surface realisation is very much back on the agenda, and the term is coming back into frequent use (Zhai et al., 2019; Zhao et al., 2020). Our aim for future editions of the SR Shared Task is to test whether multi-hop gives better results overall than single-hop, but also to link up with content selection modules capable of supplying the inputs required by SR systems. For this year, our main objective is to explore the impact of restricted vs. unrestricted resources in system"
2020.msr-1.1,D19-6304,0,0.216682,"er proposed in (Yu et al., 2020), which models the task of word ordering as a Traveling Salesman Problem, and uses a biaffine attention model to calculate the bigram scores for the output sequence. To remedy the restriction of projectivity, it uses a transition system to reorder the sentence. Furthermore, model ensembling and data augmentation is applied to push the performance. The NILC submission explores different ways to represent a UD structure linearly, and models the generation task by using the small version of GPT-2. 3.2 SR’19 systems run on the SR’20 new test sets The BME-UW system (Kovács et al., 2019) performs word order restoration by learning Interpreted Regular Tree Grammar (IRTG) rules encoding the correspondence between UD-subgraphs and word orderings. The grammars build strings and UD graphs simultaneously, using pairs of operations each connecting a set of dependents to their common head while concatenating the corresponding words. Rule 3 Data type Dataset Track train dev test In-domain arabic_padt (ar) chinese_gsd (zh) english_ewt (en) english_gum (en) english_lines (en) english_partut (en) french_gsd (fr) french_partut (fr) french_sequoia (fr) hindi_hdtb (hi) indonesian_gsd (id) j"
2020.msr-1.1,2020.acl-main.703,0,0.0144601,"mmon head while concatenating the corresponding words. The approach extends the team’s 2019 system by allowing rules to reference lemmas in addition to POS-tags and by giving preference to derivations that use a smaller number of more specific rules to construct a particular UD graph. Word order restoration is performed separately for each clause. For the inflection step, a standard sequence-to-sequence model with biLSTM encoder and LSTM decoder with attention is used. Concordia uses a text-to-text model to tackle graph-to-text surface realisation. The approach finetunes the pre-trained BART (Lewis et al., 2020) language model on the task of surface realisation where the model receives the linearised representation of the dependency tree and generates the surface text. The IMS system builds on their system from the previous year with a substantial change in the lineariser proposed in (Yu et al., 2020), which models the task of word ordering as a Traveling Salesman Problem, and uses a biaffine attention model to calculate the bigram scores for the output sequence. To remedy the restriction of projectivity, it uses a transition system to reorder the sentence. Furthermore, model ensembling and data augm"
2020.msr-1.1,K18-2014,0,0.0243276,"ta, in the same domains as the training and development data; (ii) Out-of-domain, which are test sets of parallel sentences in different languages in domains not covered by the training and development data; and (iii) silver standard data, which consists of automatically parsed sentences. The in-domain and out-of-domain data is provided in the UD release V2.3.1 The silver standard data was processed using the best CoNLL’18 parsers for the chosen datasets: the Harbin HIT-SCIR (HIT) parser (Che et al., 2017) for English_ewt, Hindi_hdtb, Korean_kaist and Spanish_ancora; the LATTICE (LAT) parser (Lim et al., 2018) for English_pud and the Stanford (STF) parser (Qi et al., 2019) for Portuguese_bosque.2 A detailed description of all SR’19 datasets and how they were processed can be found in the SR’19 report paper (Mille et al., 2019). 4.2 SR’20 new test sets To obtain new test sets,3 we selected sentences from Wikipedia in six out of the eleven SR’19 languages for which it was possible to get a good quantity of clean texts on the same topics. The used articles contain mostly landmarks and some historical figures. On the extracted sentences, we applied extensive filtering to achieve reasonably good text qu"
2020.msr-1.1,D17-1262,1,0.833898,"look at improvements this year compared to 2019, we see for instance, on the English_ewt test set, last year’s top BLEU score in T1 (the Shallow Track) was 82.98 (IMS); in 2020, it goes up to 86.16 in the restricted track (IMS), and 87.5 in the open track (ADAPT). In T2 (the Deep Track), top BLEU scores also increased, from 54.75 (IMS) to 58.84 in the restricted track, and 58.66 in the unrestricted track (both IMS). We next look at overall improvements of team submissions across all test sets they submitted outputs bias into the evaluation revealing no significant evidence of reference-bias (Ma et al., 2017). 8 ADAPT 20a 20b –BLEU-4– T1_ar_padt T1_en_ewt T2_en_ewt T1_en_gum T2_en_gum T1_en_lines T2_en_lines T1_en_partut T2_en_partut T1_es_ancora T2_es_ancora T1_es_gsd T2_es_gsd T1_fr_gsd T2_fr_gsd T1_fr_partut T2_fr_partut T1_fr_sequoia T2_fr_sequoia T1_hi_hdtb T1_id_gsd T1_ja_gsd T1_ko_gsd T1_ko_kaist T1_pt_bosque T1_pt_gsd T1_ru_gsd T1_ru_syntagrus T1_zh_gsd BME 20a 19 26 57.25 26.4 59.22 60.77 57.57 55.98 48.78 57.96 61.37 59.32 61.09 54.6 53.74 43.21 43.8 52.46 49.17 45.25 46.72 57.2 59.16 50.89 58.37 57.05 39.89 30.68 54.28 54.79 50.58 63.63 54.22 49.53 46.08 47.23 39.53 30.39 54.58 50.91 58"
2020.msr-1.1,W17-3517,1,0.829222,"et al., 2011), it was to address a situation where there were many different approaches to SR but none of them were comparable. We developed a commonground input representation that different approaches could map their normal inputs to, making results directly comparable for the first time. Most SR’11 systems (and all top performing ones) were statistical dependency realisers that did not make use of an explicit, pre-existing grammar. However, the question of how inputs to the realisers were going to be provided in an embedding system was left open. By the time we proposed the second SR Task (Mille et al., 2017), Universal Dependencies (UDs) had emerged as a convenient standard in parsing, with many associated data sets, that we were able to pick up and use as the common-ground representation. By now, the third, neural generation of NLG methods was beginning to dominate the field, and systems participating in SR’18 were all trained to map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisatio"
2020.msr-1.1,W18-3601,1,0.732438,"ny associated data sets, that we were able to pick up and use as the common-ground representation. By now, the third, neural generation of NLG methods was beginning to dominate the field, and systems participating in SR’18 were all trained to map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisation stage. Nevertheless, the community enthusiastically participated in SR’18 and SR’19 (Mille et al., 2018; Mille et al., 2019) as we expanded tracks to 11 languages. This year, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Mult"
2020.msr-1.1,D19-6301,1,0.741899,"ets, that we were able to pick up and use as the common-ground representation. By now, the third, neural generation of NLG methods was beginning to dominate the field, and systems participating in SR’18 were all trained to map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisation stage. Nevertheless, the community enthusiastically participated in SR’18 and SR’19 (Mille et al., 2018; Mille et al., 2019) as we expanded tracks to 11 languages. This year, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are"
2020.msr-1.1,W15-4719,0,0.0195724,"rojective tree; the completion model generates absent function words sequentially given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert lemmas to word forms character by character; the contraction model predicts BIO tags to group words to be contracted, and then generates the contracted word form of each group with a seq2seq model. The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises texts by first preprocessing the dependency tree into a preordered linearized form, which is then converted into its textual counterpart using a rule-based approach together with a statistical machine translation (SMT) model. A singular version of the model was trained for each language considered in the experiment. 4 4.1 Data Sets T1 and T2 training and test sets (same as in SR’19) There are 42 datasets in 11 languages, 29 datasets for T1,"
2020.msr-1.1,P02-1040,0,0.108533,"-20-multilingual 2 5 Figure 1: Sample UD structure (without the last two columns). Figure 2: Sample T1 input structure (without the last two columns). Figure 3: Sample T2 input structure (without the last two columns). and Deep Track inputs is available on GitLab.4 Figures 1, 2 and 3 shown sample UD, Track 1 and Track 2 structures respectively, taken from the parsed Wikipedia English dataset. 5 Evaluation Methods 5.1 Automatic methods We used BLEU, NIST, BERT, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems. BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences. We use the smoothed version and report results for n = 4. NIST5 is a related n-gram similarity metric weighted in favor of less frequent n-grams which are taken to be more informative. DIST starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn the system output into the (single) reference text. The resulting number is then divided by the number of characters"
2020.msr-1.1,N18-1202,0,0.0268272,"rd order and inflecting words. a. Restricted-resources subtrack (same as SR’19 Track 1): Teams built models trained on the provided T1 dataset(s), but use of external task-specific data was not permitted. However, teams were allowed to use external generic resources. For example, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material. Also permitted was the use of generic publicly available off-the-shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013). Alternatively, BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). b. Open subtrack: In this track, teams built models trained on the provided T1 dataset(s), also using any additional resources, without restrictions. Teams could even use the SR conversion tool to produce data with the exact same specifications as the data provided in the track, by applying the converter to a parsed output (see Section 4.2). T2 Deep Track: Inputs in this track"
2020.msr-1.1,2020.acl-demos.14,0,0.0252606,"xt quality. We skipped sentences that include special characters, contain unusual tokens (e.g. ISBN), or have unbalanced quotation marks or brackets. Furthermore, we took only sentences with more than 5 tokens and shorter than 50 tokens. After the initial filtering, quite a few malformed sentences remained. In order to remove those, we scored the sentences with BERT and kept only the best scored half. Finally, via manual inspection we identified patterns and expressions to reduce the number of malformed sentences still further. We parsed the cleaned Wikipedia sentences with the Stanza parser (Qi et al., 2020), using the trained models provided for the respective languages; the Stanza parser gets very competitive results on a large set of languages (see Table 3). For each language, we executed the parser with the processors for Tokenisation and Sentence Split, Multi-word Token Expansion, Part-of-Speech and Morphological Tagging, Lemmatisation and Dependency Parsing. The performance of the parser for all six languages in terms of Labelled Attachment Score and lemmatisation, two of the crucial aspects for our task, is provided in Table 3; for reference, we also provide the LAS and lemma scores of the"
2020.msr-1.1,K18-2011,1,0.865135,"Missing"
2020.msr-1.1,D19-6306,0,0.147951,", T2 T1, T2 T1 T1 T1 T1, T2 - - 1,795 1,032 1,675 2,287 471 1,723 Automatically parsed Wikipedia english_wikiST Z (en) french_wikiST Z (fr) korean_wikiST Z (ko) portuguese_wikiST Z (pt) russian_wikiST Z (ru) spanish_wikiST Z (es) T1, T2 T1, T2 T1 T1 T1 T1, T2 - - 1,313 1,313 530 1,135 1,291 1,280 Table 2: SR’20 dataset sizes for training, development and test sets (number of sentences). weights are proportional to the observed frequency of each pattern in the training data. The inflection step uses a standard sequence-to-sequence model with biLSTM encoder and LSTM decoder with attention. IMS (Yu et al., 2019) uses a pipeline approach for both tracks, consisting of linearisation, completion (for T2 only), inflection, and contraction. All models use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search, then combining the trees into a full projective tree; the completion model generates absent function words sequentially given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert lemmas to word forms character by character; the contraction model predicts BIO tags to group w"
2020.msr-1.1,2020.acl-main.134,0,0.342113,"estoration is performed separately for each clause. For the inflection step, a standard sequence-to-sequence model with biLSTM encoder and LSTM decoder with attention is used. Concordia uses a text-to-text model to tackle graph-to-text surface realisation. The approach finetunes the pre-trained BART (Lewis et al., 2020) language model on the task of surface realisation where the model receives the linearised representation of the dependency tree and generates the surface text. The IMS system builds on their system from the previous year with a substantial change in the lineariser proposed in (Yu et al., 2020), which models the task of word ordering as a Traveling Salesman Problem, and uses a biaffine attention model to calculate the bigram scores for the output sequence. To remedy the restriction of projectivity, it uses a transition system to reorder the sentence. Furthermore, model ensembling and data augmentation is applied to push the performance. The NILC submission explores different ways to represent a UD structure linearly, and models the generation task by using the small version of GPT-2. 3.2 SR’19 systems run on the SR’20 new test sets The BME-UW system (Kovács et al., 2019) performs wo"
2020.msr-1.1,W19-3404,0,0.023759,"ferent again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are increasingly proposed to address such issues (Hua and Wang, 2019; Zhai et al., 2019; Zhao et al., 2020), and are beginning to look somewhat like the old NLG pipeline. In this context, surface realisation is very much back on the agenda, and the term is coming back into frequent use (Zhai et al., 2019; Zhao et al., 2020). Our aim for future editions of the SR Shared Task is to test whether multi-hop gives better results overall than single-hop, but also to link up with content selection modules capable of supplying the inputs required by SR systems. For this year, our main objective is to explore the impact of restricted vs. unrestricted resources in system training, and cros"
bohnet-seniv-2004-mapping,W03-2404,0,\N,Missing
bohnet-seniv-2004-mapping,H01-1014,0,\N,Missing
bohnet-seniv-2004-mapping,P01-1029,0,\N,Missing
bohnet-seniv-2004-mapping,P99-1065,0,\N,Missing
bohnet-seniv-2004-mapping,P98-1026,0,\N,Missing
bohnet-seniv-2004-mapping,C98-1026,0,\N,Missing
bohnet-seniv-2004-mapping,W01-0807,1,\N,Missing
bohnet-wanner-2010-open,levy-andrew-2006-tregex,0,\N,Missing
bohnet-wanner-2010-open,C00-1007,0,\N,Missing
bohnet-wanner-2010-open,P03-1011,0,\N,Missing
bohnet-wanner-2010-open,P03-1003,0,\N,Missing
bohnet-wanner-2010-open,knight-al-onaizan-1998-translation,0,\N,Missing
bohnet-wanner-2010-open,P03-2041,0,\N,Missing
bohnet-wanner-2010-open,J97-3002,0,\N,Missing
bohnet-wanner-2010-open,W02-2105,0,\N,Missing
bohnet-wanner-2010-open,A97-1039,0,\N,Missing
bohnet-wanner-2010-open,W01-0807,1,\N,Missing
bohnet-wanner-2010-open,N03-1019,0,\N,Missing
C10-1011,W08-2123,0,0.0509236,"a summary and an outline of further research. head and child part-of-speech tag combination. The transition based parsers have a lower complexity. Nevertheless, the reported run times in the last shared tasks were similar to the maximum spanning tree parsers. For a transition based parser, Gesmundo et al. (2009) reported run times between 2.2 days for English and 4.7 days for Czech for the joint training of syntactic and semantic dependencies. The parsing times were about one word per second, which speeds up quickly with a smaller beam-size, although the accuracy of the parser degrades a bit. Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007). 2 Related Work The two main approaches to dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto., 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Nivre et al., 2004; Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadra"
C10-1011,W06-2922,0,0.0565762,"ds up quickly with a smaller beam-size, although the accuracy of the parser degrades a bit. Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007). 2 Related Work The two main approaches to dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto., 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Nivre et al., 2004; Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, which used a beam search with a latent variable machine learning technique. Maximum spanning tree dependency based parsers decomposes a dependency structure into parts known as “factors”. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label. This algorithm has a quadratic com"
C10-1011,E06-1011,0,0.886586,"for the joint training of syntactic and semantic dependencies. The parsing times were about one word per second, which speeds up quickly with a smaller beam-size, although the accuracy of the parser degrades a bit. Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007). 2 Related Work The two main approaches to dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto., 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Nivre et al., 2004; Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, which used a beam search with a latent variable machine learning technique. Maximum spanning tree dependency based parsers decomposes a dependency structure into parts known as “factors”. The factors of the first order maximum spanning tree parsing algorithm"
C10-1011,P05-1012,0,0.40745,"arallel algorithms that use several CPU cores, and feature selection that eliminates the features that do not improve accuracy. We employ, as a basis for our parser, the second order maximum spanning tree dependency parsing algorithm of Carreras (2007). This algorithm frequently reaches very good, or even the best labeled attachment scores, and was one of the most used parsing algorithms in the shared task 2009 of the Conference on Natural Language Learning (CoNLL) (Hajiˇc et al., 2009). We combined this parsing algorithm with the passive-aggressive perceptron algorithm (Crammer et al., 2003; McDonald et al., 2005; Crammer et al., 2006). A parser build out of these two algorithms provides a good baseline and starting point to improve upon the parsing and training times. 1 Introduction Highly accurate dependency parsers have high demands on resources and long parsing times. The training of a parser frequently takes several days and the parsing of a sentence can take on average up to a minute. The parsing time usage is important for many applications. For instance, dialog The rest of the paper is structured as follows. In Section 2, we describe related work. In section 3, we analyze the time usage of the"
C10-1011,W09-1210,1,0.663159,"ist ∪{(w1 , w2 )} c ← number of CPU cores for t ← 1 to c Tt ← create-array-thread(t, xi ,data-list) start array-thread Tt // start thread t for t ← 1 to c join Tt // wait until thread t is finished A ← A ∪ collect-result(Tt ) return A // array-thread T d ← remove-first-element(data-list) if d is empty then end-thread ... // extract features and calculate part d of A 6 Non-Projective Approximation Threshold For non-projective parsing, we use the NonProjective Approximation Algorithm of McDonald and Pereira (2006). The algorithm rearranges edges in a dependency tree when they improve the score. Bohnet (2009) extended the algorithm by a threshold which biases the rearrangement of the edges. With a threshold, it is possible to gain a higher percentage of correct dependency links. We determined a threshold in experiments for Czech, English and German. In the experiment, we use the Hash Kernel and increase the thresh95 System Top CoNLL 09 Baseline Parser this work Average (1) 85.77 85.10 86.33 Catalan (1) 87.86 85.70 87.45 Chinese (4) 79.19 76.88 76.99 Czech (1) 80.38 76.93 80.96 English (2) 89.88 90.14 90.33 German (2) 87.48 87.64 88.06 Japanese (3) 92.57 92.26 92.47 Spanish 87.64(1) 86.12 88.13 Tab"
C10-1011,P08-1108,0,0.0150516,"ser with Hash Kernel. The numbers in bold face mark the top scores. We used for Catalan, Chinese, Japanese and Spanish the projective parsing algorithm. old at the beginning in small steps by 0.1 and later scores. However, the parser would have ranked in larger steps by 0.5 and 1.0. Figure 2 shows second for these languages. For Catalan and the labeled attachment scores for the Czech, En- Chinese, the top results obtained transition-based glish and German development set in relation to parsers. Therefore, the integration of both techthe rearrangement threshold. The curves for all niques as in Nivre and McDonald (2008) seems languages are a bit volatile. The English curve to be very promising. For instance, to improve is rather flat. It increases a bit until about 0.3 the accuracy further, more global constrains capand remains relative stable before it slightly de- turing the subcategorization correct could be intecreases. The labeled attachment score for Ger- grated as in Riedel and Clarke (2006). Our faster man and Czech increases until 0.3 as well and then algorithms may make it feasible to consider furboth scores start to decrease. For English a thresh- ther higher order factors. old between 0.3 and abo"
C10-1011,D07-1101,0,0.757549,"in the accuracy level, or methods that trade accuracy against better parsing times. Software developers and researchers are usually unwilling to reduce the quality of their applications. Consequently, we have to consider at first methods to improve a parser, which do not involve an accuracy loss, such as faster algorithms, faster implementation of algorithms, parallel algorithms that use several CPU cores, and feature selection that eliminates the features that do not improve accuracy. We employ, as a basis for our parser, the second order maximum spanning tree dependency parsing algorithm of Carreras (2007). This algorithm frequently reaches very good, or even the best labeled attachment scores, and was one of the most used parsing algorithms in the shared task 2009 of the Conference on Natural Language Learning (CoNLL) (Hajiˇc et al., 2009). We combined this parsing algorithm with the passive-aggressive perceptron algorithm (Crammer et al., 2003; McDonald et al., 2005; Crammer et al., 2006). A parser build out of these two algorithms provides a good baseline and starting point to improve upon the parsing and training times. 1 Introduction Highly accurate dependency parsers have high demands on"
C10-1011,W04-2407,0,0.0176679,"r second, which speeds up quickly with a smaller beam-size, although the accuracy of the parser degrades a bit. Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007). 2 Related Work The two main approaches to dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto., 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Nivre et al., 2004; Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, which used a beam search with a latent variable machine learning technique. Maximum spanning tree dependency based parsers decomposes a dependency structure into parts known as “factors”. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label. This algorithm has"
C10-1011,W09-1207,0,0.0189204,"Missing"
C10-1011,W03-3017,0,0.154308,"spanning tree parsers. For a transition based parser, Gesmundo et al. (2009) reported run times between 2.2 days for English and 4.7 days for Czech for the joint training of syntactic and semantic dependencies. The parsing times were about one word per second, which speeds up quickly with a smaller beam-size, although the accuracy of the parser degrades a bit. Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007). 2 Related Work The two main approaches to dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto., 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Nivre et al., 2004; Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, which used a beam search with a latent variable machine learning technique. Maximum spanning t"
C10-1011,W02-1001,0,0.0453151,"aining set, the total number of features in million, the labeled attachment score of the test set, and the unlabeled attachment score. → gorithm updates − w according to the difference between the predicted dependency structures ya → and the reference structure yi . It updates − v as well, whereby the algorithm additionally weights the updates by γ. Since the algorithm decreases γ in each round, the algorithm adapts the weights more aggressively at the beginning (Crammer et al., 2006). After all iterations, the algorithm com→ putes the average of − v , which reduces the effect of overfitting (Collins, 2002). We have inserted into the training algorithm functions to measure the start times ts and the end times te for the procedures to compute and store the features, to read the features, to predict the projective parse, and to calculate the nonprojective approximation. We calculate the average elapsed time per instance, as the average over all training examples and epochs: Algorithm 1: Training – baseline algorithm τ = {(xi , yi )}Ii=1 // Training data → − − w = 0, → v =0 γ = E ∗ I // passive-aggresive update weight for i = 1 to I tss+e ; extract-and-store-features(xi ); tes+e ; for n = 1 to E //"
C10-1011,P09-1040,0,0.034311,"th a smaller beam-size, although the accuracy of the parser degrades a bit. Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007). 2 Related Work The two main approaches to dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto., 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Nivre et al., 2004; Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, which used a beam search with a latent variable machine learning technique. Maximum spanning tree dependency based parsers decomposes a dependency structure into parts known as “factors”. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label. This algorithm has a quadratic complexity. The s"
C10-1011,W09-1215,0,0.0134344,"ble to gain a higher percentage of correct dependency links. We determined a threshold in experiments for Czech, English and German. In the experiment, we use the Hash Kernel and increase the thresh95 System Top CoNLL 09 Baseline Parser this work Average (1) 85.77 85.10 86.33 Catalan (1) 87.86 85.70 87.45 Chinese (4) 79.19 76.88 76.99 Czech (1) 80.38 76.93 80.96 English (2) 89.88 90.14 90.33 German (2) 87.48 87.64 88.06 Japanese (3) 92.57 92.26 92.47 Spanish 87.64(1) 86.12 88.13 Table 6: Top LAS of the CoNLL 2009 of (1) Gesmundo et al. (2009), (2) Bohnet (2009), (3) Che et al. (2009), and (4) Ren et al. (2009); LAS of the baseline parser and the parser with Hash Kernel. The numbers in bold face mark the top scores. We used for Catalan, Chinese, Japanese and Spanish the projective parsing algorithm. old at the beginning in small steps by 0.1 and later scores. However, the parser would have ranked in larger steps by 0.5 and 1.0. Figure 2 shows second for these languages. For Catalan and the labeled attachment scores for the Czech, En- Chinese, the top results obtained transition-based glish and German development set in relation to parsers. Therefore, the integration of both techthe rearrangement thr"
C10-1011,C96-1058,0,0.0819631,"lish and 4.7 days for Czech for the joint training of syntactic and semantic dependencies. The parsing times were about one word per second, which speeds up quickly with a smaller beam-size, although the accuracy of the parser degrades a bit. Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007). 2 Related Work The two main approaches to dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto., 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Nivre et al., 2004; Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, which used a beam search with a latent variable machine learning technique. Maximum spanning tree dependency based parsers decomposes a dependency structure into parts known as “factors”. The factors of the first ord"
C10-1011,W06-1616,0,0.0176941,"En- Chinese, the top results obtained transition-based glish and German development set in relation to parsers. Therefore, the integration of both techthe rearrangement threshold. The curves for all niques as in Nivre and McDonald (2008) seems languages are a bit volatile. The English curve to be very promising. For instance, to improve is rather flat. It increases a bit until about 0.3 the accuracy further, more global constrains capand remains relative stable before it slightly de- turing the subcategorization correct could be intecreases. The labeled attachment score for Ger- grated as in Riedel and Clarke (2006). Our faster man and Czech increases until 0.3 as well and then algorithms may make it feasible to consider furboth scores start to decrease. For English a thresh- ther higher order factors. old between 0.3 and about 2.0 would work well. In this paper, we have investigated possibilities For German and Czech, a threshold of about 0.3 for increasing parsing speed without any accuracy is the best choice. We selected for all three lan- loss. The parsing time is 3.5 times faster on a singuages a threshold of 0.3. gle CPU core than the baseline parser which has an typical architecture for a maximum"
C10-1011,W09-1205,0,0.0296558,"Missing"
C10-1011,W07-2218,0,0.02342,"ased parser, Gesmundo et al. (2009) reported run times between 2.2 days for English and 4.7 days for Czech for the joint training of syntactic and semantic dependencies. The parsing times were about one word per second, which speeds up quickly with a smaller beam-size, although the accuracy of the parser degrades a bit. Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007). 2 Related Work The two main approaches to dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto., 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Nivre et al., 2004; Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, which used a beam search with a latent variable machine learning technique. Maximum spanning tree dependency based parsers decomposes a dependency stru"
C10-1011,W03-3023,0,0.344429,"Missing"
C10-1012,C00-1007,0,0.036318,"ent. Its disadvantage is that it requires at least syntactically annotated corpora of significant size (Bangalore et al., 2001). Given the aspiration of NLG to start from numeric time series or conceptual or semantic structures, syntactic annotation even does not suffice: the corpora must also be at least semantically annotated. Up to date, deep stochastic sentence realization was hampered by the lack of multiple-level annotated corpora. As a consequence, available stochastic sentence generators either take syntactic structures as input (and avoid thus the need for multiple-level annotation) (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or draw upon hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators"
C10-1012,P98-1026,0,0.035183,"Missing"
C10-1012,P01-1024,0,0.0300827,"om the analyzer, which makes both approaches not directly comparable. 5.3 Discussion The overall performance of our SVM-based deep sentence generator ranges between 0.611 (for German) and 0.688 (for Chinese) of the BLEU score. HALogen’s (Langkilde-Geary, 2002) scores range between 0.514 and 0.924, depending on the completeness of the input. The figures are not directly comparable since HALogen takes as input syntactic structures. However, it gives us an idea where our generator is situated. Traditional linearization approaches are rulebased; cf., e.g., (Br¨oker, 1998; Gerdes and Kahane, 2001; Duchier and Debusmann, 2001), and (Bohnet, 2004). More recently, statistic language models have been used to derive word order, cf. (Ringger et al., 2004; Wan et al., 2009) and (Filippova and Strube, 2009). Because of its partially free order, which is more difficult to handle than fixed word order, German has often been worked with in the context of linearization. Filippova and Strube (2009) adapted their linearization model originally developed for German to English. They use two classifiers to determine the word order in a sentence. The first classifier uses a trigram LM to order words within constituents, and the sec"
C10-1012,W96-0501,0,0.0268062,"ubsequent integration of other generation tasks such as referring expression generation, ellipsis generation, and aggregation. As a matter of fact, this generator instantiates the Reference Architecture for Generation Systems (Mellish et al., 2006) for linguistic generation. A more practical advantage of the presented deep stochastic sentence generator (as, in principle, of all stochastic generators) is that, if trained on a representative corpus, it is domainindependent. As rightly pointed out by Belz (2008), traditional wide coverage realizers such as KPML (Bateman et al., 2005), FUF/SURGE (Elhadad and Robin, 1996) and RealPro (Lavoie and Rambow, 1997), which were also intended as off-the-shelf plug-in realizers still tend to require a considerable amount of work for integration and fine-tuning of the grammatical and lexical resources. Deep stochastic sentence realizers have the potential to become real off-the-shelf modules. Our realizer is freely available for download at http://www.recerca.upf.edu/taln. 3 We are currently working on a generation-oriented multilevel annotation of corpora for a number of languages. The corpora will be made available to the community. 105 Acknowledgments Many thanks to"
C10-1012,D08-1019,0,0.0243456,"syntactically annotated corpora of significant size (Bangalore et al., 2001). Given the aspiration of NLG to start from numeric time series or conceptual or semantic structures, syntactic annotation even does not suffice: the corpora must also be at least semantically annotated. Up to date, deep stochastic sentence realization was hampered by the lack of multiple-level annotated corpora. As a consequence, available stochastic sentence generators either take syntactic structures as input (and avoid thus the need for multiple-level annotation) (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or draw upon hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators require semantically annotated, or, even better, mul"
C10-1012,N09-2057,0,0.382889,"is the head, 2 if w2 is the head, etc. and else 0; dist is the position within the constituent; contains-? is a boolean value which is true if the sentence contains a question mark and false otherwise; pos-head is the position of the head in the constituent) 4.2 Dependency Tree Linearization Since we use unordered dependency trees as syntactic structures, our realizer has to find the optimal linear order for the lexemes of each dependency tree. Algorithm 4 shows our linearization algorithm. To order the dependency tree, we use a one classifier-approach for all languages—in contrast to, e.g., Filippova and Strube (2009), who use a two-classifier approach for German.1 The algorithm is again a beam search. It starts with an elementary list for each node of the dependency tree. Each elementary list is first extended by the children of the node in the list; then, the lists are extended stepwise by the children of the newly added nodes. If the number of lists during this procedure exceeds the threshold of 1000, the lists are sorted in accordance with their score, and the first 1000 are kept. The remaining lists are removed. Afterwards, the score of each list is adjusted according to a global score function which"
C10-1012,P01-1029,0,0.0295193,"or is directly derived from the analyzer, which makes both approaches not directly comparable. 5.3 Discussion The overall performance of our SVM-based deep sentence generator ranges between 0.611 (for German) and 0.688 (for Chinese) of the BLEU score. HALogen’s (Langkilde-Geary, 2002) scores range between 0.514 and 0.924, depending on the completeness of the input. The figures are not directly comparable since HALogen takes as input syntactic structures. However, it gives us an idea where our generator is situated. Traditional linearization approaches are rulebased; cf., e.g., (Br¨oker, 1998; Gerdes and Kahane, 2001; Duchier and Debusmann, 2001), and (Bohnet, 2004). More recently, statistic language models have been used to derive word order, cf. (Ringger et al., 2004; Wan et al., 2009) and (Filippova and Strube, 2009). Because of its partially free order, which is more difficult to handle than fixed word order, German has often been worked with in the context of linearization. Filippova and Strube (2009) adapted their linearization model originally developed for German to English. They use two classifiers to determine the word order in a sentence. The first classifier uses a trigram LM to order words wi"
C10-1012,W09-1201,0,0.0950376,"Missing"
C10-1012,P09-1091,0,0.394975,"ore (ULA) is the proportion of correct tokens that are assigned the correct head. To assess the quality of linearization, we use three different evaluation metrics. The first metric is the per-phrase/per-clause accuracy (acc snt.), which facilitates the automatic evaluation of results: acc = correct constituents all constituents As second evaluation metric, we use a metric related to the edit distance: di = 1 − m total number of words (with m as the minimum number of deletions combined with insertions to obtain the correct order (Ringger et al., 2004)). To be able to compare our results with (He et al., 2009) and (Ringger et al., 2004), we use the BLEU score as a third metric. For the asessment of the quality of the word form generation, we use the accuracy score. The accuracy is the ratio between correctly generated word forms and the entire set of generated word forms. For the evaluation of the sentence realizer as a whole, we use the BLEU metric. 5.2 Experimental Results Table 4 displays the results obtained for the isolated stages of sentence realization and of the realization as a whole, with reference to a baseline and to some state-of-the-art works. The baseline is the deep sentence realiza"
C10-1012,P95-1034,0,0.032851,"syntactic annotation even does not suffice: the corpora must also be at least semantically annotated. Up to date, deep stochastic sentence realization was hampered by the lack of multiple-level annotated corpora. As a consequence, available stochastic sentence generators either take syntactic structures as input (and avoid thus the need for multiple-level annotation) (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or draw upon hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators require semantically annotated, or, even better, multilevel annotated corpora. Only then can they deal with such crucial generation issues as sentence planning, linearization and morphologization. Multilevel annotated corpora are increa"
C10-1012,P98-1116,0,0.894255,"suffice: the corpora must also be at least semantically annotated. Up to date, deep stochastic sentence realization was hampered by the lack of multiple-level annotated corpora. As a consequence, available stochastic sentence generators either take syntactic structures as input (and avoid thus the need for multiple-level annotation) (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or draw upon hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators require semantically annotated, or, even better, multilevel annotated corpora. Only then can they deal with such crucial generation issues as sentence planning, linearization and morphologization. Multilevel annotated corpora are increasingly available for multiple"
C10-1012,W02-2103,0,0.227142,"t it requires at least syntactically annotated corpora of significant size (Bangalore et al., 2001). Given the aspiration of NLG to start from numeric time series or conceptual or semantic structures, syntactic annotation even does not suffice: the corpora must also be at least semantically annotated. Up to date, deep stochastic sentence realization was hampered by the lack of multiple-level annotated corpora. As a consequence, available stochastic sentence generators either take syntactic structures as input (and avoid thus the need for multiple-level annotation) (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or draw upon hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators require semantically an"
C10-1012,A97-1039,0,0.0615238,"ion tasks such as referring expression generation, ellipsis generation, and aggregation. As a matter of fact, this generator instantiates the Reference Architecture for Generation Systems (Mellish et al., 2006) for linguistic generation. A more practical advantage of the presented deep stochastic sentence generator (as, in principle, of all stochastic generators) is that, if trained on a representative corpus, it is domainindependent. As rightly pointed out by Belz (2008), traditional wide coverage realizers such as KPML (Bateman et al., 2005), FUF/SURGE (Elhadad and Robin, 1996) and RealPro (Lavoie and Rambow, 1997), which were also intended as off-the-shelf plug-in realizers still tend to require a considerable amount of work for integration and fine-tuning of the grammatical and lexical resources. Deep stochastic sentence realizers have the potential to become real off-the-shelf modules. Our realizer is freely available for download at http://www.recerca.upf.edu/taln. 3 We are currently working on a generation-oriented multilevel annotation of corpora for a number of languages. The corpora will be made available to the community. 105 Acknowledgments Many thanks to the three anonymous reviewers for thei"
C10-1012,W00-0306,0,0.0191148,"Missing"
C10-1012,J05-1004,0,0.0546622,"0’ for “A0 realized as a relative clause”, and ‘AM-MNR’ for “manner modifier”. As can be seen, 6 out of the total of 14 edges in the complete representation of this example have been added by Algorithm 1. We still did not finish the formal evaluation of the principal changes necessary to adapt the PropBank annotation for generation, nor the quality of our completion algorithm. However, the need of an annotation with generation in mind is obvious. Completing the Semantic Annotation The semantic annotation of sentences in CoNLL ’09 shared task corpora follows the PropBank annotation guidelines (Palmer et al., 2005). Prob99 a Algorithm 1: Complete semantic graph //si is a semantic graph and yi a dependency tree // si = hNsi , Lsi , Esi i, where Nsi is the set of nodes // Lsi the set of edge labels // Esi ⊆ Ns × Ns × Ls is the set of edges for i ← 1 to |I |// iteration over the training examples let ry ∈ yi be the root node of the dependency tree // initialization of the queue nodeQueue ← children(ry ) while nodeQueue 6= ∅ do ny ← removeFirst(nodeQueue) // breath first: add nodes at the end of the queue nodeQueue ← nodeQueue ∪ children(ny ) nys ← sem(ny ); pys ← sem(parent(ny )) //get the semantic equival"
C10-1012,C04-1097,0,0.73441,"n The morphological realization algorithm selects the edit script in accordance with the highest score for each lemma of a sentence obtained during training (see Algorithm 2 above) and applies then the scripts to obtain the word forms; cf. Algorithm 5. Table 2 lists the feature schemas used for morphological realization. 5 Experiments To evaluate the performance of our realizer, we carried out experiments on deep generation of Chinese, English, German and Spanish, starting from CoNLL ’09 shared task corpora. The size of the test sets is listed in Table 3.2 2 As in (Langkilde-Geary, 2002) and (Ringger et al., 2004), we used Section 23 of the WSJ corpus as test set for English. 102 Algorithm 3: Semantic generation Algorithm 4: Dependency tree linearization //si , y semantic graph and its dependency tree for i ← 1 to |I |// iteration over the training examples // build an initial tree for all n1 ∈ si do trees ← {} // initialize the constructed trees list for all n2 ∈ si do if n1 6= n2 then for all l ∈ dependency-labels do trees = trees ∪ {(synt(n1 ),synt(n2 ),l)} trees ← sort-trees-descending-to-score(trees) trees ← look-forward(1000,sublist(trees,20)) //assess at most 1000 edges of the 20 best trees tree"
C10-1012,E09-1097,0,0.189557,"s between 0.611 (for German) and 0.688 (for Chinese) of the BLEU score. HALogen’s (Langkilde-Geary, 2002) scores range between 0.514 and 0.924, depending on the completeness of the input. The figures are not directly comparable since HALogen takes as input syntactic structures. However, it gives us an idea where our generator is situated. Traditional linearization approaches are rulebased; cf., e.g., (Br¨oker, 1998; Gerdes and Kahane, 2001; Duchier and Debusmann, 2001), and (Bohnet, 2004). More recently, statistic language models have been used to derive word order, cf. (Ringger et al., 2004; Wan et al., 2009) and (Filippova and Strube, 2009). Because of its partially free order, which is more difficult to handle than fixed word order, German has often been worked with in the context of linearization. Filippova and Strube (2009) adapted their linearization model originally developed for German to English. They use two classifiers to determine the word order in a sentence. The first classifier uses a trigram LM to order words within constituents, and the second (which is a maximum entropy classifier) determines the order of constituents that depend on a finite verb. For English, we achieve with our"
C10-1012,C98-1112,0,\N,Missing
C10-1012,C98-1026,0,\N,Missing
C10-1012,W01-0520,0,\N,Missing
C10-2129,W06-2922,0,0.0131657,"ble combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label. This algorithm has a quadratic complexity. The second orde"
C10-2129,W09-1210,1,0.879438,"Missing"
C10-2129,W06-2920,0,0.0226861,"over the labels. This algorithm therefore has a complexity of O(n4 ). Johansson and Nugues (2008) reduced the required number of loops over the edge labels by considering only the edges that existed in the training corpus for a distinct head and child part-of-speech tag combination. Predating the surge of interest in data-based dependency parsing, there is a relatively long tradition of dependency parsing work on German, including for instance Menzel and Schr¨oder (1998) and Duchier and Debusmann (2001). German was included in the CoNLL shared tasks in 2006 (Multilingual Dependency Parsing, (Buchholz and Marsi, 2006)) and in 2009 (Syntactic and Semantic Dependencies in Multiple Languages, (Hajiˇc et al., 2009)) with data based on the TIGER 1123 corpus (Brants et al., 2002) in both cases. Since the original TIGER treebank is in a hybrid phrasestructural/dependency format with a relatively flat hierarchical structure, conversion to a pure dependency format involves some non-trivial steps. The 2008 ACL Workshop on Parsing German included a specific shared task on dependency parsing of German (K¨ubler, 2008), based on two sets of data: again the TIGER corpus – however with a different conversion routine than"
C10-2129,D07-1101,0,0.0256126,"learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label. This algorithm has a quadratic complexity. The second order parsing algorithm of McDonald and Pereira (2006) uses a separate algorithm for edge labeling. In addition to the first order factors, this algorithm uses the edges to those children which are closest to the dependent and has a complexity of O(n3 ). The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild. The edge labeling is an integral part of the algorithm which requires an additional loop over the labels. This algorithm therefore has a complexity of O(n4 ). Johansson and Nugues (2008) reduced the required number of loops over the edge labels by considering only the edges that existed in the training corpus for a distinct head and child part-of-speech tag combination. Predating the surge of interest i"
C10-2129,P01-1024,0,0.132315,"he dependents to a grandchild. The edge labeling is an integral part of the algorithm which requires an additional loop over the labels. This algorithm therefore has a complexity of O(n4 ). Johansson and Nugues (2008) reduced the required number of loops over the edge labels by considering only the edges that existed in the training corpus for a distinct head and child part-of-speech tag combination. Predating the surge of interest in data-based dependency parsing, there is a relatively long tradition of dependency parsing work on German, including for instance Menzel and Schr¨oder (1998) and Duchier and Debusmann (2001). German was included in the CoNLL shared tasks in 2006 (Multilingual Dependency Parsing, (Buchholz and Marsi, 2006)) and in 2009 (Syntactic and Semantic Dependencies in Multiple Languages, (Hajiˇc et al., 2009)) with data based on the TIGER 1123 corpus (Brants et al., 2002) in both cases. Since the original TIGER treebank is in a hybrid phrasestructural/dependency format with a relatively flat hierarchical structure, conversion to a pure dependency format involves some non-trivial steps. The 2008 ACL Workshop on Parsing German included a specific shared task on dependency parsing of German (K"
C10-2129,W98-0509,0,0.562004,"Missing"
C10-2129,P05-1013,0,0.02721,"al., 2006) is a languageindependent system for data-driven dependency parsing which is freely available.7 It is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parsing actions. MaltParser employs a rich feature representation in order to guide parsing. For the training of the Malt parser model that we use in the stacking experiments, we use learner and parser settings identical to the ones optimized for German in the CoNLL-X shared task (Nivre et al., 2006). Furthermore, we employ the technique of pseudo-projective parsing described in Nilsson and Nivre (2005) and a split prediction strategy for predicting parse transitions and arc labels (Nivre and Hall, 2008).8 In order to obtain automatic parses for the whole data set, we perform a 10fold split. For the parser stacking, we follow the approach of Nivre and McDonald (2008), using MaltParser as a guide for the MST parser with the hash kernel, i.e., providing the arcs and labels assigned by MaltParser as features. Table 5 shows the scores we obtain by parser stacking. Although our version of MaltParser does not quite have the same performance as for instance the version of Hall and Nivre (2008), its"
C10-2129,C96-1058,0,0.125123,"pilot study (Section 7), and finally, we apply Integer Linear Programming in a targeted way to add some global constraints on possible combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The factors of the first order maximum spanning tree parsing algorithm a"
C10-2129,W08-1007,0,0.257232,"It is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parsing actions. MaltParser employs a rich feature representation in order to guide parsing. For the training of the Malt parser model that we use in the stacking experiments, we use learner and parser settings identical to the ones optimized for German in the CoNLL-X shared task (Nivre et al., 2006). Furthermore, we employ the technique of pseudo-projective parsing described in Nilsson and Nivre (2005) and a split prediction strategy for predicting parse transitions and arc labels (Nivre and Hall, 2008).8 In order to obtain automatic parses for the whole data set, we perform a 10fold split. For the parser stacking, we follow the approach of Nivre and McDonald (2008), using MaltParser as a guide for the MST parser with the hash kernel, i.e., providing the arcs and labels assigned by MaltParser as features. Table 5 shows the scores we obtain by parser stacking. Although our version of MaltParser does not quite have the same performance as for instance the version of Hall and Nivre (2008), its guidance leads to a small improvement in the overall parsing results. MaltParser our parser +stacking"
C10-2129,P08-1108,0,0.0752809,"ts significantly. This seems to support our intuition that number helps in disambiguating case values. However, adding gender information does not further increase this effect but hurts parser performance even more than case annotation alone. This leaves us with a puzzle here. Annotating case and number helps the parser, but case alone or having case, number and gender together affects performance negatively. A possible explanation might be that the effect of the gender information is masked by the increased number of feature values (24) which confuses the parsing algorithm. 7 Parser Stacking Nivre and McDonald (2008) show how two different approaches to data-driven dependency pars6 Person would be another syntactically relevant information. However, since we are dealing with a newspaper corpus, first and second person features appear very rarely. 1127 ing, the graph-based and transition-based approaches, may be combined and subsequently learn to complement each other to achieve improved parsing results for different languages. MaltParser (Nivre et al., 2006) is a languageindependent system for data-driven dependency parsing which is freely available.7 It is based on a deterministic parsing strategy in com"
C10-2129,W06-2933,0,0.0343479,"Missing"
C10-2129,W03-3017,0,0.265855,"nation of different parsing strategies is advantageous; we include a relatively simple parser stacking procedure in our pilot study (Section 7), and finally, we apply Integer Linear Programming in a targeted way to add some global constraints on possible combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parser"
C10-2129,P09-1040,0,0.0510405,"of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label. This algorithm has a quadratic complexity. The second order parsing algo"
C10-2129,W08-2123,0,0.0137423,") uses a separate algorithm for edge labeling. In addition to the first order factors, this algorithm uses the edges to those children which are closest to the dependent and has a complexity of O(n3 ). The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild. The edge labeling is an integral part of the algorithm which requires an additional loop over the labels. This algorithm therefore has a complexity of O(n4 ). Johansson and Nugues (2008) reduced the required number of loops over the edge labels by considering only the edges that existed in the training corpus for a distinct head and child part-of-speech tag combination. Predating the surge of interest in data-based dependency parsing, there is a relatively long tradition of dependency parsing work on German, including for instance Menzel and Schr¨oder (1998) and Duchier and Debusmann (2001). German was included in the CoNLL shared tasks in 2006 (Multilingual Dependency Parsing, (Buchholz and Marsi, 2006)) and in 2009 (Syntactic and Semantic Dependencies in Multiple Languages,"
C10-2129,P95-1037,0,0.0587336,"s of the noun phrase directly at PP level. This annotation was kept in the dependency version and it can cause problems for the parser since there are two different ways of annotating NPs: (i) for normal NPs where all dependents of the noun are attached as daughters of the head noun and (ii) for NPs in PPs where all dependents of the noun are attached as daughters to the preposition thus being sisters to their head noun. We changed the annotation of PPs by identifying the head noun in the PP and attaching all of its siblings to it. To find the correct head, we used a heuristic in the style of Magerman (1995). The head is chosen by taking the rightmost daughter of the preposition that has a category label according to the heuristic and is labeled with NK (noun kernel element). Table 1 shows the parser performance on the data after PP-restructuring.4 The explanation for the benefit of the restructuring is of course that 4 Note that we are evaluating against a gold standard here (and in the rest of the paper) which has been restructured as well. With a different gold standard one could argue that the absolute figures we obtain are not fully comparable with the original CoNLL shared task. However, si"
C10-2129,W96-0213,0,0.25313,"y likely to mislead the parser in its decision process. A lot of the parser’s features include PoS tags and reducing the amount of errors during PoS tagging will therefore reduce misleading feature values as well. Since the quality of the automatically assigned PoS tags in the German CoNLL ’09 data is not state-of-the-art (see Table 2 below), we decided to retag the data with our own tagger which uses additional information from a symbolic morphological analyzer to direct a statistical classifier. For the assignment of PoS tags, we apply a standard maximum entropy classification approach (see Ratnaparkhi (1996)). The classes of the classifier are the PoS categories defined in the Stuttgart-T¨ubingen Tag Set (STTS) (Schiller et al., 1999). We use standard binarized features like the word itself, its last three letters, whether the word is capitalized, contains a hyphen, a digit or whether it consists of digits only. As the only nonbinary feature, word length is recorded. These standard features are augmented by a number of binary features that support the classification process by providing a preselection of possible PoS tags. Every word is analyzed by DMOR, a finite state morphological analyzer, fro"
C10-2129,P10-1111,1,0.801761,"Missing"
C10-2129,W07-2218,0,0.0457627,"advantageous; we include a relatively simple parser stacking procedure in our pilot study (Section 7), and finally, we apply Integer Linear Programming in a targeted way to add some global constraints on possible combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The fac"
C10-2129,W03-3023,0,0.0797297,"ferent parsing strategies is advantageous; we include a relatively simple parser stacking procedure in our pilot study (Section 7), and finally, we apply Integer Linear Programming in a targeted way to add some global constraints on possible combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency str"
C10-2129,E06-1011,0,0.041155,"finally, we apply Integer Linear Programming in a targeted way to add some global constraints on possible combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the depend"
C10-2129,W08-1008,0,\N,Missing
C10-2129,W09-1201,0,\N,Missing
C10-3009,W09-1206,1,0.272157,"Missing"
C10-3009,W09-1210,1,0.918454,"gues† †Department of Computer science ‡Institute for Natural Language Processing Lund University University of Stuttgart anders.bjorkelund@cs.lth.se bohnet@ims.uni-stuttgart.de love.hafdell@cs.lth.se pierre.nugues@cs.lth.se Abstract This demonstration presents a highperformance syntactic and semantic dependency parser. The system consists of a pipeline of modules that carry out the tokenization, lemmatization, part-of-speech tagging, dependency parsing, and semantic role labeling of a sentence. The system’s two main components draw on improved versions of a state-of-the-art dependency parser (Bohnet, 2009) and semantic role labeler (Bj¨orkelund et al., 2009) developed independently by the authors. The system takes a sentence as input and produces a syntactic and semantic annotation using the CoNLL 2009 format. The processing time needed for a sentence typically ranges from 10 to 1000 milliseconds. The predicate–argument structures in the ﬁnal output are visualized in the form of segments, which are more intuitive for a user. 1 Motivation and Overview Semantic analyzers consist of processing pipelines to tokenize, lemmatize, tag, and parse sentences, where all the steps are crucial to their over"
C10-3009,burchardt-etal-2006-salsa,0,0.0332486,"Missing"
C10-3009,D07-1101,0,0.0749361,"anging from 100 to 1000 milliseconds. This demonstration is a practical semantic parser that takes an English sentence as input and produces syntactic and semantic dependency graphs using the CoNLL 2009 format. It builds on lemmatization and POS tagging preprocessing steps, as well as on two systems, one dealing with syntax and the other with semantic dependencies that reported respectively state-of-the-art results in the CoNLL 2009 shared task (Bohnet, 2009; Bj¨orkelund et al., 2009). The complete system architecture is shown in Fig. 1. The dependency parser is based on Carreras’s algorithm (Carreras, 2007) and second order spanning trees. The parser is trained with the margin infused relaxed algorithm (MIRA) (McDonald et al., 2005) and combined with a hash kernel (Shi et al., 2009). In combination with the system’s lemmatizer and POS tagger, this parser achieves an average labeled attachment score (LAS) of 89.88 when trained and tested on the English corpus of the CoNLL 2009 shared task (Surdeanu et al., 2008). The semantic role labeler (SRL) consists of a pipeline of independent, local classiﬁers that identify the predicates, their senses, the arguments of the predicates, and the argument labe"
C10-3009,W08-2123,1,0.853891,"Missing"
C10-3009,P05-1012,0,0.0393551,"input and produces syntactic and semantic dependency graphs using the CoNLL 2009 format. It builds on lemmatization and POS tagging preprocessing steps, as well as on two systems, one dealing with syntax and the other with semantic dependencies that reported respectively state-of-the-art results in the CoNLL 2009 shared task (Bohnet, 2009; Bj¨orkelund et al., 2009). The complete system architecture is shown in Fig. 1. The dependency parser is based on Carreras’s algorithm (Carreras, 2007) and second order spanning trees. The parser is trained with the margin infused relaxed algorithm (MIRA) (McDonald et al., 2005) and combined with a hash kernel (Shi et al., 2009). In combination with the system’s lemmatizer and POS tagger, this parser achieves an average labeled attachment score (LAS) of 89.88 when trained and tested on the English corpus of the CoNLL 2009 shared task (Surdeanu et al., 2008). The semantic role labeler (SRL) consists of a pipeline of independent, local classiﬁers that identify the predicates, their senses, the arguments of the predicates, and the argument labels. The SRL module achieves an average labeled semantic F1 of 80.90 when trained and tested on the English corpus of CoNLL 2009"
C10-3009,W08-2121,0,0.0470103,"Missing"
C10-3009,W09-1201,0,\N,Missing
C12-1052,C10-1011,1,0.873148,"order graph-based parser corresponds to a dependency edge. McDonald et al. (2005a) first used second order factors which incorporates siblings of the head and dependent. The second order algorithm of Carreras (2007) uses three second order factors: the sibling, the leftmost and rightmost grandchildren. The edge labeling is an integral part of the algorithm, which requires an additional loop over the labels. This algorithm has a complexity of O(n4 L). 851 Koo and Collins (2010) presented a parser that can evaluate factors of three edges. In this paper, we utilize the state-of-the-art parser of Bohnet (2010), a graph-based parser which employs online training with a perceptron which employs the MIRA update (Crammer and Singer, 2003). The parser contains a feature function for the first order factor, one for the sibling factor, and one for the grandchildren. We integrated in each of the functions features representing information on the phrase structure. 3.1 Features Defined on Phrase Structure Parses We explore new feature templates to include features from the phrase structures. These templates are defined on the phrase structures themselves which is an important difference compared to previous"
C12-1052,D07-1101,0,0.203635,"approach in the dependency parsing environment is still considerable in the presence of other extensions. • We give explanations why the proposed stacking approach works. 850 2 Related Work A number of studies have addressed feature-rich dependency and phrase structure parsing, cf. (Nivre et al., 2004; McDonald et al., 2005b; Charniak and Johnson, 2005; Huang, 2008). The two main approaches to dependency parsing are transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2003; Titov and Henderson, 2007), and graph-based dependency parsing (Eisner, 1996; McDonald et al., 2005a; Carreras, 2007; Rush et al., 2010). The most successful supervised phrase structure parsers are feature-rich discriminative parsers which heavily depend on an underlying PCFG (Charniak and Johnson, 2005; Huang, 2008). These approaches consist of two stages. At the first stage they apply a PCFG to extract possible parses, then at the second stage select the best parse from the set of possible parses (i.e. rerank this set) employing a large feature set (Collins, 2000; Charniak and Johnson, 2005). There is little related work on combining the two approaches. Some generative parsing approaches have exploited th"
C12-1052,W08-2102,0,0.417452,"ploit the difference of the representations of dependency and phrase structure parses and the divergence in the parsing techniques. We use features derived from the 1-best automatic phrase structure parse to augment the feature set of the dependency parser and vice versa. This way of combining parsers proved to be effective and provides a substantial accuracy gain for both parsing approaches. Our ultimate objective is to advance one parser by a second one. This motivation is different from previous approaches for combining phrase-structure and dependency parsers (cf. (Klein and Manning, 2003; Carreras et al., 2008; Rush et al., 2010)) which aimed to achieve a joint optimum of the two approaches. Our proposal has three advantages over previous work. (i) Our approach is simple to implement by defining new feature templates, yet very effective. (ii) It does not require a joint representation of different parse structures. (iii) The participating dependency and phrase-structure parsers can be easily replaced, and each can be developed and optimized independently. In our experiments both parsers utilize the same (training) data set – having two different representations – without having access to external i"
C12-1052,A00-2018,0,0.123613,"ish development set where the discriminative parsers with and without these features gave different output. We identified two categories of improvement. The first one is the “attachment” of adverbs and adjectives, especially in the case of collocational modifiers like more like and many more. The second phenomenon where the dependency parse-based features could eliminate a considerable amount of errors – much more than was introduced – is PP attachment. The explanation for both of these contributions might be the “lossless” lexicalization of the dependency parsers. Although the PCFG parser of Charniak (2000) is lexicalized8 and the re-ranking stage is deeply lexicalized, using only 50 candidate parses and a frequency-based feature pruning implies some extent of information loss while dependency parsers can exploit very rare lexical cues. 7.3 What is the difference between stacking directly and stacking after conversion? We also manually compared the parses of the two stacking approaches, i.e. the one where the features are defined directly on phrase structures versus the other where the features are defined on a dependency tree which were automatically converted from the phrase structures (see Se"
C12-1052,P05-1022,0,0.217984,"hest reported accuracy score on dependency parsing of the Penn Treebank. The phrase structure parser obtains 91.72 F-score with the features derived from the dependency trees, and this is also competitive with the best reported PARSEVAL scores for the Penn Treebank. KEYWORDS: parsing, stacking, dependency parsing, phrase structure parsing. Proceedings of COLING 2012: Technical Papers, pages 849–866, COLING 2012, Mumbai, December 2012. 849 1 Introduction Both phrase structure and dependency parsers have developed considerably in the last decade, cf. (Nivre et al., 2004; McDonald et al., 2005b; Charniak and Johnson, 2005; Huang, 2008). The development has taken rather different directions as phrase structure parsers and dependency parsers employ different techniques to parse sentences. Phrase structure parsers usually apply probabilistic context free grammars and focus on the relationships among phrases. Dependency parsers use edge factored models for parsing that primarily model the interaction between the a head word and a dependent word. Second order graph-based parsers consider in addition for the decision on a dependency edge the interaction with siblings and grandchildren. Phrase structure and dependenc"
C12-1052,J03-4003,0,0.063618,"age they apply a PCFG to extract possible parses, then at the second stage select the best parse from the set of possible parses (i.e. rerank this set) employing a large feature set (Collins, 2000; Charniak and Johnson, 2005). There is little related work on combining the two approaches. Some generative parsing approaches have exploited the differences between phrase structure and dependency parsers. For instance, Klein and Manning (2003) introduced an approach where the objective function is the product of the probabilities of a generative phrase structure and a dependency parser. Model 1 of Collins (2003) is based on the dependencies between pairs of head words. On the other hand, the related work on this topic for discriminative parsing is sparse, and we are only aware of the following works. Carreras et al. (2008) and Rush et al. (2010) introduced frameworks for joint learning of phrase and dependency structures, and showed improvements on both tasks for English. These frameworks require special formulation of – one or both – parsing approaches while our approach allows the usage of arbitrary dependency parsers and any feature-based phrase structure parser. Our motivation differs from these"
C12-1052,C96-1058,0,0.0793168,"nd vice versa. The added value of the approach in the dependency parsing environment is still considerable in the presence of other extensions. • We give explanations why the proposed stacking approach works. 850 2 Related Work A number of studies have addressed feature-rich dependency and phrase structure parsing, cf. (Nivre et al., 2004; McDonald et al., 2005b; Charniak and Johnson, 2005; Huang, 2008). The two main approaches to dependency parsing are transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2003; Titov and Henderson, 2007), and graph-based dependency parsing (Eisner, 1996; McDonald et al., 2005a; Carreras, 2007; Rush et al., 2010). The most successful supervised phrase structure parsers are feature-rich discriminative parsers which heavily depend on an underlying PCFG (Charniak and Johnson, 2005; Huang, 2008). These approaches consist of two stages. At the first stage they apply a PCFG to extract possible parses, then at the second stage select the best parse from the set of possible parses (i.e. rerank this set) employing a large feature set (Collins, 2000; Charniak and Johnson, 2005). There is little related work on combining the two approaches. Some generat"
C12-1052,W11-2924,1,0.883475,"of phrase and dependency structures, and showed improvements on both tasks for English. These frameworks require special formulation of – one or both – parsing approaches while our approach allows the usage of arbitrary dependency parsers and any feature-based phrase structure parser. Our motivation differs from these solutions as we focus on advancing one approach rather than achieving a joint optimum. Our approach can be regarded as a special stacking procedure (Nivre and McDonald, 2008), specifically, the stacking of a phrase structure parser with a dependency parser. In our previous work (Farkas et al., 2011), we reported results with a stacking approach for phrase structure parsing evaluated on a German corpus. We focus herein on the reverse direction as well, i.e. we define features for dependency parsers, we report performance outstanding scores on English for both directions, compare to the state-of-the-art results and carried out a detailed analysis of the stacking’s contributions. Wang and Zong (2010) introduced a procedure that exploits dependency parses to improve a phrase structure parser. They used automatic dependency parses for pruning the chart of a phrase structure parser and reporte"
C12-1052,W09-1205,0,0.0237672,"09 This work (G+P+T+C) English (UAS) Devel. Test 92.5 93.5 93.04 93.26 92.14 94.04 91.85 92.96 93.16 93.8 94.14 GHPT’09 B’10 German (LAS) Devel. Test 87.28 88.06 88.54 89.69 88.50 89.90 Table 4: Comparison to previous work. We report UAS only for the English Penn Treebank since usually unlabeled scores were published and labeled attachments scores only for the German Tiger Treebank since in the CoNLL shared task 2009 only labeled attachment scores were reported. For German the two top scoring parsers of the CoNLL 2009 shared task on Syntactic and Semantic Dependency parsing were the parser of Gesmundo et al. (2009), who had the best overall scores and the parser of Bohnet (2010), which performed best on English and German. However we note that directly comparing these results to our ones is not fully fair since the POS tagging accuracy of the data sets were lower than 95.5 at the CoNLL 2009 shared task. Table 5 sketches the phrase-structure parsing results in the context of previously published results. We retrained the current version of the Charniak and Johnson (2005) parser5 (C&J) for English. We also cite the scores reported by Rush et al. (2010) who also exploited phrase structure and dependency pa"
C12-1052,W09-1201,0,0.0305944,"Missing"
C12-1052,P08-1067,0,0.546645,"on dependency parsing of the Penn Treebank. The phrase structure parser obtains 91.72 F-score with the features derived from the dependency trees, and this is also competitive with the best reported PARSEVAL scores for the Penn Treebank. KEYWORDS: parsing, stacking, dependency parsing, phrase structure parsing. Proceedings of COLING 2012: Technical Papers, pages 849–866, COLING 2012, Mumbai, December 2012. 849 1 Introduction Both phrase structure and dependency parsers have developed considerably in the last decade, cf. (Nivre et al., 2004; McDonald et al., 2005b; Charniak and Johnson, 2005; Huang, 2008). The development has taken rather different directions as phrase structure parsers and dependency parsers employ different techniques to parse sentences. Phrase structure parsers usually apply probabilistic context free grammars and focus on the relationships among phrases. Dependency parsers use edge factored models for parsing that primarily model the interaction between the a head word and a dependent word. Second order graph-based parsers consider in addition for the decision on a dependency edge the interaction with siblings and grandchildren. Phrase structure and dependency parsers have"
C12-1052,P08-1068,0,0.0440803,"of 0.89 and 0.54 employing them as an extension of the graph-based and the stacked dependency parser G+T. The added value of the features from the transition-based dependency parses on top of the extended feature set (i.e. the difference between G+P and G+P+T) is minor – and there is a decrease at 2 out of the 8 settings. These results indicate that the features gathered from the phrase structure parse contain almost all of the information which can be gathered from the transition-based dependency parse. The two last rows show scores when we use in addition cluster-based features similarly to Koo et al. (2008). The cluster-based feature provided an improvement of 0.17 on top of the results of the graph-based parser (G+C features). The last row (G+P+T+C features) shows the scores for the features from all sources where we finally obtain 94.14 UAS. Similarly to the results for English, we can also report large improvements for the German Tiger Treebank. With the G+T features, we gain 0.24 UAS and 0.71 with the “G+P features” compared with the “baseline G”. When we utilized each feature set, the parser achieved 91.88 UAS and 89.90 LAS (0.81 improvements in UAS), which is the highest reported accuracy"
C12-1052,P10-1001,0,0.0986425,"apted to a transition-based parser. Graph-based dependency parsers decompose the dependency structure into factors. Each factor of the first order graph-based parser corresponds to a dependency edge. McDonald et al. (2005a) first used second order factors which incorporates siblings of the head and dependent. The second order algorithm of Carreras (2007) uses three second order factors: the sibling, the leftmost and rightmost grandchildren. The edge labeling is an integral part of the algorithm, which requires an additional loop over the labels. This algorithm has a complexity of O(n4 L). 851 Koo and Collins (2010) presented a parser that can evaluate factors of three edges. In this paper, we utilize the state-of-the-art parser of Bohnet (2010), a graph-based parser which employs online training with a perceptron which employs the MIRA update (Crammer and Singer, 2003). The parser contains a feature function for the first order factor, one for the sibling factor, and one for the grandchildren. We integrated in each of the functions features representing information on the phrase structure. 3.1 Features Defined on Phrase Structure Parses We explore new feature templates to include features from the phras"
C12-1052,D10-1004,0,0.102696,"Missing"
C12-1052,P05-1012,0,0.798663,"95 UAS, which is the highest reported accuracy score on dependency parsing of the Penn Treebank. The phrase structure parser obtains 91.72 F-score with the features derived from the dependency trees, and this is also competitive with the best reported PARSEVAL scores for the Penn Treebank. KEYWORDS: parsing, stacking, dependency parsing, phrase structure parsing. Proceedings of COLING 2012: Technical Papers, pages 849–866, COLING 2012, Mumbai, December 2012. 849 1 Introduction Both phrase structure and dependency parsers have developed considerably in the last decade, cf. (Nivre et al., 2004; McDonald et al., 2005b; Charniak and Johnson, 2005; Huang, 2008). The development has taken rather different directions as phrase structure parsers and dependency parsers employ different techniques to parse sentences. Phrase structure parsers usually apply probabilistic context free grammars and focus on the relationships among phrases. Dependency parsers use edge factored models for parsing that primarily model the interaction between the a head word and a dependent word. Second order graph-based parsers consider in addition for the decision on a dependency edge the interaction with siblings and grandchildren. P"
C12-1052,H05-1066,0,0.847248,"95 UAS, which is the highest reported accuracy score on dependency parsing of the Penn Treebank. The phrase structure parser obtains 91.72 F-score with the features derived from the dependency trees, and this is also competitive with the best reported PARSEVAL scores for the Penn Treebank. KEYWORDS: parsing, stacking, dependency parsing, phrase structure parsing. Proceedings of COLING 2012: Technical Papers, pages 849–866, COLING 2012, Mumbai, December 2012. 849 1 Introduction Both phrase structure and dependency parsers have developed considerably in the last decade, cf. (Nivre et al., 2004; McDonald et al., 2005b; Charniak and Johnson, 2005; Huang, 2008). The development has taken rather different directions as phrase structure parsers and dependency parsers employ different techniques to parse sentences. Phrase structure parsers usually apply probabilistic context free grammars and focus on the relationships among phrases. Dependency parsers use edge factored models for parsing that primarily model the interaction between the a head word and a dependent word. Second order graph-based parsers consider in addition for the decision on a dependency edge the interaction with siblings and grandchildren. P"
C12-1052,P07-1122,0,0.0168907,"introduces 3 attachment errors (for Therese, for the comma and for Quebec). Besides noun phrases the parses of institutionalized phrases like with or without and rather than to were also considerable improved. These improvements can be probably attributed to the phrase-based thinking of the second parser. Phrase structure-based features improved considerably the accuracy of coordinations. It is a known issue that the representation of coordination in the dependency structure has an impact on the accuracy. The Penn2Malt converter builds dependency structures where the conjunction is the head. Nilsson et al. (2007) showed that this representation of coordinations yields a lower parsing accuracy – across four languages – than a representation where the conjunction and the members of the coordination form one chain. Our features defined on the phrase structure trees can be regarded as a third type of representation of coordination. Although these features help in the current setting we expect a smaller contribution if a different dependency representation (like the chain type) is targeted. The counting of the errors – or error fixes – related to coordination is not straightforward because besides the wron"
C12-1052,W03-3017,0,0.115821,"Treebank for stacking a phrase structure parser on and a dependency parser and vice versa. The added value of the approach in the dependency parsing environment is still considerable in the presence of other extensions. • We give explanations why the proposed stacking approach works. 850 2 Related Work A number of studies have addressed feature-rich dependency and phrase structure parsing, cf. (Nivre et al., 2004; McDonald et al., 2005b; Charniak and Johnson, 2005; Huang, 2008). The two main approaches to dependency parsing are transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2003; Titov and Henderson, 2007), and graph-based dependency parsing (Eisner, 1996; McDonald et al., 2005a; Carreras, 2007; Rush et al., 2010). The most successful supervised phrase structure parsers are feature-rich discriminative parsers which heavily depend on an underlying PCFG (Charniak and Johnson, 2005; Huang, 2008). These approaches consist of two stages. At the first stage they apply a PCFG to extract possible parses, then at the second stage select the best parse from the set of possible parses (i.e. rerank this set) employing a large feature set (Collins, 2000; Charniak and Johnson, 200"
C12-1052,W04-2407,0,0.177788,"parser and reach 93.95 UAS, which is the highest reported accuracy score on dependency parsing of the Penn Treebank. The phrase structure parser obtains 91.72 F-score with the features derived from the dependency trees, and this is also competitive with the best reported PARSEVAL scores for the Penn Treebank. KEYWORDS: parsing, stacking, dependency parsing, phrase structure parsing. Proceedings of COLING 2012: Technical Papers, pages 849–866, COLING 2012, Mumbai, December 2012. 849 1 Introduction Both phrase structure and dependency parsers have developed considerably in the last decade, cf. (Nivre et al., 2004; McDonald et al., 2005b; Charniak and Johnson, 2005; Huang, 2008). The development has taken rather different directions as phrase structure parsers and dependency parsers employ different techniques to parse sentences. Phrase structure parsers usually apply probabilistic context free grammars and focus on the relationships among phrases. Dependency parsers use edge factored models for parsing that primarily model the interaction between the a head word and a dependent word. Second order graph-based parsers consider in addition for the decision on a dependency edge the interaction with siblin"
C12-1052,P08-1108,0,0.407715,"cture parsers usually apply probabilistic context free grammars and focus on the relationships among phrases. Dependency parsers use edge factored models for parsing that primarily model the interaction between the a head word and a dependent word. Second order graph-based parsers consider in addition for the decision on a dependency edge the interaction with siblings and grandchildren. Phrase structure and dependency parsers have both shown to be efficient and to provide accurate parsing results. Different parsing approaches have different strengths on distinct linguistic constructions, cf. (Nivre and McDonald, 2008). In this paper, we exploit the difference of the representations of dependency and phrase structure parses and the divergence in the parsing techniques. We use features derived from the 1-best automatic phrase structure parse to augment the feature set of the dependency parser and vice versa. This way of combining parsers proved to be effective and provides a substantial accuracy gain for both parsing approaches. Our ultimate objective is to advance one parser by a second one. This motivation is different from previous approaches for combining phrase-structure and dependency parsers (cf. (Kle"
C12-1052,P06-1055,0,0.076953,"al scores are competitive with the best reported supervised PARSEVAL score in the Penn Treebank by (Huang, 2008) who also extended the feature set of Charniak and Johnson (2005) and applied forest-based reranking. C&J Rush’10 Huang’08 English Devel. Test 90.60 91.36 – 90.7 – 91.69 This work (bl+dep) 90.92 Berkeley PyNLP 91.72 German Devel. Test 76.60 (65.25) 76.05 (64.00) 76.76 (66.29) 76.27 (65.21) 80.90 (70.39) 79.45 (68.73) Table 5: Comparison to previous work. Our method significantly outperforms state-of-the-art systems. For German, we trained the current version of the Berkeley parser6 (Petrov et al., 2006) and PyNLP7 parsers (Versley and Rehbein, 2009). Note that the results of the generative Berkeley parser is competitive with the PyNLP parser which utilizes reranking, while the first-stage parser, which we employ here – using a fine-tuned German-specific tree annotation schema – outperforms both of them. The discriminative approach with the baseline feature set we employed gave an improvement of 2 percentage points over the first-stage parser and the features gathered from dependency parsers could add an additional 1 point to this. 5 6 7 http://www.cog.brown.edu/ mj/Software.htm http://code.g"
C12-1052,D10-1001,0,0.403278,"the representations of dependency and phrase structure parses and the divergence in the parsing techniques. We use features derived from the 1-best automatic phrase structure parse to augment the feature set of the dependency parser and vice versa. This way of combining parsers proved to be effective and provides a substantial accuracy gain for both parsing approaches. Our ultimate objective is to advance one parser by a second one. This motivation is different from previous approaches for combining phrase-structure and dependency parsers (cf. (Klein and Manning, 2003; Carreras et al., 2008; Rush et al., 2010)) which aimed to achieve a joint optimum of the two approaches. Our proposal has three advantages over previous work. (i) Our approach is simple to implement by defining new feature templates, yet very effective. (ii) It does not require a joint representation of different parse structures. (iii) The participating dependency and phrase-structure parsers can be easily replaced, and each can be developed and optimized independently. In our experiments both parsers utilize the same (training) data set – having two different representations – without having access to external information. Thus, ou"
C12-1052,C04-1024,0,0.0394137,"h-based parser of the dependency parsers of Bohnet (2011)1 . The second-order parser employs online training with a perceptron (Crammer and Singer, 2003) and it is based on a Hash Kernel. We employed this parser since it is quick to train, provides useful labeled dependency trees and achieves state-of-the-art accuracies. We used the transition-based parser that is included in the same package for our experiments as well. For the phrase structure parsing experiments, we also employed state-of-the-art parsers. We used the first-stage parser of Charniak and Johnson (2005) for English and Bitpar (Schmid, 2004) for German. The latter employs a grammar engineered for German (Farkas et al., 2011). At the second stage, we used the 50 and 100-best parses to rerank and filtered out rare features (which occurred in less than 5 sentences). We employed the ranking MaxEnt implementation of the MALLET package (McCallum, 2002) and optimized the L2 regularizer coefficient on the development set. 5.3 Baseline Feature Sets For conducting baseline dependency parsing experiments, we employed the feature set of Bohnet (2010). For the stacking of graph-based and transition-based dependency parsers, we used the featur"
C12-1052,P08-1076,0,0.0598484,"Missing"
C12-1052,D09-1058,0,0.078152,"Missing"
C12-1052,W07-2218,0,0.020652,"stacking a phrase structure parser on and a dependency parser and vice versa. The added value of the approach in the dependency parsing environment is still considerable in the presence of other extensions. • We give explanations why the proposed stacking approach works. 850 2 Related Work A number of studies have addressed feature-rich dependency and phrase structure parsing, cf. (Nivre et al., 2004; McDonald et al., 2005b; Charniak and Johnson, 2005; Huang, 2008). The two main approaches to dependency parsing are transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2003; Titov and Henderson, 2007), and graph-based dependency parsing (Eisner, 1996; McDonald et al., 2005a; Carreras, 2007; Rush et al., 2010). The most successful supervised phrase structure parsers are feature-rich discriminative parsers which heavily depend on an underlying PCFG (Charniak and Johnson, 2005; Huang, 2008). These approaches consist of two stages. At the first stage they apply a PCFG to extract possible parses, then at the second stage select the best parse from the set of possible parses (i.e. rerank this set) employing a large feature set (Collins, 2000; Charniak and Johnson, 2005). There is little related"
C12-1052,D08-1017,0,0.0200495,"50 and 100-best parses to rerank and filtered out rare features (which occurred in less than 5 sentences). We employed the ranking MaxEnt implementation of the MALLET package (McCallum, 2002) and optimized the L2 regularizer coefficient on the development set. 5.3 Baseline Feature Sets For conducting baseline dependency parsing experiments, we employed the feature set of Bohnet (2010). For the stacking of graph-based and transition-based dependency parsers, we used the feature template definitions from Nivre and McDonald (2008) which was extended by grandchildren factors (similarly to Torres Martins et al. (2008)). For phrase structure reranking we utilized the features of Charniak and Johnson (2005) and for German we reimplemented the feature templates of Versley and Rehbein (2009) as baseline feature sets. The latter is the state-of-the-art feature set for German, which consists of features constructed from the lexicalized parse tree and its typed dependencies along with features based on external statistical information (like the clustering of unknown words according to their context of occurrences and PP attachment statistics gathered from the automatic POS tagged DE-WaC corpus, a 1.7G words sampl"
C12-1052,N03-1033,0,0.0207322,"task (Kübler, 2008) for phrase structure parsing. For dependency parsing, we used the dependency corpora of the CoNLL-2009 shared task, cf. (Hajiˇc et al., 2009) (which consists of automatically converted trees from a part of the Tiger Treebank). The phrase structure parsers traditionally conduct POS tagging during parsing (the different POS alternatives are on the chart). For obtaining POS tags for the dependency parsing experiments, we jackknifed the training data 10 fold, i.e. we trained the POS tagger on nine folds and tagged the tenth fold. On the English training set, the POS tagger of Toutanova et al. (2003) achieves an accuracy of 97.1, on the development set, 97.2 and the test set, 97.3. For German, we used a SVM-based tagger and we obtained on the training set an accuracy of 97.2, on the development set, 97.5 and on the test set, 97.4. We followed the same jackknifing procedure in the stacking experiments in order to obtain the parses from which the features are extracted. We generated the dependency and constituent parses of the training corpus by training the respective parser on nine folds and parsing the tenth fold. During parsing the test set and development set we utilized the parses fro"
C12-1052,W09-3820,0,0.222646,"e–all), VP-PP, VP-PP-LOC (from invite–to). We also investigated the role of case and grammatical functions of the phrase structure parser in German and extended the POSRel and ConstRel feature sets by adding this information to the labels. Note that the value of outArc is 1 iff the word span in question has a dominating dependency subtree in the automatic parse. Wang and Zong (2010) prune hyperedges with outArc= 6 1 thus this feature can be regarded as a generalization of their approach. These features are similar in nature to the lexical head-based features of Charniak and Johnson (2005) and Versley and Rehbein (2009) but their role is different. Those dependency-like features try to describe the local hyperedges using the internal state of the parsing approach while we exploit here a globally (i.e. sentence-level) optimized dependency parse tree which represents external knowledge to the parser. 5 5.1 Experimental Setting Data Sets We used the Penn Treebank (section 23 served as test set and section 22 and 24 were used as development set in the phrase-structure and dependency experiments, respectively) with the Penn2Malt converter, and the head-finding rules of Yamada and Matsumoto (2003) for our experime"
C12-1052,C10-2148,0,0.217194,"Our approach can be regarded as a special stacking procedure (Nivre and McDonald, 2008), specifically, the stacking of a phrase structure parser with a dependency parser. In our previous work (Farkas et al., 2011), we reported results with a stacking approach for phrase structure parsing evaluated on a German corpus. We focus herein on the reverse direction as well, i.e. we define features for dependency parsers, we report performance outstanding scores on English for both directions, compare to the state-of-the-art results and carried out a detailed analysis of the stacking’s contributions. Wang and Zong (2010) introduced a procedure that exploits dependency parses to improve a phrase structure parser. They used automatic dependency parses for pruning the chart of a phrase structure parser and reported a significant improvement. One of our feature templates for the phrase structure parser can be regarded as a generalization of this approach. 3 Dependency Parser Exploiting Phrase Structure Parses In this study we focus on graph-based parsers, however, our features from the phrase structures can also be adapted to a transition-based parser. Graph-based dependency parsers decompose the dependency struc"
C12-1052,I11-1140,0,0.0629179,"forest (chart) (Huang, 2008). The task of the second stage is to select the best parse from the set of possible parses (i.e. rerank this set). These methods employ a large feature set (usually a few millions features) (Collins, 2000; Charniak and Johnson, 2005). The n-best list approaches can straightforwardly employ local and non-local features as well because they decide at the sentence-level (Charniak and Johnson, 2005) while involving non-local features is more complicated in the forest-based approaches and studies show only a minor empirical advantage of the latter approach (Huang, 2008; Wang and Zong, 2011). In this study, we experiment with n-best list reranking using a Maximum Entropy machine learning model and our goal is to investigate the extension of the standard feature set of these models by features extracted from the automatic dependency parse of the sentence in question. 4.1 Features Defined on Dependency Parses The objective of the features defined for phrase structure parsing is to characterize the divergence/similarity of constituents with the corresponding part of the automatic (1-best) dependency parse of the sentence in question. We defined three feature templates for representi"
C12-1052,W03-3023,0,0.727028,"bank and on the German Tiger Treebank for stacking a phrase structure parser on and a dependency parser and vice versa. The added value of the approach in the dependency parsing environment is still considerable in the presence of other extensions. • We give explanations why the proposed stacking approach works. 850 2 Related Work A number of studies have addressed feature-rich dependency and phrase structure parsing, cf. (Nivre et al., 2004; McDonald et al., 2005b; Charniak and Johnson, 2005; Huang, 2008). The two main approaches to dependency parsing are transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2003; Titov and Henderson, 2007), and graph-based dependency parsing (Eisner, 1996; McDonald et al., 2005a; Carreras, 2007; Rush et al., 2010). The most successful supervised phrase structure parsers are feature-rich discriminative parsers which heavily depend on an underlying PCFG (Charniak and Johnson, 2005; Huang, 2008). These approaches consist of two stages. At the first stage they apply a PCFG to extract possible parses, then at the second stage select the best parse from the set of possible parses (i.e. rerank this set) employing a large feature set (Collins, 2000; Charniak and"
C12-1052,W08-1008,0,\N,Missing
C12-2105,P04-1082,0,0.203759,"rve the parallelism. The German example on the bottom shows a coordination of two sentences that share the finite and the passive auxiliary with each other, both represented as a phonetically empty head in the structure. By introducing empty nodes into the annotation, the parallelism in the underlying syntactic structure of the two conjuncts is preserved. We would like to stress that the problem of empty heads in dependency syntax is rather different from the problem of introducing trace elements previously addressed by work on the English Penn Treebank (Johnson, 2002; Dienes and Dubey, 2003; Campbell, 2004). The PTB encodes a lot of different elements that do not show on the surface, but most of these would be leaf nodes in dependency representation.1 1 There is a small number of cases, where the PTB annotates a missing verb (marked as *?*, see Section 4.6 in Bies et al. (1995)). We found 581 instances of those in the whole corpus, 293 of which were dominated by a VP node. Only those empty elements correspond to empty heads in a dependency representation since they would normally have dependents on their own. But in contrast to dependency formalisms, it is not a problem to annotate a head-less p"
C12-2105,W11-0417,0,0.208415,"ian. We conclude with an error analysis and a discussion of the results. 2 Related Work We are aware of two previous papers where the issue of empty heads has been addressed in the context of dependency parsing: One is Dukes and Habash (2011) who present a parser for the Quranic Arabic Dependency Treebank, which also contains empty heads. Unfortunately, they do not evaluate or discuss the role of empty heads. Their solution of the problem – introducing a new transition into a transition-based parser – is similar to one of our proposed procedures (the in-parsing approach). The other work is by Chaitanya et al. (2011), who use hand-crafted rules to recover empty nodes from the output of a rule-based dependency parser for the Hindi Dependency Treebank. They achieve good results on some phenomena and a bit lower results on others, proving that it is indeed possible to treat this problem in syntactic processing. However, given that their data base is very small, and they used a rule-based, language-specific approach, the question remains if we can use statistical learning to address this problem. 3 Approaches for Parsing with Empty Heads The parser that we use for our experiment is basically a best-first pars"
C12-2105,P03-1055,0,0.710356,"ead can be used to preserve the parallelism. The German example on the bottom shows a coordination of two sentences that share the finite and the passive auxiliary with each other, both represented as a phonetically empty head in the structure. By introducing empty nodes into the annotation, the parallelism in the underlying syntactic structure of the two conjuncts is preserved. We would like to stress that the problem of empty heads in dependency syntax is rather different from the problem of introducing trace elements previously addressed by work on the English Penn Treebank (Johnson, 2002; Dienes and Dubey, 2003; Campbell, 2004). The PTB encodes a lot of different elements that do not show on the surface, but most of these would be leaf nodes in dependency representation.1 1 There is a small number of cases, where the PTB annotates a missing verb (marked as *?*, see Section 4.6 in Bies et al. (1995)). We found 581 instances of those in the whole corpus, 293 of which were dominated by a VP node. Only those empty elements correspond to empty heads in a dependency representation since they would normally have dependents on their own. But in contrast to dependency formalisms, it is not a problem to annot"
C12-2105,W11-2912,0,0.42051,"oach where the presence of empty heads is determined by a classifier run prior to parsing. The paper is structured as follows: we first review some related work and continue with the presentation of the three different methods. We then define the metric that we use to measure the quality of the empty head prediction and use it to evaluate parsing experiments on German and Hungarian. We conclude with an error analysis and a discussion of the results. 2 Related Work We are aware of two previous papers where the issue of empty heads has been addressed in the context of dependency parsing: One is Dukes and Habash (2011) who present a parser for the Quranic Arabic Dependency Treebank, which also contains empty heads. Unfortunately, they do not evaluate or discuss the role of empty heads. Their solution of the problem – introducing a new transition into a transition-based parser – is similar to one of our proposed procedures (the in-parsing approach). The other work is by Chaitanya et al. (2011), who use hand-crafted rules to recover empty nodes from the output of a rule-based dependency parser for the Hindi Dependency Treebank. They achieve good results on some phenomena and a bit lower results on others, pro"
C12-2105,N10-1115,0,0.0348517,"recover empty nodes from the output of a rule-based dependency parser for the Hindi Dependency Treebank. They achieve good results on some phenomena and a bit lower results on others, proving that it is indeed possible to treat this problem in syntactic processing. However, given that their data base is very small, and they used a rule-based, language-specific approach, the question remains if we can use statistical learning to address this problem. 3 Approaches for Parsing with Empty Heads The parser that we use for our experiment is basically a best-first parser like the ones described in (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011), which is trained with the Guided Learning technique proposed in Shen et al. (2007) embedded in a MIRA framework (Crammer et al., 2003). The best-first parsing approach has the advantage that it is easy to modify in order to allow for the introduction of empty heads, while for graph-based parsers (McDonald et al., 2005) it is not even clear how to do it. The approach is also more suitable here than the standard transition-based approach (Nivre et al., 2004), since it can build context on both sides of the current attachment site while it achieves competitive results. In"
C12-2105,P02-1018,0,0.449938,"ces, an empty head can be used to preserve the parallelism. The German example on the bottom shows a coordination of two sentences that share the finite and the passive auxiliary with each other, both represented as a phonetically empty head in the structure. By introducing empty nodes into the annotation, the parallelism in the underlying syntactic structure of the two conjuncts is preserved. We would like to stress that the problem of empty heads in dependency syntax is rather different from the problem of introducing trace elements previously addressed by work on the English Penn Treebank (Johnson, 2002; Dienes and Dubey, 2003; Campbell, 2004). The PTB encodes a lot of different elements that do not show on the surface, but most of these would be leaf nodes in dependency representation.1 1 There is a small number of cases, where the PTB annotates a missing verb (marked as *?*, see Section 4.6 in Bies et al. (1995)). We found 581 instances of those in the whole corpus, 293 of which were dominated by a VP node. Only those empty elements correspond to empty heads in a dependency representation since they would normally have dependents on their own. But in contrast to dependency formalisms, it i"
C12-2105,P06-2066,0,0.0283657,"resort to a swap operation as is done in Tratz and Hovy (2011). It also increases theoretical decoding complexity to O(n2 ). However, there are non-projective structures that cannot be produced by this approach.2 To allow the derivation of these structures, we reintroduce the swap operation from the parser in Tratz and Hovy (2011), but during training, the parser is only allowed to apply the swap operation in case of an ill-nested structure, which leads to a very small number of swaps. 2 These structures do not fulfill the well-nestedness condition that is described in Bodirsky et al. (2005); Kuhlmann and Nivre (2006) and appear for example in German centerfield scrambling structures. 1083 The feature set of the parser uses the word forms, lemmata, POS tags, and already predicted dependency labels for the head and its prospective dependent, as well as combinations thereof for up to three surrounding tokens in the sentence. The same features and combinations are extracted for up to three surrounding partially built structures. We also add features for the left-most and right-most dependent of a token, the labels of the dependents, distance features, and valency features as proposed by Zhang and Nivre (2011)"
C12-2105,P05-1012,0,0.0104219,"guage-specific approach, the question remains if we can use statistical learning to address this problem. 3 Approaches for Parsing with Empty Heads The parser that we use for our experiment is basically a best-first parser like the ones described in (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011), which is trained with the Guided Learning technique proposed in Shen et al. (2007) embedded in a MIRA framework (Crammer et al., 2003). The best-first parsing approach has the advantage that it is easy to modify in order to allow for the introduction of empty heads, while for graph-based parsers (McDonald et al., 2005) it is not even clear how to do it. The approach is also more suitable here than the standard transition-based approach (Nivre et al., 2004), since it can build context on both sides of the current attachment site while it achieves competitive results. In contrast to the best-first parser in Goldberg and Elhadad (2010), the decoding algorithm is modified so that it works like the LTAG dependency parser described in Shen and Joshi (2008), which allows an edge to attach to an inside node of an already built structure. This difference makes it possible to directly produce a large portion of non-p"
C12-2105,W04-2407,0,0.0203665,"eads The parser that we use for our experiment is basically a best-first parser like the ones described in (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011), which is trained with the Guided Learning technique proposed in Shen et al. (2007) embedded in a MIRA framework (Crammer et al., 2003). The best-first parsing approach has the advantage that it is easy to modify in order to allow for the introduction of empty heads, while for graph-based parsers (McDonald et al., 2005) it is not even clear how to do it. The approach is also more suitable here than the standard transition-based approach (Nivre et al., 2004), since it can build context on both sides of the current attachment site while it achieves competitive results. In contrast to the best-first parser in Goldberg and Elhadad (2010), the decoding algorithm is modified so that it works like the LTAG dependency parser described in Shen and Joshi (2008), which allows an edge to attach to an inside node of an already built structure. This difference makes it possible to directly produce a large portion of non-projective structures (e. g. sentence extraposition or WH-extraction) without having to resort to a swap operation as is done in Tratz and Ho"
C12-2105,W01-0708,0,0.0254884,"before to-infinitive constructions) whereas our empty heads can occur more freely due to the free word order of German and Hungarian. We therefore pursue here a clause-based empty head preinsertion procedure since we think that the decision about inserting an empty head (which is basically the identification of the absence of the verb) can be made on the clause-level. For this, we implemented a clause boundary identification module and a classifier that predicts whether an empty word form should be inserted into a particular clause. Clause boundary identification is a difficult problem (cf. (Sang and Déjean, 2001)) as clauses usually form a hierarchy – and this hierarchy is important for predicting the insertion of empty heads. Our clause boundary detector achieves f-scores of 92.6 and 86.8 on the German and Hungarian development datasets respectively. These results are in line with the state-of-the-art results on the English Penn Treebank (Carreras et al., 2005; Ram and Lalitha Devi, 2008). If we evaluate only the in-sentence clauses, we get f-scores of 85.4 and 78.2 for German and Hungarian respectively. In order to decide whether to insert an empty head, we implemented a classifier that decides for"
C12-2105,seeker-kuhn-2012-making,1,0.832743,"e of the verb in the verb-second word order of German). For Hungarian, the manual annotation of the position of the empty word forms is quite irregular and we insert them at the beginning of the clause. Finally, we train the best-first parser on the original training dataset containing the gold standard empty heads and use it to parse the sentences that contain the automatically inserted empty heads from the preinserter. 4 Experiments In order to test the parsing methods, we performed two experiments each: we trained the parser on the German TiGer corpus using the dependency representation by Seeker and Kuhn (2012) , and on the Szeged Dependency Treebank of Hungarian (Vincze et al., 2010), both of which data sets explicitly represent empty heads in the dependency trees. Table 1 shows the data sizes and the splits we used for the experiment. The German data was preprocessed (lemma, POS, morphology) with the mate-tools,4 the Hungarian data comes with automatic annotation.5 data set German Hungarian # sentences 50,474 81,960 training # sents # empty 36,000 2,618 61,034 14,850 development # sents # empty 2,000 117 11,688 2,536 # sents 10,472 9,238 test # empty 722 2,106 Table 1: Data sets 4.1 Evaluation Met"
C12-2105,D08-1052,0,0.0558341,"best-first parsing approach has the advantage that it is easy to modify in order to allow for the introduction of empty heads, while for graph-based parsers (McDonald et al., 2005) it is not even clear how to do it. The approach is also more suitable here than the standard transition-based approach (Nivre et al., 2004), since it can build context on both sides of the current attachment site while it achieves competitive results. In contrast to the best-first parser in Goldberg and Elhadad (2010), the decoding algorithm is modified so that it works like the LTAG dependency parser described in Shen and Joshi (2008), which allows an edge to attach to an inside node of an already built structure. This difference makes it possible to directly produce a large portion of non-projective structures (e. g. sentence extraposition or WH-extraction) without having to resort to a swap operation as is done in Tratz and Hovy (2011). It also increases theoretical decoding complexity to O(n2 ). However, there are non-projective structures that cannot be produced by this approach.2 To allow the derivation of these structures, we reintroduce the swap operation from the parser in Tratz and Hovy (2011), but during training"
C12-2105,P07-1096,0,0.0170372,"eve good results on some phenomena and a bit lower results on others, proving that it is indeed possible to treat this problem in syntactic processing. However, given that their data base is very small, and they used a rule-based, language-specific approach, the question remains if we can use statistical learning to address this problem. 3 Approaches for Parsing with Empty Heads The parser that we use for our experiment is basically a best-first parser like the ones described in (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011), which is trained with the Guided Learning technique proposed in Shen et al. (2007) embedded in a MIRA framework (Crammer et al., 2003). The best-first parsing approach has the advantage that it is easy to modify in order to allow for the introduction of empty heads, while for graph-based parsers (McDonald et al., 2005) it is not even clear how to do it. The approach is also more suitable here than the standard transition-based approach (Nivre et al., 2004), since it can build context on both sides of the current attachment site while it achieves competitive results. In contrast to the best-first parser in Goldberg and Elhadad (2010), the decoding algorithm is modified so th"
C12-2105,D11-1116,0,0.134076,"e output of a rule-based dependency parser for the Hindi Dependency Treebank. They achieve good results on some phenomena and a bit lower results on others, proving that it is indeed possible to treat this problem in syntactic processing. However, given that their data base is very small, and they used a rule-based, language-specific approach, the question remains if we can use statistical learning to address this problem. 3 Approaches for Parsing with Empty Heads The parser that we use for our experiment is basically a best-first parser like the ones described in (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011), which is trained with the Guided Learning technique proposed in Shen et al. (2007) embedded in a MIRA framework (Crammer et al., 2003). The best-first parsing approach has the advantage that it is easy to modify in order to allow for the introduction of empty heads, while for graph-based parsers (McDonald et al., 2005) it is not even clear how to do it. The approach is also more suitable here than the standard transition-based approach (Nivre et al., 2004), since it can build context on both sides of the current attachment site while it achieves competitive results. In contrast to the best-f"
C12-2105,vincze-etal-2010-hungarian,0,0.105132,"nual annotation of the position of the empty word forms is quite irregular and we insert them at the beginning of the clause. Finally, we train the best-first parser on the original training dataset containing the gold standard empty heads and use it to parse the sentences that contain the automatically inserted empty heads from the preinserter. 4 Experiments In order to test the parsing methods, we performed two experiments each: we trained the parser on the German TiGer corpus using the dependency representation by Seeker and Kuhn (2012) , and on the Szeged Dependency Treebank of Hungarian (Vincze et al., 2010), both of which data sets explicitly represent empty heads in the dependency trees. Table 1 shows the data sizes and the splits we used for the experiment. The German data was preprocessed (lemma, POS, morphology) with the mate-tools,4 the Hungarian data comes with automatic annotation.5 data set German Hungarian # sentences 50,474 81,960 training # sents # empty 36,000 2,618 61,034 14,850 development # sents # empty 2,000 117 11,688 2,536 # sents 10,472 9,238 test # empty 722 2,106 Table 1: Data sets 4.1 Evaluation Method Since the number of edges in the gold standard does not always equal th"
C12-2105,P11-2033,0,0.0210148,"lmann and Nivre (2006) and appear for example in German centerfield scrambling structures. 1083 The feature set of the parser uses the word forms, lemmata, POS tags, and already predicted dependency labels for the head and its prospective dependent, as well as combinations thereof for up to three surrounding tokens in the sentence. The same features and combinations are extracted for up to three surrounding partially built structures. We also add features for the left-most and right-most dependent of a token, the labels of the dependents, distance features, and valency features as proposed by Zhang and Nivre (2011) but adapted to the best-first decoder. For internal feature representation and combination the parser implements the hash kernel method by Bohnet (2010). 3.1 Empty Head Introduction during Parsing For the first method, we change the parser so that it can decide for an empty head during the parsing itself. To the three moves that the standard parser can perform – attach_left(label), attach_right(label), and swap – we add a fourth move (see Figure 2), that allows the parser to introduce an empty head for a particular dependent (together with a dependency label). This is similar in spirit to the"
C14-1076,P11-2123,0,0.0146111,"te of a second order factor is composed of properties drawn from up to all three vertex, e.g., the part-of-speech of the head, the dependent and a child denoted as POS ( H )+ POS ( D )+ POS ( C ). In our experiments, we use in addition to the transition-based model, a completion model that uses graph-based feature templates with up to third order factors to re-score the beam. 2.4 Feature Selection There has been some recent research on trying to manually find better feature models for dependency parsers, such as Nivre et al. (2006), Hall et al. (2007), Hall (2008), Zhang and Nivre (2011), and Agirre et al. (2011). There is also research on automatic feature selection in the case of transition-based dependency parsing, a good example is MaltOptimizer (Ballesteros and Nivre, 2014) which implements a search for the best feature model that it can find, following acquired previous experience and deep linguistic knowledge (Hall et al., 2007; Nivre and Hall, 2010); Nilsson and Nugues (2010) also tried to search for optimal feature sets in the case of transition-based parsing, starting from a reduced test set using the concept of topological neighbors. Finally, He He et al. (2013) also tried automatic feature"
C14-1076,W13-4907,1,0.80843,"Finding an optimal and accurate set of feature templates is crucial when training statistical parsers; in fact it is essential when building any machine learning system (Smith, 2011). In dependency parsing, the features are based on the linguistic information that is annotated within the words and the information that is being calculated during the parsing process. Researchers normally tend to include a large set of feature templates in their machine learning models, following the idea that more is always better; however some recent research on feature selection for transition-based parsing (Ballesteros, 2013; Ballesteros and Nivre, 2014) and graph-based parsing (He et al., 2013) have shown that more features are not always better, at least in the case of dependency parsing; models containing more features are always slower in parsing and training time and they do not always provide better results. This indicates that a smart feature template selection could be the key in the trade-off for finding an accurate and fast feature model for a given parsing model. On the one hand, we want a parser that should provide the best results possible, while on the other hand, we want a parser that should provid"
C14-1076,C00-2143,0,0.0333958,"used the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), with the same split as in (Zhang and Clark, 2008) and (Li et al., 2011).3 English: We used the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 German: We used Tiger Treebank (Brants et al., 2002) in the improved dependency conversion by Seeker and Kuhn (2012). Hungarian: We used the Szeged Dependency Treebank (Farkas et al., 2012). Russian: We used the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). 4.1 Parser settings As outlined in Section 3, our feature selection experiments require the training of a large number of parsing models and applying these to the development set.5 Therefore, we aimed to find a training setup for the parser that provided fast training times while maintaining a realistic training and optimization scenario. A major factor for the time usage is the beam size. The beam contains the alternative syntactic structures that are considered in the parsing process, and thus it requires more time and memory while it normally provides better res"
C14-1076,boguslavsky-etal-2002-development,0,0.025467,"bank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), with the same split as in (Zhang and Clark, 2008) and (Li et al., 2011).3 English: We used the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 German: We used Tiger Treebank (Brants et al., 2002) in the improved dependency conversion by Seeker and Kuhn (2012). Hungarian: We used the Szeged Dependency Treebank (Farkas et al., 2012). Russian: We used the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). 4.1 Parser settings As outlined in Section 3, our feature selection experiments require the training of a large number of parsing models and applying these to the development set.5 Therefore, we aimed to find a training setup for the parser that provided fast training times while maintaining a realistic training and optimization scenario. A major factor for the time usage is the beam size. The beam contains the alternative syntactic structures that are considered in the parsing process, and thus it requires more time and memory while it normally provides better results. The parser uses two a"
C14-1076,E12-1009,1,0.868382,"β). 2 2.1 Related Work Mate Parser For our experiments, we used the transition-based parser of Bohnet et al. (2013). This parser performs joint part-of-speech tagging, morphological tagging, and non-projective labeled dependency parsing. The parser employs a number of techniques that lead to very competitive accuracy such as beam-search with early update (Collins and Roark, 2004), a hash kernel that can quickly cope with a large feature set, a graph-based completion model that adds scores for tree parts which a transition-based parser would not be able to consider, cf. (Zhang and Clark, 2008; Bohnet and Kuhn, 2012). The graph-based model takes into account second and third order factors and obtains a score as soon as the tree parts are completed. The parser employs a rich feature set for a transition-based model (Zhang and Nivre, 2011; Bohnet et al., 2013) as well as for a graph-based model. In total, there are 326 different feature templates for the two models. The drawback of such a large feature set is a huge impact on the speed. Important research questions include (1) whether the number of features could be reduced to speed up the parser and (2) whether languages dependent feature sets would be ben"
C14-1076,D12-1133,1,0.900663,"Missing"
C14-1076,Q13-1034,1,0.64751,"e fastest way possible. For practical applications, a fast model is crucial. In this paper, we report the results of feature selection experiments that we carried out with the intention of obtaining accurate and faster feature models, for the transition-based Mate parser with and without graph-based completion models. The Mate parser is a beam search parser that uses a hash kernel for training, joint part-of-speech tagging, morphological tagging and dependency parsing. As a result of this research, we provide a framework that allows to find an optimal feature template set for the Mate parser (Bohnet et al., 2013). Moreover, our models provide some of the highest results ever reported for a set of treebanks. The paper is organized as follows. Section 2 describes related work including the used agenda-based dependency parser. This section depicts the feature templates that can be used by a transition-based or a graph-based parser. Section 3 describes the feature selection algorithm that we implemented for our experiments. Section 4 shows the experimental set-up. Section 5 reports the main results of our experiments. Section 6 provides the parsing times and memory requirements. Finally, Section 7 conclud"
C14-1076,W08-2102,0,0.217725,"Missing"
C14-1076,D07-1101,0,0.0325551,"sentence. Frequently, dynamic programming techniques are used to find the optimal tree for each span, considering candidate spans by successively building larger spans in a bottom-up fashion. A classifier is used to decide among alternative spans. The typical feature models are based on combinations of edges (as known as, factors). A factor consists either of a single edge, two or three edges; which are called first order, second and third order factors, respectively. The later are employed in more advanced and recent parsers trading off accuracy with complexity, cf. (McDonald et al., 2005b; Carreras, 2007; Koo and Collins, 2010). The features in a graph-based algorithm consist of sets of features drawn from the 795 vertexes involved in the factors. A feature template of a second order factor is composed of properties drawn from up to all three vertex, e.g., the part-of-speech of the head, the dependent and a child denoted as POS ( H )+ POS ( D )+ POS ( C ). In our experiments, we use in addition to the transition-based model, a completion model that uses graph-based feature templates with up to third order factors to re-score the beam. 2.4 Feature Selection There has been some recent research"
C14-1076,P04-1015,0,0.0618449,"σ|j], [i|β], Γ) i 6= 0 0&lt;i&lt;j Figure 1: Transition set for joint morphological and syntactic analysis. The stack Σ is represented as a list with its head to the right (and tail σ) and the buffer B as a list with its head to the left (and tail β). 2 2.1 Related Work Mate Parser For our experiments, we used the transition-based parser of Bohnet et al. (2013). This parser performs joint part-of-speech tagging, morphological tagging, and non-projective labeled dependency parsing. The parser employs a number of techniques that lead to very competitive accuracy such as beam-search with early update (Collins and Roark, 2004), a hash kernel that can quickly cope with a large feature set, a graph-based completion model that adds scores for tree parts which a transition-based parser would not be able to consider, cf. (Zhang and Clark, 2008; Bohnet and Kuhn, 2012). The graph-based model takes into account second and third order factors and obtains a score as soon as the tree parts are completed. The parser employs a rich feature set for a transition-based model (Zhang and Nivre, 2011; Bohnet et al., 2013) as well as for a graph-based model. In total, there are 326 different feature templates for the two models. The d"
C14-1076,E12-1007,0,0.0128836,"We used the following corpora for our experiments. Chinese: We used the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), with the same split as in (Zhang and Clark, 2008) and (Li et al., 2011).3 English: We used the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 German: We used Tiger Treebank (Brants et al., 2002) in the improved dependency conversion by Seeker and Kuhn (2012). Hungarian: We used the Szeged Dependency Treebank (Farkas et al., 2012). Russian: We used the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). 4.1 Parser settings As outlined in Section 3, our feature selection experiments require the training of a large number of parsing models and applying these to the development set.5 Therefore, we aimed to find a training setup for the parser that provided fast training times while maintaining a realistic training and optimization scenario. A major factor for the time usage is the beam size. The beam contains the alternative syntactic structures that are considered in the parsing process, and thus it"
C14-1076,I11-1136,0,0.0137306,"ng model with the model with selected transition-based and graph-based features, we observe a parsing time of 0.066 seconds per sentence and a small accuracy difference of only 0.27. If we use six CPU cores then parsing time decreases drastically to 0.016 seconds per sentence for the selected transition-based feature model, 0.023 for the selected transition- and graph-based feature model and to 0.05 seconds per sentence for the model with all features (which is much slower). Our experiments 801 Parser UAS POS MSTParser1 75.56 93.51 MSTParser2 77.73 93.51 Li et al. (2011) 3rd-order 80.60 92.80 Hatori et al. (2011) HS 79.60 94.01 Hatori et al. (2011) ZN 81.20 93.94 this work (sel. trans.) 81.20 94.17 this work (sel. trans.+ sel. cmp.) 81.77 94.28 Parser UAS LAS POS McDonald et al. (2005a) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Koo and Collins (2010) 93.04 Zhang and Nivre (2011) 92.9 Martins et al. (2010) 93.26 Bohnet and Nivre (2012) 93.38 92.44 97.33 this work (sel. trans.& sel. cmpl.) 93.05 92.08 97.44 this work (P&M cf. Table 3) 93.49 92.53 – Koo et al. (2008) † 93.16 Carreras et al. (2008) † 93.5 Suzuki et al. (2009) † 93.79 (b) Accuracy scores for the Chinese treebank con"
C14-1076,D13-1152,0,0.0813251,"Missing"
C14-1076,P10-1110,0,0.143518,"Missing"
C14-1076,P10-1001,0,0.0281682,"ently, dynamic programming techniques are used to find the optimal tree for each span, considering candidate spans by successively building larger spans in a bottom-up fashion. A classifier is used to decide among alternative spans. The typical feature models are based on combinations of edges (as known as, factors). A factor consists either of a single edge, two or three edges; which are called first order, second and third order factors, respectively. The later are employed in more advanced and recent parsers trading off accuracy with complexity, cf. (McDonald et al., 2005b; Carreras, 2007; Koo and Collins, 2010). The features in a graph-based algorithm consist of sets of features drawn from the 795 vertexes involved in the factors. A feature template of a second order factor is composed of properties drawn from up to all three vertex, e.g., the part-of-speech of the head, the dependent and a child denoted as POS ( H )+ POS ( D )+ POS ( C ). In our experiments, we use in addition to the transition-based model, a completion model that uses graph-based feature templates with up to third order factors to re-score the beam. 2.4 Feature Selection There has been some recent research on trying to manually fi"
C14-1076,P08-1068,0,0.196155,"Missing"
C14-1076,D11-1109,0,0.0898476,"iments for the feature selection algorithm, we carried out a series of tests based on the parser settings. From these experiments, we obtained the best parser settings, the threshold that provides the best results given a development set, and the best scoring method and some additional configurations, that gave us reliable results and a fast outcome. We used the following corpora for our experiments. Chinese: We used the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), with the same split as in (Zhang and Clark, 2008) and (Li et al., 2011).3 English: We used the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 German: We used Tiger Treebank (Brants et al., 2002) in the improved dependency conversion by Seeker and Kuhn (2012). Hungarian: We used the Szeged Dependency Treebank (Farkas et al., 2012). Russian: We used the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). 4.1 Parser settings As outlined in Section 3, our feature selection experiments require the training of a large number of parsing models and applyi"
C14-1076,D10-1004,0,0.172862,"Missing"
C14-1076,E06-1011,0,0.29438,"Missing"
C14-1076,P05-1012,0,0.170027,"trees of the words of a sentence. Frequently, dynamic programming techniques are used to find the optimal tree for each span, considering candidate spans by successively building larger spans in a bottom-up fashion. A classifier is used to decide among alternative spans. The typical feature models are based on combinations of edges (as known as, factors). A factor consists either of a single edge, two or three edges; which are called first order, second and third order factors, respectively. The later are employed in more advanced and recent parsers trading off accuracy with complexity, cf. (McDonald et al., 2005b; Carreras, 2007; Koo and Collins, 2010). The features in a graph-based algorithm consist of sets of features drawn from the 795 vertexes involved in the factors. A feature template of a second order factor is composed of properties drawn from up to all three vertex, e.g., the part-of-speech of the head, the dependent and a child denoted as POS ( H )+ POS ( D )+ POS ( C ). In our experiments, we use in addition to the transition-based model, a completion model that uses graph-based feature templates with up to third order factors to re-score the beam. 2.4 Feature Selection There has been some"
C14-1076,H05-1066,0,0.191657,"Missing"
C14-1076,C10-1093,0,0.128242,"beam. 2.4 Feature Selection There has been some recent research on trying to manually find better feature models for dependency parsers, such as Nivre et al. (2006), Hall et al. (2007), Hall (2008), Zhang and Nivre (2011), and Agirre et al. (2011). There is also research on automatic feature selection in the case of transition-based dependency parsing, a good example is MaltOptimizer (Ballesteros and Nivre, 2014) which implements a search for the best feature model that it can find, following acquired previous experience and deep linguistic knowledge (Hall et al., 2007; Nivre and Hall, 2010); Nilsson and Nugues (2010) also tried to search for optimal feature sets in the case of transition-based parsing, starting from a reduced test set using the concept of topological neighbors. Finally, He He et al. (2013) also tried automatic feature selection but for a graph-based parsing algorithm, where they pruned the feature space, removing unused features, in a first-order graph-based dependency parser, providing models that are equally accurate and faster. Zhang and Nivre (2011) pointed out that two different parsers based on the same algorithm may need different feature templates since other design aspects of a p"
C14-1076,W06-2933,0,0.076751,"Missing"
C14-1076,seeker-kuhn-2012-making,0,0.0299622,"ditional configurations, that gave us reliable results and a fast outcome. We used the following corpora for our experiments. Chinese: We used the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), with the same split as in (Zhang and Clark, 2008) and (Li et al., 2011).3 English: We used the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 German: We used Tiger Treebank (Brants et al., 2002) in the improved dependency conversion by Seeker and Kuhn (2012). Hungarian: We used the Szeged Dependency Treebank (Farkas et al., 2012). Russian: We used the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). 4.1 Parser settings As outlined in Section 3, our feature selection experiments require the training of a large number of parsing models and applying these to the development set.5 Therefore, we aimed to find a training setup for the parser that provided fast training times while maintaining a realistic training and optimization scenario. A major factor for the time usage is the beam size. The beam contains the alternative synt"
C14-1076,D09-1058,0,0.0955016,"Missing"
C14-1076,W03-3023,0,0.286938,"se experiments, we obtained the best parser settings, the threshold that provides the best results given a development set, and the best scoring method and some additional configurations, that gave us reliable results and a fast outcome. We used the following corpora for our experiments. Chinese: We used the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), with the same split as in (Zhang and Clark, 2008) and (Li et al., 2011).3 English: We used the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 German: We used Tiger Treebank (Brants et al., 2002) in the improved dependency conversion by Seeker and Kuhn (2012). Hungarian: We used the Szeged Dependency Treebank (Farkas et al., 2012). Russian: We used the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). 4.1 Parser settings As outlined in Section 3, our feature selection experiments require the training of a large number of parsing models and applying these to the development set.5 Therefore, we aimed to find a training setup for the parser that provided fast training time"
C14-1076,D08-1059,0,0.651666,"to the left (and tail β). 2 2.1 Related Work Mate Parser For our experiments, we used the transition-based parser of Bohnet et al. (2013). This parser performs joint part-of-speech tagging, morphological tagging, and non-projective labeled dependency parsing. The parser employs a number of techniques that lead to very competitive accuracy such as beam-search with early update (Collins and Roark, 2004), a hash kernel that can quickly cope with a large feature set, a graph-based completion model that adds scores for tree parts which a transition-based parser would not be able to consider, cf. (Zhang and Clark, 2008; Bohnet and Kuhn, 2012). The graph-based model takes into account second and third order factors and obtains a score as soon as the tree parts are completed. The parser employs a rich feature set for a transition-based model (Zhang and Nivre, 2011; Bohnet et al., 2013) as well as for a graph-based model. In total, there are 326 different feature templates for the two models. The drawback of such a large feature set is a huge impact on the speed. Important research questions include (1) whether the number of features could be reduced to speed up the parser and (2) whether languages dependent f"
C14-1076,P11-2033,0,0.678302,"ncy parsing. The parser employs a number of techniques that lead to very competitive accuracy such as beam-search with early update (Collins and Roark, 2004), a hash kernel that can quickly cope with a large feature set, a graph-based completion model that adds scores for tree parts which a transition-based parser would not be able to consider, cf. (Zhang and Clark, 2008; Bohnet and Kuhn, 2012). The graph-based model takes into account second and third order factors and obtains a score as soon as the tree parts are completed. The parser employs a rich feature set for a transition-based model (Zhang and Nivre, 2011; Bohnet et al., 2013) as well as for a graph-based model. In total, there are 326 different feature templates for the two models. The drawback of such a large feature set is a huge impact on the speed. Important research questions include (1) whether the number of features could be reduced to speed up the parser and (2) whether languages dependent feature sets would be beneficiary. 2.2 Features in transition-based dependency parsing Every transition-based parser uses two data structures: (1) a buffer that contains at the beginning of the parsing process all words of the sentence that have to"
C14-1076,D07-1097,0,\N,Missing
C14-1133,W13-2322,0,0.0603372,"Missing"
C14-1133,D12-1133,1,0.846199,"tically and information structure influenced relation tags to obtain an annotation granularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)). Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and 86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentences (5641 tokens in the DSyntS treebank and 8955 tokens in the SSyntS treebank). To obtain the SSyntS, we use Bohnet and Nivre (2012)’s transition-based parser, which combines lemmatization, PoS tagging, and syntactic dependency parsing—tuned and trained on the respective sets of the SSyntS treebank. Cf. Table 1 for the performance of the parser on the development set. POS LEMMA LAS UAS 96.14 91.10 78.64 86.49 Table 1: Results of Bohnet and Nivre’s surface-syntactic parser on the development set In what follows, we first present the realization of the SSyntS–DSyntS transducer and then the realization of the baseline. 3.1 SSyntS–DSyntS transducer As outlined in Section 2.2, the SSyntS–DSyntS transducer is composed of three s"
C14-1133,W06-2920,0,0.0316683,"c parsing pipeline 5 Related Work To the best of our knowledge, data-driven deep-syntactic parsing as proposed in this paper is novel. As semantic role labeling and frame-semantic analysis, it has the goal to obtain more semantically oriented structures than those delivered by state-of-the-art syntactic parsing. Semantic role labeling received considerable attention in the CoNLL shared tasks for syntactic dependency parsing in 2006 and 2007 8 We also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too weak to be used as baseline. 1409 (Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated synta"
C14-1133,J08-1003,0,0.0551117,"Missing"
C14-1133,W09-1207,0,0.0198429,"also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too weak to be used as baseline. 1409 (Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu´ıs et al., 2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform structural changes—as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS. Klimeˇs (2006)’s parser removes nodes (producing tectogrammatical structures as in the Prague Dependency Treebank), but is based on rules i"
C14-1133,J07-4004,0,0.0802815,"Missing"
C14-1133,fillmore-etal-2002-framenet,0,0.0465585,"matical functions such as, e.g., SBJ, OBJ, PRD, PMOD, etc. (Johansson and Nugues, 2007). For many NLP-applications, including machine translation, paraphrasing, text simplification, etc., such a high idiosyncrasy is obstructive because of the recurrent divergence between the source and the target structures. Therefore, the use of more abstract “syntactico-semantic” structures seems more appropriate. Following Mel’ˇcuk (1988), we call these structures deep-syntactic structures (DSyntSs). DSyntSs are situated between SSyntSs and PropBank- (Palmer et al., 2005) or Semantic Frame-like structures (Fillmore et al., 2002). Compared to SSyntSs, they have the advantage to abstract from language-specific grammatical idiosyncrasies. Compared to PropBank and Semantic Frame stuctures, they have the advantage to be connected and complete, i.e., capture all argumentative, attributive and coordinative dependencies between the meaningful lexical items of a sentence, while PropBank and Semantic Frame structures are not always connected, may contain either individual lexical items or phrasal chunks as nodes, and discard attributive and coordinative relations (be they within the chunks or sentential). In other words, they"
C14-1133,W09-1205,0,0.0190178,"int parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu´ıs et al., 2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform structural changes—as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS. Klimeˇs (2006)’s parser removes nodes (producing tectogrammatical structures as in the Prague Dependency Treebank), but is based on rules instead of classifiers, as in our case. The same applies to earlier works in the TAG-framework, as, e.g., in (Rambow and Joshi, 1997). However, this is not to say that the idea of the surface→surface syntax→deep syntax pipeline is"
C14-1133,P03-1046,0,0.0820135,"Missing"
C14-1133,W12-3602,0,0.0984375,", publish-COORD→or-II→perish, and so on. APPEND subsumes all parentheticals, interjections, direct addresses, etc., as, e.g., in Listen, John!: listen-APPEND→John. DSyntSs thus show a strong similarity with PropBank structures, with four important differences: (i) their lexical labels are not disambiguated; (ii) instead of circumstantial thematic roles of the kind ARGM-LOC, ARGM-DIR, etc. they use a unique ATTR relation; (iii) they capture all existing dependencies between meaningful lexical nodes; and (iv) they are connected.4 A number of other annotations have resemblance with DSyntSs; cf. (Ivanova et al., 2012) for an overview of deep dependency structures. Formally, a DSyntS is defined as follows: Definition 2 (DSyntS) An DSyntS of a language L is a quintuple TDS = hN, A, λls →n , ρrs →a , γn→g i defined over the full lexical items Ld of L, the set of semantic grammemes Gsem , and the set of deepsyntactic relations Rdsynt , where • the set N of nodes and the set A of directed arcs form a connected tree, • λls →n assigns to each n ∈ N an ls ∈ Ld , • ρrs →a assigns to each a ∈ A an r ∈ Rdsynt , and • γn→g assigns to each λls →n (n) a set of grammemes Gt ∈ Gsem . Consider in Figure 1 an example for an"
C14-1133,W07-2416,0,0.483157,"(forests of trees defined over individual lexemes or phrasal chunks and abstract semantic role labels which capture the argument structure of predicative elements, dropping all attributive and coordinative dependencies). We propose a parser that delivers deep syntactic structures as output. 1 Introduction Surface-syntactic structures (SSyntSs) as produced by data-driven syntactic dependency parsers are per force idiosyncratic in that they contain governed prepositions, determiners, support verb constructions and language-specific grammatical functions such as, e.g., SBJ, OBJ, PRD, PMOD, etc. (Johansson and Nugues, 2007). For many NLP-applications, including machine translation, paraphrasing, text simplification, etc., such a high idiosyncrasy is obstructive because of the recurrent divergence between the source and the target structures. Therefore, the use of more abstract “syntactico-semantic” structures seems more appropriate. Following Mel’ˇcuk (1988), we call these structures deep-syntactic structures (DSyntSs). DSyntSs are situated between SSyntSs and PropBank- (Palmer et al., 2005) or Semantic Frame-like structures (Fillmore et al., 2002). Compared to SSyntSs, they have the advantage to abstract from l"
C14-1133,W08-2123,0,0.0217204,"rsing in 2006 and 2007 8 We also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too weak to be used as baseline. 1409 (Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu´ıs et al., 2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform structural changes—as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS. Klimeˇs (2006)’s parser removes nodes (producing tectogrammatical structures as in the Prague Dependency Treebank), but"
C14-1133,P86-1038,0,0.257327,"proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 The term ‘tree transduction’ is used in this paper in the sense of Rounds (1970) and Thatcher (1970) to denote an extension of finite state transduction (Aho, 1972) to trees. 1402 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1402–1413, Dublin, Ireland, August 23-29 2014. 2.1 Defining SSyntS and DSyntS SSyntSs and DSyntSs are directed, node- and edge-labeled dependency trees with standard feature-value structures (Kasper and Rounds, 1986) as node labels and dependency relations as edge labels. The features of the node labels in SSyntSs are lexssynt , and “syntactic grammemes” of the value of lexssynt , i.e., number, gender, case, definiteness, person for nouns and tense, aspect, mood and voice for verbs. The value of lexssynt can be any (either full or functional) lexical item; in graphical representations of SSyntSs, usually only the value of lexssynt is shown. The edge labels of a SSyntS are grammatical functions ‘subj’, ‘dobj’, ‘det’, ‘modif’, etc. In other words, SSyntSs are syntactic structures of the kind as encountered"
C14-1133,C12-2082,1,0.889085,"ucer and integrated it into a pipeline shown in Figure 2. DSynt Treebank SSynt Treebank Joint PoS Tagger SSynt parser Plain Sentences SSyntS SSynt−DSynt Transducer DSynS Figure 2: Setup of a deep-syntactic parser For our experiments, we use the AnCora-UPF SSyntS and DSyntS treebanks of Spanish (Mille et al., 2013) in CoNLL format, adjusted for our needs. In particular, we removed from the 79-tag SSyntS treebank the semantically and information structure influenced relation tags to obtain an annotation granularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)). Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and 86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentences (5641 tokens in the DSyntS treebank and 8955 tokens in the SSyntS treebank). To obtain the SSyntS, we use Bohnet and Nivre (2012)’s transition-based parser, which combines lemmatization, PoS tagging, and syntactic dependency parsing—tuned and trained on the respective sets of the SSyntS treebank. Cf."
C14-1133,W13-3724,1,0.686421,"less than a dozen grammemes, etc. 3 Experiments In order to validate the outlined SSyntS–DSyntS transduction and to assess its performance in combination with a surface dependency parser, i.e., starting from plain sentences, we carried out a number of 1405 experiments in which we implemented the transducer and integrated it into a pipeline shown in Figure 2. DSynt Treebank SSynt Treebank Joint PoS Tagger SSynt parser Plain Sentences SSyntS SSynt−DSynt Transducer DSynS Figure 2: Setup of a deep-syntactic parser For our experiments, we use the AnCora-UPF SSyntS and DSyntS treebanks of Spanish (Mille et al., 2013) in CoNLL format, adjusted for our needs. In particular, we removed from the 79-tag SSyntS treebank the semantically and information structure influenced relation tags to obtain an annotation granularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)). Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and 86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentenc"
C14-1133,C69-0101,0,0.518574,"r outcome. Section 5 summarizes the related work, before in Section 6 some conclusions and plans for future work are presented. 2 Fundamentals of SSyntS–DSyntS transduction Before we set out to discuss the principles of the SSyntS–DSynt transduction, we must specify the DSyntSs and SSyntSs as used in our experiments. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 The term ‘tree transduction’ is used in this paper in the sense of Rounds (1970) and Thatcher (1970) to denote an extension of finite state transduction (Aho, 1972) to trees. 1402 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1402–1413, Dublin, Ireland, August 23-29 2014. 2.1 Defining SSyntS and DSyntS SSyntSs and DSyntSs are directed, node- and edge-labeled dependency trees with standard feature-value structures (Kasper and Rounds, 1986) as node labels and dependency relations as edge labels. The features of the node labels in SSyntSs are lexssynt , and “syntactic grammemes” of the value of lexssynt ,"
C14-1133,W08-2121,0,0.228819,"Missing"
C14-1133,taule-etal-2008-ancora,0,0.0989872,"Missing"
C14-1133,P08-1101,0,0.0611179,"Missing"
C14-1133,W09-1201,0,\N,Missing
C14-1133,P01-1033,0,\N,Missing
C14-1133,J05-1004,0,\N,Missing
C14-1133,Q13-1018,0,\N,Missing
C14-1133,D07-1096,0,\N,Missing
C14-1133,P13-2017,0,\N,Missing
C14-1133,ballesteros-nivre-2012-maltoptimizer-system,1,\N,Missing
D12-1085,P11-2040,0,0.0226499,"Missing"
D12-1085,W11-2832,0,0.241299,"algorithm for tree linearization. We obtain statistically significant improvements on six typologically different languages: English, German, Dutch, Danish, Hungarian, and Czech. 1 (1) a. *It is federal support should try to what achieve b. *It is federal support should try to achieve what c. *It is try to achieve what federal support should Introduction There is a growing interest in language-independent data-driven approaches to natural language generation (NLG). An important subtask of NLG is surface realization, which was recently addressed in the 2011 Shared Task on Surface Realisation (Belz et al., 2011). Here, the input is a linguistic representation, such as a syntactic dependency tree lacking all precedence information, and the task is to determine a natural, coherent linearization of the words. The standard data-driven approach is to traverse the dependency tree deciding locally at each node on the relative order of the head and its children. The shared task results have proven this approach to be both effective and efficient when applied to English. ROOT SBJ OBJ PRD NMOD SBJ VC OPRD IM It is what federal support should try to achieve Figure 1: A non-projective example from the CoNLL 2009"
D12-1085,C10-1012,1,0.879608,"s correspond to the lower and upper bound from the nonlifted and the gold-lifted baseline. It clearly emerges from this figure that the range of improvements obtainable from lifting is closely tied to the general 936 We also evaluated our linearizer on the data of 2011 Shared Task on Surface Realisation, which is based on the English CoNLL 2009 data (like our previous evaluations) but excludes information on morphological realization. For training and evaluation, we used the exact set up of the Shared Task. For the morphological realization, we used the morphological realizer of Bohnet et al. (2010) that predicts the word form using shortest edit scripts. For the language model (LM), we use a 5-gram model with Kneser-Ney (Kneser and Ney, 1995) smoothing derived from 11 million sentences of the Wikipedia. In Table 6, we compare our two linearizers (with and without lifting) to the two top systems of the 2011 Shared Task on Surface Realisation, (Bohnet et al., 2011) and (Guo et al., 2011). Without the lifting, our system reaches a score comparable to the topranked system in the Shared Task. With the lifting, we get a small7 but statistically significant improvement in BLEU such that our sy"
D12-1085,W11-2835,1,0.79302,"method to obtain the word order domains from dependency trees and to order the words in the tree is to use each word and its children as domain and then to order the domains and contained words recursively. As outlined in the introduction, the direct mapping of syntactic trees to domains does not provide the possibility to obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Imme"
D12-1085,P98-1026,0,0.161246,"Missing"
D12-1085,W06-2920,0,0.0163117,"Missing"
D12-1085,W07-2303,0,0.0304587,"surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on"
D12-1085,P01-1024,0,0.0448278,"tical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization hav"
D12-1085,P07-1041,0,0.0267414,"d linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machine-learning techniques to lift edges in a preprocessing step to a surface realizer. Their objective is the same as ours: by lifting, they avoid crossing edges. Ho"
D12-1085,N09-2057,0,0.133446,"LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002"
D12-1085,C02-1036,0,0.215683,"a and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machine-learning techniques to lift edges in a preprocessing step to a surface realizer. Their objective is the same as ours: by lifting, they avoid crossing edges. However, contrary to our work, they use phrase-structure syntax and focus on a limited number of cases of crossing branches in German only. 3 Lifting Dependency Edges In this section, we describe the first of the two stages in our approach, namely the classifier that lifts edges in dependency trees. The classifier we aim to train is meant to predict liftings on a given unordered dependency tree, yielding a tree that, wit"
D12-1085,P01-1029,0,0.0320148,"structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et"
D12-1085,W11-2833,0,0.0579172,"Missing"
D12-1085,W09-1201,0,0.070125,"Missing"
D12-1085,P09-1091,0,0.0665074,"rarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machine-learning techni"
D12-1085,P98-1106,0,0.0820264,"eration has addressed certain aspects of the problem. – OA# NK SB OC – Das Mandat will er zur¨ uckgeben . the.ACC mandate.ACC want.3SG he.NOM return.INF . ’He wants to return the mandate.’ Figure 2: German object fronting with complex verb introducing a non-projective edge. In this paper, we aim for a general data-driven approach that can deal with various causes for nonprojectivity and will work for typologically different languages. Our technique is inspired by work in data-driven multilingual parsing, where non-projectivity has received considerable attention. In pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005), the parsing algorithm is restricted to projective structures, but the issue is side-stepped by converting non-projective structures to projective ones prior to training and application, and then restoring the original structure afterwards. Similarly, we split the linearization task in two stages: initially, the input tree is modified by lifting certain edges in such a way that new orderings become possible even under a projectivity constraint; the second stage is the original, projective linearization step. In parsing, projectivization is a deterministic process tha"
D12-1085,P95-1024,0,0.0216643,"obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG,"
D12-1085,kow-belz-2012-lg,0,0.0286802,"percentage of nonprojective edges in our data sets, which are however important to linearize correctly (see Figure 1). linearizer. In particular, we wanted to check whether the lifting-based linearizer produces more natural word orders for sentences that had a non-projective tree in the corpus, and maybe less natural word orders on originally projective sentences. Therefore, we divided the evaluated items into originally projective and non-projective sentences. We asked four annotators to judge 60 sentence pairs comparing the lifting-based against the nonlifted linearizer using the toolkit by Kow and Belz (2012). All annotators are students, two of them have a background in linguistics. The items were randomly sampled from the subset of the development set containing those sentences where the linearizers produced different surface realizations. The items are subdivided into 30 originally projective and 30 originally non-projective sentences. For each item, we presented the original context sentence from the corpus and the pair of automatically produced linearizations for the current sentence. The annotators had to decide on two criteria: (i) which sentence do they prefer? (ii) how fluent is that sent"
D12-1085,P98-1116,0,0.302823,"tion systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word"
D12-1085,P05-1013,0,0.592937,"certain aspects of the problem. – OA# NK SB OC – Das Mandat will er zur¨ uckgeben . the.ACC mandate.ACC want.3SG he.NOM return.INF . ’He wants to return the mandate.’ Figure 2: German object fronting with complex verb introducing a non-projective edge. In this paper, we aim for a general data-driven approach that can deal with various causes for nonprojectivity and will work for typologically different languages. Our technique is inspired by work in data-driven multilingual parsing, where non-projectivity has received considerable attention. In pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005), the parsing algorithm is restricted to projective structures, but the issue is side-stepped by converting non-projective structures to projective ones prior to training and application, and then restoring the original structure afterwards. Similarly, we split the linearization task in two stages: initially, the input tree is modified by lifting certain edges in such a way that new orderings become possible even under a projectivity constraint; the second stage is the original, projective linearization step. In parsing, projectivization is a deterministic process that lifts edges based on the"
D12-1085,E89-1014,0,0.400442,"onent. This classifier has to be trained on suitable data, and it is an empirical question whether the projective linearizer can take advantage of this preceding lifting step. We present experiments on six languages with varying degrees of non-projective structures: English, German, Dutch, Danish, Czech and Hungarian, which exhibit substantially different word order properties. Our approach achieves significant improvements on all six languages. On German, we also report results of a pilot human evaluation. 929 2 Related Work An important concept for tree linearization are word order domains (Reape, 1989). The domains are bags of words (constituents) that are not allowed to be discontinuous. A straightforward method to obtain the word order domains from dependency trees and to order the words in the tree is to use each word and its children as domain and then to order the domains and contained words recursively. As outlined in the introduction, the direct mapping of syntactic trees to domains does not provide the possibility to obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surf"
D12-1085,C04-1097,0,0.282171,"ear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours i"
D12-1085,seeker-kuhn-2012-making,1,0.896576,"Missing"
D12-1085,W11-2834,0,0.0293792,"from dependency trees and to order the words in the tree is to use each word and its children as domain and then to order the domains and contained words recursively. As outlined in the introduction, the direct mapping of syntactic trees to domains does not provide the possibility to obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees)"
D12-1085,vincze-etal-2010-hungarian,0,0.062248,"Missing"
D12-1085,E09-1097,0,0.0255443,"(2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machin"
D12-1085,D09-1043,0,0.0262488,"rom linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal"
D12-1085,C98-1112,0,\N,Missing
D12-1085,C98-1102,0,\N,Missing
D12-1085,C98-1026,0,\N,Missing
D12-1133,W06-2922,0,0.0120535,"re et al. (2004), who used classifiers trained to predict individual actions of a deterministic shift-reduce parser. Recent research has shown that better accuracy can be achieved by using beam search and optimizing models on the entire sequence of decisions needed to parse a sentence instead of single actions (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Clark, 2011; Zhang and Nivre, 2011; Bohnet, 2011). In addition, a number of different transition systems have been proposed, in particular for dealing with non-projective dependencies, which were beyond the scope of early systems (Attardi, 2006; Nivre, 2007; Nivre, 2009; Titov et al., 2009). In this section, we start by defining a transition system for joint tagging and parsing based on the non-projective transition system proposed in Nivre (2009). We then show how to perform beam search and structured online learning with this model, and conclude by discussing feature representations. 2.1 Transition System Given a set P of part-of-speech tags and a set D of dependency labels, a tagged dependency tree for a sentence x = w1 , . . . , wn is a directed tree T = (Vx , A) with labeling functions π and δ such that: 1. Vx = {0, 1, . . . ,"
D12-1133,E12-1009,1,0.548119,"on) in combination with neighboring words, word prefixes, word suffixes, score differences and tag rank. Finally, in some experiments, we make use of two additional feature sets, which we call graph features (G) and cluster features (C), respectively. Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008). However, while their features were 1459 limited to certain first- and second-order factors, we use features over second- and third-order factors as found in the parsers of Bohnet and Kuhn (2012). These features are scored as soon as the factors are completed, using a technique that is similar to what Hatori et al. (2011) call delayed features, although they use it for part-of-speech tags in the lookahead while we use it for subgraphs of the dependency tree. Cluster features, finally, are features over word clusters, as first used by Koo et al. (2008), which replace part-of-speech tag features.2 We use a hash kernel to map features to weights. It has been observed that most of the computing time in feature-rich parsers is spent retrieving the index of each feature in the weight vector"
D12-1133,C10-1011,1,0.712467,"ccuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages. 1 Introduction Dependency-based syntactic parsing has been the focus of intense research efforts during the last decade, and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning. Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010). Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and"
D12-1133,W08-2102,0,0.0226881,"Missing"
D12-1133,D07-1101,0,0.0656566,"rovements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages. 1 Introduction Dependency-based syntactic parsing has been the focus of intense research efforts during the last decade, and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning. Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010). Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimizat"
D12-1133,P05-1022,0,0.0491642,"search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011). It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated using (at least) a part-of-speech tagger. This is in stark contrast to the best parsers based on PCFG models, such as the Brown parser (Charniak and Johnson, 2005) and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which not only can perform their own part-of-speech tagging but normally give better parsing accuracy when they are allowed to do so. This suggests that joint models for tagging and parsing might improve accuracy also in the case of dependency parsing. It has been argued that joint morphological and syntactic disambiguation is especially important for richly inflected languages, where there is considerable interaction between morphology and syntax such that neither can be fully disambiguated without considering the other."
D12-1133,A00-2018,0,0.172564,"Missing"
D12-1133,D07-1022,0,0.0449469,"Missing"
D12-1133,P04-1015,0,0.203067,"ary experiments. One final thing to note about the inference algorithm is that the notion of permissibility for a transition t out of a configuration c can be used to capture not only formal constraints on transitions – such as the fact that it is impossible to perform a S HIFTp transition with an empty buffer or illegal to perform a L EFT-A RCd transition with the special root node on top of the stack – but also to filter out unlike1458 τ= f(xj , yj ) − f(xj , y ∗ ) ||f(xj , yj ) − f(xj , y ∗ )||2 We also use the early update strategy found beneficial for parsing in several previous studies (Collins and Roark, 2004; Zhang and Clark, 2008; Huang and Sagae, 2010), which means that, during learning, we terminate the beam search as soon as the hypothesis corresponding to the gold parse yj falls out of the beam and update with respect to the partial transition sequence constructed up to that point. Finally, we use the standard technique of averaging over all weight vectors, as originally proposed by Collins (2002). 2.3 Feature Representations As already noted, the feature representation f(x, y) of an input sentence x with parse y decomposes into feature representations f(c, t) for the transitions t(c) needed"
D12-1133,P97-1003,0,0.137903,"Missing"
D12-1133,W02-1001,0,0.469906,"so that S HIFTp is permissible only if p is one of the k best part-of-speech tags with a score no more than α below the score of the 1-best tag, as determined by a preprocessing tagger. We also filter out instances of L EFT-A RCd and R IGHT-A RCd , where d does not occur in the training data for the predicted part-ofspeech tag combination of the head and dependent. This procedure leads to a significant speed up. In order to learn a weight vector w from a training set {(xj , yj )}Tj=1 of sentences with their tagged dependency trees, we use a variant of the structured perceptron, introduced by Collins (2002), which makes N iterations over the training data and updates the weight vector for every sentence xj where the highest scoring parse y ∗ is different from yj . More precisely, we use the passive-aggressive update of Crammer et al. (2006): wi+1 = wi + τ (f(xj , yj ) − f(xj , y ∗ )) (c,t)∈C0,m s(x, y) = X f(c, t) · w where (c,t)∈C0,m Finally, the configuration of the new hypothesis is obtained by evaluating t(h.c) (line 11). The new hypothesis is then inserted into T MP in score-sorted order (line 12). The pruning parameters b1 and b2 determine the number of hypotheses allowed in the beam and a"
D12-1133,W09-1205,0,0.0189706,"Missing"
D12-1133,P08-1043,0,0.0681111,"Missing"
D12-1133,I11-1136,0,0.819459,"or tagging and parsing might improve accuracy also in the case of dependency parsing. It has been argued that joint morphological and syntactic disambiguation is especially important for richly inflected languages, where there is considerable interaction between morphology and syntax such that neither can be fully disambiguated without considering the other. Thus, Lee et al. (2011) show that a discriminative model for joint morphological disambiguation and dependency parsing outperforms a pipeline model in experiments on Latin, Ancient Greek, Czech and Hungarian. However, Li et al. (2011) and Hatori et al. (2011) report improvements with a joint model also for Chinese, which is not a richly inflected language but is nevertheless rich in part-of-speech ambiguities. In this paper, we present a transition-based model for joint part-of-speech tagging and labeled dependency parsing with non-projective trees. Exper1455 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1455–1465, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics iments show that joint modeling improves both taggin"
D12-1133,P10-1110,0,0.394881,"led dependency parsing. It is also the first joint system that achieves state-of-the-art accuracy for non-projective dependency parsing. 2 Transition-Based Tagging and Parsing Transition-based dependency parsing was pioneered by Yamada and Matsumoto (2003) and Nivre et al. (2004), who used classifiers trained to predict individual actions of a deterministic shift-reduce parser. Recent research has shown that better accuracy can be achieved by using beam search and optimizing models on the entire sequence of decisions needed to parse a sentence instead of single actions (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Clark, 2011; Zhang and Nivre, 2011; Bohnet, 2011). In addition, a number of different transition systems have been proposed, in particular for dealing with non-projective dependencies, which were beyond the scope of early systems (Attardi, 2006; Nivre, 2007; Nivre, 2009; Titov et al., 2009). In this section, we start by defining a transition system for joint tagging and parsing based on the non-projective transition system proposed in Nivre (2009). We then show how to perform beam search and structured online learning with this model, and conclude by discussing feature representati"
D12-1133,W06-2930,0,0.0370504,"Missing"
D12-1133,P10-1001,0,0.127268,"h tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages. 1 Introduction Dependency-based syntactic parsing has been the focus of intense research efforts during the last decade, and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning. Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010). Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2"
D12-1133,P08-1068,0,0.208457,"racy of a transition-based parser by Zhang and Clark (2008). However, while their features were 1459 limited to certain first- and second-order factors, we use features over second- and third-order factors as found in the parsers of Bohnet and Kuhn (2012). These features are scored as soon as the factors are completed, using a technique that is similar to what Hatori et al. (2011) call delayed features, although they use it for part-of-speech tags in the lookahead while we use it for subgraphs of the dependency tree. Cluster features, finally, are features over word clusters, as first used by Koo et al. (2008), which replace part-of-speech tag features.2 We use a hash kernel to map features to weights. It has been observed that most of the computing time in feature-rich parsers is spent retrieving the index of each feature in the weight vector (Bohnet, 2010). This is usually done via a hash table, but significant speedups can be achieved by using a hash kernel, which simply replaces table lookup by a hash function (Bloom, 1970; Shi et al., 2009; Bohnet, 2010). The price to pay for these speedups is that there may be collisions, so that different features are mapped to the same index, but this is of"
D12-1133,D10-1125,0,0.0155999,"arch efforts during the last decade, and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning. Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010). Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011). It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated using (at least) a pa"
D12-1133,P11-1089,0,0.0147608,"he Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which not only can perform their own part-of-speech tagging but normally give better parsing accuracy when they are allowed to do so. This suggests that joint models for tagging and parsing might improve accuracy also in the case of dependency parsing. It has been argued that joint morphological and syntactic disambiguation is especially important for richly inflected languages, where there is considerable interaction between morphology and syntax such that neither can be fully disambiguated without considering the other. Thus, Lee et al. (2011) show that a discriminative model for joint morphological disambiguation and dependency parsing outperforms a pipeline model in experiments on Latin, Ancient Greek, Czech and Hungarian. However, Li et al. (2011) and Hatori et al. (2011) report improvements with a joint model also for Chinese, which is not a richly inflected language but is nevertheless rich in part-of-speech ambiguities. In this paper, we present a transition-based model for joint part-of-speech tagging and labeled dependency parsing with non-projective trees. Exper1455 Proceedings of the 2012 Joint Conference on Empirical Met"
D12-1133,D11-1109,0,0.358563,"s that joint models for tagging and parsing might improve accuracy also in the case of dependency parsing. It has been argued that joint morphological and syntactic disambiguation is especially important for richly inflected languages, where there is considerable interaction between morphology and syntax such that neither can be fully disambiguated without considering the other. Thus, Lee et al. (2011) show that a discriminative model for joint morphological disambiguation and dependency parsing outperforms a pipeline model in experiments on Latin, Ancient Greek, Czech and Hungarian. However, Li et al. (2011) and Hatori et al. (2011) report improvements with a joint model also for Chinese, which is not a richly inflected language but is nevertheless rich in part-of-speech ambiguities. In this paper, we present a transition-based model for joint part-of-speech tagging and labeled dependency parsing with non-projective trees. Exper1455 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1455–1465, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics iments show that joint mode"
D12-1133,P09-1039,0,0.0147995,"c parsing has been the focus of intense research efforts during the last decade, and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning. Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010). Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011). It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentence have been morphologi"
D12-1133,D10-1004,0,0.0861162,"focus of intense research efforts during the last decade, and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning. Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010). Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011). It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated us"
D12-1133,E06-1011,0,0.209765,"German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages. 1 Introduction Dependency-based syntactic parsing has been the focus of intense research efforts during the last decade, and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning. Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010). Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selecti"
D12-1133,P05-1012,0,0.455901,"ese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages. 1 Introduction Dependency-based syntactic parsing has been the focus of intense research efforts during the last decade, and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning. Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010). Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy"
D12-1133,W04-2407,1,0.409781,"ciation for Computational Linguistics iments show that joint modeling improves both tagging and parsing accuracy, leading to state-of-the-art accuracy for richly inflected languages like Czech and German as well as more configurational languages like Chinese and English. To our knowledge, this is the first joint system that performs labeled dependency parsing. It is also the first joint system that achieves state-of-the-art accuracy for non-projective dependency parsing. 2 Transition-Based Tagging and Parsing Transition-based dependency parsing was pioneered by Yamada and Matsumoto (2003) and Nivre et al. (2004), who used classifiers trained to predict individual actions of a deterministic shift-reduce parser. Recent research has shown that better accuracy can be achieved by using beam search and optimizing models on the entire sequence of decisions needed to parse a sentence instead of single actions (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Clark, 2011; Zhang and Nivre, 2011; Bohnet, 2011). In addition, a number of different transition systems have been proposed, in particular for dealing with non-projective dependencies, which were beyond the scope of early systems (Attardi, 2006; N"
D12-1133,N07-1050,1,0.825999,"), who used classifiers trained to predict individual actions of a deterministic shift-reduce parser. Recent research has shown that better accuracy can be achieved by using beam search and optimizing models on the entire sequence of decisions needed to parse a sentence instead of single actions (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Clark, 2011; Zhang and Nivre, 2011; Bohnet, 2011). In addition, a number of different transition systems have been proposed, in particular for dealing with non-projective dependencies, which were beyond the scope of early systems (Attardi, 2006; Nivre, 2007; Nivre, 2009; Titov et al., 2009). In this section, we start by defining a transition system for joint tagging and parsing based on the non-projective transition system proposed in Nivre (2009). We then show how to perform beam search and structured online learning with this model, and conclude by discussing feature representations. 2.1 Transition System Given a set P of part-of-speech tags and a set D of dependency labels, a tagged dependency tree for a sentence x = w1 , . . . , wn is a directed tree T = (Vx , A) with labeling functions π and δ such that: 1. Vx = {0, 1, . . . , n} is a set o"
D12-1133,J08-4003,1,0.798923,"the tree. The set Vx of nodes is the set of positive integers up to and including n, each corresponding to the linear position of a word in the sentence, plus an extra 1456 artificial root node 0. The set A of arcs is a set of pairs (i, j), where i is the head node and j is the dependent node. The functions π and δ assign a unique part-of-speech label to each node/word and a unique dependency label to each arc, respectively. This notion of dependency tree differs from the standard definition only by including part-of-speech labels as well as dependency labels (K¨ubler et al., 2009). Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S = (C, T, cs , Ct ), where 1. C is a set of configurations, 2. T is a set of transitions, each of which is a (partial) function t : C → C, 3. cs is an initialization function, mapping a sentence x to a configuration c ∈ C, 4. Ct ⊆ C is a set of terminal configurations. A transition sequence for a sentence x in S is a sequence of configuration-transition pairs C0,m = [(c0 , t0 ), (c1 , t1 ), . . . , (cm , tm )] where c0 = cs (x), tm (cm ) ∈ Ct and ti (ci ) = ci+1 (0 ≤ i < m).1 In this paper, we take the set C of configuratio"
D12-1133,P09-1040,1,0.809231,"lassifiers trained to predict individual actions of a deterministic shift-reduce parser. Recent research has shown that better accuracy can be achieved by using beam search and optimizing models on the entire sequence of decisions needed to parse a sentence instead of single actions (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Clark, 2011; Zhang and Nivre, 2011; Bohnet, 2011). In addition, a number of different transition systems have been proposed, in particular for dealing with non-projective dependencies, which were beyond the scope of early systems (Attardi, 2006; Nivre, 2007; Nivre, 2009; Titov et al., 2009). In this section, we start by defining a transition system for joint tagging and parsing based on the non-projective transition system proposed in Nivre (2009). We then show how to perform beam search and structured online learning with this model, and conclude by discussing feature representations. 2.1 Transition System Given a set P of part-of-speech tags and a set D of dependency labels, a tagged dependency tree for a sentence x = w1 , . . . , wn is a directed tree T = (Vx , A) with labeling functions π and δ such that: 1. Vx = {0, 1, . . . , n} is a set of nodes, 2. A"
D12-1133,N07-1051,0,0.0174838,"2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011). It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated using (at least) a part-of-speech tagger. This is in stark contrast to the best parsers based on PCFG models, such as the Brown parser (Charniak and Johnson, 2005) and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which not only can perform their own part-of-speech tagging but normally give better parsing accuracy when they are allowed to do so. This suggests that joint models for tagging and parsing might improve accuracy also in the case of dependency parsing. It has been argued that joint morphological and syntactic disambiguation is especially important for richly inflected languages, where there is considerable interaction between morphology and syntax such that neither can be fully disambiguated without considering the other. Thus, Lee et al. (2011) show that a discriminative model for joint mor"
D12-1133,P06-1055,0,0.0905061,"008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011). It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated using (at least) a part-of-speech tagger. This is in stark contrast to the best parsers based on PCFG models, such as the Brown parser (Charniak and Johnson, 2005) and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which not only can perform their own part-of-speech tagging but normally give better parsing accuracy when they are allowed to do so. This suggests that joint models for tagging and parsing might improve accuracy also in the case of dependency parsing. It has been argued that joint morphological and syntactic disambiguation is especially important for richly inflected languages, where there is considerable interaction between morphology and syntax such that neither can be fully disambiguated without considering the other. Thus, Lee et al. (2011) show that a discrimin"
D12-1133,W06-1616,0,0.00579486,"nguages. 1 Introduction Dependency-based syntactic parsing has been the focus of intense research efforts during the last decade, and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning. Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010). Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011). It is noteworthy, however, that almost all dependency parsers presuppose that the w"
D12-1133,C08-1098,0,0.037857,"Missing"
D12-1133,D08-1016,0,0.00900578,"ependency-based syntactic parsing has been the focus of intense research efforts during the last decade, and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning. Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010). Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011). It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentenc"
D12-1133,P11-2009,0,0.0602517,"Missing"
D12-1133,D09-1058,0,0.0481609,"Missing"
D12-1133,W07-2218,0,0.100716,"uced using structured learning. Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010). Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011). It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated using (at least) a part-of-speech tagger. This is in stark contrast to the best parsers based on PCFG models, such as the Brown parser (Charniak and Johnson, 2005) and t"
D12-1133,N03-1033,0,0.143986,"onverted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 In order to assign k-best part-of-speech tags and scores to words in the training set, we used a perceptron tagger with 10-fold jack-knifing. The same type of tagger was trained on the entire training set in order to supply tags for the development and test sets. The feature set of the tagger was optimized for English and German and provides state-of-theart accuracy for these two languages. The 1-best tagging accuracy for section 23 of the Penn Treebank is 97.28, which is on a par with Toutanova et al. (2003). For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RF-Tagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German.5 The results are not directly comparable to the RF-Tagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Treebank. We could not use the larger training set as it contains the test set of the CoNLL 2009 data that we use to evaluate the joint model. For Czech, the 1best tagging accuracy is 99.11 and for Chinese 92.65 on the CoNLL 2009 test set. We trained parsers"
D12-1133,P06-3009,0,0.198008,"Missing"
D12-1133,W03-3023,0,0.916128,"orea, 12–14 July 2012. 2012 Association for Computational Linguistics iments show that joint modeling improves both tagging and parsing accuracy, leading to state-of-the-art accuracy for richly inflected languages like Czech and German as well as more configurational languages like Chinese and English. To our knowledge, this is the first joint system that performs labeled dependency parsing. It is also the first joint system that achieves state-of-the-art accuracy for non-projective dependency parsing. 2 Transition-Based Tagging and Parsing Transition-based dependency parsing was pioneered by Yamada and Matsumoto (2003) and Nivre et al. (2004), who used classifiers trained to predict individual actions of a deterministic shift-reduce parser. Recent research has shown that better accuracy can be achieved by using beam search and optimizing models on the entire sequence of decisions needed to parse a sentence instead of single actions (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Clark, 2011; Zhang and Nivre, 2011; Bohnet, 2011). In addition, a number of different transition systems have been proposed, in particular for dealing with non-projective dependencies, which were beyond the scope of early s"
D12-1133,D08-1059,0,0.814263,"ing. Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010). Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011). It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated using (at least) a part-of-speech tagger. This is in stark contrast to the best parsers based on PCFG models, such as the Brown parser (Charniak and Johnson, 2005) and the Berkeley parser (Pet"
D12-1133,J11-1005,0,0.0446316,"parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010). Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011). It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated using (at least) a part-of-speech tagger. This is in stark contrast to the best parsers based on PCFG models, such as the Brown parser (Charniak and Johnson, 2005) and the Berkeley parser (Petrov et al., 2006; Petrov"
D12-1133,P11-2033,1,0.842746,"net, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010). Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011). It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated using (at least) a part-of-speech tagger. This is in stark contrast to the best parsers based on PCFG models, such as the Brown parser (Charniak and Johnson, 2005) and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which not only can perform their own part-of-speech tagging but normally give better parsing accuracy when they are allowed to do so. This suggests that joint models for tagging and parsing might imp"
D12-1133,W09-1201,1,\N,Missing
D19-6301,W13-3520,0,0.0329649,"e, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material, and publicly available off-the3 In the case of one team, we agreed to move the two week window between test data release and submission to one week earlier. 4 At SR’18, there were ten languages from five families. 5 https://www.aclweb.org/portal/ content/sigmorphon-shared-task-2019 6 universaldependencies.org shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013) or BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). Datasets were created for 11 languages in the Shallow Track, and for three of those languages, namely English, French and Spanish, in the Deep Track. As in 2018, Shallow Track inputs were generated with the aid of Python scripts from the original UD structures, this time using all available input sentences. Deep Track inputs were then generated by automatically processing the Shallow Track structures using a series of gra"
D19-6301,P11-2040,1,0.829216,"availability of evaluators: four Shallow Track in-domain datasets (Chinese-GSD, English-EWT, RussianSynTagRus, Spanish-AnCora), one Shallow Track dataset coming from parsed data (SpanishAnCoraHIT ) and one (in-domain) Deep Track dataset (English-EWT). As in SR’11 (Belz et al., 2011) and SR’18 (Mille et al., 2018), we assessed two quality criteria in the human evaluations, in separate evaluation experiments, Readability and Meaning Similarity, and used continuous sliders as rating tools, the evidence being that raters tend to prefer them 14 Thank you to Yevgeniy Puzikov for pointing this out. (Belz and Kow, 2011). Slider positions were mapped to values from 0 to 100 (best). Raters were first given brief instructions, including the direction to ignore formatting errors, superfluous whitespace, capitalisation issues, and poor hyphenation. The statement to be assessed in the Readability evaluation was: The text reads well and is free from grammatical errors and awkward constructions. The corresponding statement in the Meaning Similarity evaluation, in which system outputs (‘the black text’) were compared to reference sentences (‘the grey text’), was as follows: The meaning of the grey text is adequately"
D19-6301,W11-2832,1,0.65823,"ncies.org/ Bernd Bohnet Google Inc. bohnetbd@google.com Leo Wanner ICREA and UPF, Barcelona leo.wanner@upf.edu growing, as is their size (and thus the volume of available training material).2 The SR tasks require participating systems to generate sentences from structures at the level of abstraction of outputs produced by state-of-the-art parsing. In order to promote linkage with parsing and earlier stages of generation, participants are encouraged to explore the extent to which neural network parsing algorithms can be reversed for generation. As was the case with its predecessor tasks SR’11 (Belz et al., 2011) and SR’18 (Mille et al., 2018), SR’19 comprises two tracks distinguished by the level of specificity of the inputs: Shallow Track (T1): This track starts from UD structures in which most of the word order information has been removed and tokens have been lemmatised. In other words, it starts from unordered dependency trees with lemmatised nodes that hold PoS tags and morphological information as found in the original treebank annotations. The task in this track therefore amounts to determining the word order and inflecting words. Deep Track (T2): This track starts from UD structures from whic"
D19-6301,K17-3005,0,0.0303018,"bed by syntactic structure or agreement (such as verbal finiteness or verbal number) was removed, whereas semanticlevel information such as nominal number and verbal tense was retained. UD2.3 version of the dataset, whereas CoNLL’18 used UD2.2; we selected treebanks that had not undergone major updates from one version to the next according to their README files on the UD site, and for which the best available parse reached a Labeled Attachment Score of 85 and over.11 There were datasets meeting these criteria for English (2), Hindi, Korean, Portuguese and Spanish; the Harbin HIT-SCIR parser (Che et al., 2017) had best scores on four of these datasets; LATTICE (Lim et al., 2018) and Stanford (Qi et al., 2019) had the best scores for the remaining two;12 see Table 3 for an overview. As is the case for all test data, in the additional automatically parsed test data alignments with surface tokens and with Shallow Track tokens are not provided; however, in the cases described in 4 above, the relative order is provided. 10. Fine-grained PoS labels found in some treebanks (see e.g. column 5 in Figure 2) were removed, and only coarse-grained ones were retained (column 4 in Figures 2 and 3). 11. In the tra"
D19-6301,D19-6302,0,0.0736613,"tion is very similar to ADAPT’s SR’18 submission (Elder and Hokamp, 2018). The BME-UW system (Kov´acs et al., 2019) learns weighted rules of an Interpreted Regular Tree Grammar (IRTG) to encode the correspondence between word sequences and UDsubgraphs. For the inflection step, a standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder with attention is used. CLaC (Farahnak et al., 2019) is a pointer network trained to find the best order of the input. A slightly modified version of the transformer model was used as the encoder and decoder for the pointer network. The CMU (Du and Black, 2019) system uses a graph neural network for end-to-end ordering, and a character RNN for morphology. DepDist (Dyer, 2019) uses syntactic embeddings and a graph neural network with message passing to learn the tolerances for how far a dependent tends to be from its head. These directed 15 https://github.com/ygraham/ segment-mteval dependency distance tolerances form an edgeweighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions app"
D19-6301,D19-6303,0,0.0332354,"ighted rules of an Interpreted Regular Tree Grammar (IRTG) to encode the correspondence between word sequences and UDsubgraphs. For the inflection step, a standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder with attention is used. CLaC (Farahnak et al., 2019) is a pointer network trained to find the best order of the input. A slightly modified version of the transformer model was used as the encoder and decoder for the pointer network. The CMU (Du and Black, 2019) system uses a graph neural network for end-to-end ordering, and a character RNN for morphology. DepDist (Dyer, 2019) uses syntactic embeddings and a graph neural network with message passing to learn the tolerances for how far a dependent tends to be from its head. These directed 15 https://github.com/ygraham/ segment-mteval dependency distance tolerances form an edgeweighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions approximating productive inflectional paradigms. The DipInfoUnito realiser (Mazzei and Basile, 2019) is a supervised sta"
D19-6301,W18-3606,0,0.194017,"andard scores (or z-scores) computed on the set of all raw scores by the given evaluator using each evaluator’s mean and standard deviation. For both raw and standard scores, we compute the mean of sentence-level scores. Code: We were able to reuse, with minor adaptations, the code produced for the WMT’17 evaluations.15 4 Overview of Submitted Systems ADAPT is a sequence to sequence model with dependency features attached to word embeddings. A BERT sentence classifier was used as a reranker to choose between different hypotheses. The implementation is very similar to ADAPT’s SR’18 submission (Elder and Hokamp, 2018). The BME-UW system (Kov´acs et al., 2019) learns weighted rules of an Interpreted Regular Tree Grammar (IRTG) to encode the correspondence between word sequences and UDsubgraphs. For the inflection step, a standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder with attention is used. CLaC (Farahnak et al., 2019) is a pointer network trained to find the best order of the input. A slightly modified version of the transformer model was used as the encoder and decoder for the pointer network. The CMU (Du and Black, 2019) system uses a graph neural network for end-to-end ord"
D19-6301,W18-3604,0,0.0728895,"nto a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. Surfers (Hong et al., 2019) first performs delexicalisation to obtain a dictionary for proper names and numbers. A GCN is then used to encode the tree inputs, and an LSTM encoder-decoder with copy attention to generate delexicalised outputs. No part-of-speech tags, universal features or pretrained embeddings / language models are used. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises multilingual texts by first preprocessing an input dependency tree into an ordered linearised string, which is then realised using a rule-based and a statistical machine translation (SMT) model. Baseline: In order to set a lower boundary for the automatic and human evaluations, a simple English baseline consisting of 7 lines of python code was implemented16 . It generates from a UD file with an in-order traversal of the tree read by pyconll and outputting the form of each node. 5 Evaluation results There were 14 submissions to the task, of which two were withdrawn; 9 teams participa"
D19-6301,D19-6310,0,0.021759,"tree-LSTM encoders) whose outputs also contain tokens marking the tree structure. N-best outputs are obtained for orderings and the highest confidence output sequence with a valid tree is chosen (i.e, one where the input and output trees are isomorphic up to sibling order, ensuring projectivity). The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. Surfers (Hong et al., 2019) first performs delexicalisation to obtain a dictionary for proper names and numbers. A GCN is then used to encode the tree inputs, and an LSTM encoder-decoder with copy attention to generate delexicalised outputs. No part-of-speech tags, universal features or pretrained embeddings / language models are used. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises multilingual texts by first preprocessing an input dependency tree into an ordered linearised string, which is then realised using a rule-based and a statistical machine translation (SMT) model. B"
D19-6301,D19-6304,0,0.339584,"Missing"
D19-6301,K18-2014,0,0.041076,"verbal number) was removed, whereas semanticlevel information such as nominal number and verbal tense was retained. UD2.3 version of the dataset, whereas CoNLL’18 used UD2.2; we selected treebanks that had not undergone major updates from one version to the next according to their README files on the UD site, and for which the best available parse reached a Labeled Attachment Score of 85 and over.11 There were datasets meeting these criteria for English (2), Hindi, Korean, Portuguese and Spanish; the Harbin HIT-SCIR parser (Che et al., 2017) had best scores on four of these datasets; LATTICE (Lim et al., 2018) and Stanford (Qi et al., 2019) had the best scores for the remaining two;12 see Table 3 for an overview. As is the case for all test data, in the additional automatically parsed test data alignments with surface tokens and with Shallow Track tokens are not provided; however, in the cases described in 4 above, the relative order is provided. 10. Fine-grained PoS labels found in some treebanks (see e.g. column 5 in Figure 2) were removed, and only coarse-grained ones were retained (column 4 in Figures 2 and 3). 11. In the training data, the alignments with the tokens of the Shallow Track struct"
D19-6301,D19-6311,0,0.0501754,"ter RNN for morphology. DepDist (Dyer, 2019) uses syntactic embeddings and a graph neural network with message passing to learn the tolerances for how far a dependent tends to be from its head. These directed 15 https://github.com/ygraham/ segment-mteval dependency distance tolerances form an edgeweighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions approximating productive inflectional paradigms. The DipInfoUnito realiser (Mazzei and Basile, 2019) is a supervised statistical system for surface realisation, in which two neural network-based models run in parallel on the same input structure, namely a list-wise learning to rank network for linearisation and a seq2seq network for morphology inflection prediction. IMS (Yu et al., 2019) uses a pipeline approach for both tracks, consisting of linearisation, completion (for T2 only), inflection, and contraction. All models use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search and then combines them into a full projec"
D19-6301,W04-2705,0,0.268698,"Missing"
D19-6301,W18-3601,1,0.502146,"Inc. bohnetbd@google.com Leo Wanner ICREA and UPF, Barcelona leo.wanner@upf.edu growing, as is their size (and thus the volume of available training material).2 The SR tasks require participating systems to generate sentences from structures at the level of abstraction of outputs produced by state-of-the-art parsing. In order to promote linkage with parsing and earlier stages of generation, participants are encouraged to explore the extent to which neural network parsing algorithms can be reversed for generation. As was the case with its predecessor tasks SR’11 (Belz et al., 2011) and SR’18 (Mille et al., 2018), SR’19 comprises two tracks distinguished by the level of specificity of the inputs: Shallow Track (T1): This track starts from UD structures in which most of the word order information has been removed and tokens have been lemmatised. In other words, it starts from unordered dependency trees with lemmatised nodes that hold PoS tags and morphological information as found in the original treebank annotations. The task in this track therefore amounts to determining the word order and inflecting words. Deep Track (T2): This track starts from UD structures from which functional words (in particul"
D19-6301,W15-4719,0,0.125325,"The linearised constituent trees are fed to seq2seq models (including models with copy and with tree-LSTM encoders) whose outputs also contain tokens marking the tree structure. N-best outputs are obtained for orderings and the highest confidence output sequence with a valid tree is chosen (i.e, one where the input and output trees are isomorphic up to sibling order, ensuring projectivity). The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. Surfers (Hong et al., 2019) first performs delexicalisation to obtain a dictionary for proper names and numbers. A GCN is then used to encode the tree inputs, and an LSTM encoder-decoder with copy attention to generate delexicalised outputs. No part-of-speech tags, universal features or pretrained embeddings / language models are used. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises multilingual texts by first preprocessing an input dependency tree into an ordered linearised string, whic"
D19-6301,J05-1004,0,0.0811396,"n the English-gum dataset);8 3. The lines corresponding to combined lexical units (e.g. Spanish “del” &lt;de+el&gt; lit. ’of.the’) and the contents of columns [9] and [10] were removed; 4. Information about the relative order of components of named entities, multiple coordinations and punctuation signs was added in the FEATS column (dependency relations compound, compound:prt, compound:svc, flat, flat:foreign, flat:name, fixed, conj, punct); For the Deep Track, the following steps were additionally carried out: 5. Edge labels were generalised into predicate/argument labels, in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion. That is, the 8 Thank you to Guy Lapalme for spotting this. syntactic relations were mapped to core (A1, A2, etc.) and non-core (AM) labels, applying the following rules: (i) the first argument is always labeled A1 (i.e. there is no external argument A0); (ii) in order to maintain the tree structure and account for some cases of shared arguments, there can be inverted argument relations; (iii) all modifier edges are assigned the same generic label AM; (iv) there is a coordinating relation. See also the inventory of relations in Table 2. 6. Functional prepositions"
D19-6301,P02-1040,0,0.108135,"a que los nuevos miembros del CNE deben tener experiencia para “dirigir procesos complejos”. In the original UD files, the reference sentences are by default detokenised. In order to carry out the evaluations of the tokenised outputs, we built a tokenised version of the reference sentences by concatenating the words of the second column of the UD structures (see Figure 1) separated by a whitespace. 3 Evaluation Methods 3.1 Automatic methods We used BLEU, NIST, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems. BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences. We use the smoothed version and report results for n = 4. NIST13 is a related n-gram similarity metric 13 http://www.itl.nist.gov/iad/mig/ tests/mt/doc/ngram-study.pdf; http:// www.itl.nist.gov/iad/mig/tests/mt/2009/ weighted in favor of less frequent n-grams which are taken to be more informative. DIST starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn t"
D19-6301,N18-1202,0,0.019743,"however, permissible. For example, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material, and publicly available off-the3 In the case of one team, we agreed to move the two week window between test data release and submission to one week earlier. 4 At SR’18, there were ten languages from five families. 5 https://www.aclweb.org/portal/ content/sigmorphon-shared-task-2019 6 universaldependencies.org shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013) or BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). Datasets were created for 11 languages in the Shallow Track, and for three of those languages, namely English, French and Spanish, in the Deep Track. As in 2018, Shallow Track inputs were generated with the aid of Python scripts from the original UD structures, this time using all available input sentences. Deep Track inputs were then generated by automatically processing the Shallow Track"
D19-6301,D19-6312,0,0.0504468,"els use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search and then combines them into a full projective tree; the completion model generates absent function words in a sequential way given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert the lemma to word form character by character; the contraction model predicts BIO tags to group the words to be contracted, and then generate the contracted word form of each group with a seq2seq model. The LORIA submission (Shimorina and Gardent, 2019) presents a modular approach to surface realisation with three subsequent steps: word ordering, morphological inflection, and contraction generation (for some languages). For word ordering, the data is delexicalised, the input tree is linearised, and the mapping between an input tree and output lemma sequence is learned using a factored sequence-to-sequence model. Morphological inflection makes use of a neural characterbased model, which produces word forms based on lemmas coupled with morphological features; finally, a rule-based contraction generation module is applied for some languages. Th"
D19-6301,K18-2011,1,0.820368,"Missing"
D19-6301,D19-6309,0,0.0269112,"rface realisation with three subsequent steps: word ordering, morphological inflection, and contraction generation (for some languages). For word ordering, the data is delexicalised, the input tree is linearised, and the mapping between an input tree and output lemma sequence is learned using a factored sequence-to-sequence model. Morphological inflection makes use of a neural characterbased model, which produces word forms based on lemmas coupled with morphological features; finally, a rule-based contraction generation module is applied for some languages. The OSU-FB pipeline for generation (Upasani et al., 2019) starts by generating inflected word forms in the tree using character seq2seq models. These inflected syntactic trees are then linearised as constituent trees by converting the relations to non-terminals. The linearised constituent trees are fed to seq2seq models (including models with copy and with tree-LSTM encoders) whose outputs also contain tokens marking the tree structure. N-best outputs are obtained for orderings and the highest confidence output sequence with a valid tree is chosen (i.e, one where the input and output trees are isomorphic up to sibling order, ensuring projectivity)."
D19-6301,D19-6306,0,0.0857639,"weighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions approximating productive inflectional paradigms. The DipInfoUnito realiser (Mazzei and Basile, 2019) is a supervised statistical system for surface realisation, in which two neural network-based models run in parallel on the same input structure, namely a list-wise learning to rank network for linearisation and a seq2seq network for morphology inflection prediction. IMS (Yu et al., 2019) uses a pipeline approach for both tracks, consisting of linearisation, completion (for T2 only), inflection, and contraction. All models use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search and then combines them into a full projective tree; the completion model generates absent function words in a sequential way given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert the lemma to word form character by character; the contraction model predicts BIO tags to g"
E12-1009,W06-2922,0,0.0623484,"a set which fits to the elements composed by the dy79 namic programming approach. This is a trade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodr´ıguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner"
E12-1009,W09-1210,1,0.861553,"Missing"
E12-1009,C10-1011,1,0.415151,"the position of the nodes relative to the other nodes of the part and a factor identifier. Training. For the training of our parser, we use a variant of the perceptron algorithm that uses the Passive-Aggressive update function, cf. (Freund and Schapire, 1998; Collins, 2002; Crammer et al., 2006). The Passive-Aggressive perceptron uses an aggressive update strategy by modifying the weight vector by as much as needed to classify correctly the current example, cf. (Crammer et al., 2006). We apply a random function (hash function) to retrieve the weights from the weight vector instead of a table. Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionaly negative features. Ganchev and Dredze (2008) used 82 this technique for structured prediction in NLP to reduce the needed space, cf. (Shi et al., 2009). We use as weight vector size 800 million. After the training, we counted 65 millions non zero weights for English (penn2malt), 83 for Czech and 87 millions for German. The feature vectors are the union of features originating from the transition sequence of a sentence and the features of the factors over all edges of a dependency tree (e.g. G2a ,"
E12-1009,W08-2102,0,0.0715818,"Missing"
E12-1009,D07-1101,0,0.438734,"et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodr´ıguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introduced a dynamic programming algorithm to solve this problem efficiently. Carreras (2007) introduced the left-most and right-most grandchild as factors. We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. We extend Carreras (2007) graphbased model with factors involving three edges similar to that of Koo and Collins (2010). 3 Transition-based Parser with a Beam This section specifies the transition-based beamsearch parser underlying the combined approach more formally. Sec. 4 will discuss the graphbased scoring model that we are adding. The input to the parser is a word string x, the goal is to find the optimal set y of labeled edges xi"
E12-1009,P04-1015,0,0.430939,"umoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early update strategy of Collins and Roark (2004) to dependency parsing. In this approach, the parser stops and updates the model when the oracle transition sequence drops out of the beam. In contrast to most other approaches, the training procedure of Zhang and Clark (2008) takes the complete transition sequence into account as it is calculating the update. Zhang and Clark compare aspects of transition-based and graph-based parsing, and end up using a transition-based parser with a combined transition-based/second-order graph-based scoring model (Zhang and Clark, 2008, 567), which is similar to the approach we describe in this paper. Howeve"
E12-1009,W02-1001,0,0.0950514,"hree-, and four-grams for the first order and second order. The algorithm includes these features only the words left and right do not overlap with the factor (e.g. the head, dependent, etc.). We use feature extraction procedure for second order, and third order factors. Each feature extracted in this procedure includes information about the position of the nodes relative to the other nodes of the part and a factor identifier. Training. For the training of our parser, we use a variant of the perceptron algorithm that uses the Passive-Aggressive update function, cf. (Freund and Schapire, 1998; Collins, 2002; Crammer et al., 2006). The Passive-Aggressive perceptron uses an aggressive update strategy by modifying the weight vector by as much as needed to classify correctly the current example, cf. (Crammer et al., 2006). We apply a random function (hash function) to retrieve the weights from the weight vector instead of a table. Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionaly negative features. Ganchev and Dredze (2008) used 82 this technique for structured prediction in NLP to reduce the needed space, cf. (Shi et al., 2009). We use a"
E12-1009,C96-1058,0,0.408336,"ed features and show that the completion model leads to a substantial increase in accuracy. We apply the new transition-based parser on typologically different languages such as English, Chinese, Czech, and German and report competitive labeled and unlabeled attachment scores. 1 Introduction Background. A considerable amount of recent research has gone into data-driven dependency parsing, and interestingly throughout the continuous process of improvements, two classes of parsing algorithms have stayed at the centre of attention, the transition-based (Nivre, 2003) vs. the graph-based approach (Eisner, 1996; McDonald et al., 2005).1 The two approaches apply fundamentally different strategies to solve the task of finding the optimal labeled dependency tree over the words of an input sentence (where supervised machine learning is used to estimate the scoring parameters on a treebank). The transition-based approach is based on the conceptually (and cognitively) compelling idea 1 More references will be provided in sec. 2. that machine learning, i.e., a model of linguistic experience, is used in exactly those situations when there is an attachment choice in an otherwise deterministic incremental lef"
E12-1009,W08-0804,0,0.0157914,", we use a variant of the perceptron algorithm that uses the Passive-Aggressive update function, cf. (Freund and Schapire, 1998; Collins, 2002; Crammer et al., 2006). The Passive-Aggressive perceptron uses an aggressive update strategy by modifying the weight vector by as much as needed to classify correctly the current example, cf. (Crammer et al., 2006). We apply a random function (hash function) to retrieve the weights from the weight vector instead of a table. Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionaly negative features. Ganchev and Dredze (2008) used 82 this technique for structured prediction in NLP to reduce the needed space, cf. (Shi et al., 2009). We use as weight vector size 800 million. After the training, we counted 65 millions non zero weights for English (penn2malt), 83 for Czech and 87 millions for German. The feature vectors are the union of features originating from the transition sequence of a sentence and the features of the factors over all edges of a dependency tree (e.g. G2a , etc.). To prevent over-fitting, we use averaging to cope with this problem, cf. (Freund and Schapire, 1998; Collins, 2002). We calculate the e"
E12-1009,W09-1205,0,0.06999,"Missing"
E12-1009,N10-1115,0,0.0312009,"transition and the beam has to be sorted. The parser sorts the beam when it exceeds the maximal beam size, in order to discard superfluous parses or when the parsing algorithm terminates in order to select the best parse tree. The complexity of the transition-based parser is quadratic due to swap operation in the worse case, which is rare, and O(n) in the best case, cf. (Nivre, 2009). The beam size B is constant. Hence, the complexity is in the worst case O(n2 ). The parsing time is to a large degree determined by the feature extraction, the score calculation and the implementation, cf. also (Goldberg and Elhadad, 2010). The transition-based parser is able to parse 30 sentences per second. The parser with completion model processes about 5 sentences per second with a beam size of 80. Note, we use a rich feature set, a completion model with third order factors, negative features, and a large beam. 3 We implemented the following optimizations: (1) We use a parallel feature extraction for the beam elements. Each process extracts the features, scores the possible transitions and computes the score of the completion model. After the extension step, the beam is sorted and the best elements are selected according t"
E12-1009,P10-1151,0,0.0272703,"Missing"
E12-1009,W09-1201,0,0.0368525,"Missing"
E12-1009,P10-1110,0,0.656844,"th a combined transition-based/second-order graph-based scoring model (Zhang and Clark, 2008, 567), which is similar to the approach we describe in this paper. However, their approach does not involve beam rescoring as the partial structures built by the transition-based parser are subsequently augmented; hence, there are cases in which our approach is able to differentiate based on higher-order factors that go unnoticed by the combined model of (Zhang and Clark, 2008, 567). One step beyond the use of a beam is a dynamic programming approach to carry out a full search in the state space, cf. (Huang and Sagae, 2010; Kuhlmann et al., 2011). However, in this case one has to restrict the employed features to a set which fits to the elements composed by the dy79 namic programming approach. This is a trade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre a"
E12-1009,W06-2930,0,0.248217,"issue and solution strategy. In order to preserve the conceptual (and complexity) advantages of the transition-based strategy, the integrated algorithm we are looking for has to be transition-based at the top level. The advantages of the graph-based approach – a more globally informed basis for the decision among different attachment options – have to be included as part of the scoring procedure. As a prerequisite, our algorithm will require a memory for storing alternative analyses among which to choose. This has been previously introduced in transitionbased approaches in the form of a beam (Johansson and Nugues, 2006): rather than representing only the best-scoring history of transitions, the k best-scoring alternative histories are kept around. As we will indicate in the following, the mere addition of beam search does not help overcome a representational key issue of transition-based parsing: in many situations, a transition-based parser is forced to make an attachment decision for a given input word at a point where no or only partial information about the word’s own dependents (and further decendents) is available. Figure 1 illustrates such a case. Figure 1: The left set of brackets indicates material"
E12-1009,W08-2123,0,0.0286038,"Missing"
E12-1009,P98-1106,0,0.115036,", cf. (Huang and Sagae, 2010; Kuhlmann et al., 2011). However, in this case one has to restrict the employed features to a set which fits to the elements composed by the dy79 namic programming approach. This is a trade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodr´ıguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006)."
E12-1009,P10-1001,0,0.195548,"l known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introduced a dynamic programming algorithm to solve this problem efficiently. Carreras (2007) introduced the left-most and right-most grandchild as factors. We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. We extend Carreras (2007) graphbased model with factors involving three edges similar to that of Koo and Collins (2010). 3 Transition-based Parser with a Beam This section specifies the transition-based beamsearch parser underlying the combined approach more formally. Sec. 4 will discuss the graphbased scoring model that we are adding. The input to the parser is a word string x, the goal is to find the optimal set y of labeled edges xi →l xj forming a dependency tree over x ∪{root}. We characterize the state of a transitionbased parser as πi =hσi , βi , yi , hi i, πi ∈ Π, the set of possible states. σi is a stack of words from x that are still under consideration; βi is the input buffer, the suffix of x yet to"
E12-1009,P08-1068,0,0.0443972,"Missing"
E12-1009,W02-2016,0,0.0284448,"drops out of the beam before complete information becomes available. But as our experiments show, this does not seem to be a serious issue empirically. parser requires only one training phase (without jackknifing) and it uses only a single transitionbased decoder. The structure of this paper is as follows. In Section 2, we discuss related work. In Section 3, we introduce our transition-based parser and in Section 4 the completion model as well as the implementation of third order models. In Section 5, we describe experiments and provide evaluation results on selected data sets. 2 Related Work Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early"
E12-1009,P11-1068,0,0.0308158,"Missing"
E12-1009,D10-1004,0,0.0466354,"Missing"
E12-1009,E06-1011,0,0.45361,"parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodr´ıguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introduced a dynamic programming algorithm to solve this problem efficiently. Carreras (2007) introduced the left-most and right-most grandchild as factors. We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. We extend Carreras (2007) graphbased model with factors involving three edges similar to that of Koo and Collins (2010). 3 Transition-based Parser with a Beam This section specifies the transition-based beamsearch parser u"
E12-1009,P05-1012,0,0.726541,"d show that the completion model leads to a substantial increase in accuracy. We apply the new transition-based parser on typologically different languages such as English, Chinese, Czech, and German and report competitive labeled and unlabeled attachment scores. 1 Introduction Background. A considerable amount of recent research has gone into data-driven dependency parsing, and interestingly throughout the continuous process of improvements, two classes of parsing algorithms have stayed at the centre of attention, the transition-based (Nivre, 2003) vs. the graph-based approach (Eisner, 1996; McDonald et al., 2005).1 The two approaches apply fundamentally different strategies to solve the task of finding the optimal labeled dependency tree over the words of an input sentence (where supervised machine learning is used to estimate the scoring parameters on a treebank). The transition-based approach is based on the conceptually (and cognitively) compelling idea 1 More references will be provided in sec. 2. that machine learning, i.e., a model of linguistic experience, is used in exactly those situations when there is an attachment choice in an otherwise deterministic incremental left-to-right parsing proce"
E12-1009,P08-1108,0,0.06918,"of larger spans are more cumbersome to integrate into the model (since the combination algorithm has to be adjusted), in particular for third-order factors or higher. Empirically, i.e., when applied in supervised machine learning experiments based on existing treebanks for various languages, both strategies (and further refinements of them not mentioned here) turn out roughly equal in their capability of picking up most of the relevant patterns well; some subtle strengths and weaknesses are complementary, such that stacking of two parsers representing both strategies yields the best results (Nivre and McDonald, 2008): in training and application, one of the parsers is run on each sentence prior to the other, providing additional feature information for the other parser. Another successful technique to combine parsers is voting as carried out by Sagae and Lavie (2006). The present paper addresses the question if and how a more integrated combination of the strengths of the two strategies can be achieved and implemented efficiently to warrant competitive results. The main issue and solution strategy. In order to preserve the conceptual (and complexity) advantages of the transition-based strategy, the integr"
E12-1009,P05-1013,0,0.122173,"e, 2010; Kuhlmann et al., 2011). However, in this case one has to restrict the employed features to a set which fits to the elements composed by the dy79 namic programming approach. This is a trade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodr´ıguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser"
E12-1009,W09-3811,0,0.03112,"rade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodr´ıguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introduced a dynamic programming algorithm to solve this problem efficiently. Carreras (2007"
E12-1009,W03-3017,0,0.506576,"tion that allows for the use of sophisticated features and show that the completion model leads to a substantial increase in accuracy. We apply the new transition-based parser on typologically different languages such as English, Chinese, Czech, and German and report competitive labeled and unlabeled attachment scores. 1 Introduction Background. A considerable amount of recent research has gone into data-driven dependency parsing, and interestingly throughout the continuous process of improvements, two classes of parsing algorithms have stayed at the centre of attention, the transition-based (Nivre, 2003) vs. the graph-based approach (Eisner, 1996; McDonald et al., 2005).1 The two approaches apply fundamentally different strategies to solve the task of finding the optimal labeled dependency tree over the words of an input sentence (where supervised machine learning is used to estimate the scoring parameters on a treebank). The transition-based approach is based on the conceptually (and cognitively) compelling idea 1 More references will be provided in sec. 2. that machine learning, i.e., a model of linguistic experience, is used in exactly those situations when there is an attachment choice in"
E12-1009,P09-1040,0,0.475175,"s to the elements composed by the dy79 namic programming approach. This is a trade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodr´ıguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introdu"
E12-1009,N06-2033,0,0.0409521,"ng treebanks for various languages, both strategies (and further refinements of them not mentioned here) turn out roughly equal in their capability of picking up most of the relevant patterns well; some subtle strengths and weaknesses are complementary, such that stacking of two parsers representing both strategies yields the best results (Nivre and McDonald, 2008): in training and application, one of the parsers is run on each sentence prior to the other, providing additional feature information for the other parser. Another successful technique to combine parsers is voting as carried out by Sagae and Lavie (2006). The present paper addresses the question if and how a more integrated combination of the strengths of the two strategies can be achieved and implemented efficiently to warrant competitive results. The main issue and solution strategy. In order to preserve the conceptual (and complexity) advantages of the transition-based strategy, the integrated algorithm we are looking for has to be transition-based at the top level. The advantages of the graph-based approach – a more globally informed basis for the decision among different attachment options – have to be included as part of the scoring pro"
E12-1009,D09-1058,0,0.0832753,"Missing"
E12-1009,W07-2218,0,0.100726,"xperiments and provide evaluation results on selected data sets. 2 Related Work Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early update strategy of Collins and Roark (2004) to dependency parsing. In this approach, the parser stops and updates the model when the oracle transition sequence drops out of the beam. In contrast to most other approaches, the training procedure of Zhang and Clark (2008) takes the complete transition sequence into account as it is calculating the update. Zhang and Clark compare aspects of transition-based and graph-based parsing, and end up using a transition-based parser with a combined transition-based/second-order"
E12-1009,W03-3023,0,0.815496,"omplete information becomes available. But as our experiments show, this does not seem to be a serious issue empirically. parser requires only one training phase (without jackknifing) and it uses only a single transitionbased decoder. The structure of this paper is as follows. In Section 2, we discuss related work. In Section 3, we introduce our transition-based parser and in Section 4 the completion model as well as the implementation of third order models. In Section 5, we describe experiments and provide evaluation results on selected data sets. 2 Related Work Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early update strategy of Collins and"
E12-1009,D08-1059,0,0.846364,"tion results on selected data sets. 2 Related Work Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early update strategy of Collins and Roark (2004) to dependency parsing. In this approach, the parser stops and updates the model when the oracle transition sequence drops out of the beam. In contrast to most other approaches, the training procedure of Zhang and Clark (2008) takes the complete transition sequence into account as it is calculating the update. Zhang and Clark compare aspects of transition-based and graph-based parsing, and end up using a transition-based parser with a combined transition-based/second-order graph-based scoring mode"
E12-1009,P11-2033,0,0.688368,"n, w is the corresponding weight vector. In order to add the factor of Figure 4 to our model, we have to add the scoring function (2a) the sum: (2b) scoreG2b (x, y) = scoreG2a (x, y) P + (h,c,cmi)∈y w · fgra (x,h,c,cmi) In order to build a scoring function for combination of the factors shown in Figure 5 to 7, we have to add to the equation 2b one or more of the following sums: (3a) P (3b) P w · fgra (x,h,c,cm1,cm2) (3c) P w · fgra (x,h,c,cmo,tmo) (h,c,ch1,ch2)∈y (h,c,cm1,cm2)∈y (h,c,cmo,tmo)∈y w · fgra (x,h,c,ch1,ch2) Feature Set. The feature set of the transition model is similar to that of Zhang and Nivre (2011). In addition, we use the cross product of morphologic features between the head and the dependent since we apply also the parser on morphologic rich languages. The feature sets of the completion model described above are mostly based on previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). The models denoted with + use all combinations of words before and after the head, dependent, sibling, grandchilrden, etc. These are respectively three-, and four-grams for the first order and second order. The algorithm includes these features only the wo"
E12-1009,C98-1102,0,\N,Missing
I13-1123,H05-1066,0,0.0382647,"Missing"
I13-1123,C12-1007,0,0.114532,"of state-of-theart parsers. We trained different systems over a large Spanish treebank and tested them over a treebank from a different domain. Our experiments show that though the obtained LAS is high, the performance over some concrete labels is very low in all cases, limiting the usability of the parsed data for tasks than rely on label-specific accuracy. One important future line is to look for parser modifications that allow the system to perform better in the labels we are interested in. To do so, one idea would be to use semantic features to give more information to the parser like in (Agirre et al., 2012). We did some preliminary experiments in that line, using information about the semantic classes for common nouns (specifically for the classes human and location), but the results showed that this information did not lead to a better performance of the parser. This is probably due to the sparsity of this information, but it is still an interesting line to study, since it lead to good results in other cases (Agirre et al., 2012). 4.1 Specific Label Performance The results obtained in terms of LAS are very satisfactory (the best parsing results reported for Spanish so far), specially for Mate p"
I13-1123,ballesteros-nivre-2012-maltoptimizer-system,1,0.843917,"3 tokens 471,624 114,610 41,620 Table 1: Sizes of the used corpora Experiments 2 sentences 33,679 8,125 3,376 MaltParser (Nivre and Hall, 2005; Nivre et al., 2007b) is a transition-based dependency parser generator. It was one of the best parsers in the CoNLL Shared Tasks in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007a) and it contains four different families of transition-based parsers. A transition-based parser is based on an automaton that performs shift-reduce operations, whose transitions manage the input words in order to assign dependencies between them. MaltOptimizer (Ballesteros and Nivre, 2012) is a system designed to optimize MaltParser models by implementing a search of parameters and feature specifications. MaltOptimizer takes a training set in CoNLL data format,4 and provides an optimal configuration that includes the best parsing algorithm, parsing parameters and a complex feature model over the data structures involved in the parsing process. MaltOptimizer searches the optimal model maximizing the score of a single evaluation measure, either LAS, LCM (Labeled complete match) or unlabeled evaluation measures. As we mentioned in section 2, our intention is to enhance the perform"
I13-1123,W06-2932,0,0.0123438,"zer modifications that we explained in section 3.2.1. This therefore confirms our expectations that optimizing over very frequent labels may improve the overall accuracy6 . The algorithm selected by MaltOptimizer was Covington non-projective, which is described by Covington (2001) and included in MaltParser by Nivre (2008). For the Mate parsers, we used the default training settings for the graph-based parser. For the transition-based parser, we used a beam size of 40 and 25 training iterations. 4 Results and Discussion MSTParser is an arc-factored spanning tree parser (McDonald et al., 2005; McDonald et al., 2006). It implements a graph-based second-order parsing model which scores all possible dependency arcs in the sentence and then extracts the dependency tree with the highest score. The score of the trees is calculated basically adding the score of every arc, having at the end a sum with the score of the whole tree. 3.2.3 Joint tagger Transition-based parser and Graph-based parser with Hash Kernel and Beam Search- Mate tools The Mate-Tools provide two parser types: a graphbased (Bohnet, 2010) and a transition-based parser with graph-based re-scoring (completion model) that is able to perform joint"
I13-1123,E12-1009,1,0.806962,"-order parsing model which scores all possible dependency arcs in the sentence and then extracts the dependency tree with the highest score. The score of the trees is calculated basically adding the score of every arc, having at the end a sum with the score of the whole tree. 3.2.3 Joint tagger Transition-based parser and Graph-based parser with Hash Kernel and Beam Search- Mate tools The Mate-Tools provide two parser types: a graphbased (Bohnet, 2010) and a transition-based parser with graph-based re-scoring (completion model) that is able to perform joint PoS tagging and dependency parsing (Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012). We refer to the graphbased parser as Mate-G, to the transition-based parser without graph-based re-scoring as MateT, and to the transition-based parser with enabled graph-based completion model to Mate-C. These parsers benefit from a passive-aggressive perceptron algorithm implemented as a Hash Kernel, which makes the parser fast to train and improves accuracy. Mate-T provides joint PoS tagging and dependency parsing to give account for the interaction between morphology and syntax. It uses a beam search over the space of possible transitions and Parser Malt MST Mate"
I13-1123,P08-3010,0,0.0604335,"Missing"
I13-1123,D12-1133,1,0.715122,"ich scores all possible dependency arcs in the sentence and then extracts the dependency tree with the highest score. The score of the trees is calculated basically adding the score of every arc, having at the end a sum with the score of the whole tree. 3.2.3 Joint tagger Transition-based parser and Graph-based parser with Hash Kernel and Beam Search- Mate tools The Mate-Tools provide two parser types: a graphbased (Bohnet, 2010) and a transition-based parser with graph-based re-scoring (completion model) that is able to perform joint PoS tagging and dependency parsing (Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012). We refer to the graphbased parser as Mate-G, to the transition-based parser without graph-based re-scoring as MateT, and to the transition-based parser with enabled graph-based completion model to Mate-C. These parsers benefit from a passive-aggressive perceptron algorithm implemented as a Hash Kernel, which makes the parser fast to train and improves accuracy. Mate-T provides joint PoS tagging and dependency parsing to give account for the interaction between morphology and syntax. It uses a beam search over the space of possible transitions and Parser Malt MST Mate-T Mate-C Mate-G IULA Tes"
I13-1123,C10-1011,1,0.806568,"4 Results and Discussion MSTParser is an arc-factored spanning tree parser (McDonald et al., 2005; McDonald et al., 2006). It implements a graph-based second-order parsing model which scores all possible dependency arcs in the sentence and then extracts the dependency tree with the highest score. The score of the trees is calculated basically adding the score of every arc, having at the end a sum with the score of the whole tree. 3.2.3 Joint tagger Transition-based parser and Graph-based parser with Hash Kernel and Beam Search- Mate tools The Mate-Tools provide two parser types: a graphbased (Bohnet, 2010) and a transition-based parser with graph-based re-scoring (completion model) that is able to perform joint PoS tagging and dependency parsing (Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012). We refer to the graphbased parser as Mate-G, to the transition-based parser without graph-based re-scoring as MateT, and to the transition-based parser with enabled graph-based completion model to Mate-C. These parsers benefit from a passive-aggressive perceptron algorithm implemented as a Hash Kernel, which makes the parser fast to train and improves accuracy. Mate-T provides joint PoS tagging and depend"
I13-1123,A97-1052,0,0.0979151,"Missing"
I13-1123,W06-2920,0,0.143429,"periments using IULA Spanish LSP Treebank2 (Marimon et al., 2012). This corpus (henceforth IULA) contains the syntactic annotation of 42,000 sentences (around 590,000 tokens) taken from domain-specific (technical literature) texts. We used the train and test partitions provided by the Treebank developers which are publicly available for replicability.3 Furthermore, we used the Tibidabo Treebank (Marimon, 2010) as an alternative test set. Tibidabo contains a set of sentences extracted from the Ancora corpus (Taul´e et al., 2008), which was used in the CoNLL-X Shared Task of dependency parsing (Buchholz and Marsi, 2006). The Tibidabo Treebank was annotated using the same guidelines as IULA Treebank. Therefore, it has the same functions and tag-set as IULA, but since the sentences come from a completely different corpus, it represents a good evaluation frame with regards to the influence of domain change. In summary, we used a training set to train the different models and two different test sets to evaluate each model. See table 1 for details about the size of the different partitions. The treebanks used in this work contain up to 25 different dependency relations. In this work we will pay special attention"
I13-1123,J08-4003,0,0.0133764,"are those obtained with the configuration that leads to better LAS results over IULA Test. The same configuration is used to annotate Tibidabo. For MaltParser, the best LAS was obtained when optimizing the parser for the DO dependency label, which was obtained by applying the MaltOptimizer modifications that we explained in section 3.2.1. This therefore confirms our expectations that optimizing over very frequent labels may improve the overall accuracy6 . The algorithm selected by MaltOptimizer was Covington non-projective, which is described by Covington (2001) and included in MaltParser by Nivre (2008). For the Mate parsers, we used the default training settings for the graph-based parser. For the transition-based parser, we used a beam size of 40 and 25 training iterations. 4 Results and Discussion MSTParser is an arc-factored spanning tree parser (McDonald et al., 2005; McDonald et al., 2006). It implements a graph-based second-order parsing model which scores all possible dependency arcs in the sentence and then extracts the dependency tree with the highest score. The score of the trees is calculated basically adding the score of every arc, having at the end a sum with the score of the w"
I13-1123,W09-3828,0,0.0462243,"Missing"
I13-1123,taule-etal-2008-ancora,0,0.0494356,"Missing"
I13-1123,marimon-etal-2012-iula,0,0.175262,"Missing"
I13-1123,D07-1013,0,0.0432006,"Missing"
I13-1123,D07-1096,0,\N,Missing
I13-1178,W05-0307,0,0.0554544,"Missing"
I13-1178,W02-1001,0,0.166923,"Missing"
I13-1178,P04-1015,0,0.0242056,"the root node n is contained in the stack Σ: s = ([0], [], Vc , Z, E, δ, x). Figure 2 shows the possible transitions. As features of the transition-based system, we use a rich feature set based on the dependency structure drawn from (Zhang and Nivre, 2011) (since we use as input a dependency structure these features are available). In addition, we use the path from the top stack element to the word of the last open bracket (as sequence of pos tags). For the training of the transition-based system, we use the perception algorithm with averaging, a beamsearch with 10 elements and early update (Collins and Roark, 2004). The oracle for training of the system follows the bottom-up parsing strategy. As soon as the communicative part is completed, we remove (reduce) the nodes that belong to it from the stack. Figure 3 shows a sequence of transitions that the analyser performs to create the Tc of the example sentence in Figure 1. 4 Experiments Following the criteria in Section 2, four annotators in teams of two manually annotated a fragment of 435 sentences of the PTB with the thematicity structure, in a series of blocks of about 40–50 sentences. To ensure high mutual agreement, the annotation procedure went as"
I13-1178,W04-2407,0,0.0139458,"cative tree Tc of a sentence x = w1 ...wn is a quintuple Tc = (V, E, L, δ, 00 ), such that V = Vt ∪ Vc is a set of nodes, with Vt = 0, ..., n as a set of terminal nodes and Vc = o0 , 10 , ..., m0 as a set of non-terminal communicative (label) nodes; E ⊆ V × V is a set of edges; L is the set of communicative labels (in the case of thematicity: SP, T, R and P); δ : E → L is a labeling function for nodes; 00 is the root node. That is, we interprete the Tc as a kind of constituency tree. For the implementation of the parser, we use the idea of transition-based parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004), which uses a classifier to predict the shift/reduce actions. We draw upon the transition set of the arc-eager parser Nivre (2004), but with a slightly different semantics in that we define a transition system for the derivation of the Tc as a quadruple C = (S, Y, c0 , Sy ), where S is a set of parsing states; Y is a set of transitions, each of which is a (partial) function t: S → S; s0 is an initialization function that maps a sentence x to a configuration s ∈ S; and Sy ⊆ S is a set of terminal states. A transition sequence for a sentence x in C is a sequence of pairs of states and transitio"
I13-1178,W04-0308,0,0.02419,", n as a set of terminal nodes and Vc = o0 , 10 , ..., m0 as a set of non-terminal communicative (label) nodes; E ⊆ V × V is a set of edges; L is the set of communicative labels (in the case of thematicity: SP, T, R and P); δ : E → L is a labeling function for nodes; 00 is the root node. That is, we interprete the Tc as a kind of constituency tree. For the implementation of the parser, we use the idea of transition-based parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004), which uses a classifier to predict the shift/reduce actions. We draw upon the transition set of the arc-eager parser Nivre (2004), but with a slightly different semantics in that we define a transition system for the derivation of the Tc as a quadruple C = (S, Y, c0 , Sy ), where S is a set of parsing states; Y is a set of transitions, each of which is a (partial) function t: S → S; s0 is an initialization function that maps a sentence x to a configuration s ∈ S; and Sy ⊆ S is a set of terminal states. A transition sequence for a sentence x in C is a sequence of pairs of states and transitions. As set S of states, we use the tuple s = (Σ, B, Vc , Z, E, δ, o), where the stack Σ and the input buffer B are disjoint sublist"
I13-1178,J05-1004,0,0.0166827,"tion, TFA (Sgall, 1967) in the Prague School and Communicative Structure, CommStr (Mel’ˇcuk, 2001) in the Meaning-Text Theory) determines the “communicative” segmentation of the meaning of an utterance. This makes it central to the semantics–syntax–intonation interface (Lambrecht, 1994; Hajiˇcov´a et al., 1998; Steedman, 2000; Mel’ˇcuk, 2001; Erteschik-Shir, 2007) and therefore also to NLP. However, despite its prominence, IS has been largely ignored so far in the context of the reference treebanks for data-driven NLP: Penn Treebank (Marcus et al., 1993) and its semantic counterpart PropBank (Palmer et al., 2005) for English, Tiger (Thielen et al., 1999) for German, Ancora (Taul´e et al., 2008) for Spanish, etc. To the best of our knowledge, only the Prague Leo Wanner ICREA and DTIC Pompeu Fabra University Barcelona, Spain leo.wanner@upf.edu Dependency Treebank (PDT) (Hajiˇc et al., 2006) is annotated with IS in terms of TFA. This is not to say that no proposals have been made for the annotation of IS in general; see, e.g., (Calhoun et al., 2005) for English, (Dipper et al., 2004) for German, (Paggio, 2006) for Danish, etc. However, in the light of the above mentioned interface, it is crucial to have"
I13-1178,paggio-2006-annotating,0,0.060546,"Missing"
I13-1178,H05-1002,0,0.0480223,"Missing"
I13-1178,taule-etal-2008-ancora,0,0.0364344,"Missing"
I13-1178,W03-3023,0,0.0455217,"given sentence. The communicative tree Tc of a sentence x = w1 ...wn is a quintuple Tc = (V, E, L, δ, 00 ), such that V = Vt ∪ Vc is a set of nodes, with Vt = 0, ..., n as a set of terminal nodes and Vc = o0 , 10 , ..., m0 as a set of non-terminal communicative (label) nodes; E ⊆ V × V is a set of edges; L is the set of communicative labels (in the case of thematicity: SP, T, R and P); δ : E → L is a labeling function for nodes; 00 is the root node. That is, we interprete the Tc as a kind of constituency tree. For the implementation of the parser, we use the idea of transition-based parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004), which uses a classifier to predict the shift/reduce actions. We draw upon the transition set of the arc-eager parser Nivre (2004), but with a slightly different semantics in that we define a transition system for the derivation of the Tc as a quadruple C = (S, Y, c0 , Sy ), where S is a set of parsing states; Y is a set of transitions, each of which is a (partial) function t: S → S; s0 is an initialization function that maps a sentence x to a configuration s ∈ S; and Sy ⊆ S is a set of terminal states. A transition sequence for a sentence x in C is a sequence of pairs of"
I13-1178,J93-2004,0,0.0421149,"Missing"
I13-1178,P11-2033,0,0.026146,"communicative (label) nodes, Z is the stack of communicative nodes, E is the set of edges, δ is a labeling function for communicative label nodes n ∈ Vc , and o is a counter for the number of pairs of delimitation brackets. The initial state for a sentence x is s0 = ([0], [1, ..., n], {00 }, [00 ], {}, δ, 0). Terminal configurations have an empty buffer and only the root node n is contained in the stack Σ: s = ([0], [], Vc , Z, E, δ, x). Figure 2 shows the possible transitions. As features of the transition-based system, we use a rich feature set based on the dependency structure drawn from (Zhang and Nivre, 2011) (since we use as input a dependency structure these features are available). In addition, we use the path from the top stack element to the word of the last open bracket (as sequence of pos tags). For the training of the transition-based system, we use the perception algorithm with averaging, a beamsearch with 10 elements and early update (Collins and Roark, 2004). The oracle for training of the system follows the bottom-up parsing strategy. As soon as the communicative part is completed, we remove (reduce) the nodes that belong to it from the stack. Figure 3 shows a sequence of transitions t"
I13-1178,W09-1201,0,\N,Missing
K18-2011,C16-1012,0,0.0132499,"OS tagging and morphological features (UFEATS). Treebanks sharing a parsing model grouped together; substitute and proxy treebanks for segmentation, tagging, parsing far right (SPECIAL models detailed in the text). Confidence intervals for coloring: |< µ−σ < |< µ− SE < µ < µ+ SE < |< µ+σ < |. 119 able method for pooling training data both within and across languages. It is also worth noting that this method is easy to use and does not require extra external resources used in most work on multilingual parsing, like multilingual word embeddings (Ammar et al., 2016) or linguistic re-write rules (Aufrant et al., 2016) to achieve good results. coding. Scores that are significantly higher/lower than the mean score of the 21 systems that successfully parsed all test sets are marked with two shades of green/red. The lighter shade marks differences that are outside the interval defined by √the standard error of the mean (µ ± SE, SE = σ/ N ) but within one standard deviation (std dev) from the mean. The darker shade marks differences that are more than one std dev above/below the mean (µ ± σ). Finally, scores that are no longer valid because of the Thai UPOS tagger are crossed out in yellow cells, and corrected"
K18-2011,P82-1020,0,0.805881,"Missing"
K18-2011,P18-1246,1,0.843945,"(2017a). We use the default parameter settings introduced by Shao et al. (2018) and train a segmentation model for all treebanks with at least 50 sentences of training data. For treebanks with less or no training data (except Thai discussed below), we substitute a model for another treebank/language: • For Japanese Modern, Czech PUD, English PUD and Swedish PUD, we use the model trained on the largest treebank from the same language (Japanese GSD, Czech PDT, English EWT and Swedish Talbanken). 4 Tagging and Morphological Analysis We use two separate instantiations of the tagger6 described in Bohnet et al. (2018) to predict UPOS tags and morphological features, respectively. The tagger uses a Meta-BiLSTM over the output of a sentence-based character model and a word model. There are two features that mainly distinguishes the tagger from previous work. The character BiLSTMs use the full context of the sentence in contrast to most other taggers which use words only as context for the character model. This character model is combined with the word model in the Meta-BiLSTM relatively late, after two layers of BiLSTMs. For both the word and character models, we use two layers of BiLSTMs with 300 LSTM cells"
K18-2011,Q17-1010,0,0.119675,"Missing"
K18-2011,W13-4902,1,0.850186,"ogical Features Having a strong morphological analyzer, we were interested in finding out whether or not we can improve parsing accuracy using predicted morphological information. We conducted several experiments on the development sets for a subset of treebanks. However, no experiment gave us any improvement in terms of LAS and we decided not to use this technique for the shared task. What we tried was to create an embedding representing either the full set of morphological features or a subset of potentially useful features, for example case (which has been shown to be useful for parsing by Kapociute-Dzikiene et al. (2013) and Eryigit et al. (2008)), verb form and a few others. That embedding was concatenated to the word embedding at the input of the BiLSTM. We varied the embedding size (10, 20, 30, 40), tried different subsets of morphological features, and tried with and without using dropout on that embedding. We also tried creating an embedding of a concatenation of the universal POS tag and the Case feature and replace the POS embedding with this one. We are currently unsure why none of these experiments were successful and plan to investigate this 9 https://radimrehurek.com/gensim/ An alternative strategy"
K18-2011,D14-1082,0,0.0860011,"pped to universal POS tags and a few morphological features like person, number and gender. For Thai, we annotated about 33,000 sentences from Wikipedia using PyThaiNLP8 and mapped only to UPOS tags (no features). Unfortunately, we realized only after the test phase that PyThaiNLP was not a permitted resource, which invalidates our UPOS tagging scores for Thai, as well as the LAS and MLAS scores which depend on the tagger. Note, however, that the score for morphological features xi = e(wi ) ◦ e(pi ) ◦ BiLSTM(ch1:m ). Here, e(wi ) represents the word embedding and e(pi ) the POS tag embedding (Chen and Manning, 2014); these are concatenated to a character-based vector, obtained by running a BiLSTM over the characters ch1:m of wi . With the aim of training multi-treebank models, we additionally created a variant of the parser which adds a treebank embedding e(tbi ) to input vectors in a spirit similar to the language embeddings of Ammar et al. (2016) and de Lhoneux et al. (2017a): xi = e(wi ) ◦ e(pi ) ◦ BiLSTM(ch1:m ) ◦ e(tbi ). We have previously shown that treebank embeddings provide an effective way to combine multiple monolingual heterogeneous treebanks (Stymne et al., 2018) and applied them to lowreso"
K18-2011,Q16-1032,0,0.158942,"or the PUD treebanks (except Thai), Japanese Modern and Naija NSC, we use the same model substitutions as for segmentation (see Table 2). is not affected, as we did not predict features at all for Thai. The same goes for sentence and word segmentation, which do not depend on the tagger. Lemmas Due to time constraints we chose not to focus on the BLEX metric in this shared task. In order to avoid zero scores, however, we simply copied a lowercased version of the raw token into the lemma column. 5 Dependency Parsing We use a greedy transition-based parser (Nivre, 2008) based on the framework of Kiperwasser and Goldberg (2016b) where BiLSTMs (Hochreiter and Schmidhuber, 1997; Graves, 2008) learn representations of tokens in context, and are trained together with a multi-layer perceptron that predicts transitions and arc labels based on a few BiLSTM vectors. Our parser is extended with a S WAP transition to allow the construction of nonprojective dependency trees (Nivre, 2009). We also introduce a static-dynamic oracle to allow the parser to learn from non-optimal configurations at training time in order to recover better from mistakes at test time (de Lhoneux et al., 2017b). In our parser, the vector representatio"
K18-2011,K17-3022,1,0.72705,"Missing"
K18-2011,Q16-1023,0,0.434449,"or the PUD treebanks (except Thai), Japanese Modern and Naija NSC, we use the same model substitutions as for segmentation (see Table 2). is not affected, as we did not predict features at all for Thai. The same goes for sentence and word segmentation, which do not depend on the tagger. Lemmas Due to time constraints we chose not to focus on the BLEX metric in this shared task. In order to avoid zero scores, however, we simply copied a lowercased version of the raw token into the lemma column. 5 Dependency Parsing We use a greedy transition-based parser (Nivre, 2008) based on the framework of Kiperwasser and Goldberg (2016b) where BiLSTMs (Hochreiter and Schmidhuber, 1997; Graves, 2008) learn representations of tokens in context, and are trained together with a multi-layer perceptron that predicts transitions and arc labels based on a few BiLSTM vectors. Our parser is extended with a S WAP transition to allow the construction of nonprojective dependency trees (Nivre, 2009). We also introduce a static-dynamic oracle to allow the parser to learn from non-optimal configurations at training time in order to recover better from mistakes at test time (de Lhoneux et al., 2017b). In our parser, the vector representatio"
K18-2011,W17-6314,1,0.712346,"Missing"
K18-2011,K17-3002,0,0.0838483,"ainian IU. All in all, the 2018 edition of the Uppsala parser can be characterized as a system that is strong on segmentation (especially word segmentation) and prediction of UPOS tags and morphological features, and where the dependency parsing component performs well in low-resource scenarios thanks to the use of multi-treebank models, both within and across languages. For what it is worth, we also seem to have the highest ranking singleparser transition-based system in a task that is otherwise dominated by graph-based models, in particular variants of the winning Stanford system from 2017 (Dozat et al., 2017). 8 9 Conclusion We have described the Uppsala submission to the CoNLL 2018 shared task, consisting of a segmenter that jointly extracts words and sentences from a raw text, a tagger that provides UPOS tags and morphological features, and a parser that builds a dependency tree given the words and tags of each sentence. For the parser we applied multi-treebank models both monolingually and multilingually, resulting in only 34 models for 82 treebanks as well as significant improvements in parsing accuracy especially for low-resource languages. We ranked 7th for the official LAS and MLAS scores,"
K18-2011,K18-2002,0,0.0159289,"icial LAS and MLAS scores, and first for the unofficial scores on word segmentation, UPOS tagging and morphological features. Acknowledgments We are grateful to the shared task organizers and to Dan Zeman and Martin Potthast in particular, and we acknowledge the computational resources provided by CSC in Helsinki and Sigma2 in Oslo through NeIC-NLPL (www.nlpl.eu). Aaron Smith was supported by the Swedish Research Council. Extrinsic Parser Evaluation In addition to the official shared task evaluation, we also participated in the 2018 edition of the Extrinsic Parser Evaluation Initiative (EPE) (Fares et al., 2018), where parsers developed for the CoNLL 2018 shared task were evaluated with respect to their contribution to three downstream systems: biological event extraction, fine-grained opinion analysis, and negation resolution. The downstream systems are available for English only, and we participated with our English model trained on English EWT, English LinES and English GUM, using English EWT as the proxy. In the extrinsic evaluation, the Uppsala system ranked second for event extraction, first for opinion analysis, and 16th (out of 16 systems) for negation resolution. Our results for the first tw"
K18-2011,J08-4003,1,0.671677,"e adopt three different strategies: • For the PUD treebanks (except Thai), Japanese Modern and Naija NSC, we use the same model substitutions as for segmentation (see Table 2). is not affected, as we did not predict features at all for Thai. The same goes for sentence and word segmentation, which do not depend on the tagger. Lemmas Due to time constraints we chose not to focus on the BLEX metric in this shared task. In order to avoid zero scores, however, we simply copied a lowercased version of the raw token into the lemma column. 5 Dependency Parsing We use a greedy transition-based parser (Nivre, 2008) based on the framework of Kiperwasser and Goldberg (2016b) where BiLSTMs (Hochreiter and Schmidhuber, 1997; Graves, 2008) learn representations of tokens in context, and are trained together with a multi-layer perceptron that predicts transitions and arc labels based on a few BiLSTM vectors. Our parser is extended with a S WAP transition to allow the construction of nonprojective dependency trees (Nivre, 2009). We also introduce a static-dynamic oracle to allow the parser to learn from non-optimal configurations at training time in order to recover better from mistakes at test time (de Lhoneu"
K18-2011,P09-1040,1,0.839321,"task. In order to avoid zero scores, however, we simply copied a lowercased version of the raw token into the lemma column. 5 Dependency Parsing We use a greedy transition-based parser (Nivre, 2008) based on the framework of Kiperwasser and Goldberg (2016b) where BiLSTMs (Hochreiter and Schmidhuber, 1997; Graves, 2008) learn representations of tokens in context, and are trained together with a multi-layer perceptron that predicts transitions and arc labels based on a few BiLSTM vectors. Our parser is extended with a S WAP transition to allow the construction of nonprojective dependency trees (Nivre, 2009). We also introduce a static-dynamic oracle to allow the parser to learn from non-optimal configurations at training time in order to recover better from mistakes at test time (de Lhoneux et al., 2017b). In our parser, the vector representation xi of a word type wi before it is passed to the BiLSTM feature extractors is given by: • For Faroese we used the model for Norwegian Nynorsk, as we believe this to be the most closely related language. • For treebanks with small training sets we use only the provided training sets for training. Since these treebanks do not have development sets, we use"
K18-2011,J93-1004,0,0.0296153,"Missing"
K18-2011,Q18-1030,1,0.830131,"arginally) higher score with the mono-treebank baseline model: Estonian EDT, Russian SynTagRus, Slovenian SSJ, and Turkish IMST. Looking at the aggregate sets, we see that, as expected, the pooling of resources helps most for LOW- RESOURCE (25.33 vs. 17.72) and SMALL (63.60 vs. 60.06), but even for BIG there is some improvement (80.21 vs. 79.61). We find these results very encouraging, as they indicate that our treebank embedding method is a reliFor word segmentation, we obtained the best results overall, strongly outperforming the mean for all groups except SMALL. We know from previous work (Shao et al., 2018) that our word segmenter performs well on more challenging languages like Arabic, Hebrew, Japanese, and Chinese (although we were beaten by the Stanford team for the former two and by the HIT-SCIR team for the latter two). By contrast, it sometimes falls below the mean for the easier languages, but typically only by a very small fraction (for example 99.99 vs. 100.00 for 3 treebanks). Finally, it is worth noting that the maximum-matching segmenter developed specifically for Thai achieved a score of 69.93, which was more than 5 points better than any other system. Our results for UPOS tagging i"
K18-2011,I17-1018,1,0.925902,"ext, without any linguistic annotation, and output full labelled dependency trees for 82 test treebanks covering 46 different languages. Besides the labeled attachment score (LAS) used to evaluate systems in the 2017 edition of the Shared Task (Zeman et al., 2017), this year’s task introduces two new metrics: morphology-aware labeled attachment score (MLAS) and bi-lexical dependency score (BLEX). The Uppsala system focuses exclusively on LAS and MLAS, and consists of a three-step pipeline. The first step is a model for joint sentence and word segmentation which uses the BiRNN-CRF framework of Shao et al. (2017, 2018) to predict sentence and word boundaries in the raw input and Corrigendum: After the test phase was over, we discovered that we had used a non-permitted resource when developing the UPOS tagger for Thai PUD (see Section 4). Setting our LAS, MLAS and UPOS scores to 0.00 for Thai PUD gives the corrected scores: LAS 72.31, MLAS 59.17, UPOS 90.50. This does not affect the ranking for any of the three scores, as confirmed by the shared task organizers. 2 Resources All three components of our system were trained principally on the training sets of Universal Dependencies v2.2 released to coinc"
K18-2011,D18-1291,1,0.828983,"Missing"
K18-2011,P18-2098,1,0.816536,"Missing"
K18-2011,tiedemann-2012-parallel,0,0.122801,"Missing"
K18-2011,C96-1035,0,0.0335101,"Missing"
K18-2011,K18-2001,1,0.844683,"Missing"
K18-2011,P07-2045,0,\N,Missing
K18-2011,J08-4010,1,\N,Missing
K18-2011,J08-3003,1,\N,Missing
N15-1042,P13-2009,0,0.00851502,"ic generator that is able to cope with projection between non-isomorphic structures. The generator, which starts from PropBank-like structures, consists of a cascade of SVM-classifier based submodules that map in a series of transitions the input structures onto sentences. The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered AncoraUPF corpus. 1 Introduction Applications such as machine translation that inherently draw upon sentence generation increasingly deal with deep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and"
N15-1042,2004.tmi-1.14,0,0.0251845,"y avoided. We present a fully stochastic generator that is able to cope with projection between non-isomorphic structures. The generator, which starts from PropBank-like structures, consists of a cascade of SVM-classifier based submodules that map in a series of transitions the input structures onto sentences. The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered AncoraUPF corpus. 1 Introduction Applications such as machine translation that inherently draw upon sentence generation increasingly deal with deep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filipp"
N15-1042,C14-1133,1,0.883628,"ructures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et al., 2012) (where the transfer is done at a deep level). In abstractive summarization, it facilitates the generation of the summaries, and in extractive summarization a better sentence fusion.2 1 The data-driven sentence generator is available for public downloading at https://github.com/ talnsoftware/deepgenerator/wiki. 2 For all of these applications, the deep representation can be obtained by a deep parser, such as, e.g., (Ballesteros et al., 2014a). 387 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 387–397, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier i"
N15-1042,W14-4416,1,0.917886,"ructures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et al., 2012) (where the transfer is done at a deep level). In abstractive summarization, it facilitates the generation of the summaries, and in extractive summarization a better sentence fusion.2 1 The data-driven sentence generator is available for public downloading at https://github.com/ talnsoftware/deepgenerator/wiki. 2 For all of these applications, the deep representation can be obtained by a deep parser, such as, e.g., (Ballesteros et al., 2014a). 387 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 387–397, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier i"
N15-1042,C00-1007,0,0.43582,"eep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a t"
N15-1042,W11-2832,0,0.444251,"SyntSs). While SSyntSs and linearized structures are isomorphic, the difference in the linguistic abstraction of the DSyntSs and SSyntSs leads to divergences that impede the isomorphy between the two and make the first mapping a challenge for statistical generation. Therefore, we focus in this section on 388 the presentation of the DSyntSs and SSyntSs and the mapping between them. 2.1 2.1.1 DSyntSs and SSyntSs Input DSyntSs DSyntSs are very similar to the PropBank (Babko-Malaya, 2005) structures and the structures as used for the deep track of the First Surface Realization Shared Task (SRST, (Belz et al., 2011)) annotations. DSyntSs are connected trees that contain only meaning-bearing lexical items and both predicate-argument (indicated by Roman numbers: I, II, III, IV, . . . ) and lexico-structural, or deepsyntactic, (ATTR(ibutive), APPEND(itive) and COORD(inative)) relations. In other words, they do not contain any punctuation and functional nodes, i.e., governed elements, auxiliaries and determiners. Governed elements such governed prepositions and subordinating conjunctions are dropped because they are imposed by sub-categorization restrictions of the predicative head and void of own meaning— a"
N15-1042,W05-1601,0,0.0234085,"focuses on syntactic generation; see, among others (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or only on linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 395 2009; Guo et al., 2011a). A number of proposals are hybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learning approach."
N15-1042,C10-1012,1,0.761097,"neration still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et al., 2012) (where the transfer is done at a deep level). In abstractive summarization, it facilitates t"
N15-1042,W11-2835,1,0.945822,"projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et"
N15-1042,de-marneffe-etal-2006-generating,0,0.0170839,"Missing"
N15-1042,P07-1041,0,0.125277,"sentations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portabi"
N15-1042,D08-1019,0,0.28662,", 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module rai"
N15-1042,W11-2833,0,0.0446441,"Missing"
N15-1042,W13-2131,0,0.0280973,"th rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learning approach. This generator has been successfully tested on an English and a Spanish corpus, as a stand-alone DSyntS–SSyntS generator and as a part of the generation pipeline. We are currently about to apply it to other languages—including Chinese, French and German. Furthermore, resources are compiled to use it for generation of spoken discourse in Arabic, Polish and Turkish. We believe"
N15-1042,P09-1091,0,0.108813,"their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this pap"
N15-1042,W07-2416,0,0.0989154,"starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier is defined for the mapping of each linguistic category. The generator has been tested on Spanish with the multi-layered Ancora-UPF corpus (Mille et al., 2013) and on English with an extended version of the dependency Penn TreeBank (Johansson and Nugues, 2007). The remainder of the paper is structured as follows. In the next section, we briefly outline the fundamentals of sentence generation as we view it in our work, focusing in particular on the most challenging part of it: the transition between the non-isomorphic predicateargument lexico-structural structures and surfacesyntactic structures. Section 3 outlines the setup of our system. Section 4 discusses the experiments we carried out and the results we obtained. In Section 5, we briefly summarize related work, before in Section 6 some conclusions are drawn and future work is outlined. 2 The Fu"
N15-1042,C12-1083,0,0.0809232,"ent a fully stochastic generator that is able to cope with projection between non-isomorphic structures. The generator, which starts from PropBank-like structures, consists of a cascade of SVM-classifier based submodules that map in a series of transitions the input structures onto sentences. The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered AncoraUPF corpus. 1 Introduction Applications such as machine translation that inherently draw upon sentence generation increasingly deal with deep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008"
N15-1042,P95-1034,0,0.355379,"ans that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov e"
N15-1042,P98-1116,0,0.439226,"to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine"
N15-1042,W02-2103,0,0.194135,"see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and"
N15-1042,P10-1157,0,0.0362809,"Missing"
N15-1042,C12-2082,1,0.839143,"he experiments Spanish Treebank For the validation of the performance of our generator on Spanish, we use the AnCora-UPF treebank, which contains only about 100,000 tokens, but which has been manually annotated and validated on the SSyntS- and DSyntS-layers, such that its quality is rather high. The deep annotation does not contain any functional prepositions since they have been removed for all predicates of the corpus, and the DSyntS-relations have been edited following annotation guidelines. AnCora-UPF SSyntSs are annotated with fine-grained dependencies organized in a hierarchical scheme (Mille et al., 2012), in a similar fashion as the dependencies of the Stanford Scheme (de Marneffe et al., 2006).7 Thus, it is possible to use the full set of labels or to reduce it according to our needs. We performed preliminary experiments in order to assess which tag granularity is better suited for generation and came up with the 31-label tagset. 7 The main difference with the Stanford scheme is that in AnCora-UPF no distinction is explicitly made between argumental and non-argumental dependencies. 2.3.2 English Treebank For the validation of the generator on English, we use the dependency Penn TreeBank (abo"
N15-1042,W13-3724,1,0.879189,"o, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier is defined for the mapping of each linguistic category. The generator has been tested on Spanish with the multi-layered Ancora-UPF corpus (Mille et al., 2013) and on English with an extended version of the dependency Penn TreeBank (Johansson and Nugues, 2007). The remainder of the paper is structured as follows. In the next section, we briefly outline the fundamentals of sentence generation as we view it in our work, focusing in particular on the most challenging part of it: the transition between the non-isomorphic predicateargument lexico-structural structures and surfacesyntactic structures. Section 3 outlines the setup of our system. Section 4 discusses the experiments we carried out and the results we obtained. In Section 5, we briefly summari"
N15-1042,W11-2836,0,0.029485,"ybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learning approach. This generator has been successfully tested on an English and a Spanish corpus, as a stand-alone DSyntS–SSyntS generator and as a part of the generation pipeline. We are currently about to apply it to other languages—including Chinese, French and German. Furthermore, resources are compiled to use i"
N15-1042,C04-1097,0,0.0373783,"state-of-the-art work focuses on syntactic generation; see, among others (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or only on linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 395 2009; Guo et al., 2011a). A number of proposals are hybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learni"
N15-1042,P04-1011,0,0.0319245,"on.2 1 The data-driven sentence generator is available for public downloading at https://github.com/ talnsoftware/deepgenerator/wiki. 2 For all of these applications, the deep representation can be obtained by a deep parser, such as, e.g., (Ballesteros et al., 2014a). 387 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 387–397, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier is defined for the mapping of each linguistic category. The generator has been tested on Spanish with the multi-layered Ancora-UPF corpus (Mille et al., 2013) and on English with an extended version of the dependency Penn TreeBank (Johansson and Nugues, 2007). The remainder of the paper is structured as follows. In the next section, we briefly outline t"
N15-1042,E09-1097,0,0.0213842,"nd number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a f"
N15-1042,N07-1022,0,0.00899511,"lassifiers for data-driven generators. As already mentioned in Section 1, most of the state-of-the-art work focuses on syntactic generation; see, among others (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or only on linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 395 2009; Guo et al., 2011a). A number of proposals are hybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between mean"
N15-1042,C08-1038,0,\N,Missing
N15-1042,C98-1112,0,\N,Missing
N15-1042,W09-1201,0,\N,Missing
N15-3012,I13-2007,1,0.859674,"d simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an operational interface for the visualization of the output of a deep-syntactic parser and of surface-syntactic structures that serve it as input. The interface is flexible in that it"
N15-3012,C14-1133,1,0.913287,"structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-syntactic parsing has been introduced as a new parsing paradigm; see, e.g., (Ballesteros et al., 2014).1 No visualization interfaces are available as yet to control the output of deep-syntactic parsers. In this paper, we propose such a visualization interface. The interface can be used for both a pipeline consisting of a syntactic parser and a deep parser and a joint syntactic+deep parser. In the first configuration, it facilitates the visualization of the output of the syntactic parser and of the output of the deep parser. In the second configuration, it visualizes directly the output of the joint parser. In what follows, we present its use for the first configuration applied to English. As s"
N15-3012,N15-1042,1,0.837746,"million jobs have been created by the state in that time. DSyntSs have a great potential for such downstream applications as deep machine translation, summarization or information extraction. In deep machine translation as discussed, e.g., by Jones et al. (2012), DSyntSs simplify the alignment between the source and target language structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-syntactic parsing has been introduced as a new parsing paradigm; see, e.g., (Ballesteros et al., 2014).1 No visualization interfaces are available as yet to control the output of deep-syntactic parsers. In this paper, we propose such a visualization interface. The interface can be used for both a pipeline consisting of a syntactic parser and a deep parser and a joi"
N15-3012,D12-1133,1,0.928036,"are available as yet to control the output of deep-syntactic parsers. In this paper, we propose such a visualization interface. The interface can be used for both a pipeline consisting of a syntactic parser and a deep parser and a joint syntactic+deep parser. In the first configuration, it facilitates the visualization of the output of the syntactic parser and of the output of the deep parser. In the second configuration, it visualizes directly the output of the joint parser. In what follows, we present its use for the first configuration applied to English. As surfacesyntactic parser, we use Bohnet and Nivre (2012)’s joint tagger+lemmatizer+parser. As deep parser, we use Ballesteros et al. (2014)’s implementation. Both have been trained on the dependency Penn Treebank (Johansson and Nugues, 2007), which has been extended by the DSyntS-annotation. The interface can be inspected online; cf. http://dparse. 1 The source code of Ballesteros et al.’s deep parser and a short manual on how to use it can be downloaded from https://github.com/talnsoftware/ deepsyntacticparsing/wiki. 56 Proceedings of NAACL-HLT 2015, pages 56–60, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguis"
N15-3012,bohnet-wanner-2010-open,1,0.822011,"3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an operational interface for the visualization of"
N15-3012,E14-2003,0,0.0630282,"Missing"
N15-3012,S10-1059,0,0.0176587,"best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an operational interface for the visualization of the output of a deep-syntactic parser and of surface-syntactic structures that serve it as input. The interface is flexible in that it allows for the display of any additional structural information provided by an extended parsing pipeline. For instance, if the obtained deep-syntactic structure is projected onto a frame-like structure (Chen et al., 2010) with semantic roles as arc labels, this frame structure can be displayed as well. We are currently working on such an extension. Furthermore, we aim to expand our visualization interface to facilitate active exploration of linguistic structures with Brat and thus add to the static display of structures the dimension of Visual Analytics (Keim et al., 2008). Acknowledgments This work has been partially funded by the European Union’s Seventh Framework and Horizon 2020 Research and Innovation Programmes under the Grant Agreement numbers FP7-ICT-610411, FP7-SME606163, and H2020-RIA-645012. Referen"
N15-3012,P08-5006,0,0.0225905,"plicit hints are available in the surface structure. (1a) (2a) (3a) Figure 2: Visualization of surface syntactic structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that"
N15-3012,N10-1011,0,0.0251457,"ructure. (1a) (2a) (3a) Figure 2: Visualization of surface syntactic structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntacti"
N15-3012,D08-1019,0,0.0297568,"ions between full (i.e., meaningful) words of a sentence. For illustration, Figure 1 shows a surface-syntactic structure (above) and deep-syntactic structure (below) for the sentence: almost 1.2 million jobs have been created by the state in that time. DSyntSs have a great potential for such downstream applications as deep machine translation, summarization or information extraction. In deep machine translation as discussed, e.g., by Jones et al. (2012), DSyntSs simplify the alignment between the source and target language structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-syntactic parsing has been introduced as a new parsing paradigm; see, e.g., (Ballesteros et al., 2014).1 No visualization interfaces are available as yet to control the output"
N15-3012,W07-2416,0,0.13618,"ing of a syntactic parser and a deep parser and a joint syntactic+deep parser. In the first configuration, it facilitates the visualization of the output of the syntactic parser and of the output of the deep parser. In the second configuration, it visualizes directly the output of the joint parser. In what follows, we present its use for the first configuration applied to English. As surfacesyntactic parser, we use Bohnet and Nivre (2012)’s joint tagger+lemmatizer+parser. As deep parser, we use Ballesteros et al. (2014)’s implementation. Both have been trained on the dependency Penn Treebank (Johansson and Nugues, 2007), which has been extended by the DSyntS-annotation. The interface can be inspected online; cf. http://dparse. 1 The source code of Ballesteros et al.’s deep parser and a short manual on how to use it can be downloaded from https://github.com/talnsoftware/ deepsyntacticparsing/wiki. 56 Proceedings of NAACL-HLT 2015, pages 56–60, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics adv adv quant quant subj analyt perf (a) almost 1.2 million jobs have analyt pass prepos det prepos det been created by the state in that time ATTR ATTR agent ATTR ATTR II I II ATT"
N15-3012,C12-1083,0,0.016108,"ucture of a sentence. More precisely, a deep-syntactic structure (DSyntS) is a dependency tree that captures the argumentative, attributive and coordinative relations between full (i.e., meaningful) words of a sentence. For illustration, Figure 1 shows a surface-syntactic structure (above) and deep-syntactic structure (below) for the sentence: almost 1.2 million jobs have been created by the state in that time. DSyntSs have a great potential for such downstream applications as deep machine translation, summarization or information extraction. In deep machine translation as discussed, e.g., by Jones et al. (2012), DSyntSs simplify the alignment between the source and target language structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-synta"
N15-3012,nilsson-nivre-2008-malteval,0,0.0344817,"structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an opera"
N15-3012,J05-1004,0,0.0170667,", 2012). Brat takes an annotation file, which is produced by transforming the CoNLL files that the parsers output into Brat’s native format, and generates the graphical interface for the dependency trees. Figure 2 shows three sample surface syntactic structures in Brat. In Figure 3, their equivalent deepsyntactic structures are displayed. As already Figure 1, the figures illustrate the difference of both types of structures with respect to the abstraction of linguistic phenomena. The DSyntSs are clearly much closer to semantics. As a matter of fact, they are equivalent to PropBank structures (Palmer et al., 2005). However, this does not mean that they must per se be “simpler” than their corresponding surface-syntactic structures—compare, for instance, the structures (3a) and (3b) in Figures 2 and 3, where both SSyntS and DSyntS contain the same number of nodes, i.e., are isomorphic. The structures (2a) and (2b) illustrate the capacity of the deep parser to correctly identify the arguments of a lexical item without that explicit hints are available in the surface structure. (1a) (2a) (3a) Figure 2: Visualization of surface syntactic structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-sy"
N15-3012,E12-2021,0,0.106271,"Missing"
N15-3012,P13-4010,0,0.0552154,"Missing"
P15-1165,P14-2131,0,0.0530581,"a mean vector to out-of-vocabulary words. System For our system, we simply augment the delexicalized POS tagger with the I NVERTED distributional representation of the current word. The best parameter setting on Spanish development data was σ = 0.01, δ = 160. 3.3 http://code.google.com/p/uni-dep-tb/ https://code.google.com/p/ wikily-supervised-pos-tagger/ 8 For our embeddings baselines, we augment the feature space by adding embedding vectors for head h and dependent d. We experimented with different versions of combining embedding vectors, from firing separate h and d per-dimension features (Bansal et al., 2014) to combining their information. We found that combining the embeddings of h and d is effective and consistently use the absolute difference between the embedding vectors, since that worked better than addition and multiplication on development data. Delexicalized transfer (D ELEX) uses three (3) iterations over the data in both the single-source and the multi-source set-up, a parameter set on the Spanish development data. The remaining parameters were obtained by averaging over performance with different embeddings on the Spanish development data, obtaining: σ = 0.005, δ = 20, i = 3, and abso"
P15-1165,P14-1023,0,0.0109756,"proaches focus on different kinds of similarity, some more syntactic, some more semantic. The representations are typically either clusters of distributionally similar words, e.g., Brown et al. (1992), or vector representations. In this paper, we focus on vector representations. In vector-based approaches, similar representations are vectors close in some multi-dimensional space. 2.1 Count-based and prediction-based representations There are, briefly put, two approaches to inducing vector-based distributional word representations from large corpora: count-based and predictionbased approaches (Baroni et al., 2014). Countbased approaches represent words by their cooccurrences. Dimensionality reduction is typically performed on a raw or weighted co-occurrence matrix using methods such as singular value decomposition (SVD), a method for maximizing the variance in a dataset in few dimensions. In our inverted indexing, we use raw co-occurrence data. Prediction-based methods use discriminative learning techniques to learn how to predict words from their context, or vice versa. They rely on a neural network architecture, and once the network converges, they use word representations from a middle layer as thei"
P15-1165,C10-1011,1,0.695318,"ing. For compatibility with Xiao and Guo (2014), we also present results on CoNLL 2006 and 2007 treebanks for languages for which we had baseline and system word representations (de, es, sv). Our parameter settings for these experiments were the same as those tuned on the Spanish development data from the Google Universal Treebanks v. 1.0. Baselines The most obvious baseline in our experiments is delexicalized transfer (D ELEX) (McDonald et al., 2011; Søgaard, 2011). This baseline system simply learns models without lexical features. We use a modified version of the first-order Mate 7 parser (Bohnet, 2010) that also takes continuousvalued embeddings as input an disregards features that include lexical items. 3.4 Word alignment Data We use the manually word-aligned EnglishSpanish Europarl data from Graca et al. (2008). The dataset contains 100 sentences. The annotators annotated whether word alignments were certain or possible, and we present results with all word alignments and with only the certain ones. See Graca et al. (2008) for details. Baselines For word alignment, we simply align every aligned word in the gold data, for which we have a word embedding, to its (Euclidean) nearest neighbor"
P15-1165,J92-4003,0,0.127536,"al and sparse models. Also, simple bagof-words models fail to capture the relatedness of words. In many tasks, synonymous words should be treated alike, but their bag-of-words representations are as different as those of dog and therefore. Distributional word representations are supposed to capture distributional similarities between words. Intuitively, we want similar words to have similar representations. Known approaches focus on different kinds of similarity, some more syntactic, some more semantic. The representations are typically either clusters of distributionally similar words, e.g., Brown et al. (1992), or vector representations. In this paper, we focus on vector representations. In vector-based approaches, similar representations are vectors close in some multi-dimensional space. 2.1 Count-based and prediction-based representations There are, briefly put, two approaches to inducing vector-based distributional word representations from large corpora: count-based and predictionbased approaches (Baroni et al., 2014). Countbased approaches represent words by their cooccurrences. Dimensionality reduction is typically performed on a raw or weighted co-occurrence matrix using methods such as sing"
P15-1165,W02-1001,0,0.0575931,"s well as tag dictionaries (Li et al., 2012) needed for the POS tagging experiments. Baselines One baseline method is a typeconstrained structured perceptron with only ortographic features, which are expected to transfer across languages. The type constraints come from Wiktionary, a crowd-sourced tag dictionary.8 Type constraints from Wiktionary were first used by Li et al. (2012), but note that their set-up is unsupervised learning. T¨ackstr¨om et al. (2013) also used type constraints in a supervised set-up. Our learning algorithm is the structured perceptron algorithm originally proposed by Collins (2002). In our POS tagging experiments, we always do 10 passes over the data. We also present two other baselines, where we augment the feature representation with different embeddings for the target word, K LEMENTIEV and C HANDAR. With all the embeddings in POS tagging, we assign a mean vector to out-of-vocabulary words. System For our system, we simply augment the delexicalized POS tagger with the I NVERTED distributional representation of the current word. The best parameter setting on Spanish development data was σ = 0.01, δ = 160. 3.3 http://code.google.com/p/uni-dep-tb/ https://code.google.com"
P15-1165,N15-1157,1,0.757356,"peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better results were obtained with delexicalized transfer in McDonald et al. (2011) by simply using several source languages. This paper introduces a simple method for obtaining truly inter-lingual word representations in order to train models with lexical features on several source languages at the same time. Briefly put,"
P15-1165,graca-etal-2008-building,0,0.0195628,"tings for these experiments were the same as those tuned on the Spanish development data from the Google Universal Treebanks v. 1.0. Baselines The most obvious baseline in our experiments is delexicalized transfer (D ELEX) (McDonald et al., 2011; Søgaard, 2011). This baseline system simply learns models without lexical features. We use a modified version of the first-order Mate 7 parser (Bohnet, 2010) that also takes continuousvalued embeddings as input an disregards features that include lexical items. 3.4 Word alignment Data We use the manually word-aligned EnglishSpanish Europarl data from Graca et al. (2008). The dataset contains 100 sentences. The annotators annotated whether word alignments were certain or possible, and we present results with all word alignments and with only the certain ones. See Graca et al. (2008) for details. Baselines For word alignment, we simply align every aligned word in the gold data, for which we have a word embedding, to its (Euclidean) nearest neighbor in the target sentence. We evaluate this strategy by its precision (P@1). System We compare I NVERTED with K LEMEN TIEV and C HANDAR . To ensure a fair comparison, we use the subset of words covered by all three emb"
P15-1165,C12-1089,0,0.772355,"th rely on three level architectures with input, output and a middle layer for intermediate target word representations. The major difference is that skip-gram uses the target word as input and the context as output, whereas the CBOW model does it the other way around. Learning goes by back-propagation, and random target words are used as negative examples. Levy and Goldberg (2014) show that prediction-based representations obtained with the skip-gram model can be related to count-based ones obtained with PMI. They argue that which is best, varies across tasks. 2.1.2 Bilingual representations Klementiev et al. (2012) learn distinct embedding models for the source and target languages, but while learning to minimize the sum of the two models’ losses, they jointly learn a regularizing interaction matrix, enforcing word pairs aligned in parallel text to have similar representations. Note that Klementiev et al. (2012) rely on word-aligned parallel text, and thereby on a large-coverage soft mapping of source words to target words. Other approaches rely on small coverage dictionaries with hard 1:1 mappings between words. Klementiev et al. (2012) do not use skip-gram or CBOW, but the language model presented in"
P15-1165,P14-2050,0,0.00763126,"ed Table 1: Three nearest neighbors in the English training data of six words occurring in the Spanish test data, in the embeddings used in our experiments. Only 2/6 words were in the German data. skip-gram model and CBOW. The two models both rely on three level architectures with input, output and a middle layer for intermediate target word representations. The major difference is that skip-gram uses the target word as input and the context as output, whereas the CBOW model does it the other way around. Learning goes by back-propagation, and random target words are used as negative examples. Levy and Goldberg (2014) show that prediction-based representations obtained with the skip-gram model can be related to count-based ones obtained with PMI. They argue that which is best, varies across tasks. 2.1.2 Bilingual representations Klementiev et al. (2012) learn distinct embedding models for the source and target languages, but while learning to minimize the sum of the two models’ losses, they jointly learn a regularizing interaction matrix, enforcing word pairs aligned in parallel text to have similar representations. Note that Klementiev et al. (2012) rely on word-aligned parallel text, and thereby on a lar"
P15-1165,D12-1127,0,0.0369956,"Missing"
P15-1165,D11-1006,0,0.435697,"the availability of English resources and the availability of parallel data for (and translations between) English and most other languages. In cross-lingual syntactic parsing, for example, two approaches to cross-lingual learning have been explored, namely annotation projection and delexicalized transfer. Annotation projection (Hwa et al., 2005) uses word-alignments in human translations to project predicted sourceside analyses to the target language, producing a noisy syntactically annotated resource for the target language. On the other hand, delexicalized transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao"
P15-1165,D09-1139,0,0.0521645,"Missing"
P15-1165,P10-1114,0,0.0276803,"on and word alignment, we fix the number of dimensions to 40. For both our baselines and systems, we also tune a scaling factor σ ∈ {1.0, 0.1, 0.01, 0.001} for POS tagging and dependency parsing, using the scaling method from Turian et al. (2010), also used in Gouws and Søgaard (2015). We do not scale our embeddings for document classification or word alignment. 3 Experiments The data set characteristics are found in Table 2.3. 3.1 Document classification Data Our first document classification task is topic classification on the cross-lingual multi-domain sentiment analysis dataset A MAZON in Prettenhofer and Stein (2010).4 We represent each document by the average of the representations of those words that we find both in the documents and in our embeddings. Rather than classifying reviews by sentiment, we classify by topic, trying to discriminate between book reviews, music reviews and DVD reviews, as a three-way classification problem, training on English and testing on German. Unlike in the other tasks below, we always 4 use unscaled word representations, since these are our only features. All word representations have 40 dimensions. The other document classification task is a fourway classification proble"
P15-1165,P11-1061,0,0.039505,"the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better results were obtained with delexicalized transfer in McDonald et al. (2011) by simply using several source lan"
P15-1165,P11-2120,1,0.534532,"lish resources and the availability of parallel data for (and translations between) English and most other languages. In cross-lingual syntactic parsing, for example, two approaches to cross-lingual learning have been explored, namely annotation projection and delexicalized transfer. Annotation projection (Hwa et al., 2005) uses word-alignments in human translations to project predicted sourceside analyses to the target language, producing a noisy syntactically annotated resource for the target language. On the other hand, delexicalized transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014)."
P15-1165,I05-1075,0,0.0207395,"reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better results were obtained with delexicalized transfer in McDonald et al. (2011) by simply u"
P15-1165,Q13-1001,0,0.0450202,"Missing"
P15-1165,P10-1040,0,0.0800173,"NLL 07 – D EPENDENCY PARSING en es de sv 18.6 – – – 447k – – – en es – – – – – 206 357 389 – 5.7k 5.7k 5.7k – 0.841 0.616 n/a E UROPARL – W ORD A LIGNMENT 100 100 – – 0.370 0.533 Table 2: Characteristics of the data sets. Embeddings coverage (token-level) for K LEMENTIEV, C HAN DAR and I NVERTED on the test sets. We use the common vocabulary on W ORD A LIGNMENT . sification and word alignment, we fix the number of dimensions to 40. For both our baselines and systems, we also tune a scaling factor σ ∈ {1.0, 0.1, 0.01, 0.001} for POS tagging and dependency parsing, using the scaling method from Turian et al. (2010), also used in Gouws and Søgaard (2015). We do not scale our embeddings for document classification or word alignment. 3 Experiments The data set characteristics are found in Table 2.3. 3.1 Document classification Data Our first document classification task is topic classification on the cross-lingual multi-domain sentiment analysis dataset A MAZON in Prettenhofer and Stein (2010).4 We represent each document by the average of the representations of those words that we find both in the documents and in our embeddings. Rather than classifying reviews by sentiment, we classify by topic, trying t"
P15-1165,W14-1613,0,0.53136,"2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, b"
P15-1165,I08-3008,0,0.017072,"asons for this; namely, the availability of English resources and the availability of parallel data for (and translations between) English and most other languages. In cross-lingual syntactic parsing, for example, two approaches to cross-lingual learning have been explored, namely annotation projection and delexicalized transfer. Annotation projection (Hwa et al., 2005) uses word-alignments in human translations to project predicted sourceside analyses to the target language, producing a noisy syntactically annotated resource for the target language. On the other hand, delexicalized transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackst"
P16-1015,D15-1159,0,0.0182363,"alized parser. In terms of empirical accuracy, from the early success of Nivre and colleagues (Nivre et al., 2006b; Hall et al., 2007; Nivre, 2008), there has been an succession of improvements in training and decoding, including structured training with beam search (Zhang and Clark, 2008; Zhang and Nivre, 2011), incorporating graph-based rescoring features (Bohnet and Kuhn, 2012), the aformentioned work on joint parsing and tagging (Bohnet and Nivre, 2012), and more recently the adoption of neural networks and feature embeddings (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Alberti et al., 2015). 3.1 Basic Notation In the following, we use a directed unlabeled dependency tree T = hV, Ai for a sentence x = In terms of abstract generalizations of transi151 Transitions This is an example with two arcs “Adds an arc from j to i, both ∈ ACTIVE(O).” ←i,j (O, U, A) ⇒ (O, U, A ∪ (j → i)) “Adds an arc from i to j, both ∈ ACTIVE(O).” →i,j (O, U, A) ⇒ (O, U, A ∪ (i → j)) “Removes token i ∈ ACTIVE(O) from O.” −i (O...i..., U, A) ⇒ (O, U, A) (a) Arc-standard: is and example are eligible for arcs. This is an example with two arcs (b) Arc-eager: example and with are eligible for arcs. ”Moves the top"
P16-1015,W06-2922,0,0.155799,"he stack). Key to our generalization is the notion of active tokens, which is the set of tokens in which new arcs can be created and/or removed from consideration. A parser instantiation is defined by a set of control parameters, which dictate: the types of transitions that are permitted and their properties; the capacity of the active token set; and the maximum arc distance. We show that a number of different transition systems can be described via this framework. Critically the two most common systems are covered – arc-eager and arc-standard (Nivre, 2008). But also Attardi’s non-projective (Attardi, 2006), Kuhlmann’s hybrid system (Kuhlmann et al., 2011), the directed acyclic graph (DAG) parser of Sagae and Tsujii (2008), and likely others. More interestingly, the easy-first framework of Goldberg and Elhadad (2010) can be described as an arc-standard system with an unbounded active token capacity. We present a number of experiments with an implementation of our generalized framework. One major advantage of our generalization (and its implementation) is that it allows for easy exploration of novel systems not previously studied. In Section 5 we discuss some possibilities and provide experiments"
P16-1015,P12-1110,0,0.0212868,"ncy trees. The work of Attardi (2006), Nivre (2009), G´omez-Rodr´ıguez and Nivre (2010), Choi and Palmer (2011), and Pitler and McDonald (2015) derived transition systems that could parse nonprojective trees. Each of these systems traded-off complexity for empirical coverage. Additionally, Sagae and Tsujii (2008) developed transition systems that could parse DAGs by augmentating the arc-standard and the arc-eager system. Bohnet and Nivre (2012) derived a system that could produce both labeled dependency trees as well as part-ofspeech tags in a joint transition system. Taking this idea further Hatori et al. (2012) defined a transition system that performed joint segmentation, tagging and parsing. 3 Generalized Transition-based Parsing A transition system must define a parser state as well as a set of transitions that move the system from one state to the next. Correct sequences of transitions create valid parse trees. A parser state is typically a tuple of data structures and variables that represent the dependency tree constructed thus far and, implicitly, possible valid transitions to the next state. In order to generalize across the parser states of transition-based parsing systems, we preserve thei"
P16-1015,D14-1082,0,0.0508596,"eters without changing the specific implementation of the generalized parser. In terms of empirical accuracy, from the early success of Nivre and colleagues (Nivre et al., 2006b; Hall et al., 2007; Nivre, 2008), there has been an succession of improvements in training and decoding, including structured training with beam search (Zhang and Clark, 2008; Zhang and Nivre, 2011), incorporating graph-based rescoring features (Bohnet and Kuhn, 2012), the aformentioned work on joint parsing and tagging (Bohnet and Nivre, 2012), and more recently the adoption of neural networks and feature embeddings (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Alberti et al., 2015). 3.1 Basic Notation In the following, we use a directed unlabeled dependency tree T = hV, Ai for a sentence x = In terms of abstract generalizations of transi151 Transitions This is an example with two arcs “Adds an arc from j to i, both ∈ ACTIVE(O).” ←i,j (O, U, A) ⇒ (O, U, A ∪ (j → i)) “Adds an arc from i to j, both ∈ ACTIVE(O).” →i,j (O, U, A) ⇒ (O, U, A ∪ (i → j)) “Removes token i ∈ ACTIVE(O) from O.” −i (O...i..., U, A) ⇒ (O, U, A) (a) Arc-standard: is and example are eligible for arcs. This is an example with two arcs (b) Arc"
P16-1015,P11-2121,0,0.132564,"ms. 1 Introduction Transition-based dependency parsing is perhaps the most successful parsing framework in use today (Nivre, 2008). This is due to the fact that it can process sentences in linear time (Nivre, 2003); is highly accurate (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Weiss et al., 2015); and has elegant mechanisms for parsing non-projective sentences (Nivre, 2009). As a result, there have been numerous studies into different transition systems, each with varying properties and complexities (Nivre, 2003; Attardi, 2006; Nivre, 2008; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010; Choi and Palmer, 2011; Pitler and McDonald, 2015). While connections between these transition systems have been noted, there has been little work on developing frameworks that generalize the phenomena parsed by these diverse systems. Such a framework would be beneficial for many reasons: It would provide a language from which we can theoretically compare known transition systems; it can give rise to new systems that could have favorable empirical properties; and an implementation of the generalization allows for comprehensive empirical studies. In this work we provide such a generalized 2 Related Work Transition-b"
P16-1015,W00-1303,0,0.102026,"een that work and the present study is the distinction between complex actions versus control parameters. In terms of theoretical coverage, the frameworks are not equivalent. For instance, our generalization covers the system of Attardi (2006), whereas GR&N13 cover transition systems where multiple arcs can be created in tandem. In Section 7 we compare the two generalizations. state as well as a finite set of operations that move the system from one state to another (Nivre, 2008). In terms of modern statistical models that dominate the discourse today, the starting point is likely the work of Kudo and Matsumoto (2000) and Yamada and Matsumoto (2003), who adopted the idea of cascaded chunking from Abney (1991) in a greedy dependency parsing framework. From this early work, transition-based parsing quickly grew in scope with the formalization of the arc-eager versus arc-standard paradigms (Nivre, 2003; Nivre, 2008), the latter largely being based on well-known shift-reduce principles in the phrase-structure literature (Ratnaparkhi, 1999). The speed and empirical accuracy of these systems – as evident in the widely used MaltParser software (Nivre et al., 2006a) – led to the study of a number of different tran"
P16-1015,D11-1114,0,0.044509,"Missing"
P16-1015,P11-1068,0,0.0421966,"Missing"
P16-1015,P04-1015,0,0.184462,"Missing"
P16-1015,P13-2020,1,0.903162,"Missing"
P16-1015,de-marneffe-etal-2006-generating,0,0.0234025,"Missing"
P16-1015,P81-1022,0,0.722336,"Missing"
P16-1015,P15-1033,0,0.0120347,"tation of the generalized parser. In terms of empirical accuracy, from the early success of Nivre and colleagues (Nivre et al., 2006b; Hall et al., 2007; Nivre, 2008), there has been an succession of improvements in training and decoding, including structured training with beam search (Zhang and Clark, 2008; Zhang and Nivre, 2011), incorporating graph-based rescoring features (Bohnet and Kuhn, 2012), the aformentioned work on joint parsing and tagging (Bohnet and Nivre, 2012), and more recently the adoption of neural networks and feature embeddings (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Alberti et al., 2015). 3.1 Basic Notation In the following, we use a directed unlabeled dependency tree T = hV, Ai for a sentence x = In terms of abstract generalizations of transi151 Transitions This is an example with two arcs “Adds an arc from j to i, both ∈ ACTIVE(O).” ←i,j (O, U, A) ⇒ (O, U, A ∪ (j → i)) “Adds an arc from i to j, both ∈ ACTIVE(O).” →i,j (O, U, A) ⇒ (O, U, A ∪ (i → j)) “Removes token i ∈ ACTIVE(O) from O.” −i (O...i..., U, A) ⇒ (O, U, A) (a) Arc-standard: is and example are eligible for arcs. This is an example with two arcs (b) Arc-eager: example and with are eligible f"
P16-1015,W06-2933,0,0.063628,"Missing"
P16-1015,N10-1115,0,0.359881,"a set of control parameters, which dictate: the types of transitions that are permitted and their properties; the capacity of the active token set; and the maximum arc distance. We show that a number of different transition systems can be described via this framework. Critically the two most common systems are covered – arc-eager and arc-standard (Nivre, 2008). But also Attardi’s non-projective (Attardi, 2006), Kuhlmann’s hybrid system (Kuhlmann et al., 2011), the directed acyclic graph (DAG) parser of Sagae and Tsujii (2008), and likely others. More interestingly, the easy-first framework of Goldberg and Elhadad (2010) can be described as an arc-standard system with an unbounded active token capacity. We present a number of experiments with an implementation of our generalized framework. One major advantage of our generalization (and its implementation) is that it allows for easy exploration of novel systems not previously studied. In Section 5 we discuss some possibilities and provide experiments for these in Section 6. In this paper, we present a generalized transition-based parsing framework where parsers are instantiated in terms of a set of control parameters that constrain transitions between parser s"
P16-1015,W03-3017,0,0.8129,"ition-based parsing framework where parsers are instantiated in terms of a set of control parameters that constrain transitions between parser states. This generalization provides a unified framework to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective. This includes well-known transition systems, but also previously unstudied systems. 1 Introduction Transition-based dependency parsing is perhaps the most successful parsing framework in use today (Nivre, 2008). This is due to the fact that it can process sentences in linear time (Nivre, 2003); is highly accurate (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Weiss et al., 2015); and has elegant mechanisms for parsing non-projective sentences (Nivre, 2009). As a result, there have been numerous studies into different transition systems, each with varying properties and complexities (Nivre, 2003; Attardi, 2006; Nivre, 2008; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010; Choi and Palmer, 2011; Pitler and McDonald, 2015). While connections between these transition systems have been noted, there has been little work on developing frameworks that generalize the phenomena parsed by the"
P16-1015,P10-1151,0,0.0339731,"Missing"
P16-1015,J08-4003,0,0.278678,"r); and a set of operative tokens (often called the stack). Key to our generalization is the notion of active tokens, which is the set of tokens in which new arcs can be created and/or removed from consideration. A parser instantiation is defined by a set of control parameters, which dictate: the types of transitions that are permitted and their properties; the capacity of the active token set; and the maximum arc distance. We show that a number of different transition systems can be described via this framework. Critically the two most common systems are covered – arc-eager and arc-standard (Nivre, 2008). But also Attardi’s non-projective (Attardi, 2006), Kuhlmann’s hybrid system (Kuhlmann et al., 2011), the directed acyclic graph (DAG) parser of Sagae and Tsujii (2008), and likely others. More interestingly, the easy-first framework of Goldberg and Elhadad (2010) can be described as an arc-standard system with an unbounded active token capacity. We present a number of experiments with an implementation of our generalized framework. One major advantage of our generalization (and its implementation) is that it allows for easy exploration of novel systems not previously studied. In Section 5 we"
P16-1015,J13-4002,0,0.46584,"Missing"
P16-1015,P09-1040,0,0.451661,"ion provides a unified framework to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective. This includes well-known transition systems, but also previously unstudied systems. 1 Introduction Transition-based dependency parsing is perhaps the most successful parsing framework in use today (Nivre, 2008). This is due to the fact that it can process sentences in linear time (Nivre, 2003); is highly accurate (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Weiss et al., 2015); and has elegant mechanisms for parsing non-projective sentences (Nivre, 2009). As a result, there have been numerous studies into different transition systems, each with varying properties and complexities (Nivre, 2003; Attardi, 2006; Nivre, 2008; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010; Choi and Palmer, 2011; Pitler and McDonald, 2015). While connections between these transition systems have been noted, there has been little work on developing frameworks that generalize the phenomena parsed by these diverse systems. Such a framework would be beneficial for many reasons: It would provide a language from which we can theoretically compare known transition systems"
P16-1015,D07-1097,0,0.0791386,"Missing"
P16-1015,N15-1068,1,0.891429,"sition-based dependency parsing is perhaps the most successful parsing framework in use today (Nivre, 2008). This is due to the fact that it can process sentences in linear time (Nivre, 2003); is highly accurate (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Weiss et al., 2015); and has elegant mechanisms for parsing non-projective sentences (Nivre, 2009). As a result, there have been numerous studies into different transition systems, each with varying properties and complexities (Nivre, 2003; Attardi, 2006; Nivre, 2008; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010; Choi and Palmer, 2011; Pitler and McDonald, 2015). While connections between these transition systems have been noted, there has been little work on developing frameworks that generalize the phenomena parsed by these diverse systems. Such a framework would be beneficial for many reasons: It would provide a language from which we can theoretically compare known transition systems; it can give rise to new systems that could have favorable empirical properties; and an implementation of the generalization allows for comprehensive empirical studies. In this work we provide such a generalized 2 Related Work Transition-based dependency parsing can"
P16-1015,C08-1095,0,0.295862,"arcs can be created and/or removed from consideration. A parser instantiation is defined by a set of control parameters, which dictate: the types of transitions that are permitted and their properties; the capacity of the active token set; and the maximum arc distance. We show that a number of different transition systems can be described via this framework. Critically the two most common systems are covered – arc-eager and arc-standard (Nivre, 2008). But also Attardi’s non-projective (Attardi, 2006), Kuhlmann’s hybrid system (Kuhlmann et al., 2011), the directed acyclic graph (DAG) parser of Sagae and Tsujii (2008), and likely others. More interestingly, the easy-first framework of Goldberg and Elhadad (2010) can be described as an arc-standard system with an unbounded active token capacity. We present a number of experiments with an implementation of our generalized framework. One major advantage of our generalization (and its implementation) is that it allows for easy exploration of novel systems not previously studied. In Section 5 we discuss some possibilities and provide experiments for these in Section 6. In this paper, we present a generalized transition-based parsing framework where parsers are"
P16-1015,N03-1033,0,0.121686,"Missing"
P16-1015,P15-1032,0,0.0481564,"control parameters that constrain transitions between parser states. This generalization provides a unified framework to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective. This includes well-known transition systems, but also previously unstudied systems. 1 Introduction Transition-based dependency parsing is perhaps the most successful parsing framework in use today (Nivre, 2008). This is due to the fact that it can process sentences in linear time (Nivre, 2003); is highly accurate (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Weiss et al., 2015); and has elegant mechanisms for parsing non-projective sentences (Nivre, 2009). As a result, there have been numerous studies into different transition systems, each with varying properties and complexities (Nivre, 2003; Attardi, 2006; Nivre, 2008; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010; Choi and Palmer, 2011; Pitler and McDonald, 2015). While connections between these transition systems have been noted, there has been little work on developing frameworks that generalize the phenomena parsed by these diverse systems. Such a framework would be beneficial for many reasons: It would prov"
P16-1015,W03-3023,0,0.387403,"study is the distinction between complex actions versus control parameters. In terms of theoretical coverage, the frameworks are not equivalent. For instance, our generalization covers the system of Attardi (2006), whereas GR&N13 cover transition systems where multiple arcs can be created in tandem. In Section 7 we compare the two generalizations. state as well as a finite set of operations that move the system from one state to another (Nivre, 2008). In terms of modern statistical models that dominate the discourse today, the starting point is likely the work of Kudo and Matsumoto (2000) and Yamada and Matsumoto (2003), who adopted the idea of cascaded chunking from Abney (1991) in a greedy dependency parsing framework. From this early work, transition-based parsing quickly grew in scope with the formalization of the arc-eager versus arc-standard paradigms (Nivre, 2003; Nivre, 2008), the latter largely being based on well-known shift-reduce principles in the phrase-structure literature (Ratnaparkhi, 1999). The speed and empirical accuracy of these systems – as evident in the widely used MaltParser software (Nivre et al., 2006a) – led to the study of a number of different transition systems. Many of these ne"
P16-1015,D08-1059,0,0.0282985,"asic operations, different transition systems can be defined and configured within one single unified system. As a consequence, we obtain a generalized parser that is capable of executing a wide range of different transition systems by setting a number of control parameters without changing the specific implementation of the generalized parser. In terms of empirical accuracy, from the early success of Nivre and colleagues (Nivre et al., 2006b; Hall et al., 2007; Nivre, 2008), there has been an succession of improvements in training and decoding, including structured training with beam search (Zhang and Clark, 2008; Zhang and Nivre, 2011), incorporating graph-based rescoring features (Bohnet and Kuhn, 2012), the aformentioned work on joint parsing and tagging (Bohnet and Nivre, 2012), and more recently the adoption of neural networks and feature embeddings (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Alberti et al., 2015). 3.1 Basic Notation In the following, we use a directed unlabeled dependency tree T = hV, Ai for a sentence x = In terms of abstract generalizations of transi151 Transitions This is an example with two arcs “Adds an arc from j to i, both ∈ ACTIVE(O).” ←i,j (O, U, A)"
P16-1015,P14-2107,1,0.909235,"Missing"
P16-1015,P11-2033,0,0.266001,"e parsers are instantiated in terms of a set of control parameters that constrain transitions between parser states. This generalization provides a unified framework to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective. This includes well-known transition systems, but also previously unstudied systems. 1 Introduction Transition-based dependency parsing is perhaps the most successful parsing framework in use today (Nivre, 2008). This is due to the fact that it can process sentences in linear time (Nivre, 2003); is highly accurate (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Weiss et al., 2015); and has elegant mechanisms for parsing non-projective sentences (Nivre, 2009). As a result, there have been numerous studies into different transition systems, each with varying properties and complexities (Nivre, 2003; Attardi, 2006; Nivre, 2008; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010; Choi and Palmer, 2011; Pitler and McDonald, 2015). While connections between these transition systems have been noted, there has been little work on developing frameworks that generalize the phenomena parsed by these diverse systems. Such a framework would"
P16-1015,nivre-etal-2006-maltparser,0,\N,Missing
P16-1015,E12-1009,1,\N,Missing
P16-1015,J13-1002,0,\N,Missing
P18-1246,D15-1159,0,0.0224111,"twork for tagging. While this first study used only word embeddings, a subsequent model extended the representation to include suffix embeddings (Collobert et al., 2011). The seminal dependency parsing paper of Chen and Manning (2014) led to a number of tagging papers that used their basic architecture of highly featurized (and embedded) feed-forward neural networks. Botha et al. (2017), for example, studied this architecture in a low resource setting using word, sub-word (prefix/suffix) and induced cluster features to obtain competitive accuracy with the state-of-the-art. Zhou et al. (2015), Alberti et al. (2015) and Andor et al. (2016) extended the work of Chen et al. to a structured prediction setting, the later two use again a mix of word and sub-word features. The idea of using a recurrent layer over characters to induce a complementary view of a word has occurred in numerous papers. Perhaps the earliest is Santos and Zadrozny (2014) who compare character-based LSTM encodings to traditional word-based embeddings. Ling et al. (2015) take this a step further and combine the word embeddings with a recurrent character encoding of the word—instead of just relying on one or the other. Alberti et al. (20"
P18-1246,P16-1231,1,0.930794,"his first study used only word embeddings, a subsequent model extended the representation to include suffix embeddings (Collobert et al., 2011). The seminal dependency parsing paper of Chen and Manning (2014) led to a number of tagging papers that used their basic architecture of highly featurized (and embedded) feed-forward neural networks. Botha et al. (2017), for example, studied this architecture in a low resource setting using word, sub-word (prefix/suffix) and induced cluster features to obtain competitive accuracy with the state-of-the-art. Zhou et al. (2015), Alberti et al. (2015) and Andor et al. (2016) extended the work of Chen et al. to a structured prediction setting, the later two use again a mix of word and sub-word features. The idea of using a recurrent layer over characters to induce a complementary view of a word has occurred in numerous papers. Perhaps the earliest is Santos and Zadrozny (2014) who compare character-based LSTM encodings to traditional word-based embeddings. Ling et al. (2015) take this a step further and combine the word embeddings with a recurrent character encoding of the word—instead of just relying on one or the other. Alberti et al. (2017) use characters encod"
P18-1246,K17-3004,0,0.048126,"Missing"
P18-1246,D17-1309,1,0.849286,"the time, they use ngram affix features, which were made context sensitive via manually constructed conjunctions with features from other words in a fixed window. Collobert and Weston (2008) was perhaps the first modern neural network for tagging. While this first study used only word embeddings, a subsequent model extended the representation to include suffix embeddings (Collobert et al., 2011). The seminal dependency parsing paper of Chen and Manning (2014) led to a number of tagging papers that used their basic architecture of highly featurized (and embedded) feed-forward neural networks. Botha et al. (2017), for example, studied this architecture in a low resource setting using word, sub-word (prefix/suffix) and induced cluster features to obtain competitive accuracy with the state-of-the-art. Zhou et al. (2015), Alberti et al. (2015) and Andor et al. (2016) extended the work of Chen et al. to a structured prediction setting, the later two use again a mix of word and sub-word features. The idea of using a recurrent layer over characters to induce a complementary view of a word has occurred in numerous papers. Perhaps the earliest is Santos and Zadrozny (2014) who compare character-based LSTM enc"
P18-1246,W16-1603,0,0.0233447,"ord the tag with highest probability. Table 8 investigates the empirical impact of alternative definitions of gi that concatenate only subsets of {F1st (w), Flast (w), B1st (w), Blast (w)}. 3.2 Word-based Character Model To investigate whether a sentence sensitive character model is better than a character model where the context is restricted to the characters of a word, we reimplemented the word-based character model of Dozat et al. (2017) as shown in Figure 1a. This model uses the final state of a unidirectional LSTM over the characters of the word, combined with the attention mechanism of Cao and Rei (2016) over all characters. We refer the reader to those works for more details. Critically, however, all the information fed to this representation comes from the word itself, and not a wider sentence-level context. 3.3 Sentence-based Word Model We used a similar setup for our context sensitive word encodings as the character encodings. There are a few differences. Obviously, the inputs are the words of the sentence. For each of the words, we use pretrained word embeddings (pword , ..., pword ) n 1 summed with a dynamically learned word embedding for each word in the corpus (eword , ..., eword ): n"
P18-1246,D14-1082,0,0.0362967,"merate, Gim´enez and Marquez (2004) is a good example of an accurate linear model that uses both word and sub-word features. Specifically, like most systems of the time, they use ngram affix features, which were made context sensitive via manually constructed conjunctions with features from other words in a fixed window. Collobert and Weston (2008) was perhaps the first modern neural network for tagging. While this first study used only word embeddings, a subsequent model extended the representation to include suffix embeddings (Collobert et al., 2011). The seminal dependency parsing paper of Chen and Manning (2014) led to a number of tagging papers that used their basic architecture of highly featurized (and embedded) feed-forward neural networks. Botha et al. (2017), for example, studied this architecture in a low resource setting using word, sub-word (prefix/suffix) and induced cluster features to obtain competitive accuracy with the state-of-the-art. Zhou et al. (2015), Alberti et al. (2015) and Andor et al. (2016) extended the work of Chen et al. to a structured prediction setting, the later two use again a mix of word and sub-word features. The idea of using a recurrent layer over characters to ind"
P18-1246,N16-1031,0,0.0806653,"Missing"
P18-1246,K17-3002,0,0.0717431,"ural networks—specifically BiLSTMs (Schuster and Paliwal, 1997; Graves and Schmidhuber, 2005) to create sentence-level context sensitive encodings of words. A successful recipe is to first create an initial context insensitive word representation, which usually has three main parts: 1) A dynamically trained word embedding; 2) a fixed pre-trained word-embedding, induced from a large corpus; and 3) a sub-word character model, which itself is usually the final state of a recurrent model that ingests one character at a time. Such word/sub-word models originated with Plank et al. (2016). Recently, Dozat et al. (2017) used precisely such a context insensitive word representation as input to a BiLSTM in order to obtain context sensitive word encodings used to predict partof-speech tags. The Dozat et al. model had the highest accuracy of all participating systems in the CoNLL 2017 shared task (Zeman et al., 2017). In such a model, sub-word character-based representations only interact indirectly via subsequent recurrent layers. For example, consider the sentence I had shingles, which is a painful disease. Context insensitive character and word representations may have learned that for unknown or infrequent w"
P18-1246,D15-1176,0,0.0286171,"low resource setting using word, sub-word (prefix/suffix) and induced cluster features to obtain competitive accuracy with the state-of-the-art. Zhou et al. (2015), Alberti et al. (2015) and Andor et al. (2016) extended the work of Chen et al. to a structured prediction setting, the later two use again a mix of word and sub-word features. The idea of using a recurrent layer over characters to induce a complementary view of a word has occurred in numerous papers. Perhaps the earliest is Santos and Zadrozny (2014) who compare character-based LSTM encodings to traditional word-based embeddings. Ling et al. (2015) take this a step further and combine the word embeddings with a recurrent character encoding of the word—instead of just relying on one or the other. Alberti et al. (2017) use characters encodings for parsing. Peters et al. (2018) show that contextual embeddings using character convolutions improve accuracy for number of NLP tasks. Plank et al. (2016) is probably the jumping-off point for most current architectures for tagging models with recurrent neural networks. Specifically, they used a combined word embedding and recurrent character encoding as the initial input to a BiLSTM that generate"
P18-1246,P16-2067,0,0.369836,"gh the adoption of recurrent neural networks—specifically BiLSTMs (Schuster and Paliwal, 1997; Graves and Schmidhuber, 2005) to create sentence-level context sensitive encodings of words. A successful recipe is to first create an initial context insensitive word representation, which usually has three main parts: 1) A dynamically trained word embedding; 2) a fixed pre-trained word-embedding, induced from a large corpus; and 3) a sub-word character model, which itself is usually the final state of a recurrent model that ingests one character at a time. Such word/sub-word models originated with Plank et al. (2016). Recently, Dozat et al. (2017) used precisely such a context insensitive word representation as input to a BiLSTM in order to obtain context sensitive word encodings used to predict partof-speech tags. The Dozat et al. model had the highest accuracy of all participating systems in the CoNLL 2017 shared task (Zeman et al., 2017). In such a model, sub-word character-based representations only interact indirectly via subsequent recurrent layers. For example, consider the sentence I had shingles, which is a painful disease. Context insensitive character and word representations may have learned t"
P18-1246,P11-2009,0,0.0677303,"Missing"
P18-1246,K17-3009,0,0.0452285,"Missing"
P18-1246,P16-1147,0,0.0567865,"Missing"
P18-1246,P15-1117,0,0.0196013,"rst modern neural network for tagging. While this first study used only word embeddings, a subsequent model extended the representation to include suffix embeddings (Collobert et al., 2011). The seminal dependency parsing paper of Chen and Manning (2014) led to a number of tagging papers that used their basic architecture of highly featurized (and embedded) feed-forward neural networks. Botha et al. (2017), for example, studied this architecture in a low resource setting using word, sub-word (prefix/suffix) and induced cluster features to obtain competitive accuracy with the state-of-the-art. Zhou et al. (2015), Alberti et al. (2015) and Andor et al. (2016) extended the work of Chen et al. to a structured prediction setting, the later two use again a mix of word and sub-word features. The idea of using a recurrent layer over characters to induce a complementary view of a word has occurred in numerous papers. Perhaps the earliest is Santos and Zadrozny (2014) who compare character-based LSTM encodings to traditional word-based embeddings. Ling et al. (2015) take this a step further and combine the word embeddings with a recurrent character encoding of the word—instead of just relying on one or the ot"
Q13-1034,C00-2143,1,0.361351,"bank covered by morphological analyzer. Clusters: number of tokens and types in unlabeled corpus. treebank annotation we have to rely on a heuristic mapping between the two. Word clusters are derived from the so-called Huge German Corpus.7 Hungarian For training and test we use the Szeged Dependency Treebank (Farkas et al., 2012). We use a finite-state morphological analyzer constructed from the morphdb.hu lexical resource (Tr´on et al., 2006), and word clusters come from the Hungarian National Corpus (V´aradi, 2002). Russian Parsers are trained and tested on data from the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). The morphological analyzer is a module of the ETAP-3 linguistic processor (Apresian et al., 2003) with a dictionary comprising more than 130,000 lexemes (Iomdin and Sizov, 2008). Word clusters have been produced on the basis of an unlabeled corpus of Russian compiled by the Russian Language Institute of the Russian Academy of Sciences and tokenized by the ETAP-3 analyzer. 4 Joint Morphology and Syntax We start by exploring different ways of integrating morphology and syntax in a data-driven setting, that is, where our only knowledge source is the annotated training"
Q13-1034,boguslavsky-etal-2002-development,1,0.676986,"cal analyzer. Clusters: number of tokens and types in unlabeled corpus. treebank annotation we have to rely on a heuristic mapping between the two. Word clusters are derived from the so-called Huge German Corpus.7 Hungarian For training and test we use the Szeged Dependency Treebank (Farkas et al., 2012). We use a finite-state morphological analyzer constructed from the morphdb.hu lexical resource (Tr´on et al., 2006), and word clusters come from the Hungarian National Corpus (V´aradi, 2002). Russian Parsers are trained and tested on data from the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). The morphological analyzer is a module of the ETAP-3 linguistic processor (Apresian et al., 2003) with a dictionary comprising more than 130,000 lexemes (Iomdin and Sizov, 2008). Word clusters have been produced on the basis of an unlabeled corpus of Russian compiled by the Russian Language Institute of the Russian Academy of Sciences and tokenized by the ETAP-3 analyzer. 4 Joint Morphology and Syntax We start by exploring different ways of integrating morphology and syntax in a data-driven setting, that is, where our only knowledge source is the annotated training corpus. At both learning a"
Q13-1034,E12-1009,1,0.573095,"ther support for this choice, at least for the languages considered in this paper. Note also that the choice is not motivated by efficiency concerns, since increasing the values of kp and km has only a marginal effect on running time, as explained in Section 2.4. Finally, the choice not to consider k-best lemmas is dictated by the fact that our lemmatizer only provides a 1-best analysis. For the first three models, we use the same feature representations as Bohnet and Nivre (2012),9 consisting of their adaptation of the features used by Zhang and Nivre (2011), the graph completion features of Bohnet and Kuhn (2012), and the special features over k-best tags introduced specifically for joint tagging and parsing by Bohnet and Nivre (2012). For the J OINT model, we simply add features over the k-best morphological descriptions analogous to the features over k-best tags.10 Experimental results for these four models can be found in Table 2. From the P IPELINE results, we see that the 1-best accuracy of the preprocessing tagger ranges from 95.0 (Finnish) to 99.2 (Czech) for POS, and from 89.4 (Finnish) to 96.5 (Hungarian) for MOR. The lemmatizer does a good job for four of the languages (93.9–97.9) but has re"
Q13-1034,D12-1133,1,0.0763756,"s, which tend to assume that all morphological disambiguation has been performed before syntactic analysis begins. However, as argued by Lee et al. (2011), in morphologically rich languages there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. Lee et al. (2011) go on to show that a discriminative model for joint morphological disambiguation and dependency parsing gives consistent improvements in morphological and syntactic accuracy, compared to a pipeline model, for Ancient Greek, Czech, Hungarian and Latin. Similarly, Bohnet and Nivre (2012) propose a model for 1 See https://sites.google.com/site/spmrl2013/home/sharedtask. 415 Transactions of the Association for Computational Linguistics, 1 (2013) 415–428. Action Editor: Brian Roark. c Submitted 7/2013; Revised 9/2013; Published 10/2013. 2013 Association for Computational Linguistics. joint part-of-speech tagging and dependency parsing and report improved accuracy for Czech and German (but also for Chinese and English), although in this case the joint model is limited to basic part-ofspeech tags and does not involve the full complex of morphological features. An integrated approa"
Q13-1034,C10-1011,1,0.684406,"nguages. For Czech, the best previous UAS on the standard train-test split of the PDT is 87.32, reported by Koo et al. (2010) with a parser using non-projective head automata and dual decomposition, while the best LAS is 78.82 LAS from Nilsson et al. (2006), using a greedy arc-eager transitionbased system with pseudo-projective parsing. Our best results are 1.7 percentage points better for UAS (89.0) and almost 5 percentage points better for LAS (83.7).14 For Finnish, the only previous results are from Haverinen et al. (2013), who achieve 81.01 LAS and 84.97 UAS with the graph-based parser of Bohnet (2010). We get substantial improvements with 83.1 LAS and 86.6 UAS. We also improve slightly over their best POS score, obtained with the HunPos tagger (Hal´acsy et al., 2007) together with the OMorFi analyzer (95.7 vs. 95.4). For German, the best previous results on the same train-test split are from Seeker and Kuhn (2012), using the graphbased parser of Bohnet (2010) in a pipeline architecture. With the same evaluation setup as in this paper, they achieve 91.50 LAS and 93.48 UAS – 13 L EX S OFT averages 0.132 ms per sentence on an Intel i73930K processor with 6 cores, against 0.112 ms for P IPELIN"
Q13-1034,J92-4003,0,0.242366,"striking for German, where the soft lexical constraints are clearly beneficial (especially for the MOR score) despite not being quite compatible with the morphological descriptions in the training set. In terms of statistical signifance, L EX S OFT outperforms the J OINT model with respect to the PMD score for all languages (p < 0.01). It is also significantly better than L EX H ARD for all languages except Finnish (p < 0.01). 6 Word Clusters Finally, we add word cluster features to the best model for each language (L EX H ARD for Finnish, L EX S OFT for the others).11 We use Brown clusters (Brown et al., 1992), with 800 clusters for all languages, and we use the same feature representation as Bohnet and Nivre (2012). The results in Table 2 show small but consistent improvements in almost all metrics for all languages, confirming the benefit of cluster features for morphologically rich languages. It is worth noting that we see the biggest improvement for Finnish, the language with the smallest training set and therefore most likely to 11 The best model was selected according to results on the dev set (cross-validation on the training set for Finnish). suffer from sparse data, where the syntactic acc"
Q13-1034,W06-2920,0,0.719913,"re languages, it has also been observed that typological differences between languages lead to new challenges. In particular, it has been found over and over again that languages exhibiting rich morphological structure, often together with a relatively free word order, usually obtain lower parsing accuracy, especially in comparison to English. One striking demonstration of this tendency can be found in the CoNLL shared tasks on multilingual dependency parsing, organized in 2006 and 2007, where richly inflected languages clustered at the lower end of the scale with respect to parsing accuracy (Buchholz and Marsi, 2006; Nivre et al., 2007). These and similar observations have led to an increased interest in the special challenges posed by parsing morphologically rich languages, as evidenced most clearly by a new series of workshops devoted to this topic (Tsarfaty et al., 2010), as well as a special issue in Computational Linguistics (Tsarfaty et al., 2013) and a shared task on parsing morphologically rich languages.1 One hypothesized explanation for the lower parsing accuracy observed for richly inflected languages is the strict separation of morphological and syntactic analysis assumed in many parsing fram"
Q13-1034,D07-1022,0,0.0694561,"ish), although in this case the joint model is limited to basic part-ofspeech tags and does not involve the full complex of morphological features. An integrated approach to morphological and syntactic analysis can also be found in grammar-based dependency parsers, such as the ETAP-3 linguistic processor (Apresian et al., 2003), where morphological disambiguation is mostly carried out together with syntactic analysis. Finally, it is worth noting that joint models of morphology and syntax have been more popular in constituency-based statistical parsing (Cowan and Collins, 2005; Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). Another hypothesis from the literature is that the high type-token ratio resulting from large morphological paradigms leads to data sparseness when estimating the parameters of a statistical parsing model (Tsarfaty et al., 2010; Tsarfaty et al., 2013). In particular, for many words in the language, only a subset of its morphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus, Goldberg and Elhadad (2013) show that"
Q13-1034,D11-1114,0,0.0374383,"Missing"
Q13-1034,P04-1015,0,0.261816,"put sentence x with weight vector w. The symbols h.c, h.s and h.f denote, respectively, the configuration, score and feature vector of a hypothesis h; Γc denotes the MS-parse defined by c. to 0.0, make N iterations over the training data and update the weight vector for every sentence x where the transition sequence C0,m corresponding to the gold parse is different from the highest scoring tran4 More precisely, we use the ∗ sition sequence C0,m 0. passive-aggressive update of Crammer et al. (2006). We also use the early update strategy found beneficial for parsing in several previous studies (Collins and Roark, 2004; Zhang and Clark, 2008; Huang and Sagae, 2010). This means that, at learning time, we terminate the beam search as soon as the hypothesis corresponding to the gold parse is pruned from the beam and then update with respect to the partial transition sequences constructed up to that point. Finally, we use the standard technique of averaging over all weight vectors seen in training, as originally proposed by Collins (2002). 4 Note that there may be more than one transition sequence corresponding to the gold parse, in which case we pick the canonical transition sequence that processes all left-de"
Q13-1034,W02-1001,0,0.162454,"inear for natural language data sets, due to the sparsity of non-projective dependencies (Nivre, 2009). The running time is also linear in |D |+ |P × M |, which means that joint prediction only gives a linear increase in running time, often quite marginal because |D |> |P × M |. This assumes that the lemma is predicted deterministically given a tag and a morphological description, an assumption that is enforced in all our experiments. 2.5 Learning In order to learn a weight vector w from a training set of sentences with gold parses, we use a variant of the structured perceptron, introduced by Collins (2002) and first used for transition-based parsing by Zhang and Clark (2008). We initialize all weights 3 While there exist exact dynamic programming algorithms for projective transition systems (Huang and Sagae, 2010; Kuhlmann et al., 2011) and even for restricted non-projective systems (Cohen et al., 2011), parsing is intractable for systems like ours that permit arbitrary non-projective trees. PARSE(x, w) 1 h0 .c ← cs (x) 2 h0 .s ← 0.0 3 h0 .f ← {0.0}dim(w) 4 B EAM ← [h0 ] 5 while ∃h ∈ B EAM : h.c 6∈ Ct 6 T MP ← [ ] 7 foreach h ∈ B EAM 8 foreach t ∈ T : P ERMISSIBLE(h.c, t) 9 h.f ← h.f + f(x, h.c"
Q13-1034,H05-1100,0,0.0303135,"and German (but also for Chinese and English), although in this case the joint model is limited to basic part-ofspeech tags and does not involve the full complex of morphological features. An integrated approach to morphological and syntactic analysis can also be found in grammar-based dependency parsers, such as the ETAP-3 linguistic processor (Apresian et al., 2003), where morphological disambiguation is mostly carried out together with syntactic analysis. Finally, it is worth noting that joint models of morphology and syntax have been more popular in constituency-based statistical parsing (Cowan and Collins, 2005; Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). Another hypothesis from the literature is that the high type-token ratio resulting from large morphological paradigms leads to data sparseness when estimating the parameters of a statistical parsing model (Tsarfaty et al., 2010; Tsarfaty et al., 2013). In particular, for many words in the language, only a subset of its morphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus"
Q13-1034,E12-1007,1,0.93185,"22 1105 33 151,971 71,263 200,249,814 538,138 14 454 78 97,905 35,039 195,897,041 639,446 Table 1: Statistics about data sets and resources used in the experiments. Treebank: number of tokens in data sets; number of labels in label sets. Morphology: number of word forms and lemmas in treebank covered by morphological analyzer. Clusters: number of tokens and types in unlabeled corpus. treebank annotation we have to rely on a heuristic mapping between the two. Word clusters are derived from the so-called Huge German Corpus.7 Hungarian For training and test we use the Szeged Dependency Treebank (Farkas et al., 2012). We use a finite-state morphological analyzer constructed from the morphdb.hu lexical resource (Tr´on et al., 2006), and word clusters come from the Hungarian National Corpus (V´aradi, 2002). Russian Parsers are trained and tested on data from the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). The morphological analyzer is a module of the ETAP-3 linguistic processor (Apresian et al., 2003) with a dictionary comprising more than 130,000 lexemes (Iomdin and Sizov, 2008). Word clusters have been produced on the basis of an unlabeled corpus of Russian compiled by the Rus"
Q13-1034,W09-1205,0,0.0234606,"binary), and w is a weight vector of the same dimensionality, where each component wi is the real-valued weight of the feature fi (x, c, t). The choice of features to include in f (x, c, t) is discussed separately for each instantiation of the model in Sections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark, 2008; Zhang and Nivre, 2012). While still not exact, beam search decoders explore a larger part of the search space than greedy parsers, which is likely to be especially important for joint models, where the search space is larger than for plain dependen"
Q13-1034,J13-1007,0,0.0248062,"Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). Another hypothesis from the literature is that the high type-token ratio resulting from large morphological paradigms leads to data sparseness when estimating the parameters of a statistical parsing model (Tsarfaty et al., 2010; Tsarfaty et al., 2013). In particular, for many words in the language, only a subset of its morphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus, Goldberg and Elhadad (2013) show that integrating an external wide-coverage lexicon with a treebank-trained PCFG parser improves parsing accuracy for Modern Hebrew, which is in line with earlier studies of part-of-speech tagging for morphologically rich languages (Hajiˇc, 2000). The sparsity of lexical features can also be tackled by the use of distributional word clusters as pioneered by Koo et al. (2008). In this paper, we present a transition-based model that jointly predicts complex morphological representations and dependency relations, generalizing the approach of Bohnet and Nivre (2012) to include the full range"
Q13-1034,P08-1043,0,0.0558272,"case the joint model is limited to basic part-ofspeech tags and does not involve the full complex of morphological features. An integrated approach to morphological and syntactic analysis can also be found in grammar-based dependency parsers, such as the ETAP-3 linguistic processor (Apresian et al., 2003), where morphological disambiguation is mostly carried out together with syntactic analysis. Finally, it is worth noting that joint models of morphology and syntax have been more popular in constituency-based statistical parsing (Cowan and Collins, 2005; Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). Another hypothesis from the literature is that the high type-token ratio resulting from large morphological paradigms leads to data sparseness when estimating the parameters of a statistical parsing model (Tsarfaty et al., 2010; Tsarfaty et al., 2013). In particular, for many words in the language, only a subset of its morphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus, Goldberg and Elhadad (2013) show that integrating an external wide-"
Q13-1034,P98-1080,1,0.597993,"Missing"
Q13-1034,A00-2013,0,0.0703501,"Missing"
Q13-1034,P07-2053,0,0.0543962,"Missing"
Q13-1034,I11-1136,0,0.0554181,"hang and Clark (2008), we assume that the score is given by a linear model whose feature representations decompose in the same way: s(x, C0,m ) = f (x, C0,m ) · w m X = f (x, ci , ti ) · w (2) i=0 Here, f (x, c, t) is a high-dimensional feature vector, where each component fi (x, c, t) is a nonnegative numerical feature (usually binary), and w is a weight vector of the same dimensionality, where each component wi is the real-valued weight of the feature fi (x, c, t). The choice of features to include in f (x, c, t) is discussed separately for each instantiation of the model in Sections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has show"
Q13-1034,P10-1110,0,0.18704,"near increase in running time, often quite marginal because |D |> |P × M |. This assumes that the lemma is predicted deterministically given a tag and a morphological description, an assumption that is enforced in all our experiments. 2.5 Learning In order to learn a weight vector w from a training set of sentences with gold parses, we use a variant of the structured perceptron, introduced by Collins (2002) and first used for transition-based parsing by Zhang and Clark (2008). We initialize all weights 3 While there exist exact dynamic programming algorithms for projective transition systems (Huang and Sagae, 2010; Kuhlmann et al., 2011) and even for restricted non-projective systems (Cohen et al., 2011), parsing is intractable for systems like ours that permit arbitrary non-projective trees. PARSE(x, w) 1 h0 .c ← cs (x) 2 h0 .s ← 0.0 3 h0 .f ← {0.0}dim(w) 4 B EAM ← [h0 ] 5 while ∃h ∈ B EAM : h.c 6∈ Ct 6 T MP ← [ ] 7 foreach h ∈ B EAM 8 foreach t ∈ T : P ERMISSIBLE(h.c, t) 9 h.f ← h.f + f(x, h.c, t) 10 h.s ← h.s + f(x, h.c, t) · w 11 h.c ← t(h.c) 12 T MP ← I NSERT(h, T MP) 13 B EAM ← P RUNE(T MP) 14 h∗ ← T OP(B EAM) 15 return Γh∗c Figure 2: Beam search algorithm for finding the best MSparse for input s"
Q13-1034,P08-1068,0,0.0257504,"rphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus, Goldberg and Elhadad (2013) show that integrating an external wide-coverage lexicon with a treebank-trained PCFG parser improves parsing accuracy for Modern Hebrew, which is in line with earlier studies of part-of-speech tagging for morphologically rich languages (Hajiˇc, 2000). The sparsity of lexical features can also be tackled by the use of distributional word clusters as pioneered by Koo et al. (2008). In this paper, we present a transition-based model that jointly predicts complex morphological representations and dependency relations, generalizing the approach of Bohnet and Nivre (2012) to include the full range of morphological information. We start by investigating different ways of integrating morphological features into the model, go on to examine the effect of using rule-based morphological analyzers to derive hard or soft constraints on the morphological analysis, and finally add word cluster features to combat lexical sparsity. We evaluate our methods on data from Czech, Finnish,"
Q13-1034,D10-1125,0,0.0274644,"Missing"
Q13-1034,P11-1068,0,0.0182537,"Missing"
Q13-1034,P11-1089,0,0.0278511,"al., 2010), as well as a special issue in Computational Linguistics (Tsarfaty et al., 2013) and a shared task on parsing morphologically rich languages.1 One hypothesized explanation for the lower parsing accuracy observed for richly inflected languages is the strict separation of morphological and syntactic analysis assumed in many parsing frameworks (Tsarfaty et al., 2010; Tsarfaty et al., 2013). This is true in particular for data-driven dependency parsers, which tend to assume that all morphological disambiguation has been performed before syntactic analysis begins. However, as argued by Lee et al. (2011), in morphologically rich languages there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. Lee et al. (2011) go on to show that a discriminative model for joint morphological disambiguation and dependency parsing gives consistent improvements in morphological and syntactic accuracy, compared to a pipeline model, for Ancient Greek, Czech, Hungarian and Latin. Similarly, Bohnet and Nivre (2012) propose a model for 1 See https://sites.google.com/site/spmrl2013/home/sharedtask. 415 Transactions of the Association for Computa"
Q13-1034,P06-1033,1,0.867756,"Missing"
Q13-1034,W09-3811,1,0.599747,"ng time, we terminate the beam search as soon as the hypothesis corresponding to the gold parse is pruned from the beam and then update with respect to the partial transition sequences constructed up to that point. Finally, we use the standard technique of averaging over all weight vectors seen in training, as originally proposed by Collins (2002). 4 Note that there may be more than one transition sequence corresponding to the gold parse, in which case we pick the canonical transition sequence that processes all left-dependents before right-dependents and applies the lazy swapping strategy of Nivre et al. (2009). 419 3 Data Sets and Resources Throughout the paper, we experiment with data from five languages: Czech, Finnish, German, Hungarian, and Russian. For each language, we use a morphologically and syntactically annotated corpus (treebank), divided into a training set, a development set and a test set. In addition, we use a lexicon generated by a rule-based morphological analyzer, and distributional word clusters derived from a large unlabeled corpus. Below we describe the specific resources used for each language. Table 1 provides descriptive statistics about the resources. Czech For training an"
Q13-1034,W03-3017,1,0.552345,"ections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark, 2008; Zhang and Nivre, 2012). While still not exact, beam search decoders explore a larger part of the search space than greedy parsers, which is likely to be especially important for joint models, where the search space is larger than for plain dependency parsing without morphology (even more so with the S WAP transition for nonprojectivity). Figure 2 outlines the beam search algorithm used for decoding with our model. Different instantiations of the model will require slightly different impl"
Q13-1034,W04-0308,1,0.586101,"e representations decompose in the same way: s(x, C0,m ) = f (x, C0,m ) · w m X = f (x, ci , ti ) · w (2) i=0 Here, f (x, c, t) is a high-dimensional feature vector, where each component fi (x, c, t) is a nonnegative numerical feature (usually binary), and w is a weight vector of the same dimensionality, where each component wi is the real-valued weight of the feature fi (x, c, t). The choice of features to include in f (x, c, t) is discussed separately for each instantiation of the model in Sections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark,"
Q13-1034,P09-1040,1,0.939947,"Γ = (A, π, µ, λ, δ) is an MS-parse for x. We take the initial configuration for a sentence x = w1 , . . . , wn to be cs (x) = ([0], [1, . . . , n], (∅, ⊥, ⊥, ⊥, ⊥)), where ⊥ is the function that is undefined for all arguments, and we take the set Ct of terminal configurations to be the set of all configurations of the form c = ([0], [ ], Γ) (for any Γ). The MS-parse defined for x by c = (Σ, B, (A, π, µ, λ, δ)) is Γc = (A, π, µ, λ, δ), and the MS-parse defined for x by a complete transition sequence C0,m is Γtm (cm ) . The set T of transitions is shown in Figure 1. It is based on the system of Nivre (2009), where a dependency tree is built by repeated applications of the L EFT-A RCd and R IGHT-A RCd transitions, which add an arc (with some label d ∈ D) between the two topmost nodes on the stack (with the leftmost or rightmost node as the dependent, respectively). The S HIFT transition is used to move nodes from the buffer to the stack, and the S WAP transition is used to permute nodes in order to allow non-projective dependencies. Bohnet and Nivre (2012) modified this system by replacing the simple S HIFT transition by S HIFTp , which not only moves a node from the buffer to the stack but also"
Q13-1034,W11-4644,0,0.0793275,"Missing"
Q13-1034,W09-3829,0,0.0396525,"Missing"
Q13-1034,schmid-etal-2004-smor,0,0.0195468,"Missing"
Q13-1034,seeker-kuhn-2012-making,0,0.0289485,"Missing"
Q13-1034,C10-2129,1,0.896137,"Missing"
Q13-1034,spoustova-spousta-2012-high,0,0.0221884,"Missing"
Q13-1034,W07-2218,0,0.0373151,") = f (x, C0,m ) · w m X = f (x, ci , ti ) · w (2) i=0 Here, f (x, c, t) is a high-dimensional feature vector, where each component fi (x, c, t) is a nonnegative numerical feature (usually binary), and w is a weight vector of the same dimensionality, where each component wi is the real-valued weight of the feature fi (x, c, t). The choice of features to include in f (x, c, t) is discussed separately for each instantiation of the model in Sections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark, 2008; Zhang and Nivre, 2012). While still not exact, beam search deco"
Q13-1034,tron-etal-2006-morphdb,0,0.119292,"Missing"
Q13-1034,W10-1401,0,0.0166567,"Missing"
Q13-1034,J13-1003,1,0.829911,"Missing"
Q13-1034,P06-3009,0,0.0901789,"Chinese and English), although in this case the joint model is limited to basic part-ofspeech tags and does not involve the full complex of morphological features. An integrated approach to morphological and syntactic analysis can also be found in grammar-based dependency parsers, such as the ETAP-3 linguistic processor (Apresian et al., 2003), where morphological disambiguation is mostly carried out together with syntactic analysis. Finally, it is worth noting that joint models of morphology and syntax have been more popular in constituency-based statistical parsing (Cowan and Collins, 2005; Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). Another hypothesis from the literature is that the high type-token ratio resulting from large morphological paradigms leads to data sparseness when estimating the parameters of a statistical parsing model (Tsarfaty et al., 2010; Tsarfaty et al., 2013). In particular, for many words in the language, only a subset of its morphological forms will be observed at training time. This suggests that using rule-based morphological analyzers or other lexical resources may be a viable strategy to improve coverage and performance. Thus, Goldberg and E"
Q13-1034,varadi-2002-hungarian,0,0.105288,"Missing"
Q13-1034,W03-3023,0,0.0799723,"tantiation of the model in Sections 4–6. 2 Hatori et al. (2011) previously made the same modification to the arc-standard system (Nivre, 2004), without the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark, 2008; Zhang and Nivre, 2012). While still not exact, beam search decoders explore a larger part of the search space than greedy parsers, which is likely to be especially important for joint models, where the search space is larger than for plain dependency parsing without morphology (even more so with the S WAP transition for nonprojectivity). Figure 2 outlines the beam search algorithm used for decoding with our model. Different instantiations of the model will require slightly"
Q13-1034,D08-1059,0,0.738873,"so that a node moved from the buffer to the stack is assigned not only a tag p but also a morphological description m and a lemma l. In this way, we get a joint model for the prediction of part-ofspeech tags, morphological features, lemmas, and dependency trees. 2.3 Scoring In transition-based parsing, we score parses in an indirect fashion by scoring transition sequences. In general, we assume that the score function s factors by configuration-transition pairs: s(x, C0,m ) = m X s(x, ci , ti ) (1) i=0 Moreover, when using structured learning, as first proposed for transition-based parsing by Zhang and Clark (2008), we assume that the score is given by a linear model whose feature representations decompose in the same way: s(x, C0,m ) = f (x, C0,m ) · w m X = f (x, ci , ti ) · w (2) i=0 Here, f (x, c, t) is a high-dimensional feature vector, where each component fi (x, c, t) is a nonnegative numerical feature (usually binary), and w is a weight vector of the same dimensionality, where each component wi is the real-valued weight of the feature fi (x, c, t). The choice of features to include in f (x, c, t) is discussed separately for each instantiation of the model in Sections 4–6. 2 Hatori et al. (2011)"
Q13-1034,P11-2033,1,0.457721,"ection 7, we present an empirical analysis that gives further support for this choice, at least for the languages considered in this paper. Note also that the choice is not motivated by efficiency concerns, since increasing the values of kp and km has only a marginal effect on running time, as explained in Section 2.4. Finally, the choice not to consider k-best lemmas is dictated by the fact that our lemmatizer only provides a 1-best analysis. For the first three models, we use the same feature representations as Bohnet and Nivre (2012),9 consisting of their adaptation of the features used by Zhang and Nivre (2011), the graph completion features of Bohnet and Kuhn (2012), and the special features over k-best tags introduced specifically for joint tagging and parsing by Bohnet and Nivre (2012). For the J OINT model, we simply add features over the k-best morphological descriptions analogous to the features over k-best tags.10 Experimental results for these four models can be found in Table 2. From the P IPELINE results, we see that the 1-best accuracy of the preprocessing tagger ranges from 95.0 (Finnish) to 99.2 (Czech) for POS, and from 89.4 (Finnish) to 96.5 (Hungarian) for MOR. The lemmatizer does a"
Q13-1034,C12-2136,1,0.0524599,"out the S WAP transition. Similarly, Titov and Henderson (2007) added a word parameter to the S HIFT transition to get a joint model of word strings and dependency trees. A similar model was considered but finally not used by Gesmundo et al. (2009). 418 2.4 Decoding Exact decoding for transition-based parsing is hard in general.3 Early transition-based parsers mostly relied on greedy, deterministic decoding, which makes for very efficient parsing (Yamada and Matsumoto, 2003; Nivre, 2003), but research has shown that accuracy can be improved by using beam search instead (Zhang and Clark, 2008; Zhang and Nivre, 2012). While still not exact, beam search decoders explore a larger part of the search space than greedy parsers, which is likely to be especially important for joint models, where the search space is larger than for plain dependency parsing without morphology (even more so with the S WAP transition for nonprojectivity). Figure 2 outlines the beam search algorithm used for decoding with our model. Different instantiations of the model will require slightly different implementations of the permissibility condition invoked in line 8, which can be used to filter out labels that are improbable or incom"
Q13-1034,C98-1077,0,\N,Missing
Q13-1034,W09-1201,1,\N,Missing
Q13-1034,D07-1096,1,\N,Missing
S14-2122,N07-1038,0,0.229175,"e to ensure only the most relevant terms remain. These techniques include statistical association tests (Yi et al., 2003), associative mining rules with additional rule-based post-processing steps (Hu and Liu, 2004), and measures of association with certain pre-defined classes of words, such as part-whole relation indicators (Popescu and Etzioni, 2005). 2.2 I liked the service and the staff, but not the food. Aspect Category Recognition Aspect category recognition is often addressed as a text classification problem, where a classifier is learned from reviews manually tagged for aspects (e.g., Snyder and Barzilay, 2007, Ganu et al., 2009). Titov and McDonald (2008) present an approach which jointly detects aspect categories and their sentiment using a classifier trained on topics discovered via Multi-Grain LDA and star ratings available in training data. Zhai et al. (2010) presented an approach based on ExpectationMaximization to group aspect expressions into user-defined aspect categories. aspect terms are service, staff and food, where the first two are evaluated positively and the last one negatively; and aspect categories are S ERVICE and F OOD, where the former is associated with positive sentiment and"
S14-2122,baccianella-etal-2010-sentiwordnet,0,0.0297203,"Missing"
S14-2122,D13-1170,0,0.0148866,"s”, phrases causing the polarity of a lexical item to reverse. Early work on detection of polarity shifters used surface-level patterns (Yu and Hatzivassilouglu, 2003; Hu and Liu, 2004). Moilanen and Pulman (2007) provide a logic-oriented framework to compute the polarity of grammatical structures, that is capable of dealing with phenomena such as sentiment propagation, polarity reversal, and polarity conflict. Several papers looked at different ways to use syntactic dependency information in a machine learning framework, to better account for negations and their scope (Nakagawa et al., 2010; Socher et al., 2013). To adapt a generic sentiment lexicon to a new application domain, previous work exploited semantic relations encoded in WordNet (Kim and Hovy, 2006), unannotated data (Li et al, 2012), or queries to a search engine (Taboada et al., 2006). 3 Our Approach In the following sections, we will describe our approach to each stage of the Shared Task, reporting experiments on the provided training data using a 10-fold cross-validation. 3.1 • Normalized form: the surface form of the term after normalization; • Term lemmas: lemmas of content words found in the term; Aspect Term Extraction • Lexicon ter"
S14-2122,D12-1133,1,0.794033,"ns encoded in WordNet (Kim and Hovy, 2006), unannotated data (Li et al, 2012), or queries to a search engine (Taboada et al., 2006). 3 Our Approach In the following sections, we will describe our approach to each stage of the Shared Task, reporting experiments on the provided training data using a 10-fold cross-validation. 3.1 • Normalized form: the surface form of the term after normalization; • Term lemmas: lemmas of content words found in the term; Aspect Term Extraction • Lexicon term: if the term is in the lexicon; During pre-processing training data was parsed using a dependency parser (Bohnet and Nivre, 2012), and sentiment words were recognized in it using a sentiment lexicon (see Section 6.1). Candidate terms were extracted as single nouns, noun phrases, adjectives and verbs, enforcing certain exceptions as detailed in the annotation guidelines for the Shared Task (Pontiki et al., 2014), namely: • Lexicon lemmas ratio: the ratio of lexicon lemmas in the term; • Unigram: 3 unigrams on either side of the term; • Bigrams: The two bigrams around the term; • Adj+term: If an adjective depends on the term1 or related to it via a link verb (“be”, “get”, “become”, etc); • Sentiment words were not allowed"
S14-2122,taboada-etal-2006-methods,0,0.0455523,"framework to compute the polarity of grammatical structures, that is capable of dealing with phenomena such as sentiment propagation, polarity reversal, and polarity conflict. Several papers looked at different ways to use syntactic dependency information in a machine learning framework, to better account for negations and their scope (Nakagawa et al., 2010; Socher et al., 2013). To adapt a generic sentiment lexicon to a new application domain, previous work exploited semantic relations encoded in WordNet (Kim and Hovy, 2006), unannotated data (Li et al, 2012), or queries to a search engine (Taboada et al., 2006). 3 Our Approach In the following sections, we will describe our approach to each stage of the Shared Task, reporting experiments on the provided training data using a 10-fold cross-validation. 3.1 • Normalized form: the surface form of the term after normalization; • Term lemmas: lemmas of content words found in the term; Aspect Term Extraction • Lexicon term: if the term is in the lexicon; During pre-processing training data was parsed using a dependency parser (Bohnet and Nivre, 2012), and sentiment words were recognized in it using a sentiment lexicon (see Section 6.1). Candidate terms wer"
S14-2122,P08-1036,0,0.0388042,". These techniques include statistical association tests (Yi et al., 2003), associative mining rules with additional rule-based post-processing steps (Hu and Liu, 2004), and measures of association with certain pre-defined classes of words, such as part-whole relation indicators (Popescu and Etzioni, 2005). 2.2 I liked the service and the staff, but not the food. Aspect Category Recognition Aspect category recognition is often addressed as a text classification problem, where a classifier is learned from reviews manually tagged for aspects (e.g., Snyder and Barzilay, 2007, Ganu et al., 2009). Titov and McDonald (2008) present an approach which jointly detects aspect categories and their sentiment using a classifier trained on topics discovered via Multi-Grain LDA and star ratings available in training data. Zhai et al. (2010) presented an approach based on ExpectationMaximization to group aspect expressions into user-defined aspect categories. aspect terms are service, staff and food, where the first two are evaluated positively and the last one negatively; and aspect categories are S ERVICE and F OOD, where the former is associated with positive sentiment and the latter with negative. It should be noted t"
S14-2122,P13-1173,0,0.0316324,"Missing"
S14-2122,N06-1026,0,0.0218268,"ssilouglu, 2003; Hu and Liu, 2004). Moilanen and Pulman (2007) provide a logic-oriented framework to compute the polarity of grammatical structures, that is capable of dealing with phenomena such as sentiment propagation, polarity reversal, and polarity conflict. Several papers looked at different ways to use syntactic dependency information in a machine learning framework, to better account for negations and their scope (Nakagawa et al., 2010; Socher et al., 2013). To adapt a generic sentiment lexicon to a new application domain, previous work exploited semantic relations encoded in WordNet (Kim and Hovy, 2006), unannotated data (Li et al, 2012), or queries to a search engine (Taboada et al., 2006). 3 Our Approach In the following sections, we will describe our approach to each stage of the Shared Task, reporting experiments on the provided training data using a 10-fold cross-validation. 3.1 • Normalized form: the surface form of the term after normalization; • Term lemmas: lemmas of content words found in the term; Aspect Term Extraction • Lexicon term: if the term is in the lexicon; During pre-processing training data was parsed using a dependency parser (Bohnet and Nivre, 2012), and sentiment wor"
S14-2122,W03-1017,0,0.101351,"Missing"
S14-2122,N10-1120,0,0.0318222,"gnize “polarity shifters”, phrases causing the polarity of a lexical item to reverse. Early work on detection of polarity shifters used surface-level patterns (Yu and Hatzivassilouglu, 2003; Hu and Liu, 2004). Moilanen and Pulman (2007) provide a logic-oriented framework to compute the polarity of grammatical structures, that is capable of dealing with phenomena such as sentiment propagation, polarity reversal, and polarity conflict. Several papers looked at different ways to use syntactic dependency information in a machine learning framework, to better account for negations and their scope (Nakagawa et al., 2010; Socher et al., 2013). To adapt a generic sentiment lexicon to a new application domain, previous work exploited semantic relations encoded in WordNet (Kim and Hovy, 2006), unannotated data (Li et al, 2012), or queries to a search engine (Taboada et al., 2006). 3 Our Approach In the following sections, we will describe our approach to each stage of the Shared Task, reporting experiments on the provided training data using a 10-fold cross-validation. 3.1 • Normalized form: the surface form of the term after normalization; • Term lemmas: lemmas of content words found in the term; Aspect Term Ex"
S14-2122,S14-2004,0,0.0224324,"ata using a 10-fold cross-validation. 3.1 • Normalized form: the surface form of the term after normalization; • Term lemmas: lemmas of content words found in the term; Aspect Term Extraction • Lexicon term: if the term is in the lexicon; During pre-processing training data was parsed using a dependency parser (Bohnet and Nivre, 2012), and sentiment words were recognized in it using a sentiment lexicon (see Section 6.1). Candidate terms were extracted as single nouns, noun phrases, adjectives and verbs, enforcing certain exceptions as detailed in the annotation guidelines for the Shared Task (Pontiki et al., 2014), namely: • Lexicon lemmas ratio: the ratio of lexicon lemmas in the term; • Unigram: 3 unigrams on either side of the term; • Bigrams: The two bigrams around the term; • Adj+term: If an adjective depends on the term1 or related to it via a link verb (“be”, “get”, “become”, etc); • Sentiment words were not allowed as part of terms; • Noun phrases with all elements capitalized and acronyms were excluded, under the assumption they refer to brands rather than product aspects; • Sentiment+term: If a sentiment word depends on the term or related via a link verb; • Nouns referring to the product cla"
S14-2122,H05-1043,0,0.0489812,"e sentence. For example, in: Related Work Aspect Term Extraction To recognize terms that express key notions in a product or service review, a common general approach has been to extract nouns and noun phrases as potential terms and then apply a certain filtering technique to ensure only the most relevant terms remain. These techniques include statistical association tests (Yi et al., 2003), associative mining rules with additional rule-based post-processing steps (Hu and Liu, 2004), and measures of association with certain pre-defined classes of words, such as part-whole relation indicators (Popescu and Etzioni, 2005). 2.2 I liked the service and the staff, but not the food. Aspect Category Recognition Aspect category recognition is often addressed as a text classification problem, where a classifier is learned from reviews manually tagged for aspects (e.g., Snyder and Barzilay, 2007, Ganu et al., 2009). Titov and McDonald (2008) present an approach which jointly detects aspect categories and their sentiment using a classifier trained on topics discovered via Multi-Grain LDA and star ratings available in training data. Zhai et al. (2010) presented an approach based on ExpectationMaximization to group aspec"
S14-2122,P92-1053,0,0.126715,"Eval 2014), pages 683–687, Dublin, Ireland, August 23-24, 2014. 2.3 Candidate terms that exactly overlapped with manually annotated terms were discarded, while those that did not were used as negative examples of aspect terms. In order to provide the term extraction process with additional lexical knowledge, from the training data we extracted those manually annotated terms that corresponded to a single aspect category. Then the set of terms belonging to each category was augmented using WordNet: first we determined the 5 most prominent hyperonyms of these terms in the WordNet hierarchy using Resnik (1992)’s algorithm for learning a class in a semantic hierarchy that best represents selectional preferences of a verb, additionally requiring that each hypernym is at least 7 nodes away from the root, to make them sufficiently specific. Then we obtained all lexical items that belong to children synsets of these hypernyms, and further extended these lexical items with their meronyms and morphological derivatives. The resulting set of lexical items was later used as an extended aspect term lexicon. We additionally created a list of all individual lemmas of content words found in this lexicon. For eac"
S14-2122,H05-2017,0,\N,Missing
S14-2122,P12-1043,0,\N,Missing
S14-2122,R09-1048,0,\N,Missing
U04-1010,P99-1017,0,0.0157516,"(set; (van Deemter, 2002)) extends the basic approach to references to sets, as in the red cups. Some approaches combine algorithms which reuse only parts of other algorithms: the Branch and Bound (bab; (Krahmer et al., 2003)) algorithm uses the Full Brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using a graph-based technique. We have identified here what we believe to be the most cited strands of research in this area, but of course there are many other algorithms described in the literature: see, for example, (Horacek, 1997; Bateman, 1999; Stone, 2000). Space limitations prevent a complete summary of this other work here, but our intention is to extend the analysis presented in this paper to as many of these other algorithms as possible. All these algorithms focus on the generation of definite references; they are typically embedded in a higher-level algorithm that includes cases for when the entity has not been previously mentioned (thus leading to an initial indefinite reference) or when the referent is in focus (thus leading to a pronominal reference); see, for example, (Dale, 1989; Krahmer and Theune, 2002; Dale, 2003). 3"
U04-1010,E91-1028,1,0.832964,"articular, see (Winograd, 1972; McDonald, 1980; Appelt, 1981), the first formally explicit algorithm was introduced in Dale (1989). This algorithm, which we will refer to as the Full Brevity (fb) algorithm, is still frequently used as a basis for other gre algorithms. The fb algorithm searches for the best solution amongst all possible referring expressions for an entity; the algorithm derives the smallest set of attributes for the referent in question, producing a referring expression that is both adequate and efficient. This initial algorithm limited its application to one-place predicates. Dale and Haddock (1991) introduced a constraint-based procedure that could generate referring expressions involving relations (henceforth ir), using a greedy heuristic to guide the search. As a response to the computational complexity of greedy algorithms, (Reiter and Dale, 1992; Dale and Reiter, 1995) introduced the psycholinguistically motivated Incremental Algorithm (ia). The most used and adapted algorithm, this is based on the observation that people often produce referring expressions which are informationally redundant; the algorithm uses a preference ordering over the attributes to be used in a referring exp"
U04-1010,P89-1009,1,0.697278,"ade it very difficult to compare and contrast the algorithms provided in any meaningful way. In this paper, we propose a characterisation of the problem of referring expression generation as a search problem; this allows us to recast existing algorithms in a way that makes their similarities and differences clear. 1 Introduction A major component task in natural language generation (nlg) is the generation of referring expressions: given an entity that we want to refer to, how do we determine the content of a referring expression that uniquely identifies that intended referent? Since at least (Dale, 1989), the standard conception of this task in the literature has been as follows: 1. We assume we have a knowledge base that characterises the entities in the domain in terms of a set of attributes and the values that the entities have for these attributes; so, for example, our knowledge base might represent the fact that entity e1 has the value cup for the attribute type, and the value red for the attribute colour. 2. In a typical context where we want to refer to some ei , which we call the intended referent, there will be other entities from which the intended referent must be distinguished; th"
U04-1010,U03-1002,1,0.85466,"; Bateman, 1999; Stone, 2000). Space limitations prevent a complete summary of this other work here, but our intention is to extend the analysis presented in this paper to as many of these other algorithms as possible. All these algorithms focus on the generation of definite references; they are typically embedded in a higher-level algorithm that includes cases for when the entity has not been previously mentioned (thus leading to an initial indefinite reference) or when the referent is in focus (thus leading to a pronominal reference); see, for example, (Dale, 1989; Krahmer and Theune, 2002; Dale, 2003). 3 gre from the Perspective of Problem Solving With so many algorithms to choose from, it would be useful to have a uniform framework in which to discuss and compare algorithms; unfortunately, this is rather difficult given the variety of different approaches that have been taken to the problem. Within the wider context of ai, Russell and Norvig (2003) present an elegant definition of a general algorithm for problem solving by search. The search graph consists of nodes with the components state and path-cost; the problem is represented by an initial-state, an expandmethod which identifies new"
U04-1010,P97-1027,0,0.0201059,"Sets algorithm (set; (van Deemter, 2002)) extends the basic approach to references to sets, as in the red cups. Some approaches combine algorithms which reuse only parts of other algorithms: the Branch and Bound (bab; (Krahmer et al., 2003)) algorithm uses the Full Brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using a graph-based technique. We have identified here what we believe to be the most cited strands of research in this area, but of course there are many other algorithms described in the literature: see, for example, (Horacek, 1997; Bateman, 1999; Stone, 2000). Space limitations prevent a complete summary of this other work here, but our intention is to extend the analysis presented in this paper to as many of these other algorithms as possible. All these algorithms focus on the generation of definite references; they are typically embedded in a higher-level algorithm that includes cases for when the entity has not been previously mentioned (thus leading to an initial indefinite reference) or when the referent is in focus (thus leading to a pronominal reference); see, for example, (Dale, 1989; Krahmer and Theune, 2002;"
U04-1010,J03-1003,0,0.0309219,"Missing"
U04-1010,C92-1038,1,0.589916,"lgorithms. The fb algorithm searches for the best solution amongst all possible referring expressions for an entity; the algorithm derives the smallest set of attributes for the referent in question, producing a referring expression that is both adequate and efficient. This initial algorithm limited its application to one-place predicates. Dale and Haddock (1991) introduced a constraint-based procedure that could generate referring expressions involving relations (henceforth ir), using a greedy heuristic to guide the search. As a response to the computational complexity of greedy algorithms, (Reiter and Dale, 1992; Dale and Reiter, 1995) introduced the psycholinguistically motivated Incremental Algorithm (ia). The most used and adapted algorithm, this is based on the observation that people often produce referring expressions which are informationally redundant; the algorithm uses a preference ordering over the attributes to be used in a referring expression, accumulating those attributes which rule out at least one potential distractor. In recent years there have been a number of important extensions to the ia. The Context-Sensitive extension (cs; (Krahmer and Theune, 2002)) is able to generate referr"
U04-1010,W00-1416,0,0.0267671,"ter, 2002)) extends the basic approach to references to sets, as in the red cups. Some approaches combine algorithms which reuse only parts of other algorithms: the Branch and Bound (bab; (Krahmer et al., 2003)) algorithm uses the Full Brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using a graph-based technique. We have identified here what we believe to be the most cited strands of research in this area, but of course there are many other algorithms described in the literature: see, for example, (Horacek, 1997; Bateman, 1999; Stone, 2000). Space limitations prevent a complete summary of this other work here, but our intention is to extend the analysis presented in this paper to as many of these other algorithms as possible. All these algorithms focus on the generation of definite references; they are typically embedded in a higher-level algorithm that includes cases for when the entity has not been previously mentioned (thus leading to an initial indefinite reference) or when the referent is in focus (thus leading to a pronominal reference); see, for example, (Dale, 1989; Krahmer and Theune, 2002; Dale, 2003). 3 gre from the P"
U04-1010,J02-1003,0,0.0322508,"Missing"
U04-1010,H89-1033,0,0.0115928,"problems in an elegant and uniform way (see, for example, (Simon and Newell, 1963); (Russell and Norvig, 2003)), sketching how gre algorithms can be expressed in terms of problem-solving by search. In Section 4, we explore how the most well-known algorithms can be expressed in this framework. In Section 5, we discuss how this approach enables a more fruitful comparison of existing algorithms, and we point to ways of taking this work further. 2 A Brief Review of Work To Date Although the task of referring expression generation is discussed informally in earlier work on nlg (in particular, see (Winograd, 1972; McDonald, 1980; Appelt, 1981), the first formally explicit algorithm was introduced in Dale (1989). This algorithm, which we will refer to as the Full Brevity (fb) algorithm, is still frequently used as a basis for other gre algorithms. The fb algorithm searches for the best solution amongst all possible referring expressions for an entity; the algorithm derives the smallest set of attributes for the referent in question, producing a referring expression that is both adequate and efficient. This initial algorithm limited its application to one-place predicates. Dale and Haddock (1991) introd"
W00-1436,A97-1039,0,0.109442,"Missing"
W00-1436,C92-3158,0,\N,Missing
W01-0807,W96-0404,0,\N,Missing
W01-0807,W98-1406,1,\N,Missing
W01-0807,A00-1009,0,\N,Missing
W01-0807,C00-2149,0,\N,Missing
W01-0807,A97-1039,0,\N,Missing
W01-0807,P98-1060,0,\N,Missing
W01-0807,C98-1058,0,\N,Missing
W01-0807,W00-1436,1,\N,Missing
W08-1132,2007.mtsummit-ucnlg.15,1,0.741143,"expressions and expect then the next expression in the same style? And are we confused when we don’t get what we expact? Or does FBN look too much on the expressions of the humans and too less on the domain? We hope to get answers for these questions from the shared task evaluation of IS-FP. 1.2 The IS-FP Algorithm The basis for the IS-FP algorithm is an extended full brevity implementation in terms of problem solving by search which computes all referring expression, cf. (Bohnet and Dale, 2005). IS-FP uses also the nearest neighbour technique like the IS-FBN algorithm that was introduced by Bohnet (2007). With the nearest neighbour technique, IS-FP selects the expressions which are most similar to the referring expressions of the same human and a human that builds referring expressions similar. The similarity is computed as the average of all dice values between all combinations of the available trails for two humans. From the result of the nearest neighbour evaluation, FP selects the shortest and if still more than one expressions remain then it computes the similarity among them and chooses the most typical and finally, if still alternatives remain, it selects one with the attributes having"
W08-1132,mille-wanner-2008-making,0,0.0142791,"surface syntactic dependency tree, cf. (Mel’ˇcuk, 1988). For the realization of the text, we use the Text Generator and Linguistic Environment MATE, cf. (Bohnet, 2006). We reportet the first time about MATE on the first International Natural Language Generation Conference, cf. (Bohnet et al., 2000). It was since then continuously enhanced and in the last years, large grammars for several languages such as Catalan, English, Finnish, French, German, Polish, Portougees have been developed within the European Project MARQUIS and PatExpert, cf. (Wanner et al., 2007), (Lareau and Wanner, 2007) and (Mille and Wanner, 2008). 2.1 The Referring Expression Models A learning program builds a Referring Expression Model for each person that contributed referring expression to the corpus. The model contains the following information: (1) The lexicalization for the 208 values of a attribute such as couch for the value sofa, man for value person, etc. (2) The prefered usage of determiners for the type that can be definite (the), indefinite (a), no article. (3) The syntactic preferences such as the top left chair, the chair at the bottom to the left, etc. The information about the determiner and the lexicalization is coll"
W08-1132,W00-1436,1,\N,Missing
W09-1210,W06-2920,0,0.0315882,"nts behind the first ranked system. For this task, our system has the highest accuracy for English with 89.88, German with 87.48 and the out-of-domain data in average with 78.79. The semantic role labeler works not as well as our parser and we reached therefore the fourth place (ranked by the macro F1 score) in the joint task for syntactic and semantic dependency parsing. 2 Parsing Algorithm 1 Introduction Depedendency parsing and semantic role labeling improved in the last years significantly. One of the reasons are CoNLL shared tasks for syntactic dependency parsing in the years 2006, 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007) and the CoNLL shared task for joint parsing of syntactic and semantic dependencies in the year 2008 and of cause this shared task in 2009, cf. (Surdeanu et al., 2008; We adopted the second order MST parsing algorithm as outlined by Eisner (1996). This algorithm has a higher accuracy compared to the first order parsing algorithm since it considers also siblings and grandchildren of a node. Eisner‘s second order approach can compute a projective dependency tree within cubic time (O(n3 )). Both algorithms are bottom up parsing algorithms based on dynamic programming similar"
W09-1210,burchardt-etal-2006-salsa,0,0.0231711,"Missing"
W09-1210,W02-1001,0,0.01103,"g dependency trees → − − w (0) = 0, → v =0 for n = 1 to N for x = 1 to X wi+1 = update wi according to instance (Sx , Tx ) v = v + wi+1 i=i+1 end for end for w = v/(N ∗ X) The inner loop iterates over all sentences x of the training set while the outer loop repeats the train i times. The algorithm returns an averaged weight vector and uses an auxiliary weight vector v that accumulates the values of w after each iteration. At the end, the algorithm computes the average of all weight vectors by dividing it by the number of training iterations and sentences. This helps to avoid overfitting, cf. (Collins, 2002). The update function computes the update to the weight vector wi during the training so that wrong classified edges of the training instances are possibly correctly classified. This is computed by increasing the weight for the correct features and decreasing the weight for wrong features of the vectors for the tree −→ of the training set fTx ∗ wi and the vector for the −→ predicted dependency tree fTx′ ∗ wi . The update function tries to keep the change to the parameter vector wi as small as possible for cor69 rectly classifying the current instance with a difference at least as large as the"
W09-1210,C96-1058,0,0.189781,"fornia bohnet@icsi.Berkeley.edu Abstract Hajiˇc et al., 2009). The CoNLL Shared Task 2009 is to parse syntactic and semantic dependencies of seven languages. Therefore, training and development data in form of annotated corpora for Catalan, Chinese, Czech, English, German, Japanese and Spanish is provided, cf. (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Kawahara et al., 2002). There are two main approaches to dependency parsing: Maximum Spanning Tree (MST) based dependency parsing and Transition based dependency parsing, cf. (Eisner, 1996; Nivre et al., 2004; McDonald and Pereira, 2006). Our system uses the first approach since we saw better chance to improve the parsing speed and additionally, the MST had so far slightly better parsing results. For the task of semantic role labeling, we adopted a pipeline architecture where we used for each step the same learning technique (SVM) since we opted for the possibility to build a synchronous combined parser with one score function. In this paper, we describe our system for the 2009 CoNLL shared task for joint parsing of syntactic and semantic dependency structures of multiple langu"
W09-1210,W08-2123,0,0.239687,"Missing"
W09-1210,kawahara-etal-2002-construction,0,0.0381631,"Missing"
W09-1210,E06-1011,0,0.613105,"stract Hajiˇc et al., 2009). The CoNLL Shared Task 2009 is to parse syntactic and semantic dependencies of seven languages. Therefore, training and development data in form of annotated corpora for Catalan, Chinese, Czech, English, German, Japanese and Spanish is provided, cf. (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Kawahara et al., 2002). There are two main approaches to dependency parsing: Maximum Spanning Tree (MST) based dependency parsing and Transition based dependency parsing, cf. (Eisner, 1996; Nivre et al., 2004; McDonald and Pereira, 2006). Our system uses the first approach since we saw better chance to improve the parsing speed and additionally, the MST had so far slightly better parsing results. For the task of semantic role labeling, we adopted a pipeline architecture where we used for each step the same learning technique (SVM) since we opted for the possibility to build a synchronous combined parser with one score function. In this paper, we describe our system for the 2009 CoNLL shared task for joint parsing of syntactic and semantic dependency structures of multiple languages. Our system combines and implements efficien"
W09-1210,P05-1012,0,0.104305,"erefore by starting with the highest scoring projective tree, typically the highest scoring non-projective tree is only a small number of transformations away. Our experiments showed that with the nonprojective Approximate Dependency Parsing Algorithm and a threshold for the improvment of score higher than about 0.7, the parsing accuracy improves even for English slightly. With a threshold of 1.1, we got the highest improvements. 5 Learning Framework As learning technique, we use Margin Infused Relaxed Algorithm (MIRA) as developed by Crammer et al. (2003) and applied to dependency parsing by McDonald et al. (2005). The online Algorithm in Figure 1 processes one training instance on each iteration, and updates the parameters accordingly. Algorithm 1: MIRA τ = {Sx , Tx }X x=1 // The set of training data consists // of sentences and the corresponding dependency trees → − − w (0) = 0, → v =0 for n = 1 to N for x = 1 to X wi+1 = update wi according to instance (Sx , Tx ) v = v + wi+1 i=i+1 end for end for w = v/(N ∗ X) The inner loop iterates over all sentences x of the training set while the outer loop repeats the train i times. The algorithm returns an averaged weight vector and uses an auxiliary weight v"
W09-1210,W06-2932,0,0.00633955,"or and weight vector. In order to compute the weight vector, we reimplemented the support vector machine MIRA which implements online Margin Infused Relaxed Algorithm, cf. (Crammer et al., 2003). 3 Labeled Dependency Parsing The second order parsing algorithm builds an unlabeled dependency tree. However, all dependency tree banks of the shared task provide trees with edge labels. The following two approaches are common to solve this problem. An additional algorithm labels the edges or the parsing algorithm itself is extended and the labeling algorithm is integrated into the parsing algorithm. McDonald et al. (2006) use an additional algorithm. Their two stage model has a good computational complexity since the labeling algorithm contributes again only a cubic time complexity to the algorithm and keeps therefore the joint algorithm still cubic. The algorithm selects the highest scored label due to the score function score(wi , label) + score(wj , label) and inserts the highest scored label into a matrix. The scores are also used in the parsing algorithms and added to 68 the edge scores which improves the overall parsing results as well. In the first order parsing scenario, this procedure is sufficient si"
W09-1210,W04-2705,0,0.116956,"Missing"
W09-1210,P05-1013,0,0.1768,"Missing"
W09-1210,W04-2407,0,0.0140369,"icsi.Berkeley.edu Abstract Hajiˇc et al., 2009). The CoNLL Shared Task 2009 is to parse syntactic and semantic dependencies of seven languages. Therefore, training and development data in form of annotated corpora for Catalan, Chinese, Czech, English, German, Japanese and Spanish is provided, cf. (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Kawahara et al., 2002). There are two main approaches to dependency parsing: Maximum Spanning Tree (MST) based dependency parsing and Transition based dependency parsing, cf. (Eisner, 1996; Nivre et al., 2004; McDonald and Pereira, 2006). Our system uses the first approach since we saw better chance to improve the parsing speed and additionally, the MST had so far slightly better parsing results. For the task of semantic role labeling, we adopted a pipeline architecture where we used for each step the same learning technique (SVM) since we opted for the possibility to build a synchronous combined parser with one score function. In this paper, we describe our system for the 2009 CoNLL shared task for joint parsing of syntactic and semantic dependency structures of multiple languages. Our system com"
W09-1210,J05-1004,0,0.00782743,"Ghz as well for our implementation (2). For system (3), we use a computer with Intel i7 3.2 Ghz which is faster than the MacPro. For all systems, we use 10 training iterations for the SVM Mira. use the system (3) with integrated labeling. 8 Semantic Role Labeling The semantic role labeler is implemented as a pipeline architecture. The components of the pipeline are predicate selection (PS), argument identification (AI), argument classification (AC), and word sense disambiguation (WSD). In order to select the predicates, we look up the lemmas in the Prob Bank, Nom Bank, etc. if available, cf. (Palmer et al., 2005; Meyers et al., 2004). For all other components, we use the support vector machine MIRA to select and classify the semantic role labels as well as to disambiguate the word senese. The AI component identifies the arguments of each predicate. It iterates over the predicates and over the words of a sentence. In the case that the score function is large or equal to zero the argument is added to the set of arguments of the predicate in question. Table 5 lists for the attribute identification and semantic role labeling. The argument classification algorithm labels each Language Catalan LAS Semantic"
W09-1210,W08-2121,0,0.0915396,"Missing"
W09-1210,taule-etal-2008-ancora,0,0.0861953,"Missing"
W09-1210,D07-1096,0,\N,Missing
W09-2818,W07-2302,0,\N,Missing
W10-4230,W09-2818,1,0.789022,"as to choose. This task is challenging because of the following aspects: 1. The data is imperfect as it is a patchwork of multiple authors’ writing. 2. The problem is hard to handle with a classifier because text is predicted, not classes. 3. The problem has a complex graph structure. 4. Some decisions are recursive for embedded references, i.e. “his father”. 5. Syntactic/semantic features cannot be extracted with a classical parser because the word sequence is latent. We do not deal with all of these challenges but we try to mitigate their impact. Our system extends our approach for GREC’09 (Favre and Bonhet, 2009). We use a sequence classifier to predict generic labels for the possible expressions. Labels for classification • “short” if the expression is a one-word long name or common name. • “nesting” if the expression is recursive. For recursive expressions, a special handling is applied: All possible assignments of the embedded entities are generated with labels corresponding to the concatenation of the involved entities’ labels. If the embedding is on the right (left) side of the expression, “right” (“left”) is added to the label. Non-sensical labels (i.e. “he father”) are not seen in the training"
W11-2835,C10-1012,1,0.800448,"atical function relation labels (as SSyntR).1 The system thus realizes the following steps: 1. Semantic graph → Deep-syntactic tree 2. Deep-syntactic tree → Surface-syntactic tree 3. Surface-syntactic tree → Linearized structure 4. Linearized structure → Surface In addition, two auxiliary steps are carried out. The first one is part-of-speech tagging; it is carried out after step 3. The second one is introduction of commata; it is done after step 4. Each step is implemented as a decoder that uses a classifier to select the appropriate operations. For the realization of the classifiers, we use Bohnet et al. (2010)’s implementation of MIRA (Margin Infused Relaxed Algorithm) (Crammer et al., 2006). 2 Sentence Realization Sentence generation consists in the application of the previously trained decoders in sequence 1.–4., plus the two auxiliary steps. 1 The DSyntR is inspired by the DSynt structures in (Mel’ˇcuk, 1988), only that the latter are still “deeper”. Semantic Generation Our derivation of the DSynt-tree from an input Sem-graph is analogous to graph-based parsing algorithms (Eisner, 1996). It is defined as search for the highest scoring tree y from all possible trees given an input graph x: F (x)"
W11-2835,P04-1015,0,0.0509894,"the DSynt structures in (Mel’ˇcuk, 1988), only that the latter are still “deeper”. Semantic Generation Our derivation of the DSynt-tree from an input Sem-graph is analogous to graph-based parsing algorithms (Eisner, 1996). It is defined as search for the highest scoring tree y from all possible trees given an input graph x: F (x) = argmax Score(y), where y ∈ M AP (x) (with M AP (x) as the set of all trees spanning over the nodes of the Sem-graph x). As in (Bohnet et al., 2011), the search is a beam search which creates a maximum spanning tree using “early update” as introduced for parsing by Collins and Roark (2004): when the correct beam element drops out of the beam, we stop and update the model using the best partial solution. The idea is that when all items in the current beam are incorrect, further processing is obsolete since the correct solution cannot be reached extending any elements of the beam. When we reach a final state, i.e. a tree spanning over all words and the correct solution is in the beam but not ranked first, we perform an update as well, since the correct element should have ranked first in the beam. Algorithm 1 displays the algorithm for the generation of the DSyntR from the SemR."
W11-2835,C96-1058,0,0.0241739,"uses a classifier to select the appropriate operations. For the realization of the classifiers, we use Bohnet et al. (2010)’s implementation of MIRA (Margin Infused Relaxed Algorithm) (Crammer et al., 2006). 2 Sentence Realization Sentence generation consists in the application of the previously trained decoders in sequence 1.–4., plus the two auxiliary steps. 1 The DSyntR is inspired by the DSynt structures in (Mel’ˇcuk, 1988), only that the latter are still “deeper”. Semantic Generation Our derivation of the DSynt-tree from an input Sem-graph is analogous to graph-based parsing algorithms (Eisner, 1996). It is defined as search for the highest scoring tree y from all possible trees given an input graph x: F (x) = argmax Score(y), where y ∈ M AP (x) (with M AP (x) as the set of all trees spanning over the nodes of the Sem-graph x). As in (Bohnet et al., 2011), the search is a beam search which creates a maximum spanning tree using “early update” as introduced for parsing by Collins and Roark (2004): when the correct beam element drops out of the beam, we stop and update the model using the best partial solution. The idea is that when all items in the current beam are incorrect, further proces"
W11-2924,D10-1125,0,0.0414044,"Missing"
W11-2924,C10-1011,1,0.837107,"tituent labels (noGF) and on the conflation of these labels and grammatical functions (GF). We have to mention that our F-values are not comparable to the official results of PaGe – which was our original goal – because the evaluation metric there was a special imExperiments We evaluate our approach on the Tiger corpora of the Parsing German Shared Task (PaGe) (K¨ubler, 2008). Its training, development, and test datasets consist of 20894, 2611 and 2611 sentences respectively. We decided to use these corpora to be able to compare our results with other results. We used the dependency parser of Bohnet (2010) to generate the parses for the feature extraction. We selected the parser since it had top scores for German in the CoNLL Shared Task 2009. The parser is a second order dependency parser that models the interaction between siblings as well as grandchildren. The parser was after the Shared Task enhanced by a Hash Kernel, which leads to significantly higher accuracy. We generated the dependency structures by 10-fold cross-validation training of the training corpus. The model for the annotation of the test set and development set was trained on the entire training corpus. We evaluated the depend"
W11-2924,W08-2102,0,0.0262582,"discriminative dependency parsers – is also manifest which we will address as future work. Some generative parsing approaches exploited the difference between phrase-structure and dependency parsers. For instance, Klein and Manning (2003) introduced an approach where the objective function is the product of the probabilities of a generative phrase-structure and a dependency parsers. Model 1 of Collins (2003) is based on the dependencies between pairs of head words. On the other hand, the related work on this topic for discriminative parsing is sparse, we are only aware of the following works. Carreras et al. (2008) and Koo et al. (2010) introduced frameworks for joint learning of phrase-structure and dependency 209 Proceedings of the 12th International Conference on Parsing Technologies, pages 209–214, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. have the corresponding arcs in the dependency tree. goal is to investigate the extension of the standard feature set of these models by features extracted from the automatic dependency parse of the sentence in question. 3 ConstRel features are similar to POSRel but use the constituent labels rather than the POS tag"
W11-2924,W09-0424,0,0.0189035,"es. Our prior experiments found the forest pruning threshold to be optimal at the order of 10−2 which resulted in packed forests with average node number of 108. The oracle scores were 87.1 and 91.4 for the 100-best lists and packed forests, respectively. At the second stage, we filtered out rare features (which occurred in less than 5 sentences). The new dependency parse-based feature set consists of 9240 and 5359 features before and after filtering. We employed the ranking MaxEnt implementation of the MALLET package (McCallum, 2002) and the average perceptron training of the Joshua package (Li et al., 2009). The update mechanism of the latter one was extended by using the F-score of the candidate full parse against the oracle parse as a loss function (see MIRA (Crammer and Singer, 2003) for the motivation). We used the state-of-the-art feature set of the German phrase-structure parse reranker of Versley and Rehbein (2009) as a baseline feature set. This feature set is rich and consists of features constructed from the lexicalized parse tree and its typed dependencies along with features based on external statistical information (like the clustering of unknown words according to their context of"
W11-2924,P05-1022,0,0.919132,"hes have been proved to be effective for phrase-structure and dependency parsers in the last decade. Here, we aim to exploit the divergence in these approaches and show the utility of features extracted from the automatic dependency parses of sentences for a discriminative phrase-structure parser. Our experiments show a significant improvement over the state-of-the-art German discriminative constituent parser. 2 1 Introduction Feature-Rich Parse Reranking The most successful supervised phrase-structure parsers are feature-rich discriminative parsers which heavily depend on an underlying PCFG (Charniak and Johnson, 2005; Huang, 2008). These approaches consists of two stages. At the first stage they apply a PCFG to extract possible parses. The full set of possible parses cannot be iterated through in practice, and is usually pruned as a consequence. The n-best list parsers keep just the 50-100 best parses according to the PCFG. Other methods remove nodes and hyperedges whose posterior probability is under a predefined threshold from the forest (chart). The task of the second stage is to select the best parse from the set of possible parses (i.e. rerank this set). These methods employ a large feature set (usua"
W11-2924,H05-1066,0,0.0766452,"plicated in the forest-based approaches. The conditional random field methods usually use only local features (Miyao and Tsujii, 2002; Finkel et al., 2008). Huang (2008) introduced a beam-search and average perceptron-based procedure for incorporating them, however his empirical results show only minor improvement from incorporating non-local features. In this study, we experiment with n-best list reranking and a packed-forest based model as well along with local features exclusively. Our Both phrase-structure and dependency parsers have developed a lot in the last decade (Nivre et al., 2004; McDonald et al., 2005; Charniak and Johnson, 2005; Huang, 2008). Different approaches have been proved to be effective for these two parsing tasks which has implicated a divergence between techniques used (and a growing gap between researcher communities). In this work, we exploit this divergence and show the added value of features extracted from automatic dependency parses of sentences for a discriminative phrase-structure parser. We report results on German phrase-structure parsing, however, we note that the reverse direction of our approach – i.e. defining features from automatic phrase-structure parses for di"
W11-2924,J03-4003,0,0.0488488,"se-structure parser. We report results on German phrase-structure parsing, however, we note that the reverse direction of our approach – i.e. defining features from automatic phrase-structure parses for discriminative dependency parsers – is also manifest which we will address as future work. Some generative parsing approaches exploited the difference between phrase-structure and dependency parsers. For instance, Klein and Manning (2003) introduced an approach where the objective function is the product of the probabilities of a generative phrase-structure and a dependency parsers. Model 1 of Collins (2003) is based on the dependencies between pairs of head words. On the other hand, the related work on this topic for discriminative parsing is sparse, we are only aware of the following works. Carreras et al. (2008) and Koo et al. (2010) introduced frameworks for joint learning of phrase-structure and dependency 209 Proceedings of the 12th International Conference on Parsing Technologies, pages 209–214, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. have the corresponding arcs in the dependency tree. goal is to investigate the extension of the standard"
W11-2924,P08-1109,0,0.0209595,"d from the forest (chart). The task of the second stage is to select the best parse from the set of possible parses (i.e. rerank this set). These methods employ a large feature set (usually a few millions features) (Collins, 2000; Charniak and Johnson, 2005). The n-best list approaches can straightforwardly employ local and non-local features as well because they decide at the sentence-level (Charniak and Johnson, 2005). Involving non-local features is more complicated in the forest-based approaches. The conditional random field methods usually use only local features (Miyao and Tsujii, 2002; Finkel et al., 2008). Huang (2008) introduced a beam-search and average perceptron-based procedure for incorporating them, however his empirical results show only minor improvement from incorporating non-local features. In this study, we experiment with n-best list reranking and a packed-forest based model as well along with local features exclusively. Our Both phrase-structure and dependency parsers have developed a lot in the last decade (Nivre et al., 2004; McDonald et al., 2005; Charniak and Johnson, 2005; Huang, 2008). Different approaches have been proved to be effective for these two parsing tasks which ha"
W11-2924,W04-2407,0,0.163726,"features is more complicated in the forest-based approaches. The conditional random field methods usually use only local features (Miyao and Tsujii, 2002; Finkel et al., 2008). Huang (2008) introduced a beam-search and average perceptron-based procedure for incorporating them, however his empirical results show only minor improvement from incorporating non-local features. In this study, we experiment with n-best list reranking and a packed-forest based model as well along with local features exclusively. Our Both phrase-structure and dependency parsers have developed a lot in the last decade (Nivre et al., 2004; McDonald et al., 2005; Charniak and Johnson, 2005; Huang, 2008). Different approaches have been proved to be effective for these two parsing tasks which has implicated a divergence between techniques used (and a growing gap between researcher communities). In this work, we exploit this divergence and show the added value of features extracted from automatic dependency parses of sentences for a discriminative phrase-structure parser. We report results on German phrase-structure parsing, however, we note that the reverse direction of our approach – i.e. defining features from automatic phrase-"
W11-2924,W08-1007,0,0.0231942,"dels the interaction between siblings as well as grandchildren. The parser was after the Shared Task enhanced by a Hash Kernel, which leads to significantly higher accuracy. We generated the dependency structures by 10-fold cross-validation training of the training corpus. The model for the annotation of the test set and development set was trained on the entire training corpus. We evaluated the dependency parses themselves in line with PaGe. Table 1 shows the labeled (LAS) and unlabeled attachment scores (UAS) of the dependency parser and compares it with the Malt parser (Nivre et al., 2004; Hall and Nivre, 2008), which was the only and therefore best dependency parser that participated in the PaGe’s dependency parsing track. Bohnet’s parser reaches higher labeled and unlabeled scores. The last row shows the parsing accuracy with predicted Part-ofSpeech. We used the parses with predicted pos tags for our reranking experiments. 211 Table 3: Results achieved by the enriched feature set. Develop. Test noGF GF noGF GF Rafferty’08 77.40 – – – Versley’09 78.43 67.90 – – Baseline 78.48 66.29 79.21 66.63 RR 80.51 68.55 80.95 68.67 Dep 80.35 68.48 80.56 68.39 RR+Dep 81.34 69.73 81.49 69.44 AvgPer 81.41 69.67 8"
W11-2924,W08-1006,0,0.0260334,"0 – – Baseline 78.48 66.29 79.21 66.63 RR 80.51 68.55 80.95 68.67 Dep 80.35 68.48 80.56 68.39 RR+Dep 81.34 69.73 81.49 69.44 AvgPer 81.41 69.67 81.68 69.42 Table 2: Results achieved by dependency feature-based reranking. noGF GF Baseline 78.48 66.34 outArc 79.19 67.21 POSRel 79.99 68.13 ConstRel 79.67 67.72 All 80.20 68.32 All+Case 80.35 68.48 plementation for calculating F-value (which differs from evalb for example in handling punctuation marks) and it used gold-standard POS tags in the input (which we thought to be unrealistic). On the other hand, our results are comparable with results of Rafferty and Manning (2008) and Versley and Rehbein (2009). Table 2 shows the results achieved by the MaxEnt 100-best list reranker using one out of the three feature templates alone and their union (All) on the development set. All+Case refers to the enriched feature set incorporating case information for POS tag and grammatical functions for labels. Baseline here refers to the top parse of Bitpar (the first stage parser). We note that the inside probability estimation of Bitpar for an edge is always in our feature set. Each of the three feature templates achieved significant improvements over a strong baseline – note"
W11-2924,P08-1067,0,0.536996,"ffective for phrase-structure and dependency parsers in the last decade. Here, we aim to exploit the divergence in these approaches and show the utility of features extracted from the automatic dependency parses of sentences for a discriminative phrase-structure parser. Our experiments show a significant improvement over the state-of-the-art German discriminative constituent parser. 2 1 Introduction Feature-Rich Parse Reranking The most successful supervised phrase-structure parsers are feature-rich discriminative parsers which heavily depend on an underlying PCFG (Charniak and Johnson, 2005; Huang, 2008). These approaches consists of two stages. At the first stage they apply a PCFG to extract possible parses. The full set of possible parses cannot be iterated through in practice, and is usually pruned as a consequence. The n-best list parsers keep just the 50-100 best parses according to the PCFG. Other methods remove nodes and hyperedges whose posterior probability is under a predefined threshold from the forest (chart). The task of the second stage is to select the best parse from the set of possible parses (i.e. rerank this set). These methods employ a large feature set (usually a few mill"
W11-2924,C04-1024,1,0.870116,"ndency tree lays outside the span of words in question. We use the absolute count and the ratio of outArcs among the words of the span. The more arcs go out, the further away is the dependency subtree over the words of the constituent from a dominating subtree. Hence, these features try to capture the ”phraseness” of the span of words in question based on the dependency tree. For E1 we have outArc=2 and outArcRatio=2/4 as the parent of Inseln and von lay outside the constituent. For E2 we have outArc=1 and outArcRatio=1/5. 4 Two-Stage Parsing of German As a first-stage parser, we used BitPar (Schmid, 2004), a fast unlexicalized PCFG parser based on a first pass where non-probabilistic bottom-up parsing and top-down pruning is efficiently carried out by storing the chart in bit vectors. Bitpar constructs the probabilistic forest only after top-down pruning, i.e. after computing the posterior probability of each hyperedge given the input sentence. The forest is pruned by deleting hyperedges whose posterior probability is below some threshold. We used a treebank grammar enriched with case information, lexicalization of selected prepositions, conjunctions, and punctuation symbols, coarse parent cat"
W11-2924,W09-3820,0,0.676679,"ch occurred in less than 5 sentences). The new dependency parse-based feature set consists of 9240 and 5359 features before and after filtering. We employed the ranking MaxEnt implementation of the MALLET package (McCallum, 2002) and the average perceptron training of the Joshua package (Li et al., 2009). The update mechanism of the latter one was extended by using the F-score of the candidate full parse against the oracle parse as a loss function (see MIRA (Crammer and Singer, 2003) for the motivation). We used the state-of-the-art feature set of the German phrase-structure parse reranker of Versley and Rehbein (2009) as a baseline feature set. This feature set is rich and consists of features constructed from the lexicalized parse tree and its typed dependencies along with features based on external statistical information (like the clustering of unknown words according to their context of occurrences and PP attachment statistics gathered from the automatic POS tagged DE-WaC corpus, a 1.7G words sample of the German-language WWW). This feature set consists of 1.7 and 0.2 million of features before and after filtering and enables the direct comparison of our results with state-of-theart discriminative resu"
W11-2924,C10-2148,0,0.0425065,"POSRel but use the constituent labels rather than the POS tags of the heads. Thus, once again we do not have any positive feature for E1 , but for E2 we extract: VP-NP, VP-NP-OBJA, VP-PP, VP-PP-PP. We also investigated the role of case and grammatical functions and extended the POSRel and ConstRel feature sets by adding this information to the labels. For instance besides VVFIN-NN-OBJA and VP-NP-OBJA from our example E2 we also used VVFIN-NN-Acc-OBJA and VP-NP-OA-OBJA. Note that the value of outArc is 1 iff the word span in question has a dominating dependency subtree in the automatic parse. Wang and Zong (2010) prune hyperedges with outArc6= 1 thus this feature can be regarded as a generalization of their approach. Dependency Parse-Based Features for Phrase-Structure Parsing Given the automatic (1-best) dependency parse of the sentence in question, we defined three feature templates for representing hyperedges (i.e. a CFG rule applied over a certain span of words). We illustrate them on two hyperedges E1 = (NP die Inseln (PP von Rußland)) and E2 = (VP fordern (NP die Inseln) (PP von Rußland)). Let’s assume that the corresponding dependency subtree consists of the followDET ing arcs: ROOT→fordern, In"
W11-2924,W08-1008,0,\N,Missing
W12-1506,C00-1007,0,0.0623427,"ose an annotation schema that is based on these principles. Experiments shows that making the semantic corpora comply with the suggested principles does not need to have a negative impact on the quality of the stochastic generators trained on them. 1 Introduction With the increasing interest in data-driven surface realization, the question on the adequate annotation of corpora for generation also becomes increasingly important. While in the early days of stochastic generation, annotations produced for other applications were used (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; LangkildeGeary, 2002), the poor results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoN"
W12-1506,W11-2832,0,0.222768,"for other applications were used (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; LangkildeGeary, 2002), the poor results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (Surdeanu et al., 2008), i.e., PropBank (Palmer et al., 2005), which served as the reference treebank, into a more “generation friendly” annotation. However, all of the available annotations are to a certain extent still syntactic. Even PropBank and its generation-oriented variants contain a significant number of syntactic features (Bohnet et al., 2011b). Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that th"
W12-1506,C10-3009,1,0.871223,"Missing"
W12-1506,C10-1012,1,0.93562,"erators trained on them. 1 Introduction With the increasing interest in data-driven surface realization, the question on the adequate annotation of corpora for generation also becomes increasingly important. While in the early days of stochastic generation, annotations produced for other applications were used (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; LangkildeGeary, 2002), the poor results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (Surdeanu et al., 2008), i.e., PropBank (Palmer et al., 2005), which served as the reference treebank, into a more “generation friendly” annotation. However, all of the available annotations are to a ce"
W12-1506,W11-2835,1,0.926256,"thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (Surdeanu et al., 2008), i.e., PropBank (Palmer et al., 2005), which served as the reference treebank, into a more “generation friendly” annotation. However, all of the available annotations are to a certain extent still syntactic. Even PropBank and its generation-oriented variants contain a significant number of syntactic features (Bohnet et al., 2011b). Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that they use hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). (Walker et al., 2002), (Stent et al., 2004), (Wong and Mooney, 2007), and (Mairesse et al., 2010) start from deeper structures: Walker et al. and Stent et al. from deep-syntactic structures (Mel’ˇcuk, 1988), and Wong and Mooney and Mairesse et al. from higher order pr"
W12-1506,P98-1116,0,0.715111,"oriented annotation and propose an annotation schema that is based on these principles. Experiments shows that making the semantic corpora comply with the suggested principles does not need to have a negative impact on the quality of the stochastic generators trained on them. 1 Introduction With the increasing interest in data-driven surface realization, the question on the adequate annotation of corpora for generation also becomes increasingly important. While in the early days of stochastic generation, annotations produced for other applications were used (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; LangkildeGeary, 2002), the poor results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion o"
W12-1506,W02-2103,0,0.0370789,"standard annotation, with the goal to obtain an increasingly more semantic annotation, can only be accepted if the quality of (deep) stochastic generation does not unacceptably decrease. To assess this aspect, we converted automatically the PropBank annotation of the WSJ journal as used in the CoNLL shared task 2009 into an annotation that complies with all of the principles sketched above 28 for deep statistical generation and trained (Bohnet et al., 2010)’s generator on this new annotation.11 For our experiments, we used the usual training, development and test data split of the WSJ corpus (Langkilde-Geary, 2002; Ringger et al., 2004; Bohnet et al., 2010); Table 1 provides an overview of the used data. set training development test section 2 - 21 24 23 # sentences 39218 1334 2400 Table 1: Data split of the used data in the WSJ Corpus The resulting BLEU score of our experiment was 0.64, which is comparable with the accuracy reported in (Bohnet et al., 2010) (namely, 0.659), who used an annotation that still contained all functional nodes (such that their generation task was considerably more syntactic and thus more straightforward). To assess furthermore whether the automatically converted PropBank al"
W12-1506,P10-1157,0,0.0356542,"Missing"
W12-1506,W00-0306,0,0.0630775,"t is based on these principles. Experiments shows that making the semantic corpora comply with the suggested principles does not need to have a negative impact on the quality of the stochastic generators trained on them. 1 Introduction With the increasing interest in data-driven surface realization, the question on the adequate annotation of corpora for generation also becomes increasingly important. While in the early days of stochastic generation, annotations produced for other applications were used (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; LangkildeGeary, 2002), the poor results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (Surdea"
W12-1506,J05-1004,0,0.0754015,"results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (Surdeanu et al., 2008), i.e., PropBank (Palmer et al., 2005), which served as the reference treebank, into a more “generation friendly” annotation. However, all of the available annotations are to a certain extent still syntactic. Even PropBank and its generation-oriented variants contain a significant number of syntactic features (Bohnet et al., 2011b). Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that they use hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Lang"
W12-1506,C04-1097,0,0.22939,"th the goal to obtain an increasingly more semantic annotation, can only be accepted if the quality of (deep) stochastic generation does not unacceptably decrease. To assess this aspect, we converted automatically the PropBank annotation of the WSJ journal as used in the CoNLL shared task 2009 into an annotation that complies with all of the principles sketched above 28 for deep statistical generation and trained (Bohnet et al., 2010)’s generator on this new annotation.11 For our experiments, we used the usual training, development and test data split of the WSJ corpus (Langkilde-Geary, 2002; Ringger et al., 2004; Bohnet et al., 2010); Table 1 provides an overview of the used data. set training development test section 2 - 21 24 23 # sentences 39218 1334 2400 Table 1: Data split of the used data in the WSJ Corpus The resulting BLEU score of our experiment was 0.64, which is comparable with the accuracy reported in (Bohnet et al., 2010) (namely, 0.659), who used an annotation that still contained all functional nodes (such that their generation task was considerably more syntactic and thus more straightforward). To assess furthermore whether the automatically converted PropBank already offers some adva"
W12-1506,P04-1011,0,0.0192458,"more “generation friendly” annotation. However, all of the available annotations are to a certain extent still syntactic. Even PropBank and its generation-oriented variants contain a significant number of syntactic features (Bohnet et al., 2011b). Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that they use hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). (Walker et al., 2002), (Stent et al., 2004), (Wong and Mooney, 2007), and (Mairesse et al., 2010) start from deeper structures: Walker et al. and Stent et al. from deep-syntactic structures (Mel’ˇcuk, 1988), and Wong and Mooney and Mairesse et al. from higher order predicate logic structures. However, to the best of our knowledge, 1 Trained on the original ConLL 2009 corpora, (Bohnet et al., 2010)’s SVM-based generator reached a BLEU score of 0.12 for Chinese, 0.18 for English, 0.11 for German and 0.14 for Spanish. Joining the unconnected parts of the sentence annotations to connected trees (as required by a stochastic realizer) improv"
W12-1506,W08-2121,0,0.0817605,"Missing"
W12-1506,N07-1022,0,0.0136454,"dly” annotation. However, all of the available annotations are to a certain extent still syntactic. Even PropBank and its generation-oriented variants contain a significant number of syntactic features (Bohnet et al., 2011b). Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that they use hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). (Walker et al., 2002), (Stent et al., 2004), (Wong and Mooney, 2007), and (Mairesse et al., 2010) start from deeper structures: Walker et al. and Stent et al. from deep-syntactic structures (Mel’ˇcuk, 1988), and Wong and Mooney and Mairesse et al. from higher order predicate logic structures. However, to the best of our knowledge, 1 Trained on the original ConLL 2009 corpora, (Bohnet et al., 2010)’s SVM-based generator reached a BLEU score of 0.12 for Chinese, 0.18 for English, 0.11 for German and 0.14 for Spanish. Joining the unconnected parts of the sentence annotations to connected trees (as required by a stochastic realizer) improved the performance to a B"
W12-1506,P95-1034,0,\N,Missing
W12-1506,C98-1112,0,\N,Missing
W12-1525,W04-2705,0,\N,Missing
W12-1525,C10-1012,1,\N,Missing
W12-1525,W08-2121,0,\N,Missing
W12-1525,W11-2832,1,\N,Missing
W12-1525,W07-2416,0,\N,Missing
W12-1525,J05-1004,0,\N,Missing
W12-1525,W12-1528,1,\N,Missing
W12-1525,W04-3250,0,\N,Missing
W12-1525,kow-belz-2012-lg,1,\N,Missing
W12-1525,W12-1527,1,\N,Missing
W14-6105,P14-2131,0,0.103638,"d in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabelled target domain texts (Candito et al., 2011; Bansal et al., 2014), selectional preferences modelled from word co-occurrences obtained from unannotated texts (Zhou et al., 2011). The goal of this paper is to investigate a combination of such techniques to adapt a general-language parser to parse web data (weblogs, online reviews, newsgroups, and answers) without resorting to manual annotation. In our study we include several techniques that have been shown to be reasonably effective for domain adaptation: text normalization, the use of word clusters, an external crowd-sourced lexicon, as well as automatically annotated texts produced with the help of co-trai"
W14-6105,Q13-1034,1,0.862762,"interesting finding of their work was that the agreement between the two classifiers during testing was a very good predictor of accuracy. More recently, Zhang et al. (2012) used a tri-training algorithm for parser domain adaptation. The algorithm uses three learners and each learner was designed to learn from those automatically classified unlabelled data where the other two learners agreed on the classification label. 3 Experimental set-up 3.1 Parsers In the experiments we included the Malt parser (Nivre, 2009), the MST parser (McDonald and Pereira, 2006), the transition-based Mate parser (Bohnet et al., 2013), and the graph-based Turbo parser (Martins et al., 2010). All the parsers were used with their default settings, and PoS tags used in the input of all the parsers were the same and came from the Mate parser. 3.2 Baseline As the baseline we used the Mate parser, as it showed the highest accuracy when no domain adaptation techniques were used, i.e. trained on an in-domain training dataset and applied directly to out-of-domain test data. 3.3 Data The experiments were conducted on annotated data on web-related domains available in the Ontonotes v.5 and SANCL datasets, since a large amount of unla"
W14-6105,J92-4003,0,0.0387856,"lexica have also been used to improve out-of-domain PoS tagging (Li et al., 2012). 2.3 Word clusters In order to reduce the amount of annotated data to train a dependency parser, Koo et al. (2008) used word clusters computed from unlabelled data as features for training a parser. The same approach has proved to be effective for out-of-domain parsing, where there are many words in the test data unseen during training, and word clusters computed from in-domain data similarly help to deal with the vocabulary discrepancies between the training and test datasets. Discrete word clusters produced by Brown et al. (1992) method have been shown to be beneficial for adapting dependency parsers to biomedical texts (Candito et al., 2011) and web texts (Øvrelid and Skjærholt, 2012). Word clusters created with Brown clustering method have also been used to adapt a PoS tagger to Twitter posts (Owoputi et al., 2013). Bansal et al. (2014) introduced continuous word representations and showed them to increase parsing accuracy both on the Penn Treebank and on web data. 55 2.4 Co-training Co-training (Blum and Mitchell, 1998) is a paradigm for weakly supervised learning of a classification problem from a limited amount o"
W14-6105,W11-2905,0,0.208836,"entences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabelled target domain texts (Candito et al., 2011; Bansal et al., 2014), selectional preferences modelled from word co-occurrences obtained from unannotated texts (Zhou et al., 2011). The goal of this paper is to investigate a combination of such techniques to adapt a general-language parser to parse web data (weblogs, online reviews, newsgroups, and answers) without resorting to manual annotation. In our study we include several techniques that have been shown to be reasonably effective for domain adaptation: text normalization, the use of word clusters, an external crowd-sourced lexicon, as well as automatically annotated texts produced wi"
W14-6105,N10-1060,0,0.0211128,"utomatically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabelled target domain texts (Candito et al., 2011; Bansal et al., 2014), selectional preferences modelled from word co-occurrences obtained from unannotated texts (Zhou et al., 2011). The goal of this paper is to investigate a combination of such techniques to adapt a general-language parser to parse web data (weblogs, online reviews, newsgroups, and answers) without resorting to manual annotation."
W14-6105,W01-0521,0,0.103307,"Missing"
W14-6105,W07-2416,0,0.0457666,"Missing"
W14-6105,W13-1101,0,0.0265587,"Missing"
W14-6105,R13-1046,0,0.0341484,"Missing"
W14-6105,P08-1068,0,0.182623,"Missing"
W14-6105,I05-1006,0,0.0972676,"Missing"
W14-6105,D12-1127,0,0.0595001,"Missing"
W14-6105,D10-1004,0,0.0286883,"t between the two classifiers during testing was a very good predictor of accuracy. More recently, Zhang et al. (2012) used a tri-training algorithm for parser domain adaptation. The algorithm uses three learners and each learner was designed to learn from those automatically classified unlabelled data where the other two learners agreed on the classification label. 3 Experimental set-up 3.1 Parsers In the experiments we included the Malt parser (Nivre, 2009), the MST parser (McDonald and Pereira, 2006), the transition-based Mate parser (Bohnet et al., 2013), and the graph-based Turbo parser (Martins et al., 2010). All the parsers were used with their default settings, and PoS tags used in the input of all the parsers were the same and came from the Mate parser. 3.2 Baseline As the baseline we used the Mate parser, as it showed the highest accuracy when no domain adaptation techniques were used, i.e. trained on an in-domain training dataset and applied directly to out-of-domain test data. 3.3 Data The experiments were conducted on annotated data on web-related domains available in the Ontonotes v.5 and SANCL datasets, since a large amount of unlabelled data required for most domain adaptation technique"
W14-6105,P08-2026,0,0.0239503,"kshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 54–65 Dublin, Ireland, August 23-29 2014. parsed during parser testing. In addition, a certain amount of unlabelled target domain texts may be available that can be leveraged in this or that way to facilitate domain adaptation. To address the problem of domain adaption, previous work focused on weakly supervised methods to re-train parsers on automatically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), dis"
W14-6105,E06-1011,0,0.0549001,"ain track of the CoNLL07 shared task (Nilsson et al., 2007). An interesting finding of their work was that the agreement between the two classifiers during testing was a very good predictor of accuracy. More recently, Zhang et al. (2012) used a tri-training algorithm for parser domain adaptation. The algorithm uses three learners and each learner was designed to learn from those automatically classified unlabelled data where the other two learners agreed on the classification label. 3 Experimental set-up 3.1 Parsers In the experiments we included the Malt parser (Nivre, 2009), the MST parser (McDonald and Pereira, 2006), the transition-based Mate parser (Bohnet et al., 2013), and the graph-based Turbo parser (Martins et al., 2010). All the parsers were used with their default settings, and PoS tags used in the input of all the parsers were the same and came from the Mate parser. 3.2 Baseline As the baseline we used the Mate parser, as it showed the highest accuracy when no domain adaptation techniques were used, i.e. trained on an in-domain training dataset and applied directly to out-of-domain test data. 3.3 Data The experiments were conducted on annotated data on web-related domains available in the Ontono"
W14-6105,P09-1040,0,0.0222507,"e best result on the out-ofdomain track of the CoNLL07 shared task (Nilsson et al., 2007). An interesting finding of their work was that the agreement between the two classifiers during testing was a very good predictor of accuracy. More recently, Zhang et al. (2012) used a tri-training algorithm for parser domain adaptation. The algorithm uses three learners and each learner was designed to learn from those automatically classified unlabelled data where the other two learners agreed on the classification label. 3 Experimental set-up 3.1 Parsers In the experiments we included the Malt parser (Nivre, 2009), the MST parser (McDonald and Pereira, 2006), the transition-based Mate parser (Bohnet et al., 2013), and the graph-based Turbo parser (Martins et al., 2010). All the parsers were used with their default settings, and PoS tags used in the input of all the parsers were the same and came from the Mate parser. 3.2 Baseline As the baseline we used the Mate parser, as it showed the highest accuracy when no domain adaptation techniques were used, i.e. trained on an in-domain training dataset and applied directly to out-of-domain test data. 3.3 Data The experiments were conducted on annotated data o"
W14-6105,C12-2088,0,0.013163,"o train a dependency parser, Koo et al. (2008) used word clusters computed from unlabelled data as features for training a parser. The same approach has proved to be effective for out-of-domain parsing, where there are many words in the test data unseen during training, and word clusters computed from in-domain data similarly help to deal with the vocabulary discrepancies between the training and test datasets. Discrete word clusters produced by Brown et al. (1992) method have been shown to be beneficial for adapting dependency parsers to biomedical texts (Candito et al., 2011) and web texts (Øvrelid and Skjærholt, 2012). Word clusters created with Brown clustering method have also been used to adapt a PoS tagger to Twitter posts (Owoputi et al., 2013). Bansal et al. (2014) introduced continuous word representations and showed them to increase parsing accuracy both on the Penn Treebank and on web data. 55 2.4 Co-training Co-training (Blum and Mitchell, 1998) is a paradigm for weakly supervised learning of a classification problem from a limited amount of labelled data and a large amount of unlabelled data, whereby two or more views on the data, i.e. feature subsets, or two or more different learning algorithm"
W14-6105,N13-1039,0,0.0599141,"Missing"
W14-6105,D10-1069,0,0.0194492,"s and Syntactic Analysis of Non-Canonical Languages, pages 54–65 Dublin, Ireland, August 23-29 2014. parsed during parser testing. In addition, a certain amount of unlabelled target domain texts may be available that can be leveraged in this or that way to facilitate domain adaptation. To address the problem of domain adaption, previous work focused on weakly supervised methods to re-train parsers on automatically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabe"
W14-6105,P11-1157,0,0.0493798,"Missing"
W14-6105,W11-3808,0,0.018604,"of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 54–65 Dublin, Ireland, August 23-29 2014. parsed during parser testing. In addition, a certain amount of unlabelled target domain texts may be available that can be leveraged in this or that way to facilitate domain adaptation. To address the problem of domain adaption, previous work focused on weakly supervised methods to re-train parsers on automatically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), discrete or continu"
W14-6105,D07-1111,0,0.211849,"Missing"
W14-6105,N01-1023,0,0.243836,"eativecommons.org/licenses/by/4.0/ 54 First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 54–65 Dublin, Ireland, August 23-29 2014. parsed during parser testing. In addition, a certain amount of unlabelled target domain texts may be available that can be leveraged in this or that way to facilitate domain adaptation. To address the problem of domain adaption, previous work focused on weakly supervised methods to re-train parsers on automatically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unkn"
W14-6105,E03-1008,0,0.0473048,"org/licenses/by/4.0/ 54 First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 54–65 Dublin, Ireland, August 23-29 2014. parsed during parser testing. In addition, a certain amount of unlabelled target domain texts may be available that can be leveraged in this or that way to facilitate domain adaptation. To address the problem of domain adaption, previous work focused on weakly supervised methods to re-train parsers on automatically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words"
W14-6105,Q13-1001,0,0.0378229,"Missing"
W14-6105,P11-1156,0,0.0159652,"t al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabelled target domain texts (Candito et al., 2011; Bansal et al., 2014), selectional preferences modelled from word co-occurrences obtained from unannotated texts (Zhou et al., 2011). The goal of this paper is to investigate a combination of such techniques to adapt a general-language parser to parse web data (weblogs, online reviews, newsgroups, and answers) without resorting to manual annotation. In our study we include several techniques that have been shown to be reasonably effective for domain adaptation: text normalization, the use of word clusters, an external crowd-sourced lexicon, as well as automatically annotated texts produced with the help of co-training. All these techniques are domain-independent and can be applied to new target domains given unlabelled tex"
W14-6105,W09-1201,0,\N,Missing
W14-6105,D07-1096,0,\N,Missing
W15-2133,C12-2082,0,0.0306107,"and clitics are not separated from the head words but analyzed with special labels at the syntactic level instead. Therefore, in the annotation scheme of the Uppsala Persian Dependency Treebank, apart from 48 dependency labels for basic relations there are 48 complex dependency labels to cover syntactic relations for words containing unsegmented clitics. Fine-grained annotated data in treebanks normally provides a more complete grammatical analysis which in turn enhances the quality of parsing results. However, complex annotation may not always be beneficial and can impair automatic analysis (Mille et al., 2012; Jelínek, 2014). In this paper, we present different empirical studies where we systematically simplify the annotation schemes for part-of-speech tags and dependency relations within the treebank. This paper is organized as follows: Section 2 briefly presents the Uppsala Persian Dependency Treebank. Section 3 introduces the experimental design. In Section 4, ParsPer is presented and evaluated. Finally, Section 5 concludes the paper. 2 The treebank’s syntactic annotation scheme is based on Stanford Typed Dependencies (STD) (de Marneffe and Manning, 2008) with extensions for Persian. This versi"
W15-2133,nivre-etal-2006-maltparser,1,0.809925,"Missing"
W15-2133,D10-1082,0,0.0767403,"Missing"
W15-2133,ballesteros-nivre-2012-maltoptimizer-system,1,0.899063,"Missing"
W15-2133,C10-1011,1,0.876388,"Missing"
W15-2133,E12-1009,1,0.834294,"Missing"
W15-2133,D12-1133,1,0.898005,"Missing"
W15-2133,W08-1301,0,0.430254,"Missing"
W15-2133,foth-etal-2014-size,0,0.237485,"Missing"
W15-2133,jelinek-2014-improvements,0,0.0200708,"separated from the head words but analyzed with special labels at the syntactic level instead. Therefore, in the annotation scheme of the Uppsala Persian Dependency Treebank, apart from 48 dependency labels for basic relations there are 48 complex dependency labels to cover syntactic relations for words containing unsegmented clitics. Fine-grained annotated data in treebanks normally provides a more complete grammatical analysis which in turn enhances the quality of parsing results. However, complex annotation may not always be beneficial and can impair automatic analysis (Mille et al., 2012; Jelínek, 2014). In this paper, we present different empirical studies where we systematically simplify the annotation schemes for part-of-speech tags and dependency relations within the treebank. This paper is organized as follows: Section 2 briefly presents the Uppsala Persian Dependency Treebank. Section 3 introduces the experimental design. In Section 4, ParsPer is presented and evaluated. Finally, Section 5 concludes the paper. 2 The treebank’s syntactic annotation scheme is based on Stanford Typed Dependencies (STD) (de Marneffe and Manning, 2008) with extensions for Persian. This version of STD has a"
W15-2133,D10-1004,0,0.0612832,"Missing"
W15-2133,H05-1066,0,0.166131,"Missing"
W15-2133,vincze-etal-2010-hungarian,0,\N,Missing
W15-2138,Q13-1034,1,0.935043,"om large unlabeled datasets. The method has been shown effective for five languages out of the nine languages of the SPMRL Shared Task 2014 datasets. We obtained the largest absolute improvement of two percentage points on Korean data. Our selftraining experiments show improvements upon the best state-of-the-art systems of the SPMRL shared task that employs one parser only. 1 Introduction The availability of the manually annotated treebanks and state-of-the-art dependency parsers (McDonald and Pereira, 2006; Nivre, 2009; Martins et al., 2010; Goldberg and Elhadad, 2010; Zhang and Nivre, 2011; Bohnet et al., 2013) leads to high accuracy on some languages such as English (Marcus et al., 1994), German (K¨ubler et al., 2006) and Chinese (Levy and Manning, 2003) that have large manually annotated datasets. In contrast to resource-rich languages, languages that have less training data show a lower accuracy (Buchholz and Marsi, 2006; Nivre et al., 2007; Seddah et al., 2013; Seddah et al., 2014). Semi-supervised techniques gain popularity as they are able to improve parsing accuracy by exploiting unlabeled data which avoids the cost of labeling new data. Self-training is one of these appealing techniques that"
W15-2138,W06-2920,0,0.0713949,"ystems of the SPMRL shared task that employs one parser only. 1 Introduction The availability of the manually annotated treebanks and state-of-the-art dependency parsers (McDonald and Pereira, 2006; Nivre, 2009; Martins et al., 2010; Goldberg and Elhadad, 2010; Zhang and Nivre, 2011; Bohnet et al., 2013) leads to high accuracy on some languages such as English (Marcus et al., 1994), German (K¨ubler et al., 2006) and Chinese (Levy and Manning, 2003) that have large manually annotated datasets. In contrast to resource-rich languages, languages that have less training data show a lower accuracy (Buchholz and Marsi, 2006; Nivre et al., 2007; Seddah et al., 2013; Seddah et al., 2014). Semi-supervised techniques gain popularity as they are able to improve parsing accuracy by exploiting unlabeled data which avoids the cost of labeling new data. Self-training is one of these appealing techniques that have been successfully used for instance in constituency parsing for English texts 1. We present an effective confidence-based self-training approach. 2. We evaluate our approach on nine languages in a resource-poor parsing scenario. 3. We successfully improved the parsing performances on five languages which are Bas"
W15-2138,P05-1022,0,0.0942543,"Missing"
W15-2138,C08-1015,0,0.0482426,"Missing"
W15-2138,I11-1041,0,0.0393891,"Missing"
W15-2138,N06-1020,0,0.386511,"y effective in a few cases, in contrast to co-training which works for dependency parsing well too. In a co-training approach, at least another parser is employed to label additional training data. McClosky et al. (2006a) used self-training for English constituency parsing. In their approaches, self-training was most effective when the parser is retrained on the combination of the initial training set and the large unlabeled dataset generated by both the generative parser and reranker. This leads to many subsequent applications on English texts via self-training for constituency parsing, cf. (McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae, 2010; Petrov and McDonald, 2012). In contrast to English constituency parsing, selftraining usually has proved to be less effective or has even shown negative results when applied to dependency parsing, cf. (Kawahara and Uchimoto, 2008; Plank, 2011; Cerisara, 2014; Bj¨orkelund et al., 2014). This paper makes the following contributions: This paper presents a novel self-training approach that we use to explore a scenario which is typical for under-resourced languages. We apply self-training on small multilingual dependency corpora of nine languages. Our a"
W15-2138,N10-1115,0,0.016222,"ce-based method to gain additional training data from large unlabeled datasets. The method has been shown effective for five languages out of the nine languages of the SPMRL Shared Task 2014 datasets. We obtained the largest absolute improvement of two percentage points on Korean data. Our selftraining experiments show improvements upon the best state-of-the-art systems of the SPMRL shared task that employs one parser only. 1 Introduction The availability of the manually annotated treebanks and state-of-the-art dependency parsers (McDonald and Pereira, 2006; Nivre, 2009; Martins et al., 2010; Goldberg and Elhadad, 2010; Zhang and Nivre, 2011; Bohnet et al., 2013) leads to high accuracy on some languages such as English (Marcus et al., 1994), German (K¨ubler et al., 2006) and Chinese (Levy and Manning, 2003) that have large manually annotated datasets. In contrast to resource-rich languages, languages that have less training data show a lower accuracy (Buchholz and Marsi, 2006; Nivre et al., 2007; Seddah et al., 2013; Seddah et al., 2014). Semi-supervised techniques gain popularity as they are able to improve parsing accuracy by exploiting unlabeled data which avoids the cost of labeling new data. Self-train"
W15-2138,P06-1043,0,0.238752,"y effective in a few cases, in contrast to co-training which works for dependency parsing well too. In a co-training approach, at least another parser is employed to label additional training data. McClosky et al. (2006a) used self-training for English constituency parsing. In their approaches, self-training was most effective when the parser is retrained on the combination of the initial training set and the large unlabeled dataset generated by both the generative parser and reranker. This leads to many subsequent applications on English texts via self-training for constituency parsing, cf. (McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae, 2010; Petrov and McDonald, 2012). In contrast to English constituency parsing, selftraining usually has proved to be less effective or has even shown negative results when applied to dependency parsing, cf. (Kawahara and Uchimoto, 2008; Plank, 2011; Cerisara, 2014; Bj¨orkelund et al., 2014). This paper makes the following contributions: This paper presents a novel self-training approach that we use to explore a scenario which is typical for under-resourced languages. We apply self-training on small multilingual dependency corpora of nine languages. Our a"
W15-2138,I11-1172,0,0.0339777,"Missing"
W15-2138,D07-1013,0,0.0800439,"Missing"
W15-2138,P98-1106,0,0.169638,"ings of 10,000 iterations (Nivre et al., 2007). The statistically significant results are marked due to their p-values (*) p-value<0.05, (**) p-value<0.01. joint inference that improves both part-of-speech tagging and parsing accuracy. We use the arc-standard transition-based parser employing beam search and a graph-based rescoring model. This parser computes a score for each dependency tree by summing up the scores for each transition and dividing the score by the total number of transitions, due to the swap-operation (used for non-projective parsing), the number of transition can vary, cf. (Kahane et al., 1998; Nivre, 2007). For our self-training approach, we use the parse scores as confidence measure to select sentences. We observed that although the original parse score is the averaged value of a sequence of transitions of a parse, long sentences generally exhibit a higher score. Therefore, the score does not correlate well with the Labeled Attachment Score (LAS) as shown in Figure 2. Thus, we adjusted the score of the parser to maximize the correlation between the parse score and the labeled attachment score for each parse tree by subtracting the sentence length (L) multiplied by a fixed number"
W15-2138,E06-1011,0,0.0384063,"ency corpora of nine languages. Our approach employs a confidence-based method to gain additional training data from large unlabeled datasets. The method has been shown effective for five languages out of the nine languages of the SPMRL Shared Task 2014 datasets. We obtained the largest absolute improvement of two percentage points on Korean data. Our selftraining experiments show improvements upon the best state-of-the-art systems of the SPMRL shared task that employs one parser only. 1 Introduction The availability of the manually annotated treebanks and state-of-the-art dependency parsers (McDonald and Pereira, 2006; Nivre, 2009; Martins et al., 2010; Goldberg and Elhadad, 2010; Zhang and Nivre, 2011; Bohnet et al., 2013) leads to high accuracy on some languages such as English (Marcus et al., 1994), German (K¨ubler et al., 2006) and Chinese (Levy and Manning, 2003) that have large manually annotated datasets. In contrast to resource-rich languages, languages that have less training data show a lower accuracy (Buchholz and Marsi, 2006; Nivre et al., 2007; Seddah et al., 2013; Seddah et al., 2014). Semi-supervised techniques gain popularity as they are able to improve parsing accuracy by exploiting unlabe"
W15-2138,I08-2097,0,0.0169647,"sing. In their approaches, self-training was most effective when the parser is retrained on the combination of the initial training set and the large unlabeled dataset generated by both the generative parser and reranker. This leads to many subsequent applications on English texts via self-training for constituency parsing, cf. (McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae, 2010; Petrov and McDonald, 2012). In contrast to English constituency parsing, selftraining usually has proved to be less effective or has even shown negative results when applied to dependency parsing, cf. (Kawahara and Uchimoto, 2008; Plank, 2011; Cerisara, 2014; Bj¨orkelund et al., 2014). This paper makes the following contributions: This paper presents a novel self-training approach that we use to explore a scenario which is typical for under-resourced languages. We apply self-training on small multilingual dependency corpora of nine languages. Our approach employs a confidence-based method to gain additional training data from large unlabeled datasets. The method has been shown effective for five languages out of the nine languages of the SPMRL Shared Task 2014 datasets. We obtained the largest absolute improvement of"
W15-2138,D10-1095,0,0.0604235,"Missing"
W15-2138,W06-1614,0,0.0846891,"Missing"
W15-2138,N12-1068,0,0.0438608,"Missing"
W15-2138,P03-1056,0,0.0294997,"sets. We obtained the largest absolute improvement of two percentage points on Korean data. Our selftraining experiments show improvements upon the best state-of-the-art systems of the SPMRL shared task that employs one parser only. 1 Introduction The availability of the manually annotated treebanks and state-of-the-art dependency parsers (McDonald and Pereira, 2006; Nivre, 2009; Martins et al., 2010; Goldberg and Elhadad, 2010; Zhang and Nivre, 2011; Bohnet et al., 2013) leads to high accuracy on some languages such as English (Marcus et al., 1994), German (K¨ubler et al., 2006) and Chinese (Levy and Manning, 2003) that have large manually annotated datasets. In contrast to resource-rich languages, languages that have less training data show a lower accuracy (Buchholz and Marsi, 2006; Nivre et al., 2007; Seddah et al., 2013; Seddah et al., 2014). Semi-supervised techniques gain popularity as they are able to improve parsing accuracy by exploiting unlabeled data which avoids the cost of labeling new data. Self-training is one of these appealing techniques that have been successfully used for instance in constituency parsing for English texts 1. We present an effective confidence-based self-training appro"
W15-2138,N07-1050,0,0.0142309,"ions (Nivre et al., 2007). The statistically significant results are marked due to their p-values (*) p-value<0.05, (**) p-value<0.01. joint inference that improves both part-of-speech tagging and parsing accuracy. We use the arc-standard transition-based parser employing beam search and a graph-based rescoring model. This parser computes a score for each dependency tree by summing up the scores for each transition and dividing the score by the total number of transitions, due to the swap-operation (used for non-projective parsing), the number of transition can vary, cf. (Kahane et al., 1998; Nivre, 2007). For our self-training approach, we use the parse scores as confidence measure to select sentences. We observed that although the original parse score is the averaged value of a sequence of transitions of a parse, long sentences generally exhibit a higher score. Therefore, the score does not correlate well with the Labeled Attachment Score (LAS) as shown in Figure 2. Thus, we adjusted the score of the parser to maximize the correlation between the parse score and the labeled attachment score for each parse tree by subtracting the sentence length (L) multiplied by a fixed number d. The new par"
W15-2138,D10-1004,0,0.0339361,"Missing"
W15-2138,P09-1040,0,0.0225529,"es. Our approach employs a confidence-based method to gain additional training data from large unlabeled datasets. The method has been shown effective for five languages out of the nine languages of the SPMRL Shared Task 2014 datasets. We obtained the largest absolute improvement of two percentage points on Korean data. Our selftraining experiments show improvements upon the best state-of-the-art systems of the SPMRL shared task that employs one parser only. 1 Introduction The availability of the manually annotated treebanks and state-of-the-art dependency parsers (McDonald and Pereira, 2006; Nivre, 2009; Martins et al., 2010; Goldberg and Elhadad, 2010; Zhang and Nivre, 2011; Bohnet et al., 2013) leads to high accuracy on some languages such as English (Marcus et al., 1994), German (K¨ubler et al., 2006) and Chinese (Levy and Manning, 2003) that have large manually annotated datasets. In contrast to resource-rich languages, languages that have less training data show a lower accuracy (Buchholz and Marsi, 2006; Nivre et al., 2007; Seddah et al., 2013; Seddah et al., 2014). Semi-supervised techniques gain popularity as they are able to improve parsing accuracy by exploiting unlabeled data whic"
W15-2138,P07-1078,0,0.0277017,"es, in contrast to co-training which works for dependency parsing well too. In a co-training approach, at least another parser is employed to label additional training data. McClosky et al. (2006a) used self-training for English constituency parsing. In their approaches, self-training was most effective when the parser is retrained on the combination of the initial training set and the large unlabeled dataset generated by both the generative parser and reranker. This leads to many subsequent applications on English texts via self-training for constituency parsing, cf. (McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae, 2010; Petrov and McDonald, 2012). In contrast to English constituency parsing, selftraining usually has proved to be less effective or has even shown negative results when applied to dependency parsing, cf. (Kawahara and Uchimoto, 2008; Plank, 2011; Cerisara, 2014; Bj¨orkelund et al., 2014). This paper makes the following contributions: This paper presents a novel self-training approach that we use to explore a scenario which is typical for under-resourced languages. We apply self-training on small multilingual dependency corpora of nine languages. Our approach employs a confidence-ba"
W15-2138,W10-2606,0,0.0198677,"which works for dependency parsing well too. In a co-training approach, at least another parser is employed to label additional training data. McClosky et al. (2006a) used self-training for English constituency parsing. In their approaches, self-training was most effective when the parser is retrained on the combination of the initial training set and the large unlabeled dataset generated by both the generative parser and reranker. This leads to many subsequent applications on English texts via self-training for constituency parsing, cf. (McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae, 2010; Petrov and McDonald, 2012). In contrast to English constituency parsing, selftraining usually has proved to be less effective or has even shown negative results when applied to dependency parsing, cf. (Kawahara and Uchimoto, 2008; Plank, 2011; Cerisara, 2014; Bj¨orkelund et al., 2014). This paper makes the following contributions: This paper presents a novel self-training approach that we use to explore a scenario which is typical for under-resourced languages. We apply self-training on small multilingual dependency corpora of nine languages. Our approach employs a confidence-based method to"
W15-2138,N01-1023,0,0.0138563,"Missing"
W15-2138,W14-6111,0,0.022168,"Missing"
W15-2138,P11-2033,0,0.0221472,"tional training data from large unlabeled datasets. The method has been shown effective for five languages out of the nine languages of the SPMRL Shared Task 2014 datasets. We obtained the largest absolute improvement of two percentage points on Korean data. Our selftraining experiments show improvements upon the best state-of-the-art systems of the SPMRL shared task that employs one parser only. 1 Introduction The availability of the manually annotated treebanks and state-of-the-art dependency parsers (McDonald and Pereira, 2006; Nivre, 2009; Martins et al., 2010; Goldberg and Elhadad, 2010; Zhang and Nivre, 2011; Bohnet et al., 2013) leads to high accuracy on some languages such as English (Marcus et al., 1994), German (K¨ubler et al., 2006) and Chinese (Levy and Manning, 2003) that have large manually annotated datasets. In contrast to resource-rich languages, languages that have less training data show a lower accuracy (Buchholz and Marsi, 2006; Nivre et al., 2007; Seddah et al., 2013; Seddah et al., 2014). Semi-supervised techniques gain popularity as they are able to improve parsing accuracy by exploiting unlabeled data which avoids the cost of labeling new data. Self-training is one of these app"
W15-2138,E12-1009,1,\N,Missing
W15-2138,C98-1102,0,\N,Missing
W15-2138,D07-1096,0,\N,Missing
W15-2201,Q13-1034,1,0.842249,"nces for training, development and test set. The source data sets of the chemical domain are smaller than the ones for web domains. The training set has about half of the size. Thus we use only 250k unlabeled sentences from the chemical domain which share the same ratio of training set size to unlabeled data set size compared to the web domain data sets. To keep the same scale for training and unlabeled sets allows us easily adapt the best setting from web domain experiments. 4.3 Dependency Parser We use the Mate transition-based dependency parser with default settings in our experiments, cf. Bohnet et al. (2013). For tagging, we use predicted pos tags to carry out the experiments as we believe that this is a more realistic scenario. The parser’s internal tagger is used to supply the pos tags for both unlabeled sets and test datasets. In order to compare with previous work, we evaluate the approaches additionally on gold pos tags for texts of the chemical domain as gold tags were used by previous work. The baselines are generated by training the parser on the source domain and testing the parser on the described target domain test sets. Evaluation Method For the evaluation of the parser’s accuracy, we"
W15-2201,W14-6110,0,0.0307578,"Missing"
W15-2201,P05-1022,0,0.149739,"Missing"
W15-2201,C08-1015,0,0.178396,"Missing"
W15-2201,D13-1129,0,0.0943568,"Missing"
W15-2201,N06-1020,0,0.496804,"Missing"
W15-2201,P06-1043,0,0.30751,"Missing"
W15-2201,N12-1068,0,0.171949,"which is based on the observation that a higher parse score is correlated with a higher parsing quality. The second method uses the method of Mejer and Crammer (2012) to compute the Delta score. Mejer and Crammer (2012) compute a confidence score for each edge. The algorithm attaches each edge to an alternative head. The Delta is the score difference between the original dependency tree and the tree with the changed edge. This method provides a per-edge confidence score. Note that the scores are real numbers and might be greater than 1. We changed the Deltaapproach in two aspects from that of Mejer and Crammer (2012). The new parse tree contains a node that has either a different head or might have a different edge label or both since we use labeled dependency trees in contrast to Mejer and Crammer (2012). To obtain a single score for a tree, we use the averaged score of all score differences gained for each edge by the ‘Delta’-method. We use the Mate tools1 to implement our selftraining approach. The Mate tools contain a part-of-speech (pos) tagger, morphological tagger, lemmatizer, graph-based parser and an arcstandard transition-based parser. The arc-standard transition-based parser has the option to u"
W15-2201,I11-1172,0,0.0382397,"Missing"
W15-2201,N07-1050,0,0.0236988,"Missing"
W15-2201,W07-2416,0,0.0361359,"Missing"
W15-2201,W14-6105,1,0.871313,"Missing"
W15-2201,P98-1106,0,0.272765,"Missing"
W15-2201,I08-2097,0,0.167603,"ant results are marked due to their p-values, (*) p-value&lt;0.05, (**) p-value&lt;0.01. 5 Results and Discussion Random Selection-based Self-training. As a baseline experiment, we apply self-training on 6 Parse Score Delta Baseline Kawahara (Self-trained) Kawahara (Baseline) Sagae (Co-training) PPOS LAS UAS 80.8* 83.62* 81.1* 83.71* 79.68 82.5 - GPOS LAS UAS 83.44** 85.74** 83.58** 85.8** 81.96 84.28 84.12 83.58 81.06 83.42 Table 5: The results of the adjusted parse score-based and the Delta-based self-training approaches on the chemical test set compared with the best-reported self-training gain (Kawahara and Uchimoto, 2008) and the best results of CoNLL 2007 shared task, cf. Sagae and Tsujii (2007). (PPOS: results based on predicted pos tags, GPOS: results based on gold pos tags, Self-trained: results of self-training experiments, Co-trained: results of co-training experiments.) Weblogs Newsgroups Reviews Average PS 79.80** 75.88** 75.43* 77.03 Delta 79.68** 75.87* 75.6** 77.05 Baseline 78.99 75.3 75.07 76.45 we add 50k to 100k sentences. Our error analysis shows that these parse trees are mainly short sentences consisting of only three words. These sentences contribute probably no additional information that th"
W15-2201,P07-1078,0,0.364305,"lly used for instance in constituency parsing for in-domain and out-of-domain parsing (McClosky et al., 2006a; McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae, 2010). McClosky et al. (2006a) used self-training for constituency parsing. In their approaches, self-training was most effective when the parser is retrained on the combination of the initial training set and the large unlabeled dataset generated by both the generative parser and the reranker. This leads to many subsequent applications on domain adaptation via self-training for constituency parsing (McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae, 2010; Petrov and McDonald, 2012), while for dependency parsing, self-training was only effective in few cases. The question why it does not work equally well for dependency parsing is still a question that has not been satisfactorily answered. The paper tries to shed some light on the question under which circumstances and why self-training is applicable. More precisely, this paper makes the following contributions: This paper presents a successful approach for domain adaptation of a dependency parser via self-training. We improve parsing accuracy for out-of-domain texts with a self-t"
W15-2201,D07-1111,0,0.163851,". 5 Results and Discussion Random Selection-based Self-training. As a baseline experiment, we apply self-training on 6 Parse Score Delta Baseline Kawahara (Self-trained) Kawahara (Baseline) Sagae (Co-training) PPOS LAS UAS 80.8* 83.62* 81.1* 83.71* 79.68 82.5 - GPOS LAS UAS 83.44** 85.74** 83.58** 85.8** 81.96 84.28 84.12 83.58 81.06 83.42 Table 5: The results of the adjusted parse score-based and the Delta-based self-training approaches on the chemical test set compared with the best-reported self-training gain (Kawahara and Uchimoto, 2008) and the best results of CoNLL 2007 shared task, cf. Sagae and Tsujii (2007). (PPOS: results based on predicted pos tags, GPOS: results based on gold pos tags, Self-trained: results of self-training experiments, Co-trained: results of co-training experiments.) Weblogs Newsgroups Reviews Average PS 79.80** 75.88** 75.43* 77.03 Delta 79.68** 75.87* 75.6** 77.05 Baseline 78.99 75.3 75.07 76.45 we add 50k to 100k sentences. Our error analysis shows that these parse trees are mainly short sentences consisting of only three words. These sentences contribute probably no additional information that the parser can exploit. Evaluating on Test Sets. We adapt our best settings of"
W15-2201,W10-2606,0,0.0843831,"Missing"
W15-2201,N01-1023,0,0.6895,"Missing"
W15-2201,C10-1120,0,0.345056,"Missing"
W15-2201,C98-1102,0,\N,Missing
W15-2201,P08-1068,0,\N,Missing
W15-2201,W09-1201,0,\N,Missing
W15-2201,D07-1096,0,\N,Missing
W17-3517,W11-2832,1,0.948241,"Missing"
W17-3517,kow-belz-2012-lg,1,0.830242,"em, synonym and paraphrase matches. We will apply text normalization before scoring. For n-best ranked system outputs, we will compute a single score for all outputs by computing the weighted sum of their individual scores, with a weight assigned to an output in inverse proportion to its rank. For a subset of the test data we may obtain additional alternative realizations via Mechanical Turk for use in the automatic evaluations. 8 http://universaldependencies.org/ format.html For the human-assessed evaluation, we are planning to use a type of evaluation that is based on preference judgements (Kow and Belz, 2012, p.4035), using the existing evaluation interface described in Kow and Belz’s paper. As in SR’11, we plan to use students in the third year of an undergraduate degree, from Cambridge, Oxford and Edinburgh. Two candidate outputs9 will be presented to the evaluators, who will assess them for Clarity, Fluency and Meaning Similarity. For each criterion, they will be asked not only to state which system output they prefer, but also how strong is their preference. We plan to organize a workshop collocated with ACL ’18, COLING ’18, or EMNLP ’18 at which the results of the SR’18 will be presented. To"
W17-3517,S17-2090,0,0.0229781,"rained PoS and morphological information will be removed from the input trees. The first shared task on Surface Realization was carried out in 2011 with a similar setup, with a focus on English. We think that it is time for relaunching such a shared task effort in view of the arrival of Universal Dependencies annotated treebanks for a large number of languages on the one hand, and the increasing dominance of Deep Learning, which proved to be a game changer for NLP, on the other hand. 1 Introduction In 2017, three shared tasks on Natural Language Generation (NLG) take place: Task 9 of SemEval (May and Priyadarshi, 2017), WebNLG1 and E2E2 . The first starts from Abstract Meaning Representations (AMRs), the second from RDF triples, and the third from dialog act-based Meaning Representations (MRs) respectively. With these efforts, the focus is put on “real-life” generation, since the respective inputs come from existing analyzers (for AMRs) or existing databases (for RDF triples and 1 http://talc1.loria.fr/webnlg/stories/ challenge.html 2 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ MRs). This shows that the research on NLG is on the right track and that there is an interest in large scale “deep” NLG. However,"
W17-3517,W04-2705,0,0.187846,"Missing"
W17-3517,L16-1262,0,0.0611377,"Missing"
W17-3517,J05-1004,0,0.0504225,"their lemmas or stems, depending on the availability of lemmatization and stemming tools, respectively. For the Deep Track, additionally: 3. functional prepositions and conjunctions that can be inferred from other lexical units or from the syntactic structure will be removed, as e.g., “by” and “of” in Figure 2; 4. determiners and auxiliaries will be replaced (when needed) by attribute/value pairs, as, e.g., “Definiteness” and “Aspect” in Figure 3; 5. edge labels will be generalized into predicate argument labels, following the PropBank/NomBank edge label nomenclature (Meyers and et al., 2004; Palmer et al., 2005), with three main differences: (i) there will be no special label for external arguments (i.e., no “A0”), which means that all first arguments of a predicate will be mapped to A1, and the rest of the arguments will be labeled starting from A2; (ii) all modifier edges “AM-...” will be generalized to “AM”; (iii) there will be a coordinative relation; and (iv) any relation that does not fall into the first three cases will be assigned an underspecified edge label. 6. morphological information coming from the syntactic structure or from agreements will be removed; in other words, only “semantic” i"
W17-6302,P16-1231,0,0.023196,"Missing"
W17-6302,C08-1015,0,0.0626407,"Missing"
W17-6302,P12-1023,0,0.0465776,"Missing"
W17-6302,P08-1068,0,0.0459435,"ness of the proposed approach, we evaluate our parser on standard English and Chinese data where the base parser could achieve competitive accuracy scores. Our enhanced parser achieved state-of-the-art accuracy on Chinese data and competitive results on English data. We gained a large absolute improvement of one point (UAS) on Chinese and 0.5 points for English. 1 Introduction In recent years, using unlabeled data to improve natural language parsing has seen a surge of interest as the data can easy and inexpensively be obtained, cf. (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008; Søgaard and Rishøj, 2010; Petrov and McDonald, 2012; Chen et al., 2013; Weiss et al., 2015). This is in stark contrast to the high costs of manually labeling new data. Some of the techniques such as self-training (McClosky et al., 2006) and cotraining (Sarkar, 2001) use auto-parsed data as additional training data. This enables the parser to learn from its own or other parser’s annotations. Other techniques include word clustering (Koo et al., 2008) and word embedding (Bengio et al., 2003) which are generated from a large amount of unannotated data. The outputs can be used as features or inp"
W17-6302,D13-1129,0,0.0209182,"sh and Chinese data where the base parser could achieve competitive accuracy scores. Our enhanced parser achieved state-of-the-art accuracy on Chinese data and competitive results on English data. We gained a large absolute improvement of one point (UAS) on Chinese and 0.5 points for English. 1 Introduction In recent years, using unlabeled data to improve natural language parsing has seen a surge of interest as the data can easy and inexpensively be obtained, cf. (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008; Søgaard and Rishøj, 2010; Petrov and McDonald, 2012; Chen et al., 2013; Weiss et al., 2015). This is in stark contrast to the high costs of manually labeling new data. Some of the techniques such as self-training (McClosky et al., 2006) and cotraining (Sarkar, 2001) use auto-parsed data as additional training data. This enables the parser to learn from its own or other parser’s annotations. Other techniques include word clustering (Koo et al., 2008) and word embedding (Bengio et al., 2003) which are generated from a large amount of unannotated data. The outputs can be used as features or inputs for parsers. Both groups 1. We applied the DLM to a strong parser th"
W17-6302,C12-1103,0,0.0329179,"Missing"
W17-6302,Q17-1029,0,0.0387804,"Missing"
W17-6302,de-marneffe-etal-2006-generating,0,0.145647,"Missing"
W17-6302,J93-2004,0,0.0773518,"hare the same head with xch but closer to the head word according to the word sequence in the sentence. Let’s consider the left side child xLk in the 12 < N ODLM , φ(Pu (s0 )), φ(Pu (s1 )), label > < N ODLM , φ(Pu (s0 )), φ(Pu (s1 )), label, s0 < N ODLM , φ(Pu (s0 )), φ(Pu (s1 )), label, s0 < N ODLM , φ(Pu (s0 )), φ(Pu (s1 )), label, s1 < N ODLM , φ(Pu (s0 )), φ(Pu (s1 )), label, s1 < N ODLM , φ(Pu (s0 )), φ(Pu (s1 )), label, s0 < N ODLM , φ(Pu (s0 )), φ(Pu (s1 )), label, s0 4 pos > word > pos > word > pos, s1 pos > word, s1 word > For our experiments, we used the Penn English Treebank (PTB) (Marcus et al., 1993) and Chinese Treebank 5 (CTB5) (Xue et al., 2005). For English, we follow the standard splits and used Stanford parser 1 v3.3.0 to convert the constituency trees into Stanford style dependencies (de Marneffe et al., 2006). For Chinese, we follow the splits of Zhang and Nivre (2011), the constituency trees are converted to dependency relations by Penn2Malt2 tool using head rules of Zhang and Clark (2008). Table 2 shows the splits of our data. We used gold segmentation for Chinese tests to make our work comparable with previous work. We used predicted part-of-speech tags for both languages in al"
W17-6302,P13-2109,0,0.0613045,"Missing"
W17-6302,P15-1033,0,0.0385048,"Missing"
W17-6302,N06-1020,0,0.0367185,"monstrate the effectiveness of the proposed approach, we evaluate our parser on standard English and Chinese data where the base parser could achieve competitive accuracy scores. Our enhanced parser achieved state-of-the-art accuracy on Chinese data and competitive results on English data. We gained a large absolute improvement of one point (UAS) on Chinese and 0.5 points for English. 1 Introduction In recent years, using unlabeled data to improve natural language parsing has seen a surge of interest as the data can easy and inexpensively be obtained, cf. (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008; Søgaard and Rishøj, 2010; Petrov and McDonald, 2012; Chen et al., 2013; Weiss et al., 2015). This is in stark contrast to the high costs of manually labeling new data. Some of the techniques such as self-training (McClosky et al., 2006) and cotraining (Sarkar, 2001) use auto-parsed data as additional training data. This enables the parser to learn from its own or other parser’s annotations. Other techniques include word clustering (Koo et al., 2008) and word embedding (Bengio et al., 2003) which are generated from a large amount of unannotated data. The outputs can be used"
W17-6302,P12-1082,0,0.0430191,"Missing"
W17-6302,I11-1136,0,0.0646743,"Missing"
W17-6302,N07-1051,0,0.0167702,"a of Chelba et al. (2013) which contains around 30 million sentences (800 million words) from the news domain. For Chinese, we used Xinhua portion of Chinese Gigaword 3 Version 5.0 (LDC2011T13). The Chinese unlabeled data we used consists of 20 million sentences which is roughly 450 million words after being segmented by ZPar4 v0.7.5. The word segmentor is trained on the CTB5 training set. In most of our experiments, the DLMs are extracted from data annotated by our base parser. For the evaluation on higher quality DLMs, the unlabeled data is additionally tagged and parsed by Berkeley parser (Petrov and Klein, 2007) and is converted to dependency trees with the same tools as for gold data. We used Mate transition-based parser with its default setting and a beam of 40 as our baseline. Table 1: Feature templates which we use in the parser. PTB CTB5 train 2-21 001-815, 1001-1136 dev 22 886-931, 1148-1151 test 23 816-885, 1137-1147 Table 2: Our data splits for English and Chinese dependency relations (xLk ...xL1 , xh , xR1 ...xRm ) as an example, the N-1 immediate previous children for xLk are xLk−1 ..xLk−N +1 . In our approach, we estimate Pu (xch |H) by the relative frequency: Pu (xch |H) = P count(xch , H"
W17-6302,D09-1127,0,0.0723353,"Missing"
W17-6302,I08-2097,0,0.0223421,"Missing"
W17-6302,P07-1078,0,0.0553448,"Missing"
W17-6302,D15-1158,0,0.0311992,"Missing"
W17-6302,D07-1111,0,0.105377,"Missing"
W17-6302,D08-1059,0,0.0401453,"0 )), φ(Pu (s1 )), label, s0 < N ODLM , φ(Pu (s0 )), φ(Pu (s1 )), label, s0 4 pos > word > pos > word > pos, s1 pos > word, s1 word > For our experiments, we used the Penn English Treebank (PTB) (Marcus et al., 1993) and Chinese Treebank 5 (CTB5) (Xue et al., 2005). For English, we follow the standard splits and used Stanford parser 1 v3.3.0 to convert the constituency trees into Stanford style dependencies (de Marneffe et al., 2006). For Chinese, we follow the splits of Zhang and Nivre (2011), the constituency trees are converted to dependency relations by Penn2Malt2 tool using head rules of Zhang and Clark (2008). Table 2 shows the splits of our data. We used gold segmentation for Chinese tests to make our work comparable with previous work. We used predicted part-of-speech tags for both languages in all evaluations. Tags are assigned by base parser’s internal joint tagger trained on the training set. We report labeled (LAS) and unlabeled (UAS) attachment scores, punctuation marks are excluded from the evaluation. For the English unlabeled data, we used the data of Chelba et al. (2013) which contains around 30 million sentences (800 million words) from the news domain. For Chinese, we used Xinhua port"
W17-6302,N01-1023,0,0.0386996,"anguage models into the parser. To demonstrate the effectiveness of the proposed approach, we evaluate our parser on standard English and Chinese data where the base parser could achieve competitive accuracy scores. Our enhanced parser achieved state-of-the-art accuracy on Chinese data and competitive results on English data. We gained a large absolute improvement of one point (UAS) on Chinese and 0.5 points for English. 1 Introduction In recent years, using unlabeled data to improve natural language parsing has seen a surge of interest as the data can easy and inexpensively be obtained, cf. (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008; Søgaard and Rishøj, 2010; Petrov and McDonald, 2012; Chen et al., 2013; Weiss et al., 2015). This is in stark contrast to the high costs of manually labeling new data. Some of the techniques such as self-training (McClosky et al., 2006) and cotraining (Sarkar, 2001) use auto-parsed data as additional training data. This enables the parser to learn from its own or other parser’s annotations. Other techniques include word clustering (Koo et al., 2008) and word embedding (Bengio et al., 2003) which are generated from a large amount"
W17-6302,P08-1066,0,0.0435031,"hnet Google London, UK bohnetbd@google.com Abstract of techniques have been shown effective on syntactic parsing tasks (Zhou and Li, 2005; Reichart and Rappoport, 2007; Sagae, 2010; Søgaard and Rishøj, 2010; Yu et al., 2015; Weiss et al., 2015). However, most word clustering and the word embedding approaches do not consider the syntactic structures and most self-/co-training approaches can use only a relatively small additional training data as training parsers on a large corpus might be time-consuming or even intractable on a corpus of millions of sentences. Dependency language models (DLM) (Shen et al., 2008) are variants of language models based on dependency structures. An N-gram DLM is able to predict the next child when given N-1 immediate previous children and their head. Chen et al. (2012) integrated first a high-order DLM into a second-order graph-based parser. The DLM allows the parser to explore high-order features but not increasing the time complexity. Following Chen et al. (2012), we adapted the DLM to transition-based dependency parsing. Our approach is different from Chen et al. (2012)’s in a number of important aspects: In this paper, we present an approach to improve the accuracy o"
W17-6302,P11-2033,0,0.0287395,", label, s0 < N ODLM , φ(Pu (s0 )), φ(Pu (s1 )), label, s1 < N ODLM , φ(Pu (s0 )), φ(Pu (s1 )), label, s1 < N ODLM , φ(Pu (s0 )), φ(Pu (s1 )), label, s0 < N ODLM , φ(Pu (s0 )), φ(Pu (s1 )), label, s0 4 pos > word > pos > word > pos, s1 pos > word, s1 word > For our experiments, we used the Penn English Treebank (PTB) (Marcus et al., 1993) and Chinese Treebank 5 (CTB5) (Xue et al., 2005). For English, we follow the standard splits and used Stanford parser 1 v3.3.0 to convert the constituency trees into Stanford style dependencies (de Marneffe et al., 2006). For Chinese, we follow the splits of Zhang and Nivre (2011), the constituency trees are converted to dependency relations by Penn2Malt2 tool using head rules of Zhang and Clark (2008). Table 2 shows the splits of our data. We used gold segmentation for Chinese tests to make our work comparable with previous work. We used predicted part-of-speech tags for both languages in all evaluations. Tags are assigned by base parser’s internal joint tagger trained on the training set. We report labeled (LAS) and unlabeled (UAS) attachment scores, punctuation marks are excluded from the evaluation. For the English unlabeled data, we used the data of Chelba et al."
W17-6302,C10-1120,0,0.149487,"ed approach, we evaluate our parser on standard English and Chinese data where the base parser could achieve competitive accuracy scores. Our enhanced parser achieved state-of-the-art accuracy on Chinese data and competitive results on English data. We gained a large absolute improvement of one point (UAS) on Chinese and 0.5 points for English. 1 Introduction In recent years, using unlabeled data to improve natural language parsing has seen a surge of interest as the data can easy and inexpensively be obtained, cf. (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008; Søgaard and Rishøj, 2010; Petrov and McDonald, 2012; Chen et al., 2013; Weiss et al., 2015). This is in stark contrast to the high costs of manually labeling new data. Some of the techniques such as self-training (McClosky et al., 2006) and cotraining (Sarkar, 2001) use auto-parsed data as additional training data. This enables the parser to learn from its own or other parser’s annotations. Other techniques include word clustering (Koo et al., 2008) and word embedding (Bengio et al., 2003) which are generated from a large amount of unannotated data. The outputs can be used as features or inputs for parsers. Both grou"
W17-6302,D09-1058,0,0.0571661,"Missing"
W17-6302,P15-1032,0,0.0139686,"where the base parser could achieve competitive accuracy scores. Our enhanced parser achieved state-of-the-art accuracy on Chinese data and competitive results on English data. We gained a large absolute improvement of one point (UAS) on Chinese and 0.5 points for English. 1 Introduction In recent years, using unlabeled data to improve natural language parsing has seen a surge of interest as the data can easy and inexpensively be obtained, cf. (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008; Søgaard and Rishøj, 2010; Petrov and McDonald, 2012; Chen et al., 2013; Weiss et al., 2015). This is in stark contrast to the high costs of manually labeling new data. Some of the techniques such as self-training (McClosky et al., 2006) and cotraining (Sarkar, 2001) use auto-parsed data as additional training data. This enables the parser to learn from its own or other parser’s annotations. Other techniques include word clustering (Koo et al., 2008) and word embedding (Bengio et al., 2003) which are generated from a large amount of unannotated data. The outputs can be used as features or inputs for parsers. Both groups 1. We applied the DLM to a strong parser that on its own has a c"
W17-6302,W03-3023,0,0.336359,"Missing"
W17-6302,W15-2201,1,0.859309,"Missing"
W18-3601,P11-2040,1,0.888439,"Missing"
W18-3601,W11-2832,1,0.629164,"e languages may also work for others. The SR’18 task is to generate sentences from structures at the level of abstraction of outputs in state-of-the-art parsing, encouraging participants to explore the extent to which neural network parsing algorithms can be reversed for generation. SR’18 also addresses questions about just how suitable and useful the notion of universal dependencies—which is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular—is for NLG. SR’18 follows the SR’11 pilot surface realisation task for English (Belz et al., 2011) which was part of Generation Challenges 2011 (GenChal’11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks. Outside of the SR tasks, just three ‘deep’ NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG3 (Gardent et al., 2017), SemEval Task 94 (May and Priyadarshi, 2017), and E2E5 (Novikova et al., 2017). What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016): http:// universaldependencies.org/conll17/. 3 http://talc"
W18-3601,W17-4755,1,0.838761,"gy) of 100 outputs, of which 20 are used solely for quality assurance (QA) (i.e. do not count towards system scores): (i) some are repeated as are, (ii) some are repeated in a ‘damaged’ version and (iii) some are replaced by their corresponding reference texts. In each case, a minimum threshold has to be reached for the HIT to be accepted: for (i), scores must be similar enough, for (ii) the score for the damaged version must be worse, and for (iii) the score for the reference text must be high. For full details of how these additional texts are created and thresholds applied, please refer to Bojar et al. (2017a). Below we report QA figures for the MTurk evaluations (Section 3.2.1). Code: We were able to reuse, with minor adaptations, the code produced for the WMT’17 evaluations.10 3.2.2 Google Data Compute Evaluation In order to cover more languages, and to enable comparison between crowdsourced and expert evaluation, we also conducted human evaluations using Google’s internal ‘Data Compute’ system evaluation service, where experienced evaluators carefully assess each system output. We used an interface that matches the WMT’17 interface above, as closely as was possible within the constraints of th"
W18-3601,P17-1017,0,0.0630018,"he notion of universal dependencies—which is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular—is for NLG. SR’18 follows the SR’11 pilot surface realisation task for English (Belz et al., 2011) which was part of Generation Challenges 2011 (GenChal’11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks. Outside of the SR tasks, just three ‘deep’ NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG3 (Gardent et al., 2017), SemEval Task 94 (May and Priyadarshi, 2017), and E2E5 (Novikova et al., 2017). What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016): http:// universaldependencies.org/conll17/. 3 http://talc1.loria.fr/webnlg/stories/ challenge.html 4 http://alt.qcri.org/semeval2017/ task9/ 5 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ tasks have only been offered for English. As in SR’11, the Multilingual Surface Realisation shared task (SR’18) comprises two tracks with different levels of difficulty: Shallow Track: This track starts from genuine UD str"
W18-3601,D14-1020,1,0.831205,"Missing"
W18-3601,S17-2090,0,0.0582517,"h is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular—is for NLG. SR’18 follows the SR’11 pilot surface realisation task for English (Belz et al., 2011) which was part of Generation Challenges 2011 (GenChal’11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks. Outside of the SR tasks, just three ‘deep’ NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG3 (Gardent et al., 2017), SemEval Task 94 (May and Priyadarshi, 2017), and E2E5 (Novikova et al., 2017). What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016): http:// universaldependencies.org/conll17/. 3 http://talc1.loria.fr/webnlg/stories/ challenge.html 4 http://alt.qcri.org/semeval2017/ task9/ 5 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ tasks have only been offered for English. As in SR’11, the Multilingual Surface Realisation shared task (SR’18) comprises two tracks with different levels of difficulty: Shallow Track: This track starts from genuine UD structures in which word order information has b"
W18-3601,W04-2705,0,0.458005,"Missing"
W18-3601,L16-1262,0,0.0728681,"Missing"
W18-3601,W17-5525,0,0.0981398,"Missing"
W18-3601,J05-1004,0,0.102132,"in CoNLL-U format, with no meta-information.7 Figures 1, 2 and 3 show 6 universaldependencies.org 7 http://universaldependencies.org/ a sample original UD annotation for English, and the corresponding shallow and deep input structures derived from it. To create inputs to the Shallow Track, the UD structures were processed as follows: 1. Word order information was removed by randomised scrambling; 2. Words were replaced by their lemmas. For the Deep Track, the following steps were additionally carried out: 3. Edge labels were generalised into predicate/argument labels, in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion. That is, the syntactic relations were mapped to core (A1, A2, etc.) and non-core (AM) labels, applying the following rules: (i) the first argument is always labeled A1 (i.e. there is no external argument A0); (ii) in order to maintain the tree structure and account for some cases of shared arguments, there can be inverted argument relations; (iii) all modifier edges are assigned the same generic label AM; (iv) there is a coordinating relation; see the inventory of relations in Table 1. 4. Functional prepositions and conjunctions in argument position (i.e. prepos"
W18-3601,P02-1040,0,0.102984,"nt Example fall→ the ball the ball→ fall fall→ last night fall→ [and] bounce Tower→ Eiffel N/A Table 1: Deep labels. train dev test ar 6,016 897 676 cs 66,485 9,016 9,876 en 12,375 1,978 2,061 es 14,289 1,651 1,719 fi 12,030 1,336 1,525 fr 14,529 1,473 416 it 12,796 562 480 nl 12,318 720 685 pt 8,325 559 476 ru 48,119 6,441 6,366 Table 2: SR’18 dataset sizes for training, development and test sets. 3 3.1 Evaluation Methods Automatic methods We used BLEU, NIST, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems. BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences. We use the smoothed version and report results for n = 4. NIST9 is a related n-gram similarity metric weighted in favour of less frequent n-grams which are taken to be more informative. Inverse, normalised, character-based string-edit distance (DIST in the tables below) starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn the system output into the (single)"
W18-6527,W10-4226,1,0.705033,"atically parsed data mentioned above shows. It is conceivable that a future shared task in NLG will involve paired (structured) data and text, plus an automatically created intermediate level of representation comprising underspecified UD (UUD) structures enriched with additional information obtained from the structured data level. This would correspond to three linked tracks (data-to-text, data-to-UUD, and UUD-totext) where one track is the end-to-end task, and the other two tracks are subtasks that can be combined to solve the end-to-end task, similar to the GREC’10 shared task competition (Belz and Kow, 2010). Or it could be argued, perhaps controversially still, that the days of structured linguistic representations in NLG are numbered anyway. The rapid development and spread of highly successful neural approaches to diverse NLG tasks, and the limited success so far of attempts to inject linguistic knowledge directly into neural networks, certainly lends some strength to this point of view. In the meantime, the above tripartite shared-task structure has the potential to accommodate both systems that map directly from data to text without structured representations, and two-component systems with"
W18-6527,W11-2832,1,0.853605,"arious ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the corresponding Penn TreeBank parse trees to dependency structures (Surdeanu et al., 2008). While dependency structures offer a more flexible input structure and statistical systems, in principle, offer more robustness, the uptake of such systems as comp"
W18-6527,W04-2705,0,0.339346,"Missing"
W18-6527,W18-3601,1,0.855834,"as Inputs for Multilingual Surface Realisation Simon Mille Universitat Pompeu Fabra Barcelona, Spain simon.mille@upf.edu Anja Belz University of Brighton Brighton, UK a.s.belz@brighton.ac.uk Bernd Bohnet Google Inc. London, UK bohnetbd@google.com Leo Wanner ICREA and Universitat Pompeu Fabra Barcelona, Spain leo.wanner@upf.edu Abstract resolved. The success of SimpleNLG (Gatt and Reiter, 2009) which had much reduced grammatical coverage, but accepted radically simpler inputs demonstrated the importance of this issue. The recently completed first Multilingual Surface Realisation Task (SR’18) (Mille et al., 2018) used for the first time inputs derived from the Universal Dependencies (UDs) (de Marneffe et al., 2014), a framework which was devised with the aim of facilitating cross-linguistically consistent grammatical annotation, and which has grown into a large-scale community effort involving more than 200 contributors, who have created over 100 treebanks in over 70 languages between them.1 UDs provide a more general and potentially flexible input representation for surface realisation (SR). However, their use for NLG has not so far been demonstrated. In this paper, we present the UD datasets used in"
W18-6527,bohnet-wanner-2010-open,1,0.85998,"Missing"
W18-6527,P06-1130,0,0.111814,"Missing"
W18-6527,W96-0501,0,0.474787,"In addition, we examine the motivation for, and likely usefulness of, deriving NLG inputs from annotations in resources originally developed for Natural Language Understanding (NLU), and assess whether the resulting inputs supply enough information of the right kind for the final stage in the NLG process. 1 Introduction There has long been an assumption in Natural Language Generation (NLG) that surface realisation can be treated as an independent subtask for which stand-alone, plug-and-play tools can, and should, be created. Early surface realisers such as KPML (Bateman, 1997) and FUF/Surge (Elhadad and Robin, 1996) were ambitious, independent surface realisation tools for English with wide grammatical coverage. However, the question of how the NLG components addressing the stage before surface realisation were supposed to put together inputs of the level of grammatical sophistication required by such tools was never quite 1 http://universaldependencies.org/ 199 Proceedings of The 11th International Natural Language Generation Conference, pages 199–209, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics while Section 6 discusses their suitability for SR and NLG"
W18-6527,P17-1017,0,0.0215344,"mmatical coverage. However, the question of how the NLG components addressing the stage before surface realisation were supposed to put together inputs of the level of grammatical sophistication required by such tools was never quite 1 http://universaldependencies.org/ 199 Proceedings of The 11th International Natural Language Generation Conference, pages 199–209, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics while Section 6 discusses their suitability for SR and NLG more generally. Some conclusions are presented in Section 7. • WebNLG dataset (Gardent et al., 2017): DBpedia triples covering properties of 15 DBpedia categories; 2 • E2E dataset (Novikova et al., 2017): attribute-value pairs covering 8 properties related to the restaurant domain. Background With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journa"
W18-6527,W05-1510,0,0.0204745,"ibute-value pairs covering 8 properties related to the restaurant domain. Background With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the corresponding Penn Tree"
W18-6527,W09-0613,0,0.204092,"Missing"
W18-6527,W17-5525,0,0.0700189,"Missing"
W18-6527,W02-2103,0,0.0384065,"ova et al., 2017): attribute-value pairs covering 8 properties related to the restaurant domain. Background With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the"
W18-6527,J05-1004,0,0.700717,"0 2 3 4 A2 ROOT A1 A2 AM Figure 3: Deep input (Track 2) derived from UD structure in Figure 1. (left: CoNLL-U, righ: graphical) ures 1 and 2 show an original UD structure and a SR’18 Shallow input, respectively. • the in the head can be seen as a marker for nominal definiteness; 3.2 • the conjunction (complementiser) that in, e.g., I demand that you apologise, appears because it connects a finite verb apologise as an argument of another verb demand. Deep inputs The Deep Track input structures are trees that contain only content words linked by predicateargument edges, in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion. The Deep inputs can be seen as closer to a realistic application context for NLG systems, in which the component that generates the inputs presumably would not have access to syntactic or language-specific information. At the same time, we used only information found in the UD structures to create the Deep inputs, and tried to keep their structure simple. In Deep inputs, words are not disambiguated, full (semantically loaded) prepositions may be missing, and some argument relations may be underspecified or missing. The next two subsections provide more details a"
W18-6527,P09-1011,0,0.0151157,"ncies, the UD V2.0 treebank, as released in the context of the CoNLL 2017 shared task on multilingual dependency parsing (Zeman et al., 2017), was used. A subset of ten languages was selected that contains the necessary part-ofspeech and morphological tags for the Shallow Track: Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish. Three of these languages, namely English, French and Spanish were used also for the Deep Track. Starting from UD structures as they appear in the treebanks, Shallow and Deep inputs • Weather forecast generation (Weather) dataset (Liang et al., 2009): time series from weather-related measurements; • Abstract Meaning Representation (AMR) dataset (May and Priyadarshi, 2017): abstract predicate-argument graphs that cover several genres; 200 1 2 3 4 5 6 7 8 9 10 11 12 13 The third was being run by the head of an investment firm . the third be be run by the head of a investment firm . DET ADJ AUX AUX VERB ADP DET NOUN ADP DET NOUN NOUN PUNCT DT JJ VBD VBG VBN IN DT NN IN DT NN NN . Definite=Def|PronType=Art Degree=Pos|NumType=Ord Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin VerbForm=Ger Tense=Past|VerbForm=Part|Voice=Pass Definite=Def"
W18-6527,W08-2121,0,0.136007,"Missing"
W18-6527,de-marneffe-etal-2014-universal,0,0.0679075,"Missing"
W18-6527,D09-1043,0,0.027139,"ground With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the corresponding Penn TreeBank parse trees to dependency structures (Surdeanu et al., 2008). While dependen"
W18-6527,K17-3001,0,0.0322515,"a Shallow Track, starting from syntactic structures in which word order information has been removed and tokens have been lemmatised, and a Deep Track, which starts from more abstract structures from which, additionally, functional words (in particular, auxiliaries, functional prepositions and conjunctions) and surface-oriented morphological information have been removed. Taking advantage of the growing availability of multilingual treebanks annotated with Universal Dependencies, the UD V2.0 treebank, as released in the context of the CoNLL 2017 shared task on multilingual dependency parsing (Zeman et al., 2017), was used. A subset of ten languages was selected that contains the necessary part-ofspeech and morphological tags for the Shallow Track: Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish. Three of these languages, namely English, French and Spanish were used also for the Deep Track. Starting from UD structures as they appear in the treebanks, Shallow and Deep inputs • Weather forecast generation (Weather) dataset (Liang et al., 2009): time series from weather-related measurements; • Abstract Meaning Representation (AMR) dataset (May and Priyadarshi, 201"
W19-8012,P16-1231,0,0.0190811,"quential information relating to the position in the sentence. The resulting parser only requires the vector representations of the top two items on the parser stack, which is, to the best of our knowledge, the smallest feature set ever published for Arc-Standard parsers to date, while still managing to achieve competitive results. 1 Introduction Neural network-based dependency parsers have typically relied on combination of raw features, as represented by their dense vector embeddings to represent features of a sentence as well as the parser state (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Zhou et al., 2015). On the other hand, there has been substantial work on using innovative deep learning architectures to build more informative feature representations. One approach has been to model an input sentence using bi-directional Long Short-Term Memory Networks (bi-lstms) (Cross and Huang, 2016; Kiperwasser and Goldberg, 2016b). The result is a vector for each word that encodes both its information, and relevant information from other parts of the sentence. This approach enabled better results with fewer features than was possible before (Cross and Huang, 2016; Shi et al., 2017). A"
W19-8012,D16-1211,0,0.0430659,"Missing"
W19-8012,D14-1082,0,0.763186,"lating to the dependency tree, as well as sequential information relating to the position in the sentence. The resulting parser only requires the vector representations of the top two items on the parser stack, which is, to the best of our knowledge, the smallest feature set ever published for Arc-Standard parsers to date, while still managing to achieve competitive results. 1 Introduction Neural network-based dependency parsers have typically relied on combination of raw features, as represented by their dense vector embeddings to represent features of a sentence as well as the parser state (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Zhou et al., 2015). On the other hand, there has been substantial work on using innovative deep learning architectures to build more informative feature representations. One approach has been to model an input sentence using bi-directional Long Short-Term Memory Networks (bi-lstms) (Cross and Huang, 2016; Kiperwasser and Goldberg, 2016b). The result is a vector for each word that encodes both its information, and relevant information from other parts of the sentence. This approach enabled better results with fewer features than was possible before (Cro"
W19-8012,P16-2006,0,0.166929,"sbury, UK mohab.elkaref@ibm.com Bernd Bohnet Google Inc. London, UK bohnetbd@google.com Abstract We propose a method to represent dependency trees as dense vectors through the recursive application of Long Short-Term Memory networks to build Recursive LSTM Trees (RLTs). We show that the dense vectors produced by Recursive LSTM Trees replace the need for structural features by using them as feature vectors for a greedy Arc-Standard transition-based dependency parser. We also show that RLTs have the ability to incorporate useful information from the bi-LSTM contextualized representation used by Cross and Huang (2016) and Kiperwasser and Goldberg (2016b). The resulting dense vectors are able to express both structural information relating to the dependency tree, as well as sequential information relating to the position in the sentence. The resulting parser only requires the vector representations of the top two items on the parser stack, which is, to the best of our knowledge, the smallest feature set ever published for Arc-Standard parsers to date, while still managing to achieve competitive results. 1 Introduction Neural network-based dependency parsers have typically relied on combination of raw featur"
W19-8012,de-marneffe-etal-2006-generating,0,0.209315,"Missing"
W19-8012,P15-1033,0,0.266979,"to build more informative feature representations. One approach has been to model an input sentence using bi-directional Long Short-Term Memory Networks (bi-lstms) (Cross and Huang, 2016; Kiperwasser and Goldberg, 2016b). The result is a vector for each word that encodes both its information, and relevant information from other parts of the sentence. This approach enabled better results with fewer features than was possible before (Cross and Huang, 2016; Shi et al., 2017). Another approach has been to represent the dependency tree itself with some form of recursive network, either bottom-up (Dyer et al., 2015; Kiperwasser and Goldberg, 2016a; Stenetorp, 2013), or topdown (Le and Zuidema, 2014). In this paper we propose a new method of recursively modelling dependency trees using LSTMs, which we call Recursive Tree LSTMs. Our experiments show that this method of representation is very powerful, and can even be used as an additional layer of encoding over bi-lstm feature representation, which results in a more informative model. The final parser is capable of achieving competitive results with a feature set consisting of only the top two items on the stack, which is the smallest feature set for an A"
W19-8012,Q16-1023,0,0.342785,".com Bernd Bohnet Google Inc. London, UK bohnetbd@google.com Abstract We propose a method to represent dependency trees as dense vectors through the recursive application of Long Short-Term Memory networks to build Recursive LSTM Trees (RLTs). We show that the dense vectors produced by Recursive LSTM Trees replace the need for structural features by using them as feature vectors for a greedy Arc-Standard transition-based dependency parser. We also show that RLTs have the ability to incorporate useful information from the bi-LSTM contextualized representation used by Cross and Huang (2016) and Kiperwasser and Goldberg (2016b). The resulting dense vectors are able to express both structural information relating to the dependency tree, as well as sequential information relating to the position in the sentence. The resulting parser only requires the vector representations of the top two items on the parser stack, which is, to the best of our knowledge, the smallest feature set ever published for Arc-Standard parsers to date, while still managing to achieve competitive results. 1 Introduction Neural network-based dependency parsers have typically relied on combination of raw features, as represented by their dense v"
W19-8012,D14-1081,0,0.0312011,"ut sentence using bi-directional Long Short-Term Memory Networks (bi-lstms) (Cross and Huang, 2016; Kiperwasser and Goldberg, 2016b). The result is a vector for each word that encodes both its information, and relevant information from other parts of the sentence. This approach enabled better results with fewer features than was possible before (Cross and Huang, 2016; Shi et al., 2017). Another approach has been to represent the dependency tree itself with some form of recursive network, either bottom-up (Dyer et al., 2015; Kiperwasser and Goldberg, 2016a; Stenetorp, 2013), or topdown (Le and Zuidema, 2014). In this paper we propose a new method of recursively modelling dependency trees using LSTMs, which we call Recursive Tree LSTMs. Our experiments show that this method of representation is very powerful, and can even be used as an additional layer of encoding over bi-lstm feature representation, which results in a more informative model. The final parser is capable of achieving competitive results with a feature set consisting of only the top two items on the stack, which is the smallest feature set for an Arc-Standard dependency parser used successfully to date. 2 Recursive LSTM Trees (RLTs)"
W19-8012,J93-2004,0,0.0644417,"cture of the parser is show in figure 2. All weights and pos tag vectors were initialisedPuniformly (Glorot and Bengio, 2010). For training we use a negative log likelihood loss function, − i log(yi ), where yi is the score of the gold transition from the final softmax layer for the training input/output pair i in the mini-batch. We use mini-batch updates of 10 sentences, and stop training after 30 epochs. We optimize the model parameters using Adam (Kingma and Ba, 2014) with a learning rate α = 1 × 10−3 . We train our models using the Wall Street Journal (WSJ) section from the Penn Treebank (Marcus et al., 1993). We use §2-21 for training, §22 for development, and §23 for testing. We use Stanford Dependencies (SD) (De Marneffe et al., 2006) converted from constituency trees using version 3.3.0 of the converter. As is standard we use predicted POS tags for the train, dev, and test sets. We report unlabeled attachment score (UAS) and labeled attachment score (LAS), with punctuation excluded. The models are tuned on 1 Implementation available at https://github.com/MohabElkaref/rlt Encoding Type word/pos embeddings contextualized vectors K & G (2016a) Dyer et al. (2015) UAS 92.94 94.26 93.3 93.2 LAS 90.6"
W19-8012,D14-1162,0,0.099355,"2: Example of a parser configuration with features using RLTs. 3 Implementation & Training Details We implemented1 our model in python using the DyNet framework (Neubig et al., 2017). The encoding mechanisms used by the RLTs in our experiments used 2 layers of LSTMs of size 256. For experiments using bi-lstm contextualized representation, we also used 2 layers of bi-lstms of size 256 in each direction. For the basic vector representations we used randomly initialized part-of-speech tag vectors of size 50, and for word embeddings we used vectors of size 100 initialized using the GloVe vectors (Pennington et al., 2014) trained on 6B. tokens with 400k vocabulary. The tree vectors of relevant RLTs are then concatenated and passed as input to two sets of two feedforward hidden layers of size 256, with rectified linear units (ReLUs) (Nair and Hinton, 2010) as activation functions. The two sets of hidden layers are responsible for modelling the relevant information for dependency parsing and dependency labelling separately, similar to the hierarchical architecture used by Cross and Huang (2016). We set a dropout rate of 0.3 on all LSTMs (Gal, 2015) and the hidden layers (Hinton et al., 2012). In our experiments"
W19-8012,D17-1002,0,0.011988,"; Andor et al., 2016; Zhou et al., 2015). On the other hand, there has been substantial work on using innovative deep learning architectures to build more informative feature representations. One approach has been to model an input sentence using bi-directional Long Short-Term Memory Networks (bi-lstms) (Cross and Huang, 2016; Kiperwasser and Goldberg, 2016b). The result is a vector for each word that encodes both its information, and relevant information from other parts of the sentence. This approach enabled better results with fewer features than was possible before (Cross and Huang, 2016; Shi et al., 2017). Another approach has been to represent the dependency tree itself with some form of recursive network, either bottom-up (Dyer et al., 2015; Kiperwasser and Goldberg, 2016a; Stenetorp, 2013), or topdown (Le and Zuidema, 2014). In this paper we propose a new method of recursively modelling dependency trees using LSTMs, which we call Recursive Tree LSTMs. Our experiments show that this method of representation is very powerful, and can even be used as an additional layer of encoding over bi-lstm feature representation, which results in a more informative model. The final parser is capable of ac"
W19-8012,P15-1150,0,0.114312,"Missing"
W19-8012,P15-1032,0,0.276997,"tree, as well as sequential information relating to the position in the sentence. The resulting parser only requires the vector representations of the top two items on the parser stack, which is, to the best of our knowledge, the smallest feature set ever published for Arc-Standard parsers to date, while still managing to achieve competitive results. 1 Introduction Neural network-based dependency parsers have typically relied on combination of raw features, as represented by their dense vector embeddings to represent features of a sentence as well as the parser state (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Zhou et al., 2015). On the other hand, there has been substantial work on using innovative deep learning architectures to build more informative feature representations. One approach has been to model an input sentence using bi-directional Long Short-Term Memory Networks (bi-lstms) (Cross and Huang, 2016; Kiperwasser and Goldberg, 2016b). The result is a vector for each word that encodes both its information, and relevant information from other parts of the sentence. This approach enabled better results with fewer features than was possible before (Cross and Huang, 2016;"
W19-8012,P15-1117,0,0.0138197,"relating to the position in the sentence. The resulting parser only requires the vector representations of the top two items on the parser stack, which is, to the best of our knowledge, the smallest feature set ever published for Arc-Standard parsers to date, while still managing to achieve competitive results. 1 Introduction Neural network-based dependency parsers have typically relied on combination of raw features, as represented by their dense vector embeddings to represent features of a sentence as well as the parser state (Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Zhou et al., 2015). On the other hand, there has been substantial work on using innovative deep learning architectures to build more informative feature representations. One approach has been to model an input sentence using bi-directional Long Short-Term Memory Networks (bi-lstms) (Cross and Huang, 2016; Kiperwasser and Goldberg, 2016b). The result is a vector for each word that encodes both its information, and relevant information from other parts of the sentence. This approach enabled better results with fewer features than was possible before (Cross and Huang, 2016; Shi et al., 2017). Another approach has"
