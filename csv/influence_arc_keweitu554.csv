2020.aacl-main.12,Q17-1010,0,0.0164857,"d Manning (2018), and our two second-order approaches respectively. For all the approaches, we use the MST algorithm to guarantee treestructured output in testing. We use the concatenation of word embeddings, character-level embeddings and part-of-speech (POS) tag embeddings to represent words and additionally concatenate BERT embeddings for experiments with BERT. For a fair comparison with previous work, we use GloVe (Pennington et al., 2014) and BERTLarge-Uncased model for PTB, and structuredskipgram (Ling et al., 2015) and BERT-BaseChinese model for CTB. For UD, we use fastText embeddings (Bojanowski et al., 2017) and BERTBase-Multilingual-Cased model for different languages. We set the default iteration number for our approaches to 3 because we find no improvement on more or less iterations. For GNN1 , we rerun the code based on the official release of Ji et al. (2019). For Single1O, Local1O2 , Single2O3 , we implement these api,j L =λL(label) + (1 − λ)L(edge) ∗(edge) ∗(label) where yi is the head of word wi and yij is the label of edge wi → wj in the golden parse tree, λ is a hyper-parameter and 1(x) is an indicator function that returns 1 when x is true and 0 otherwise. 3 3.1 Experiments Setups Foll"
2020.aacl-main.12,Q15-1035,0,0.0129497,". 1 Introduction Graph-based dependency parsing is a popular approach to dependency parsing that scores parse components of a sentence and then finds the highest scoring tree through inference. First-order graphbased dependency parsing takes individual dependency edges as the components of a parse tree, while higher-order dependency parsing considers more complex components consisting of multiple edges. There exist both exact inference algorithms (Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) and approximate inference algorithms (McDonald and Pereira, 2006; Smith and Eisner, 2008; Gormley et al., 2015) to find the best parse tree. Recent work focused on neural network based graph dependency parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Cheng et al., 2016; Kuncoro et al., 2016; Ma and Hovy, 2017; Dozat and Manning, 2017). Dozat and Manning (2017) proposed a first-order graph-based neural dependency parsing approach with a simple headselection training objective. It uses a biaffine function to score dependency edges and has high efficiency and good performance. Subsequent work ∗ Kewei Tu is the corresponding author. 93 Proceedings of the 1st Conference of the Asia-Pacific Cha"
2020.aacl-main.12,D07-1101,0,0.0647345,"rder parsing over first-order parsing and observe that the usefulness of the head-selection structured constraint vanishes when using BERT embedding. 1 Introduction Graph-based dependency parsing is a popular approach to dependency parsing that scores parse components of a sentence and then finds the highest scoring tree through inference. First-order graphbased dependency parsing takes individual dependency edges as the components of a parse tree, while higher-order dependency parsing considers more complex components consisting of multiple edges. There exist both exact inference algorithms (Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) and approximate inference algorithms (McDonald and Pereira, 2006; Smith and Eisner, 2008; Gormley et al., 2015) to find the best parse tree. Recent work focused on neural network based graph dependency parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Cheng et al., 2016; Kuncoro et al., 2016; Ma and Hovy, 2017; Dozat and Manning, 2017). Dozat and Manning (2017) proposed a first-order graph-based neural dependency parsing approach with a simple headselection training objective. It uses a biaffine function to score dependency edges and has"
2020.aacl-main.12,P19-1237,0,0.0726546,"ddings to represent words and additionally concatenate BERT embeddings for experiments with BERT. For a fair comparison with previous work, we use GloVe (Pennington et al., 2014) and BERTLarge-Uncased model for PTB, and structuredskipgram (Ling et al., 2015) and BERT-BaseChinese model for CTB. For UD, we use fastText embeddings (Bojanowski et al., 2017) and BERTBase-Multilingual-Cased model for different languages. We set the default iteration number for our approaches to 3 because we find no improvement on more or less iterations. For GNN1 , we rerun the code based on the official release of Ji et al. (2019). For Single1O, Local1O2 , Single2O3 , we implement these api,j L =λL(label) + (1 − λ)L(edge) ∗(edge) ∗(label) where yi is the head of word wi and yij is the label of edge wi → wj in the golden parse tree, λ is a hyper-parameter and 1(x) is an indicator function that returns 1 when x is true and 0 otherwise. 3 3.1 Experiments Setups Following previous work (Dozat and Manning, 2017; Ma et al., 2018), we use PTB 3.0 (Marcus et al., 1993), CTB 5.1 (Xue et al., 2002) and 12 languages in Universal Dependencies (Nivre et al., 2018) (UD) 2.2 to evaluate our parser. Punctuation is ignored in all the e"
2020.aacl-main.12,D16-1238,0,0.0201845,"through inference. First-order graphbased dependency parsing takes individual dependency edges as the components of a parse tree, while higher-order dependency parsing considers more complex components consisting of multiple edges. There exist both exact inference algorithms (Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) and approximate inference algorithms (McDonald and Pereira, 2006; Smith and Eisner, 2008; Gormley et al., 2015) to find the best parse tree. Recent work focused on neural network based graph dependency parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Cheng et al., 2016; Kuncoro et al., 2016; Ma and Hovy, 2017; Dozat and Manning, 2017). Dozat and Manning (2017) proposed a first-order graph-based neural dependency parsing approach with a simple headselection training objective. It uses a biaffine function to score dependency edges and has high efficiency and good performance. Subsequent work ∗ Kewei Tu is the corresponding author. 93 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 93–99 c December 4 - 7, 2020. 2020 A"
2020.aacl-main.12,D18-1217,0,0.105698,"We use GNN, Local1O, Single1O, Local2O and Single2O to represent the approaches of Ji et al. (2019), Dozat and Manning (2017), Dozat 1 https://github.com/AntNLP/ gnn-dep-parsing 2 https://github.com/tdozat/Parser-v3 3 https://github.com/wangxinyu0922/ Second_Order_SDP 95 PTB UAS LAS Dozat and Manning (2017) 95.74 94.08 Ma et al. (2018)♠ 95.87 94.19 F&G (2019)♠ 96.04 94.43 GNN 95.87 94.15 Single1O 95.75 94.04 95.83 94.23 Local1O Single2O 95.86 94.19 95.98 94.34 Local2O Ji et al. (2019)† 95.97 94.31 96.14 94.49 Zhang et al. (2020)†‡ Local2O†‡ 96.12 94.47 +BERT Zhou and Zhao (2019)♣ 97.20 95.72 Clark et al. (2018) 96.60 95.00 Single1O 96.82 95.20 Local1O 96.86 95.32 Single2O 96.86 95.31 Local2O 96.91 95.34 CTB UAS LAS 89.30 88.23 90.59 89.29 90.78 89.50 90.53 89.28 90.59 89.28 90.75 89.55 90.81 89.57 - 92.73 92.47 92.78 92.55 6000 tokens and stopped the training early after 10,000 iterations without improvements on development sets. Different from previous approaches such as Dozat and Manning (2017) and Ji et al. (2019), we use Adam (Kingma and Ba, 2015) with a learning rate of 0.01 and anneal the learning rate by 0.85 for every 500 iterations without improvement on the development set for optimizatio"
2020.aacl-main.12,Q16-1023,0,0.0263866,"of a sentence and then finds the highest scoring tree through inference. First-order graphbased dependency parsing takes individual dependency edges as the components of a parse tree, while higher-order dependency parsing considers more complex components consisting of multiple edges. There exist both exact inference algorithms (Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) and approximate inference algorithms (McDonald and Pereira, 2006; Smith and Eisner, 2008; Gormley et al., 2015) to find the best parse tree. Recent work focused on neural network based graph dependency parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Cheng et al., 2016; Kuncoro et al., 2016; Ma and Hovy, 2017; Dozat and Manning, 2017). Dozat and Manning (2017) proposed a first-order graph-based neural dependency parsing approach with a simple headselection training objective. It uses a biaffine function to score dependency edges and has high efficiency and good performance. Subsequent work ∗ Kewei Tu is the corresponding author. 93 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,"
2020.aacl-main.12,N19-1423,0,0.013694,"vel second-order dependency parser. We empirically compare the two second-order approaches and the first-order baselines on English Penn Tree Bank 3.0 (PTB), Chinese Penn Tree Bank 5.1 (CTB) and datasets of 12 languages in Universal Dependencies (UD). We show that our approaches achieve state-of-the-art performance on both PTB and CTB and our approaches are significantly faster than recently proposed second-order parsers. We also make two interesting observations from our empirical study. First, it is a common belief that contextual word embeddings such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) already conveys sufficient high-order information that renders high-order parsing less useful, but we find that second-order decoding is still helpful even with strong contextual embeddings like BERT. Second, while Zhang et al. (2019) previously found that incoperating the head-selection constraint is helpful in first-order parsing, we find that with a better loss function design and hyper-parameter tunIn this paper, we propose second-order graphbased neural dependency parsing using message passing and end-to-end neural networks. We empirically show that our approaches match the accuracy of v"
2020.aacl-main.12,P10-1001,0,0.019093,"r first-order parsing and observe that the usefulness of the head-selection structured constraint vanishes when using BERT embedding. 1 Introduction Graph-based dependency parsing is a popular approach to dependency parsing that scores parse components of a sentence and then finds the highest scoring tree through inference. First-order graphbased dependency parsing takes individual dependency edges as the components of a parse tree, while higher-order dependency parsing considers more complex components consisting of multiple edges. There exist both exact inference algorithms (Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) and approximate inference algorithms (McDonald and Pereira, 2006; Smith and Eisner, 2008; Gormley et al., 2015) to find the best parse tree. Recent work focused on neural network based graph dependency parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Cheng et al., 2016; Kuncoro et al., 2016; Ma and Hovy, 2017; Dozat and Manning, 2017). Dozat and Manning (2017) proposed a first-order graph-based neural dependency parsing approach with a simple headselection training objective. It uses a biaffine function to score dependency edges and has high efficiency and go"
2020.aacl-main.12,D16-1180,0,0.0158475,"irst-order graphbased dependency parsing takes individual dependency edges as the components of a parse tree, while higher-order dependency parsing considers more complex components consisting of multiple edges. There exist both exact inference algorithms (Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) and approximate inference algorithms (McDonald and Pereira, 2006; Smith and Eisner, 2008; Gormley et al., 2015) to find the best parse tree. Recent work focused on neural network based graph dependency parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Cheng et al., 2016; Kuncoro et al., 2016; Ma and Hovy, 2017; Dozat and Manning, 2017). Dozat and Manning (2017) proposed a first-order graph-based neural dependency parsing approach with a simple headselection training objective. It uses a biaffine function to score dependency edges and has high efficiency and good performance. Subsequent work ∗ Kewei Tu is the corresponding author. 93 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 93–99 c December 4 - 7, 2020. 2020 Association for Computa"
2020.aacl-main.12,P18-2077,0,0.0281276,"Missing"
2020.aacl-main.12,N15-1142,0,0.0321515,"and Local2O in our experiment. i (label) L =− X 1(yj∗(edge) = i) log(P (yij∗(label) |w)) and Manning (2018), and our two second-order approaches respectively. For all the approaches, we use the MST algorithm to guarantee treestructured output in testing. We use the concatenation of word embeddings, character-level embeddings and part-of-speech (POS) tag embeddings to represent words and additionally concatenate BERT embeddings for experiments with BERT. For a fair comparison with previous work, we use GloVe (Pennington et al., 2014) and BERTLarge-Uncased model for PTB, and structuredskipgram (Ling et al., 2015) and BERT-BaseChinese model for CTB. For UD, we use fastText embeddings (Bojanowski et al., 2017) and BERTBase-Multilingual-Cased model for different languages. We set the default iteration number for our approaches to 3 because we find no improvement on more or less iterations. For GNN1 , we rerun the code based on the official release of Ji et al. (2019). For Single1O, Local1O2 , Single2O3 , we implement these api,j L =λL(label) + (1 − λ)L(edge) ∗(edge) ∗(label) where yi is the head of word wi and yij is the label of edge wi → wj in the golden parse tree, λ is a hyper-parameter and 1(x) is a"
2020.aacl-main.12,K17-3002,0,0.0299601,"Missing"
2020.aacl-main.12,I17-1007,0,0.0246991,"dependency parsing takes individual dependency edges as the components of a parse tree, while higher-order dependency parsing considers more complex components consisting of multiple edges. There exist both exact inference algorithms (Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) and approximate inference algorithms (McDonald and Pereira, 2006; Smith and Eisner, 2008; Gormley et al., 2015) to find the best parse tree. Recent work focused on neural network based graph dependency parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Cheng et al., 2016; Kuncoro et al., 2016; Ma and Hovy, 2017; Dozat and Manning, 2017). Dozat and Manning (2017) proposed a first-order graph-based neural dependency parsing approach with a simple headselection training objective. It uses a biaffine function to score dependency edges and has high efficiency and good performance. Subsequent work ∗ Kewei Tu is the corresponding author. 93 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 93–99 c December 4 - 7, 2020. 2020 Association for Computational Linguistics"
2020.aacl-main.12,N19-1076,0,0.0220119,"Missing"
2020.aacl-main.12,P18-1130,0,0.0122512,"ferent languages. We set the default iteration number for our approaches to 3 because we find no improvement on more or less iterations. For GNN1 , we rerun the code based on the official release of Ji et al. (2019). For Single1O, Local1O2 , Single2O3 , we implement these api,j L =λL(label) + (1 − λ)L(edge) ∗(edge) ∗(label) where yi is the head of word wi and yij is the label of edge wi → wj in the golden parse tree, λ is a hyper-parameter and 1(x) is an indicator function that returns 1 when x is true and 0 otherwise. 3 3.1 Experiments Setups Following previous work (Dozat and Manning, 2017; Ma et al., 2018), we use PTB 3.0 (Marcus et al., 1993), CTB 5.1 (Xue et al., 2002) and 12 languages in Universal Dependencies (Nivre et al., 2018) (UD) 2.2 to evaluate our parser. Punctuation is ignored in all the evaluations. We use the same treebanks and preprocessing as Ma et al. (2018) for PTB, CTB, and UD. For all the datasets, we remove sentences longer than 90 words in training sets for faster computation. We use GNN, Local1O, Single1O, Local2O and Single2O to represent the approaches of Ji et al. (2019), Dozat and Manning (2017), Dozat 1 https://github.com/AntNLP/ gnn-dep-parsing 2 https://github.com/"
2020.aacl-main.12,2020.acl-main.776,0,0.0389123,"rder parser based on Loopy Belief Propagation (LBP). Our work differs from theirs in that: 1) we use Mean Field Variational Inference (MFVI) instead of LBP, which Wang et al. (2019) found is faster and equally accurate in practice; 2) we add the head-selection constraint and do not include the global tree constraint that is shown to produce only slight improvement (Zhang et al., 2019) but would complicate our neural network design and implementation; 3) we employ modern neural encoders and achieve much better parsing accuracy. Our approaches are also closely related to the very recent work of Fonseca and Martins (2020). The main difference is that we use MFVI while they use the dual decomposition algorithm AD3 (Martins et al., 2011, 2013) for approximate inference. 2 Inspired by Wang et al. (2019), below we propose a Local second-order parsing approach. While the Single approach uses Boolean random variables to represent existence of possible dependency edges, our Local approach defines a discrete random variable for each word specifying its dependency head, thus enforcing the head-selection constraint and leading to different formulation of the message passing inference steps. 2.1 Scoring Following Dozat a"
2020.aacl-main.12,C12-2077,0,0.0251673,"nd observe that the usefulness of the head-selection structured constraint vanishes when using BERT embedding. 1 Introduction Graph-based dependency parsing is a popular approach to dependency parsing that scores parse components of a sentence and then finds the highest scoring tree through inference. First-order graphbased dependency parsing takes individual dependency edges as the components of a parse tree, while higher-order dependency parsing considers more complex components consisting of multiple edges. There exist both exact inference algorithms (Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) and approximate inference algorithms (McDonald and Pereira, 2006; Smith and Eisner, 2008; Gormley et al., 2015) to find the best parse tree. Recent work focused on neural network based graph dependency parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Cheng et al., 2016; Kuncoro et al., 2016; Ma and Hovy, 2017; Dozat and Manning, 2017). Dozat and Manning (2017) proposed a first-order graph-based neural dependency parsing approach with a simple headselection training objective. It uses a biaffine function to score dependency edges and has high efficiency and good performance. Subs"
2020.aacl-main.12,D08-1016,0,0.0615226,"hen using BERT embedding. 1 Introduction Graph-based dependency parsing is a popular approach to dependency parsing that scores parse components of a sentence and then finds the highest scoring tree through inference. First-order graphbased dependency parsing takes individual dependency edges as the components of a parse tree, while higher-order dependency parsing considers more complex components consisting of multiple edges. There exist both exact inference algorithms (Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) and approximate inference algorithms (McDonald and Pereira, 2006; Smith and Eisner, 2008; Gormley et al., 2015) to find the best parse tree. Recent work focused on neural network based graph dependency parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Cheng et al., 2016; Kuncoro et al., 2016; Ma and Hovy, 2017; Dozat and Manning, 2017). Dozat and Manning (2017) proposed a first-order graph-based neural dependency parsing approach with a simple headselection training objective. It uses a biaffine function to score dependency edges and has high efficiency and good performance. Subsequent work ∗ Kewei Tu is the corresponding author. 93 Proceedings of the 1st Conference"
2020.aacl-main.12,J93-2004,0,0.0696081,"lt iteration number for our approaches to 3 because we find no improvement on more or less iterations. For GNN1 , we rerun the code based on the official release of Ji et al. (2019). For Single1O, Local1O2 , Single2O3 , we implement these api,j L =λL(label) + (1 − λ)L(edge) ∗(edge) ∗(label) where yi is the head of word wi and yij is the label of edge wi → wj in the golden parse tree, λ is a hyper-parameter and 1(x) is an indicator function that returns 1 when x is true and 0 otherwise. 3 3.1 Experiments Setups Following previous work (Dozat and Manning, 2017; Ma et al., 2018), we use PTB 3.0 (Marcus et al., 1993), CTB 5.1 (Xue et al., 2002) and 12 languages in Universal Dependencies (Nivre et al., 2018) (UD) 2.2 to evaluate our parser. Punctuation is ignored in all the evaluations. We use the same treebanks and preprocessing as Ma et al. (2018) for PTB, CTB, and UD. For all the datasets, we remove sentences longer than 90 words in training sets for faster computation. We use GNN, Local1O, Single1O, Local2O and Single2O to represent the approaches of Ji et al. (2019), Dozat and Manning (2017), Dozat 1 https://github.com/AntNLP/ gnn-dep-parsing 2 https://github.com/tdozat/Parser-v3 3 https://github.com/"
2020.aacl-main.12,P13-2109,0,0.0532313,"Missing"
2020.aacl-main.12,P16-1218,0,0.0190772,"highest scoring tree through inference. First-order graphbased dependency parsing takes individual dependency edges as the components of a parse tree, while higher-order dependency parsing considers more complex components consisting of multiple edges. There exist both exact inference algorithms (Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) and approximate inference algorithms (McDonald and Pereira, 2006; Smith and Eisner, 2008; Gormley et al., 2015) to find the best parse tree. Recent work focused on neural network based graph dependency parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Cheng et al., 2016; Kuncoro et al., 2016; Ma and Hovy, 2017; Dozat and Manning, 2017). Dozat and Manning (2017) proposed a first-order graph-based neural dependency parsing approach with a simple headselection training objective. It uses a biaffine function to score dependency edges and has high efficiency and good performance. Subsequent work ∗ Kewei Tu is the corresponding author. 93 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 93–99 c December"
2020.aacl-main.12,P19-1454,1,0.878054,"nd Information Technology, Chinese Academy of Sciences University of Chinese Academy of Sciences {wangxy1,tukw}@shanghaitech.edu.cn Abstract introduced second-order inference into their parser. Ji et al. (2019) proposed a graph neural network that captures second-order information in token representations, which are then used for first-order parsing. Very recently, Zhang et al. (2020) proposed an efficient second-order tree CRF model for dependency parsing and achieved state-of-the-art performance. In this paper, we first show how a previously proposed second-order semantic dependency parser (Wang et al., 2019) can be applied to syntactic dependency parsing with simple modifications. The parser is an end-to-end neural network derived from message passing inference on a conditional random field that encodes the second-order parsing problem. We then propose an alternative conditional random field that incorporates the head-selection constraint of syntactic dependency parsing, and derive a novel second-order dependency parser. We empirically compare the two second-order approaches and the first-order baselines on English Penn Tree Bank 3.0 (PTB), Chinese Penn Tree Bank 5.1 (CTB) and datasets of 12 lang"
2020.aacl-main.12,D11-1022,0,0.0120528,"al Inference (MFVI) instead of LBP, which Wang et al. (2019) found is faster and equally accurate in practice; 2) we add the head-selection constraint and do not include the global tree constraint that is shown to produce only slight improvement (Zhang et al., 2019) but would complicate our neural network design and implementation; 3) we employ modern neural encoders and achieve much better parsing accuracy. Our approaches are also closely related to the very recent work of Fonseca and Martins (2020). The main difference is that we use MFVI while they use the dual decomposition algorithm AD3 (Martins et al., 2011, 2013) for approximate inference. 2 Inspired by Wang et al. (2019), below we propose a Local second-order parsing approach. While the Single approach uses Boolean random variables to represent existence of possible dependency edges, our Local approach defines a discrete random variable for each word specifying its dependency head, thus enforcing the head-selection constraint and leading to different formulation of the message passing inference steps. 2.1 Scoring Following Dozat and Manning (2017), we predict edge existence and edge labels separately. Suppose the input sentence is w = [w0 , w1"
2020.aacl-main.12,C02-1145,0,0.0950213,"roaches to 3 because we find no improvement on more or less iterations. For GNN1 , we rerun the code based on the official release of Ji et al. (2019). For Single1O, Local1O2 , Single2O3 , we implement these api,j L =λL(label) + (1 − λ)L(edge) ∗(edge) ∗(label) where yi is the head of word wi and yij is the label of edge wi → wj in the golden parse tree, λ is a hyper-parameter and 1(x) is an indicator function that returns 1 when x is true and 0 otherwise. 3 3.1 Experiments Setups Following previous work (Dozat and Manning, 2017; Ma et al., 2018), we use PTB 3.0 (Marcus et al., 1993), CTB 5.1 (Xue et al., 2002) and 12 languages in Universal Dependencies (Nivre et al., 2018) (UD) 2.2 to evaluate our parser. Punctuation is ignored in all the evaluations. We use the same treebanks and preprocessing as Ma et al. (2018) for PTB, CTB, and UD. For all the datasets, we remove sentences longer than 90 words in training sets for faster computation. We use GNN, Local1O, Single1O, Local2O and Single2O to represent the approaches of Ji et al. (2019), Dozat and Manning (2017), Dozat 1 https://github.com/AntNLP/ gnn-dep-parsing 2 https://github.com/tdozat/Parser-v3 3 https://github.com/wangxinyu0922/ Second_Order_"
2020.aacl-main.12,E06-1011,0,0.0460241,"ctured constraint vanishes when using BERT embedding. 1 Introduction Graph-based dependency parsing is a popular approach to dependency parsing that scores parse components of a sentence and then finds the highest scoring tree through inference. First-order graphbased dependency parsing takes individual dependency edges as the components of a parse tree, while higher-order dependency parsing considers more complex components consisting of multiple edges. There exist both exact inference algorithms (Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) and approximate inference algorithms (McDonald and Pereira, 2006; Smith and Eisner, 2008; Gormley et al., 2015) to find the best parse tree. Recent work focused on neural network based graph dependency parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Cheng et al., 2016; Kuncoro et al., 2016; Ma and Hovy, 2017; Dozat and Manning, 2017). Dozat and Manning (2017) proposed a first-order graph-based neural dependency parsing approach with a simple headselection training objective. It uses a biaffine function to score dependency edges and has high efficiency and good performance. Subsequent work ∗ Kewei Tu is the corresponding author. 93 Proceeding"
2020.aacl-main.12,2020.acl-main.302,0,0.277901,"assing and End-to-End Training Xinyu Wang and Kewei Tu∗ School of Information Science and Technology, ShanghaiTech University Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences University of Chinese Academy of Sciences {wangxy1,tukw}@shanghaitech.edu.cn Abstract introduced second-order inference into their parser. Ji et al. (2019) proposed a graph neural network that captures second-order information in token representations, which are then used for first-order parsing. Very recently, Zhang et al. (2020) proposed an efficient second-order tree CRF model for dependency parsing and achieved state-of-the-art performance. In this paper, we first show how a previously proposed second-order semantic dependency parser (Wang et al., 2019) can be applied to syntactic dependency parsing with simple modifications. The parser is an end-to-end neural network derived from message passing inference on a conditional random field that encodes the second-order parsing problem. We then propose an alternative conditional random field that incorporates the head-selection constraint of syntactic dependency parsing"
2020.aacl-main.12,H05-1066,0,0.318157,"Missing"
2020.aacl-main.12,P19-1562,0,0.299523,"endencies (UD). We show that our approaches achieve state-of-the-art performance on both PTB and CTB and our approaches are significantly faster than recently proposed second-order parsers. We also make two interesting observations from our empirical study. First, it is a common belief that contextual word embeddings such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) already conveys sufficient high-order information that renders high-order parsing less useful, but we find that second-order decoding is still helpful even with strong contextual embeddings like BERT. Second, while Zhang et al. (2019) previously found that incoperating the head-selection constraint is helpful in first-order parsing, we find that with a better loss function design and hyper-parameter tunIn this paper, we propose second-order graphbased neural dependency parsing using message passing and end-to-end neural networks. We empirically show that our approaches match the accuracy of very recent state-ofthe-art second-order graph-based neural dependency parsers and have significantly faster speed in both training and testing. We also empirically show the advantage of second-order parsing over first-order parsing and"
2020.aacl-main.12,D14-1162,0,0.0872038,"0.85 500 Mean/Stddev 0.0/1.0 0.0/0.25 Table 1: Hyper-parameter for Local1O, Single2O and Local2O in our experiment. i (label) L =− X 1(yj∗(edge) = i) log(P (yij∗(label) |w)) and Manning (2018), and our two second-order approaches respectively. For all the approaches, we use the MST algorithm to guarantee treestructured output in testing. We use the concatenation of word embeddings, character-level embeddings and part-of-speech (POS) tag embeddings to represent words and additionally concatenate BERT embeddings for experiments with BERT. For a fair comparison with previous work, we use GloVe (Pennington et al., 2014) and BERTLarge-Uncased model for PTB, and structuredskipgram (Ling et al., 2015) and BERT-BaseChinese model for CTB. For UD, we use fastText embeddings (Bojanowski et al., 2017) and BERTBase-Multilingual-Cased model for different languages. We set the default iteration number for our approaches to 3 because we find no improvement on more or less iterations. For GNN1 , we rerun the code based on the official release of Ji et al. (2019). For Single1O, Local1O2 , Single2O3 , we implement these api,j L =λL(label) + (1 − λ)L(edge) ∗(edge) ∗(label) where yi is the head of word wi and yij is the labe"
2020.aacl-main.12,P19-1230,0,0.0752922,"ining sets for faster computation. We use GNN, Local1O, Single1O, Local2O and Single2O to represent the approaches of Ji et al. (2019), Dozat and Manning (2017), Dozat 1 https://github.com/AntNLP/ gnn-dep-parsing 2 https://github.com/tdozat/Parser-v3 3 https://github.com/wangxinyu0922/ Second_Order_SDP 95 PTB UAS LAS Dozat and Manning (2017) 95.74 94.08 Ma et al. (2018)♠ 95.87 94.19 F&G (2019)♠ 96.04 94.43 GNN 95.87 94.15 Single1O 95.75 94.04 95.83 94.23 Local1O Single2O 95.86 94.19 95.98 94.34 Local2O Ji et al. (2019)† 95.97 94.31 96.14 94.49 Zhang et al. (2020)†‡ Local2O†‡ 96.12 94.47 +BERT Zhou and Zhao (2019)♣ 97.20 95.72 Clark et al. (2018) 96.60 95.00 Single1O 96.82 95.20 Local1O 96.86 95.32 Single2O 96.86 95.31 Local2O 96.91 95.34 CTB UAS LAS 89.30 88.23 90.59 89.29 90.78 89.50 90.53 89.28 90.59 89.28 90.75 89.55 90.81 89.57 - 92.73 92.47 92.78 92.55 6000 tokens and stopped the training early after 10,000 iterations without improvements on development sets. Different from previous approaches such as Dozat and Manning (2017) and Ji et al. (2019), we use Adam (Kingma and Ba, 2015) with a learning rate of 0.01 and anneal the learning rate by 0.85 for every 500 iterations without improvement on th"
2020.aacl-main.12,N18-1202,0,0.0190277,"ndency parsing, and derive a novel second-order dependency parser. We empirically compare the two second-order approaches and the first-order baselines on English Penn Tree Bank 3.0 (PTB), Chinese Penn Tree Bank 5.1 (CTB) and datasets of 12 languages in Universal Dependencies (UD). We show that our approaches achieve state-of-the-art performance on both PTB and CTB and our approaches are significantly faster than recently proposed second-order parsers. We also make two interesting observations from our empirical study. First, it is a common belief that contextual word embeddings such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) already conveys sufficient high-order information that renders high-order parsing less useful, but we find that second-order decoding is still helpful even with strong contextual embeddings like BERT. Second, while Zhang et al. (2019) previously found that incoperating the head-selection constraint is helpful in first-order parsing, we find that with a better loss function design and hyper-parameter tunIn this paper, we propose second-order graphbased neural dependency parsing using message passing and end-to-end neural networks. We empirically show that our app"
2020.aacl-main.12,K18-2016,0,0.0182472,"ocal2O based on this code. In speed comparison, we implement the second-order approaches based on an PyTorch implementation biaffine parser4 implemented by Zhang et al. (2020) for a fair speed comparison with their approach5 . Since we find that the accuracy of our approaches based on PyTorch implementation on PTB does not change, we only report scores based on Wang et al. (2019). 3.2 Results Hyper-parameters The hyper-parameters we used in our experiments is shown in Table 1. We tune the the hidden size (edge) for calculating sij (Unary Arc in the table) separately for PTB and CTB. Following Qi et al. (2018), we switch to AMSGrad (Reddi et al., 2018) after 5,000 iterations without improvement. We train models for 75,000 iterations with batch sizes of 4 https://github.com/yzhangcs/parser At the time we finished the paper, the official code for the second-order tree CRF parser have not release yet. We believe it is a fair comparison since we use the same settings and GPU as Zhang et al. (2020). 5 6 Note that Zhang et al. (2019) reports higher difference in accuracy between first-order Local and Single approaches. The discrepancy is most likely caused by our better designed loss function and tuned h"
2020.acl-main.304,Q17-1010,0,0.0288087,"dings are obtained by average pooling. We feed the token embeddings into the BiLSTM-CRF for decoding. The hidden size of the BiLSTM layer is 256 for the monolingual teacher models and 600 or 800 for the multilingual student model depending on the dataset as larger hidden size for the multilingual model results in better performance in our experiment. The settings of teacher and student models are as follows: • Monolingual Teachers: Each teacher is trained with a dataset of a specific language. We use M-BERT concatenated with languagespecific Flair (Akbik et al., 2018) embeddings and fastText (Bojanowski et al., 2017) word embeddings as token embeddings2 for all the monolingual teacher models. • Multilingual Student: The student model is trained with the datasets of all the languages combined. We only use M-BERT as token embeddings for the multilingual student model. Training For model training, the mini-batch size is set to 2000 tokens. We train all models with SGD optimizer with a learning rate of 0.1 and anneal the learning rate by 0.5 if there is no improvements on the development set for 10 epochs. For all models, we use a single NVIDIA Tesla V100 GPU for training including the student model. We tune"
2020.acl-main.304,P19-1595,0,0.0318413,"9: end for 10: 11: while S < S do 12: S = S + 1. ˆ do 13: for mini-batch (x, y, pˆ) sampled from D 14: Compute the KD loss LKD (x, pˆ). 15: Compute the golden target loss LNLL (x, y). 16: Compute the final loss L = λLKD + (1 − λ)LNLL . 17: Update θ: θ = θ - η ∗ ∂L/∂θ . 18: if λ − τ > 0 do 19: Update interpolation factor λ: λ = λ − τ 20: else 21: Update interpolation factor λ: λ = 0 22: end if 23: end while learns from the gold targets and pseudo targets in training by optimizing the following loss function: LALL = λLKD + (1 − λ)LNLL where λ decreases from 1 to 0 throughout training following Clark et al. (2019), LKD is one of the Eq. 5, 8, 9, 13 or an averaging of Eq. 9, 13. The overall distillation process is summarized in Algorithm 1. 4 4.1 • CoNLL NER: We collect the corpora of 4 languages from the CoNLL 2002 and 2003 shared task (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) • WikiAnn NER (Pan et al., 2017): The dataset contains silver standard NER tags that are annotated automatically on 282 languages that exist in Wikipedia. We select the data of 8 languages from different language families or from different language subgroups of IndoEuropean languages. We randomly choose 5000 sen"
2020.acl-main.304,N19-1383,0,0.0165646,"n be formulated as sequence labeling problems and these tasks can provide extra information to many downstream tasks and products such as searching engine, chat-bot and syntax parsing (Jurafsky and Martin, 2009). Most of the previ∗ Kewei Tu is the corresponding author. This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. ous work on sequence labeling focused on monolingual models, and the work on multilingual sequence labeling mainly focused on cross-lingual transfer learning to improve the performance of low-resource or zero-resource languages (Johnson et al., 2019; Huang et al., 2019a; Rahimi et al., 2019; Huang et al., 2019b; Keung et al., 2019), but their work still trains monolingual models. However, it would be very resource consuming considering if we train monolingual models for all the 7,000+ languages in the world. Besides, there are languages with limited labeled data that are required for training. Therefore it is beneficial to have a single unified multilingual sequence labeling model to handle multiple languages, while less attention is paid to the unified multilingual models due to the significant difference between different languages. Recently, Multilingual"
2020.acl-main.304,D19-1672,0,0.0149242,"n be formulated as sequence labeling problems and these tasks can provide extra information to many downstream tasks and products such as searching engine, chat-bot and syntax parsing (Jurafsky and Martin, 2009). Most of the previ∗ Kewei Tu is the corresponding author. This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. ous work on sequence labeling focused on monolingual models, and the work on multilingual sequence labeling mainly focused on cross-lingual transfer learning to improve the performance of low-resource or zero-resource languages (Johnson et al., 2019; Huang et al., 2019a; Rahimi et al., 2019; Huang et al., 2019b; Keung et al., 2019), but their work still trains monolingual models. However, it would be very resource consuming considering if we train monolingual models for all the 7,000+ languages in the world. Besides, there are languages with limited labeled data that are required for training. Therefore it is beneficial to have a single unified multilingual sequence labeling model to handle multiple languages, while less attention is paid to the unified multilingual models due to the significant difference between different languages. Recently, Multilingual"
2020.acl-main.304,D16-1139,0,0.550108,"model and then trains a weak student model through mimicking the output probabilities (Hinton et al., 2015; Lan et al., 2018; Mirzadeh et al., 2019) or hidden states (Romero 3317 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3317–3330 c July 5 - 10, 2020. 2020 Association for Computational Linguistics et al., 2014; Seunghyun Lee, 2019) of the teacher model. The student model can achieve an accuracy comparable to that of the teacher model and usually has a smaller model size through KD. Inspired by KD applied in neural machine translation (NMT) (Kim and Rush, 2016) and multilingual NMT (Tan et al., 2019), our approach contains a set of monolingual teacher models, one for each language, and a single multilingual student model. Both groups of models are based on BiLSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016), one of the state-of-the-art models in sequence labeling. In BiLSTM-CRF, the CRF layer models the relation between neighbouring labels which leads to better results than simply predicting each label separately based on the BiLSTM outputs. However, the CRF structure models the label sequence globally with the correlations between neighboring label"
2020.acl-main.304,D16-1180,0,0.191514,"Missing"
2020.acl-main.304,N18-1202,0,0.0486673,"Missing"
2020.acl-main.304,P19-1493,0,0.0592419,"Missing"
2020.acl-main.304,D18-1061,0,0.0394979,"Missing"
2020.acl-main.304,P19-1015,0,0.136206,"quence labeling problems and these tasks can provide extra information to many downstream tasks and products such as searching engine, chat-bot and syntax parsing (Jurafsky and Martin, 2009). Most of the previ∗ Kewei Tu is the corresponding author. This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. ous work on sequence labeling focused on monolingual models, and the work on multilingual sequence labeling mainly focused on cross-lingual transfer learning to improve the performance of low-resource or zero-resource languages (Johnson et al., 2019; Huang et al., 2019a; Rahimi et al., 2019; Huang et al., 2019b; Keung et al., 2019), but their work still trains monolingual models. However, it would be very resource consuming considering if we train monolingual models for all the 7,000+ languages in the world. Besides, there are languages with limited labeled data that are required for training. Therefore it is beneficial to have a single unified multilingual sequence labeling model to handle multiple languages, while less attention is paid to the unified multilingual models due to the significant difference between different languages. Recently, Multilingual BERT (M-BERT) (Devlin"
2020.acl-main.304,C18-1327,0,0.0130226,"and multilingual BiLSTM-Softmax model with token-level KD based on Eq. 4 as Softmax and Token for reference. Table 2, 3, and 4 show the effectiveness of our approach on 4 tasks over 25 datasets. In all the tables, we report scores averaged over 5 runs. Observation #0. BiLSTM-Softmax models perform inferior to BiLSTM-CRF models in most cases in the multilingual setting: The results show that the BiLSTM-CRF approach is stronger than the BiLSTM-Softmax approach on three of the four tasks, which are consistent with previous work on sequence labeling (Ma and Hovy, 2016; Reimers and Gurevych, 2017; Yang et al., 2018). The token-level KD approach performs almost the same as the BiLSTM-Softmax baseline in most of the tasks except the Aspect Extraction task. Observation #1. Monolingual teacher models outperform multilingual student models: This is probably because the monolingual teacher models are based on both multilingual embeddings M-BERT and strong monolingual embeddings (Flair/fastText). The monolingual embedding may provide additional information that is not available to the multilingual student models. Furthermore, note that the learning problem faced by a multilingual student model is much more diff"
2020.acl-main.304,W12-1908,0,\N,Missing
2020.acl-main.304,N09-1010,0,\N,Missing
2020.acl-main.304,W03-0419,0,\N,Missing
2020.acl-main.304,N16-1030,0,\N,Missing
2020.acl-main.304,P18-1129,0,\N,Missing
2020.acl-main.304,C18-1139,0,\N,Missing
2020.acl-main.304,W18-6125,0,\N,Missing
2020.acl-main.304,W02-2024,0,\N,Missing
2020.acl-main.304,N19-2023,0,\N,Missing
2020.acl-main.304,N19-1078,0,\N,Missing
2020.acl-main.304,N19-1423,0,\N,Missing
2020.acl-main.304,D19-1441,0,\N,Missing
2020.acl-main.304,D19-1374,0,\N,Missing
2020.acl-main.304,D19-1138,0,\N,Missing
2020.acl-main.333,W05-0909,0,0.63177,"ntributions. Wenjuan Han is the corresponding author. Wenjuan Han contributed to this work when at ShanghaiTech University. 1 https://github.com/alexzhou907/ dialogue_evaluation. § When various variations in the model and sets of hyper-parameters are needed, the labor-intensive human evaluation is deemed impracticable. This key drawback may hinder the research progress and render the human evaluation approach not scalable. Previous automatic evaluation metrics generally focus on the quality of the dialogue generation: context coherence and fluency. Word-overlap metrics (Papineni et al., 2002; Banerjee and Lavie, 2005; Lin, 2004) or ad-hoc classifiers (Tao et al., 2018; Ghazarian et al., 2019) are designed for measuring the quality. In open-domain dialogue, the relation between two utterances is more critical as shown in the first example of Table 1. Compared with the previous two approaches, a language model, trained on an enormous amount of text, can naturally capture coherence among both words and utterances. On the other hand, a good evaluation metric should not only measure the quality of gen3619 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3619–3629 c"
2020.acl-main.333,W12-1631,0,0.0318032,"mantic similarity between model response and reference response is low. The second generated response conflicts with its prior utterances. The italic text highlights the logical contradiction. Introduction Learning to communicate is a key capacity of intelligent agents. Research on enabling a machine to have meaningful and natural conversations with humans plays a fundamental role in developing artificial general intelligence, as can be seen in the formulation of Turing test (Turing, 1950). Recently open-domain or non-task-oriented dialogue systems have attracted a surge of research interest (Bessho et al., 2012; Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016, 2017; Ghazvininejad et al., 2018). Evaluating models of open-domain dialogue generation in an efficient manner poses a significant challenge in developing dialogue systems. The prevalent method of open-domain dialogue evaluation is human-based rating with a given rubric. ∗ Equal contributions. Wenjuan Han is the corresponding author. Wenjuan Han contributed to this work when at ShanghaiTech University. 1 https://github.com/alexzhou907/ dialogue_evaluation. § When various variations in the model and sets of h"
2020.acl-main.333,W19-2310,0,0.134761,"Missing"
2020.acl-main.333,N19-1169,0,0.153669,"hown in the first example of Table 1. Compared with the previous two approaches, a language model, trained on an enormous amount of text, can naturally capture coherence among both words and utterances. On the other hand, a good evaluation metric should not only measure the quality of gen3619 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3619–3629 c July 5 - 10, 2020. 2020 Association for Computational Linguistics eration, but also the diversity of generation, which is especially important for open-ended tasks like dialogue or story generation (Hashimoto et al., 2019). Some n-gram based metrics have been utilized to measure diversity (Mou et al., 2016; Serban et al., 2017). However, this metric might be improper for diversity evaluation since the generated utterances given various queries provided by the benchmark are generally diverse. In our experiments, we observe constantly high diversity in terms of human ratings and n-gram based entropy when evaluating the generated responses directly. In addition to the three aforementioned metrics, logical selfconsistency is also a key aspect of dialogue models (Zhang et al., 2018). An dialogue example with logical"
2020.acl-main.333,P18-1152,0,0.0368487,"semantic embedding. Context Coherence. One key component of dialogue response is its coherence to the query as explored in Tao et al. (2018) and Ghazvininejad et al. (2018). Prior work measures the coherence based on the Softmax score of a trained binary classifier. Here we explore an alternative approach based on language modeling (Bengio et al., 2003). A language model can naturally capture the coherence of the response to the query without resorting to an ad-hoc classifier. Language Fluency. Besides coherence, a good response should be fluent. Fluency is often measured by a language model (Holtzman et al., 2018; Xu et al., 2018). We define the response fluency score as negative perplexity of generated responses. Response Diversity. In addition to quality metrics, response diversity is also critical, especially for high conditional entropy tasks like dialogue or story generation (Hashimoto et al., 2019). Some n-gram based metric has been utilized to measure diversity. Mou et al. (2016) and Serban et al. (2017) compute unigram entropy across all generated utterances to measure the diversity. This metric might be improper for diversity since the generated utterances given various queries are generally"
2020.acl-main.333,P17-4012,0,0.0118942,"utterance from this agent. 4 Experiments 4.1 Dataset To facilitate comparison with prior work (Ghazarian et al., 2019), the DailyDialog dataset (Li et al., 2017) is adopted for the empirical analysis of our proposed metrics. This dataset contains 13,118 high-quality multi-turn dialogue dataset. The dialogue is split into a 42,000 / 3,700 / 3,900 traintest-validation partitions. 4.2 Response Generation A sequence-to-sequence (seq2seq) model with attention (Bahdanau et al., 2014) was trained with the train and validation partitions to generate dialogue responses. The implementation in OpenNMT (Klein et al., 2017) was used to train the model. The seq2seq consists of a 2-layer LSTM with 500 hidden units on both the encoder and decoder. The model was trained with SGD and learning rate of 1. To obtain responses on a wide spectrum of quality and diversity, we sample the data with top-k sampling where k = {1, 10, 100}. 4.3 Language Model Fine-tuning The base GPT-2 model with 12 layers was used to compute our metrics 2 . The GPT-2 model was fine-tuned on the training and validation data. In fine-tuning, the queries and responses were concatenated together as a single sentence to feed into GPT-2. The perplexi"
2020.acl-main.333,I17-1099,0,0.0268861,"adictory responses when responding to similar queries. We use WS and CTG to paraphrase the query and then calcu3622 Context of Conversation Of course. A two-week paid vacation a Speaker A: year, a five-day workweek. So, if I get a margin card, I could take a Speaker B: margin card for you to travel to a company as soon as possible. Human Score: 0.20 RUBER Score: 0.97 Our Score: 0.19 late the contradiction score of the current utterance and each prior utterance from this agent. 4 Experiments 4.1 Dataset To facilitate comparison with prior work (Ghazarian et al., 2019), the DailyDialog dataset (Li et al., 2017) is adopted for the empirical analysis of our proposed metrics. This dataset contains 13,118 high-quality multi-turn dialogue dataset. The dialogue is split into a 42,000 / 3,700 / 3,900 traintest-validation partitions. 4.2 Response Generation A sequence-to-sequence (seq2seq) model with attention (Bahdanau et al., 2014) was trained with the train and validation partitions to generate dialogue responses. The implementation in OpenNMT (Klein et al., 2017) was used to train the model. The seq2seq consists of a 2-layer LSTM with 500 hidden units on both the encoder and decoder. The model was train"
2020.acl-main.333,W04-1013,0,0.317571,"s the corresponding author. Wenjuan Han contributed to this work when at ShanghaiTech University. 1 https://github.com/alexzhou907/ dialogue_evaluation. § When various variations in the model and sets of hyper-parameters are needed, the labor-intensive human evaluation is deemed impracticable. This key drawback may hinder the research progress and render the human evaluation approach not scalable. Previous automatic evaluation metrics generally focus on the quality of the dialogue generation: context coherence and fluency. Word-overlap metrics (Papineni et al., 2002; Banerjee and Lavie, 2005; Lin, 2004) or ad-hoc classifiers (Tao et al., 2018; Ghazarian et al., 2019) are designed for measuring the quality. In open-domain dialogue, the relation between two utterances is more critical as shown in the first example of Table 1. Compared with the previous two approaches, a language model, trained on an enormous amount of text, can naturally capture coherence among both words and utterances. On the other hand, a good evaluation metric should not only measure the quality of gen3619 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3619–3629 c July 5 - 10"
2020.acl-main.333,D16-1230,0,0.0592276,"ecision and recall by calculating F-measure. These n-gram based metrics are well-suited for the generation tasks that are more source-determined or low conditional entropy such as translation, image captioning, and summarization. Some dialogue studies adopted these metrics to evaluate the quality of generated conversation responses (Ritter et al., 2011; Su et al., 2018; Sordoni et al., 2015). They nevertheless are not suitable for open-ended generations or high conditional entropy tasks like dialogue generation where a diverse range of generations is acceptable conditional on a query. Indeed, Liu et al. (2016) conducts extensive empirical studies on these metrics (e.g., BLEU, METEOR, and ROUGE) to test their effectiveness on evaluating dialogue generation and find limited relation between these automatic metrics and human judgments. The word-overlap metrics (e.g., BLEU) fail to capture the semantic similarity between model and reference responses. The following works leverage the distributed representation learned in neural network models to capture semantic similarity among context, model response, and reference response. Lowe et al. (2017) collect a dataset of human scores and train a hierarchica"
2020.acl-main.333,P17-1103,0,0.0618921,"generations is acceptable conditional on a query. Indeed, Liu et al. (2016) conducts extensive empirical studies on these metrics (e.g., BLEU, METEOR, and ROUGE) to test their effectiveness on evaluating dialogue generation and find limited relation between these automatic metrics and human judgments. The word-overlap metrics (e.g., BLEU) fail to capture the semantic similarity between model and reference responses. The following works leverage the distributed representation learned in neural network models to capture semantic similarity among context, model response, and reference response. Lowe et al. (2017) collect a dataset of human scores and train a hierarchical recurrent neural network (RNN) to predict human-like scores to input responses given the context, resulting in an automatic metric that has a medium level correlation with human judgments. Obtaining this metric however requires a large dataset of human-annotated scores, thus rendering this approach less flexible and extensible. Tao et al. (2018) proposes a referenced metric and unreferenced metric blended evaluation routine (RUBER) for open-domain dialogue systems. This blended metric is a combination of two metrics. A referenced metr"
2020.acl-main.333,C16-1316,0,0.289857,"model, trained on an enormous amount of text, can naturally capture coherence among both words and utterances. On the other hand, a good evaluation metric should not only measure the quality of gen3619 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3619–3629 c July 5 - 10, 2020. 2020 Association for Computational Linguistics eration, but also the diversity of generation, which is especially important for open-ended tasks like dialogue or story generation (Hashimoto et al., 2019). Some n-gram based metrics have been utilized to measure diversity (Mou et al., 2016; Serban et al., 2017). However, this metric might be improper for diversity evaluation since the generated utterances given various queries provided by the benchmark are generally diverse. In our experiments, we observe constantly high diversity in terms of human ratings and n-gram based entropy when evaluating the generated responses directly. In addition to the three aforementioned metrics, logical selfconsistency is also a key aspect of dialogue models (Zhang et al., 2018). An dialogue example with logical contradiction is displayed in the second example of Table 1. Welleck et al. (2019) m"
2020.acl-main.333,P02-1040,0,0.111816,"iven rubric. ∗ Equal contributions. Wenjuan Han is the corresponding author. Wenjuan Han contributed to this work when at ShanghaiTech University. 1 https://github.com/alexzhou907/ dialogue_evaluation. § When various variations in the model and sets of hyper-parameters are needed, the labor-intensive human evaluation is deemed impracticable. This key drawback may hinder the research progress and render the human evaluation approach not scalable. Previous automatic evaluation metrics generally focus on the quality of the dialogue generation: context coherence and fluency. Word-overlap metrics (Papineni et al., 2002; Banerjee and Lavie, 2005; Lin, 2004) or ad-hoc classifiers (Tao et al., 2018; Ghazarian et al., 2019) are designed for measuring the quality. In open-domain dialogue, the relation between two utterances is more critical as shown in the first example of Table 1. Compared with the previous two approaches, a language model, trained on an enormous amount of text, can naturally capture coherence among both words and utterances. On the other hand, a good evaluation metric should not only measure the quality of gen3619 Proceedings of the 58th Annual Meeting of the Association for Computational Ling"
2020.acl-main.333,D11-1054,0,0.0502906,"n various language generation tasks. For machine translation, BLEU (Papineni et al., 2002) computes n-gram precision, whereas METEOR (Banerjee and Lavie, 2005) takes into account both precision and recall. For summarization, ROUGE (Lin, 2004) also considers both precision and recall by calculating F-measure. These n-gram based metrics are well-suited for the generation tasks that are more source-determined or low conditional entropy such as translation, image captioning, and summarization. Some dialogue studies adopted these metrics to evaluate the quality of generated conversation responses (Ritter et al., 2011; Su et al., 2018; Sordoni et al., 2015). They nevertheless are not suitable for open-ended generations or high conditional entropy tasks like dialogue generation where a diverse range of generations is acceptable conditional on a query. Indeed, Liu et al. (2016) conducts extensive empirical studies on these metrics (e.g., BLEU, METEOR, and ROUGE) to test their effectiveness on evaluating dialogue generation and find limited relation between these automatic metrics and human judgments. The word-overlap metrics (e.g., BLEU) fail to capture the semantic similarity between model and reference res"
2020.acl-main.333,P15-1152,0,0.0252464,"d reference response is low. The second generated response conflicts with its prior utterances. The italic text highlights the logical contradiction. Introduction Learning to communicate is a key capacity of intelligent agents. Research on enabling a machine to have meaningful and natural conversations with humans plays a fundamental role in developing artificial general intelligence, as can be seen in the formulation of Turing test (Turing, 1950). Recently open-domain or non-task-oriented dialogue systems have attracted a surge of research interest (Bessho et al., 2012; Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016, 2017; Ghazvininejad et al., 2018). Evaluating models of open-domain dialogue generation in an efficient manner poses a significant challenge in developing dialogue systems. The prevalent method of open-domain dialogue evaluation is human-based rating with a given rubric. ∗ Equal contributions. Wenjuan Han is the corresponding author. Wenjuan Han contributed to this work when at ShanghaiTech University. 1 https://github.com/alexzhou907/ dialogue_evaluation. § When various variations in the model and sets of hyper-parameters are needed, the labor-inte"
2020.acl-main.333,N15-1020,0,0.0556557,"Missing"
2020.acl-main.333,W00-1308,0,0.069746,"Missing"
2020.acl-main.333,P19-1363,0,0.0355582,"rsity (Mou et al., 2016; Serban et al., 2017). However, this metric might be improper for diversity evaluation since the generated utterances given various queries provided by the benchmark are generally diverse. In our experiments, we observe constantly high diversity in terms of human ratings and n-gram based entropy when evaluating the generated responses directly. In addition to the three aforementioned metrics, logical selfconsistency is also a key aspect of dialogue models (Zhang et al., 2018). An dialogue example with logical contradiction is displayed in the second example of Table 1. Welleck et al. (2019) measured logical self-consistency by transferring each sentence into a rule-based triple, (category, relation, category), with the help of human annotators. We are nevertheless unaware of any reliable automatic measure of logical consistency in open-domain dialogue. In this work, we propose holistic metrics that evaluate distinctive aspects of generated dialogues. Specifically, we consider (1) context coherence of a dialogue: the meaningfulness of a response within the context of prior query, (2) language fluency of generated responses: the quality of phrasing relative to a human native speak"
2020.acl-main.333,N18-1101,0,0.13473,"another perspective, the entropy computed across all generated responses is essentially measuring the marginal entropy of the responses, while our actual interest is in the conditional entropy of the responses conditional on the queries. Logical Self-Consistency. Similar to diversity evaluation, current benchmarks are not suitable for evaluating logical self-consistency. The current dataset is well-formed making the system to generate a simple and nonredundant response, but unfortunately, there still exist logical contradictions as shown in Table 1. The natural language inference (NLI) task (Williams et al., 2018) aiming to check whether the sentence is entailed or contradicted by a previous sentence is highly related to logic evaluation on open-domain dialogues. 3 3.1 Metrics Context Coherence Language models, which predict the next token given previous tokens, naturally capture the coherence between sentences and particularly the dialogue query and response in our case. GPT-2 (Radford et al., 2019) is a large-scale pre-trained language model based on the transformer architecture (Vaswani et al., 2017). It is trained on a vast amount of diverse data and demonstrates impressive text generation capabili"
2020.acl-main.333,P18-1205,0,0.0370239,"dialogue or story generation (Hashimoto et al., 2019). Some n-gram based metrics have been utilized to measure diversity (Mou et al., 2016; Serban et al., 2017). However, this metric might be improper for diversity evaluation since the generated utterances given various queries provided by the benchmark are generally diverse. In our experiments, we observe constantly high diversity in terms of human ratings and n-gram based entropy when evaluating the generated responses directly. In addition to the three aforementioned metrics, logical selfconsistency is also a key aspect of dialogue models (Zhang et al., 2018). An dialogue example with logical contradiction is displayed in the second example of Table 1. Welleck et al. (2019) measured logical self-consistency by transferring each sentence into a rule-based triple, (category, relation, category), with the help of human annotators. We are nevertheless unaware of any reliable automatic measure of logical consistency in open-domain dialogue. In this work, we propose holistic metrics that evaluate distinctive aspects of generated dialogues. Specifically, we consider (1) context coherence of a dialogue: the meaningfulness of a response within the context"
2020.acl-main.607,P18-2077,0,0.261294,"within a sentence form a directed acyclic graph (DAG), distinguishing SDP from syntactic dependency parsing tasks, where dependencies are usually tree-structured. Extraction of such high-level structured semantic information potentially benefits downstream NLP tasks (Reddy et al., 2017; Schuster et al., 2017). Several supervised SDP models are proposed in the recent years by modifying syntactic dependency parsers. Their parsing mechanisms are either transition-based (Kanerva et al., 2015; Wang et al., ∗ Corresponding author. 2018) or graph-based (Martins and Almeida, 2014; Peng et al., 2017; Dozat and Manning, 2018; Wang et al., 2019). One limitation of supervised SDP is that labeled SDP data resources are limited in scale and diversity. Due to the rich relationships in SDP, the annotation of semantic dependency graphs is expensive and difficult, calling for professional linguists to design rules and highly skilled annotators to annotate sentences. This limitation becomes more severe with the rise of deep learning, because neural approaches are more data-hungry and susceptible to over-fitting when lacking training data. To alleviate this limitation, we investigate semi-supervised SDP capable of learning"
2020.acl-main.607,P09-1041,0,0.153996,"approaches are more data-hungry and susceptible to over-fitting when lacking training data. To alleviate this limitation, we investigate semi-supervised SDP capable of learning from both labeled and unlabeled data. While a lot of work has been done on supervised SDP, the research of unsupervised and semisupervised SDP is still lacking. Since parsing results of semantic dependencies are DAGs without the tree-shape restriction, most existing successful unsupervised (Klein and Manning, 2004; I. Spitkovsky et al., 2010; Jiang et al., 2016; Cai et al., 2017) and semi-supervised (Koo et al., 2008; Druck et al., 2009; Suzuki et al., 2009; Corro and Titov, 2019) learning models for syntactic dependency parsing cannot be applied to SDP directly and it would be non-trivial to extend these models for SDP. There also exist several unsupervised (Poon and Domingos, 2009; Titov and Klementiev, 2011) and semi-supervised (Das and Smith, 2011; Koˇcisk`y et al., 2016; Yin et al., 2018) methods for semantic parsing, but these models are designed for semantic representations different from dependency graphs, making their adaptation to SDP difficult. In this work, we propose an end-to-end neural semi-supervised model le"
2020.acl-main.607,W10-2902,0,0.0322493,"annotate sentences. This limitation becomes more severe with the rise of deep learning, because neural approaches are more data-hungry and susceptible to over-fitting when lacking training data. To alleviate this limitation, we investigate semi-supervised SDP capable of learning from both labeled and unlabeled data. While a lot of work has been done on supervised SDP, the research of unsupervised and semisupervised SDP is still lacking. Since parsing results of semantic dependencies are DAGs without the tree-shape restriction, most existing successful unsupervised (Klein and Manning, 2004; I. Spitkovsky et al., 2010; Jiang et al., 2016; Cai et al., 2017) and semi-supervised (Koo et al., 2008; Druck et al., 2009; Suzuki et al., 2009; Corro and Titov, 2019) learning models for syntactic dependency parsing cannot be applied to SDP directly and it would be non-trivial to extend these models for SDP. There also exist several unsupervised (Poon and Domingos, 2009; Titov and Klementiev, 2011) and semi-supervised (Das and Smith, 2011; Koˇcisk`y et al., 2016; Yin et al., 2018) methods for semantic parsing, but these models are designed for semantic representations different from dependency graphs, making their ad"
2020.acl-main.607,D16-1073,1,0.843835,"limitation becomes more severe with the rise of deep learning, because neural approaches are more data-hungry and susceptible to over-fitting when lacking training data. To alleviate this limitation, we investigate semi-supervised SDP capable of learning from both labeled and unlabeled data. While a lot of work has been done on supervised SDP, the research of unsupervised and semisupervised SDP is still lacking. Since parsing results of semantic dependencies are DAGs without the tree-shape restriction, most existing successful unsupervised (Klein and Manning, 2004; I. Spitkovsky et al., 2010; Jiang et al., 2016; Cai et al., 2017) and semi-supervised (Koo et al., 2008; Druck et al., 2009; Suzuki et al., 2009; Corro and Titov, 2019) learning models for syntactic dependency parsing cannot be applied to SDP directly and it would be non-trivial to extend these models for SDP. There also exist several unsupervised (Poon and Domingos, 2009; Titov and Klementiev, 2011) and semi-supervised (Das and Smith, 2011; Koˇcisk`y et al., 2016; Yin et al., 2018) methods for semantic parsing, but these models are designed for semantic representations different from dependency graphs, making their adaptation to SDP diff"
2020.acl-main.607,S15-2161,0,0.0217283,"more general, allowing a word to be either unattached or the argument of multiple predicates. The set of semantic dependencies within a sentence form a directed acyclic graph (DAG), distinguishing SDP from syntactic dependency parsing tasks, where dependencies are usually tree-structured. Extraction of such high-level structured semantic information potentially benefits downstream NLP tasks (Reddy et al., 2017; Schuster et al., 2017). Several supervised SDP models are proposed in the recent years by modifying syntactic dependency parsers. Their parsing mechanisms are either transition-based (Kanerva et al., 2015; Wang et al., ∗ Corresponding author. 2018) or graph-based (Martins and Almeida, 2014; Peng et al., 2017; Dozat and Manning, 2018; Wang et al., 2019). One limitation of supervised SDP is that labeled SDP data resources are limited in scale and diversity. Due to the rich relationships in SDP, the annotation of semantic dependency graphs is expensive and difficult, calling for professional linguists to design rules and highly skilled annotators to annotate sentences. This limitation becomes more severe with the rise of deep learning, because neural approaches are more data-hungry and susceptibl"
2020.acl-main.607,P04-1061,0,0.171362,"ighly skilled annotators to annotate sentences. This limitation becomes more severe with the rise of deep learning, because neural approaches are more data-hungry and susceptible to over-fitting when lacking training data. To alleviate this limitation, we investigate semi-supervised SDP capable of learning from both labeled and unlabeled data. While a lot of work has been done on supervised SDP, the research of unsupervised and semisupervised SDP is still lacking. Since parsing results of semantic dependencies are DAGs without the tree-shape restriction, most existing successful unsupervised (Klein and Manning, 2004; I. Spitkovsky et al., 2010; Jiang et al., 2016; Cai et al., 2017) and semi-supervised (Koo et al., 2008; Druck et al., 2009; Suzuki et al., 2009; Corro and Titov, 2019) learning models for syntactic dependency parsing cannot be applied to SDP directly and it would be non-trivial to extend these models for SDP. There also exist several unsupervised (Poon and Domingos, 2009; Titov and Klementiev, 2011) and semi-supervised (Das and Smith, 2011; Koˇcisk`y et al., 2016; Yin et al., 2018) methods for semantic parsing, but these models are designed for semantic representations different from depend"
2020.acl-main.607,D16-1116,0,0.0355521,"Missing"
2020.acl-main.607,P08-1068,0,0.435013,"ng, because neural approaches are more data-hungry and susceptible to over-fitting when lacking training data. To alleviate this limitation, we investigate semi-supervised SDP capable of learning from both labeled and unlabeled data. While a lot of work has been done on supervised SDP, the research of unsupervised and semisupervised SDP is still lacking. Since parsing results of semantic dependencies are DAGs without the tree-shape restriction, most existing successful unsupervised (Klein and Manning, 2004; I. Spitkovsky et al., 2010; Jiang et al., 2016; Cai et al., 2017) and semi-supervised (Koo et al., 2008; Druck et al., 2009; Suzuki et al., 2009; Corro and Titov, 2019) learning models for syntactic dependency parsing cannot be applied to SDP directly and it would be non-trivial to extend these models for SDP. There also exist several unsupervised (Poon and Domingos, 2009; Titov and Klementiev, 2011) and semi-supervised (Das and Smith, 2011; Koˇcisk`y et al., 2016; Yin et al., 2018) methods for semantic parsing, but these models are designed for semantic representations different from dependency graphs, making their adaptation to SDP difficult. In this work, we propose an end-to-end neural semi"
2020.acl-main.607,S14-2082,0,0.0735773,"Missing"
2020.acl-main.607,S14-2008,0,0.113879,"at predicts the latent parse graph of the input sentence. Our decoder is a generative neural model that reconstructs the input sentence conditioned on the latent parse graph. Our model is arc-factored and therefore parsing and learning are both tractable. Experiments show our model achieves significant and consistent improvement over the supervised baseline. 1 Introduction Semantic dependency parsing (SDP) is a task aiming at discovering sentence-internal linguistic information. The focus of SDP is the identification of predicate-argument relationships for all content words inside a sentence (Oepen et al., 2014, 2015). Compared with syntactic dependencies, semantic dependencies are more general, allowing a word to be either unattached or the argument of multiple predicates. The set of semantic dependencies within a sentence form a directed acyclic graph (DAG), distinguishing SDP from syntactic dependency parsing tasks, where dependencies are usually tree-structured. Extraction of such high-level structured semantic information potentially benefits downstream NLP tasks (Reddy et al., 2017; Schuster et al., 2017). Several supervised SDP models are proposed in the recent years by modifying syntactic de"
2020.acl-main.607,P17-1186,0,0.0222561,"mantic dependencies within a sentence form a directed acyclic graph (DAG), distinguishing SDP from syntactic dependency parsing tasks, where dependencies are usually tree-structured. Extraction of such high-level structured semantic information potentially benefits downstream NLP tasks (Reddy et al., 2017; Schuster et al., 2017). Several supervised SDP models are proposed in the recent years by modifying syntactic dependency parsers. Their parsing mechanisms are either transition-based (Kanerva et al., 2015; Wang et al., ∗ Corresponding author. 2018) or graph-based (Martins and Almeida, 2014; Peng et al., 2017; Dozat and Manning, 2018; Wang et al., 2019). One limitation of supervised SDP is that labeled SDP data resources are limited in scale and diversity. Due to the rich relationships in SDP, the annotation of semantic dependency graphs is expensive and difficult, calling for professional linguists to design rules and highly skilled annotators to annotate sentences. This limitation becomes more severe with the rise of deep learning, because neural approaches are more data-hungry and susceptible to over-fitting when lacking training data. To alleviate this limitation, we investigate semi-supervise"
2020.acl-main.607,D14-1162,0,0.0888256,"Missing"
2020.acl-main.607,D09-1001,0,0.157629,"one on supervised SDP, the research of unsupervised and semisupervised SDP is still lacking. Since parsing results of semantic dependencies are DAGs without the tree-shape restriction, most existing successful unsupervised (Klein and Manning, 2004; I. Spitkovsky et al., 2010; Jiang et al., 2016; Cai et al., 2017) and semi-supervised (Koo et al., 2008; Druck et al., 2009; Suzuki et al., 2009; Corro and Titov, 2019) learning models for syntactic dependency parsing cannot be applied to SDP directly and it would be non-trivial to extend these models for SDP. There also exist several unsupervised (Poon and Domingos, 2009; Titov and Klementiev, 2011) and semi-supervised (Das and Smith, 2011; Koˇcisk`y et al., 2016; Yin et al., 2018) methods for semantic parsing, but these models are designed for semantic representations different from dependency graphs, making their adaptation to SDP difficult. In this work, we propose an end-to-end neural semi-supervised model leveraging both labeled and unlabeled data to learn a dependency graph parser. Our model employs the framework of Conditional 6795 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6795–6805 c July 5 - 10, 20"
2020.acl-main.607,D17-1009,0,0.0262052,"e focus of SDP is the identification of predicate-argument relationships for all content words inside a sentence (Oepen et al., 2014, 2015). Compared with syntactic dependencies, semantic dependencies are more general, allowing a word to be either unattached or the argument of multiple predicates. The set of semantic dependencies within a sentence form a directed acyclic graph (DAG), distinguishing SDP from syntactic dependency parsing tasks, where dependencies are usually tree-structured. Extraction of such high-level structured semantic information potentially benefits downstream NLP tasks (Reddy et al., 2017; Schuster et al., 2017). Several supervised SDP models are proposed in the recent years by modifying syntactic dependency parsers. Their parsing mechanisms are either transition-based (Kanerva et al., 2015; Wang et al., ∗ Corresponding author. 2018) or graph-based (Martins and Almeida, 2014; Peng et al., 2017; Dozat and Manning, 2018; Wang et al., 2019). One limitation of supervised SDP is that labeled SDP data resources are limited in scale and diversity. Due to the rich relationships in SDP, the annotation of semantic dependency graphs is expensive and difficult, calling for professional li"
2020.acl-main.607,D09-1058,0,0.23041,"data-hungry and susceptible to over-fitting when lacking training data. To alleviate this limitation, we investigate semi-supervised SDP capable of learning from both labeled and unlabeled data. While a lot of work has been done on supervised SDP, the research of unsupervised and semisupervised SDP is still lacking. Since parsing results of semantic dependencies are DAGs without the tree-shape restriction, most existing successful unsupervised (Klein and Manning, 2004; I. Spitkovsky et al., 2010; Jiang et al., 2016; Cai et al., 2017) and semi-supervised (Koo et al., 2008; Druck et al., 2009; Suzuki et al., 2009; Corro and Titov, 2019) learning models for syntactic dependency parsing cannot be applied to SDP directly and it would be non-trivial to extend these models for SDP. There also exist several unsupervised (Poon and Domingos, 2009; Titov and Klementiev, 2011) and semi-supervised (Das and Smith, 2011; Koˇcisk`y et al., 2016; Yin et al., 2018) methods for semantic parsing, but these models are designed for semantic representations different from dependency graphs, making their adaptation to SDP difficult. In this work, we propose an end-to-end neural semi-supervised model leveraging both labeled"
2020.acl-main.607,P06-1124,0,0.0768568,"antic dependency parsing. Existing unsupervised and semi-supervised approaches to semantic parsing focused on semantic representations different from dependency graphs, e.g., general-purpose logic forms (Sondheimer and Nebel, 1986) and formal meaning representations (Bordes et al., 2012). Poon and Domingos (2009) presented the first unsupervised semantic parser to transform dependency trees into quasi-logical forms with Markov logic. Following this work, Titov and Klementiev (2011) proposed a non-parametric Bayesian model for unsupervised semantic parsing using hierarchical PitmanYor process (Teh, 2006). Das and Smith (2011) described a semi-supervised approach to framesemantic parsing. Koˇcisk`y et al. (2016) proposed a semi-supervised semantic parsing approach making use of unpaired logical forms with sentence being unobserved. Recently, Yin et al. (2018) proposed a variational autoencoding model for semisupervised semantic parsing of tree-structured semantic representations. Take Yin et al. (2018) for example. To extend their approach for SDP, one needs to design a different transition system for their encoder for graph parsing and design a graph linearization method for their sequence-to"
2020.acl-main.607,P11-1145,0,0.0311581,"Missing"
2020.acl-main.607,P19-1454,1,0.853605,"directed acyclic graph (DAG), distinguishing SDP from syntactic dependency parsing tasks, where dependencies are usually tree-structured. Extraction of such high-level structured semantic information potentially benefits downstream NLP tasks (Reddy et al., 2017; Schuster et al., 2017). Several supervised SDP models are proposed in the recent years by modifying syntactic dependency parsers. Their parsing mechanisms are either transition-based (Kanerva et al., 2015; Wang et al., ∗ Corresponding author. 2018) or graph-based (Martins and Almeida, 2014; Peng et al., 2017; Dozat and Manning, 2018; Wang et al., 2019). One limitation of supervised SDP is that labeled SDP data resources are limited in scale and diversity. Due to the rich relationships in SDP, the annotation of semantic dependency graphs is expensive and difficult, calling for professional linguists to design rules and highly skilled annotators to annotate sentences. This limitation becomes more severe with the rise of deep learning, because neural approaches are more data-hungry and susceptible to over-fitting when lacking training data. To alleviate this limitation, we investigate semi-supervised SDP capable of learning from both labeled a"
2020.acl-main.607,P18-1070,0,0.093205,"emantic dependencies are DAGs without the tree-shape restriction, most existing successful unsupervised (Klein and Manning, 2004; I. Spitkovsky et al., 2010; Jiang et al., 2016; Cai et al., 2017) and semi-supervised (Koo et al., 2008; Druck et al., 2009; Suzuki et al., 2009; Corro and Titov, 2019) learning models for syntactic dependency parsing cannot be applied to SDP directly and it would be non-trivial to extend these models for SDP. There also exist several unsupervised (Poon and Domingos, 2009; Titov and Klementiev, 2011) and semi-supervised (Das and Smith, 2011; Koˇcisk`y et al., 2016; Yin et al., 2018) methods for semantic parsing, but these models are designed for semantic representations different from dependency graphs, making their adaptation to SDP difficult. In this work, we propose an end-to-end neural semi-supervised model leveraging both labeled and unlabeled data to learn a dependency graph parser. Our model employs the framework of Conditional 6795 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6795–6805 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Random Field Autoencoder (Ammar et al., 2014), modeling the co"
2020.acl-main.607,D17-1179,1,0.813522,"the best of our knowledge, is dominated by tree-structured parsing (Koo et al., 2008; Druck et al., 2009; Suzuki et al., 2009). Recently, Corro and Titov (2019) introduced an approximate inference method with a Variational Autoencoder (Kingma et al., 2014) for semi-supervised syntactic dependency parsing. Our decoder is inspired by their work, but differs from theirs in that our decoder handles parse graphs and is arc-factored. Cai et al. (2017) used the framework of CRF Autoencoder (Ammar et al., 2014) to perform unsupervised syntactic dependency parsing. The same framework has been used by Zhang et al. (2017) for semi-supervised sequence labeling. Our work also adopts the CRF Autoencoder framework, but with both the encoder and the decoder redesigned for semantic dependency parsing. Existing unsupervised and semi-supervised approaches to semantic parsing focused on semantic representations different from dependency graphs, e.g., general-purpose logic forms (Sondheimer and Nebel, 1986) and formal meaning representations (Bordes et al., 2012). Poon and Domingos (2009) presented the first unsupervised semantic parser to transform dependency trees into quasi-logical forms with Markov logic. Following"
2020.coling-main.224,abeille-etal-2000-building,0,0.678588,"Missing"
2020.coling-main.224,D17-1171,1,0.928288,"y of labelled data in training dependency parsers (Sagae and Tsujii, 2007; Chen et al., 2009; Suzuki et al., 2011; Li et al., 2014). In previous work, the autoencoder framework is a prevalent approach for the utilization of unlabelled data (Ammar et al., 2014; Kingma and Welling, 2014). When this framework is applied to dependency parsing, training sentences are reconstructed from a decoder conditioned on dependency trees predicted by an encoder. Concretely, the autoencoder approaches to dependency parsing are mainly divided into two categories: the Conditional Random Field (CRF) Autoencoder (Cai et al., 2017) and the Variational Autoencoder (VAE) (Corro and Titov, 2019; Li et al., 2019). The CRF autoencoder predicts the dependency structure with the encoder and tries to reconstruct the input sentence based on the predicted structure. The variational autoencoder assumes the decoder is a generative model which generates the observed sentence from a group of latent variables containing information of the dependency tree, while the encoder tries to infer the posterior of the latent variables from the observed sentence. The tree structure constraint of the dependency parsing brings challenges to approa"
2020.coling-main.224,D18-1020,0,0.0204257,"entence content information. Then they develop a sophisticated sampling method to the approximately marginalize the structural latent variables and employ the Gumbel-Softmax trick (Jang et al., 2017) to facilitate backpropagation in the training process of the structural variables while the continuous variables are treated with traditional VAE procedures. In the area of transition-based dependency parsing and semantic parsing, the REINFORCE algorithm is introduced to VAE-based models for the marginalization of structural latent variables (Yin et al., 2018; Li et al., 2019). On the other hand, Chen et al. (2018), which apply VAE in structure related semi-supervised sequence 2486 y z <latexit sha1_base64=&quot;mdPScjrxRFpG5Qd976Gn7i1Adcg=&quot;&gt;AAACzXicjVHLTsJAFD3UF+ILdemmkZi4IgVNdEl0405M5BGBmGkZYEJfaacmiLj1B9zqbxn/QP/CO2NJVGJ0mrZnzr3nzNx77dAVsbSs14wxN7+wuJRdzq2srq1v5De36nGQRA6vOYEbRE2bxdwVPq9JIV3eDCPOPNvlDXt4quKNGx7FIvAv5SjkHY/1fdETDpNEXbU9Jgd2b3w7uc4XrKKllzkLSikoIF3VIP+CNroI4CCBBw4fkrALhpieFkqwEBLXwZi4iJDQcY4JcqRNKItTBiN2SN8+7Vop69NeecZa7dApLr0RKU3skSagvIiwOs3U8UQ7K/Y377H2VHcb0d9OvTxiJQbE/qWbZv5Xp2qR6OFY1yCoplAzqjondUl0V9TNzS9VSXIIiVO4S/GIsKOV0z6bWhPr2lVvmY6/6UzFqr2T5iZ4V7ekAZd+jnMW1MvF0kGxfHFYqJyko85iB7vYp3k"
2020.coling-main.224,D18-1217,0,0.015722,", a generative model adapting the Expectation-Maximization (EM) algorithm for its parameter optimization. Limited by strong contextfree assumption, DMV and its variants fail to capture useful contextual information in the sentence when scoring dependency parses. There are attempts made to incorporate discriminative information with DMV to remedy this problem (Han et al., 2019), leading to models similar to autoencoders. Another line of research is to incorporate traditional machine learning methods for unlabelled data utilization, such as self-training and tri-training (McClosky et al., 2006; Clark et al., 2018; Søgaard and Rishøj, 2010). A third line of research is to utilize the autoencoder framework, as discussed in Section 1. Previous work has already broadly applied the autoencoder framework in many NLP tasks other than dependency parsing such as Part-Of-Speech (POS) tagging (Zhang et al., 2017a) and sentence generation (Guu et al., 2018). Among them, VAE has been proved to be a useful tool in modelling problems with latent representations. However, compared with the CRF autoencoder, it is more difficult to use VAE in tasks involving latent structures. The main reason is that VAE requires margi"
2020.coling-main.224,C96-1058,0,0.376482,"o reconstruct the input sentence based on the predicted structure. The variational autoencoder assumes the decoder is a generative model which generates the observed sentence from a group of latent variables containing information of the dependency tree, while the encoder tries to infer the posterior of the latent variables from the observed sentence. The tree structure constraint of the dependency parsing brings challenges to approaches of both categories. For the CRF autoencoder, the encoder predicts the dependency tree structure using dynamic programming with the time complexity of O(n3 ) (Eisner, 1996), which is quite time-consuming in practice. For the VAE, the learning objective function contains an expectation over the posterior of the parse ∗ Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 2485 Proceedings of the 28th International Conference on Computational Linguistics, pages 2485–2496 Barcelona, Spain (Online), December 8-13, 2020 tree modeled by the encoder and sophisticated sampling techniques have to be applied to approximate it in previous work. Another probl"
2020.coling-main.224,Q18-1031,0,0.0181692,"remedy this problem (Han et al., 2019), leading to models similar to autoencoders. Another line of research is to incorporate traditional machine learning methods for unlabelled data utilization, such as self-training and tri-training (McClosky et al., 2006; Clark et al., 2018; Søgaard and Rishøj, 2010). A third line of research is to utilize the autoencoder framework, as discussed in Section 1. Previous work has already broadly applied the autoencoder framework in many NLP tasks other than dependency parsing such as Part-Of-Speech (POS) tagging (Zhang et al., 2017a) and sentence generation (Guu et al., 2018). Among them, VAE has been proved to be a useful tool in modelling problems with latent representations. However, compared with the CRF autoencoder, it is more difficult to use VAE in tasks involving latent structures. The main reason is that VAE requires marginalizing all latent variables (approximated by Monte Carlo sampling in implementation), which is intractable when both continuous and structural latent variables are present. Corro and Titov (2019) alleviate the problem by first dividing the latent variables into two parts: the discrete ones containing structural information and the cont"
2020.coling-main.224,P19-1526,1,0.843942,"d training data have been widely studied. Most of such approaches(Naseem et al., 2010; Tu and Honavar, 2012; Jiang et al., 2016; Noji et al., 2016) are extended from Dependency Model with Valence (DMV) proposed by Klein and Manning (2004), a generative model adapting the Expectation-Maximization (EM) algorithm for its parameter optimization. Limited by strong contextfree assumption, DMV and its variants fail to capture useful contextual information in the sentence when scoring dependency parses. There are attempts made to incorporate discriminative information with DMV to remedy this problem (Han et al., 2019), leading to models similar to autoencoders. Another line of research is to incorporate traditional machine learning methods for unlabelled data utilization, such as self-training and tri-training (McClosky et al., 2006; Clark et al., 2018; Søgaard and Rishøj, 2010). A third line of research is to utilize the autoencoder framework, as discussed in Section 1. Previous work has already broadly applied the autoencoder framework in many NLP tasks other than dependency parsing such as Part-Of-Speech (POS) tagging (Zhang et al., 2017a) and sentence generation (Guu et al., 2018). Among them, VAE has"
2020.coling-main.224,D16-1073,1,0.849092,"ation of each child word. We evaluate our model on datasets of several languages and the experiment results demonstrate that our model is effective in utilizing unlabelled data and achieves better parsing accuracy and faster speed than previous work. 2 Related Work Dependency parsing is a classic research topic in the Natural Language Processing (NLP) community. Because of the difficulty in obtaining dependency annotations, approaches to dependency parsing with scarce or even no labelled training data have been widely studied. Most of such approaches(Naseem et al., 2010; Tu and Honavar, 2012; Jiang et al., 2016; Noji et al., 2016) are extended from Dependency Model with Valence (DMV) proposed by Klein and Manning (2004), a generative model adapting the Expectation-Maximization (EM) algorithm for its parameter optimization. Limited by strong contextfree assumption, DMV and its variants fail to capture useful contextual information in the sentence when scoring dependency parses. There are attempts made to incorporate discriminative information with DMV to remedy this problem (Han et al., 2019), leading to models similar to autoencoders. Another line of research is to incorporate traditional machine le"
2020.coling-main.224,Q16-1023,0,0.0263241,"structures. Both the baselines and our model are evaluated after being fine-tuned on the development set for each treebank to ensure that the differences in results do not depend on the hyperparameter settings. In Table 3, we also compare our model with the method proposed in Corro and Titov (2019) denoted as C&T. Our model is not directly comparable with C&T because our encoder is stronger than theirs in terms of supervised parsing accuracy. Therefore, we report the evaluation results after weakening our encoder by removing the POS input and applying weaker scoring functions (the one used in Kiperwasser and Goldberg (2016) instead of Dozat and Manning (2017)). English C&T-Sup 88.79 Ours-Sup (weakened) 88.58 C&T 89.50 Ours-Semi(weakened) 89.67 French 84.09 84.05 84.69 84.94 Table 3: The UAS results of our semi-supervised model compared with the method of Corro and Titov (2019). ”-Sup” stands for the encoder of the model being used as a supervised dependency parser trained on labelled data only . “weakened” means our encoder is deliberately weakened to make it comparable with that of C&T. The best result for each column is shown in bold. 1 Our code is available at https://github.com/mikufan/SemiVariationalParser."
2020.coling-main.224,P04-1061,0,0.0420651,"ts demonstrate that our model is effective in utilizing unlabelled data and achieves better parsing accuracy and faster speed than previous work. 2 Related Work Dependency parsing is a classic research topic in the Natural Language Processing (NLP) community. Because of the difficulty in obtaining dependency annotations, approaches to dependency parsing with scarce or even no labelled training data have been widely studied. Most of such approaches(Naseem et al., 2010; Tu and Honavar, 2012; Jiang et al., 2016; Noji et al., 2016) are extended from Dependency Model with Valence (DMV) proposed by Klein and Manning (2004), a generative model adapting the Expectation-Maximization (EM) algorithm for its parameter optimization. Limited by strong contextfree assumption, DMV and its variants fail to capture useful contextual information in the sentence when scoring dependency parses. There are attempts made to incorporate discriminative information with DMV to remedy this problem (Han et al., 2019), leading to models similar to autoencoders. Another line of research is to incorporate traditional machine learning methods for unlabelled data utilization, such as self-training and tri-training (McClosky et al., 2006;"
2020.coling-main.224,P14-1043,0,0.0169508,"Dependency parsing is the task of finding syntactic dependency relations between words in sentences (K¨ubler et al., 2009). For each sentence, the dependency arcs between words are constrained to form a tree structure. A main bottleneck for learning a practical dependency parser is the lack of adequate training corpora as labelling raw text with dependency trees is both labourious and time costly. Semisupervised dependency parsing utilizes unlabelled data to compensate the scarcity of labelled data in training dependency parsers (Sagae and Tsujii, 2007; Chen et al., 2009; Suzuki et al., 2011; Li et al., 2014). In previous work, the autoencoder framework is a prevalent approach for the utilization of unlabelled data (Ammar et al., 2014; Kingma and Welling, 2014). When this framework is applied to dependency parsing, training sentences are reconstructed from a decoder conditioned on dependency trees predicted by an encoder. Concretely, the autoencoder approaches to dependency parsing are mainly divided into two categories: the Conditional Random Field (CRF) Autoencoder (Cai et al., 2017) and the Variational Autoencoder (VAE) (Corro and Titov, 2019; Li et al., 2019). The CRF autoencoder predicts the"
2020.coling-main.224,J93-2004,0,0.0720311,"cting the most probable tree with commonly used Maximum Spanning Tree (MST) decoding algorithms such as Eisner’s algorithm based on the score computed for each dependency arc. The decoder part does not participate in the prediction. 5 Evaluation Settings We evaluate the parsing performance of our model on datasets across 7 languages: English, French, German, Italian, Spanish, Swedish and Hindi. To make our evaluation comparable with that of Corro and Titov (2019), for English and French we choose corpora from Stanford Dependency conversion (De Marneffe and Manning, 2008) of the Penn Treebank (Marcus et al., 1993) and the French Treebank distributed for the SPMRL 2013 shared task (Abeill´e et al., 2000). The corpora for the rest five languages are all from the Universal Dependencies (UD) v2.0 (Zeman et al., 2017). In Table 1 we list the number of sentences in training, validation and test sets. English French German Italian Swedish Hindi Spanish Training 39832 14759 14118 12838 4303 13304 14187 Development 1700 1235 799 564 504 1659 1400 Test 2416 2541 977 482 1219 1684 426 Table 1: The number of sentences in treebanks used for our evaluation. The split of training, development and test sets follow the"
2020.coling-main.224,N06-1020,0,0.0925671,"lein and Manning (2004), a generative model adapting the Expectation-Maximization (EM) algorithm for its parameter optimization. Limited by strong contextfree assumption, DMV and its variants fail to capture useful contextual information in the sentence when scoring dependency parses. There are attempts made to incorporate discriminative information with DMV to remedy this problem (Han et al., 2019), leading to models similar to autoencoders. Another line of research is to incorporate traditional machine learning methods for unlabelled data utilization, such as self-training and tri-training (McClosky et al., 2006; Clark et al., 2018; Søgaard and Rishøj, 2010). A third line of research is to utilize the autoencoder framework, as discussed in Section 1. Previous work has already broadly applied the autoencoder framework in many NLP tasks other than dependency parsing such as Part-Of-Speech (POS) tagging (Zhang et al., 2017a) and sentence generation (Guu et al., 2018). Among them, VAE has been proved to be a useful tool in modelling problems with latent representations. However, compared with the CRF autoencoder, it is more difficult to use VAE in tasks involving latent structures. The main reason is tha"
2020.coling-main.224,D10-1120,0,0.0147826,"both left and right to influence the generation of each child word. We evaluate our model on datasets of several languages and the experiment results demonstrate that our model is effective in utilizing unlabelled data and achieves better parsing accuracy and faster speed than previous work. 2 Related Work Dependency parsing is a classic research topic in the Natural Language Processing (NLP) community. Because of the difficulty in obtaining dependency annotations, approaches to dependency parsing with scarce or even no labelled training data have been widely studied. Most of such approaches(Naseem et al., 2010; Tu and Honavar, 2012; Jiang et al., 2016; Noji et al., 2016) are extended from Dependency Model with Valence (DMV) proposed by Klein and Manning (2004), a generative model adapting the Expectation-Maximization (EM) algorithm for its parameter optimization. Limited by strong contextfree assumption, DMV and its variants fail to capture useful contextual information in the sentence when scoring dependency parses. There are attempts made to incorporate discriminative information with DMV to remedy this problem (Han et al., 2019), leading to models similar to autoencoders. Another line of researc"
2020.coling-main.224,D16-1004,0,0.0161488,"word. We evaluate our model on datasets of several languages and the experiment results demonstrate that our model is effective in utilizing unlabelled data and achieves better parsing accuracy and faster speed than previous work. 2 Related Work Dependency parsing is a classic research topic in the Natural Language Processing (NLP) community. Because of the difficulty in obtaining dependency annotations, approaches to dependency parsing with scarce or even no labelled training data have been widely studied. Most of such approaches(Naseem et al., 2010; Tu and Honavar, 2012; Jiang et al., 2016; Noji et al., 2016) are extended from Dependency Model with Valence (DMV) proposed by Klein and Manning (2004), a generative model adapting the Expectation-Maximization (EM) algorithm for its parameter optimization. Limited by strong contextfree assumption, DMV and its variants fail to capture useful contextual information in the sentence when scoring dependency parses. There are attempts made to incorporate discriminative information with DMV to remedy this problem (Han et al., 2019), leading to models similar to autoencoders. Another line of research is to incorporate traditional machine learning methods for u"
2020.coling-main.224,D14-1162,0,0.0835777,"n unlabelled set with the ratio of 1:9 and choose a sentence as labelled one if its index modulo 10 equals zero. The dimensions for the word embeddings and POS tag embeddings are set to 100 and 25. The dimensions in LSTMs and MLPs are uniformly set to 200 except for the layer before word generation, whose dimension is set to 100 in order to fit the word embedding size. The word embeddings used in the encoder part and the decoder part share the same initialization. Pre-trained word embeddings are used 2490 for all the datasets. For English, we use the 100 dimensions version of GLOVE embedding (Pennington et al., 2014). For all the other languages, the embeddings released for the 2017 CoNLL Shared Task on Universal Dependency Parsing are used. The network parameters are optimized by Adam (Kingma and Ba, 2015) with the setting of learning rate 0.002, β1 = 0.9, and β2 = 0.9. The hyperparameter α varies for different datasets: α = 0.2 for English and French, α = 0.9 for Spanish and α = 0.8 for the other corpora. The number of epochs taken to train the parser until convergence also varies in different datasets, but none of them is above 150 epochs. All the hyperparameters are tuned on the development set for ea"
2020.coling-main.224,D07-1111,0,0.0760953,"s approaches in both parsing accuracy and speed. 1 Introduction Dependency parsing is the task of finding syntactic dependency relations between words in sentences (K¨ubler et al., 2009). For each sentence, the dependency arcs between words are constrained to form a tree structure. A main bottleneck for learning a practical dependency parser is the lack of adequate training corpora as labelling raw text with dependency trees is both labourious and time costly. Semisupervised dependency parsing utilizes unlabelled data to compensate the scarcity of labelled data in training dependency parsers (Sagae and Tsujii, 2007; Chen et al., 2009; Suzuki et al., 2011; Li et al., 2014). In previous work, the autoencoder framework is a prevalent approach for the utilization of unlabelled data (Ammar et al., 2014; Kingma and Welling, 2014). When this framework is applied to dependency parsing, training sentences are reconstructed from a decoder conditioned on dependency trees predicted by an encoder. Concretely, the autoencoder approaches to dependency parsing are mainly divided into two categories: the Conditional Random Field (CRF) Autoencoder (Cai et al., 2017) and the Variational Autoencoder (VAE) (Corro and Titov,"
2020.coling-main.224,C10-1120,0,0.0353726,"adapting the Expectation-Maximization (EM) algorithm for its parameter optimization. Limited by strong contextfree assumption, DMV and its variants fail to capture useful contextual information in the sentence when scoring dependency parses. There are attempts made to incorporate discriminative information with DMV to remedy this problem (Han et al., 2019), leading to models similar to autoencoders. Another line of research is to incorporate traditional machine learning methods for unlabelled data utilization, such as self-training and tri-training (McClosky et al., 2006; Clark et al., 2018; Søgaard and Rishøj, 2010). A third line of research is to utilize the autoencoder framework, as discussed in Section 1. Previous work has already broadly applied the autoencoder framework in many NLP tasks other than dependency parsing such as Part-Of-Speech (POS) tagging (Zhang et al., 2017a) and sentence generation (Guu et al., 2018). Among them, VAE has been proved to be a useful tool in modelling problems with latent representations. However, compared with the CRF autoencoder, it is more difficult to use VAE in tasks involving latent structures. The main reason is that VAE requires marginalizing all latent variabl"
2020.coling-main.224,P11-2112,0,0.0286474,"peed. 1 Introduction Dependency parsing is the task of finding syntactic dependency relations between words in sentences (K¨ubler et al., 2009). For each sentence, the dependency arcs between words are constrained to form a tree structure. A main bottleneck for learning a practical dependency parser is the lack of adequate training corpora as labelling raw text with dependency trees is both labourious and time costly. Semisupervised dependency parsing utilizes unlabelled data to compensate the scarcity of labelled data in training dependency parsers (Sagae and Tsujii, 2007; Chen et al., 2009; Suzuki et al., 2011; Li et al., 2014). In previous work, the autoencoder framework is a prevalent approach for the utilization of unlabelled data (Ammar et al., 2014; Kingma and Welling, 2014). When this framework is applied to dependency parsing, training sentences are reconstructed from a decoder conditioned on dependency trees predicted by an encoder. Concretely, the autoencoder approaches to dependency parsing are mainly divided into two categories: the Conditional Random Field (CRF) Autoencoder (Cai et al., 2017) and the Variational Autoencoder (VAE) (Corro and Titov, 2019; Li et al., 2019). The CRF autoenc"
2020.coling-main.224,D12-1121,1,0.79239,"to influence the generation of each child word. We evaluate our model on datasets of several languages and the experiment results demonstrate that our model is effective in utilizing unlabelled data and achieves better parsing accuracy and faster speed than previous work. 2 Related Work Dependency parsing is a classic research topic in the Natural Language Processing (NLP) community. Because of the difficulty in obtaining dependency annotations, approaches to dependency parsing with scarce or even no labelled training data have been widely studied. Most of such approaches(Naseem et al., 2010; Tu and Honavar, 2012; Jiang et al., 2016; Noji et al., 2016) are extended from Dependency Model with Valence (DMV) proposed by Klein and Manning (2004), a generative model adapting the Expectation-Maximization (EM) algorithm for its parameter optimization. Limited by strong contextfree assumption, DMV and its variants fail to capture useful contextual information in the sentence when scoring dependency parses. There are attempts made to incorporate discriminative information with DMV to remedy this problem (Han et al., 2019), leading to models similar to autoencoders. Another line of research is to incorporate tr"
2020.coling-main.224,Q18-1019,0,0.0254364,"n Eq. 1 which ignores the gold parse tree. The unlabelled loss is solely ELBO since gold parse trees are unknown. We rewrite the 2489 loss function as the sum of three terms: Lθ,φ (D) = −α −α X x∈L X (x,y∗ )∈L qφ (y∗ |x) Eφ,θ (x) − (1 − α) X x∈U Eφ,θ (x) (3) where E denotes the ELBO formulation of Eq 1. The training process to minimize L is fully end-to-end with the reparameterization trick used to facilitate backpropagation of ELBO as mentioned earlier. Directly training autoencoders unsupervisedly may cause the learned latent structures to diverge from the linguistically correct structures (Williams et al., 2018). Therefore, in each training epoch, we start with optimizing the parsing loss on the labelled data to pretrain the encoder and then optimize the complete loss function on both labelled and unlabelled data. Note that the tree constraint over y is not enforced in training. Our definition of y only enforces the head selection constraint that each token has exactly one head. On the other hand, we enforce the tree constraint during the validation and the test phases when the encoder of our model works as a dependency parser, predicting the most probable tree with commonly used Maximum Spanning Tre"
2020.coling-main.224,P18-1070,0,0.0250824,"ctural information and the continuous ones containing sentence content information. Then they develop a sophisticated sampling method to the approximately marginalize the structural latent variables and employ the Gumbel-Softmax trick (Jang et al., 2017) to facilitate backpropagation in the training process of the structural variables while the continuous variables are treated with traditional VAE procedures. In the area of transition-based dependency parsing and semantic parsing, the REINFORCE algorithm is introduced to VAE-based models for the marginalization of structural latent variables (Yin et al., 2018; Li et al., 2019). On the other hand, Chen et al. (2018), which apply VAE in structure related semi-supervised sequence 2486 y z <latexit sha1_base64=&quot;mdPScjrxRFpG5Qd976Gn7i1Adcg=&quot;&gt;AAACzXicjVHLTsJAFD3UF+ILdemmkZi4IgVNdEl0405M5BGBmGkZYEJfaacmiLj1B9zqbxn/QP/CO2NJVGJ0mrZnzr3nzNx77dAVsbSs14wxN7+wuJRdzq2srq1v5De36nGQRA6vOYEbRE2bxdwVPq9JIV3eDCPOPNvlDXt4quKNGx7FIvAv5SjkHY/1fdETDpNEXbU9Jgd2b3w7uc4XrKKllzkLSikoIF3VIP+CNroI4CCBBw4fkrALhpieFkqwEBLXwZi4iJDQcY4JcqRNKItTBiN2SN8+7Vop69NeecZa7dApLr0RKU3skSagvIiwOs3U8UQ7K/Y377H2VHcb0d9OvTxiJQbE/qWbZv5Xp2qR6OFY1yCoplAzqjondUl0V9TNzS9VSXIIiVO4S/GIsKOV0z6bWhPr2l"
2020.coling-main.224,D17-1179,1,0.9234,"rporate discriminative information with DMV to remedy this problem (Han et al., 2019), leading to models similar to autoencoders. Another line of research is to incorporate traditional machine learning methods for unlabelled data utilization, such as self-training and tri-training (McClosky et al., 2006; Clark et al., 2018; Søgaard and Rishøj, 2010). A third line of research is to utilize the autoencoder framework, as discussed in Section 1. Previous work has already broadly applied the autoencoder framework in many NLP tasks other than dependency parsing such as Part-Of-Speech (POS) tagging (Zhang et al., 2017a) and sentence generation (Guu et al., 2018). Among them, VAE has been proved to be a useful tool in modelling problems with latent representations. However, compared with the CRF autoencoder, it is more difficult to use VAE in tasks involving latent structures. The main reason is that VAE requires marginalizing all latent variables (approximated by Monte Carlo sampling in implementation), which is intractable when both continuous and structural latent variables are present. Corro and Titov (2019) alleviate the problem by first dividing the latent variables into two parts: the discrete ones c"
2020.coling-main.224,E17-1063,0,0.139697,"rporate discriminative information with DMV to remedy this problem (Han et al., 2019), leading to models similar to autoencoders. Another line of research is to incorporate traditional machine learning methods for unlabelled data utilization, such as self-training and tri-training (McClosky et al., 2006; Clark et al., 2018; Søgaard and Rishøj, 2010). A third line of research is to utilize the autoencoder framework, as discussed in Section 1. Previous work has already broadly applied the autoencoder framework in many NLP tasks other than dependency parsing such as Part-Of-Speech (POS) tagging (Zhang et al., 2017a) and sentence generation (Guu et al., 2018). Among them, VAE has been proved to be a useful tool in modelling problems with latent representations. However, compared with the CRF autoencoder, it is more difficult to use VAE in tasks involving latent structures. The main reason is that VAE requires marginalizing all latent variables (approximated by Monte Carlo sampling in implementation), which is intractable when both continuous and structural latent variables are present. Corro and Titov (2019) alleviate the problem by first dividing the latent variables into two parts: the discrete ones c"
2020.coling-main.224,P19-1562,0,0.017682,"ehPHwAeJuetA==</latexit&gt; Figure 1: The graphical model representation. The observed variables are shaded. M is the number of sentences in the dataset. labeling tasks, avoid the marginalization of the structural variables by completely ignoring the structural constraints in model learning. Their success suggests the possibility of trying a similar solution for dependency parsing. In fact, in fully supervised parsing, Zhang et al. (2017b) and Dozat and Manning (2017) have shown that it is possible to relax the tree structure constraint during the training of dependency parsers. Further studies (Zhang et al., 2019) demonstrate that dropping the tree constraint only causes minor impact to the parsing accuracy. 3 Model Inspired by the previous work mentioned in section 2, we propose our semi-supervised dependency parsing model based on the VAE framework. Following the classic VAE setting, our model assumes that all the observed data are generated from a generative model based on a set of latent variables. The generative model forms the decoder part of VAE while the encoder part tries to infer the posterior of the latent variables. Formally, for an observed sentence represented by a sequence of tokens x ="
2020.coling-main.227,P10-1131,0,0.00963561,"ohnson (2016) use two large corpora containing more than 700k sentences. Mareˇcek and Straka (2013) utilize a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar. Han et al. (2017) use a subset of the BLLIP corpus that contains around 180k sentences. With the advancement of computing power and deep neural models, we expect to see more future work on training with big data. 4.5 Unsupervised Multilingual Parsing To tackle the lack of supervision in unsupervised dependency parsing, some previous work considers learning models of multiple languages simultaneously (Berg-Kirkpatrick and Klein, 2010; Liu et al., 2013; Jiang et al., 2019; Han et al., 2019b). Ideally, these models can learn from each other by identifying shared syntactic behaviors of different languages, especially those in the same language family. For example, Berg-Kirkpatrick and Klein (2010) propose to utilize the similarity of different languages defined by a phylogenetic tree and learn several dependency parsers jointly. Han et al. (2019b) propose to learn a unified multilingual parser with language embeddings as input. Jiang et al. (2019) propose to guide the learning process of unsupervised dependency parser from t"
2020.coling-main.227,Q13-1007,0,0.0138753,"e to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013). Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity. Berg-Kirkpatrick et al. (2010) use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV. 3.1.2 Inference Given a model parameter"
2020.coling-main.227,D10-1117,0,0.198748,"of the child tokens already generated from a head token. Headden III et al. (2009) propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013). Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity. Berg-Kirkpatrick et al. (2010) use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically l"
2020.coling-main.227,D17-1171,1,0.910925,"s is typically employed as the learning objective function for autoencoder models. For a training dataset including N sentences X = {x1 , x2 , ..., xN }, the objective function is as follows: L(Θ) = N X log P(ˆ x(i) |x(i) ; Θ) (5) i=1 where Θ is the model parameter and x ˆ(i) is a copy of x(i) representing the reconstructed sentence1 . In some cases, there is an additional regularization term (e.g., L1) of Θ. 1 In Han et al. (2019a), x is the word sequence, while x ˆ is the POS tag sequence of the same sentence. 2526 The first autoencoder model for unsupervised dependency parsing, proposed by Cai et al. (2017), is based on the conditional random field autoencoder framework (CRFAE). The encoder is a first-order graph-based discriminative dependency parser mapping an input sentence to the space of dependency trees. The decoder independently generates each token of the reconstructed sentence conditioned on the head of the token specified by the dependency tree. Both the encoder and the decoder are arc-factored, meaning that the encoding and decoding probabilities can be factorized by dependency arcs. Coordinate descent is applied to minimize the reconstruction loss and alternately updates the encoder"
2020.coling-main.227,N09-1009,0,0.056112,"Missing"
2020.coling-main.227,N19-1423,0,0.0363274,"nd Smith (2012) 64.3 Tu and Honavar (2012) 71.4 Bisk and Hockenmaier (2012) 71.5 Spitkovsky et al. (2013) 72.0 Jiang et al. (2016) 72.5 Han et al. (2017) 75.1 He et al. (2018)* 60.2 Discriminative Approaches Daum´e III (2009) Le and Zuidema (2015) † 73.2 Cai et al. (2017) 71.7 Li et al. (2019) 54.7 Han et al. (2019a) 75.6 59.1 53.1 57.0 53.3 64.4 57.6 59.5 47.9 45.4 65.8 55.7 37.8 61.4 Table 2: Reported directed dependency accuracies on section 23 of the WSJ corpus, evaluated on sentences of length ≤ 10 and all lengths. *: without gold POS tags. †: with more training data in addition to WSJ. (Devlin et al., 2019) are even more informative, capturing contextual information. However, word embeddings have not been widely used in unsupervised dependency parsing. One concern is that word embeddings are too informative and may make unsupervised models more prone to overfitting. One exception is He et al. (2018), who propose to use invertible neural projections to map word embeddings into a latent space that is more amenable to unsupervised parsing. 4.4 Big Data Although unsupervised parsing does not require syntactically annotated training corpora and can theoretically use almost unlimited raw texts for tra"
2020.coling-main.227,K15-1012,0,0.0224171,"ependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets. Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015; McDonald et al., 2011; Ma and Xia, 2014; Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either. Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because it has to induce not only edges but al"
2020.coling-main.227,N16-1024,0,0.0310519,"follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema. Three unsupervised dependency parsing models were proposed in recent years based on variational autoencoders (shown in Table 1). There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure. Recurrent Neural Network Grammars (RNNG) (Dyer et al., 2016) is a transition-based constituent parser, with a discriminative and a generative variant. Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing. Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence. The probability of each operation is calculated by a neural network. Li et al. (2019) modify RNNG for dependency parsing and use discriminative RNNG and generative RNN"
2020.coling-main.227,P10-2036,0,0.050584,"Missing"
2020.coling-main.227,N12-1069,0,0.0533256,"Missing"
2020.coling-main.227,P15-1133,0,0.0159582,"sampling algorithm. The variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution. It also specifies a Gaussian prior over the intermediate continuous vector. 2527 3.2.3 Other Discriminative Approaches Apart from the approaches based on autoencoder and variational autoencoder, there are also a few other discriminative approaches based on discriminative clustering (Grave and Elhadad, 2015), self-training (Le and Zuidema, 2015), or searching (Daum´e III, 2009). Because of space limit, below we only introduce the approach based on discriminative clustering called Convex MST (Grave and Elhadad, 2015). Convex MST employs a first-order graph-based discriminative parser. It searches for the parses of all the training sentences and learns the parser simultaneously, with a learning objective that the searched parses are close to the predicted parses by the parser. In other words, the parses should be easily predictable by the parser. The objective function can be relaxed to become conv"
2020.coling-main.227,D17-1176,1,0.909602,"tive approaches all make use of neural networks (Li et al., 2019; Corro and Titov, 2018). 4.3 Lexicalization In the most common setting of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a; He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010), and Han et al. (2017) use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007), Pate and Johnson (2016), and Spitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. For instance, Han et al. (2017) use neural networks to predict dependency probabilities that are automatically smoothed. In p"
2020.coling-main.227,P19-1526,1,0.643102,"algorithm. Lateen EM (Spitkovsky et al., 2011c) repeatedly alternates between EM and hard-EM, which is also found to produce better results than both EM and hard-EM. Approaches with more complicated objectives often require more advanced learning algorithms, but many of the algorithms can still be seen as extensions of the EM algorithm that revise either the Estep (e.g., to update Q(z) based on posterior regularization terms) or the M-step (e.g., to optimize the posterior probability that incorporates parameter priors). 2525 Autoencoder Variational Autoencoder CRFAE (Cai et al., 2017) D-NDMV (Han et al., 2019a) Deterministic Variant (Li et al., 2019) D-NDMV (Han et al., 2019a) Variational Variant (Corro and Titov, 2018) Intermediate Representation Z Encoder Decoder P (z|x) P (ˆ x|z) S P (s|x) P (z, x ˆ|s) Z P (z|x) P (z, x) S P (s|x) P (z, x|s) Z P (z|x) P (x|z) Table 1: Major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing. Z: dependency tree. S: continuous sentence representation. x ˆ is a copy of x representing the reconstructed sentence. z is the dependency tree. s is the continuous representation of sentence x. In addition to the EM algorithm,"
2020.coling-main.227,D19-1576,1,0.272781,"algorithm. Lateen EM (Spitkovsky et al., 2011c) repeatedly alternates between EM and hard-EM, which is also found to produce better results than both EM and hard-EM. Approaches with more complicated objectives often require more advanced learning algorithms, but many of the algorithms can still be seen as extensions of the EM algorithm that revise either the Estep (e.g., to update Q(z) based on posterior regularization terms) or the M-step (e.g., to optimize the posterior probability that incorporates parameter priors). 2525 Autoencoder Variational Autoencoder CRFAE (Cai et al., 2017) D-NDMV (Han et al., 2019a) Deterministic Variant (Li et al., 2019) D-NDMV (Han et al., 2019a) Variational Variant (Corro and Titov, 2018) Intermediate Representation Z Encoder Decoder P (z|x) P (ˆ x|z) S P (s|x) P (z, x ˆ|s) Z P (z|x) P (z, x) S P (s|x) P (z, x|s) Z P (z|x) P (x|z) Table 1: Major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing. Z: dependency tree. S: continuous sentence representation. x ˆ is a copy of x representing the reconstructed sentence. z is the dependency tree. s is the continuous representation of sentence x. In addition to the EM algorithm,"
2020.coling-main.227,D18-1160,0,0.676031,"the neural network in order to compute sentence-specific rule probabilities. Compared with generative approaches, it is more natural for discriminative approaches to use neural networks to score dependencies or parsing actions, so recent discriminative approaches all make use of neural networks (Li et al., 2019; Corro and Titov, 2018). 4.3 Lexicalization In the most common setting of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a; He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010), and Han et al. (2017) use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007), Pate and Johnson (2016), and Spitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much large"
2020.coling-main.227,P19-1311,0,0.0174769,"dency parsing can serve as the inspiration for studies of other unsupervised tasks, especially unsupervised structured prediction tasks. A recent example is Nishida and Nakayama (2020), who study unsupervised discourse parsing (inducing discourse structures for a given text) by borrowing techniques from unsupervised parsing such as Viterbi EM and heuristically designed initialization. Unsupervised dependency parsing techniques can also be used as building blocks for transfer learning of parsers. Some of the approaches discussed in this paper have already been applied to cross-lingual parsing (He et al., 2019; Li and Tu, 2020), and more such endeavors are expected in the future. 6.3 Interpretability One prominent problem of deep neural networks is that they act as black boxes and are generally not interpretable. How to improve the interpretability of neural networks is a research topic that gains much attention recently. For natural language texts, their linguistic structures reveal important information of the texts and at the same time can be easily understood by human. It is therefore an interesting direction to integrate techniques of unsupervised parsing into various neural models of NLP task"
2020.coling-main.227,N09-1012,0,0.110825,"Missing"
2020.coling-main.227,D16-1073,1,0.963124,"g, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013). Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity. Berg-Kirkpatrick et al. (2010) use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV. 3.1.2 Inference Given a model parameterized by Θ and a sentence x, the model predicts the parse z∗ with the highest probability. z∗ = arg max P (x, z; Θ) z∈Z(x) (1) where Z(x) is the set of all valid dependency trees of the sentence x. Due to the independence assumptions made by generative models, the inference problem can be efficiently solved exactly in most cases. For example, chart parsing can be used for DMV. 2524 3.1.3 Learning Objective Log marginal likelihood is typically employed as the ob"
2020.coling-main.227,D17-1177,1,0.84753,"unction can be relaxed to become convex and then can be optimized exactly. 3.2.4 Pros and Cons Discriminative models are capable of accessing global features from the whole input sentence and are typically more expressive than generative models. On the other hand, discriminative approaches are often more complicated and do not admit tractable exact inference. 4 4.1 Recent Trends Combined Approaches Generative approaches and discriminative approaches have different pros and cons. Therefore, a natural idea is to combine the strengths of the two types of approaches to achieve better performance. Jiang et al. (2017) propose to jointly train two state-of-the-art models of unsupervised dependency parsing, the generative LC-DMV (Noji et al., 2016) and the discriminative Convex MST, with the dual decomposition technique that encourages the two models to gradually influence each other during training. 4.2 Neural Parameterization Traditional generative approaches either directly learn or use manually-designed features to compute dependency rule probabilities. Following the recent rise of deep learning in the field of NLP, Jiang et al. (2016) propose to predict dependency rule probabilities using a neural netwo"
2020.coling-main.227,D19-1148,1,0.821377,"than 700k sentences. Mareˇcek and Straka (2013) utilize a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar. Han et al. (2017) use a subset of the BLLIP corpus that contains around 180k sentences. With the advancement of computing power and deep neural models, we expect to see more future work on training with big data. 4.5 Unsupervised Multilingual Parsing To tackle the lack of supervision in unsupervised dependency parsing, some previous work considers learning models of multiple languages simultaneously (Berg-Kirkpatrick and Klein, 2010; Liu et al., 2013; Jiang et al., 2019; Han et al., 2019b). Ideally, these models can learn from each other by identifying shared syntactic behaviors of different languages, especially those in the same language family. For example, Berg-Kirkpatrick and Klein (2010) propose to utilize the similarity of different languages defined by a phylogenetic tree and learn several dependency parsers jointly. Han et al. (2019b) propose to learn a unified multilingual parser with language embeddings as input. Jiang et al. (2019) propose to guide the learning process of unsupervised dependency parser from the knowledge of another language by us"
2020.coling-main.227,P04-1061,0,0.756548,"d word. In unsupervised dependency parsing, the goal is to obtain a dependency parser without using annotated sentences. Some work requires no training data and derives dependency trees from centrality or saliency information (Søgaard, 2012). We focus on learning a dependency parser from an unannotated dataset that consists of a set of sentences without any parse tree annotation. In many cases, part-of-speech (POS) tags of the words in the training sentences are assumed to be available during training. Two evaluation metrics are widely used in previous work of unsupervised dependency parsing (Klein and Manning, 2004): directed dependency accuracy (DDA) and undirected dependency accuracy (UDA). DDA denotes the percentage of correctly predicted dependency edges, while UDA is similar to DDA but disregards the directions of edges when evaluating their correctness. 2.2 Related Areas Supervised Dependency Parsing Supervised dependency parsing aims to train a dependency parser from training sentences that are manually annotated with their dependency parse trees. Generally, supervised dependency parsing approaches can be divided into graph-based approaches and transition-based approaches. A graph-based dependency"
2020.coling-main.227,N15-1067,0,0.0167662,"D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution. It also specifies a Gaussian prior over the intermediate continuous vector. 2527 3.2.3 Other Discriminative Approaches Apart from the approaches based on autoencoder and variational autoencoder, there are also a few other discriminative approaches based on discriminative clustering (Grave and Elhadad, 2015), self-training (Le and Zuidema, 2015), or searching (Daum´e III, 2009). Because of space limit, below we only introduce the approach based on discriminative clustering called Convex MST (Grave and Elhadad, 2015). Convex MST employs a first-order graph-based discriminative parser. It searches for the parses of all the training sentences and learns the parser simultaneously, with a learning objective that the searched parses are close to the predicted parses by the parser. In other words, the parses should be easily predictable by the parser. The objective function can be relaxed to become convex and then can be optimized exactly."
2020.coling-main.227,2020.findings-emnlp.193,1,0.735047,"serve as the inspiration for studies of other unsupervised tasks, especially unsupervised structured prediction tasks. A recent example is Nishida and Nakayama (2020), who study unsupervised discourse parsing (inducing discourse structures for a given text) by borrowing techniques from unsupervised parsing such as Viterbi EM and heuristically designed initialization. Unsupervised dependency parsing techniques can also be used as building blocks for transfer learning of parsers. Some of the approaches discussed in this paper have already been applied to cross-lingual parsing (He et al., 2019; Li and Tu, 2020), and more such endeavors are expected in the future. 6.3 Interpretability One prominent problem of deep neural networks is that they act as black boxes and are generally not interpretable. How to improve the interpretability of neural networks is a research topic that gains much attention recently. For natural language texts, their linguistic structures reveal important information of the texts and at the same time can be easily understood by human. It is therefore an interesting direction to integrate techniques of unsupervised parsing into various neural models of NLP tasks, such that the n"
2020.coling-main.227,2020.acl-main.300,1,0.770267,"ncy tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because it has to induce not only edges but also nodes of a tree. Consequently, there have been far more papers in unsupervised dependency parsing than in unsupervised constituency parsing over the past decade. More recently, however, there is a surge in interest in unsupervised constituency parsing and several novel approaches were proposed in the past two years (Li et al., 2020). While we focus on unsupervised dependency parsing in this paper, most of our discussions on the classification of approaches and recent trends apply to unsupervised constituency parsing as well. Latent Tree Models with Downstream Tasks Latent tree models treat the parse tree as a latent variable that is used in downstream tasks such as sentiment classification. While no treebank is used in training, these models rely on the performance of the downstream tasks to guide the learning of the latent parse trees. To enable end-to-end learning, the REINFORCE algorithm and the Gumbel-softmax trick ("
2020.coling-main.227,P13-1105,0,0.0406975,"Missing"
2020.coling-main.227,P14-1126,0,0.0191202,"transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets. Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015; McDonald et al., 2011; Ma and Xia, 2014; Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either. Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because it has to induce"
2020.coling-main.227,P13-1028,0,0.045653,"Missing"
2020.coling-main.227,D12-1028,0,0.049334,"Missing"
2020.coling-main.227,H05-1066,0,0.398895,"Missing"
2020.coling-main.227,D11-1006,0,0.0357848,"onald et al., 2005). A transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets. Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015; McDonald et al., 2011; Ma and Xia, 2014; Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either. Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because"
2020.coling-main.227,D10-1120,0,0.243154,"we mentioned earlier, the joint probability of a sentence and its dependency tree can be decomposed into the product of the probabilities of the components in the dependency tree. Apart from the vanilla marginal likelihood, priors and regularization terms are often added into the objective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibi"
2020.coling-main.227,2020.tacl-1.15,0,0.0780453,"dency parsing and dependency grammar induction) is the most challenging, which aims to obtain a dependency parser without using annotated sentences. Despite its difficulty, unsupervised parsing is an interesting research direction, not only because it would reveal ways to utilize almost unlimited text data without the need for human annotation, but also because it can serve as the basis for studies of transfer and semi-supervised learning of parsers. The techniques developed for unsupervised dependency parsing could also be utilized for other NLP tasks, such as unsupervised discourse parsing (Nishida and Nakayama, 2020). In addition, research in unsupervised parsing inspires and verifies cognitive research of human language acquisition. In this paper, we conduct a survey of unsupervised dependency parsing research. We first introduce the definition and evaluation metrics of unsupervised dependency parsing, and discuss research areas related to it. Then we present in detail two major classes of approaches to unsupervised dependency parsing: generative approaches and discriminative approaches. Finally, we discuss important new techniques and setups of unsupervised dependency parsing that appear in recent years"
2020.coling-main.227,D16-1004,0,0.675614,"Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV. Noji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of the parse tree. 3.1.4 Learning Algorithm The Expectation-Maximization (EM) algorithm is typically used to optimize log marginal likelihood. For each sentence, the EM algorithm aims to maximize the following lower-bound of the objective function and alternates between the E-step and M-step. log P (x; Θ) − KL(Q(z)kP (z|x, Θ)) (4) where Q(z) is an auxiliary distribution with regard to z. In the E-step, Θ is fixed and Q(z) is set to P (z|x, Θ). A set of so-called expected counts can be derived from Q(z) to faci"
2020.coling-main.227,C16-1003,0,0.0830307,"dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a; He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010), and Han et al. (2017) use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007), Pate and Johnson (2016), and Spitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. For instance, Han et al. (2017) use neural networks to predict dependency probabilities that are automatically smoothed. In principle, lexicalized approaches could also benefit from pretrained word embeddings, which capture syntactic and semantic similarities between words. Recently"
2020.coling-main.227,N18-1202,0,0.00783051,"pproaches listed in this table may use different training sets and different external 2529 knowledge in their experiments, and one should check the corresponding papers to understand such differences before comparing these accuracies. While the accuracy of unsupervised dependency parsing has increased by over thirty points in the last fifteen years, it is still well below that of supervised models, which leaves much room for improvement and challenges for future research. 6 6.1 Future Directions Utilization of Syntactic Information in Pretrained Language Modeling Pretrained language modeling (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019), as a new NLP paradigm, has been utilized in various areas including question answering, machine translation, grammatical error correction, and so on. Pretrained language models leverage a large-scale corpus for pretraining and then small data sets of specific tasks for finetuning, reducing the difficulty of downstream tasks and boosting their performance. Current state-of-the-art approaches on supervised dependency parsing, such as Zhou and Zhao (2019), adopt the new paradigm and benefit from pretrained language modeling. However, pretrained langua"
2020.coling-main.227,P07-1049,0,0.0488629,"of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a; He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010), and Han et al. (2017) use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007), Pate and Johnson (2016), and Spitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. For instance, Han et al. (2017) use neural networks to predict dependency probabilities that are automatically smoothed. In principle, lexicalized approaches could also benefit from pretrained word embeddings, which capture syntactic and semantic similaritie"
2020.coling-main.227,P06-1072,0,0.00842549,", x(2) , ..., x(N ) }: L(Θ) = N X log P(x(i) ; Θ) (2) i=1 where the model parameters are denoted by Θ. The likelihood of each sentence x is as follows: X P (x; Θ) = P (x, z; Θ) (3) z∈Z(x) where Z(x) is the set of all valid dependency trees of sentence x. As we mentioned earlier, the joint probability of a sentence and its dependency tree can be decomposed into the product of the probabilities of the components in the dependency tree. Apart from the vanilla marginal likelihood, priors and regularization terms are often added into the objective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce"
2020.coling-main.227,N10-1116,0,0.198444,"xiliary distribution with regard to z. In the E-step, Θ is fixed and Q(z) is set to P (z|x, Θ). A set of so-called expected counts can be derived from Q(z) to facilitate the subsequent Mstep and they are typically calculated using the inside-outside algorithm. In the M-step, Θ is optimized based on the expected counts with Q(z) fixed. There are a few variants of the EM algorithm. If Q(z) represents a point-estimation (i.e., the best dependency tree has a probability of 1), the algorithm becomes hard-EM or Viterbi EM, which is found to outperform standard EM in unsupervised dependency parsing (Spitkovsky et al., 2010b). SoftmaxEM (Tu and Honavar, 2012) falls between EM (considering all possible dependency trees) and hard-EM (only considering the best dependency tree), applying a softmax-like transformation to Q(z). During the EM iterations, an annealing schedule (Tu and Honavar, 2012) can be used to gradually shift from hardEM to softmax-EM and finally to the EM algorithm, which leads to better performance than sticking to a single algorithm. Lateen EM (Spitkovsky et al., 2011c) repeatedly alternates between EM and hard-EM, which is also found to produce better results than both EM and hard-EM. Approaches"
2020.coling-main.227,W10-2902,0,0.447046,"xiliary distribution with regard to z. In the E-step, Θ is fixed and Q(z) is set to P (z|x, Θ). A set of so-called expected counts can be derived from Q(z) to facilitate the subsequent Mstep and they are typically calculated using the inside-outside algorithm. In the M-step, Θ is optimized based on the expected counts with Q(z) fixed. There are a few variants of the EM algorithm. If Q(z) represents a point-estimation (i.e., the best dependency tree has a probability of 1), the algorithm becomes hard-EM or Viterbi EM, which is found to outperform standard EM in unsupervised dependency parsing (Spitkovsky et al., 2010b). SoftmaxEM (Tu and Honavar, 2012) falls between EM (considering all possible dependency trees) and hard-EM (only considering the best dependency tree), applying a softmax-like transformation to Q(z). During the EM iterations, an annealing schedule (Tu and Honavar, 2012) can be used to gradually shift from hardEM to softmax-EM and finally to the EM algorithm, which leads to better performance than sticking to a single algorithm. Lateen EM (Spitkovsky et al., 2011c) repeatedly alternates between EM and hard-EM, which is also found to produce better results than both EM and hard-EM. Approaches"
2020.coling-main.227,D11-1118,0,0.371112,"priors and regularization terms are often added into the objective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV. Noji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of the parse tree. 3.1.4 Lea"
2020.coling-main.227,W11-0303,0,0.356416,"priors and regularization terms are often added into the objective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV. Noji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of the parse tree. 3.1.4 Lea"
2020.coling-main.227,D11-1117,0,0.334101,"priors and regularization terms are often added into the objective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV. Noji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of the parse tree. 3.1.4 Lea"
2020.coling-main.227,D12-1063,0,0.0162904,"odel with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden III et al. (2009) propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013). Similar tokens may have similar syntactic behaviors in a grammar. Fo"
2020.coling-main.227,D13-1204,0,0.17345,"addition to the EM algorithm, the learning objective can also be optimized with gradient descent. Yang et al. (2020) recently observe that gradient descent can sometimes significantly outperform EM when learning neural DMV. Better learning results can also be achieved by manipulating the training data. Spitkovsky et al. (2010a) apply curriculum learning to DMV training, which starts with only the shortest sentences and then progresses to increasingly longer sentences. Tu and Honavar (2011) provide a theoretical analysis on the utility of curriculum learning in unsupervised dependency parsing. Spitkovsky et al. (2013) propose to treat different learning algorithms and configurations as modules and connect them to form a network. Some approaches discussed above, such as Lateen EM and curriculum learning, can be seen as special cases of this approach. 3.1.5 Pros and Cons It is often straightforward to incorporate various inductive biases and manually-designed local features into generative approaches. Moreover, generative models can be easily trained via the EM algorithm and its extensions. On the other hand, generative models often have limited expressive power because of the independence assumptions they m"
2020.coling-main.227,D12-1121,1,0.917515,"biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV. Noji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of the parse tree. 3.1.4 Learning Algorithm The Expectation-Maximization (EM) algorithm is typically used to optimize log marginal like"
2020.coling-main.227,2020.coling-main.347,1,0.836207,"has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden III et al. (2009) propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013). Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute ge"
2020.coling-main.227,W15-2201,0,0.026376,"of its edges (McDonald et al., 2005). A transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets. Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015; McDonald et al., 2011; Ma and Xia, 2014; Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either. Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dep"
2020.coling-main.227,P19-1230,0,0.0218836,"Utilization of Syntactic Information in Pretrained Language Modeling Pretrained language modeling (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019), as a new NLP paradigm, has been utilized in various areas including question answering, machine translation, grammatical error correction, and so on. Pretrained language models leverage a large-scale corpus for pretraining and then small data sets of specific tasks for finetuning, reducing the difficulty of downstream tasks and boosting their performance. Current state-of-the-art approaches on supervised dependency parsing, such as Zhou and Zhao (2019), adopt the new paradigm and benefit from pretrained language modeling. However, pretrained language models have not been widely used in unsupervised dependency parsing. One major concern is that pretrained language models are too informative and may make unsupervised models more prone to overfitting. Besides, massive syntactic and semantic information is encoded in pretrained language models and how to extract the syntactic part from them is a challenging task. 6.2 Inspiration for Other Tasks Unsupervised dependency parsing is a classic unsupervised learning task. Many techniques developed fo"
2020.coling-main.322,N19-1116,0,0.467607,"as text classification (Tai et al., 2015), machine translation (Eriguchi et al., 2017; Wang et al., 2018) and semantic role labelling (Gildea and Palmer, 2002; He et al., 2017). Although supervised neural methods have very high accuracy, they require annotated data which can be very limited for low-resource languages and domains. Unsupervised constituency parsing is a task aiming to learn a constituency parser from text without annotated parse trees. There is a recent trend of solving this task with neural approaches (Shen et al., 2018; Kim et al., 2019a; Shen et al., 2019; Kim et al., 2019b; Drozdov et al., 2019). DIORA (Drozdov et al., 2019) is one of these models. It mimics the inside-outside algorithm (Baker, 1979) to build an inside chart and an outside chart for each input sentence. Each cell in these charts represents a representation of the inside text or the outside context of a span of the sentence. DIORA is trained by using the contextual representation of every single word to predict the word itself, which can be seen as a reconstruction procedure of the input sentence. The original training objective of DIORA only takes the n leaf-level spans into account for an n-word sentence. Since such"
2020.coling-main.322,P17-2012,0,0.0270438,"l span, the span of length 1, to reconstruct the word inside the span, the model is trained without labeled data. In this work, we extend the training objective of DIORA by making use of all spans instead of only leaf-level spans. We test our new training objective on datasets of two languages: English and Japanese, and empirically show that our method achieves improvement in parsing accuracy over the original DIORA. 1 Introduction Constituency parsing produces constituent parse trees that can be useful for downstream tasks, such as text classification (Tai et al., 2015), machine translation (Eriguchi et al., 2017; Wang et al., 2018) and semantic role labelling (Gildea and Palmer, 2002; He et al., 2017). Although supervised neural methods have very high accuracy, they require annotated data which can be very limited for low-resource languages and domains. Unsupervised constituency parsing is a task aiming to learn a constituency parser from text without annotated parse trees. There is a recent trend of solving this task with neural approaches (Shen et al., 2018; Kim et al., 2019a; Shen et al., 2019; Kim et al., 2019b; Drozdov et al., 2019). DIORA (Drozdov et al., 2019) is one of these models. It mimics"
2020.coling-main.322,P02-1031,0,0.236837,"the model is trained without labeled data. In this work, we extend the training objective of DIORA by making use of all spans instead of only leaf-level spans. We test our new training objective on datasets of two languages: English and Japanese, and empirically show that our method achieves improvement in parsing accuracy over the original DIORA. 1 Introduction Constituency parsing produces constituent parse trees that can be useful for downstream tasks, such as text classification (Tai et al., 2015), machine translation (Eriguchi et al., 2017; Wang et al., 2018) and semantic role labelling (Gildea and Palmer, 2002; He et al., 2017). Although supervised neural methods have very high accuracy, they require annotated data which can be very limited for low-resource languages and domains. Unsupervised constituency parsing is a task aiming to learn a constituency parser from text without annotated parse trees. There is a recent trend of solving this task with neural approaches (Shen et al., 2018; Kim et al., 2019a; Shen et al., 2019; Kim et al., 2019b; Drozdov et al., 2019). DIORA (Drozdov et al., 2019) is one of these models. It mimics the inside-outside algorithm (Baker, 1979) to build an inside chart and"
2020.coling-main.322,L18-1550,0,0.0608311,"Missing"
2020.coling-main.322,P17-1044,0,0.0287045,"out labeled data. In this work, we extend the training objective of DIORA by making use of all spans instead of only leaf-level spans. We test our new training objective on datasets of two languages: English and Japanese, and empirically show that our method achieves improvement in parsing accuracy over the original DIORA. 1 Introduction Constituency parsing produces constituent parse trees that can be useful for downstream tasks, such as text classification (Tai et al., 2015), machine translation (Eriguchi et al., 2017; Wang et al., 2018) and semantic role labelling (Gildea and Palmer, 2002; He et al., 2017). Although supervised neural methods have very high accuracy, they require annotated data which can be very limited for low-resource languages and domains. Unsupervised constituency parsing is a task aiming to learn a constituency parser from text without annotated parse trees. There is a recent trend of solving this task with neural approaches (Shen et al., 2018; Kim et al., 2019a; Shen et al., 2019; Kim et al., 2019b; Drozdov et al., 2019). DIORA (Drozdov et al., 2019) is one of these models. It mimics the inside-outside algorithm (Baker, 1979) to build an inside chart and an outside chart f"
2020.coling-main.322,P19-1228,0,0.104619,"arse trees that can be useful for downstream tasks, such as text classification (Tai et al., 2015), machine translation (Eriguchi et al., 2017; Wang et al., 2018) and semantic role labelling (Gildea and Palmer, 2002; He et al., 2017). Although supervised neural methods have very high accuracy, they require annotated data which can be very limited for low-resource languages and domains. Unsupervised constituency parsing is a task aiming to learn a constituency parser from text without annotated parse trees. There is a recent trend of solving this task with neural approaches (Shen et al., 2018; Kim et al., 2019a; Shen et al., 2019; Kim et al., 2019b; Drozdov et al., 2019). DIORA (Drozdov et al., 2019) is one of these models. It mimics the inside-outside algorithm (Baker, 1979) to build an inside chart and an outside chart for each input sentence. Each cell in these charts represents a representation of the inside text or the outside context of a span of the sentence. DIORA is trained by using the contextual representation of every single word to predict the word itself, which can be seen as a reconstruction procedure of the input sentence. The original training objective of DIORA only takes the n le"
2020.coling-main.322,N19-1114,0,0.0187604,"arse trees that can be useful for downstream tasks, such as text classification (Tai et al., 2015), machine translation (Eriguchi et al., 2017; Wang et al., 2018) and semantic role labelling (Gildea and Palmer, 2002; He et al., 2017). Although supervised neural methods have very high accuracy, they require annotated data which can be very limited for low-resource languages and domains. Unsupervised constituency parsing is a task aiming to learn a constituency parser from text without annotated parse trees. There is a recent trend of solving this task with neural approaches (Shen et al., 2018; Kim et al., 2019a; Shen et al., 2019; Kim et al., 2019b; Drozdov et al., 2019). DIORA (Drozdov et al., 2019) is one of these models. It mimics the inside-outside algorithm (Baker, 1979) to build an inside chart and an outside chart for each input sentence. Each cell in these charts represents a representation of the inside text or the outside context of a span of the sentence. DIORA is trained by using the contextual representation of every single word to predict the word itself, which can be seen as a reconstruction procedure of the input sentence. The original training objective of DIORA only takes the n le"
2020.coling-main.322,2020.acl-main.300,1,0.809563,"perimental results on English PTB. DIORA DIROA-all Upper Bound F1-10µ 39.33 ± 2.92 43.30 ± 5.18 61.41 With Punctuation F1-10max F1-allµ 42.83 28.93 ± 4.29 47.73 33.00 ± 3.71 62.53 F1-allmax 32.33 36.93 - F1-10µ 44.02 ± 5.02 47.09 ± 1.79 67.25 No Punctuation F1-10max F1-allµ 49.18 35.26 ± 3.10 49.17 36.37 ± 3.58 67.32 F1-allmax 38.02 41.56 - Table 3: Experimental results on Japanese KTB. F1 score computed by Evalb1 . We report F1 scores on test sentences of length ≤ 10 and of all lengths. For the performance of the original DIORA, we rerun the experiments with the hyper-parameters provided by (Li et al., 2020). Since the predicted parse tree is binary, we also provide the upper bound of F1 scores without tree binarization for each dataset. English We list the experimental results of English in Table 2. For the setting with punctuation, DIORA with the all-span objective function outperforms the original DIORA on both sentences of length ≤ 10 and all sentences. For the setting without punctuation, our method performs better than the origin DIORA on sentences of length ≤ 10 but worse on all sentences. Japanese We list the experimental results of Japanese in Table 3. For settings both with and without"
2020.coling-main.322,J93-2004,0,0.0697089,"Missing"
2020.coling-main.322,N18-1202,0,0.0798102,"Missing"
2020.coling-main.322,P15-1150,0,0.0565003,"textual representation of each leaf-level span, the span of length 1, to reconstruct the word inside the span, the model is trained without labeled data. In this work, we extend the training objective of DIORA by making use of all spans instead of only leaf-level spans. We test our new training objective on datasets of two languages: English and Japanese, and empirically show that our method achieves improvement in parsing accuracy over the original DIORA. 1 Introduction Constituency parsing produces constituent parse trees that can be useful for downstream tasks, such as text classification (Tai et al., 2015), machine translation (Eriguchi et al., 2017; Wang et al., 2018) and semantic role labelling (Gildea and Palmer, 2002; He et al., 2017). Although supervised neural methods have very high accuracy, they require annotated data which can be very limited for low-resource languages and domains. Unsupervised constituency parsing is a task aiming to learn a constituency parser from text without annotated parse trees. There is a recent trend of solving this task with neural approaches (Shen et al., 2018; Kim et al., 2019a; Shen et al., 2019; Kim et al., 2019b; Drozdov et al., 2019). DIORA (Drozdov et"
2020.coling-main.322,D18-1509,0,0.0157217,"gth 1, to reconstruct the word inside the span, the model is trained without labeled data. In this work, we extend the training objective of DIORA by making use of all spans instead of only leaf-level spans. We test our new training objective on datasets of two languages: English and Japanese, and empirically show that our method achieves improvement in parsing accuracy over the original DIORA. 1 Introduction Constituency parsing produces constituent parse trees that can be useful for downstream tasks, such as text classification (Tai et al., 2015), machine translation (Eriguchi et al., 2017; Wang et al., 2018) and semantic role labelling (Gildea and Palmer, 2002; He et al., 2017). Although supervised neural methods have very high accuracy, they require annotated data which can be very limited for low-resource languages and domains. Unsupervised constituency parsing is a task aiming to learn a constituency parser from text without annotated parse trees. There is a recent trend of solving this task with neural approaches (Shen et al., 2018; Kim et al., 2019a; Shen et al., 2019; Kim et al., 2019b; Drozdov et al., 2019). DIORA (Drozdov et al., 2019) is one of these models. It mimics the inside-outside"
2020.coling-main.347,N10-1083,0,0.0991094,"Missing"
2020.coling-main.347,D10-1117,0,0.394079,"Missing"
2020.coling-main.347,D17-1171,1,0.862383,"del. A main disadvantage of DMV and many of its extensions is that they lack expressiveness. The generation of a dependent token is only conditioned on its parent, the relative direction of the token to its parent, and whether its parent has already generated any child in this direction, hence ignoring other contextual information. To improve model expressiveness, researchers often turn to discriminative methods, which can incorporate more contextual information into the scoring or prediction of dependency arcs. For example, Grave and Elhadad (2015) uses the idea of disrciminative clustering, Cai et al. (2017) uses a discriminative parser in the CRF-autoencoder framework, and Li et al. (2018) uses an encoder-decoder framework that contains a discriminative transitioned-based parser. For DMV, Han et al. (2019) proposes the discriminative neural DMV which uses a global sentence embedding to introduce contextual information into the calculation of grammar rule probabilities. In the literature of supervised graph-based dependency parsing, however, there exists another technique for incorporating contextual information ∗ Corresponding Author Our source code is available at: https://github.com/sustcsongl"
2020.coling-main.347,N09-1009,0,0.352879,"Missing"
2020.coling-main.347,W16-5901,0,0.260255,"ide algorithm. We can use gradient descent to update θ. X ∇θ Q(θ) = e(r, x)∇θ log pθ (r) (6) r∈R 3914 Learning via direct marginal likelihood optimization We can also use gradient descent to maximize log pθ (x) directly. Based on the derivation of Salakhutdinov et al. (2003)2 , we have X pθ (z|x)∇θ log pθ (x, z) ∇θ (log pθ (x)) = z∈T (x) X = pθ (z|x) X c(r, x, z)∇θ log pθ (r) (7) r∈R z∈T (x) = X e(r, x)∇θ log pθ (r) r∈R where e(r, x) is the expected count of grammar rule r in sentence x based on pθ (z|x). Traditionally, we use the inside-outside algorithm to obtain the expected count e(r, x). Eisner (2016) points out that we can use back-propagation to calculate the expected count e(r, x). e(r, x) = ∂ log pθ (x) ∂ log pθ (r) (8) So we only need to use the inside algorithm to calculate log pθ (x) and then use back-propagation to update the parameters directly, without the need for the outside algorithm. Mini-batch gradient descent as online EM In Equation 7, we note that the gradient contains the term e(r, x). If we use mini-batch gradient descent to optimize log pθ (x), it is analogous to the onlineEM algorithm (Liang and Klein, 2009). To compute the gradient for each mini-batch, we first need"
2020.coling-main.347,P15-1133,0,0.727993,"MV) (Klein and Manning, 2004), which is a probabilistic generative model. A main disadvantage of DMV and many of its extensions is that they lack expressiveness. The generation of a dependent token is only conditioned on its parent, the relative direction of the token to its parent, and whether its parent has already generated any child in this direction, hence ignoring other contextual information. To improve model expressiveness, researchers often turn to discriminative methods, which can incorporate more contextual information into the scoring or prediction of dependency arcs. For example, Grave and Elhadad (2015) uses the idea of disrciminative clustering, Cai et al. (2017) uses a discriminative parser in the CRF-autoencoder framework, and Li et al. (2018) uses an encoder-decoder framework that contains a discriminative transitioned-based parser. For DMV, Han et al. (2019) proposes the discriminative neural DMV which uses a global sentence embedding to introduce contextual information into the calculation of grammar rule probabilities. In the literature of supervised graph-based dependency parsing, however, there exists another technique for incorporating contextual information ∗ Corresponding Author"
2020.coling-main.347,D17-1176,1,0.941554,"he generation steps. 2.2 Neuralized DMV Models Neural DMV One limitation of the DMV model is that it does not consider the correlation between tokens. Jiang et al. (2016) proposed the Neural DMV (NDMV) model, which uses continuous POS embedding to represent discrete POS tags and calculate rule probabilities through neural networks based on the POS embedding. In this way, the model can learn the correlation between POS tags and smooth grammar rule probabilities accordingly. Lexicalized NDMV Neural DMV is still an unlexicalized model which is based on POS tags and does not use word information. Han et al. (2017) proposed the Lexicalized NDMV (L-NDMV) in which each token is a POS/word pair. The neural network that computes rule probabilities takes both the POS embedding and the word embedding as input. To reduce the vocabulary size, they replace low-frequency words with their POS tags. 3912 3 Method 3.1 Second-Order Parsing In our proposed second-order NDMV, we calculate each rule probability based additionally on the information of the sibling or grandparent. We take sibling-NDMV for example to demonstrate the generative story. • We start with the imaginary root token, generating its only child c wit"
2020.coling-main.347,P19-1526,1,0.915404,"its parent, and whether its parent has already generated any child in this direction, hence ignoring other contextual information. To improve model expressiveness, researchers often turn to discriminative methods, which can incorporate more contextual information into the scoring or prediction of dependency arcs. For example, Grave and Elhadad (2015) uses the idea of disrciminative clustering, Cai et al. (2017) uses a discriminative parser in the CRF-autoencoder framework, and Li et al. (2018) uses an encoder-decoder framework that contains a discriminative transitioned-based parser. For DMV, Han et al. (2019) proposes the discriminative neural DMV which uses a global sentence embedding to introduce contextual information into the calculation of grammar rule probabilities. In the literature of supervised graph-based dependency parsing, however, there exists another technique for incorporating contextual information ∗ Corresponding Author Our source code is available at: https://github.com/sustcsonglin/second-order-neural-dmv This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. 1 Licence details: 3911 Proceedings of the 28"
2020.coling-main.347,N09-1012,0,0.339347,"Missing"
2020.coling-main.347,D16-1073,1,0.822588,"decision from the DECISION distribution PDECISION (dec|p, dir, val) to determine whether to generate a new child in direction dir. If dec is CONTINUE, then a new child p is generated from the CHILD distribution PCHILD (c|p, dir, val). If dec is STOP, then p stops generating children in direction dir. The joint probability of the sequence and its corresponding dependency parse tree can be calculated by taking product of the probabilities of all the generation steps. 2.2 Neuralized DMV Models Neural DMV One limitation of the DMV model is that it does not consider the correlation between tokens. Jiang et al. (2016) proposed the Neural DMV (NDMV) model, which uses continuous POS embedding to represent discrete POS tags and calculate rule probabilities through neural networks based on the POS embedding. In this way, the model can learn the correlation between POS tags and smooth grammar rule probabilities accordingly. Lexicalized NDMV Neural DMV is still an unlexicalized model which is based on POS tags and does not use word information. Han et al. (2017) proposed the Lexicalized NDMV (L-NDMV) in which each token is a POS/word pair. The neural network that computes rule probabilities takes both the POS em"
2020.coling-main.347,P19-1228,0,0.123765,"Wp xp es = Ws xs We feed ec , ep , es to the same neural network that consists of three consecutive MLPs. The first and second MLPs are used respectively to insert valence and direction information into the representations, and the last MLP is used to produce final hidden representations hc , hp , hs (see the appendix for the complete formulation). We use different parameters of the first and second MLPs for different values of valence val and direction dir. We add skip-connections to the first and second MLPs because skipconnections have been found very useful in unsupervised neural parsing (Kim et al., 2019). We then follow Wang et al. (2019) and use a decomposed trilinear function to compute the unnormalized rule probability from the three vectors hc , hp , hs . S(p, s, c) = q X op,i × os,i × oc,i i=1 op = Cp hp o c = C c hc os = Cs hs where Cp , Cc , Cs ∈ Rq×d are the parameters of the decomposed trilinear function and × is scalar multiplication. Then we apply a softmax function to produce the final rule probability. eS(p,s,c) PCHILD (c|p, s, dir, val) = P S(p,s,c0 ) e c0 ∈C where C is the vocabulary. 3913 Figure 1: Illustration of our neural architecture. 3.3 Learning The learning objective fu"
2020.coling-main.347,P04-1061,0,0.721677,"sers can reach a very high accuracy (Dozat and Manning, 2017; Zhang et al., 2020). Unfortunately, supervised parsing requires treebanks (annotated parse trees) for training, which are very expensive and time-consuming to build. On the other hand, unsupervised dependency parsing requires only unannotated corpora for training, though the accuracy of unsupervised parsing still lags far behind that of supervised parsing. We focus on unsupervised dependency parsing in this paper. Most methods in the literature of unsupervised dependency parsing are based on the Dependency Model with Valence (DMV) (Klein and Manning, 2004), which is a probabilistic generative model. A main disadvantage of DMV and many of its extensions is that they lack expressiveness. The generation of a dependent token is only conditioned on its parent, the relative direction of the token to its parent, and whether its parent has already generated any child in this direction, hence ignoring other contextual information. To improve model expressiveness, researchers often turn to discriminative methods, which can incorporate more contextual information into the scoring or prediction of dependency arcs. For example, Grave and Elhadad (2015) uses"
2020.coling-main.347,P10-1001,0,0.269076,"ature of supervised graph-based dependency parsing, however, there exists another technique for incorporating contextual information ∗ Corresponding Author Our source code is available at: https://github.com/sustcsonglin/second-order-neural-dmv This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. 1 Licence details: 3911 Proceedings of the 28th International Conference on Computational Linguistics, pages 3911–3924 Barcelona, Spain (Online), December 8-13, 2020 and increasing expressiveness, namely high-order parsing (Koo and Collins, 2010; Ma and Hai, 2012). A first-order parser, such as the DMV, only considers local parent-children information. In comparison, a high-order parser takes into account the interaction between multiple dependency arcs. In this work, we propose the second-order neural DMV model, which incorporates second-order information (e.g., sibling or grandparent) into the original (neural) DMV model. To achieve better learning accuracy, we design a new neural architecture for rule probability computation and promote direct marginal likelihood optimization (Salakhutdinov et al., 2003; Tran et al., 2016) over th"
2020.coling-main.347,N15-1067,0,0.405157,"Missing"
2020.coling-main.347,N09-1069,0,0.0448775,"se the inside-outside algorithm to obtain the expected count e(r, x). Eisner (2016) points out that we can use back-propagation to calculate the expected count e(r, x). e(r, x) = ∂ log pθ (x) ∂ log pθ (r) (8) So we only need to use the inside algorithm to calculate log pθ (x) and then use back-propagation to update the parameters directly, without the need for the outside algorithm. Mini-batch gradient descent as online EM In Equation 7, we note that the gradient contains the term e(r, x). If we use mini-batch gradient descent to optimize log pθ (x), it is analogous to the onlineEM algorithm (Liang and Klein, 2009). To compute the gradient for each mini-batch, we first need to compute the expected counts from the training sentences in the mini-batch, which is exactly what the online E-step does; we then use the expected counts to compute the gradient and update the model parameters, which is similar to the M-step, except that here we only perform one update step, while in the EM algorithm multiple update steps may be taken based on the same expected counts. According to Liang and Klein (2009), online-EM has a faster convergence speed and can even find a better solution. Empirically, we do find that dire"
2020.coling-main.347,C12-2077,0,0.0321463,"ph-based dependency parsing, however, there exists another technique for incorporating contextual information ∗ Corresponding Author Our source code is available at: https://github.com/sustcsonglin/second-order-neural-dmv This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. 1 Licence details: 3911 Proceedings of the 28th International Conference on Computational Linguistics, pages 3911–3924 Barcelona, Spain (Online), December 8-13, 2020 and increasing expressiveness, namely high-order parsing (Koo and Collins, 2010; Ma and Hai, 2012). A first-order parser, such as the DMV, only considers local parent-children information. In comparison, a high-order parser takes into account the interaction between multiple dependency arcs. In this work, we propose the second-order neural DMV model, which incorporates second-order information (e.g., sibling or grandparent) into the original (neural) DMV model. To achieve better learning accuracy, we design a new neural architecture for rule probability computation and promote direct marginal likelihood optimization (Salakhutdinov et al., 2003; Tran et al., 2016) over the widely used expec"
2020.coling-main.347,D10-1120,0,0.747769,"training, section 22 for validation and section 23 for testing. We use sentences of length ≤ 10 in training and use sentences of length ≤ 10 (WSJ10) and all sentences (WSJ) in testing. Universal Dependency Treebank Following the setting of Li et al. (2018) and Han et al. (2019), we conduct experiments on selected languages from the Universal Dependency Treebank 1.4 (Nivre et al., 2016). We use sentences of length ≤ 15 in training and sentences of length ≤ 15 and ≤ 40 in testing. Setting On the WSJ dataset, for fair comparison, we follow Han et al. (2017) and Han et al. (2019) and use HDP-DEP (Naseem et al., 2010) to initialize our models. Specifically, we train the unsupervised HDP-DEP model on WSJ, use it to parse the training corpus, and then use the predicted parse trees to perform supervised learning of our model for several epochs. On the UD dataset, we use the K&M initialization (Klein and Manning, 2004). We use direct marginal likelihood optimization (DMO) as the training method and use Adam (Kingma and Ba, 2015) as the optimizer with learning rate 0.001. The batch size is set to 64 for WSJ and 100 for UD. The hyperparameters of the neural networks, the setting of L-NDMV and more details can be"
2020.coling-main.347,2020.acl-demos.38,0,0.113497,"hods on log-likelihood for UD (French). 5 10 15 20 iteration 25 30 Figure 5: Comparison of training methods on UAS for UD (French). 3919 Training method separate training joint training UAS (WSJ10 / WSJ) L-NDMV sibling-NDMV 76.6 / 62.7 77.5 / 64.8 79.2 / 65.4 78.7 / 65.6 joint parsing 78.4 / 65.8 79.9 / 67.5 Table 5: The effect of joint training and joint parsing complexity of O(n3 ) of first-order NDMV models, where n is the sentence length. However, transitionbased parsers are hard to batchify, while our model can be parallelized efficiently following the methods introduced by Torch-Struct (Rush, 2020). In practice, our second-order parser runs very fast on GPU, requiring only several minutes to train. 6 Conclusion We propose second-order NDMV models, which incorporate sibling or grandparent information. We find that sibling information is very useful in unsupervised dependency parsing. We use agreement-based learning to combine the benefits of second-order parsing and lexicalization, achieving state-of-the-art results on the WSJ dataset. We also show the effectiveness of our neural parameterization architecture with skip-connections and the direct marginal likelihood optimization method. A"
2020.coling-main.347,D13-1204,0,0.335103,"Missing"
2020.coling-main.347,W16-5907,0,0.0405888,"ing (Koo and Collins, 2010; Ma and Hai, 2012). A first-order parser, such as the DMV, only considers local parent-children information. In comparison, a high-order parser takes into account the interaction between multiple dependency arcs. In this work, we propose the second-order neural DMV model, which incorporates second-order information (e.g., sibling or grandparent) into the original (neural) DMV model. To achieve better learning accuracy, we design a new neural architecture for rule probability computation and promote direct marginal likelihood optimization (Salakhutdinov et al., 2003; Tran et al., 2016) over the widely used expectationmaximization algorithm for training. One particular challenge faced by second-order neural DMVs is that the number of grammar rules grows cubically to the vocabulary size, making it difficult to store and train a lexicalized model containing thousands of words. Therefore, instead of learning a second-order lexicalized model, we propose to jointly learn a second-order unlexicalized model (whose vocabulary consists of POS tags instead of words) and a first-order lexicalized model based on the agreement-based learning framework (Liang et al., 2007). The jointly le"
2020.coling-main.347,D12-1121,1,0.946077,"Missing"
2020.coling-main.347,P19-1454,1,0.816084,"es to the same neural network that consists of three consecutive MLPs. The first and second MLPs are used respectively to insert valence and direction information into the representations, and the last MLP is used to produce final hidden representations hc , hp , hs (see the appendix for the complete formulation). We use different parameters of the first and second MLPs for different values of valence val and direction dir. We add skip-connections to the first and second MLPs because skipconnections have been found very useful in unsupervised neural parsing (Kim et al., 2019). We then follow Wang et al. (2019) and use a decomposed trilinear function to compute the unnormalized rule probability from the three vectors hc , hp , hs . S(p, s, c) = q X op,i × os,i × oc,i i=1 op = Cp hp o c = C c hc os = Cs hs where Cp , Cc , Cs ∈ Rq×d are the parameters of the decomposed trilinear function and × is scalar multiplication. Then we apply a softmax function to produce the final rule probability. eS(p,s,c) PCHILD (c|p, s, dir, val) = P S(p,s,c0 ) e c0 ∈C where C is the vocabulary. 3913 Figure 1: Illustration of our neural architecture. 3.3 Learning The learning objective function L(θ) is the log-likelihood o"
2020.coling-main.347,2020.acl-main.302,0,0.0358361,"Missing"
2020.emnlp-main.182,P18-1152,0,0.0238944,"d C is their parsing accuracy. As we defined in Equation 1, the consensus prediction of parsers B and C is regarded as ground truth, no matter whether the prediction is actually right or wrong. Thus parsers B and C should have high accuracy and also different inductive biases so that they are unlikely to make the same mistake. In addition, B and C should not be too similar to parser A, because otherwise the first two terms in Equation 1 would become hard to optimize. 3.2 Evaluation Criteria for Sentence Quality We consider two aspects of the sentence quality as follows: • Fluency: Inspired by Holtzman et al. (2018); Xu et al. (2018); Pang et al. (2020), we use • Meaning Preservation: Adversarial examples that differ too much from the original sentences are less effective in attacks because humans can easily identify them. We use BERTScore (Zhang et al., 2019b) as another reward in learning to evaluate the similarity between two sentences at the meaning level. We choose to use BERTScore because it correlates better with human judgments than traditional measures such as BLEU (Papineni et al., 2002). sm (x, x ˆ) = BERT Score(x, x ˆ) By maximizing these criteria, we hope to make the adversarial examples loo"
2020.emnlp-main.182,D17-1215,0,0.031583,"ne word from “am” to “fires”. This change makes the perturbed example I fires a writer ungrammatical. Even if the perturbed example is “I fire a writer” that meets the rules of grammar, the true output structure is still different from the input sentence “I am a writer”. More importantly, this true parse is unknown to the attacker, which hinders the next update step. Adversarial examples, which contain perturbations to the input of a model that elicit large changes in the output, have been shown to be an effective way of assessing the robustness of models in natural language processing (NLP) (Jia and Liang, 2017; Belinkov and Bisk, 2018; Hosseini et al., 2017; Samanta and Mehta, 2017; Alzantot et al., † min log P(y =?|ˆ x + r) ∗ Introduction ∗ ˆ1 x ˆ2 x ˆ3 x ˆ4 x x4 2018; Ebrahimi et al., 2018; Michel et al., 2019; Wang et al., 2019). Adversarial training, in which models are trained on adversarial examples, has also been shown to improve the accuracy and robustness of NLP models (Goodfellow et al., 2015; Tram`er et al., 2017; Yasunaga et al., 2018). So far, most existing methods of generating adversarial examples only work for classification tasks (Jia and Liang, 2017; Wang et al., 2019) and are not"
2020.emnlp-main.182,Q16-1023,0,0.0240695,"ion task. 4.1 Data Our model does not need labeled data for training but we need a victim parser and two reference parsers in our experiments. We learn these parsers on an English dataset: Penn Treebank 3.0 (PTB, Marcus et al. (1994)). We also use the same data for training and evaluating our model. 4.2 Parser Selection We choose the Deep Biaffine parser (Dozat and Manning (2017)), one of the state-of-the-art graphbased parsers, as the victim parser A. For the reference parsers, we choose two other well-known dependency parsers: - Parser B: StackPTR from Ma et al. (2018) - Parser C: BiST from Kiperwasser and Goldberg (2016) The three parsers are trained with PTB. All the hyper-parameters of these parsers are the same as reported in their papers. 4.3 Evaluation Metrics Our goal is to generate fluent sentences that are mispredicted by the victim model. Thus, we evaluate the adversarial examples produced by our model from 2 aspects: generation fluency and attacking efficiency (6 metrics). Generation Fluency We use the perplexity on GPT-2 to evaluate the fluency of the generated sentences. Attacking efficiency We evaluate the attacking success rates at the token level and sentence level. The token level attacking su"
2020.emnlp-main.182,N15-1142,0,0.0707263,"Missing"
2020.emnlp-main.182,P16-1101,0,0.0212845,"he sentence level attacking success rate reduces from 72 to 70. We perform significance tests on the attacking success rate. The p-value is calculated by using the one-tailed sign test with bootstrap resampling on 50 samples following Chollampatt, Wang, and Ng (2019). We compare the attacking success rate with and without retraining. The p-values (5.42e20 at the token level and 3.39e-21 at the sentence level) show that the improvement is significant. 5 Experiments on POS Tagging 5.1 Experimental Setup In this section, we apply our method to the part-ofspeech tagging task using the tagger from Ma and Hovy (2016) as the victim model. For the reference taggers, we choose two state-of-the-art taggers: Stanford POS tagger from Toutanova et al. (2003) and Senna tagger from Collobert et al. (2011). All the hyper-parameters of the three taggers are the same as reported in their papers. We conduct the experiments on the PTB dataset. Similar to dependency parsing, the word level approach in section 2.3 is the baseline. For the adversarial example generator, we use the same structure and pretrain strategy as Section 4.4, except that the dimension of hidden state is set to 512. We train the sentence generator u"
2020.emnlp-main.182,P18-1130,0,0.0313215,"arsing, a well-known structured prediction task. 4.1 Data Our model does not need labeled data for training but we need a victim parser and two reference parsers in our experiments. We learn these parsers on an English dataset: Penn Treebank 3.0 (PTB, Marcus et al. (1994)). We also use the same data for training and evaluating our model. 4.2 Parser Selection We choose the Deep Biaffine parser (Dozat and Manning (2017)), one of the state-of-the-art graphbased parsers, as the victim parser A. For the reference parsers, we choose two other well-known dependency parsers: - Parser B: StackPTR from Ma et al. (2018) - Parser C: BiST from Kiperwasser and Goldberg (2016) The three parsers are trained with PTB. All the hyper-parameters of these parsers are the same as reported in their papers. 4.3 Evaluation Metrics Our goal is to generate fluent sentences that are mispredicted by the victim model. Thus, we evaluate the adversarial examples produced by our model from 2 aspects: generation fluency and attacking efficiency (6 metrics). Generation Fluency We use the perplexity on GPT-2 to evaluate the fluency of the generated sentences. Attacking efficiency We evaluate the attacking success rates at the token"
2020.emnlp-main.182,H94-1020,0,0.675587,"ense against Adversarial Attack Following Goodfellow et al. (2015), we use adversarial training to withstand attacks. More specifically, we enhance the victim model by injecting adversarial examples into the training data and retraining the model with the mixed data. 4 Experiments on Dependency Parsing We first perform experiments on dependency parsing, a well-known structured prediction task. 4.1 Data Our model does not need labeled data for training but we need a victim parser and two reference parsers in our experiments. We learn these parsers on an English dataset: Penn Treebank 3.0 (PTB, Marcus et al. (1994)). We also use the same data for training and evaluating our model. 4.2 Parser Selection We choose the Deep Biaffine parser (Dozat and Manning (2017)), one of the state-of-the-art graphbased parsers, as the victim parser A. For the reference parsers, we choose two other well-known dependency parsers: - Parser B: StackPTR from Ma et al. (2018) - Parser C: BiST from Kiperwasser and Goldberg (2016) The three parsers are trained with PTB. All the hyper-parameters of these parsers are the same as reported in their papers. 4.3 Evaluation Metrics Our goal is to generate fluent sentences that are misp"
2020.emnlp-main.182,N19-1314,0,0.181705,"cture is still different from the input sentence “I am a writer”. More importantly, this true parse is unknown to the attacker, which hinders the next update step. Adversarial examples, which contain perturbations to the input of a model that elicit large changes in the output, have been shown to be an effective way of assessing the robustness of models in natural language processing (NLP) (Jia and Liang, 2017; Belinkov and Bisk, 2018; Hosseini et al., 2017; Samanta and Mehta, 2017; Alzantot et al., † min log P(y =?|ˆ x + r) ∗ Introduction ∗ ˆ1 x ˆ2 x ˆ3 x ˆ4 x x4 2018; Ebrahimi et al., 2018; Michel et al., 2019; Wang et al., 2019). Adversarial training, in which models are trained on adversarial examples, has also been shown to improve the accuracy and robustness of NLP models (Goodfellow et al., 2015; Tram`er et al., 2017; Yasunaga et al., 2018). So far, most existing methods of generating adversarial examples only work for classification tasks (Jia and Liang, 2017; Wang et al., 2019) and are not designed for structured prediction tasks. However, since many structured prediction tasks such as partof-speech (POS) tagging and dependency parsing are essential building blocks of many AI systems, it is"
2020.emnlp-main.182,2020.acl-main.333,1,0.720944,"ed in Equation 1, the consensus prediction of parsers B and C is regarded as ground truth, no matter whether the prediction is actually right or wrong. Thus parsers B and C should have high accuracy and also different inductive biases so that they are unlikely to make the same mistake. In addition, B and C should not be too similar to parser A, because otherwise the first two terms in Equation 1 would become hard to optimize. 3.2 Evaluation Criteria for Sentence Quality We consider two aspects of the sentence quality as follows: • Fluency: Inspired by Holtzman et al. (2018); Xu et al. (2018); Pang et al. (2020), we use • Meaning Preservation: Adversarial examples that differ too much from the original sentences are less effective in attacks because humans can easily identify them. We use BERTScore (Zhang et al., 2019b) as another reward in learning to evaluate the similarity between two sentences at the meaning level. We choose to use BERTScore because it correlates better with human judgments than traditional measures such as BLEU (Papineni et al., 2002). sm (x, x ˆ) = BERT Score(x, x ˆ) By maximizing these criteria, we hope to make the adversarial examples look more like human generated sentences"
2020.emnlp-main.182,P02-1040,0,0.106776,"eria for Sentence Quality We consider two aspects of the sentence quality as follows: • Fluency: Inspired by Holtzman et al. (2018); Xu et al. (2018); Pang et al. (2020), we use • Meaning Preservation: Adversarial examples that differ too much from the original sentences are less effective in attacks because humans can easily identify them. We use BERTScore (Zhang et al., 2019b) as another reward in learning to evaluate the similarity between two sentences at the meaning level. We choose to use BERTScore because it correlates better with human judgments than traditional measures such as BLEU (Papineni et al., 2002). sm (x, x ˆ) = BERT Score(x, x ˆ) By maximizing these criteria, we hope to make the adversarial examples look more like human generated sentences and not differ too much from the original sentences in meaning. 3.3 Sentence Generator We propose to use a seq2seq model (Wang et al., 2016) as the adversarial sentence generator, which has been widely used in machine translation, dialogue and many other areas. The seq2seq model specifies P (ˆ x|x; Θ), the conditional probability of generating an adversarial sentence x ˆ given an input sentence x. We train the model by reinforcement learning guided"
2020.emnlp-main.182,N03-1033,0,0.275871,"alue is calculated by using the one-tailed sign test with bootstrap resampling on 50 samples following Chollampatt, Wang, and Ng (2019). We compare the attacking success rate with and without retraining. The p-values (5.42e20 at the token level and 3.39e-21 at the sentence level) show that the improvement is significant. 5 Experiments on POS Tagging 5.1 Experimental Setup In this section, we apply our method to the part-ofspeech tagging task using the tagger from Ma and Hovy (2016) as the victim model. For the reference taggers, we choose two state-of-the-art taggers: Stanford POS tagger from Toutanova et al. (2003) and Senna tagger from Collobert et al. (2011). All the hyper-parameters of the three taggers are the same as reported in their papers. We conduct the experiments on the PTB dataset. Similar to dependency parsing, the word level approach in section 2.3 is the baseline. For the adversarial example generator, we use the same structure and pretrain strategy as Section 4.4, except that the dimension of hidden state is set to 512. We train the sentence generator using reinforcement learning with hyper-parameter α = 1, β = 0.001, γ = 30. Adam(Kingma and Ba, 2014) is used to optimize the parameters w"
2020.emnlp-main.182,D16-1058,0,0.196075,"Missing"
2020.emnlp-main.182,N18-1089,0,0.0187015,"that elicit large changes in the output, have been shown to be an effective way of assessing the robustness of models in natural language processing (NLP) (Jia and Liang, 2017; Belinkov and Bisk, 2018; Hosseini et al., 2017; Samanta and Mehta, 2017; Alzantot et al., † min log P(y =?|ˆ x + r) ∗ Introduction ∗ ˆ1 x ˆ2 x ˆ3 x ˆ4 x x4 2018; Ebrahimi et al., 2018; Michel et al., 2019; Wang et al., 2019). Adversarial training, in which models are trained on adversarial examples, has also been shown to improve the accuracy and robustness of NLP models (Goodfellow et al., 2015; Tram`er et al., 2017; Yasunaga et al., 2018). So far, most existing methods of generating adversarial examples only work for classification tasks (Jia and Liang, 2017; Wang et al., 2019) and are not designed for structured prediction tasks. However, since many structured prediction tasks such as partof-speech (POS) tagging and dependency parsing are essential building blocks of many AI systems, it is important to study adversarial attack (generating adversarial examples) and defense (adversarial training) of structured prediction models. There are multiple challenges that have to be addressed in building an efficient and effective attac"
2020.emnlp-main.182,P19-1559,0,0.165774,"adversarial examples only work for classification tasks (Jia and Liang, 2017; Wang et al., 2019) and are not designed for structured prediction tasks. However, since many structured prediction tasks such as partof-speech (POS) tagging and dependency parsing are essential building blocks of many AI systems, it is important to study adversarial attack (generating adversarial examples) and defense (adversarial training) of structured prediction models. There are multiple challenges that have to be addressed in building an efficient and effective attacker for structured prediction models in NLP. Zhang et al. (2019a) pointed out two major prob2327 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2327–2338, c November 16–20, 2020. 2020 Association for Computational Linguistics lems encountered by attackers of NLP tasks. First, since words are discrete, making small disturbances to words in the gradient direction is difficult. Secondly, there is no guarantee that the generated adversarial examples are fluent. In addition to these two problems, there is a unique challenge faced by attackers of structured prediction tasks. While small perturbations to images or e"
2020.emnlp-main.182,D18-1316,0,0.0347452,"ea of adding continuous perturbations to inputs has been applied to tasks in NLP (Sato et al., 2018; Gong et al., 2018). In order to solve the mapping problem from the modified word vector to the word, Papernot et al. (2016) built a special dictionary to select words to replace the original words. In addition to replacement manipulation, Samanta and Mehta (2017) introduced three modification strategies: removal and addition. Michel et al. (2019) leveraged atomic character-level operation. Some attack strategies to generate adversarial examples have been proposed in the sentence level setting. Zhao et al. (2018) searched adversarial examples in the continuous vector space and then used generative adversarial networks (Goodfellow et al., 2014) to map the fixed-length vectors to data instances. However, these attackers are only designed for classification tasks or generation tasks and can not be easily applied to structured prediction systems. Attack Design on Structured Prediction Model There is also some prior work on attacking structured prediction models. Cisse et al. (2017) proposed to attack structured prediction models in the 2334 Baseline Ours Generation Fluency (Perplexity ↓) 377.36 244.69 Tok"
2020.emnlp-main.258,P18-1013,0,0.0238395,"om the WFA except those that can be reached from states of other REs, and finally convert the WFA back to an FA-RNN. 7 Related Work Neural Networks Enhanced by Rules Hu et al. (2016); Li and Rush (2020) use rules to constrain neural networks by knowledge distillation and posterior regularization. Awasthi et al. (2020) inject rule knowledge into neural networks using multitask learning. Lin et al. (2020) train a trigger matching network using additional annotation and use the output of trigger matching results as the attention of a sequence labeler. Rocktäschel et al. (2015); Xu et al. (2018); Hsu et al. (2018) use parsed rule results to regularize neural network predictions by additional loss terms. Li and Srikumar (2019); Luo et al. (2018) inject declarative knowledge in the form of parsed RE results or first-order expressions into neural networks by hacking the prediction logits or the attention scores. Hu et al. (2016); Hsu et al. (2018) use rules as additional input features. 3200 All these previous methods use matching results or truth values of rules to enhance existing neural models. In contrast, we directly turn REs into a novel type of trainable networks. thank Dongwu Lin for his support i"
2020.emnlp-main.258,2020.acl-main.752,0,0.0500857,"Missing"
2020.emnlp-main.258,N19-1024,0,0.019456,"sually in the manner of regularization via knowledge distillation (Hu et al., 2016) and multi-task learning (Awasthi et al., 2020; Xu et al., 2018), or by tuning the output logits of neural networks (Li and Srikumar, 2019; Luo et al., 2018). In this way, information from rules can be injected into neural networks, though the neural networks still require training and remain black boxes that are hard to interpret and manipulate. Another way of utilizing rules is to design novel neural network architectures inspired by rule systems (Schwartz et al., 2018; Graves et al., 2014; Peng et al., 2018; Lin et al., 2019). Models designed based on this idea usually achieve better interpretability, but they must be trained on labeled data and cannot be directly converted from rules or manually specified by human experts because of their structural differences from rule systems. In this paper, we propose finite-automaton recurrent neural networks (FA-RNN), a novel type of recurrent neural networks that is designed based on the computation process of weighted finite-state automata. Because of the equivalence between 3193 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages"
2020.emnlp-main.258,P18-1194,0,0.167567,"gs and remain very competitive in rich-resource settings. 1 Introduction Over the past several years, neural network approaches have rapidly gained popularity in natural language processing (NLP) because of their impressive performance and flexible modeling capacity. Nevertheless, symbolic rules are still an indispensable tool in various industrial NLP applications. Regular expressions (RE) are one of the most representative and useful forms of symbolic rules and are widely used for solving tasks such as pattern matching (Hosoya and Pierce, 2001; Zhang et al., 2018) and intent classification (Luo et al., 2018). RE-based systems are highly interpretable ∗ Corresponding author. and therefore support fine-grained human inspection and manipulation. For example, individual RE rules in a system can be easily added, revised, or removed to quickly adapt the system to changes in the task specification. Moreover, RE-based systems do not require a training stage with labeled data and hence can be quickly deployed with decent performance in zero-shot scenarios. However, REs rely on human experts to write and often have high precision but moderate to low recall; RE-based systems cannot evolve by training on lab"
2020.emnlp-main.258,W19-3901,0,0.0114154,"into a novel type of trainable networks. thank Dongwu Lin for his support in implementing a tool for creating automata and rules. Relating Neural Networks and WFA Schwartz et al. (2018) propose a type of neural networks for learning soft surface patterns (a subset of REs), which is inspired by WFAs but cannot be converted from WFAs or surface patterns. In contrast, our FA-RNN can be initialized from REs and converted back to REs. Peng et al. (2018); Dodge et al. (2019) formulate the update of each hidden dimension of various RNN architectures as a small WFA (2-4 states). Weiss et al. (2018); Merrill (2019) provide theoretical analysis of various neural networks and their accepting languages. Our work differs from these more theoretical studies in that we aim for a practical text classification approach. Omlin et al. (1998); Giles et al. (1999) show the equivalence between WFA and second-order RNN. The main differences between our model and theirs include the following. First, compared with the undecomposed version of our FA-RNN, their RNN model involves nonlinear activation functions which complicate the model. Second, our FA-RNN further decomposes the tensor parameter, integrate word embedding"
2020.emnlp-main.258,D18-1152,0,0.153611,"neural networks, usually in the manner of regularization via knowledge distillation (Hu et al., 2016) and multi-task learning (Awasthi et al., 2020; Xu et al., 2018), or by tuning the output logits of neural networks (Li and Srikumar, 2019; Luo et al., 2018). In this way, information from rules can be injected into neural networks, though the neural networks still require training and remain black boxes that are hard to interpret and manipulate. Another way of utilizing rules is to design novel neural network architectures inspired by rule systems (Schwartz et al., 2018; Graves et al., 2014; Peng et al., 2018; Lin et al., 2019). Models designed based on this idea usually achieve better interpretability, but they must be trained on labeled data and cannot be directly converted from rules or manually specified by human experts because of their structural differences from rule systems. In this paper, we propose finite-automaton recurrent neural networks (FA-RNN), a novel type of recurrent neural networks that is designed based on the computation process of weighted finite-state automata. Because of the equivalence between 3193 Proceedings of the 2020 Conference on Empirical Methods in Natural Languag"
2020.emnlp-main.258,D18-1224,0,0.0251145,"oaches in both zeroshot and low-resource settings and remain very competitive in rich-resource settings. 1 Introduction Over the past several years, neural network approaches have rapidly gained popularity in natural language processing (NLP) because of their impressive performance and flexible modeling capacity. Nevertheless, symbolic rules are still an indispensable tool in various industrial NLP applications. Regular expressions (RE) are one of the most representative and useful forms of symbolic rules and are widely used for solving tasks such as pattern matching (Hosoya and Pierce, 2001; Zhang et al., 2018) and intent classification (Luo et al., 2018). RE-based systems are highly interpretable ∗ Corresponding author. and therefore support fine-grained human inspection and manipulation. For example, individual RE rules in a system can be easily added, revised, or removed to quickly adapt the system to changes in the task specification. Moreover, RE-based systems do not require a training stage with labeled data and hence can be quickly deployed with decent performance in zero-shot scenarios. However, REs rely on human experts to write and often have high precision but moderate to low recall; RE-b"
2020.emnlp-main.485,C18-1139,0,0.0252366,"mation. We use these settings for a better understanding of how the decoders perform on each task when the encoders capture different levels of contextual information. Decoder We use the MaxEnt approach, the traditional CRF approach and AINs with the first-order and factorized second-order CRFs for decoding. We denote these approaches by MaxEnt, CRF, AIN-1O and AIN-F2O respectively. We set the iteration number M to 3 in AINs because we find that more iterations do not result in further improvement in accuracy. Hyper-parameters For the hyper-parameters, we follow the settings of previous work (Akbik et al., 2018). We use Stochastic Gradient Descent for optimization with a fixed learning rate of 0.1 and a batch size of 32. We fix the hidden size of the CNN and BiLSTM layer to 512 and 256 respectively, and the kernel size of CNN to 3. We anneal the learning rate by 0.5 if there is no improvement in the development sets for 10 epochs when training. Evaluation We use F1 score to evaluate the NER, slot filling and chunking tasks and use accuracy to evaluate the POS tagging task. We convert the BIO 6022 format into BIOES format for NERs, slot filling and chunking datasets and use the official release of CoN"
2020.emnlp-main.485,Q17-1010,0,0.0139758,"rsal POS tag annotations with 8 languages for experiments. The list of treebanks is shown in Table 3. We use the standard training/development/test split for experiments. Slot Filling Slot filling is a task that interprets user commands by extracting relevant slots, which can be formulated as a sequence labeling task. We use the Air Travel Information System (ATIS) (Hemphill et al., 1990) dataset for the task. 4 . 3.2 Settings Embeddings For word embeddings in the NER, chunking and slot filling experiments, we use the same word embedding as in Lample et al. (2016) except that we use fastText (Bojanowski et al., 2017) embedding for Dutch which we find significantly improves the accuracy (more than 5 F1 scores on CoNLL NER). We use fastText embeddings for all UD tagging experiments. For character embedding, we use a single layer character CNN with a hidden size of 50, because Yang et al. (2018) empirically showed that it has competitive performance with character LSTM. We concatenate the word embedding and character CNN output for the final word representation. 3 https://lindat.mff.cuni.cz/repository/xmlui/handle/ 11234/1-2837 4 We use the same dataset split as https://github.com/sz128/ slot_filling_and_int"
2020.emnlp-main.485,P17-1178,0,0.0569513,"to a CRF (Lafferty et al., 2001) decoder layer to produce final predictions. The CRF layer is a linear-chain structure that models the relation between neighboring labels. In the traditional CRF approach, exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are applied for training and prediction respectively. In many sequence labeling tasks, the CRF layer leads to better results than the simpler method of predicting each label independently. In practice, we sometimes require very fast sequence labelers for training (e.g., on huge datasets like WikiAnn (Pan et al., 2017)) and prediction (e.g. for low latency online serving). The BiLSTM encoder and the CRF layer both contain sequential computation and require O(n) time over n input words even when parallelized on GPU. A common practice to improve the speed of the encoder is to replace the BiLSTM with a CNN structure (Collobert et al., 2011; Strubell et al., 2017), distill larger encoders into smaller ones (Tsai et al., 2019; Mukherjee and Awadallah, 2020) or in other settings (Tu and Gimpel, 2018; Yang et al., 2018; Tu and Gimpel, 2019; Cui and Zhang, 2019). The CRF layer, however, is more difficult to replace"
2020.emnlp-main.485,D17-1283,0,0.0132944,"ctively. In many sequence labeling tasks, the CRF layer leads to better results than the simpler method of predicting each label independently. In practice, we sometimes require very fast sequence labelers for training (e.g., on huge datasets like WikiAnn (Pan et al., 2017)) and prediction (e.g. for low latency online serving). The BiLSTM encoder and the CRF layer both contain sequential computation and require O(n) time over n input words even when parallelized on GPU. A common practice to improve the speed of the encoder is to replace the BiLSTM with a CNN structure (Collobert et al., 2011; Strubell et al., 2017), distill larger encoders into smaller ones (Tsai et al., 2019; Mukherjee and Awadallah, 2020) or in other settings (Tu and Gimpel, 2018; Yang et al., 2018; Tu and Gimpel, 2019; Cui and Zhang, 2019). The CRF layer, however, is more difficult to replace because of its superior accuracy compared with faster alternatives in many tasks. In order to achieve sublinear time complexity on the CRF layer, we must parallelize the CRF prediction over the tokens. In this paper, we apply Mean-Field Variational Inference (MFVI) to approximately decode the linear-chain CRF. MFVI iteratively passes messages am"
2020.emnlp-main.485,P19-1454,1,0.646152,"e Viterbi decoding and one iteration of our MFVI inference on the CRF model. Yi is the random variable representing the i-th label with three possible values. The illustrated vectors represent Viterbi scores and Qi distributions respectively. i ˜ has the same shape as U in where the matrix U Eq. 2. The factor graph of our factorized secondorder CRF is shown at the bottom of Figure 1. The update formula is similar to that of our first-order approach but with more neighbors: ′ Qm i (yi |x)∝ exp{ψu (x, yi )+s (i−2, i, m) +s(i−1, i, m)+s(i+1, i, m)+s′ (i+2, i, m)} et al., 2016; Chen et al., 2018; Wang et al., 2019) using the MFVI algorithm for solving intractable problems of densely connected probabilistic models to get better accuracy, we propose to employ the MFVI algorithm to accelerate tractable inference of sequence-structured probabilistic models. As far as we know, this is the first attempt of using approximate inference on tractable models for speedup with GPU parallelization. The time complexity of each iteration of the MFVI algorithm is O(nL2 ), which is on par with the time complexity of the exact probabilistic inference algorithms. However, in each iteration, the update of each distribution"
2020.emnlp-main.485,C18-1327,0,0.119662,"sometimes require very fast sequence labelers for training (e.g., on huge datasets like WikiAnn (Pan et al., 2017)) and prediction (e.g. for low latency online serving). The BiLSTM encoder and the CRF layer both contain sequential computation and require O(n) time over n input words even when parallelized on GPU. A common practice to improve the speed of the encoder is to replace the BiLSTM with a CNN structure (Collobert et al., 2011; Strubell et al., 2017), distill larger encoders into smaller ones (Tsai et al., 2019; Mukherjee and Awadallah, 2020) or in other settings (Tu and Gimpel, 2018; Yang et al., 2018; Tu and Gimpel, 2019; Cui and Zhang, 2019). The CRF layer, however, is more difficult to replace because of its superior accuracy compared with faster alternatives in many tasks. In order to achieve sublinear time complexity on the CRF layer, we must parallelize the CRF prediction over the tokens. In this paper, we apply Mean-Field Variational Inference (MFVI) to approximately decode the linear-chain CRF. MFVI iteratively passes messages among neighboring labels to update their distributions locally. Unlike the exact probabilistic inference algorithms, MFVI can be parallelized over different"
2020.findings-emnlp.193,N15-1144,0,0.0291663,"t data in an unsupervised way while being regularized by the source parser. We employ three regularization methods proposed by Jiang et al. (2019) that encourage similarity between model parameters and edge scores respectively of the source and target parsers. Our experiments of transferring from English to 20 target languages show that our method significantly outperforms previous methods. 2 2.1 Method CRF Autoencoder The CRF autoencoder is a framework of unsupervised structured prediction (Ammar et al., 2014) and has been applied to unsupervised parsing (Cai et al., 2017) and POS induction (Lin et al., 2015). It consists of an encoder that predicts a structure 2127 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2127–2133 c November 16 - 20, 2020. 2020 Association for Computational Linguistics (in our case, a dependency parse tree) from the input sentence and a decoder that reconstructs the sentence from the structure. Let x = (x1 , x2 , . . . , xn ) be the input sentence, where xi is the i-th word; let y = (y1 , y2 , . . . , yn ) be the dependency parse tree, where yi is a tuple hhi , pi i in which hi is the index of the dependency head of xi and pi is the POS tag of"
2020.findings-emnlp.193,D11-1006,0,0.0474959,"Missing"
2020.findings-emnlp.193,I08-3008,0,0.0178421,"for this task. We train a source parser and use it to initialize and regularize a target parser that is trained on unannotated target data. We conduct experiments that transfer an English parser to 20 target languages. The results show that our method significantly outperforms previous methods.1 1 Introduction Supervised learning of dependency parsing is difficult for low-resource languages because of the lack of large treebanks. On the other hand, crosslingual adaptation of dependency parsers from richresource languages to low-resource languages has shown a lot of promise (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Xiao and Guo, 2014; Tiedemann, 2015; Schlichtkrull and Søgaard, 2017; Ahmad et al., 2019), especially with the help of cross-lingual word representation (Wu and Dredze, 2019) or part-of-speech (POS) tags (Guo et al., 2015). In this paper, we consider the scenario in which there is only unannotated data for the target language that is not parallel to the source language treebank. A simple strategy is zero-shot transfer or direct transfer, which trains a parser on the source ∗ Corresponding Author Code is available at https://github.com/livc/ cross-crfae. 1 treebank and"
2020.findings-emnlp.193,E17-1063,0,0.0142694,"independently predict each POS tag pˆi in the reconstructed sentence conditioned only on pi , the true POS tag of its dependency head. Our decoder simply specifies a categorical distribun X y∈Y(x) (log P (hi |x) + log P (ˆ pi |pi )) i=1 (5) where Y(x) contains all parse trees of sentence x. We can use Eisner’s algorithm (Eisner, 1996) to find the  best projective dependency parse tree in O n3 time or use Chu-Liu/Edmonds’ algorithm to find the best non-projective dependency parse tree (Chu,  1965; Edmonds, 1967; Tarjan, 1977) in O n2 time. Additionally, we can use the head selection method (Zhang et al., 2017) in O n2 time, which often, but not always, produce a tree structure. 2.1.4 Monolingual Learning In the unsupervised setting, the parse tree y is unknown. We follow Cai et al. (2017) and minimize the negative conditional Viterbi log likelihood as the training loss function: (2) Enc (4) Parsing L=− esj,i P (ˆ pi |pi ) i=1 i where P (hi |x) can be computed by a softmax function on sEnc : n Y N X i=1 xi , y|xi ) max log PΘ,Λ (ˆ y∈Y(xi ) (6) where N is the number of training sentences. Since both the encoding and the decoding probabilities can be factorized (Eq. 2 and 4), we can rewrite Eq. 6 as f"
2020.findings-emnlp.193,E17-1021,0,0.0202471,"arget parser that is trained on unannotated target data. We conduct experiments that transfer an English parser to 20 target languages. The results show that our method significantly outperforms previous methods.1 1 Introduction Supervised learning of dependency parsing is difficult for low-resource languages because of the lack of large treebanks. On the other hand, crosslingual adaptation of dependency parsers from richresource languages to low-resource languages has shown a lot of promise (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Xiao and Guo, 2014; Tiedemann, 2015; Schlichtkrull and Søgaard, 2017; Ahmad et al., 2019), especially with the help of cross-lingual word representation (Wu and Dredze, 2019) or part-of-speech (POS) tags (Guo et al., 2015). In this paper, we consider the scenario in which there is only unannotated data for the target language that is not parallel to the source language treebank. A simple strategy is zero-shot transfer or direct transfer, which trains a parser on the source ∗ Corresponding Author Code is available at https://github.com/livc/ cross-crfae. 1 treebank and then directly applies it to the target language (Schuster et al., 2019; Wang et al., 2019). I"
2020.findings-emnlp.193,N19-1162,0,0.0207981,"emann, 2015; Schlichtkrull and Søgaard, 2017; Ahmad et al., 2019), especially with the help of cross-lingual word representation (Wu and Dredze, 2019) or part-of-speech (POS) tags (Guo et al., 2015). In this paper, we consider the scenario in which there is only unannotated data for the target language that is not parallel to the source language treebank. A simple strategy is zero-shot transfer or direct transfer, which trains a parser on the source ∗ Corresponding Author Code is available at https://github.com/livc/ cross-crfae. 1 treebank and then directly applies it to the target language (Schuster et al., 2019; Wang et al., 2019). In order to leverage unannotated target data, He et al. (2019) propose to employ an unsupervised generative parser that can be trained on the target data while also regularized via soft parameter tying by a source parser. However, generative parsers are known to underperform discriminative parsers in rich-resource scenarios, mostly because of the unrealistic independence assumptions typically made by generative parsers. In fact, He et al. (2019) show that when they use multilingual BERT (Kenton and Toutanova, 2019) as the cross-lingual word representation, their method un"
2020.findings-emnlp.193,W15-2137,0,0.0197185,"nd regularize a target parser that is trained on unannotated target data. We conduct experiments that transfer an English parser to 20 target languages. The results show that our method significantly outperforms previous methods.1 1 Introduction Supervised learning of dependency parsing is difficult for low-resource languages because of the lack of large treebanks. On the other hand, crosslingual adaptation of dependency parsers from richresource languages to low-resource languages has shown a lot of promise (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Xiao and Guo, 2014; Tiedemann, 2015; Schlichtkrull and Søgaard, 2017; Ahmad et al., 2019), especially with the help of cross-lingual word representation (Wu and Dredze, 2019) or part-of-speech (POS) tags (Guo et al., 2015). In this paper, we consider the scenario in which there is only unannotated data for the target language that is not parallel to the source language treebank. A simple strategy is zero-shot transfer or direct transfer, which trains a parser on the source ∗ Corresponding Author Code is available at https://github.com/livc/ cross-crfae. 1 treebank and then directly applies it to the target language (Schuster et"
2020.findings-emnlp.193,D19-1575,0,0.0166228,"ull and Søgaard, 2017; Ahmad et al., 2019), especially with the help of cross-lingual word representation (Wu and Dredze, 2019) or part-of-speech (POS) tags (Guo et al., 2015). In this paper, we consider the scenario in which there is only unannotated data for the target language that is not parallel to the source language treebank. A simple strategy is zero-shot transfer or direct transfer, which trains a parser on the source ∗ Corresponding Author Code is available at https://github.com/livc/ cross-crfae. 1 treebank and then directly applies it to the target language (Schuster et al., 2019; Wang et al., 2019). In order to leverage unannotated target data, He et al. (2019) propose to employ an unsupervised generative parser that can be trained on the target data while also regularized via soft parameter tying by a source parser. However, generative parsers are known to underperform discriminative parsers in rich-resource scenarios, mostly because of the unrealistic independence assumptions typically made by generative parsers. In fact, He et al. (2019) show that when they use multilingual BERT (Kenton and Toutanova, 2019) as the cross-lingual word representation, their method underperforms direct t"
2020.findings-emnlp.193,D19-1077,0,0.0175457,"target languages. The results show that our method significantly outperforms previous methods.1 1 Introduction Supervised learning of dependency parsing is difficult for low-resource languages because of the lack of large treebanks. On the other hand, crosslingual adaptation of dependency parsers from richresource languages to low-resource languages has shown a lot of promise (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Xiao and Guo, 2014; Tiedemann, 2015; Schlichtkrull and Søgaard, 2017; Ahmad et al., 2019), especially with the help of cross-lingual word representation (Wu and Dredze, 2019) or part-of-speech (POS) tags (Guo et al., 2015). In this paper, we consider the scenario in which there is only unannotated data for the target language that is not parallel to the source language treebank. A simple strategy is zero-shot transfer or direct transfer, which trains a parser on the source ∗ Corresponding Author Code is available at https://github.com/livc/ cross-crfae. 1 treebank and then directly applies it to the target language (Schuster et al., 2019; Wang et al., 2019). In order to leverage unannotated target data, He et al. (2019) propose to employ an unsupervised generative"
2020.findings-emnlp.235,W04-0902,0,0.0675687,"lary size and use a ‘UNK’ symbol to represent OOV words. Recent work (Naik et al., 2019) shows that these popular methods do not handle numerals adequately. Wallace et al. (2019) shows that existing word embedding methods can encode numeracy implicitly for high-frequency numerals, but the embedding’s numeracy for OOV numerals is not investigated. Our goal is to design numeral embedding methods that can be integrated into traditional word embedding methods and handle the OOV problem for numerals. Numeracy in natural language Numeral understanding has been found important in textual entailment (Lev et al., 2004; De Marneffe et al., 2008; Roy et al., 2015) and information extraction (Intxaurrondo et al., 2015; Madaan et al., 2016), but existing systems often use manually defined task-specific features and logic rules to identify numerals, which is hard to generalize to other tasks. A lot of research has been done trying to solve math problems, using either manually designed features and rules (Roy et al., 2015; Upadhyay et al., 2016) or sequence-to-sequence neural networks (Wang et al., 2017), but the quantity of numerals is not important in this task and hence existing methods often replace numerals"
2020.findings-emnlp.235,P19-1329,0,0.235665,"PPMI, and then apply dimension reduction techniques such as principle component analysis to produce a lowdimensional representation for each word. The second class of methods (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a,b) use a simple neural network to model the relation between a word and its context within a sliding window in the training corpus. GloVe (Pennington et al., 2014) has been proposed as a method that combines the advantages of both classes. All the above methods have a finite vocabulary size and use a ‘UNK’ symbol to represent OOV words. Recent work (Naik et al., 2019) shows that these popular methods do not handle numerals adequately. Wallace et al. (2019) shows that existing word embedding methods can encode numeracy implicitly for high-frequency numerals, but the embedding’s numeracy for OOV numerals is not investigated. Our goal is to design numeral embedding methods that can be integrated into traditional word embedding methods and handle the OOV problem for numerals. Numeracy in natural language Numeral understanding has been found important in textual entailment (Lev et al., 2004; De Marneffe et al., 2008; Roy et al., 2015) and information extraction"
2020.findings-emnlp.235,D14-1162,0,0.110516,"XXL."" (190 is a reasonable height for size XXL. If we replace 190 with 160, the sentence becomes unreasonable.) Introduction Word embeddings have become an essential building block for deep learning approaches to natural language processing (NLP). The quality of pretrained word embeddings has been shown to significantly impact the performance of neural approaches to a variety of NLP tasks. Over the past two decades, significant progress has been made in the development of word embedding techniques (Lund and Burgess, 1996; Bengio et al., 2003; Bullinaria and Levy, 2007; Mikolov et al., 2013b; Pennington et al., 2014). However, existing word embedding methods do not handle numerals ∗ Corresponding author. • “Jeff is 10, so he should wear size XS."" (10 is an age instead of a height.) If the numerals in the example are OOV or their embeddings are not accurately learned, then it becomes impossible to judge the categories of the numerals or the reasonableness of the sentences. In this paper, we propose two novel methods that can produce reasonable embeddings for any numerals. The key idea is to represent the embedding of a numeral as a weighted average of a small set 2586 Findings of the Association for Comput"
2020.findings-emnlp.236,N19-1078,0,0.0199641,"sed quadrilinear potential function based on the vector representations of two neighboring labels and two neighboring words consistently achieves the best performance. 1 BiLSTM Encoder h1 h2 h3 … hn Word Representations x1 x2 x3 … xn Figure 1: Neural architecture for sequence labeling Introduction Sequence labeling is the task of labeling each token of a sequence. It is an important task in natural language processing and has a lot of applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003; Xin et al., 2018), Named Entity Recognition (NER) (Ritter et al., 2011; Akbik et al., 2019), Chunking (Tjong Kim Sang and Buchholz, 2000; Suzuki et al., 2006). The neural CRF model is one of the most widelyused approaches to sequence labeling and can achieve superior performance on many tasks (Collobert et al., 2011; Chen et al., 2015; Ling et al., 2015; Ma and Hovy, 2016; Lample et al., 2016a). It often employs an encoder such as a BiLSTM to compute the contextual vector representation of each word in the input sequence. The potential function at each position of the input sequence in a neural CRF is typically decomposed into an emission function (of the current label and the vecto"
2020.findings-emnlp.236,D15-1141,0,0.026453,"ral architecture for sequence labeling Introduction Sequence labeling is the task of labeling each token of a sequence. It is an important task in natural language processing and has a lot of applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003; Xin et al., 2018), Named Entity Recognition (NER) (Ritter et al., 2011; Akbik et al., 2019), Chunking (Tjong Kim Sang and Buchholz, 2000; Suzuki et al., 2006). The neural CRF model is one of the most widelyused approaches to sequence labeling and can achieve superior performance on many tasks (Collobert et al., 2011; Chen et al., 2015; Ling et al., 2015; Ma and Hovy, 2016; Lample et al., 2016a). It often employs an encoder such as a BiLSTM to compute the contextual vector representation of each word in the input sequence. The potential function at each position of the input sequence in a neural CRF is typically decomposed into an emission function (of the current label and the vector representation of the current word) and a transition function (of the previous and current labels) (Liu et al., 2018; Yang et al., 2018). ∗ Softmax or various CRFs Kewei Tu and Yong Jiang are the corresponding authors. In this paper, we design"
2020.findings-emnlp.236,D19-1422,0,0.136821,"t sequence. The potential function at each position of the input sequence in a neural CRF is typically decomposed into an emission function (of the current label and the vector representation of the current word) and a transition function (of the previous and current labels) (Liu et al., 2018; Yang et al., 2018). ∗ Softmax or various CRFs Kewei Tu and Yong Jiang are the corresponding authors. In this paper, we design a series of increasingly expressive potential functions for neural CRF models. First, we compute the transition function from label embeddings (Ma et al., 2016; Nam et al., 2016; Cui and Zhang, 2019) instead of label identities. Second, we use a single potential function over the current word and the previous and current labels, instead of decomposing it into the emission and transition functions, leading to more expressiveness. We also employ tensor decomposition in order to keep the potential function tractable. Thirdly, we take the representations of additional neighboring words as input to the potential function, instead of solely relying on the BiLSTM to capture contextual information. To empirically evaluate different approaches, we conduct experiments on four well-known sequence la"
2020.findings-emnlp.356,N19-1078,0,0.122839,"Missing"
2020.findings-emnlp.356,C18-1139,0,0.258865,"ettings, does combining different kinds of contextual embeddings result in a better sequence labeler? Are noncontextual embeddings helpful when the models are equipped with contextual embeddings? 2. When we train models in low-resource and cross-domain settings, do the conclusions from the rich-resource settings still hold? 3. Can sequence labelers automatically learn the importance of each kind of embeddings when they are concatenated? 2 2.1 Introduction In recent years, sequence labelers equipped with contextual embeddings have achieved significant accuracy improvement (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2019; Martin et al., 2019) over approaches that use static non-contextual word embeddings (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014). Different types of embeddings have different inductive biases to guide the learning process. However, little work has been done to study how to concatenate these contextual embeddings and non-contextual embeddings to build better sequence labelers in ∗ Yong Jiang and Kewei Tu are the corresponding authors. ‡ : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. Model Architecture Sequen"
2020.findings-emnlp.356,Q17-1010,0,0.0461714,"t use contextual word embeddings such as ELMo (Peters et al., 2018) since Akbik et al. (2018) showed that concatenating Flair embeddings with ELMo embeddings cannot further improve the accuracy. 2 We do not use the pooled version of Flair due to its slower speed in training. Non-contextual Word Embeddings (NWEs) The most common approach to the NWEs is Word2vec (Mikolov et al., 2013), which is a skipgram model learning word representations by predicting neighboring words. Based on this approach, GloVe (Pennington et al., 2014) creates a co-occurrence matrix for global information and fastText (Bojanowski et al., 2017) represents each word as an n-gram of characters. We use fastText in our experiments as there are pretrained embeddings for 294 languages. Non-contextual Character Embeddings (NCEs) Using character information to represent the embeddings of word is proposed by Santos and Zadrozny (2014) with a lot of following work using a CNN structure to encode character representation (dos Santos and Guimarães, 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016). Lample et al. (2016) utilized BiLSTM on the character sequence of each word. We follow this approach as it usually results in better accuracy (Yang e"
2020.findings-emnlp.356,Q16-1026,0,0.0341466,"esentations by predicting neighboring words. Based on this approach, GloVe (Pennington et al., 2014) creates a co-occurrence matrix for global information and fastText (Bojanowski et al., 2017) represents each word as an n-gram of characters. We use fastText in our experiments as there are pretrained embeddings for 294 languages. Non-contextual Character Embeddings (NCEs) Using character information to represent the embeddings of word is proposed by Santos and Zadrozny (2014) with a lot of following work using a CNN structure to encode character representation (dos Santos and Guimarães, 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016). Lample et al. (2016) utilized BiLSTM on the character sequence of each word. We follow this approach as it usually results in better accuracy (Yang et al., 2018). 3 Experiments and Results For simplicity, we use M to represent M-BERT embeddings, F to represent Flair embeddings, W to represent fastText embeddings, C to represent non-contextual character embeddings, All to represents the concatenation of all types of embeddings and the operator “+” to represent the concatenation operation. We use the MaxEnt approach for all experiments3 . Due to the space limit, some detail"
2020.findings-emnlp.356,L16-1262,0,0.0773898,"Missing"
2020.findings-emnlp.356,P17-1178,0,0.0164893,"M to represent M-BERT embeddings, F to represent Flair embeddings, W to represent fastText embeddings, C to represent non-contextual character embeddings, All to represents the concatenation of all types of embeddings and the operator “+” to represent the concatenation operation. We use the MaxEnt approach for all experiments3 . Due to the space limit, some detailed experiment settings, extra experiments and discussions are included in the appendix. 3.1 Settings Datasets We use datasets from three multilingual sequence labeling tasks over 8 languages in our experiments: WikiAnn NER datasets (Pan et al., 2017), UD Part-Of-Speech (POS) tagging datasets (Nivre et al., 2016), and CoNLL 2003 chunking datasets (Tjong Kim Sang and De Meulder, 2003). We use language-specific fastText and Flair embeddings depending on the dataset. Embedding Concatenation Since experimenting on all 15 concatenation combinations of the four embeddings is not essential for evaluating the effectiveness of each kind of embeddings, we experiment on the following 7 concatenations: F, F+W, 3 We find that the observations from the MaxEnt experiments do not change in all experiments with the CRF approach. 3993 Relative Scores 10 5 0"
2020.findings-emnlp.356,D14-1162,0,0.0877623,"use the Flair embeddings due to their high accuracy for sequence labeling task2 . 1 We do not use contextual word embeddings such as ELMo (Peters et al., 2018) since Akbik et al. (2018) showed that concatenating Flair embeddings with ELMo embeddings cannot further improve the accuracy. 2 We do not use the pooled version of Flair due to its slower speed in training. Non-contextual Word Embeddings (NWEs) The most common approach to the NWEs is Word2vec (Mikolov et al., 2013), which is a skipgram model learning word representations by predicting neighboring words. Based on this approach, GloVe (Pennington et al., 2014) creates a co-occurrence matrix for global information and fastText (Bojanowski et al., 2017) represents each word as an n-gram of characters. We use fastText in our experiments as there are pretrained embeddings for 294 languages. Non-contextual Character Embeddings (NCEs) Using character information to represent the embeddings of word is proposed by Santos and Zadrozny (2014) with a lot of following work using a CNN structure to encode character representation (dos Santos and Guimarães, 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016). Lample et al. (2016) utilized BiLSTM on the character se"
2020.findings-emnlp.356,N18-1202,0,0.293936,". In rich-resources settings, does combining different kinds of contextual embeddings result in a better sequence labeler? Are noncontextual embeddings helpful when the models are equipped with contextual embeddings? 2. When we train models in low-resource and cross-domain settings, do the conclusions from the rich-resource settings still hold? 3. Can sequence labelers automatically learn the importance of each kind of embeddings when they are concatenated? 2 2.1 Introduction In recent years, sequence labelers equipped with contextual embeddings have achieved significant accuracy improvement (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2019; Martin et al., 2019) over approaches that use static non-contextual word embeddings (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014). Different types of embeddings have different inductive biases to guide the learning process. However, little work has been done to study how to concatenate these contextual embeddings and non-contextual embeddings to build better sequence labelers in ∗ Yong Jiang and Kewei Tu are the corresponding authors. ‡ : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. Model"
2021.acl-long.142,N19-1078,0,0.0383627,"l together with the input sentence. Finally, we calculate the negative likelihood loss LNLL and LNLL-EXT together with the CL loss (either LCL-L2 or LCL-KL ). ternal contexts: ˜ = [sep_token; x ˆ1; · · · ; x ˆl] x where sep_token is a special token representing a separate of sentences in the transformer-based pretrained contextual embeddings (for example, “[SEP]” in BERT). 2.2 NER Model We solve the NER task as a sequence labeling problem. We apply a neural model with a CRF layer, which is one of the most popular state-of-the-art approaches to the task (Lample et al., 2016; Ma and Hovy, 2016; Akbik et al., 2019). In the sequence labeling model, the input sentence x is fed into a transformer-based pretrained contextual embeddings model to get the token representations {v1 , · · · , vn } by vi =embedi (x). The token representations are fed into a CRF layer to get the conditional probability pθ (y|x): ψ(y 0 , y, vi ) = exp(WyT vi + by0 ,y ) (1) n Q ψ(yi−1 , yi , vi ) i=1 pθ (y|x) = n P Q 0 , y0 , v ) ψ(yi−1 i i y 0 ∈Y(x) i=1 where ψ is the potential function and θ represents the model parameters. Y(x) denotes the set of all possible label sequences given x. y0 is defined to be a special start symbol. WT"
2021.acl-long.142,C18-1139,0,0.337962,"uch to the Supreme Court . Senate Republicans deployed the nuclear option on Wednesday to drastically reduce the time it takes to confirm hundreds of President Trump s nominees . Label: Group Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of “democrats” and “republican”. Introduction ∗ Retrieved Texts: senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . Label: Non Entity Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP"
2021.acl-long.142,D19-1539,0,0.0937785,"Missing"
2021.acl-long.142,D19-1195,0,0.0140693,"nslation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens. For the re-ranking models, fuzzy match score (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020), attention mechanisms (Cao et al., 2018; Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020; Xu et al., 2020) are usual scoring functions to re-rank the retrieved texts. Instead, we use BERTScore to re-rank the retrieved texts instead as BERTScore evaluates semantic correlations between the texts 1807 based on pretrained contextual embeddings. 6 Multi-View Learning Multi-View Learning is a technique applied to inputs that can be split into multiple subsets. Co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Niyogi, 2005) train a separate model for each view. These approaches are semi-super"
2021.acl-long.142,P18-1015,0,0.0136535,"neural machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens. For the re-ranking models, fuzzy match score (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020), attention mechanisms (Cao et al., 2018; Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020; Xu et al., 2020) are usual scoring functions to re-rank the retrieved texts. Instead, we use BERTScore to re-rank the retrieved texts instead as BERTScore evaluates semantic correlations between the texts 1807 based on pretrained contextual embeddings. 6 Multi-View Learning Multi-View Learning is a technique applied to inputs that can be split into multiple subsets. Co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Niyogi, 2005) train a separate model for each view. These approa"
2021.acl-long.142,D18-1217,0,0.0495751,"at can be split into multiple subsets. Co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Niyogi, 2005) train a separate model for each view. These approaches are semi-supervised learning techniques that require two independent views of the data. The model with higher confidence is applied to construct additional labeled data by predicting on unlabeled data. Sun (2013) and Xu et al. (2013) have extensively studied various multiview learning approaches. Hu et al. (2021) shows the effectiveness of multi-view learning on crosslingual structured prediction tasks. Recently, Clark et al. (2018) proposed Cross-View Training (CVT), which trains a unified model instead of multiple models and targets at minimizing the KL divergence between the probability distributions of the model and auxiliary prediction modules. Comparing with CVT, CL targets at improving the accuracy of two kinds of inputs rather than only one of them. We also propose to minimize the distance of token representations between different views in addition to KL-divergence. Besides, CL utilizes the external contexts and therefore we do not need to construct auxiliary prediction modules in the model. Moreover, CVT cannot"
2021.acl-long.142,P19-1082,0,0.0282248,"rength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieval Retrieving related texts from a certain database (such as the training set) has been widely applied in tasks such as neural machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens. For the re-ranking models, fuzzy match score (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020), attention mechanisms (Cao et al., 2018; Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020; Xu et al., 2020) are usual scoring functions to re-rank the retrieved texts. Instead, we us"
2021.acl-long.142,D18-1111,0,0.0779912,"Table 3: A comparison of different approaches in transfer learning. The models are trained on the CoNLL-03 dataset. Approach CL-L2 CL-KL CL–L2 +S EMI CL-KL+S EMI SE 59.95 61.79 4.1 Comparison of Re-ranking Approaches Various re-ranking approaches may affect the token representations of the model. We compare our approach with three other re-ranking approaches. The first is the ranking from the search engine without any re-ranking approaches. The second is reranking through a fuzzy match score. The approach has been widely applied in a lot of previous work (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020). The third is BERTScore with tf-idf importance weighting which makes rare words more indicative than common words in scoring. We train our models (W / C ONTEXT) with external contexts from these re-ranking approaches and report the averaged and best results on WNUT17 in Table 5. Our results show that re-ranking with BERTScore performs the best, which shows the semantic relevance is helpful for the performance. However, for BERTScore with the tf-idf weighting, the accuracy of the model drops significantly (with p < 0.05). The possible reason might be that the tf-idf weighting"
2021.acl-long.142,2020.acl-main.747,0,0.0789365,"(Liu et al., 2019) for token representations which is the default configuration in the code5 of BERTScore (Zhang et al., 2020). For token representations in the NER model, 2 the accuracy of a query counts 1.0 if all the entities in the query are correctly recognized and 0.0 otherwise. 3 If the descriptions are not available, we use the titles of the results instead. 4 We determined that 6 is a reasonable number based on preliminary experiments. 5 https://github.com/Tiiiger/bert_score 1804 we use pretrained Bio-BERT (Lee et al., 2020) for datasets from the biomedical domain and use XLMRoBERTa (Conneau et al., 2020) for datasets from other domains. Training During training, we fine-tune the pretrained contextual embeddings by AdamW (Loshchilov and Hutter, 2018) optimizer with a batch size of 4. We use a learning rate of 5 × 10−6 to update the parameters in the pretrained contextual embeddings. For the CRF layer parameters, we use a learning rate of 0.05. We train the NER models for 10 epochs for the datasets in Social Media and Biomedical domains while we train the NER models for 5 epochs for other datasets for efficiency as these datasets have more training sentences. 3.2 Results We experiment on the fo"
2021.acl-long.142,2021.acl-long.207,1,0.753104,"ased on pretrained contextual embeddings. 6 Multi-View Learning Multi-View Learning is a technique applied to inputs that can be split into multiple subsets. Co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Niyogi, 2005) train a separate model for each view. These approaches are semi-supervised learning techniques that require two independent views of the data. The model with higher confidence is applied to construct additional labeled data by predicting on unlabeled data. Sun (2013) and Xu et al. (2013) have extensively studied various multiview learning approaches. Hu et al. (2021) shows the effectiveness of multi-view learning on crosslingual structured prediction tasks. Recently, Clark et al. (2018) proposed Cross-View Training (CVT), which trains a unified model instead of multiple models and targets at minimizing the KL divergence between the probability distributions of the model and auxiliary prediction modules. Comparing with CVT, CL targets at improving the accuracy of two kinds of inputs rather than only one of them. We also propose to minimize the distance of token representations between different views in addition to KL-divergence. Besides, CL utilizes the e"
2021.acl-long.142,W17-4418,0,0.101114,"Missing"
2021.acl-long.142,N19-1423,0,0.0665368,"ate Republicans deployed the nuclear option on Wednesday to drastically reduce the time it takes to confirm hundreds of President Trump s nominees . Label: Group Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of “democrats” and “republican”. Introduction ∗ Retrieved Texts: senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . Label: Non Entity Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/CLNER. ‡ Input Sentence: in wh"
2021.acl-long.142,P19-1236,0,0.15668,"ar dataset for NER. CoNLL++ is a revision of the CoNLL-03 datasets. Wang et al. (2019) fixed annotation errors on the test set by professional annotators and improved the quality of the training data through their CrossWeigh approach. We use the standard dataset split for these datasets. • Biomedical: We use BC5CDR (Li et al., 2016) and NCBI-disease (Do˘gan et al., 2014) datasets, which are two popular biomedical NER datasets. We merge the training and development data as training set following Nooralahzadeh et al. (2019). • Science and Technology: We use CBS SciTech News dataset collected by Jia et al. (2019). The dataset only contains the test set with the same label set as the CoNLL-03 dataset. We use the dataset to evaluate the effectiveness of crossdomain transferability from the news domain. • E-commerce: We collect and annotate an internal dataset from one anonymous E-commerce website. The dataset contains 25 named entity labels for goods in short texts. We also collect 300,000 unlabeled sentences for semi-supervised training. We show the statistics of the datasets in Table 1. Annotations of the E-commerce dataset We manually labeled the user queries through crowdsourcing from www.aliexpress"
2021.acl-long.142,2020.coling-main.207,0,0.0253611,"t work (Yu et al., 2020; Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieval Retrieving related texts from a certain database (such as the training set) has been widely applied in tasks such as neural machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens. For the re-ranking models, fuzzy match score (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020), attention mechanisms (Cao et al., 2018; Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020; Xu et al., 2020) are usual sco"
2021.acl-long.142,N16-1030,0,0.0914812,"ds the texts to a transformer-based model together with the input sentence. Finally, we calculate the negative likelihood loss LNLL and LNLL-EXT together with the CL loss (either LCL-L2 or LCL-KL ). ternal contexts: ˜ = [sep_token; x ˆ1; · · · ; x ˆl] x where sep_token is a special token representing a separate of sentences in the transformer-based pretrained contextual embeddings (for example, “[SEP]” in BERT). 2.2 NER Model We solve the NER task as a sequence labeling problem. We apply a neural model with a CRF layer, which is one of the most popular state-of-the-art approaches to the task (Lample et al., 2016; Ma and Hovy, 2016; Akbik et al., 2019). In the sequence labeling model, the input sentence x is fed into a transformer-based pretrained contextual embeddings model to get the token representations {v1 , · · · , vn } by vi =embedi (x). The token representations are fed into a CRF layer to get the conditional probability pθ (y|x): ψ(y 0 , y, vi ) = exp(WyT vi + by0 ,y ) (1) n Q ψ(yi−1 , yi , vi ) i=1 pθ (y|x) = n P Q 0 , y0 , v ) ψ(yi−1 i i y 0 ∈Y(x) i=1 where ψ is the potential function and θ represents the model parameters. Y(x) denotes the set of all possible label sequences given x. y0 is"
2021.acl-long.142,2020.acl-main.45,0,0.0391605,"Missing"
2021.acl-long.142,2021.ccl-1.108,0,0.0533332,"Missing"
2021.acl-long.142,2020.coling-main.78,0,0.312318,"• CL-L2 represents minimizing the L2 distance between token representations (Eq. 5). • CL-KL represents minimizing the KL divergence (Eq. 8) between CRF output distributions. Besides, we also compare our approaches with previous state-of-the-art approaches over entity-level F1 scores6 . During the evaluation, our approaches are evaluated using inputs without external contexts (W / O C ONTEXT) and inputs with them (W / C ONTEXT). We report the results averaged over 5 runs in our experiments. The results are listed in 6 We do not compare the results from previous work such as Yu et al. (2020); Luoma and Pyysalo (2020); Yamada et al. (2020) that utilizes the document-level contexts in CoNLL-03 NER here. We conduct a comparison with these approaches in Appendix A. Table 27 . With the external contexts, our models with CL outperform previous state-of-the-art approaches on most of the datasets. Our approaches significantly outperform the baseline that is trained without external contexts with only one exception. Comparing with LUKE, our approaches and our baseline outperform LUKE in all the cases. The possible reason is that LUKE is pretrained only using long word sequences, which makes the model prone to fail"
2021.acl-long.142,P16-1101,0,0.175555,"nsformer-based model together with the input sentence. Finally, we calculate the negative likelihood loss LNLL and LNLL-EXT together with the CL loss (either LCL-L2 or LCL-KL ). ternal contexts: ˜ = [sep_token; x ˆ1; · · · ; x ˆl] x where sep_token is a special token representing a separate of sentences in the transformer-based pretrained contextual embeddings (for example, “[SEP]” in BERT). 2.2 NER Model We solve the NER task as a sequence labeling problem. We apply a neural model with a CRF layer, which is one of the most popular state-of-the-art approaches to the task (Lample et al., 2016; Ma and Hovy, 2016; Akbik et al., 2019). In the sequence labeling model, the input sentence x is fed into a transformer-based pretrained contextual embeddings model to get the token representations {v1 , · · · , vn } by vi =embedi (x). The token representations are fed into a CRF layer to get the conditional probability pθ (y|x): ψ(y 0 , y, vi ) = exp(WyT vi + by0 ,y ) (1) n Q ψ(yi−1 , yi , vi ) i=1 pθ (y|x) = n P Q 0 , y0 , v ) ψ(yi−1 i i y 0 ∈Y(x) i=1 where ψ is the potential function and θ represents the model parameters. Y(x) denotes the set of all possible label sequences given x. y0 is defined to be a spe"
2021.acl-long.142,2020.emnlp-demos.2,0,0.0268364,"Missing"
2021.acl-long.142,2020.emnlp-main.107,0,0.0631329,"Missing"
2021.acl-long.142,D19-6125,0,0.0628019,"ulder, 2003) dataset and CoNLL++ (Wang et al., 2019) dataset. The CoNLL-03 dataset is the most popular dataset for NER. CoNLL++ is a revision of the CoNLL-03 datasets. Wang et al. (2019) fixed annotation errors on the test set by professional annotators and improved the quality of the training data through their CrossWeigh approach. We use the standard dataset split for these datasets. • Biomedical: We use BC5CDR (Li et al., 2016) and NCBI-disease (Do˘gan et al., 2014) datasets, which are two popular biomedical NER datasets. We merge the training and development data as training set following Nooralahzadeh et al. (2019). • Science and Technology: We use CBS SciTech News dataset collected by Jia et al. (2019). The dataset only contains the test set with the same label set as the CoNLL-03 dataset. We use the dataset to evaluate the effectiveness of crossdomain transferability from the news domain. • E-commerce: We collect and annotate an internal dataset from one anonymous E-commerce website. The dataset contains 25 named entity labels for goods in short texts. We also collect 300,000 unlabeled sentences for semi-supervised training. We show the statistics of the datasets in Table 1. Annotations of the E-comme"
2021.acl-long.142,N18-1202,0,0.0156664,"ibuster and confirm Neil Gorsuch to the Supreme Court . Senate Republicans deployed the nuclear option on Wednesday to drastically reduce the time it takes to confirm hundreds of President Trump s nominees . Label: Group Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of “democrats” and “republican”. Introduction ∗ Retrieved Texts: senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . Label: Non Entity Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at htt"
2021.acl-long.142,W16-3919,0,0.0224422,"Missing"
2021.acl-long.142,W02-2024,0,0.403859,"Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018, 2019; Wang et al., 2020b). Recently, the improvement of accuracy mainly benefits from stronger token representations such as pretrained contextual embeddings such as BERT (Devlin et al., 2019), Flair (Akbik et al., 2018) and LUKE (Yamada et al., 2020). Very recent work (Yu et al., 2020; Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieval Retrieving related texts from a certain database (such as the training set) has been widely applied in tasks such as neural machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve th"
2021.acl-long.142,2020.acl-main.304,1,0.865963,"(θ) = LCL-L2 (θ) + LCL-KL (θ) in Eq. 9). Results in Table 7 show that the external context can help to improve the accuracy even when the NER model is trained without the contexts. However, when the model is trained with the external contexts, the accuracy of the model Related Work Named Entity Recognition Named Entity Recognition (Sundheim, 1995) has been studied for decades. Most of the work takes NER as a sequence labeling problem and applies the linear-chain CRF (Lafferty et al., 2001) to achieve state-of-the-art accuracy (Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018, 2019; Wang et al., 2020b). Recently, the improvement of accuracy mainly benefits from stronger token representations such as pretrained contextual embeddings such as BERT (Devlin et al., 2019), Flair (Akbik et al., 2018) and LUKE (Yamada et al., 2020). Very recent work (Yu et al., 2020; Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieva"
2021.acl-long.142,2021.acl-long.206,1,0.769048,"Missing"
2021.acl-long.142,2020.findings-emnlp.356,1,0.898596,"(θ) = LCL-L2 (θ) + LCL-KL (θ) in Eq. 9). Results in Table 7 show that the external context can help to improve the accuracy even when the NER model is trained without the contexts. However, when the model is trained with the external contexts, the accuracy of the model Related Work Named Entity Recognition Named Entity Recognition (Sundheim, 1995) has been studied for decades. Most of the work takes NER as a sequence labeling problem and applies the linear-chain CRF (Lafferty et al., 2001) to achieve state-of-the-art accuracy (Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018, 2019; Wang et al., 2020b). Recently, the improvement of accuracy mainly benefits from stronger token representations such as pretrained contextual embeddings such as BERT (Devlin et al., 2019), Flair (Akbik et al., 2018) and LUKE (Yamada et al., 2020). Very recent work (Yu et al., 2020; Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieva"
2021.acl-long.142,2021.acl-long.46,1,0.801203,"Missing"
2021.acl-long.142,D19-1519,0,0.105801,"‡ Input Sentence: in which document-level contexts are unavailable in practice. For example, there are sometimes no available contexts in users’ search queries, tweets and short comments in various domains such as social media and E-commerce domains. When professional annotators annotate ambiguous named entities in such cases, they usually rely on domain knowledge for disambiguation. This kind of knowledge can often be found through a search engine. Moreover, when the annotators are not sure about a certain entity, they are usually encouraged to find related knowledge through a search engine (Wang et al., 2019). Therefore, we believe that NER models can benefit from such a process as well. In this paper, we propose to improve NER models by retrieving texts related to the input sentence by an off-the-shelf search engine. We re-rank the retrieved texts according to their semantic relevance to the input sentence and select several top-ranking texts as the external contexts. Consequently, we concatenate the input sentence and external contexts together as a new retrieval-based input view and feed it to the pretrained contextual embedding 1800 Proceedings of the 59th Annual Meeting of the Association for"
2021.acl-long.142,2020.acl-main.144,0,0.18718,"n of different approaches in transfer learning. The models are trained on the CoNLL-03 dataset. Approach CL-L2 CL-KL CL–L2 +S EMI CL-KL+S EMI SE 59.95 61.79 4.1 Comparison of Re-ranking Approaches Various re-ranking approaches may affect the token representations of the model. We compare our approach with three other re-ranking approaches. The first is the ranking from the search engine without any re-ranking approaches. The second is reranking through a fuzzy match score. The approach has been widely applied in a lot of previous work (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020). The third is BERTScore with tf-idf importance weighting which makes rare words more indicative than common words in scoring. We train our models (W / C ONTEXT) with external contexts from these re-ranking approaches and report the averaged and best results on WNUT17 in Table 5. Our results show that re-ranking with BERTScore performs the best, which shows the semantic relevance is helpful for the performance. However, for BERTScore with the tf-idf weighting, the accuracy of the model drops significantly (with p < 0.05). The possible reason might be that the tf-idf weighting gives high weight"
2021.acl-long.142,2020.emnlp-main.523,0,0.252158,"Group Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of “democrats” and “republican”. Introduction ∗ Retrieved Texts: senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . Label: Non Entity Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/CLNER. ‡ Input Sentence: in which document-level contexts are unavailable in practice. For example, there are sometimes no available contexts in users’ search queries, tweets and short"
2021.acl-long.142,2020.acl-main.577,0,0.336047,"nominees . Label: Group Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of “democrats” and “republican”. Introduction ∗ Retrieved Texts: senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . Label: Non Entity Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/CLNER. ‡ Input Sentence: in which document-level contexts are unavailable in practice. For example, there are sometimes no available contexts in users’ search quer"
2021.acl-long.142,N18-1120,0,0.146753,"core. BS: BERTScore. Table 3: A comparison of different approaches in transfer learning. The models are trained on the CoNLL-03 dataset. Approach CL-L2 CL-KL CL–L2 +S EMI CL-KL+S EMI SE 59.95 61.79 4.1 Comparison of Re-ranking Approaches Various re-ranking approaches may affect the token representations of the model. We compare our approach with three other re-ranking approaches. The first is the ranking from the search engine without any re-ranking approaches. The second is reranking through a fuzzy match score. The approach has been widely applied in a lot of previous work (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020). The third is BERTScore with tf-idf importance weighting which makes rare words more indicative than common words in scoring. We train our models (W / C ONTEXT) with external contexts from these re-ranking approaches and report the averaged and best results on WNUT17 in Table 5. Our results show that re-ranking with BERTScore performs the best, which shows the semantic relevance is helpful for the performance. However, for BERTScore with the tf-idf weighting, the accuracy of the model drops significantly (with p < 0.05). The possible reason might be that"
2021.acl-long.142,P19-1336,0,0.0214626,"Missing"
2021.acl-long.142,W18-5713,0,0.0243972,"l., 2020). Very recent work (Yu et al., 2020; Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieval Retrieving related texts from a certain database (such as the training set) has been widely applied in tasks such as neural machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens. For the re-ranking models, fuzzy match score (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020), attention mechanisms (Cao et al., 2018; Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020; Xu et al.,"
2021.acl-long.206,N19-1078,0,0.0939277,"Missing"
2021.acl-long.206,J88-1003,0,0.685257,"CE can find a strong word representation on a single GPU with only a few GPU-hours for structured prediction tasks. In comparison, a lot of NAS approaches require dozens or even thousands of GPU-hours to search for good neural architectures for their corresponding tasks. Empirical results show that ACE outperforms strong baselines. Furthermore, when ACE is applied to concatenate pretrained contextualized embeddings fine-tuned on specific tasks, we can achieve state-of-the-art accuracy on 6 structured prediction tasks including Named Entity Recognition (Sundheim, 1995), Part-Of-Speech tagging (DeRose, 1988), chunking (Tjong Kim Sang and Buchholz, 2000), aspect extraction (Hu and Liu, 2004), syntactic dependency parsing (Tesnière, 1959) and semantic dependency parsing (Oepen et al., 2014) over 21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the baselines and show the advantage of ACE over ensemble models. 2 2.1 Related Work Embeddings Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (Santos and Zadrozny, 2014) are t"
2021.acl-long.206,C18-1139,0,0.623884,"rms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.1 1 Introduction Recent developments on pretrained contextualized embeddings have significantly improved the performance of structured prediction tasks in natural ∗ Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/ACE. ‡ language processing. Approaches based on contextualized embeddings, such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), BERT (Devlin et al., 2019), and XLM-R (Conneau et al., 2020), have been consistently raising the state-of-the-art for various structured prediction tasks. Concurrently, research has also showed that word representations based on the concatenation of multiple pretrained contextualized embeddings and traditional non-contextualized embeddings (such as word2vec (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014)) can further improve performance (Peters et al., 2018; Akbik et al., 2018; Straková et al., 2019; Wang et al., 2020b). Given the ever-increasing number of embeddi"
2021.acl-long.206,N19-1423,0,0.618458,"hieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.1 1 Introduction Recent developments on pretrained contextualized embeddings have significantly improved the performance of structured prediction tasks in natural ∗ Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/ACE. ‡ language processing. Approaches based on contextualized embeddings, such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), BERT (Devlin et al., 2019), and XLM-R (Conneau et al., 2020), have been consistently raising the state-of-the-art for various structured prediction tasks. Concurrently, research has also showed that word representations based on the concatenation of multiple pretrained contextualized embeddings and traditional non-contextualized embeddings (such as word2vec (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014)) can further improve performance (Peters et al., 2018; Akbik et al., 2018; Straková et al., 2019; Wang et al., 2020b). Given the ever-increasing number of embedding learning methods that ope"
2021.acl-long.206,D19-1539,0,0.111357,"Missing"
2021.acl-long.206,Q17-1010,0,0.0604138,"diction tasks including Named Entity Recognition (Sundheim, 1995), Part-Of-Speech tagging (DeRose, 1988), chunking (Tjong Kim Sang and Buchholz, 2000), aspect extraction (Hu and Liu, 2004), syntactic dependency parsing (Tesnière, 1959) and semantic dependency parsing (Oepen et al., 2014) over 21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the baselines and show the advantage of ACE over ensemble models. 2 2.1 Related Work Embeddings Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (Santos and Zadrozny, 2014) are trained together with the task and applied in many structured prediction tasks (Ma and Hovy, 2016; Lample et al., 2016; Dozat and Manning, 2018). For pretrained contextualized embeddings, ELMo (Peters et al., 2018), a pretrained contextualized word embedding generated with multiple Bidirectional LSTM layers, significantly outperforms previous state-of-the-art approaches on several NLP tasks. Following this idea, Akbik et al. (2018) proposed Flair embeddings, which is a kind of contextualized character embeddings and"
2021.acl-long.206,W06-2920,0,0.0403544,"ur search design can usually lead to better results compared to both of the baselines. 4.3.2 Comparison With State-of-the-Art approaches As we have shown, ACE has an advantage in searching for better embedding concatenations. We further show that ACE is competitive or even stronger than state-of-the-art approaches. We additionally use XLNet (Yang et al., 2019) and RoBERTa as the candidates of ACE. In some tasks, we have several additional settings to better compare with previous work. In NER, we also conduct a comparison on the revised version of German datasets in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). Recent work such as Yu et al. (2020) and Yamada et al. (2020) utilizes document contexts in the datasets. We follow their work and extract document embeddings for the transformer-based embeddings. Specifically, we follow the fine-tune process of Yamada et al. (2020) to fine-tune the transformer-based embeddings over the document except for BERT and M-BERT embeddings. For BERT and M-BERT, we follow the document extraction process of Yu et al. (2020) because we find that the model with such document embeddings is significantly stronger than the model trained with the fine-tuning process of Yam"
2021.acl-long.206,2020.acl-main.777,0,0.0351125,"Missing"
2021.acl-long.206,D18-1217,0,0.10887,"Missing"
2021.acl-long.206,2020.acl-main.607,1,0.770928,"Missing"
2021.acl-long.206,D16-1139,0,0.0528067,"also find that the PTB dataset used by Mrini et al. (2020) is not identical to the dataset in previous work such as Zhang et al. (2020) and Wang and Tu (2020). ‡ : For reference, we confirmed with the authors of He and Choi (2020) that they used a different data pre-processing script with previous work. . ACE No discount (Eq. 5) Simple (Eq. 4) D EV 93.18 92.98 92.89 T EST 90.00 89.90 89.82 embeddings that are not very useful in the concatenation. Moreover, ACE models can be used to guide the training of weaker models through techniques such as knowledge distillation in structured prediction (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a, 2021b), leading to models that are both stronger and faster. Table 5: Comparison of reward functions. NER POS AE CHK All Random ACE All+Weight Ensemble Ensembledev Ensembletest 92.4 92.6 93.0 92.7 92.2 92.2 92.7 90.6 91.3 91.7 90.4 90.6 90.8 91.4 73.2 74.7 75.6 73.7 68.1 70.2 73.9 96.7 96.7 96.8 96.7 96.5 96.7 96.7 DP SDP UAS LAS ID OOD 96.7 95.1 94.3 90.8 96.8 95.2 94.4 90.8 96.9 95.3 94.5 90.9 96.7 95.1 94.3 90.7 96.1 94.3 94.1 90.3 96.8 95.2 94.3 90.7 96.8 95.2 94.4 90.8 7 Table 6: A comparison among All, Random, ACE, All+Weight and Ensemble. CHK:"
2021.acl-long.206,D19-1279,0,0.0134518,"e each edge represents the inputs and outputs between these nodes. In ACE, we represent each 2645 embedding candidate as a node. The input to the nodes is the input sentence x, and the outputs are the embeddings v l . Since we concatenate the embeddings as the word representation of the task model, there is no connection between nodes in our search space. Therefore, the search space can be significantly reduced. For each node, there are a lot of options to extract word features. Taking BERT embeddings as an example, Devlin et al. (2019) concatenated the last four layers as word features while Kondratyuk and Straka (2019) applied a weighted sum of all twelve layers. However, the empirical results (Devlin et al., 2019) do not show a significant difference in accuracy. We follow the typical usage for each embedding to further reduce the search space. As a result, each embedding only has a fixed operation and the resulting search space contains 2L −1 possible combinations of nodes. In NAS, weight sharing (Pham et al., 2018a) shares the weight of structures in training different neural architectures to reduce the training cost. In comparison, we fixed the weight of pretrained embedding candidates in ACE except for"
2021.acl-long.206,D16-1180,0,0.0966408,"PTB dataset used by Mrini et al. (2020) is not identical to the dataset in previous work such as Zhang et al. (2020) and Wang and Tu (2020). ‡ : For reference, we confirmed with the authors of He and Choi (2020) that they used a different data pre-processing script with previous work. . ACE No discount (Eq. 5) Simple (Eq. 4) D EV 93.18 92.98 92.89 T EST 90.00 89.90 89.82 embeddings that are not very useful in the concatenation. Moreover, ACE models can be used to guide the training of weaker models through techniques such as knowledge distillation in structured prediction (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a, 2021b), leading to models that are both stronger and faster. Table 5: Comparison of reward functions. NER POS AE CHK All Random ACE All+Weight Ensemble Ensembledev Ensembletest 92.4 92.6 93.0 92.7 92.2 92.2 92.7 90.6 91.3 91.7 90.4 90.6 90.8 91.4 73.2 74.7 75.6 73.7 68.1 70.2 73.9 96.7 96.7 96.8 96.7 96.5 96.7 96.7 DP SDP UAS LAS ID OOD 96.7 95.1 94.3 90.8 96.8 95.2 94.4 90.8 96.9 95.3 94.5 90.9 96.7 95.1 94.3 90.7 96.1 94.3 94.1 90.3 96.8 95.2 94.3 90.7 96.8 95.2 94.4 90.8 7 Table 6: A comparison among All, Random, ACE, All+Weight and Ensemble. CHK: chunking. performs al"
2021.acl-long.206,N16-1030,0,0.474791,"pendency parsing (Tesnière, 1959) and semantic dependency parsing (Oepen et al., 2014) over 21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the baselines and show the advantage of ACE over ensemble models. 2 2.1 Related Work Embeddings Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (Santos and Zadrozny, 2014) are trained together with the task and applied in many structured prediction tasks (Ma and Hovy, 2016; Lample et al., 2016; Dozat and Manning, 2018). For pretrained contextualized embeddings, ELMo (Peters et al., 2018), a pretrained contextualized word embedding generated with multiple Bidirectional LSTM layers, significantly outperforms previous state-of-the-art approaches on several NLP tasks. Following this idea, Akbik et al. (2018) proposed Flair embeddings, which is a kind of contextualized character embeddings and achieved strong performance in sequence labeling tasks. Recently, Devlin et al. (2019) proposed BERT, which encodes contextualized sub-word information by Transformers (Vaswani et al., 2017) and s"
2021.acl-long.206,D19-5505,0,0.0269117,"Missing"
2021.acl-long.206,N18-1088,0,0.0142024,"e hyper-parameter of each structure and decide the input order of each structure. Evolutionary algorithms have been applied to architecture search for many decades (Miller et al., 1989; Angeline et al., 1994; Stanley and Miikkulainen, 2002; Floreano et al., 2008; Jozefowicz et al., 2015). The algorithm repeatedly generates new populations through recombination and mutation operations and selects survivors through competing among the population. Recent work with evolutionary algorithms differ in the method on parent/survivor selection and population generation. For example, Real et al. (2017), Liu et al. (2018a), Wistuba (2018) and Real et al. (2019) applied tournament selection (Goldberg and Deb, 1991) for the parent selection while Xie and Yuille (2017) keeps all parents. Suganuma et al. (2017) and Elsken et al. (2018) chose the best model while Real et al. (2019) chose several latest models as survivors. 3 Given an embedding concatenation generated from the controller, the task model is trained over the task data and returns a reward to the controller. The controller receives the reward to update its parameter and samples a new embedding concatenation for the task model. Figure 1 shows the gener"
2021.acl-long.206,2020.emnlp-demos.2,0,0.0370801,"the controller for 50 steps. Table 5 shows that both the discount factor and the binary vector |at − ai |for the task are helpful in both development and test datasets. 4 Please refer to Appendix for more details about the embeddings. 5 We compare ACE with other fine-tuned embeddings in Appendix. 2649 Baevski et al. (2019) Straková et al. (2019) Yu et al. (2020) Yamada et al. (2020) XLM-R+Fine-tune ACE+Fine-tune de 85.1 86.4 87.7 88.3 de06 90.3 91.4 91.7 NER en 93.5 93.4 93.5 94.3 94.1 94.6 es 88.8 90.3 89.3 95.9 nl 92.7 93.7 95.3 95.7 Owoputi et al. (2013) Gui et al. (2017) Gui et al. (2018) Nguyen et al. (2020) XLM-R+Fine-tune ACE+Fine-tune Ritter 90.4 90.9 91.2 90.1 92.3 93.4 POS ARK 93.2 92.4 94.1 93.7 94.4 TB-v2 94.6 92.8 95.2 95.4 95.8 Table 2: Comparison with state-of-the-art approaches in NER and POS tagging. † : Models are trained on both train and development set. C HUNK CoNLL 2000 Akbik et al. (2018) Clark et al. (2018) Liu et al. (2019b) Chen et al. (2020) XLM-R+Fine-tune ACE+Fine-tune AE 14Lap 14Res 15Res 16Res Xu et al. (2018)† Xu et al. (2019) Wang et al. (2020a) Wei et al. (2020) XLM-R+Fine-tune ACE+Fine-tune 96.7 97.0 97.3 95.5 97.0 97.3 84.2 84.3 82.7 85.9 87.4 84.6 87.1 90.5 92.0 72"
2021.acl-long.206,S15-2153,0,0.0506143,"Missing"
2021.acl-long.206,S14-2008,0,0.0159013,"thousands of GPU-hours to search for good neural architectures for their corresponding tasks. Empirical results show that ACE outperforms strong baselines. Furthermore, when ACE is applied to concatenate pretrained contextualized embeddings fine-tuned on specific tasks, we can achieve state-of-the-art accuracy on 6 structured prediction tasks including Named Entity Recognition (Sundheim, 1995), Part-Of-Speech tagging (DeRose, 1988), chunking (Tjong Kim Sang and Buchholz, 2000), aspect extraction (Hu and Liu, 2004), syntactic dependency parsing (Tesnière, 1959) and semantic dependency parsing (Oepen et al., 2014) over 21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the baselines and show the advantage of ACE over ensemble models. 2 2.1 Related Work Embeddings Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (Santos and Zadrozny, 2014) are trained together with the task and applied in many structured prediction tasks (Ma and Hovy, 2016; Lample et al., 2016; Dozat and Manning, 2018). For pretrained contextualized embedding"
2021.acl-long.206,N13-1039,0,0.0434432,"Missing"
2021.acl-long.206,D14-1162,0,0.0909253,"-of-the-art accuracy on 6 structured prediction tasks including Named Entity Recognition (Sundheim, 1995), Part-Of-Speech tagging (DeRose, 1988), chunking (Tjong Kim Sang and Buchholz, 2000), aspect extraction (Hu and Liu, 2004), syntactic dependency parsing (Tesnière, 1959) and semantic dependency parsing (Oepen et al., 2014) over 21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the baselines and show the advantage of ACE over ensemble models. 2 2.1 Related Work Embeddings Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (Santos and Zadrozny, 2014) are trained together with the task and applied in many structured prediction tasks (Ma and Hovy, 2016; Lample et al., 2016; Dozat and Manning, 2018). For pretrained contextualized embeddings, ELMo (Peters et al., 2018), a pretrained contextualized word embedding generated with multiple Bidirectional LSTM layers, significantly outperforms previous state-of-the-art approaches on several NLP tasks. Following this idea, Akbik et al. (2018) proposed Flair embeddings, which is a kind of"
2021.acl-long.206,N18-1202,0,0.0524695,"ow that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.1 1 Introduction Recent developments on pretrained contextualized embeddings have significantly improved the performance of structured prediction tasks in natural ∗ Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/ACE. ‡ language processing. Approaches based on contextualized embeddings, such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), BERT (Devlin et al., 2019), and XLM-R (Conneau et al., 2020), have been consistently raising the state-of-the-art for various structured prediction tasks. Concurrently, research has also showed that word representations based on the concatenation of multiple pretrained contextualized embeddings and traditional non-contextualized embeddings (such as word2vec (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014)) can further improve performance (Peters et al., 2018; Akbik et al., 2018; Straková et al., 2019; Wang et al., 2020b). Given the ever-"
2021.acl-long.206,S15-2082,0,0.0548003,"Missing"
2021.acl-long.206,S14-2004,0,0.0702456,"Missing"
2021.acl-long.206,2020.acl-main.304,1,0.783349,", such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), BERT (Devlin et al., 2019), and XLM-R (Conneau et al., 2020), have been consistently raising the state-of-the-art for various structured prediction tasks. Concurrently, research has also showed that word representations based on the concatenation of multiple pretrained contextualized embeddings and traditional non-contextualized embeddings (such as word2vec (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014)) can further improve performance (Peters et al., 2018; Akbik et al., 2018; Straková et al., 2019; Wang et al., 2020b). Given the ever-increasing number of embedding learning methods that operate on different granularities (e.g., word, subword, or character level) and with different model architectures, choosing the best embeddings to concatenate for a specific task becomes non-trivial, and exploring all possible concatenations can be prohibitively demanding in computing resources. Neural architecture search (NAS) is an active area of research in deep learning to automatically search for better model architectures, and has achieved state-of-the-art performance on various tasks in computer vision, such as im"
2021.acl-long.207,P19-1299,0,0.110715,"situation where some source languages are not as similar to the target language and may lead to worse performance (Rosenstein et al., 2005; Rahimi et al., 2019) (we provide an example in the Appendix A). To tackle this challenging problem, most of the previous works do majority voting (Plank and Agi´c, 2018) and truth inference on hard predictions of multiple sources (Rahimi et al., 2019). To better incorporate target language information, some recent works train a new model on the target unlabeled data with hard/soft predictions from multiple source models, such as mixture-of-experts model (Chen et al., 2019) and knowledge distillation (KD) (Wu et al., 2020), and assign weights to multiple sources based on language similarity. However, these similaritybased approaches are heuristic-based, and cannot well learn the confidence level of multiple source models. In this paper, we propose to leverage a small number of labeled target data to selectively transfer the knowledge from multiple source models. 2661 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2661–2674 August 1–6, 2021."
2021.acl-long.207,D18-1217,0,0.335977,"a new task-specific model in the target language. Both the aggregation model and target task-specific model can map the inputs to the structured outputs but there exists a tradeoff. The aggregation model generally has strong cross-lingual ability since source models are firstly well trained1 , but has lower flexibility since source models are usually frozen. Instead, the target taskspecific model tends to be more flexible and has strong capacity but has poor performance since the model is easily over-fitted on the small training sample. Inspired by previous work on multi/cross-view learning (Clark et al., 2018; Jiang et al., 2019; Fei and Li, 2020), we regard the aggregation model (aggregated source view) and the target taskspecific model (target view) as two views since they both can map the input sentence to structured outputs. We propose a novel multi-view framework to achieve a good trade-off between the two views. To capture the diverse strength and weakness of multiple source models, we propose three approaches to obtain the aggregated source view from language/sentence/sub-structure level in a coarse-to-fine manner. By encouraging two views to influence each other, the proposed framework can"
2021.acl-long.207,2020.acl-main.747,0,0.194106,"Missing"
2021.acl-long.207,J88-1003,0,0.0882324,"ews to interact with each other, our framework can dynamically adjust the confidence level of each source model and improve the performance of both views during training. Experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches, including these with access to additional source language data. 1 Introduction Structured prediction is the task of mapping input sentences to structured outputs. It is a fundamental task in natural language processing and has many applications, i.e., sequence labeling (DeRose, 1988; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al., 2019) and semantic role labeling (van der Plas et al., 2011; Strubell et al., 2018; Cai and Lapata, 2020). ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. To achieve strong performance, structured prediction models mostly require manually labeled data that are costly to obtain in general. Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019;"
2021.acl-long.207,D17-1005,0,0.0176857,"base model for all approaches. We run each approach five times and report the averaged accuracy for POS tagging, f1-score for NER, and unlabelled attachment score (UAS) and labeled attachment score (LAS) for dependency parsing. More details can be found in the Appendix B.1. 3.1 We compare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-theart cross-lingual baselines: direct fine-tuning (DTfinetuning), direct transfer (DT), hard knowledge distillation (hard-KD) (Liu et al., 2017), soft knowledge distillation (soft-KD) (Hinton et al., 2015; Wu et al., 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al., 2018; Akbik et al., 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM. DT-finetuning We directly fine-tune the taskspecific view on fifty labeled data. DT In DT, there is only test data in the target language. Therefore, we evaluate this approach in three ways: 1) using the mean probability distribution of source models (DT-mean); 2) using the maximal probability di"
2021.acl-long.207,D15-1166,0,0.0559339,"el Aggregation We simply introduce a trainable probability vector αlang , which is depicted on the bottom right part of 2663 the Figure 1. The final output distribution of the aggregated source view can be computed as, pS (y|x) = K X (1) (k) αlang k=1 · αsub (xi ) = ] (t) Softmax(hi WKTi ) Then the aggregation distribution becomes, pS (y|x) = n X K Y i=1 k=1 (k) αsub (xi ) · p(k) s (yi |x) In this approach, our target model acts as a selector to dynamically assess the multiple source models on sub-structure level. Sentence-level Aggregation In this section, we leverage an attention mechanism (Luong et al., 2015; Vaswani et al., 2017) to learn the weight of each source model on an input sentence, as shown on the top right part of Figure 1. Firstly, we use the internal states of the [CLS] (t) token as sentence representation. Secondly, h0 from the target model T is used as a query to attend (k) h0 from the k-th source model Sk to produce the probabilities αsent (x) ∈ RK . (1) (t) αsent (x) = Softmax(h0 WKT0 ) where K0 is the concatenation of sentence representations from K source models, and W ∈ Rd×d is the bilinear weight matrix. Then the probabilities are utilized to compute the aggregation distribu"
2021.acl-long.207,D18-1061,0,0.0650358,"Missing"
2021.acl-long.207,P11-2052,0,0.097664,"Missing"
2021.acl-long.207,P19-1015,0,0.453502,"require manually labeled data that are costly to obtain in general. Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019; Hu et al., 2021) recently attracted attention for tackling that problem, by transferring the knowledge from high-resource languages to low-resource ones. Existing works can be categorized into two types: single-source transfer and multi-source transfer. The former is limited to transferring knowledge from one source language and generally results in inferior performance than the latter (McDonald et al., 2011; Rahimi et al., 2019), especially when the target language is similar to multiple source language over various characteristics, i.e., domain, word order, capitalization, and script style. However, in practice, we are more likely to encounter the situation where some source languages are not as similar to the target language and may lead to worse performance (Rosenstein et al., 2005; Rahimi et al., 2019) (we provide an example in the Appendix A). To tackle this challenging problem, most of the previous works do majority voting (Plank and Agi´c, 2018) and truth inference on hard predictions of multiple sources (Rahi"
2021.acl-long.207,D17-1038,0,0.0266652,"d weakness of different source models (see Sec.1 for more discussion.). Sub-structure-level Aggregation We further propose a fine-grained aggregation approach on sub-structure level, which is also based on the attention mechanism. As shown in the left part of Figure 1, for token xi in a given sentence x, (t) we use its representation hi as the query to attend the corresponding representation from each source 3 We also try many metrics of measuring the similarity between two probability distributions, e.g., mean squared error (MSE) (Wu et al., 2020), Cosine, and Jensen-Shannon divergence (JS) (Ruder and Plank, 2017), and we find KL perform best. 2664 2. KD assigns equal importance to multiple source models, which can be seen as a fixed uniform vector in our language-level aggregation approach. 3. Besides language-level aggregation, we propose two fine-grained aggregation strategies to dynamically balance the information from source models. 4. To achieve the previously described goal, our approach has trainable parameters in the aggregation component and our multi-view learning framework can jointly learn the parameters of two views. 2.5 Training and Inference Strategies Following previous work on cross-l"
2021.acl-long.207,P18-1096,0,0.552775,"ompare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-theart cross-lingual baselines: direct fine-tuning (DTfinetuning), direct transfer (DT), hard knowledge distillation (hard-KD) (Liu et al., 2017), soft knowledge distillation (soft-KD) (Hinton et al., 2015; Wu et al., 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al., 2018; Akbik et al., 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM. DT-finetuning We directly fine-tune the taskspecific view on fifty labeled data. DT In DT, there is only test data in the target language. Therefore, we evaluate this approach in three ways: 1) using the mean probability distribution of source models (DT-mean); 2) using the maximal probability distribution of source models over the sub-structure level (DT-max). 3) evaluating each source model and voting on the sub-structure level (DT-vote). We also provide the maximal results of DT on language level (DT-Max(lang)) 6 . Hard-KD The hard knowledge distillation approaches first pred"
2021.acl-long.207,D18-1548,0,0.053178,"Missing"
2021.acl-long.207,W02-2024,0,0.719114,"2020), we conduct the experiments in a leaveone-out setting in which we hold out one language as the target language and the others as the source languages. To simulate the low-resources scenario, for each training set in a specific target language, we randomly select fifty sentences 4 with the gold annotations and discard the annotations of the remaining sentences to construct the training set. We randomly select six languages from Universal Dependencies Treebanks (v2.2)5 for dependency parsing and POS tagging tasks. We use the datasets from CoNLL 2002 and CoNLL 2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for NER tasks. We utilize the base cased multilingual BERT (Devlin et al., 2019) as base model for all approaches. We run each approach five times and report the averaged accuracy for POS tagging, f1-score for NER, and unlabelled attachment score (UAS) and labeled attachment score (LAS) for dependency parsing. More details can be found in the Appendix B.1. 3.1 We compare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-thear"
2021.acl-long.207,Q14-1005,0,0.0892508,"has many applications, i.e., sequence labeling (DeRose, 1988; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al., 2019) and semantic role labeling (van der Plas et al., 2011; Strubell et al., 2018; Cai and Lapata, 2020). ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. To achieve strong performance, structured prediction models mostly require manually labeled data that are costly to obtain in general. Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019; Hu et al., 2021) recently attracted attention for tackling that problem, by transferring the knowledge from high-resource languages to low-resource ones. Existing works can be categorized into two types: single-source transfer and multi-source transfer. The former is limited to transferring knowledge from one source language and generally results in inferior performance than the latter (McDonald et al., 2011; Rahimi et al., 2019), especially when the target language is similar to multiple source language over various characteristics, i.e., domain, word ord"
2021.acl-long.207,2021.acl-long.142,1,0.835839,"Missing"
2021.acl-long.207,2020.acl-main.581,0,0.0860103,"milar to the target language and may lead to worse performance (Rosenstein et al., 2005; Rahimi et al., 2019) (we provide an example in the Appendix A). To tackle this challenging problem, most of the previous works do majority voting (Plank and Agi´c, 2018) and truth inference on hard predictions of multiple sources (Rahimi et al., 2019). To better incorporate target language information, some recent works train a new model on the target unlabeled data with hard/soft predictions from multiple source models, such as mixture-of-experts model (Chen et al., 2019) and knowledge distillation (KD) (Wu et al., 2020), and assign weights to multiple sources based on language similarity. However, these similaritybased approaches are heuristic-based, and cannot well learn the confidence level of multiple source models. In this paper, we propose to leverage a small number of labeled target data to selectively transfer the knowledge from multiple source models. 2661 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2661–2674 August 1–6, 2021. ©2021 Association for Computational Linguistics In"
2021.acl-long.207,D19-1077,0,0.0368048,"Missing"
2021.acl-long.207,2020.repl4nlp-1.16,0,0.0169926,"o it, we focus on the cross-lingual scenario and our two views are a target task-specific model and the aggregation of multiple pre-trained source models. Contextual Multilingual Language Model Trained on massive unlabeled data of hundreds of monolingual corpus, the contextual multilingual models (Devlin et al., 2019; Conneau et al., 2020) learn common representations for multiple languages. Though cross-lingual transfer learning significantly benefits from these models (Pires et al., 2019; Wu and Dredze, 2019b), large gaps still remain between low and high-resources setups (Hu et al., 2020a; Wu and Dredze, 2020). 6 Conclusion We propose a novel multi-view framework to selectively transfer knowledge from multiple sources by utilizing a small amount of labeled dataset. Experimental results show that our approaches achieve state-of-the-art performances on all tasks. Moreover, even compared to approaches with extra resources like source language data, our substructure-level approach still shows significant improvements. Acknowledgement This work was supported by the National Natural Science Foundation of China (61976139) and by Alibaba Group through Alibaba Innovative Research Program. We thank Yuting Zh"
2021.acl-long.207,P95-1026,0,0.702103,"g. More details can be found in the Appendix B.1. 3.1 We compare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-theart cross-lingual baselines: direct fine-tuning (DTfinetuning), direct transfer (DT), hard knowledge distillation (hard-KD) (Liu et al., 2017), soft knowledge distillation (soft-KD) (Hinton et al., 2015; Wu et al., 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al., 2018; Akbik et al., 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM. DT-finetuning We directly fine-tune the taskspecific view on fifty labeled data. DT In DT, there is only test data in the target language. Therefore, we evaluate this approach in three ways: 1) using the mean probability distribution of source models (DT-mean); 2) using the maximal probability distribution of source models over the sub-structure level (DT-max). 3) evaluating each source model and voting on the sub-structure level (DT-vote). We also provide the maximal results of DT on language level (DT-Max(lang))"
2021.acl-long.207,N01-1026,0,0.292596,"language processing and has many applications, i.e., sequence labeling (DeRose, 1988; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al., 2019) and semantic role labeling (van der Plas et al., 2011; Strubell et al., 2018; Cai and Lapata, 2020). ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. To achieve strong performance, structured prediction models mostly require manually labeled data that are costly to obtain in general. Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019; Hu et al., 2021) recently attracted attention for tackling that problem, by transferring the knowledge from high-resource languages to low-resource ones. Existing works can be categorized into two types: single-source transfer and multi-source transfer. The former is limited to transferring knowledge from one source language and generally results in inferior performance than the latter (McDonald et al., 2011; Rahimi et al., 2019), especially when the target language is similar to multiple source language over various characteristics"
2021.acl-long.207,N18-1089,0,0.0259681,"hment score (UAS) and labeled attachment score (LAS) for dependency parsing. More details can be found in the Appendix B.1. 3.1 We compare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-theart cross-lingual baselines: direct fine-tuning (DTfinetuning), direct transfer (DT), hard knowledge distillation (hard-KD) (Liu et al., 2017), soft knowledge distillation (soft-KD) (Hinton et al., 2015; Wu et al., 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al., 2018; Akbik et al., 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM. DT-finetuning We directly fine-tune the taskspecific view on fifty labeled data. DT In DT, there is only test data in the target language. Therefore, we evaluate this approach in three ways: 1) using the mean probability distribution of source models (DT-mean); 2) using the maximal probability distribution of source models over the sub-structure level (DT-max). 3) evaluating each source model and voting on the sub-structure level (DT-vote). We also"
2021.acl-long.209,2020.emnlp-main.389,0,0.0112822,"to-encoder (DIORA) based methods (Drozdov et al., 2019a,b, 2020; Hong et al., 2020; Sahay et al., 2021). They use neural networks to mimic the inside-outside algorithm and they are trained with masked language model objectives. (3) Syntactic distance-based methods (Shen et al., 2018, 2019, 2020). They encode hidden syntactic trees into syntactic distances and inject them into language models. (4) Probing based methods (Kim et al., 2020; Li et al., 2020). They extract phrase-structure trees based on the attention distributions of large pre-trained language models. In addition to these methods, Cao et al. (2020) use constituency tests and Shi et al. (2021) make use of naturally-occurring bracketings such as hyperlinks on webpages to train parsers. Multimodal information such as images (Shi et al., 2019; Zhao and Titov, 2020; Jin and Schuler, 2020) and videos (Zhang et al., 2021) have also been exploited for unsupervised constituency parsing. We are only aware of a few previous studies in unsupervised joint dependency and constituency parsing. Klein and Manning (2004) propose a joint DMV and CCM (Klein and Manning, 2002) model. Shen et al. (2020) propose a transformer-based method, in which they defin"
2021.acl-long.209,2020.emnlp-main.103,0,0.0346536,"ough Eisner and Satta (1999) manage to reduce the complexity to Opl4 |G|q, inference with L-PCFGs is still relatively slow, making them less popular nowadays. Recently, Zhu et al. (2020) combine the ideas of factorizing the binary rule probabilities (Collins, 2003) and neural parameterization (Kim et al., 2019) and propose neural L-PCFGs (NL-PCFGs), achieving good results in both unsupervised dependency and constituency parsing. Neural parameterization is the key to success, which facilitates informed smoothing (Kim et al., 2019), reduces the number of learnable parameters for large grammars (Chiu and Rush, 2020; Yang et al., 2021) and facilitates advanced gradient-based optimization techniques instead of using the traditional EM algorithm (Eisner, 2016). However, Zhu et al. (2020) oversimplify the binary rules to decrease the complexity of the inside/CYK algorithm in learning (i.e., estimating the marginal sentence loglikelihood) and inference. Specifically, they make a strong independence assumption on the generation of the child word such that it is only dependent on the nonterminal symbol. Bilexical dependencies, which have been shown useful in unsupervised dependency parsing (Han et al., 2017; Y"
2021.acl-long.209,N13-1052,0,0.0194152,"Missing"
2021.acl-long.209,P97-1003,0,0.728088,"een an important probabilistic approach to syntactic analysis (Lari and Young, 1990; Jelinek et al., 1992). They assign a probability to each of the parses admitted by CFGs and rank them by the plausibility in such a way that the ambiguity of CFGs can be ameliorated. Still, due to the strong independence assumption of CFGs, vanilla PCFGs (Charniak, 1996) are far from adequate for highly ambiguous text. A common premise for tackling the issue is to incorporate lexical information and weaken the independence assumption. There have been many approaches proposed under the premise (Magerman, 1995; Collins, 1997; Johnson, 1998; Klein and Manning, 2003). Among them lexicalized PCFGs (L-PCFGs) are a relatively straightforward formalism (Collins, 2003). L-PCFGs extend PCFGs by associating a word, i.e., the lexical head, with each grammar symbol. They can thus exploit lexical ˚ Corresponding Author information to disambiguate parsing decisions and are much more expressive than vanilla PCFGs. However, they suffer from representation and inference complexities. For representation, the addition of lexical information greatly increases the number of parameters to be estimated and exacerbates the data sparsit"
2021.acl-long.209,J03-4003,0,0.555463,"ch of the parses admitted by CFGs and rank them by the plausibility in such a way that the ambiguity of CFGs can be ameliorated. Still, due to the strong independence assumption of CFGs, vanilla PCFGs (Charniak, 1996) are far from adequate for highly ambiguous text. A common premise for tackling the issue is to incorporate lexical information and weaken the independence assumption. There have been many approaches proposed under the premise (Magerman, 1995; Collins, 1997; Johnson, 1998; Klein and Manning, 2003). Among them lexicalized PCFGs (L-PCFGs) are a relatively straightforward formalism (Collins, 2003). L-PCFGs extend PCFGs by associating a word, i.e., the lexical head, with each grammar symbol. They can thus exploit lexical ˚ Corresponding Author information to disambiguate parsing decisions and are much more expressive than vanilla PCFGs. However, they suffer from representation and inference complexities. For representation, the addition of lexical information greatly increases the number of parameters to be estimated and exacerbates the data sparsity problem during learning, so the expectation-maximisation (EM) based estimation of L-PCFGs has to rely on sophisticated smoothing technique"
2021.acl-long.209,2020.emnlp-main.392,0,0.0455539,"Missing"
2021.acl-long.209,N19-1116,0,0.103606,"methods, our method does not require gold/induced POS tags or sophisticated initializations, though its performance lags behind some of these previous methods. Recent unsupervised constituency parsers can be roughly categorized into the following groups: (1) PCFG-based methods. Depth-bounded PCFGs (Jin et al., 2018a,b) limit the stack depth of centerembedding. Neurally parameterized PCFGs (Jin 2695 et al., 2019; Kim et al., 2019; Zhu et al., 2020; Yang et al., 2021) use neural networks to produce grammar rule probabilities. (2) Deep Inside-Outside Recursive Auto-encoder (DIORA) based methods (Drozdov et al., 2019a,b, 2020; Hong et al., 2020; Sahay et al., 2021). They use neural networks to mimic the inside-outside algorithm and they are trained with masked language model objectives. (3) Syntactic distance-based methods (Shen et al., 2018, 2019, 2020). They encode hidden syntactic trees into syntactic distances and inject them into language models. (4) Probing based methods (Kim et al., 2020; Li et al., 2020). They extract phrase-structure trees based on the attention distributions of large pre-trained language models. In addition to these methods, Cao et al. (2020) use constituency tests and Shi et al"
2021.acl-long.209,W16-5901,0,0.113848,"wadays. Recently, Zhu et al. (2020) combine the ideas of factorizing the binary rule probabilities (Collins, 2003) and neural parameterization (Kim et al., 2019) and propose neural L-PCFGs (NL-PCFGs), achieving good results in both unsupervised dependency and constituency parsing. Neural parameterization is the key to success, which facilitates informed smoothing (Kim et al., 2019), reduces the number of learnable parameters for large grammars (Chiu and Rush, 2020; Yang et al., 2021) and facilitates advanced gradient-based optimization techniques instead of using the traditional EM algorithm (Eisner, 2016). However, Zhu et al. (2020) oversimplify the binary rules to decrease the complexity of the inside/CYK algorithm in learning (i.e., estimating the marginal sentence loglikelihood) and inference. Specifically, they make a strong independence assumption on the generation of the child word such that it is only dependent on the nonterminal symbol. Bilexical dependencies, which have been shown useful in unsupervised dependency parsing (Han et al., 2017; Yang et al., 2020), are thus ignored. 2688 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th In"
2021.acl-long.209,P99-1059,0,0.480148,"cisions and are much more expressive than vanilla PCFGs. However, they suffer from representation and inference complexities. For representation, the addition of lexical information greatly increases the number of parameters to be estimated and exacerbates the data sparsity problem during learning, so the expectation-maximisation (EM) based estimation of L-PCFGs has to rely on sophisticated smoothing techniques and factorizations (Collins, 2003). As for inference, the CYK algorithm for L-PCFGs has a Opl5 |G|q complexity, where l is the sentence length and |G |is the grammar constant. Although Eisner and Satta (1999) manage to reduce the complexity to Opl4 |G|q, inference with L-PCFGs is still relatively slow, making them less popular nowadays. Recently, Zhu et al. (2020) combine the ideas of factorizing the binary rule probabilities (Collins, 2003) and neural parameterization (Kim et al., 2019) and propose neural L-PCFGs (NL-PCFGs), achieving good results in both unsupervised dependency and constituency parsing. Neural parameterization is the key to success, which facilitates informed smoothing (Kim et al., 2019), reduces the number of learnable parameters for large grammars (Chiu and Rush, 2020; Yang et"
2021.acl-long.209,2020.coling-main.227,1,0.766889,"e-of-the-art performance (Jiang et al., 2016; Han et al., 2017, 2019; Yang et al., 2020). However, they rely on gold POS tags and sophisticated initializations (e.g. K&M initialization or initialization with the parsing result of another unsupervised model). Noji et al. (2016) propose a left-corner parsing-based DMV model to limit the stack depth of center-embedding, which is insensitive to initialization but needs gold POS tags. He et al. (2018) propose a latent-variable based DMV model, which does not need gold POS tags but requires good initialization and high-quality induced POS tags. See Han et al. (2020) for a survey of unsupervised dependency parsing. Compared to these methods, our method does not require gold/induced POS tags or sophisticated initializations, though its performance lags behind some of these previous methods. Recent unsupervised constituency parsers can be roughly categorized into the following groups: (1) PCFG-based methods. Depth-bounded PCFGs (Jin et al., 2018a,b) limit the stack depth of centerembedding. Neurally parameterized PCFGs (Jin 2695 et al., 2019; Kim et al., 2019; Zhu et al., 2020; Yang et al., 2021) use neural networks to produce grammar rule probabilities. (2"
2021.acl-long.209,D17-1176,1,0.704863,"Chiu and Rush, 2020; Yang et al., 2021) and facilitates advanced gradient-based optimization techniques instead of using the traditional EM algorithm (Eisner, 2016). However, Zhu et al. (2020) oversimplify the binary rules to decrease the complexity of the inside/CYK algorithm in learning (i.e., estimating the marginal sentence loglikelihood) and inference. Specifically, they make a strong independence assumption on the generation of the child word such that it is only dependent on the nonterminal symbol. Bilexical dependencies, which have been shown useful in unsupervised dependency parsing (Han et al., 2017; Yang et al., 2020), are thus ignored. 2688 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2688–2699 August 1–6, 2021. ©2021 Association for Computational Linguistics To model bilexical dependencies and meanwhile reduce complexities, we draw inspiration from the canonical polyadic decomposition (CPD) (Kolda and Bader, 2009) and propose a latent-variable based neural parameterization of L-PCFGs. Cohen et al. (2013); Yang et al. (2021) have used CPD to decrease the complexi"
2021.acl-long.209,P19-1526,1,0.853898,"Missing"
2021.acl-long.209,D18-1160,0,0.0200592,"supervised dependency parsing, most methods are based on Dependency Model with Valence (DMV) (Klein and Manning, 2004). Neurally parameterized DMVs have obtained state-of-the-art performance (Jiang et al., 2016; Han et al., 2017, 2019; Yang et al., 2020). However, they rely on gold POS tags and sophisticated initializations (e.g. K&M initialization or initialization with the parsing result of another unsupervised model). Noji et al. (2016) propose a left-corner parsing-based DMV model to limit the stack depth of center-embedding, which is insensitive to initialization but needs gold POS tags. He et al. (2018) propose a latent-variable based DMV model, which does not need gold POS tags but requires good initialization and high-quality induced POS tags. See Han et al. (2020) for a survey of unsupervised dependency parsing. Compared to these methods, our method does not require gold/induced POS tags or sophisticated initializations, though its performance lags behind some of these previous methods. Recent unsupervised constituency parsers can be roughly categorized into the following groups: (1) PCFG-based methods. Depth-bounded PCFGs (Jin et al., 2018a,b) limit the stack depth of centerembedding. Ne"
2021.acl-long.209,2020.coling-main.322,1,0.738749,"quire gold/induced POS tags or sophisticated initializations, though its performance lags behind some of these previous methods. Recent unsupervised constituency parsers can be roughly categorized into the following groups: (1) PCFG-based methods. Depth-bounded PCFGs (Jin et al., 2018a,b) limit the stack depth of centerembedding. Neurally parameterized PCFGs (Jin 2695 et al., 2019; Kim et al., 2019; Zhu et al., 2020; Yang et al., 2021) use neural networks to produce grammar rule probabilities. (2) Deep Inside-Outside Recursive Auto-encoder (DIORA) based methods (Drozdov et al., 2019a,b, 2020; Hong et al., 2020; Sahay et al., 2021). They use neural networks to mimic the inside-outside algorithm and they are trained with masked language model objectives. (3) Syntactic distance-based methods (Shen et al., 2018, 2019, 2020). They encode hidden syntactic trees into syntactic distances and inject them into language models. (4) Probing based methods (Kim et al., 2020; Li et al., 2020). They extract phrase-structure trees based on the attention distributions of large pre-trained language models. In addition to these methods, Cao et al. (2020) use constituency tests and Shi et al. (2021) make use of natural"
2021.acl-long.209,D16-1073,1,0.766654,"The original implementation of NL-PCFG by Zhu et al. (2020) takes much more time when sentences are long. For example, when sentence length is 40, it needs 6.80s, while our fast implementation takes 0.43s and our NBL-PCFG takes only 0.30s. Figure 3b illustrates the time with the increase of the nonRelated Work Unsupervised parsing has a long history but has regained great attention in recent years. In unsupervised dependency parsing, most methods are based on Dependency Model with Valence (DMV) (Klein and Manning, 2004). Neurally parameterized DMVs have obtained state-of-the-art performance (Jiang et al., 2016; Han et al., 2017, 2019; Yang et al., 2020). However, they rely on gold POS tags and sophisticated initializations (e.g. K&M initialization or initialization with the parsing result of another unsupervised model). Noji et al. (2016) propose a left-corner parsing-based DMV model to limit the stack depth of center-embedding, which is insensitive to initialization but needs gold POS tags. He et al. (2018) propose a latent-variable based DMV model, which does not need gold POS tags but requires good initialization and high-quality induced POS tags. See Han et al. (2020) for a survey of unsupervis"
2021.acl-long.209,D18-1292,0,0.0137051,"itive to initialization but needs gold POS tags. He et al. (2018) propose a latent-variable based DMV model, which does not need gold POS tags but requires good initialization and high-quality induced POS tags. See Han et al. (2020) for a survey of unsupervised dependency parsing. Compared to these methods, our method does not require gold/induced POS tags or sophisticated initializations, though its performance lags behind some of these previous methods. Recent unsupervised constituency parsers can be roughly categorized into the following groups: (1) PCFG-based methods. Depth-bounded PCFGs (Jin et al., 2018a,b) limit the stack depth of centerembedding. Neurally parameterized PCFGs (Jin 2695 et al., 2019; Kim et al., 2019; Zhu et al., 2020; Yang et al., 2021) use neural networks to produce grammar rule probabilities. (2) Deep Inside-Outside Recursive Auto-encoder (DIORA) based methods (Drozdov et al., 2019a,b, 2020; Hong et al., 2020; Sahay et al., 2021). They use neural networks to mimic the inside-outside algorithm and they are trained with masked language model objectives. (3) Syntactic distance-based methods (Shen et al., 2018, 2019, 2020). They encode hidden syntactic trees into syntactic di"
2021.acl-long.209,Q18-1016,0,0.017195,"itive to initialization but needs gold POS tags. He et al. (2018) propose a latent-variable based DMV model, which does not need gold POS tags but requires good initialization and high-quality induced POS tags. See Han et al. (2020) for a survey of unsupervised dependency parsing. Compared to these methods, our method does not require gold/induced POS tags or sophisticated initializations, though its performance lags behind some of these previous methods. Recent unsupervised constituency parsers can be roughly categorized into the following groups: (1) PCFG-based methods. Depth-bounded PCFGs (Jin et al., 2018a,b) limit the stack depth of centerembedding. Neurally parameterized PCFGs (Jin 2695 et al., 2019; Kim et al., 2019; Zhu et al., 2020; Yang et al., 2021) use neural networks to produce grammar rule probabilities. (2) Deep Inside-Outside Recursive Auto-encoder (DIORA) based methods (Drozdov et al., 2019a,b, 2020; Hong et al., 2020; Sahay et al., 2021). They use neural networks to mimic the inside-outside algorithm and they are trained with masked language model objectives. (3) Syntactic distance-based methods (Shen et al., 2018, 2019, 2020). They encode hidden syntactic trees into syntactic di"
2021.acl-long.209,P19-1234,0,0.0412577,"Missing"
2021.acl-long.209,2020.aacl-main.42,0,0.0141905,"tactic distance-based methods (Shen et al., 2018, 2019, 2020). They encode hidden syntactic trees into syntactic distances and inject them into language models. (4) Probing based methods (Kim et al., 2020; Li et al., 2020). They extract phrase-structure trees based on the attention distributions of large pre-trained language models. In addition to these methods, Cao et al. (2020) use constituency tests and Shi et al. (2021) make use of naturally-occurring bracketings such as hyperlinks on webpages to train parsers. Multimodal information such as images (Shi et al., 2019; Zhao and Titov, 2020; Jin and Schuler, 2020) and videos (Zhang et al., 2021) have also been exploited for unsupervised constituency parsing. We are only aware of a few previous studies in unsupervised joint dependency and constituency parsing. Klein and Manning (2004) propose a joint DMV and CCM (Klein and Manning, 2002) model. Shen et al. (2020) propose a transformer-based method, in which they define syntactic distances to guild attentions of transformers. Zhu et al. (2020) propose neural L-PCFGs for unsupervised joint parsing. 10 Conclusion We have presented a new formalism of lexicalized PCFGs. Our formalism relies on the canonical"
2021.acl-long.209,J98-4004,0,0.0683347,"t probabilistic approach to syntactic analysis (Lari and Young, 1990; Jelinek et al., 1992). They assign a probability to each of the parses admitted by CFGs and rank them by the plausibility in such a way that the ambiguity of CFGs can be ameliorated. Still, due to the strong independence assumption of CFGs, vanilla PCFGs (Charniak, 1996) are far from adequate for highly ambiguous text. A common premise for tackling the issue is to incorporate lexical information and weaken the independence assumption. There have been many approaches proposed under the premise (Magerman, 1995; Collins, 1997; Johnson, 1998; Klein and Manning, 2003). Among them lexicalized PCFGs (L-PCFGs) are a relatively straightforward formalism (Collins, 2003). L-PCFGs extend PCFGs by associating a word, i.e., the lexical head, with each grammar symbol. They can thus exploit lexical ˚ Corresponding Author information to disambiguate parsing decisions and are much more expressive than vanilla PCFGs. However, they suffer from representation and inference complexities. For representation, the addition of lexical information greatly increases the number of parameters to be estimated and exacerbates the data sparsity problem durin"
2021.acl-long.209,P19-1228,0,0.0387082,"Missing"
2021.acl-long.209,P04-1061,0,0.630246,"llustrates the time with the increase of the sentence length and a fixed nonterminal number of 10. The original implementation of NL-PCFG by Zhu et al. (2020) takes much more time when sentences are long. For example, when sentence length is 40, it needs 6.80s, while our fast implementation takes 0.43s and our NBL-PCFG takes only 0.30s. Figure 3b illustrates the time with the increase of the nonRelated Work Unsupervised parsing has a long history but has regained great attention in recent years. In unsupervised dependency parsing, most methods are based on Dependency Model with Valence (DMV) (Klein and Manning, 2004). Neurally parameterized DMVs have obtained state-of-the-art performance (Jiang et al., 2016; Han et al., 2017, 2019; Yang et al., 2020). However, they rely on gold POS tags and sophisticated initializations (e.g. K&M initialization or initialization with the parsing result of another unsupervised model). Noji et al. (2016) propose a left-corner parsing-based DMV model to limit the stack depth of center-embedding, which is insensitive to initialization but needs gold POS tags. He et al. (2018) propose a latent-variable based DMV model, which does not need gold POS tags but requires good initia"
2021.acl-long.209,P02-1017,0,0.109213,"ention distributions of large pre-trained language models. In addition to these methods, Cao et al. (2020) use constituency tests and Shi et al. (2021) make use of naturally-occurring bracketings such as hyperlinks on webpages to train parsers. Multimodal information such as images (Shi et al., 2019; Zhao and Titov, 2020; Jin and Schuler, 2020) and videos (Zhang et al., 2021) have also been exploited for unsupervised constituency parsing. We are only aware of a few previous studies in unsupervised joint dependency and constituency parsing. Klein and Manning (2004) propose a joint DMV and CCM (Klein and Manning, 2002) model. Shen et al. (2020) propose a transformer-based method, in which they define syntactic distances to guild attentions of transformers. Zhu et al. (2020) propose neural L-PCFGs for unsupervised joint parsing. 10 Conclusion We have presented a new formalism of lexicalized PCFGs. Our formalism relies on the canonical polyadic decomposition to factorize the probability tensor of binary rules. The factorization reduces the space and time complexity of lexicalized PCFGs while keeping the independence assumptions encoded in the original binary rules intact. We further parameterize our model by"
2021.acl-long.209,D16-1004,0,0.0581063,"Missing"
2021.acl-long.209,P03-1054,0,0.124141,"approach to syntactic analysis (Lari and Young, 1990; Jelinek et al., 1992). They assign a probability to each of the parses admitted by CFGs and rank them by the plausibility in such a way that the ambiguity of CFGs can be ameliorated. Still, due to the strong independence assumption of CFGs, vanilla PCFGs (Charniak, 1996) are far from adequate for highly ambiguous text. A common premise for tackling the issue is to incorporate lexical information and weaken the independence assumption. There have been many approaches proposed under the premise (Magerman, 1995; Collins, 1997; Johnson, 1998; Klein and Manning, 2003). Among them lexicalized PCFGs (L-PCFGs) are a relatively straightforward formalism (Collins, 2003). L-PCFGs extend PCFGs by associating a word, i.e., the lexical head, with each grammar symbol. They can thus exploit lexical ˚ Corresponding Author information to disambiguate parsing decisions and are much more expressive than vanilla PCFGs. However, they suffer from representation and inference complexities. For representation, the addition of lexical information greatly increases the number of parameters to be estimated and exacerbates the data sparsity problem during learning, so the expecta"
2021.acl-long.209,2020.acl-demos.38,0,0.0801124,"er and Satta (1999) manage to reduce the complexity to Opl4 |G|q, inference with L-PCFGs is still relatively slow, making them less popular nowadays. Recently, Zhu et al. (2020) combine the ideas of factorizing the binary rule probabilities (Collins, 2003) and neural parameterization (Kim et al., 2019) and propose neural L-PCFGs (NL-PCFGs), achieving good results in both unsupervised dependency and constituency parsing. Neural parameterization is the key to success, which facilitates informed smoothing (Kim et al., 2019), reduces the number of learnable parameters for large grammars (Chiu and Rush, 2020; Yang et al., 2021) and facilitates advanced gradient-based optimization techniques instead of using the traditional EM algorithm (Eisner, 2016). However, Zhu et al. (2020) oversimplify the binary rules to decrease the complexity of the inside/CYK algorithm in learning (i.e., estimating the marginal sentence loglikelihood) and inference. Specifically, they make a strong independence assumption on the generation of the child word such that it is only dependent on the nonterminal symbol. Bilexical dependencies, which have been shown useful in unsupervised dependency parsing (Han et al., 2017; Y"
2021.acl-long.209,2020.aacl-main.43,0,0.0198457,"., 2019; Kim et al., 2019; Zhu et al., 2020; Yang et al., 2021) use neural networks to produce grammar rule probabilities. (2) Deep Inside-Outside Recursive Auto-encoder (DIORA) based methods (Drozdov et al., 2019a,b, 2020; Hong et al., 2020; Sahay et al., 2021). They use neural networks to mimic the inside-outside algorithm and they are trained with masked language model objectives. (3) Syntactic distance-based methods (Shen et al., 2018, 2019, 2020). They encode hidden syntactic trees into syntactic distances and inject them into language models. (4) Probing based methods (Kim et al., 2020; Li et al., 2020). They extract phrase-structure trees based on the attention distributions of large pre-trained language models. In addition to these methods, Cao et al. (2020) use constituency tests and Shi et al. (2021) make use of naturally-occurring bracketings such as hyperlinks on webpages to train parsers. Multimodal information such as images (Shi et al., 2019; Zhao and Titov, 2020; Jin and Schuler, 2020) and videos (Zhang et al., 2021) have also been exploited for unsupervised constituency parsing. We are only aware of a few previous studies in unsupervised joint dependency and constituency parsing."
2021.acl-long.209,P95-1037,0,0.394006,"rs (PCFGs) has been an important probabilistic approach to syntactic analysis (Lari and Young, 1990; Jelinek et al., 1992). They assign a probability to each of the parses admitted by CFGs and rank them by the plausibility in such a way that the ambiguity of CFGs can be ameliorated. Still, due to the strong independence assumption of CFGs, vanilla PCFGs (Charniak, 1996) are far from adequate for highly ambiguous text. A common premise for tackling the issue is to incorporate lexical information and weaken the independence assumption. There have been many approaches proposed under the premise (Magerman, 1995; Collins, 1997; Johnson, 1998; Klein and Manning, 2003). Among them lexicalized PCFGs (L-PCFGs) are a relatively straightforward formalism (Collins, 2003). L-PCFGs extend PCFGs by associating a word, i.e., the lexical head, with each grammar symbol. They can thus exploit lexical ˚ Corresponding Author information to disambiguate parsing decisions and are much more expressive than vanilla PCFGs. However, they suffer from representation and inference complexities. For representation, the addition of lexical information greatly increases the number of parameters to be estimated and exacerbates t"
2021.acl-long.209,H94-1020,0,0.469214,"w qq , J w1 PΣ exppuH f2 pww1 qq ppC ð |Hq “ ř exppuJ H wCð q , J C 1 PM exppuH wC 1 q ppC ñ |Hq “ ř exppuJ H wCñ q , J C 1 PM exppuH wC 1 q ppH|A, wq “ ř exppuJ H f4 prwA ; ww sqq , J H 1 PH exppuH 1 f4 prwA ; ww sqq where H “ tH1 , . . . , HdH u, M “ pN Y Pq ˆ tð , ñu, u and w are nonterminal embeddings and word embeddings respectively, and f1 p¨q, f2 p¨q, f3 p¨q, f4 p¨q are neural networks with residual layers (He et al., 2016) (Full parameterization is shown in Appendix.). 4 4.1 Experimental setup Dataset We conduct experiments on the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1994). We use the same preprocessing pipeline as in Kim et al. (2019). Specifically, punctuation is removed from all data splits and the top 10,000 frequent words in the training data are used as the vocabulary. For dependency grammar induction, we follow (Zhu et al., 2020) to use the Stanford typed dependency representation (de Marneffe and Manning, 2008). 4.2 Hyperparameters We optimize our model using the Adam optimizer with β1 “ 0.75, β2 “ 0.999, and learning rate 0.001. All parameters are initialized with Xavier uniform initialization. We set the dimension of all embeddings to 256 and the rati"
2021.acl-long.209,W08-1301,0,0.0945844,"Missing"
2021.acl-long.209,P19-1180,0,0.0207259,"asked language model objectives. (3) Syntactic distance-based methods (Shen et al., 2018, 2019, 2020). They encode hidden syntactic trees into syntactic distances and inject them into language models. (4) Probing based methods (Kim et al., 2020; Li et al., 2020). They extract phrase-structure trees based on the attention distributions of large pre-trained language models. In addition to these methods, Cao et al. (2020) use constituency tests and Shi et al. (2021) make use of naturally-occurring bracketings such as hyperlinks on webpages to train parsers. Multimodal information such as images (Shi et al., 2019; Zhao and Titov, 2020; Jin and Schuler, 2020) and videos (Zhang et al., 2021) have also been exploited for unsupervised constituency parsing. We are only aware of a few previous studies in unsupervised joint dependency and constituency parsing. Klein and Manning (2004) propose a joint DMV and CCM (Klein and Manning, 2002) model. Shen et al. (2020) propose a transformer-based method, in which they define syntactic distances to guild attentions of transformers. Zhu et al. (2020) propose neural L-PCFGs for unsupervised joint parsing. 10 Conclusion We have presented a new formalism of lexicalized"
2021.acl-long.209,2021.naacl-main.234,0,0.0364938,"Missing"
2021.acl-long.209,2020.coling-main.347,1,0.883885,"0; Yang et al., 2021) and facilitates advanced gradient-based optimization techniques instead of using the traditional EM algorithm (Eisner, 2016). However, Zhu et al. (2020) oversimplify the binary rules to decrease the complexity of the inside/CYK algorithm in learning (i.e., estimating the marginal sentence loglikelihood) and inference. Specifically, they make a strong independence assumption on the generation of the child word such that it is only dependent on the nonterminal symbol. Bilexical dependencies, which have been shown useful in unsupervised dependency parsing (Han et al., 2017; Yang et al., 2020), are thus ignored. 2688 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2688–2699 August 1–6, 2021. ©2021 Association for Computational Linguistics To model bilexical dependencies and meanwhile reduce complexities, we draw inspiration from the canonical polyadic decomposition (CPD) (Kolda and Bader, 2009) and propose a latent-variable based neural parameterization of L-PCFGs. Cohen et al. (2013); Yang et al. (2021) have used CPD to decrease the complexities of PCFGs, and o"
2021.acl-long.209,2020.tacl-1.42,0,0.0911545,"xical information greatly increases the number of parameters to be estimated and exacerbates the data sparsity problem during learning, so the expectation-maximisation (EM) based estimation of L-PCFGs has to rely on sophisticated smoothing techniques and factorizations (Collins, 2003). As for inference, the CYK algorithm for L-PCFGs has a Opl5 |G|q complexity, where l is the sentence length and |G |is the grammar constant. Although Eisner and Satta (1999) manage to reduce the complexity to Opl4 |G|q, inference with L-PCFGs is still relatively slow, making them less popular nowadays. Recently, Zhu et al. (2020) combine the ideas of factorizing the binary rule probabilities (Collins, 2003) and neural parameterization (Kim et al., 2019) and propose neural L-PCFGs (NL-PCFGs), achieving good results in both unsupervised dependency and constituency parsing. Neural parameterization is the key to success, which facilitates informed smoothing (Kim et al., 2019), reduces the number of learnable parameters for large grammars (Chiu and Rush, 2020; Yang et al., 2021) and facilitates advanced gradient-based optimization techniques instead of using the traditional EM algorithm (Eisner, 2016). However, Zhu et al."
2021.acl-long.209,2020.emnlp-main.354,1,0.787807,"Missing"
2021.acl-long.209,2021.naacl-main.117,1,0.778905,"Missing"
2021.acl-long.209,2021.naacl-main.119,0,0.0159895,"et al., 2018, 2019, 2020). They encode hidden syntactic trees into syntactic distances and inject them into language models. (4) Probing based methods (Kim et al., 2020; Li et al., 2020). They extract phrase-structure trees based on the attention distributions of large pre-trained language models. In addition to these methods, Cao et al. (2020) use constituency tests and Shi et al. (2021) make use of naturally-occurring bracketings such as hyperlinks on webpages to train parsers. Multimodal information such as images (Shi et al., 2019; Zhao and Titov, 2020; Jin and Schuler, 2020) and videos (Zhang et al., 2021) have also been exploited for unsupervised constituency parsing. We are only aware of a few previous studies in unsupervised joint dependency and constituency parsing. Klein and Manning (2004) propose a joint DMV and CCM (Klein and Manning, 2002) model. Shen et al. (2020) propose a transformer-based method, in which they define syntactic distances to guild attentions of transformers. Zhu et al. (2020) propose neural L-PCFGs for unsupervised joint parsing. 10 Conclusion We have presented a new formalism of lexicalized PCFGs. Our formalism relies on the canonical polyadic decomposition to factor"
2021.acl-long.209,P06-2101,0,0.0243732,"eterminals, and dH “ 300. We use grid search to tune the nonterminal number (from 5 to 30) and domain size dH of the latent H (from 50 to 500). 4.3 Evaluation We run each model four times with different random seeds and for ten epochs. We train our models on training sentences of length ď 40 with batch size 8 and test them on the whole testing set. For each run, we perform early stopping and select the best model according to the perplexity of the development set. We use two different parsing methods: the variant of CYK algorithm (Eisner and Satta, 1999) and Minimum Bayes-Risk (MBR) decoding (Smith and Eisner, 2006). 2 For constituent grammar induction, we report the means and standard deviations of sentence-level F1 scores.3 For dependency grammar induction, we report unlabeled directed attachment score (UDAS) and unlabeled undirected attachment score (UUAS). 5 Main result We present our main results in Table 2. Our model is referred to as Neural Bi-Lexicalized PCFGs (NBL-PCFGs). We mainly compare our approach against recent PCFG-based models: neural PCFG (N-PCFG) and compound PCFG (C-PCFG) (Kim et al., 2019), tensor decomposition based neural 2 In MBR decoding, we use automatic differentiation (Eisner,"
2021.acl-long.380,N10-1083,0,0.12678,"Missing"
2021.acl-long.380,P19-1299,0,0.0180284,"asible and efficient way to tackle the low-resource problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the"
2021.acl-long.380,2020.acl-main.747,0,0.0968285,"Missing"
2021.acl-long.380,J88-1003,0,0.0958873,"dels and the true labels. By making the risk function trainable, we draw a connection between minimum risk training and latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions fro"
2021.acl-long.380,N19-1423,0,0.125683,"gorithm, which alternates between updating a posterior distribution and optimizing model parameters. To empirically evaluate our proposed approaches, we extensively conduct experiments on four sequence labeling tasks of twenty-one datasets. Our two proposed approaches, especially the latent variable model, outperform several strong baselines. 2 Background 2.1 Sequence Labeling Given a sentence x = x1 , . . . , xn , its word representations are extracted from the pre-trained embeddings and passed into a sentence encoder such as BiLSTM, Convolutional Neural Networks (CNN) and multilingual BERT (Devlin et al., 2019) to obtain a sequence of contextual features. Without considering the dependencies between predicted labels, the Softmax layer computes the conditional probability as follows, Pθ (y|x) = n Y Pθ (yi |x) i=1 Given the gold sequence y∗ = y1∗ , . . . , yn∗ , the general training objective is to minimize the negative log-likelihood of the sequence, J (θθ ) = − log Pθ (y∗ |x) = − n X J (θθ ) = − log Pθ (ˆ y|x) = − Cross-Lingual/Domain Transfer Supervised models fail when labeled data are absent. Learning from imperfect predictions from rich-resource sources is a viable approach to tackle the problem"
2021.acl-long.380,P19-1266,0,0.0448562,"Missing"
2021.acl-long.380,D18-1498,0,0.0522747,"Missing"
2021.acl-long.380,N06-2015,0,0.188956,"tagging task, we use Universal Dependencies treebanks (UD) v2.44 and randomly select five anguages together with the English dataset. The whole datasets are English (En), Catalan (Ca), Indonesian (Id), Hindi (Hi), Finnish (Fi), and Russian (Ru). For the Aspect Extraction task, we select the restaurant domain over subtask 1 in the SemEval-2016 shared task (Pontiki et al., 2016). For the NER task, we evaluate our models on the CoNLL 2002 and 2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Cross-Domain Sequence Labeling We use English portion of the OntoNotes (v5) (Hovy et al., 2006), which contains six domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), and web (wb). More details can be found in the Appendix A.1. 4.2 Approaches Single-source Setup The following approaches are applicable for single-source setup, • DT: we use the pre-trained source model to directly predict the pseudo labels on the target unlabeled data. • Hard: we use the pseudo labels from DT on the target unlabeled data to train a new model. Inference For inference, we use Q(y) to obtain ypred 4 lowing Wu et al. (2020), the source model are previously trained on its"
2021.acl-long.380,2020.findings-emnlp.236,1,0.851087,"glish portion of the OntoNotes (v5) (Hovy et al., 2006), which contains six domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), and web (wb). More details can be found in the Appendix A.1. 4.2 Approaches Single-source Setup The following approaches are applicable for single-source setup, • DT: we use the pre-trained source model to directly predict the pseudo labels on the target unlabeled data. • Hard: we use the pseudo labels from DT on the target unlabeled data to train a new model. Inference For inference, we use Q(y) to obtain ypred 4 lowing Wu et al. (2020), the source model are previously trained on its corresponding training data. We use the BIO scheme for CoNLL and OntoNotes NER tasks and Aspect Extraction. We run each model three times and report the average accuracy for the POS tagging task and F1-score for the other tasks. 2, Multi-source Setup The following approaches are applicable for multi-source setup, Pφ (ˆ y(u) |y, u) u=1 Experiments We use the multilingual BERT (mBERT) as our word representations3 as the sentence encoder. Fol2 Another choice is to use Pθ (y|x), however, we found that utilizing Q(y) generally achieves better perform"
2021.acl-long.380,2021.acl-long.207,1,0.894793,"2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et al., 2021) is a feasible and efficient way to tackle the low-resource problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source lan"
2021.acl-long.380,N19-1383,0,0.0949712,", 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et al., 2021) is a feasible and efficient way to tackle the low-resource problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled"
2021.acl-long.380,D17-1302,0,0.0180989,"s widely studied (Steedman et al., 6 The CoNLL NER datasets have 11 labels (9 entity labels, a padding label and an ending label). 4916 F1 De Nl Es 74.1 80.6 76.1 74.0 80.5 75.9 73.9 80.4 75.8 73.7 80.2 75.6 73.6 2 3 4 10 80.1 2 3 4 τ 10 τ 75.4 2 3 4 10 τ Figure 3: The performance of MRT approach in single-source setup with soft predictions on three NER datasets by varying different τ . 2003). Existing works include bootstrapping approaches (Ruder and Plank, 2018), mixture-ofexperts (Guo et al., 2018; Wright and Augenstein, 2020), and consensus network (Lan et al., 2020). Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information. However, our proposed approaches do not require any source labeled data or parallel texts. Contextual Multilingual Embeddings Embeddings like mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020) which are trained on many languages, make great progress on cross-lingual learning for multiple NLP tasks. Recent works (Wu and Dredze, 2019; Pires et al., 2019) show the strong cross-lingual ability of the contextual multilingual embeddings. 7 Conclus"
2021.acl-long.380,N16-1030,0,0.0408353,"latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang"
2021.acl-long.380,2020.acl-main.193,0,0.46218,"guages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the imperfect soft predictions (Wu et al., 2020), produced by one or more source models on target unlabeled data , and propose two novel approaches. We start by introducing a novel approach based on the minimum risk training framework. We design a new decomposable risk function parameterized by a fixed matrix that models the relations between the noisy predictions from the source models and the true labels. We then make the matrix trainable, which leads to further expressiveness and connects minimum risk training to learning latent 4909 Proceedings of the 59th Annual Meeting of the Associ"
2021.acl-long.380,P16-1101,0,0.0435505,"l learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et"
2021.acl-long.380,P17-1135,0,0.0379881,"Missing"
2021.acl-long.380,P19-1493,0,0.0143402,"Wright and Augenstein, 2020), and consensus network (Lan et al., 2020). Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information. However, our proposed approaches do not require any source labeled data or parallel texts. Contextual Multilingual Embeddings Embeddings like mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020) which are trained on many languages, make great progress on cross-lingual learning for multiple NLP tasks. Recent works (Wu and Dredze, 2019; Pires et al., 2019) show the strong cross-lingual ability of the contextual multilingual embeddings. 7 Conclusion In this paper, we propose two approaches to the zero-shot sequence labeling problem. Our MRT approach uses a fixed matrix to model the relations between the predicted labels from the source models and the true labels. Our LVM approach uses trainable matrices to model these label relations. We extensively verify the effectiveness of our approaches on both single-source and multisource transfer over both cross-lingual and crossdomain sequence labeling problems. Experiments show that MRT and LVM general"
2021.acl-long.380,P19-1015,0,0.109868,"ed data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the imperfect soft predictions (Wu et al., 2020), produced by one or more source models on target unlabeled data , and propose two novel approaches. We start by introducing a novel approach based on the minimum risk training framework. We design a new decomposable risk function parameterized by a fixed matrix that models the relations between the noisy predictions from the source models and the true labels. We then make the matrix trainable, which leads to further expressiveness and connects minimum risk training to learning latent 4909 Proceedings of the 59th Annual Me"
2021.acl-long.380,W09-1119,0,0.0197473,"connection between minimum risk training and latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsk"
2021.acl-long.380,D11-1141,0,0.00798819,"mum risk training and latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo"
2021.acl-long.380,P18-1096,0,0.0210148,"pose a multi-view framework to selectively transfer knowledge from multiple sources by utilizing a small amount of labeled dataset. Crossdomain adaption is widely studied (Steedman et al., 6 The CoNLL NER datasets have 11 labels (9 entity labels, a padding label and an ending label). 4916 F1 De Nl Es 74.1 80.6 76.1 74.0 80.5 75.9 73.9 80.4 75.8 73.7 80.2 75.6 73.6 2 3 4 10 80.1 2 3 4 τ 10 τ 75.4 2 3 4 10 τ Figure 3: The performance of MRT approach in single-source setup with soft predictions on three NER datasets by varying different τ . 2003). Existing works include bootstrapping approaches (Ruder and Plank, 2018), mixture-ofexperts (Guo et al., 2018; Wright and Augenstein, 2020), and consensus network (Lan et al., 2020). Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information. However, our proposed approaches do not require any source labeled data or parallel texts. Contextual Multilingual Embeddings Embeddings like mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020) which are trained on many languages, make great progress on cross-lingual learning for multiple NLP task"
2021.acl-long.380,P16-1159,0,0.0343896,"hard predictions and the target model’s soft predictions, Methodology Minimum Risk Training In supervised learning, minimum risk training aims to minimize the expected error (risk) concerning the conditional probability, X J (θθ ) = Pθ (y|x)R(y∗ , y) y∈Y(x) where R(y∗ , y) is the risk function that measures the distance between the gold sequence y∗ and the candidate sequence y, and Y(x) denotes the collection of all the possible label sequences given the sentence x. The risk function can be defined in many ways depending on specific applications, such as the BLEU score in machine translation (Shen et al., 2016). However, in our setting, there are no gold labels to compute R(y∗ , y). Instead, we assume there are multiple pretrained source models which can be used to predict hard labels, and we define the risk function as R(ˆ y, y) to measure the difference between pseudo label sequence 4910 ˆ predicted by source models and the candidate y sequence y. The objective function becomes, x y∈Y(x) Conventional minimum risk training is intractable which is mainly due to the combination of two reasons: first, the set of candidate label sequences Y(x) is exponential in size and intractable to enumerate; second"
2021.acl-long.380,N03-1031,0,0.424391,"Missing"
2021.acl-long.380,N12-1052,0,0.0777677,"Missing"
2021.acl-long.380,W02-2024,0,0.400509,"hree tasks to conduct the cross-lingual sequence labeling task, which are POS tagging, NER, and Aspect Extraction. For the POS tagging task, we use Universal Dependencies treebanks (UD) v2.44 and randomly select five anguages together with the English dataset. The whole datasets are English (En), Catalan (Ca), Indonesian (Id), Hindi (Hi), Finnish (Fi), and Russian (Ru). For the Aspect Extraction task, we select the restaurant domain over subtask 1 in the SemEval-2016 shared task (Pontiki et al., 2016). For the NER task, we evaluate our models on the CoNLL 2002 and 2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Cross-Domain Sequence Labeling We use English portion of the OntoNotes (v5) (Hovy et al., 2006), which contains six domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), and web (wb). More details can be found in the Appendix A.1. 4.2 Approaches Single-source Setup The following approaches are applicable for single-source setup, • DT: we use the pre-trained source model to directly predict the pseudo labels on the target unlabeled data. • Hard: we use the pseudo labels from DT on the target unlabeled data to train a new"
2021.acl-long.380,N03-1033,0,0.314546,"rue labels. By making the risk function trainable, we draw a connection between minimum risk training and latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sou"
2021.acl-long.380,Q14-1005,0,0.0759383,"nd Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et al., 2021) is a feasible and efficient way to tackle the low-resource problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the im"
2021.acl-long.380,2020.emnlp-main.639,0,0.0335027,"from multiple sources by utilizing a small amount of labeled dataset. Crossdomain adaption is widely studied (Steedman et al., 6 The CoNLL NER datasets have 11 labels (9 entity labels, a padding label and an ending label). 4916 F1 De Nl Es 74.1 80.6 76.1 74.0 80.5 75.9 73.9 80.4 75.8 73.7 80.2 75.6 73.6 2 3 4 10 80.1 2 3 4 τ 10 τ 75.4 2 3 4 10 τ Figure 3: The performance of MRT approach in single-source setup with soft predictions on three NER datasets by varying different τ . 2003). Existing works include bootstrapping approaches (Ruder and Plank, 2018), mixture-ofexperts (Guo et al., 2018; Wright and Augenstein, 2020), and consensus network (Lan et al., 2020). Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information. However, our proposed approaches do not require any source labeled data or parallel texts. Contextual Multilingual Embeddings Embeddings like mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020) which are trained on many languages, make great progress on cross-lingual learning for multiple NLP tasks. Recent works (Wu and Dredze, 2019; Pires et al., 2019) show the"
2021.acl-long.380,2020.acl-main.581,0,0.0664042,"t languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the imperfect soft predictions (Wu et al., 2020), produced by one or more source models on target unlabeled data , and propose two novel approaches. We start by introducing a novel approach based on the minimum risk training framework. We design a new decomposable risk function parameterized by a fixed matrix that models th"
2021.acl-long.380,D19-1077,0,0.0498863,"trained on its corresponding training data. We use the BIO scheme for CoNLL and OntoNotes NER tasks and Aspect Extraction. We run each model three times and report the average accuracy for the POS tagging task and F1-score for the other tasks. 2, Multi-source Setup The following approaches are applicable for multi-source setup, Pφ (ˆ y(u) |y, u) u=1 Experiments We use the multilingual BERT (mBERT) as our word representations3 as the sentence encoder. Fol2 Another choice is to use Pθ (y|x), however, we found that utilizing Q(y) generally achieves better performance. 3 Following previous work (Wu and Dredze, 2019; Wu et al., 2020), we fine-tune mBERT’s parameters. 4913 • Hard-Cat: we apply DT with all the source languages/domains, mix the resulting pseudo labels from all the sources on the unlabeled target data, and train a new model. • Hard-Vote: we do majority voting at the token level on the pseudo labels from DT with each source and train a new model. 4 https://universaldependencies.org/ C O NLL NER English German Dutch Spanish Avg. A SPECT E XTRACTION English Spanish Dutch Russian Turkish Avg. S INGLE - SOURCE : The following approaches have access to hard predictions: — 72.17 79.54 75.13 75.61 D"
2021.acl-long.380,N15-1069,0,0.0277639,"rce problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the imperfect soft predictions (Wu et al., 2020), produced b"
2021.acl-long.449,D15-1263,0,0.02148,"adapted methods to the semi-supervised and supervised setting and surprisingly, we find that they outperform previous methods specially designed for supervised discourse parsing. Further analysis shows our adaptations result in superiority not only in parsing accuracy but also in time and space efficiency. 1 Introduction Discourse parsing, aiming to find how the text spans in a document relate to each other, benefits various down-stream tasks, such as machine translation evaluation (Guzm´an et al., 2014; Joty et al., 2014), summarization (Marcu, 2000; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015; Huber and Carenini, 2020) and automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2013). Researchers have made impressive progress on discourse parsing from the constituency perspective, which presents discourse structures as constituency trees (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Joty et al., 2015; Nishida and * Corresponding author. Nakayama, 2020). However, as demonstrated by Morey et al. (2018), discourse structure can also be formulated as a dependency structure. Besides that, there might exist ambiguous parsing in terms of the constituency perspective (Mor"
2021.acl-long.449,P14-1065,0,0.0793052,"Missing"
2021.acl-long.449,E17-1028,0,0.123549,"the performance of our adapted NCRFAE and V-DNDMV in the fully supervised setting. The results are shown in Table 3. We evaluate our models on the RSTDT and SciDTB datasets and compare them with eight models. NIVRE04 (Nivre et al., 2004) and WANG17 (Wang et al., 2017) are two transitionbased models for dependency parsing. Yang and Li (2018) adapts them to discourse dependency parsing. FENG14 (Feng and Hirst, 2014), JI14 ‡ We correct their evaluation metrics, so the result is different from the original paper (Li et al., 2014). (Ji and Eisenstein, 2014), JOTY15 (Joty et al., 2015) and BRAUD17 (Braud et al., 2017) are methods for discourse constituent parsing and they are adapted for discourse dependency parsing by Morey et al. (2018). LI14 (Li et al., 2014) and MOREY18 (Morey et al., 2018) are graph-based and transition-based methods specially designed for discourse dependency parsing, respectively. These models are statistical or simple neural models, and they do not use pretrained language models (like BERT, ELMo (Peters et al., 2018)) to extract features. As Table 3 shows, the performance of our NCRFAE is significantly better than the baseline models. Especially, the UAS and LAS of NCRFAE are 8.9 p"
2021.acl-long.449,P19-1526,1,0.908863,"Natural Language Processing, pages 5782–5794 August 1–6, 2021. ©2021 Association for Computational Linguistics lenges, we cluster the EDUs to produce clusters resembling Part-Of-Speech (POS) tags in syntactic parsing and we introduce the Hierarchical Eisner algorithm that finds the optimal parse tree conforming to the constraint. We applied our adaptation method to two stateof-the-art unsupervised syntactic dependency parsing models: Neural Conditional Random Field Autoencoder (NCRFAE, Li and Tu (2020)) and Variational Variant of Discriminative Neural Dependency Model with Valences (V-DNDMV, Han et al. (2019)). In our experiments, the adapted models performs better than the baseline on both RST Discourse Treebank (RST-DT, Carlson et al. (2001)) and SciDTB (Yang and Li, 2018) in the unsupervised setting. When we extend the two models to the semi-supervised and supervised setting, we find they can outperform previous methods specially designed for supervised discourse parsing. Further analysis indicates that the Hierarchical Eisner algorithm shows superiority not only in parsing accuracy but also in time and space efficiency. Its empirical time and space complexity is close to O(n2 ) with n being th"
2021.acl-long.449,D17-1171,1,0.934577,"sk are Dependency Model with Valences (DMV, Klein and Manning (2004)), a generative model learning the grammar from POS tags for dependency predictions, and its extensions. Jiang et al. (2016) employ neural networks to capture the similarities between POS tags ignored by vanilla DMV and Han et al. (2019) further amend the former with discriminative information obtained from an additional encoding network. Besides, there are also some discriminative approaches modeling the conditional probability or score of the dependency tree given the sentence, such as the CRF autoencoder method proposed by Cai et al. (2017). Discourse dependency parsing There is limited work focusing on discourse dependency parsing. Li et al. (2014) proposes an algorithm to convert constituency RST tree to dependency structure. In their algorithm, each non-terminal is assigned with a head EDU, which is the head EDU of its leftmost nucleus child. Then, a dependency relation is created for each non-terminal from its head to its dependent, in a procedure similar to those designed for syntactic parsing. Hirao et al. (2013) proposes another method that differs from the previous one in the processing of multinuclear relations. Yoshida"
2021.acl-long.449,W01-1605,0,0.926022,"e EDUs to produce clusters resembling Part-Of-Speech (POS) tags in syntactic parsing and we introduce the Hierarchical Eisner algorithm that finds the optimal parse tree conforming to the constraint. We applied our adaptation method to two stateof-the-art unsupervised syntactic dependency parsing models: Neural Conditional Random Field Autoencoder (NCRFAE, Li and Tu (2020)) and Variational Variant of Discriminative Neural Dependency Model with Valences (V-DNDMV, Han et al. (2019)). In our experiments, the adapted models performs better than the baseline on both RST Discourse Treebank (RST-DT, Carlson et al. (2001)) and SciDTB (Yang and Li, 2018) in the unsupervised setting. When we extend the two models to the semi-supervised and supervised setting, we find they can outperform previous methods specially designed for supervised discourse parsing. Further analysis indicates that the Hierarchical Eisner algorithm shows superiority not only in parsing accuracy but also in time and space efficiency. Its empirical time and space complexity is close to O(n2 ) with n being the number of EDUs, while the unconstrained algorithm adopted by most previous work has a complexity of O(n3 ). The code and trained models"
2021.acl-long.449,N19-1423,0,0.00943767,"+ Ck+1←j ) i≤k≤j 7: Ii←j = max (sji + Ci→k + Ck+1←j ) 8: Ci→j = max (Ii→k + Ck→j ) 9: Ci←j = max (Ck→i + Ij→k ) i≤k≤j i≤k≤j i≤k≤j end for 11: end for 10: sion of the classic Eisner algorithm, used for parse tree to produce discourse dependency parse trees that conform to the constraint that every sentence or paragraph should correspond to a complete subtree. 3.1 Clustering Given an input document represented as an EDU sequence x1 , x2 , . . . , xn , we can use word embedding or context sensitive word embedding to get the vector representation xi of the i-th EDU xi . Specifically, we use BERT (Devlin et al., 2019) to encode each word. Let wi be the encoding of the i-th word in the document. For an EDU xi spanning from word position b to e, we follow Toshniwal et al. (2020) and concatenate the encoding of the endpoints to form its representation: xi = [wb ; we ]. With the representations of all EDUs from the whole training corpus obtained, we use K-Means (Lloyd, 1982) to cluster them. Let ci be the cluster label of xi . 3.2 Ratio RST-DT SciDTB Hierarchical Eisner Algorithm The Eisner algorithm (Eisner, 1996) is a dynamic programming algorithm widely used to find the optimal syntactic dependency parse tr"
2021.acl-long.449,C96-1058,0,0.807133,"dding to get the vector representation xi of the i-th EDU xi . Specifically, we use BERT (Devlin et al., 2019) to encode each word. Let wi be the encoding of the i-th word in the document. For an EDU xi spanning from word position b to e, we follow Toshniwal et al. (2020) and concatenate the encoding of the endpoints to form its representation: xi = [wb ; we ]. With the representations of all EDUs from the whole training corpus obtained, we use K-Means (Lloyd, 1982) to cluster them. Let ci be the cluster label of xi . 3.2 Ratio RST-DT SciDTB Hierarchical Eisner Algorithm The Eisner algorithm (Eisner, 1996) is a dynamic programming algorithm widely used to find the optimal syntactic dependency parse tree. The basic idea of it is to parse the left and right dependents of an token independently and combine them at a later stage. Algorithm 1 shows the pseudo-code of the Eisner algorithm. Here Ci→j represents a complete span, which consists of a head token i and all of its descendants on one side, and Ii→j represent an incomplete span, which consists of a head i and its partial descendants on one side and can be extended by adding more descendants to that side. Discourse dependency parse trees, howe"
2021.acl-long.449,P14-1048,0,0.190212,"Introduction Discourse parsing, aiming to find how the text spans in a document relate to each other, benefits various down-stream tasks, such as machine translation evaluation (Guzm´an et al., 2014; Joty et al., 2014), summarization (Marcu, 2000; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015; Huber and Carenini, 2020) and automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2013). Researchers have made impressive progress on discourse parsing from the constituency perspective, which presents discourse structures as constituency trees (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Joty et al., 2015; Nishida and * Corresponding author. Nakayama, 2020). However, as demonstrated by Morey et al. (2018), discourse structure can also be formulated as a dependency structure. Besides that, there might exist ambiguous parsing in terms of the constituency perspective (Morey et al., 2018). All of these suggest that dependency discourse parsing is a different promising approach for discourse parsing. One of the main bottlenecks in developing discourse dependency parsing methods is the lack of annotated training data since the labeling effort is labor-intensive and time-consuming,"
2021.acl-long.449,D18-1160,0,0.0295889,"Missing"
2021.acl-long.449,D13-1158,0,0.189112,"ion is effective. Moreover, we extend the adapted methods to the semi-supervised and supervised setting and surprisingly, we find that they outperform previous methods specially designed for supervised discourse parsing. Further analysis shows our adaptations result in superiority not only in parsing accuracy but also in time and space efficiency. 1 Introduction Discourse parsing, aiming to find how the text spans in a document relate to each other, benefits various down-stream tasks, such as machine translation evaluation (Guzm´an et al., 2014; Joty et al., 2014), summarization (Marcu, 2000; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015; Huber and Carenini, 2020) and automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2013). Researchers have made impressive progress on discourse parsing from the constituency perspective, which presents discourse structures as constituency trees (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Joty et al., 2015; Nishida and * Corresponding author. Nakayama, 2020). However, as demonstrated by Morey et al. (2018), discourse structure can also be formulated as a dependency structure. Besides that, there might exist ambiguous parsing in t"
2021.acl-long.449,P14-1002,0,0.204152,"and space efficiency. 1 Introduction Discourse parsing, aiming to find how the text spans in a document relate to each other, benefits various down-stream tasks, such as machine translation evaluation (Guzm´an et al., 2014; Joty et al., 2014), summarization (Marcu, 2000; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015; Huber and Carenini, 2020) and automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2013). Researchers have made impressive progress on discourse parsing from the constituency perspective, which presents discourse structures as constituency trees (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Joty et al., 2015; Nishida and * Corresponding author. Nakayama, 2020). However, as demonstrated by Morey et al. (2018), discourse structure can also be formulated as a dependency structure. Besides that, there might exist ambiguous parsing in terms of the constituency perspective (Morey et al., 2018). All of these suggest that dependency discourse parsing is a different promising approach for discourse parsing. One of the main bottlenecks in developing discourse dependency parsing methods is the lack of annotated training data since the labeling effort is labor-intensi"
2021.acl-long.449,D16-1073,1,0.781676,"rained algorithm adopted by most previous work has a complexity of O(n3 ). The code and trained models can be found at: https://github. com/Ehaschia/DiscourseDependencyParsing. 2 Related Work Unsupervised syntactic dependency parsing Unsupervised syntactic dependency parsing is the task to find syntactic dependency relations between words in sentences without guidance from annotations. The most popular approaches to this task are Dependency Model with Valences (DMV, Klein and Manning (2004)), a generative model learning the grammar from POS tags for dependency predictions, and its extensions. Jiang et al. (2016) employ neural networks to capture the similarities between POS tags ignored by vanilla DMV and Han et al. (2019) further amend the former with discriminative information obtained from an additional encoding network. Besides, there are also some discriminative approaches modeling the conditional probability or score of the dependency tree given the sentence, such as the CRF autoencoder method proposed by Cai et al. (2017). Discourse dependency parsing There is limited work focusing on discourse dependency parsing. Li et al. (2014) proposes an algorithm to convert constituency RST tree to depen"
2021.acl-long.449,J15-3002,0,0.362687,"parsing, aiming to find how the text spans in a document relate to each other, benefits various down-stream tasks, such as machine translation evaluation (Guzm´an et al., 2014; Joty et al., 2014), summarization (Marcu, 2000; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015; Huber and Carenini, 2020) and automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2013). Researchers have made impressive progress on discourse parsing from the constituency perspective, which presents discourse structures as constituency trees (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Joty et al., 2015; Nishida and * Corresponding author. Nakayama, 2020). However, as demonstrated by Morey et al. (2018), discourse structure can also be formulated as a dependency structure. Besides that, there might exist ambiguous parsing in terms of the constituency perspective (Morey et al., 2018). All of these suggest that dependency discourse parsing is a different promising approach for discourse parsing. One of the main bottlenecks in developing discourse dependency parsing methods is the lack of annotated training data since the labeling effort is labor-intensive and time-consuming, and needs well-tra"
2021.acl-long.449,W14-3352,0,0.0338969,"Missing"
2021.acl-long.449,P04-1061,0,0.257917,"d space efficiency. Its empirical time and space complexity is close to O(n2 ) with n being the number of EDUs, while the unconstrained algorithm adopted by most previous work has a complexity of O(n3 ). The code and trained models can be found at: https://github. com/Ehaschia/DiscourseDependencyParsing. 2 Related Work Unsupervised syntactic dependency parsing Unsupervised syntactic dependency parsing is the task to find syntactic dependency relations between words in sentences without guidance from annotations. The most popular approaches to this task are Dependency Model with Valences (DMV, Klein and Manning (2004)), a generative model learning the grammar from POS tags for dependency predictions, and its extensions. Jiang et al. (2016) employ neural networks to capture the similarities between POS tags ignored by vanilla DMV and Han et al. (2019) further amend the former with discriminative information obtained from an additional encoding network. Besides, there are also some discriminative approaches modeling the conditional probability or score of the dependency tree given the sentence, such as the CRF autoencoder method proposed by Cai et al. (2017). Discourse dependency parsing There is limited wor"
2021.acl-long.449,D19-1587,0,0.0135223,"ns on dependency trees converted from RSTDT. Their parser achieved better performance on the summarization task than a similar constituencybased parser. Morey et al. (2018) reviews the RST discourse parsing from the dependency perspective. They adapt the the best discourse constituency parsing models until 2018 to the dependency task. Yang and Li (2018) constructs a discourse dependency treebank SciDTB for scientific abstracts. To the best of our knowledge, we are the first to investigate unsupervised and semi-supervised discourse dependency parsing. Unsupervised Constituent Discourse Parsing Kobayashi et al. (2019) propose two unsupervised methods that build unlabeled constituent discourse trees by using the CKY dynamic programming algorithm. Their methods build the optimal tree in terms of a similarity (dissimilarity) score function that is defined for merging (splitting) text spans into larger (smaller) ones. Nishida et al. (2020) use Viterbi EM with a margin-based criterion to train a span-based neural unsupervised constituency discourse parser. The performance of these unsupervised methods is close to that of previous supervised parsers. 3 Adaptation We propose an adaptation method that can be readi"
2021.acl-long.449,P14-1003,0,0.400304,"rom POS tags for dependency predictions, and its extensions. Jiang et al. (2016) employ neural networks to capture the similarities between POS tags ignored by vanilla DMV and Han et al. (2019) further amend the former with discriminative information obtained from an additional encoding network. Besides, there are also some discriminative approaches modeling the conditional probability or score of the dependency tree given the sentence, such as the CRF autoencoder method proposed by Cai et al. (2017). Discourse dependency parsing There is limited work focusing on discourse dependency parsing. Li et al. (2014) proposes an algorithm to convert constituency RST tree to dependency structure. In their algorithm, each non-terminal is assigned with a head EDU, which is the head EDU of its leftmost nucleus child. Then, a dependency relation is created for each non-terminal from its head to its dependent, in a procedure similar to those designed for syntactic parsing. Hirao et al. (2013) proposes another method that differs from the previous one in the processing of multinuclear relations. Yoshida et al. (2014) proposes a dependency parser built around a Maximum Spanning Tree decoder and trains on dependen"
2021.acl-long.449,2020.findings-emnlp.193,1,0.925589,"nual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5782–5794 August 1–6, 2021. ©2021 Association for Computational Linguistics lenges, we cluster the EDUs to produce clusters resembling Part-Of-Speech (POS) tags in syntactic parsing and we introduce the Hierarchical Eisner algorithm that finds the optimal parse tree conforming to the constraint. We applied our adaptation method to two stateof-the-art unsupervised syntactic dependency parsing models: Neural Conditional Random Field Autoencoder (NCRFAE, Li and Tu (2020)) and Variational Variant of Discriminative Neural Dependency Model with Valences (V-DNDMV, Han et al. (2019)). In our experiments, the adapted models performs better than the baseline on both RST Discourse Treebank (RST-DT, Carlson et al. (2001)) and SciDTB (Yang and Li, 2018) in the unsupervised setting. When we extend the two models to the semi-supervised and supervised setting, we find they can outperform previous methods specially designed for supervised discourse parsing. Further analysis indicates that the Hierarchical Eisner algorithm shows superiority not only in parsing accuracy but"
2021.acl-long.449,W99-0307,0,0.498705,"kayama, 2020). However, as demonstrated by Morey et al. (2018), discourse structure can also be formulated as a dependency structure. Besides that, there might exist ambiguous parsing in terms of the constituency perspective (Morey et al., 2018). All of these suggest that dependency discourse parsing is a different promising approach for discourse parsing. One of the main bottlenecks in developing discourse dependency parsing methods is the lack of annotated training data since the labeling effort is labor-intensive and time-consuming, and needs well-trained experts with linguistic knowledge (Marcu et al., 1999). This problem can be tackled by employing unsupervised and semisupervised methods that can utilize unlabeled data. However, while unsupervised methodology has been studied for decades in syntactic dependency parsing, there is little attention paid to the counterpart in discourse dependency parsing. Considering the similarity between syntactic and discourse dependency parsing, it is natural to suggest such methodology can be adapted from the former to the latter. In this paper, we propose a simple yet effective adaptation method that can be readily applied to different unsupervised syntactic d"
2021.acl-long.449,J18-2001,0,0.723873,"n-stream tasks, such as machine translation evaluation (Guzm´an et al., 2014; Joty et al., 2014), summarization (Marcu, 2000; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015; Huber and Carenini, 2020) and automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2013). Researchers have made impressive progress on discourse parsing from the constituency perspective, which presents discourse structures as constituency trees (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Joty et al., 2015; Nishida and * Corresponding author. Nakayama, 2020). However, as demonstrated by Morey et al. (2018), discourse structure can also be formulated as a dependency structure. Besides that, there might exist ambiguous parsing in terms of the constituency perspective (Morey et al., 2018). All of these suggest that dependency discourse parsing is a different promising approach for discourse parsing. One of the main bottlenecks in developing discourse dependency parsing methods is the lack of annotated training data since the labeling effort is labor-intensive and time-consuming, and needs well-trained experts with linguistic knowledge (Marcu et al., 1999). This problem can be tackled by employing"
2021.acl-long.449,2020.tacl-1.15,0,0.0291652,"structure, like x1 → x2 , x2 → x3 · · · . In order to develop a strong baseline, we include the hierarchical constraint introduced in Section 3.2 in this procedure. That is, we first build sentence-level discourse trees using the right branching method based on sentence segmentation. Then we build paragraph-level trees using the right branching method to form a left to right chain of sentencelevel subtrees. Finally we obtain document-level trees in the same way. Since this method has three stages, we call it “RB RB RB”. This simple procedure forms a strong baseline in terms of performance. As Nishida and Nakayama (2020) reports, the unlabeled F1 score of constituent structures of RB RB RB reaches 79.9 on RST-DT. Correspondingly, the performance of the supervised method proposed by (Joty et al., 2015) is 82.5. NISHIDA20 is a neural model for unsupervised discourse constituency parsing proposed by Nishida and Nakayama (2020). This model runs a CKY parser that uses a Bi-LSTM model to learn representations of text spans, complemented with lexical, syntactic and structural features. We convert its result to dependency structure using the same conversation method of Li et al. (2014). To make a fair comparison, we"
2021.acl-long.449,W04-2407,0,0.0671169,". Its empirical time and space complexity is close to O(n2 ) with n being the number of EDUs, while the unconstrained algorithm adopted by most previous work has a complexity of O(n3 ). The code and trained models can be found at: https://github. com/Ehaschia/DiscourseDependencyParsing. 2 Related Work Unsupervised syntactic dependency parsing Unsupervised syntactic dependency parsing is the task to find syntactic dependency relations between words in sentences without guidance from annotations. The most popular approaches to this task are Dependency Model with Valences (DMV, Klein and Manning (2004)), a generative model learning the grammar from POS tags for dependency predictions, and its extensions. Jiang et al. (2016) employ neural networks to capture the similarities between POS tags ignored by vanilla DMV and Han et al. (2019) further amend the former with discriminative information obtained from an additional encoding network. Besides, there are also some discriminative approaches modeling the conditional probability or score of the dependency tree given the sentence, such as the CRF autoencoder method proposed by Cai et al. (2017). Discourse dependency parsing There is limited wor"
2021.acl-long.449,D14-1162,0,0.0867372,"e pretrained language models (like BERT, ELMo (Peters et al., 2018)) to extract features. As Table 3 shows, the performance of our NCRFAE is significantly better than the baseline models. Especially, the UAS and LAS of NCRFAE are 8.9 points and 11.5 points higher than the best baseline models on the SciDTB dataset, respectively. Besides that, we find that V-DNDMV also beats baselines on the SciDTB dataset and reaches comparable results on RST-DT. We also test our approaches without using BERT and find that they still outperform the baselines. For example, the performance of NCRFAE with GloVe (Pennington et al., 2014) on Scidtb averaged over 5 runs is: UAS: 73.9 LAS: 55.5. These results again give evidence for our success in adapting unsupervised syntactic dependency parsing methods for discourse dependency parsing as the adapted methods not only work in the unsupervised setting, but also reach state-of-the-art in the supervised setting. As for the performance gap between V-DNDMV and NCRFAE, we believe that the main reason is their different abilities to extract contextual features from the input text for the parsing task. As a generative model, the decoder of V-DNDMV follows 5788 Clusters UAS 10 52.7 30 5"
2021.acl-long.449,N18-1202,0,0.0076795,"rrect their evaluation metrics, so the result is different from the original paper (Li et al., 2014). (Ji and Eisenstein, 2014), JOTY15 (Joty et al., 2015) and BRAUD17 (Braud et al., 2017) are methods for discourse constituent parsing and they are adapted for discourse dependency parsing by Morey et al. (2018). LI14 (Li et al., 2014) and MOREY18 (Morey et al., 2018) are graph-based and transition-based methods specially designed for discourse dependency parsing, respectively. These models are statistical or simple neural models, and they do not use pretrained language models (like BERT, ELMo (Peters et al., 2018)) to extract features. As Table 3 shows, the performance of our NCRFAE is significantly better than the baseline models. Especially, the UAS and LAS of NCRFAE are 8.9 points and 11.5 points higher than the best baseline models on the SciDTB dataset, respectively. Besides that, we find that V-DNDMV also beats baselines on the SciDTB dataset and reaches comparable results on RST-DT. We also test our approaches without using BERT and find that they still outperform the baselines. For example, the performance of NCRFAE with GloVe (Pennington et al., 2014) on Scidtb averaged over 5 runs is: UAS: 73"
2021.acl-long.449,2020.repl4nlp-1.20,0,0.01192,"of the classic Eisner algorithm, used for parse tree to produce discourse dependency parse trees that conform to the constraint that every sentence or paragraph should correspond to a complete subtree. 3.1 Clustering Given an input document represented as an EDU sequence x1 , x2 , . . . , xn , we can use word embedding or context sensitive word embedding to get the vector representation xi of the i-th EDU xi . Specifically, we use BERT (Devlin et al., 2019) to encode each word. Let wi be the encoding of the i-th word in the document. For an EDU xi spanning from word position b to e, we follow Toshniwal et al. (2020) and concatenate the encoding of the endpoints to form its representation: xi = [wb ; we ]. With the representations of all EDUs from the whole training corpus obtained, we use K-Means (Lloyd, 1982) to cluster them. Let ci be the cluster label of xi . 3.2 Ratio RST-DT SciDTB Hierarchical Eisner Algorithm The Eisner algorithm (Eisner, 1996) is a dynamic programming algorithm widely used to find the optimal syntactic dependency parse tree. The basic idea of it is to parse the left and right dependents of an token independently and combine them at a later stage. Algorithm 1 shows the pseudo-code"
2021.acl-long.449,P17-2029,0,0.0156216,"e discriminative model already outperforms the generative model significantly. Besides that, we also find our semi-supervised methods reach higher UAS scores than their supervised versions (trained with labeled data only) for all the labeled/unlabeled data ratios. Inspired by the promising results in the semisupervised setting, we also investigate the performance of our adapted NCRFAE and V-DNDMV in the fully supervised setting. The results are shown in Table 3. We evaluate our models on the RSTDT and SciDTB datasets and compare them with eight models. NIVRE04 (Nivre et al., 2004) and WANG17 (Wang et al., 2017) are two transitionbased models for dependency parsing. Yang and Li (2018) adapts them to discourse dependency parsing. FENG14 (Feng and Hirst, 2014), JI14 ‡ We correct their evaluation metrics, so the result is different from the original paper (Li et al., 2014). (Ji and Eisenstein, 2014), JOTY15 (Joty et al., 2015) and BRAUD17 (Braud et al., 2017) are methods for discourse constituent parsing and they are adapted for discourse dependency parsing by Morey et al. (2018). LI14 (Li et al., 2014) and MOREY18 (Morey et al., 2018) are graph-based and transition-based methods specially designed for"
2021.acl-long.449,P18-2071,0,0.253536,"ng Part-Of-Speech (POS) tags in syntactic parsing and we introduce the Hierarchical Eisner algorithm that finds the optimal parse tree conforming to the constraint. We applied our adaptation method to two stateof-the-art unsupervised syntactic dependency parsing models: Neural Conditional Random Field Autoencoder (NCRFAE, Li and Tu (2020)) and Variational Variant of Discriminative Neural Dependency Model with Valences (V-DNDMV, Han et al. (2019)). In our experiments, the adapted models performs better than the baseline on both RST Discourse Treebank (RST-DT, Carlson et al. (2001)) and SciDTB (Yang and Li, 2018) in the unsupervised setting. When we extend the two models to the semi-supervised and supervised setting, we find they can outperform previous methods specially designed for supervised discourse parsing. Further analysis indicates that the Hierarchical Eisner algorithm shows superiority not only in parsing accuracy but also in time and space efficiency. Its empirical time and space complexity is close to O(n2 ) with n being the number of EDUs, while the unconstrained algorithm adopted by most previous work has a complexity of O(n3 ). The code and trained models can be found at: https://github"
2021.acl-long.449,D14-1196,0,0.0222985,"(2017). Discourse dependency parsing There is limited work focusing on discourse dependency parsing. Li et al. (2014) proposes an algorithm to convert constituency RST tree to dependency structure. In their algorithm, each non-terminal is assigned with a head EDU, which is the head EDU of its leftmost nucleus child. Then, a dependency relation is created for each non-terminal from its head to its dependent, in a procedure similar to those designed for syntactic parsing. Hirao et al. (2013) proposes another method that differs from the previous one in the processing of multinuclear relations. Yoshida et al. (2014) proposes a dependency parser built around a Maximum Spanning Tree decoder and trains on dependency trees converted from RSTDT. Their parser achieved better performance on the summarization task than a similar constituencybased parser. Morey et al. (2018) reviews the RST discourse parsing from the dependency perspective. They adapt the the best discourse constituency parsing models until 2018 to the dependency task. Yang and Li (2018) constructs a discourse dependency treebank SciDTB for scientific abstracts. To the best of our knowledge, we are the first to investigate unsupervised and semi-s"
2021.acl-long.449,2020.lrec-1.663,0,0.0296521,"ency task. Yang and Li (2018) constructs a discourse dependency treebank SciDTB for scientific abstracts. To the best of our knowledge, we are the first to investigate unsupervised and semi-supervised discourse dependency parsing. Unsupervised Constituent Discourse Parsing Kobayashi et al. (2019) propose two unsupervised methods that build unlabeled constituent discourse trees by using the CKY dynamic programming algorithm. Their methods build the optimal tree in terms of a similarity (dissimilarity) score function that is defined for merging (splitting) text spans into larger (smaller) ones. Nishida et al. (2020) use Viterbi EM with a margin-based criterion to train a span-based neural unsupervised constituency discourse parser. The performance of these unsupervised methods is close to that of previous supervised parsers. 3 Adaptation We propose an adaptation method that can be readily integrated with different unsupervised syntactic dependency parsing approaches. First, we cluster the element discourse units (EDU) to produce clusters resembling POS tags or words used in syntactic parsing. This is necessary because many unsupervised syntactic parsers require enumeration of words or word categories, ty"
2021.acl-long.46,C18-1139,0,0.441402,"ank (PTB) 3.0 and follow the same pre-processing pipeline as in Ma et al. (2018). For unlabeled data, we sample sentences that belong to the same languages of the labeled data from the WikiAnn datasets for Case 1a, 2a and 4 and we sample sentences from the target languages of WikiAnn datasets for Case 3. We use the BLLIP corpus3 as the unlabeled data for Case 1b and 2b. Models For the student models in all the cases, we use fastText (Bojanowski et al., 2017) word embeddings and character embeddings as the word representation. For Case 1a, 2a and 4, we concatenate the multilingual BERT, Flair (Akbik et al., 2018), fastText embeddings and character embeddings (Santos and Zadrozny, 2014) as the word representations for stronger monolingual teacher models (Wang et al., 2020c). For Case 3, we use MBERT embeddings for the teacher. Also for Case 3, we fine-tune the teacher model on the training set of the four Indo-European languages from the WikiAnn dataset and train student models on the four additional languages. For the teacher models in Case 1b and 2b, we simply use the same embeddings as the student because there is already huge performance gap between the teacher and student in these settings and hen"
2021.acl-long.46,2020.iwpt-1.2,0,0.0868451,"Missing"
2021.acl-long.46,Q17-1010,0,0.0229249,"or English and 5000 sentences for each of the other languages. We split the datasets by 3:1:1 for training/development/test. For Case 1b and 2b, we use Penn Treebank (PTB) 3.0 and follow the same pre-processing pipeline as in Ma et al. (2018). For unlabeled data, we sample sentences that belong to the same languages of the labeled data from the WikiAnn datasets for Case 1a, 2a and 4 and we sample sentences from the target languages of WikiAnn datasets for Case 3. We use the BLLIP corpus3 as the unlabeled data for Case 1b and 2b. Models For the student models in all the cases, we use fastText (Bojanowski et al., 2017) word embeddings and character embeddings as the word representation. For Case 1a, 2a and 4, we concatenate the multilingual BERT, Flair (Akbik et al., 2018), fastText embeddings and character embeddings (Santos and Zadrozny, 2014) as the word representations for stronger monolingual teacher models (Wang et al., 2020c). For Case 3, we use MBERT embeddings for the teacher. Also for Case 3, we fine-tune the teacher model on the training set of the four Indo-European languages from the WikiAnn dataset and train student models on the four additional languages. For the teacher models in Case 1b and"
2021.acl-long.46,P19-1595,0,0.0149707,"of a large teacher model. The typical KD objective function is the cross-entropy between the output distributions predicted by the teacher model and the student model: X LKD = − Pt (y|x) log Ps (y|x) (2) y∈Y(x) where Pt and Ps are the teacher’s and the student’s distributions respectively. During training, the student jointly learns from the gold targets and the distributions predicted by the teacher by optimizing the following objective function: Lstudent = λLKD + (1 − λ)Ltarget where λ is an interpolation coefficient between the target loss Ltarget and the structural KD loss LKD . Following Clark et al. (2019); Wang et al. (2020a), one may apply teacher annealing in training by decreasing λ linearly from 1 to 0. Because KD does not require gold labels, unlabeled data can also be used in the KD loss. 551 3 Structural Knowledge Distillation When performing knowledge distillation on structured prediction, a major challenge is that the structured output space is exponential in size, leading to intractable computation of the KD objective in Eq. 2. However, if the scoring function of the student model can be factorized into scores of substructures (Eq. 1), then we can derive the following factorized form"
2021.acl-long.46,2020.acl-main.747,0,0.135739,"Missing"
2021.acl-long.46,N19-1423,0,0.273421,"This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/StructuralKD. ♠ as online serving. An interesting and viable solution to this problem is knowledge distillation (KD) (Buciluˇa et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015), which can be used to transfer the knowledge of a large model (the teacher) to a smaller model (the student). In the field of natural language processing (NLP), for example, KD has been successfully applied to compress massive pretrained language models such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) into much smaller and faster models without significant loss in accuracy (Tang et al., 2019; Sanh et al., 2019; Tsai et al., 2019; Mukherjee and Hassan Awadallah, 2020). A typical approach to KD is letting the student mimic the teacher model’s output probability distributions on the training data by using the cross-entropy objective. For structured prediction problems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label se"
2021.acl-long.46,K17-3002,0,0.015241,"l for named entity recognition (NER) that is based on large pretrained contextualized embeddings to a smaller CRF model with static embeddings that is more suitable for fast online serving. For a CRF student model described in section 2.1, if we absorb the emission score Se (yi , x) into the transition score St ((yi−1 , yi ), x) at each position i, then the substructure space Us (x) contains every two adjacent labels {(yi−1 , yi )} for i=1, . . . , n, with n beCase 1b: Graph-based Dependency Parsing ⇒ Dependency Parsing as Sequence Labeling In this case, we use the biaffine parser proposed by Dozat et al. (2017) as the teacher and the sequence labeling approach proposed by Strzyz et al. (2019) as the student for the dependency parsing task. The biaffine parser is one of the state-of-the-art models, while the sequence labeling parser provides a good speed-accuracy tradeoff. There is a big gap in accuracy between the two models and therefore KD can be used to improve the accuracy of the sequence labeling parser. Here we follow the head-selection formulation of dependency parsing without the tree constraint. The dependency parse tree y is represented by hy1 , . . . , yn i, where n is the sentence length"
2021.acl-long.46,P19-1266,0,0.0536502,"Missing"
2021.acl-long.46,D16-1139,0,0.172232,"py objective. For structured prediction problems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label set is L, then there are Ln possible label sequences for a sentence of n words and it is infeasible to compute the cross-entropy by enumerating the label sequences. Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a). In this paper, we derive a factorized form of the structural KD objective based on the fact that almost all the structured prediction models factorize the scoring function of the output structure into scores of substructures. If the student’s substructure space is polynomial in size and the teacher’s 550 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 550–564 August 1–6, 2021. ©2021 Association for Computational Linguistics margi"
2021.acl-long.46,D16-1180,0,0.12717,"ructured prediction problems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label set is L, then there are Ln possible label sequences for a sentence of n words and it is infeasible to compute the cross-entropy by enumerating the label sequences. Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a). In this paper, we derive a factorized form of the structural KD objective based on the fact that almost all the structured prediction models factorize the scoring function of the output structure into scores of substructures. If the student’s substructure space is polynomial in size and the teacher’s 550 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 550–564 August 1–6, 2021. ©2021 Association for Computational Linguistics marginal distributions over"
2021.acl-long.46,P19-1233,0,0.0774946,"ictions from marginal distributions of the CRF teacher still outperform MaxEnt, those of the NER-as-parsing teacher clearly underperform MaxEnt. This provides an explanation as to why Struct. KD in Case 4 has equal or even lower accuracy than the Token KD baseline in Case 2a in Table 3. 6 6.1 Related Work Structured Prediction In this paper, we use sequence labeling and dependency parsing as two example structured prediction tasks. In sequence labeling, a lot of work applied the linear-chain CRF and achieved state-of-the-art performance in various tasks (Ma and Hovy, 2016; Akbik et al., 2018; Liu et al., 2019b; Yu et al., 2020; Wei et al., 2020; Wang et al., 2021a,b). Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training. Another advantage of MaxEnt in comparison with CRF is its speed. Yang et al. (2018) showed that models equipped with the CRF are about two times slower than models with the MaxEnt layer in sequence labeling. In dependency parsing, recent work shows that second-order CRF parsers achieve sign"
2021.acl-long.46,P16-1101,0,0.0363014,"ons between adjacent labels. While predictions from marginal distributions of the CRF teacher still outperform MaxEnt, those of the NER-as-parsing teacher clearly underperform MaxEnt. This provides an explanation as to why Struct. KD in Case 4 has equal or even lower accuracy than the Token KD baseline in Case 2a in Table 3. 6 6.1 Related Work Structured Prediction In this paper, we use sequence labeling and dependency parsing as two example structured prediction tasks. In sequence labeling, a lot of work applied the linear-chain CRF and achieved state-of-the-art performance in various tasks (Ma and Hovy, 2016; Akbik et al., 2018; Liu et al., 2019b; Yu et al., 2020; Wei et al., 2020; Wang et al., 2021a,b). Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training. Another advantage of MaxEnt in comparison with CRF is its speed. Yang et al. (2018) showed that models equipped with the CRF are about two times slower than models with the MaxEnt layer in sequence labeling. In dependency parsing, recent work shows that"
2021.acl-long.46,P18-1130,0,0.017162,"s-lingual transfer in Case 3, we use the four Indo-European languages as the source for the teacher model and additionally select four languages from different language families as the target for the student models.2 We use the standard training/development/test split for the CoNLL datasets. For WikiAnn, we follow the sampling of Wang et al. (2020a) with 12000 sentences for English and 5000 sentences for each of the other languages. We split the datasets by 3:1:1 for training/development/test. For Case 1b and 2b, we use Penn Treebank (PTB) 3.0 and follow the same pre-processing pipeline as in Ma et al. (2018). For unlabeled data, we sample sentences that belong to the same languages of the labeled data from the WikiAnn datasets for Case 1a, 2a and 4 and we sample sentences from the target languages of WikiAnn datasets for Case 3. We use the BLLIP corpus3 as the unlabeled data for Case 1b and 2b. Models For the student models in all the cases, we use fastText (Bojanowski et al., 2017) word embeddings and character embeddings as the word representation. For Case 1a, 2a and 4, we concatenate the multilingual BERT, Flair (Akbik et al., 2018), fastText embeddings and character embeddings (Santos and Za"
2021.acl-long.46,2020.acl-main.202,0,0.036762,"Missing"
2021.acl-long.46,P17-1178,0,0.0170604,"e i-th word as its head and with its length larger than 1. It is intractable to compute such marginal probabilities by enumerating all the output structures, but we can tractably compute them using dynamic programming. See supplementary material for a detailed description of our dynamic programming method. 4 Experiments We evaluate our approaches described in Section 3 on NER (Case 1a, 2a, 3, 4) and dependency parsing (Case 1b, 2b). 4.1 Settings Datasets We use CoNLL 2002/2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for Case 1a, 2a and 4, and use WikiAnn datasets (Pan et al., 2017) for Case 1a, 2a, 3, and 4. The CoNLL datasets contain the corpora of four Indo-European languages. We use the same four languages from the WikiAnn datasets. For cross-lingual transfer in Case 3, we use the four Indo-European languages as the source for the teacher model and additionally select four languages from different language families as the target for the student models.2 We use the standard training/development/test split for the CoNLL datasets. For WikiAnn, we follow the sampling of Wang et al. (2020a) with 12000 sentences for English and 5000 sentences for each of the other language"
2021.acl-long.46,P19-1493,0,0.0162243,"re consecutive labels {(yi−1 , yi )}. In contrast, a MaxEnt model predicts the label probability distribution Ps (yi |x) of each token independently and hence the substructure space Us (x) consists of every individual label {yi }. To calculate the substructure marginal of the teacher Pt (yi |x), we can again utilize the forwardbackward algorithm: Teacher Factorization Produces More Fine-grained Substructures than Student Factorization Case 3: MaxEnt ⇒ Linear-Chain CRF Here we consider KD in the opposite direction of Case 2a. An example application is zero-shot crosslingual NER. Previous work (Pires et al., 2019; Wu and Dredze, 2019) has shown that multilingual BERT (M-BERT) has strong zero-shot crosslingual transferability in NER tasks. Many such models employ a MaxEnt decoder. In scenarios requiring fast speed and low computation cost, however, we may want to distill knowledge from such models to a model with much cheaper static monolingual embeddings while compensating the performance loss with a linear-chain CRF decoder. As described in Case 1a, the substructures of a linear-chain CRF model are consecutive labels {(yi−1 , yi )}. Because of the label independence and local normalization in the Max"
2021.acl-long.46,N19-1335,0,0.0202894,"le head-selection first-order approach (Dozat and Manning, 2017). Such speed-accuracy tradeoff as seen in sequence labeling and dependency parsing also occurs in many other structured prediction tasks. This makes KD an interesting and very useful technique that can be used to circumvent this tradeoff to some extent. 6.2 Knowledge Distillation in Structured Prediction KD has been applied in many structured prediction tasks in the fields of NLP, speech recognition and computer vision, with applications such as neural machine translation (Kim and Rush, 2016; Tan et al., 2019), sequence labeling (Tu and Gimpel, 2019; Wang et al., 2020a), connectionist temporal classification (Huang et al., 2018), image semantic segmentation (Liu et al., 2019a) and so on. In KD for structured prediction tasks, how to handle the exponential number of structured outputs is a main challenge. To address this difficult problem, recent work resorts to approximation of the KD objective. Kim and Rush (2016) proposed sequence-level distillation through predicting K-best sequences of the teacher in neural machine translation. Kuncoro et al. (2016) proposed to use multiple greedy parsers as teachers and generate the probability dist"
2021.acl-long.46,P19-1454,1,0.926723,"to the following form without the need for calculating the student partition function Zs (x). LKD = − X Pt (u|x) × logPs (u|x) (6) u∈Us (x) In all the cases except Case 1a and Case 3, the student model is locally normalized and hence we can follow this form of objective. 3.2 parse tree independently. A second-order dependency parser scores pairs of dependency arcs with a shared token. The substructures of second-order parsing are therefore all the dependency arc pairs with a shared token. It has been found that secondorder extensions of the biaffine parser often have higher parsing accuracy (Wang et al., 2019; Zhang et al., 2020; Wang et al., 2020d; Wang and Tu, 2020). Therefore, we may take a second-order dependency parser as the teacher to improve a sequence labeling parser. Here we consider the second-order dependency parser of Wang and Tu (2020). It employs mean field variational inference to estimate the probabilities of arc existence Pt (hi |x) and uses a first-order biaffine model to estimate the probabilities of arc labels Pt (li |x). Therefore, the substructure marginal can be calculated in the same way as Eq. 5. 3.3 Student Factorization Produces More Fine-grained Substructures than Teac"
2021.acl-long.46,2020.acl-main.304,1,0.125515,"oblems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label set is L, then there are Ln possible label sequences for a sentence of n words and it is infeasible to compute the cross-entropy by enumerating the label sequences. Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a). In this paper, we derive a factorized form of the structural KD objective based on the fact that almost all the structured prediction models factorize the scoring function of the output structure into scores of substructures. If the student’s substructure space is polynomial in size and the teacher’s 550 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 550–564 August 1–6, 2021. ©2021 Association for Computational Linguistics marginal distributions over these substructure"
2021.acl-long.46,2020.emnlp-main.485,1,0.0750727,"oblems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label set is L, then there are Ln possible label sequences for a sentence of n words and it is infeasible to compute the cross-entropy by enumerating the label sequences. Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a). In this paper, we derive a factorized form of the structural KD objective based on the fact that almost all the structured prediction models factorize the scoring function of the output structure into scores of substructures. If the student’s substructure space is polynomial in size and the teacher’s 550 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 550–564 August 1–6, 2021. ©2021 Association for Computational Linguistics marginal distributions over these substructure"
2021.acl-long.46,2021.acl-long.206,1,0.538582,"still outperform MaxEnt, those of the NER-as-parsing teacher clearly underperform MaxEnt. This provides an explanation as to why Struct. KD in Case 4 has equal or even lower accuracy than the Token KD baseline in Case 2a in Table 3. 6 6.1 Related Work Structured Prediction In this paper, we use sequence labeling and dependency parsing as two example structured prediction tasks. In sequence labeling, a lot of work applied the linear-chain CRF and achieved state-of-the-art performance in various tasks (Ma and Hovy, 2016; Akbik et al., 2018; Liu et al., 2019b; Yu et al., 2020; Wei et al., 2020; Wang et al., 2021a,b). Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training. Another advantage of MaxEnt in comparison with CRF is its speed. Yang et al. (2018) showed that models equipped with the CRF are about two times slower than models with the MaxEnt layer in sequence labeling. In dependency parsing, recent work shows that second-order CRF parsers achieve significantly higher accuracy than first-order parsers (Wan"
2021.acl-long.46,2021.acl-long.142,1,0.721594,"still outperform MaxEnt, those of the NER-as-parsing teacher clearly underperform MaxEnt. This provides an explanation as to why Struct. KD in Case 4 has equal or even lower accuracy than the Token KD baseline in Case 2a in Table 3. 6 6.1 Related Work Structured Prediction In this paper, we use sequence labeling and dependency parsing as two example structured prediction tasks. In sequence labeling, a lot of work applied the linear-chain CRF and achieved state-of-the-art performance in various tasks (Ma and Hovy, 2016; Akbik et al., 2018; Liu et al., 2019b; Yu et al., 2020; Wei et al., 2020; Wang et al., 2021a,b). Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training. Another advantage of MaxEnt in comparison with CRF is its speed. Yang et al. (2018) showed that models equipped with the CRF are about two times slower than models with the MaxEnt layer in sequence labeling. In dependency parsing, recent work shows that second-order CRF parsers achieve significantly higher accuracy than first-order parsers (Wan"
2021.eacl-tutorials.1,D16-1073,1,0.841115,"d and third parts, we will introduce in detail two major classes of approaches to unsupervised parsing, generative and discriminative approaches, and discuss their pros and cons. The second part will cover generative approaches, which model the joint probability of the sentence and the corresponding parse tree. Most of the existing generative approaches are based on generative grammars, in particular context-free grammars and dependency models with valence (Klein and Manning, 2004). There are also featurized and neural extensions of generative grammars, such as Berg-Kirkpatrick et al. (2010); Jiang et al. (2016). We will divide our discussion of learning generative grammars into two parts: structure learning and parameter learning. Structure learning concerns finding the optimal set of grammar rules. We will introduce both probabilistic methods such as Stolcke and Omohundro (1994) and heuristic methods such as Clark (2007). Parameter learning concerns learning the probabilities or weights of a pre-specified set of grammar rules. We will discuss a variety of priors and regularizations designed to improve parameter learning, such as Cohen and Smith (2010), Tu and Honavar (2012), Noji et al. (2016), and"
2021.eacl-tutorials.1,D18-1292,0,0.0118435,"We will divide our discussion of learning generative grammars into two parts: structure learning and parameter learning. Structure learning concerns finding the optimal set of grammar rules. We will introduce both probabilistic methods such as Stolcke and Omohundro (1994) and heuristic methods such as Clark (2007). Parameter learning concerns learning the probabilities or weights of a pre-specified set of grammar rules. We will discuss a variety of priors and regularizations designed to improve parameter learning, such as Cohen and Smith (2010), Tu and Honavar (2012), Noji et al. (2016), and (Jin et al., 2018). 3 Outline Part 1. Introduction [20 min] 2 5 • Problem definition Reading List Klein and Manning (2004) – An influential generative approach to unsupervised dependency parsing that is the basis for many subsequent papers. • Motivations and applications • Evaluation • Overview of approaches Jiang et al. (2016) – A neural extension of Klein and Manning (2004). One of the first modern neural approaches to unsupervised parsing. Part 2. Generative Approaches [60 min] • Overview Stolcke and Omohundro (1994) – One of the first structure learning approaches of context-free grammars for unsupervised c"
2021.eacl-tutorials.1,N07-1018,0,0.098062,"67) and algorithmic and empirical studies in 1970s (Baker, 1979). Although deemed an interesting topic by the NLP community, unsupervised parsing had received much less attention than supervised parsing over the past few decades. 1 Proceedings of EACL: Tutorials, pages 1–5 April 19 - 20, 2021. ©2020 Association for Computational Linguistics ing and apply or extend them to other NLP tasks that can potentially benefit from implicitly learned linguistic structures. 2 We will also discuss parameter learning algorithms such as expectation-maximization (Baker, 1979; Spitkovsky et al., 2010b), MCMC (Johnson et al., 2007) and curriculum learning (Spitkovsky et al., 2010a). After introducing approaches based on generative grammars, we will discuss recent approaches that are instead based on neural language models (Shen et al., 2018, 2019). The third part will cover discriminative approaches, which model the conditional probability or score of the parse tree given the sentence. We will first introduce autoencoder approaches such as Cai et al. (2017), which contain an encoder that maps the sentence to an intermediate representation (such as a parse tree) and a decoder that tries to reconstruct the sentence. Their"
2021.eacl-tutorials.1,N10-1083,0,0.131694,"Missing"
2021.eacl-tutorials.1,N19-1114,0,0.0768135,"ent approaches that are instead based on neural language models (Shen et al., 2018, 2019). The third part will cover discriminative approaches, which model the conditional probability or score of the parse tree given the sentence. We will first introduce autoencoder approaches such as Cai et al. (2017), which contain an encoder that maps the sentence to an intermediate representation (such as a parse tree) and a decoder that tries to reconstruct the sentence. Their training objective is typically the reconstruction probability. We will then introduce variational autoencoder approaches such as Kim et al. (2019), which has a similar model structure to autoencoder approaches but uses the evidence lower bound as the training objective. Finally, we will briefly discuss other discriminative approaches such as Grave and Elhadad (2015). In the fourth part, we will focus on several special topics. First, while most of the previous approaches to unsupervised parsing are unlexicalized, we will discuss the impact of partial and full lexicalization (e.g., the work by Pate and Johnson (2016); Han et al. (2017)). Second, we will discuss whether and how big training data could benefit unsupervised parsing (Han et"
2021.eacl-tutorials.1,P10-1131,0,0.0114646,"approaches to unsupervised parsing are unlexicalized, we will discuss the impact of partial and full lexicalization (e.g., the work by Pate and Johnson (2016); Han et al. (2017)). Second, we will discuss whether and how big training data could benefit unsupervised parsing (Han et al., 2017). Third, we will introduce recent attempts to induce syntactic parses from pretrained language models such as BERT (Rosa and Mareˇcek, 2019; Wu et al., 2020). Fourth, we will cover unsupervised multilingual parsing, the task of performing unsupervised parsing jointly on multiple languages (e.g., the work by Berg-Kirkpatrick and Klein (2010); Han et al. (2019)). Fifth, we will introduce visually grounded unsupervised parsing, which tries to improve unsupervised parsing with the help from visual data (Shi et al., 2019). Finally, we will discuss latent tree models trained with feedback from downstream tasks, which are related to unsupervised parsing (Yogatama et al., 2016; Choi et al., 2018). In the last part, we will summarize the tutorial and discuss potential future research directions of unsupervised parsing. Overview This will be a three-hour tutorial divided into five parts. In the first part, we will introduce the unsupervis"
2021.eacl-tutorials.1,D17-1171,1,0.879655,"arned linguistic structures. 2 We will also discuss parameter learning algorithms such as expectation-maximization (Baker, 1979; Spitkovsky et al., 2010b), MCMC (Johnson et al., 2007) and curriculum learning (Spitkovsky et al., 2010a). After introducing approaches based on generative grammars, we will discuss recent approaches that are instead based on neural language models (Shen et al., 2018, 2019). The third part will cover discriminative approaches, which model the conditional probability or score of the parse tree given the sentence. We will first introduce autoencoder approaches such as Cai et al. (2017), which contain an encoder that maps the sentence to an intermediate representation (such as a parse tree) and a decoder that tries to reconstruct the sentence. Their training objective is typically the reconstruction probability. We will then introduce variational autoencoder approaches such as Kim et al. (2019), which has a similar model structure to autoencoder approaches but uses the evidence lower bound as the training objective. Finally, we will briefly discuss other discriminative approaches such as Grave and Elhadad (2015). In the fourth part, we will focus on several special topics. F"
2021.eacl-tutorials.1,P04-1061,0,0.27348,"al., 2020). Finally, we will give an overview of unsupervised parsing approaches to be discussed in the rest of the tutorial. In the second and third parts, we will introduce in detail two major classes of approaches to unsupervised parsing, generative and discriminative approaches, and discuss their pros and cons. The second part will cover generative approaches, which model the joint probability of the sentence and the corresponding parse tree. Most of the existing generative approaches are based on generative grammars, in particular context-free grammars and dependency models with valence (Klein and Manning, 2004). There are also featurized and neural extensions of generative grammars, such as Berg-Kirkpatrick et al. (2010); Jiang et al. (2016). We will divide our discussion of learning generative grammars into two parts: structure learning and parameter learning. Structure learning concerns finding the optimal set of grammar rules. We will introduce both probabilistic methods such as Stolcke and Omohundro (1994) and heuristic methods such as Clark (2007). Parameter learning concerns learning the probabilities or weights of a pre-specified set of grammar rules. We will discuss a variety of priors and r"
2021.eacl-tutorials.1,2020.acl-main.300,1,0.80309,"motivations and applications of unsupervised parsing. For example, we will show that unsupervised parsing approaches can be extended for semi-supervised parsing (Jia et al., 2020) and cross-lingual syntactic transfer (He et al., 2019), and we will also show applications of unsupervised parsing approaches beyond syntactic parsing (e.g., in computer vision (Tu et al., 2013)). We will then discuss how to evaluate unsupervised parsing, including the evaluation metrics and typical experimental setups. We will promote standardized setups to enable meaningful empirical comparison between approaches (Li et al., 2020). Finally, we will give an overview of unsupervised parsing approaches to be discussed in the rest of the tutorial. In the second and third parts, we will introduce in detail two major classes of approaches to unsupervised parsing, generative and discriminative approaches, and discuss their pros and cons. The second part will cover generative approaches, which model the joint probability of the sentence and the corresponding parse tree. Most of the existing generative approaches are based on generative grammars, in particular context-free grammars and dependency models with valence (Klein and"
2021.eacl-tutorials.1,D16-1004,0,0.0209946,"0); Jiang et al. (2016). We will divide our discussion of learning generative grammars into two parts: structure learning and parameter learning. Structure learning concerns finding the optimal set of grammar rules. We will introduce both probabilistic methods such as Stolcke and Omohundro (1994) and heuristic methods such as Clark (2007). Parameter learning concerns learning the probabilities or weights of a pre-specified set of grammar rules. We will discuss a variety of priors and regularizations designed to improve parameter learning, such as Cohen and Smith (2010), Tu and Honavar (2012), Noji et al. (2016), and (Jin et al., 2018). 3 Outline Part 1. Introduction [20 min] 2 5 • Problem definition Reading List Klein and Manning (2004) – An influential generative approach to unsupervised dependency parsing that is the basis for many subsequent papers. • Motivations and applications • Evaluation • Overview of approaches Jiang et al. (2016) – A neural extension of Klein and Manning (2004). One of the first modern neural approaches to unsupervised parsing. Part 2. Generative Approaches [60 min] • Overview Stolcke and Omohundro (1994) – One of the first structure learning approaches of context-free gra"
2021.eacl-tutorials.1,C16-1003,0,0.0194621,"raining objective is typically the reconstruction probability. We will then introduce variational autoencoder approaches such as Kim et al. (2019), which has a similar model structure to autoencoder approaches but uses the evidence lower bound as the training objective. Finally, we will briefly discuss other discriminative approaches such as Grave and Elhadad (2015). In the fourth part, we will focus on several special topics. First, while most of the previous approaches to unsupervised parsing are unlexicalized, we will discuss the impact of partial and full lexicalization (e.g., the work by Pate and Johnson (2016); Han et al. (2017)). Second, we will discuss whether and how big training data could benefit unsupervised parsing (Han et al., 2017). Third, we will introduce recent attempts to induce syntactic parses from pretrained language models such as BERT (Rosa and Mareˇcek, 2019; Wu et al., 2020). Fourth, we will cover unsupervised multilingual parsing, the task of performing unsupervised parsing jointly on multiple languages (e.g., the work by Berg-Kirkpatrick and Klein (2010); Han et al. (2019)). Fifth, we will introduce visually grounded unsupervised parsing, which tries to improve unsupervised pa"
2021.eacl-tutorials.1,P15-1133,0,0.0164102,"n the sentence. We will first introduce autoencoder approaches such as Cai et al. (2017), which contain an encoder that maps the sentence to an intermediate representation (such as a parse tree) and a decoder that tries to reconstruct the sentence. Their training objective is typically the reconstruction probability. We will then introduce variational autoencoder approaches such as Kim et al. (2019), which has a similar model structure to autoencoder approaches but uses the evidence lower bound as the training objective. Finally, we will briefly discuss other discriminative approaches such as Grave and Elhadad (2015). In the fourth part, we will focus on several special topics. First, while most of the previous approaches to unsupervised parsing are unlexicalized, we will discuss the impact of partial and full lexicalization (e.g., the work by Pate and Johnson (2016); Han et al. (2017)). Second, we will discuss whether and how big training data could benefit unsupervised parsing (Han et al., 2017). Third, we will introduce recent attempts to induce syntactic parses from pretrained language models such as BERT (Rosa and Mareˇcek, 2019; Wu et al., 2020). Fourth, we will cover unsupervised multilingual parsi"
2021.eacl-tutorials.1,P19-1180,0,0.111693,"hool of Information Science and Technology, ShanghaiTech University 2 Alibaba DAMO Academy, Alibaba Group 3 School of Computing, National University of Singapore, Singapore 4 ILCC, University of Edinburgh tukw@shanghaitech.edu.cn yongjiang.jy@alibaba-inc.com dcshanw@nus.edu.sg yanp.zhao@ed.ac.uk Introduction More recently, however, there has been a resurgence of interest in unsupervised parsing, with more than ten papers on unsupervised parsing published in top NLP and AI venues over the past two years, including a best paper at ICLR 2019 (Shen et al., 2019), a best paper nominee at ACL 2019 (Shi et al., 2019), and a best paper nominee at EMNLP 2020 (Zhao and Titov, 2020). This renewed interest in unsupervised parsing can be attributed to the combination of two recent trends. First, there is a general trend in deep learning towards unsupervised training or pre-training. Second, there is an emerging trend in the NLP community towards finding or modeling linguistic structures in neural models. The research on unsupervised parsing fits these two trends perfectly. Because of the renewed attention on unsupervised parsing and its relevance to the recent trends in the NLP community, we believe a tutorial"
2021.eacl-tutorials.1,N10-1116,0,0.0401605,"etical studies in 1960s (Gold, 1967) and algorithmic and empirical studies in 1970s (Baker, 1979). Although deemed an interesting topic by the NLP community, unsupervised parsing had received much less attention than supervised parsing over the past few decades. 1 Proceedings of EACL: Tutorials, pages 1–5 April 19 - 20, 2021. ©2020 Association for Computational Linguistics ing and apply or extend them to other NLP tasks that can potentially benefit from implicitly learned linguistic structures. 2 We will also discuss parameter learning algorithms such as expectation-maximization (Baker, 1979; Spitkovsky et al., 2010b), MCMC (Johnson et al., 2007) and curriculum learning (Spitkovsky et al., 2010a). After introducing approaches based on generative grammars, we will discuss recent approaches that are instead based on neural language models (Shen et al., 2018, 2019). The third part will cover discriminative approaches, which model the conditional probability or score of the parse tree given the sentence. We will first introduce autoencoder approaches such as Cai et al. (2017), which contain an encoder that maps the sentence to an intermediate representation (such as a parse tree) and a decoder that tries to"
2021.eacl-tutorials.1,W10-2902,0,0.0423179,"etical studies in 1960s (Gold, 1967) and algorithmic and empirical studies in 1970s (Baker, 1979). Although deemed an interesting topic by the NLP community, unsupervised parsing had received much less attention than supervised parsing over the past few decades. 1 Proceedings of EACL: Tutorials, pages 1–5 April 19 - 20, 2021. ©2020 Association for Computational Linguistics ing and apply or extend them to other NLP tasks that can potentially benefit from implicitly learned linguistic structures. 2 We will also discuss parameter learning algorithms such as expectation-maximization (Baker, 1979; Spitkovsky et al., 2010b), MCMC (Johnson et al., 2007) and curriculum learning (Spitkovsky et al., 2010a). After introducing approaches based on generative grammars, we will discuss recent approaches that are instead based on neural language models (Shen et al., 2018, 2019). The third part will cover discriminative approaches, which model the conditional probability or score of the parse tree given the sentence. We will first introduce autoencoder approaches such as Cai et al. (2017), which contain an encoder that maps the sentence to an intermediate representation (such as a parse tree) and a decoder that tries to"
2021.eacl-tutorials.1,D12-1121,1,0.786319,"Kirkpatrick et al. (2010); Jiang et al. (2016). We will divide our discussion of learning generative grammars into two parts: structure learning and parameter learning. Structure learning concerns finding the optimal set of grammar rules. We will introduce both probabilistic methods such as Stolcke and Omohundro (1994) and heuristic methods such as Clark (2007). Parameter learning concerns learning the probabilities or weights of a pre-specified set of grammar rules. We will discuss a variety of priors and regularizations designed to improve parameter learning, such as Cohen and Smith (2010), Tu and Honavar (2012), Noji et al. (2016), and (Jin et al., 2018). 3 Outline Part 1. Introduction [20 min] 2 5 • Problem definition Reading List Klein and Manning (2004) – An influential generative approach to unsupervised dependency parsing that is the basis for many subsequent papers. • Motivations and applications • Evaluation • Overview of approaches Jiang et al. (2016) – A neural extension of Klein and Manning (2004). One of the first modern neural approaches to unsupervised parsing. Part 2. Generative Approaches [60 min] • Overview Stolcke and Omohundro (1994) – One of the first structure learning approaches"
2021.eacl-tutorials.1,2020.acl-main.383,0,0.0146244,"iscuss other discriminative approaches such as Grave and Elhadad (2015). In the fourth part, we will focus on several special topics. First, while most of the previous approaches to unsupervised parsing are unlexicalized, we will discuss the impact of partial and full lexicalization (e.g., the work by Pate and Johnson (2016); Han et al. (2017)). Second, we will discuss whether and how big training data could benefit unsupervised parsing (Han et al., 2017). Third, we will introduce recent attempts to induce syntactic parses from pretrained language models such as BERT (Rosa and Mareˇcek, 2019; Wu et al., 2020). Fourth, we will cover unsupervised multilingual parsing, the task of performing unsupervised parsing jointly on multiple languages (e.g., the work by Berg-Kirkpatrick and Klein (2010); Han et al. (2019)). Fifth, we will introduce visually grounded unsupervised parsing, which tries to improve unsupervised parsing with the help from visual data (Shi et al., 2019). Finally, we will discuss latent tree models trained with feedback from downstream tasks, which are related to unsupervised parsing (Yogatama et al., 2016; Choi et al., 2018). In the last part, we will summarize the tutorial and discu"
2021.eacl-tutorials.1,2020.emnlp-main.354,1,0.825072,"Missing"
2021.findings-acl.442,N18-3011,0,0.046304,"Missing"
2021.findings-acl.442,W13-2322,0,0.0466126,"he original method without supervision on attention. As the decoder applies multi-head attention, we design the SA approach, in which the attention distributions of all heads are averaged to compute the attention loss. In this way, we consider the multi-head attention as a supervised attention channel. The SMA approach is designed as in Section 4.3, in which only the first head is a supervised attention channel. In SCE and CE approaches, we used SCE and CE loss function to supervise the attention, respectively. 5.2 AMR-to-text Generation Task and Model : Abstract meaning representation (AMR) (Banarescu et al., 2013) is a semantic graph representation that is independent of the syntactic realization of a sentence. In the graph, nodes represent concepts and edges represent semantic relations between the concepts. AMR-to-text generation is to generate sentences from AMR graphs. We use the AMR dataset LDC2015E86, which contains 16,833 training samples, 1368 development samples, and 1371 test samples. We use the model2 of Mager et al. (2020) on this task, which is a GPT-2 (Radford et al., 2019) model with fine-tuning. Aligner: We apply lemma matching to build the attention supervision as shown in Figure 1. Th"
2021.findings-acl.442,J93-2003,0,0.101517,"ning ideal alignments is infeasible or extremely costly, for most NLG tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment committee to assess.”, the ideal alignment of the last word “assess” is node (10). As the names of (8) and (10) are the same, it is not easy to pick (10) exactly. On the other hand, it is much easier to obtain a candidate set containing both (8) and (10) and be rather confident that the ideal alignment is in the set. For different tasks, both EM-based algorithms (Brown et al., 1993; Pourdamghani et al., 2014) or rule-based methods (Flanigan et al., 2014) can be used to obtain such ambiguous alignments. However, little work has discussed making use of ambiguous labels for supervised attention. We investigate the generalized supervised attention (GSA), where the supervision signal aligns a target word to multiple possible source items (named the quasi alignment), although only a subset of the items are the true alignment targets. The multiple source items are named candidate set of the quasi alignment. A generalized supervised attention framework is built for various text"
2021.findings-acl.442,P16-1184,0,0.0197889,"ion supervision and design the Summation Cross-Entropy to deal with the ambiguity in quasi alignments. Learning with ambiguous labels has been widely studied, in which the true label is not precisely annotated but in a candidate label set. In crosslingual Part-of-Speech, annotations are derived for low resource languages from cross-language projection, which results in partial or uncertain labels. To solve this problem, T¨ackstr¨om et al. (2013) proposed a partially observed conditional random field (CRF) (Lafferty et al., 2001) method, Wisniewski et al. (2014) made a history-based model, and Buys and Botha (2016) proposed an HMM-based model. SCE is designed for training attention weights using ambiguous labels. Xu et al. (2020) also study learning from ambiguous labels (called partial label learning) in classification tasks. Their method is based on constructing similar and dissimilar pairs of samples. However, supervised attention is not a traditional classification problem. The label spaces are various in different samples, making it difficult to construct similar pairs. Thus, the method is not suitable for GSA. 3 3.1 Basic Model Encoder-decoder Model with Attention Encoder-decoder models, including"
2021.findings-acl.442,D14-1179,0,0.0473474,"Missing"
2021.findings-acl.442,P19-1042,0,0.018163,"Comparison of the test attention of SCE structure. least two alignments. For data-to-text and AMR-to-text generation, SMA outperforms SA. On the other hand, SA performs better than SMA for text summarization. One possible reason is that the summarization dataset has much higher alignment coverage and multialignment coverage and the alignment accuracy may also be higher; consequently, supervised attention works so well that automatic attention becomes unnecessary or even distracting. 5.5 lated using the one-tailed sign test with bootstrap resampling on the test set of all three tasks following Chollampatt et al. (2019): • For data-to-text, we compare the Rouge-L score of SMA-SCE to the result of SA-CE. • For AMR-to-text, we compare the BLEU score of SMA-SCE to the result of SA-CE. • For summarization, we compare the Rouge-L score of SA-SCE to the result of SA-CE. Significance Test To assess the evidence of significance, we perform significance tests on GSA. The p-value is calcuThe p-value results are shown in Table 4, which show that the improvements are significant. 4997 Task P-value Data-to-Text 6.5489e-12 Amr-to-Text 5.5795e-10 Summarization 3.925e-5 Generated Matching 1 Matching 2 Matching 3 Matching 4"
2021.findings-acl.442,P14-1134,0,0.0254297,"tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment committee to assess.”, the ideal alignment of the last word “assess” is node (10). As the names of (8) and (10) are the same, it is not easy to pick (10) exactly. On the other hand, it is much easier to obtain a candidate set containing both (8) and (10) and be rather confident that the ideal alignment is in the set. For different tasks, both EM-based algorithms (Brown et al., 1993; Pourdamghani et al., 2014) or rule-based methods (Flanigan et al., 2014) can be used to obtain such ambiguous alignments. However, little work has discussed making use of ambiguous labels for supervised attention. We investigate the generalized supervised attention (GSA), where the supervision signal aligns a target word to multiple possible source items (named the quasi alignment), although only a subset of the items are the true alignment targets. The multiple source items are named candidate set of the quasi alignment. A generalized supervised attention framework is built for various text generation tasks with alignment relationships between target words and so"
2021.findings-acl.442,N19-1357,0,0.01962,"captioning (Anderson et al., 2018), dialogue systems (Liu et al., 2018), and so on. The attention mechanism (Bahdanau et al., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the corresponding author. automatic weights do not necessarily encode prior knowledge, such as the alignments between input and output (Jain and Wallace, 2019). To alleviate this problem, supervised attention was considered (Liu et al., 2016; Mi et al., 2016; Kamigaito et al., 2017; Nguyen et al., 2018; Nguyen and Nguyen, 2018), which shows that human knowledge is helpful for guiding the learning process of attention models. Previous work on supervised attention assumes access to ideal alignments. Unfortunately, obtaining ideal alignments is infeasible or extremely costly, for most NLG tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment co"
2021.findings-acl.442,I17-2002,0,0.0507966,"., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the corresponding author. automatic weights do not necessarily encode prior knowledge, such as the alignments between input and output (Jain and Wallace, 2019). To alleviate this problem, supervised attention was considered (Liu et al., 2016; Mi et al., 2016; Kamigaito et al., 2017; Nguyen et al., 2018; Nguyen and Nguyen, 2018), which shows that human knowledge is helpful for guiding the learning process of attention models. Previous work on supervised attention assumes access to ideal alignments. Unfortunately, obtaining ideal alignments is infeasible or extremely costly, for most NLG tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment committee to assess.”, the ideal alignment of the last word “assess” is node (10). As the names of (8) and (10) are the same,"
2021.findings-acl.442,N19-1238,0,0.015392,"est attention of SCE structure. least two alignments. For data-to-text and AMR-to-text generation, SMA outperforms SA. On the other hand, SA performs better than SMA for text summarization. One possible reason is that the summarization dataset has much higher alignment coverage and multialignment coverage and the alignment accuracy may also be higher; consequently, supervised attention works so well that automatic attention becomes unnecessary or even distracting. 5.5 lated using the one-tailed sign test with bootstrap resampling on the test set of all three tasks following Chollampatt et al. (2019): • For data-to-text, we compare the Rouge-L score of SMA-SCE to the result of SA-CE. • For AMR-to-text, we compare the BLEU score of SMA-SCE to the result of SA-CE. • For summarization, we compare the Rouge-L score of SA-SCE to the result of SA-CE. Significance Test To assess the evidence of significance, we perform significance tests on GSA. The p-value is calcuThe p-value results are shown in Table 4, which show that the improvements are significant. 4997 Task P-value Data-to-Text 6.5489e-12 Amr-to-Text 5.5795e-10 Summarization 3.925e-5 Generated Matching 1 Matching 2 Matching 3 Matching 4"
2021.findings-acl.442,P18-2027,0,0.0126983,"arget contain the same information. We report the details of model structures and hyper-parameters in the appendix. 5.1 Data-to-text Generation Task and Model : We consider the Abstract GENeration DAtaset (AGENDA) (Ammar et al., 2018), which contains pairs of a literature abstract and a knowledge graph extracted from the abstract. The nodes in the knowledge graphs are entity types, such as “Task” and “Method”. The edges are the relations between different entities, including “COMPARE”, “PART-OF”, and so on. We use the training, development, and test splits of 38,720/1000/1000, as Ammar et al. (2018) does. We use GraphWriter1 (Koncel-Kedziorski et al., 2019) on this task. The encoder of this model is a graph transformer and the decoder is an RNN decoder with attention and copying mechanism. More detail is introduced in the appendix. Aligner: The source items of this task include entities and relations, as shown in Figure 3. We use our string matching aligner to extract the alignments from target words to the source entities and extend our aligner for the alignments of relations, such as aligning target words “use” and “apply” to source relation “USED-FOR”. For the details of 1 https://git"
2021.findings-acl.442,C16-1291,0,0.276334,"lignments, which specify candidate sets of alignments and are much easier to obtain than ideal alignments. We design a Summation Cross-Entropy (SCE) loss and a Supervised Multiple Attention (SMA) structure to accommodate quasi alignments. Experiments on three text generation tasks demonstrate that GSA improves generation performance and is robust against errors in attention supervision. 1 Introduction The encoder-decoder framework has been applied to various natural language generation (NLG) tasks, such as neural machine translation (Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Liu et al., 2016), text generation (Wiseman et al., 2017; Puduppully et al., 2019), text summarization (Liu and Lapata, 2019; Lin et al., 2018), image captioning (Anderson et al., 2018), dialogue systems (Liu et al., 2018), and so on. The attention mechanism (Bahdanau et al., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the"
2021.findings-acl.442,P18-1138,0,0.0212933,"to accommodate quasi alignments. Experiments on three text generation tasks demonstrate that GSA improves generation performance and is robust against errors in attention supervision. 1 Introduction The encoder-decoder framework has been applied to various natural language generation (NLG) tasks, such as neural machine translation (Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Liu et al., 2016), text generation (Wiseman et al., 2017; Puduppully et al., 2019), text summarization (Liu and Lapata, 2019; Lin et al., 2018), image captioning (Anderson et al., 2018), dialogue systems (Liu et al., 2018), and so on. The attention mechanism (Bahdanau et al., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the corresponding author. automatic weights do not necessarily encode prior knowledge, such as the alignments between input and output (Jain and Wallace, 2019). To alleviate this problem, supervised attention"
2021.findings-acl.442,D19-1387,0,0.0234071,"s. We design a Summation Cross-Entropy (SCE) loss and a Supervised Multiple Attention (SMA) structure to accommodate quasi alignments. Experiments on three text generation tasks demonstrate that GSA improves generation performance and is robust against errors in attention supervision. 1 Introduction The encoder-decoder framework has been applied to various natural language generation (NLG) tasks, such as neural machine translation (Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Liu et al., 2016), text generation (Wiseman et al., 2017; Puduppully et al., 2019), text summarization (Liu and Lapata, 2019; Lin et al., 2018), image captioning (Anderson et al., 2018), dialogue systems (Liu et al., 2018), and so on. The attention mechanism (Bahdanau et al., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the corresponding author. automatic weights do not necessarily encode prior knowledge, such as the alignments"
2021.findings-acl.442,D15-1166,0,0.432645,"ed Attention method (GSA) based on quasi alignments, which specify candidate sets of alignments and are much easier to obtain than ideal alignments. We design a Summation Cross-Entropy (SCE) loss and a Supervised Multiple Attention (SMA) structure to accommodate quasi alignments. Experiments on three text generation tasks demonstrate that GSA improves generation performance and is robust against errors in attention supervision. 1 Introduction The encoder-decoder framework has been applied to various natural language generation (NLG) tasks, such as neural machine translation (Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Liu et al., 2016), text generation (Wiseman et al., 2017; Puduppully et al., 2019), text summarization (Liu and Lapata, 2019; Lin et al., 2018), image captioning (Anderson et al., 2018), dialogue systems (Liu et al., 2018), and so on. The attention mechanism (Bahdanau et al., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of"
2021.findings-acl.442,2020.acl-main.167,0,0.0305511,"Missing"
2021.findings-acl.442,D16-1249,0,0.0571723,"and design the Summation Cross-Entropy to deal with the ambiguity in quasi alignments. Learning with ambiguous labels has been widely studied, in which the true label is not precisely annotated but in a candidate label set. In crosslingual Part-of-Speech, annotations are derived for low resource languages from cross-language projection, which results in partial or uncertain labels. To solve this problem, T¨ackstr¨om et al. (2013) proposed a partially observed conditional random field (CRF) (Lafferty et al., 2001) method, Wisniewski et al. (2014) made a history-based model, and Buys and Botha (2016) proposed an HMM-based model. SCE is designed for training attention weights using ambiguous labels. Xu et al. (2020) also study learning from ambiguous labels (called partial label learning) in classification tasks. Their method is based on constructing similar and dissimilar pairs of samples. However, supervised attention is not a traditional classification problem. The label spaces are various in different samples, making it difficult to construct similar pairs. Thus, the method is not suitable for GSA. 3 3.1 Basic Model Encoder-decoder Model with Attention Encoder-decoder models, including"
2021.findings-acl.442,K16-1028,0,0.203015,"and design the Summation Cross-Entropy to deal with the ambiguity in quasi alignments. Learning with ambiguous labels has been widely studied, in which the true label is not precisely annotated but in a candidate label set. In crosslingual Part-of-Speech, annotations are derived for low resource languages from cross-language projection, which results in partial or uncertain labels. To solve this problem, T¨ackstr¨om et al. (2013) proposed a partially observed conditional random field (CRF) (Lafferty et al., 2001) method, Wisniewski et al. (2014) made a history-based model, and Buys and Botha (2016) proposed an HMM-based model. SCE is designed for training attention weights using ambiguous labels. Xu et al. (2020) also study learning from ambiguous labels (called partial label learning) in classification tasks. Their method is based on constructing similar and dissimilar pairs of samples. However, supervised attention is not a traditional classification problem. The label spaces are various in different samples, making it difficult to construct similar pairs. Thus, the method is not suitable for GSA. 3 3.1 Basic Model Encoder-decoder Model with Attention Encoder-decoder models, including"
2021.findings-acl.442,C18-1193,0,0.0855731,"ework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the corresponding author. automatic weights do not necessarily encode prior knowledge, such as the alignments between input and output (Jain and Wallace, 2019). To alleviate this problem, supervised attention was considered (Liu et al., 2016; Mi et al., 2016; Kamigaito et al., 2017; Nguyen et al., 2018; Nguyen and Nguyen, 2018), which shows that human knowledge is helpful for guiding the learning process of attention models. Previous work on supervised attention assumes access to ideal alignments. Unfortunately, obtaining ideal alignments is infeasible or extremely costly, for most NLG tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment committee to assess.”, the ideal alignment of the last word “assess” is node (10). As the names of (8) and (10) are the same, it is not easy to pick (10) exactly. On the ot"
2021.findings-acl.442,D14-1048,0,0.0173769,"s is infeasible or extremely costly, for most NLG tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment committee to assess.”, the ideal alignment of the last word “assess” is node (10). As the names of (8) and (10) are the same, it is not easy to pick (10) exactly. On the other hand, it is much easier to obtain a candidate set containing both (8) and (10) and be rather confident that the ideal alignment is in the set. For different tasks, both EM-based algorithms (Brown et al., 1993; Pourdamghani et al., 2014) or rule-based methods (Flanigan et al., 2014) can be used to obtain such ambiguous alignments. However, little work has discussed making use of ambiguous labels for supervised attention. We investigate the generalized supervised attention (GSA), where the supervision signal aligns a target word to multiple possible source items (named the quasi alignment), although only a subset of the items are the true alignment targets. The multiple source items are named candidate set of the quasi alignment. A generalized supervised attention framework is built for various text generation tasks with align"
2021.findings-acl.442,P19-1195,0,0.0153126,"est attention of SCE structure. least two alignments. For data-to-text and AMR-to-text generation, SMA outperforms SA. On the other hand, SA performs better than SMA for text summarization. One possible reason is that the summarization dataset has much higher alignment coverage and multialignment coverage and the alignment accuracy may also be higher; consequently, supervised attention works so well that automatic attention becomes unnecessary or even distracting. 5.5 lated using the one-tailed sign test with bootstrap resampling on the test set of all three tasks following Chollampatt et al. (2019): • For data-to-text, we compare the Rouge-L score of SMA-SCE to the result of SA-CE. • For AMR-to-text, we compare the BLEU score of SMA-SCE to the result of SA-CE. • For summarization, we compare the Rouge-L score of SA-SCE to the result of SA-CE. Significance Test To assess the evidence of significance, we perform significance tests on GSA. The p-value is calcuThe p-value results are shown in Table 4, which show that the improvements are significant. 4997 Task P-value Data-to-Text 6.5489e-12 Amr-to-Text 5.5795e-10 Summarization 3.925e-5 Generated Matching 1 Matching 2 Matching 3 Matching 4"
2021.findings-acl.442,Q13-1001,0,0.0722298,"Missing"
2021.findings-acl.442,D17-1239,0,0.0353923,"Missing"
2021.findings-acl.442,D14-1187,0,0.0468567,"Missing"
2021.naacl-main.117,2020.emnlp-main.389,0,0.54523,"Missing"
2021.naacl-main.117,2020.emnlp-main.103,0,0.231997,"rsand therefore allows us to use a much larger ing accuracy of the grammar. For example, the number of symbols. We further use neural parameterization for the new form to improve best model from Petrov et al. (2006) contains over unsupervised parsing performance. We evalu1000 nonterminal and preterminal symbols. We are ate our model across ten languages and empiralso motivated by the recent work of Buhai et al. ically demonstrate the effectiveness of using (2019) who show that when learning latent variable 1 more symbols. models, increasing the number of hidden states is often helpful; and by Chiu and Rush (2020) who 1 Introduction show that a neural hidden Markov model with up 16 Unsupervised constituency parsing is the task of to 2 hidden states can achieve surprisingly good inducing phrase-structure grammars from raw text performance in language modeling. without using parse tree annotations. Early work inA major challenge in employing a large numduces probabilistic context-free grammars (PCFGs) ber of nonterminal and preterminal symbols is that via the Expectation Maximation algorithm and representing and parsing with a PCFG requires a finds the result unsatisfactory (Lari and Young, computational"
2021.naacl-main.117,N13-1052,0,0.784324,"ltilingual evaluation on nine additional languages. The evaluation results suggest good generalizability of our approach on languages beyond English. Our key contributions can be summarized as follows: (1) We propose a new parameterization form of PCFGs based on tensor decomposition, which enables us to use a large number of symbols in PCFGs. (2) We further apply neural parameterization to improve unsupervised parsing performance. (3) We evaluate our model across ten languages and empirically show the effectiveness of our approach. Tensor decomposition on PCFGs: Our work is closely related to Cohen et al. (2013) in that both use tensor decomposition to parameterize the probabilities of binary rules for the purpose of reducing the time complexity of the inside algorithm. However, Cohen et al. (2013) use this technique to speed up inference of an existing PCFG, and they need to actually perform tensor decomposition on the rule probability tensor of the PCFG. In contrast, we draw inspiration from this technique to design a new parameterization form of PCFG that can be directly learned from data. Since we do not have a probability tensor to start with, additional tricks have to be inserted in order to en"
2021.naacl-main.117,N19-1116,0,0.376315,"PCFGs build upon context-free grammars (CFGs). We start by introducing CFGs and establishing notations. A CFG is defined as a 5-tuple G = (S, N , P, Σ, R) where S is the start symbol, N is a finite set of nonterminal symbols, P is a finite set 2 of preterminal symbols, Σ is a finite set of terminal symbols, and R is a set of rules in the following form: Related work 3 3.1 Background Tensor form of PCFGs Grammar induction using neural networks: There is a recent resurgence of interest in unsupervised constituency parsing, mostly driven by neural network based methods (Shen et al., 2018a, 2019; Drozdov et al., 2019, 2020; Kim et al., 2019a,b; Jin et al., 2019; Zhu et al., 2020). These methods can be categorized into two major groups: those built on top of a generative grammar and those S→A A∈N without a grammar component. The approaches A → BC, A ∈ N , B, C ∈ N ∪ P most related to ours belong to the first category, T → w, T ∈ P, w ∈ Σ which use neural networks to produce grammar rule probabilities. Jin et al. (2019) use an invert- PCFGs extend CFGs by associating each rule r ∈ ible neural projection network (a.k.a. normalizing R with a probability πr . Denote n, p, and q as the flow (Rezende and Mohamed"
2021.naacl-main.117,P12-1024,0,0.064932,"Missing"
2021.naacl-main.117,2020.emnlp-main.392,0,0.195977,"Missing"
2021.naacl-main.117,W16-5901,0,0.619028,"ared between preterminal rules and start rules. 3 The time complexity of the second stage is O(l ), so the overall time complexity of our decoding 3 2 method is O(dl + mdl ), which is much faster 3 3 than O(m l ) in general. 7 6 Parsing with TD-PCFGs 7.1 ⋆ Parsing seeks the most probable parse t from all the parses TG (w) of a sentence w: ⋆ t = arg max p(t∣w) . t∈TG (w) (9) 3 Typically, the CYK algorithm can be directly used to solve this problem exactly: it first computes the score of the most likely parse; and then automatic differentiation is applied to recover the best tree ⋆ structure t (Eisner, 2016; Rush, 2020). This, however, relies on the original probability tensor T and is incompatible with our decomposed repre4 sentation. If we reconstruct T from U, V, W and then perform CYK, then the resulting time and 3 3 space complexity would degrade to O(m l ) and become unaffordable when m is large. Therefore, we resort to Minimum Bayes-Risk (MBR) style decoding because we can compute the inside probabilities efficiently. Our decoding method consists of two stages. The first stage computes the conditional probability of a substring wi,j being a constituent in a given sentence w (a.k.a. poster"
2021.naacl-main.117,P96-1024,0,0.549936,"Missing"
2021.naacl-main.117,D18-1160,0,0.162022,"endency parsing, the Dependency Model with preterminals in P. Similarly, for a preterminal rule Valence (DMV) (Klein and Manning, 2004) has we define been parameterized neurally to achieve higher inp×q duction accuracy (Jiang et al., 2016; Yang et al., QhT ,hw = πT →w , Q ∈ R . 2020). In part-of-speech (POS) induction, neurally 2 Strictly, CFGs do not distinguish nonterminals N (conparameterized Hidden Markov Models (HMM) stituent labels) from preterminals P (part-of-speech tags). also achieve state-of-the-art results (Tran et al., They are both treated as nonterminals. N , P, Σ satisfy 2016; He et al., 2018). N ∩ P = ∅ and (N ∪ P) ∩ Σ = ∅. 1488 Again, hT and hw are the preterminal index and the terminal index, respectively. Finally, for a start rule we define rhA = πS→A , n r∈R . Generative learning of PCFGs involves maximizing the log-likelihood of every observed sentence w = w1 , . . . , w l : log pθ (w) = log ∑ p(t) , t∈TG (w) where TG (w) contains all the parse trees of the sentence w under a PCFG G. The probability of a parse tree t ∈ TG is defined as p(t) = ∏r∈tR πr , where tR is the set of rules used in the derivation of t. log pθ (w) can be estimated efficiently through the inside algorit"
2021.naacl-main.117,P04-1061,0,0.54177,"hich are more expressive ThA ,hB ,hC = πA→BC , T ∈ R , than PCFGs and can model both dependency and constituency parse trees simultaneously. where T is an order-3 tensor, m = n + p, and hA ∈ In other unsupervised syntactic induction tasks, [0, n) and hB , hC ∈ [0, m) are symbol indices. there is also a trend to use neural networks to pro- For the convenience of computation, we assign duce grammar rule probabilities. In unsupervised indices [0, n) to nonterminals in N and [n, m) to dependency parsing, the Dependency Model with preterminals in P. Similarly, for a preterminal rule Valence (DMV) (Klein and Manning, 2004) has we define been parameterized neurally to achieve higher inp×q duction accuracy (Jiang et al., 2016; Yang et al., QhT ,hw = πT →w , Q ∈ R . 2020). In part-of-speech (POS) induction, neurally 2 Strictly, CFGs do not distinguish nonterminals N (conparameterized Hidden Markov Models (HMM) stituent labels) from preterminals P (part-of-speech tags). also achieve state-of-the-art results (Tran et al., They are both treated as nonterminals. N , P, Σ satisfy 2016; He et al., 2018). N ∩ P = ∅ and (N ∪ P) ∩ Σ = ∅. 1488 Again, hT and hw are the preterminal index and the terminal index, respectively."
2021.naacl-main.117,D07-1072,0,0.0929365,"use 30 nonterminals and 60 preterminals. with neural parameterization have been shown In this paper, we study PCFG induction with to be effective in unsupervised phrasea much larger number of nonterminal and preterstructure grammar induction. However, due minal symbols. We are partly motivated by the to the cubic computational complexity of PCFG representation and parsing, previous apclassic work of latent variable grammars in superproaches cannot scale up to a relatively large vised constituency parsing (Matsuzaki et al., 2005; number of (nonterminal and preterminal) symPetrov et al., 2006; Liang et al., 2007; Cohen et al., bols. In this work, we present a new parame2012; Zhao et al., 2018). While the Penn treebank terization form of PCFGs based on tensor degrammar contains only tens of nonterminals and composition, which has at most quadratic compreterminals, it has been found that dividing them putational complexity in the symbol number into subtypes could significantly improves the parsand therefore allows us to use a much larger ing accuracy of the grammar. For example, the number of symbols. We further use neural parameterization for the new form to improve best model from Petrov et al. (2006"
2021.naacl-main.117,H94-1020,0,0.221327,"e parse tree that has the highest expected number of constituents (Smith and Eisner, 2006): ⋆ t = arg max ∑ p(wi,j ∣w) . t∈TG (w) w 3 Experimental setup (10) i,j ∈t The CYK algorithm is similar to the inside algorithm. The only difference is that it uses M AX whenever the inside algorithm performs S UM over k and B, C (cf. Equation 1). 4 In Equation 8 all symbols become entangled through T T V si,k and W sk+1,j . We are unable to perform M AX over B, C as in the CYK algorithm. Datasets We evaluate TN-PCFGs across ten languages. We use the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1994) for English, the Penn Chinese Treebank 5.1 (CTB) (Xue et al., 2005) for Chinese, and the SPRML dataset (Seddah et al., 2014) for the other eight morphology-rich languages. We use a unified data preprocessing 5 pipeline provided by Zhao and Titov (2021). The same pipeline has been used in several recent papers (Shen et al., 2018a, 2019; Kim et al., 2019a; Zhao and Titov, 2020). Specifically, for every treebank, punctuation is removed from all data splits and the top 10,000 frequent words in the training data are used as the vocabulary. 7.2 Settings and hyperparameters For baseline models we us"
2021.naacl-main.117,D16-1073,1,0.859713,"ncy parse trees simultaneously. where T is an order-3 tensor, m = n + p, and hA ∈ In other unsupervised syntactic induction tasks, [0, n) and hB , hC ∈ [0, m) are symbol indices. there is also a trend to use neural networks to pro- For the convenience of computation, we assign duce grammar rule probabilities. In unsupervised indices [0, n) to nonterminals in N and [n, m) to dependency parsing, the Dependency Model with preterminals in P. Similarly, for a preterminal rule Valence (DMV) (Klein and Manning, 2004) has we define been parameterized neurally to achieve higher inp×q duction accuracy (Jiang et al., 2016; Yang et al., QhT ,hw = πT →w , Q ∈ R . 2020). In part-of-speech (POS) induction, neurally 2 Strictly, CFGs do not distinguish nonterminals N (conparameterized Hidden Markov Models (HMM) stituent labels) from preterminals P (part-of-speech tags). also achieve state-of-the-art results (Tran et al., They are both treated as nonterminals. N , P, Σ satisfy 2016; He et al., 2018). N ∩ P = ∅ and (N ∪ P) ∩ Σ = ∅. 1488 Again, hT and hw are the preterminal index and the terminal index, respectively. Finally, for a start rule we define rhA = πS→A , n r∈R . Generative learning of PCFGs involves maximizi"
2021.naacl-main.117,P05-1010,0,0.182668,"d other nonterminals) and Kim et al. (2019a) Probabilistic context-free grammars (PCFGs) use 30 nonterminals and 60 preterminals. with neural parameterization have been shown In this paper, we study PCFG induction with to be effective in unsupervised phrasea much larger number of nonterminal and preterstructure grammar induction. However, due minal symbols. We are partly motivated by the to the cubic computational complexity of PCFG representation and parsing, previous apclassic work of latent variable grammars in superproaches cannot scale up to a relatively large vised constituency parsing (Matsuzaki et al., 2005; number of (nonterminal and preterminal) symPetrov et al., 2006; Liang et al., 2007; Cohen et al., bols. In this work, we present a new parame2012; Zhao et al., 2018). While the Penn treebank terization form of PCFGs based on tensor degrammar contains only tens of nonterminals and composition, which has at most quadratic compreterminals, it has been found that dividing them putational complexity in the symbol number into subtypes could significantly improves the parsand therefore allows us to use a much larger ing accuracy of the grammar. For example, the number of symbols. We further use neu"
2021.naacl-main.117,P19-1234,0,0.216825,"start by introducing CFGs and establishing notations. A CFG is defined as a 5-tuple G = (S, N , P, Σ, R) where S is the start symbol, N is a finite set of nonterminal symbols, P is a finite set 2 of preterminal symbols, Σ is a finite set of terminal symbols, and R is a set of rules in the following form: Related work 3 3.1 Background Tensor form of PCFGs Grammar induction using neural networks: There is a recent resurgence of interest in unsupervised constituency parsing, mostly driven by neural network based methods (Shen et al., 2018a, 2019; Drozdov et al., 2019, 2020; Kim et al., 2019a,b; Jin et al., 2019; Zhu et al., 2020). These methods can be categorized into two major groups: those built on top of a generative grammar and those S→A A∈N without a grammar component. The approaches A → BC, A ∈ N , B, C ∈ N ∪ P most related to ours belong to the first category, T → w, T ∈ P, w ∈ Σ which use neural networks to produce grammar rule probabilities. Jin et al. (2019) use an invert- PCFGs extend CFGs by associating each rule r ∈ ible neural projection network (a.k.a. normalizing R with a probability πr . Denote n, p, and q as the flow (Rezende and Mohamed, 2015)) to parame- number of symbols in N ,"
2021.naacl-main.117,P06-1055,0,0.279815,"free grammars (PCFGs) use 30 nonterminals and 60 preterminals. with neural parameterization have been shown In this paper, we study PCFG induction with to be effective in unsupervised phrasea much larger number of nonterminal and preterstructure grammar induction. However, due minal symbols. We are partly motivated by the to the cubic computational complexity of PCFG representation and parsing, previous apclassic work of latent variable grammars in superproaches cannot scale up to a relatively large vised constituency parsing (Matsuzaki et al., 2005; number of (nonterminal and preterminal) symPetrov et al., 2006; Liang et al., 2007; Cohen et al., bols. In this work, we present a new parame2012; Zhao et al., 2018). While the Penn treebank terization form of PCFGs based on tensor degrammar contains only tens of nonterminals and composition, which has at most quadratic compreterminals, it has been found that dividing them putational complexity in the symbol number into subtypes could significantly improves the parsand therefore allows us to use a much larger ing accuracy of the grammar. For example, the number of symbols. We further use neural parameterization for the new form to improve best model from"
2021.naacl-main.117,P19-1228,0,0.251529,"Missing"
2021.naacl-main.117,N19-1114,0,0.168318,"the Expectation Maximation algorithm and representing and parsing with a PCFG requires a finds the result unsatisfactory (Lari and Young, computational complexity that is cubic in its sym1990; Carroll and Charniak, 1992). Recently, bol number. To resolve the issue, we rely on a new PCFGs with neural parameterization (i.e., using parameterization form of PCFGs based on tensor neural networks to generate rule probabilities) have decomposition, which reduces the computational been shown to achieve good results in unsuper- complexity from cubic to at most quadratic. Furvised constituency parsing (Kim et al., 2019a; Jin thermore, we apply neural parameterization to the et al., 2019; Zhu et al., 2020). However, due to the new form, which is crucial for boosting unsupercubic computational complexity of PCFG represen- vised parsing performance of PCFGs as shown by tation and parsing, these approaches learn PCFGs Kim et al. (2019a). with relatively small numbers of nonterminals and We empirically evaluate our approach across ten preterminals. For example, Jin et al. (2019) use 30 languages. On English WSJ, our best model with ∗ 500 preterminals and 250 nonterminals improves Corresponding Author 1 Our code:"
2021.naacl-main.117,2020.acl-demos.38,0,0.288553,"refore allows us to use a much larger ing accuracy of the grammar. For example, the number of symbols. We further use neural parameterization for the new form to improve best model from Petrov et al. (2006) contains over unsupervised parsing performance. We evalu1000 nonterminal and preterminal symbols. We are ate our model across ten languages and empiralso motivated by the recent work of Buhai et al. ically demonstrate the effectiveness of using (2019) who show that when learning latent variable 1 more symbols. models, increasing the number of hidden states is often helpful; and by Chiu and Rush (2020) who 1 Introduction show that a neural hidden Markov model with up 16 Unsupervised constituency parsing is the task of to 2 hidden states can achieve surprisingly good inducing phrase-structure grammars from raw text performance in language modeling. without using parse tree annotations. Early work inA major challenge in employing a large numduces probabilistic context-free grammars (PCFGs) ber of nonterminal and preterminal symbols is that via the Expectation Maximation algorithm and representing and parsing with a PCFG requires a finds the result unsatisfactory (Lari and Young, computational"
2021.naacl-main.117,W14-6111,0,0.0724086,"Missing"
2021.naacl-main.117,2020.emnlp-main.614,0,0.0648272,"Missing"
2021.naacl-main.117,P06-2101,0,0.273467,"ly. Our decoding method consists of two stages. The first stage computes the conditional probability of a substring wi,j being a constituent in a given sentence w (a.k.a. posteriors of spans being a constituent): p(wi,j ∣w) = 1 p(w) ∑ p(t) ⋅ 1{wi,j ∈t} . t∈TG (w) We can estimate the posteriors efficiently by using automatic differentiation after obtaining all the inside probabilities. This has the same time complexity as our improved inside algorithm, which is 3 2 O(dl + mdl ). The second stage uses the CYK algorithm to find the parse tree that has the highest expected number of constituents (Smith and Eisner, 2006): ⋆ t = arg max ∑ p(wi,j ∣w) . t∈TG (w) w 3 Experimental setup (10) i,j ∈t The CYK algorithm is similar to the inside algorithm. The only difference is that it uses M AX whenever the inside algorithm performs S UM over k and B, C (cf. Equation 1). 4 In Equation 8 all symbols become entangled through T T V si,k and W sk+1,j . We are unable to perform M AX over B, C as in the CYK algorithm. Datasets We evaluate TN-PCFGs across ten languages. We use the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1994) for English, the Penn Chinese Treebank 5.1 (CTB) (Xue et al., 2005) f"
2021.naacl-main.117,D13-1170,0,0.0028549,"vector that selects the i-th row of U. We have padded U with zeros such m×d that U ∈ R and the last m − n rows are all zeros. Thus T T z = U ⋅ ((V x) ⊙ (W y)) , (7) and accordingly, j−1 T T si,j = U ⋅ ∑ ((V si,k ) ⊙ (W sk+1,j )) . (8) k=i Equation 8 computes the inside probabilities using TD-PCFGs. It has a time complexity O(md). By T T caching V si,k and W sk+1,j , the time complex3 2 ity of the inside algorithm becomes O(dl + mdl ) (Cohen et al., 2013), which is at most quadratic in m since we typically set d = O(m). Interestingly, Equation 8 has similar forms to recursive neural networks (Socher et al., 2013) if we treat inside score vectors as span embeddings. One problem with TD-PCFGs is that, since we use three matrices U, V and W to represent tensor T of binary rule probabilities, how we can ensure that T is non-negative and properly normalized, i.e., for a given left-hand side symbol A, ∑j,k ThA ,j,k = 1. Simply reconstructing T with U, V and W and then performing normalization 3 would take O(m ) time, thus defeating the purpose of TD-PCFGs. Our solution is to require that the three matrices are non-negative and meanwhile U is row-normalized and V and W are columnnormalized (Shen et al., 2018"
2021.naacl-main.117,2020.emnlp-main.354,1,0.708781,"Missing"
2021.naacl-main.117,2021.adaptnlp-1.17,1,0.767353,"es M AX whenever the inside algorithm performs S UM over k and B, C (cf. Equation 1). 4 In Equation 8 all symbols become entangled through T T V si,k and W sk+1,j . We are unable to perform M AX over B, C as in the CYK algorithm. Datasets We evaluate TN-PCFGs across ten languages. We use the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1994) for English, the Penn Chinese Treebank 5.1 (CTB) (Xue et al., 2005) for Chinese, and the SPRML dataset (Seddah et al., 2014) for the other eight morphology-rich languages. We use a unified data preprocessing 5 pipeline provided by Zhao and Titov (2021). The same pipeline has been used in several recent papers (Shen et al., 2018a, 2019; Kim et al., 2019a; Zhao and Titov, 2020). Specifically, for every treebank, punctuation is removed from all data splits and the top 10,000 frequent words in the training data are used as the vocabulary. 7.2 Settings and hyperparameters For baseline models we use the best configurations reported by the authors. For example, we use 30 nonterminals and 60 preterminals for N-PCFGs and C-PCFGs. We implement TN-PCFGs and reimplement N-PCFGs and C-PCFGs using automatic differentiation (Eisner, 2016) and we borrow th"
2021.naacl-main.117,P18-1109,1,0.904473,"Missing"
2021.naacl-main.117,W16-5907,0,0.0468834,"Missing"
2021.naacl-main.117,2020.coling-main.347,1,0.711358,"Missing"
D12-1121,afonso-etal-2002-floresta,0,0.0246884,"3 40.6 41.8 39.8 41.6 27.8 27.2 33.0 16.1 28.6 39.3 35.6 35.2 37.4 36.0 33.6 34.1 29.4 30.9 32.7 35.1 28.0 43.1 34.9 40.3 38.8 38.8 39.4 47.8 Portuguese Slovene 23.7 27.7 27.5 26.2 27.3 26.4 30.6 23.8 15.3 15.1 14.6 16.4 Swedish 31.9 42.0 29.3 32.7 37.2 46.0 Table 3: The dependency accuracies (on sentences of all lengths in the testing corpus) of grammars learned by our approach from the corpora of the following languages: Arabic (Hajiˇc et al., 2004), Basque (Aduriz et al., 2003), Czech (Hajiˇc et al., 2000), Danish (Buch-Kromann et al., 2007), Dutch (Beek et al., 2002), English, Portuguese (Afonso et al., 2002), Slovene (Erjavec et al., 2010), Swedish (Nivre et al., 2006). other types of priors and regularizations for unsupervised grammar learning, to apply it to more advanced grammar models, and to explore alternative formulations of unambiguity regularization. Acknowledgement The work of Kewei Tu was supported at Iowa State University in part by a research assistantship from the Iowa State University Center for Computational Intelligence, Learning, and Discovery, and at University of California, Los Angeles by the DARPA grant FA 8650-11-1-7149. The work of Vasant Honavar was supported by the Natio"
D12-1121,D10-1117,0,0.650654,"s of short sentences. On the testing sentences of all lengths, σ = 0.25 achieves the best dependency accuracy, which suggests that controlling the strength of unambiguity regularization can contribute to improved performance. Testing Accuracy ≤ 10 ≤ 20 All DMV Model UR-Annealing UR-Annealing&Prior PR-S (Gillenwater et al., 2010) SLN TieV&N (Cohen and Smith, 2009) LN Families (Cohen et al., 2008) 63.6 66.6 62.1 61.3 59.3 53.1 57.7 53.8 47.4 45.1 47.9 52.3 49.1 41.4 39.0 Extended Models UR-Annealing on E-DMV(2,2) 71.4 UR-Annealing on E-DMV(3,3) 71.2 L-EVG (Headden et al., 2009) 68.8 LexTSG-DMV (Blunsom and Cohn, 2010) 67.7 62.4 61.5 - 57.0 56.0 55.7 Table 2: The dependency accuracies of grammars learned by our approach (denoted by “UR”) with annealing and prior, compared with previous published results. 4.2 Results with Annealing and Prior We annealed the value of σ from 1 to 0 when running our approach. We reduced the value of σ at a constant speed such that it reaches 0 at iteration 100. The results of this experiment (shown as “URAnnealing” in Table 2) suggest that annealing the value of σ not only helps circumvent the problem of choosing an optimal value of σ, but may also lead to substantial improveme"
D12-1121,P95-1031,0,0.136506,"valet and Bengio, 2005), or derived from the expected conditional log-likelihood (Smith and Eisner, 2007). In contrast, our approach is motivated by the observed unambiguity of natural language grammars. One implication of this difference is that if our approach is applied to semi-supervised learning, the regularization term would be applied to labeled sentences as well (by ignoring the labels) because the target grammar shall be unambiguous on all the training sentences. The sparsity bias, which favors a grammar with fewer grammar rules, has been widely used in unsupervised grammar learning (Chen, 1995; Johnson et al., 2007; Gillenwater et al., 2010). Although a more sparse grammar is often less ambiguous, in general that is not always the case. We have shown that unambiguity regularization could lead to better performance than approaches utilizing the sparsity bias, and that the two types of biases can be applied together for further improvement in the learning performance. 6 Conclusion We have introduced unambiguity regularization, a novel approach to unsupervised learning of probabilistic natural language grammars. It is based on the observation that natural language grammars are remarka"
D12-1121,N09-1009,0,0.142208,"04). More recent approaches incorporate additional prior information of the target grammar into learning. For example, Kurihara and Sato (2004) used Dirichlet priors over rule probabilities to obtain smoothed estimates of the probabilities. Johnson et al. (2007) used Dirichlet priors with hyperparameters set to values less than 1 to encourage sparsity of grammar rules. Finkel et al. (2007) and Liang et al. (2007) proposed to use the hierarchical Dirichlet process prior to bias learning towards concise grammars without the need to pre-specify the number of nonterminals. Cohen et al. (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. Gillenwater et al. (2010) incorporated a sparsity bias on grammar rules into learning by means of posterior regularization. We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias i"
D12-1121,erjavec-etal-2010-jos,0,0.0525382,"33.0 16.1 28.6 39.3 35.6 35.2 37.4 36.0 33.6 34.1 29.4 30.9 32.7 35.1 28.0 43.1 34.9 40.3 38.8 38.8 39.4 47.8 Portuguese Slovene 23.7 27.7 27.5 26.2 27.3 26.4 30.6 23.8 15.3 15.1 14.6 16.4 Swedish 31.9 42.0 29.3 32.7 37.2 46.0 Table 3: The dependency accuracies (on sentences of all lengths in the testing corpus) of grammars learned by our approach from the corpora of the following languages: Arabic (Hajiˇc et al., 2004), Basque (Aduriz et al., 2003), Czech (Hajiˇc et al., 2000), Danish (Buch-Kromann et al., 2007), Dutch (Beek et al., 2002), English, Portuguese (Afonso et al., 2002), Slovene (Erjavec et al., 2010), Swedish (Nivre et al., 2006). other types of priors and regularizations for unsupervised grammar learning, to apply it to more advanced grammar models, and to explore alternative formulations of unambiguity regularization. Acknowledgement The work of Kewei Tu was supported at Iowa State University in part by a research assistantship from the Iowa State University Center for Computational Intelligence, Learning, and Discovery, and at University of California, Los Angeles by the DARPA grant FA 8650-11-1-7149. The work of Vasant Honavar was supported by the National Science Foundation, while wo"
D12-1121,P07-1035,0,0.0577984,"Missing"
D12-1121,P10-2036,0,0.577969,"Missing"
D12-1121,N09-1012,0,0.788084,"l in learning the grammatical structures of short sentences. On the testing sentences of all lengths, σ = 0.25 achieves the best dependency accuracy, which suggests that controlling the strength of unambiguity regularization can contribute to improved performance. Testing Accuracy ≤ 10 ≤ 20 All DMV Model UR-Annealing UR-Annealing&Prior PR-S (Gillenwater et al., 2010) SLN TieV&N (Cohen and Smith, 2009) LN Families (Cohen et al., 2008) 63.6 66.6 62.1 61.3 59.3 53.1 57.7 53.8 47.4 45.1 47.9 52.3 49.1 41.4 39.0 Extended Models UR-Annealing on E-DMV(2,2) 71.4 UR-Annealing on E-DMV(3,3) 71.2 L-EVG (Headden et al., 2009) 68.8 LexTSG-DMV (Blunsom and Cohn, 2010) 67.7 62.4 61.5 - 57.0 56.0 55.7 Table 2: The dependency accuracies of grammars learned by our approach (denoted by “UR”) with annealing and prior, compared with previous published results. 4.2 Results with Annealing and Prior We annealed the value of σ from 1 to 0 when running our approach. We reduced the value of σ at a constant speed such that it reaches 0 at iteration 100. The results of this experiment (shown as “URAnnealing” in Table 2) suggest that annealing the value of σ not only helps circumvent the problem of choosing an optimal value of σ, b"
D12-1121,N07-1018,0,0.328707,"ics and Computer Science Department of Computer Science University of California, Los Angeles Iowa State University Los Angeles, CA 90095, USA Ames, IA 50011, USA tukw@ucla.edu honavar@cs.iastate.edu Abstract lihood of the grammar given the training data, typically using expectation-maximization (EM) (Baker, 1979; Lari and Young, 1990; Klein and Manning, 2004). More recent approaches incorporate additional prior information of the target grammar into learning. For example, Kurihara and Sato (2004) used Dirichlet priors over rule probabilities to obtain smoothed estimates of the probabilities. Johnson et al. (2007) used Dirichlet priors with hyperparameters set to values less than 1 to encourage sparsity of grammar rules. Finkel et al. (2007) and Liang et al. (2007) proposed to use the hierarchical Dirichlet process prior to bias learning towards concise grammars without the need to pre-specify the number of nonterminals. Cohen et al. (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. Gillenwater et al. (2010) incorporated a sparsity bias on grammar rules into learning by means of posterior regularization. We introduce a novel approach"
D12-1121,P04-1061,0,0.688225,"set of weights in mean-field variational inference (Kurihara and Sato, 2004). Therefore in order to compute q ∗ (zi ), when 0 &lt; σ &lt; 1, we simply need to raise all the weights 1 to the power of 1−σ before running the normal step ∗ of computing q (zi ) in standard mean-field variational inference; and when σ ≥ 1, we can simply use the weights to find the best parse of the training sentence and assign probability 1 to it. 4 Experiments We tested the effectiveness of unambiguity regularization in unsupervised learning of a type of dependency grammar called the dependency model with valence (DMV) (Klein and Manning, 2004). We report the results on the Wall Street Journal corpus (with section 2-21 for training and section 23 for testing) in section 4.1–4.3, and the results on the corpora of eight additional languages in section 1330 Value of σ 0 (standard EM) 0.25 0.5 0.75 1 (Viterbi EM) Testing Accuracy ≤ 10 ≤ 20 All 46.2 39.7 34.9 53.7 44.7 40.3 51.9 42.9 38.8 51.6 43.1 38.8 58.3 45.2 39.4 Table 1: The dependency accuracies of grammars learned by our approach with different values of σ. 4.4. On each corpus, we trained the learner on the gold-standard part-of-speech tags of the sentences of length ≤ 10 with pu"
D12-1121,D07-1072,0,0.0210238,"1, USA tukw@ucla.edu honavar@cs.iastate.edu Abstract lihood of the grammar given the training data, typically using expectation-maximization (EM) (Baker, 1979; Lari and Young, 1990; Klein and Manning, 2004). More recent approaches incorporate additional prior information of the target grammar into learning. For example, Kurihara and Sato (2004) used Dirichlet priors over rule probabilities to obtain smoothed estimates of the probabilities. Johnson et al. (2007) used Dirichlet priors with hyperparameters set to values less than 1 to encourage sparsity of grammar rules. Finkel et al. (2007) and Liang et al. (2007) proposed to use the hierarchical Dirichlet process prior to bias learning towards concise grammars without the need to pre-specify the number of nonterminals. Cohen et al. (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. Gillenwater et al. (2010) incorporated a sparsity bias on grammar rules into learning by means of posterior regularization. We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natu"
D12-1121,P06-1055,0,0.0277663,"in section 6. 2 The (Un)ambiguity of Natural Language Grammars A grammar is said to be ambiguous on a sentence if the sentence can be parsed in more than one way by the grammar. It is widely acknowledged that natural language grammars are ambiguous on a significant proportion of natural language sentences. For example, Manning and Sch¨utze (1999) show that a sentence randomly chosen from the Wall Street Journal — “The post office will hold out discounts and service concessions as incentives” — has at least five plausible syntactic parses. When we parse this sentence using the Berkeley parser (Petrov et al., 2006), one of the state-of-the-art English language parsers, we find many alternative parses in addition to the parses shown in (Manning and Sch¨utze, 1999). Indeed, with a probabilistic context-free grammar of only 26 nonterminals (as used in the Berkeley parser), the estimated total number of possible parses1 of the example sentence is 2 × 1037 . However, upon closer examination, we find that among this very large number of possible parses, only a few have significant probabilities. Figure 1 shows the probabilities of the 100 best parses of the example sentence. We can see that most of the parses"
D12-1121,P04-1062,0,0.0533732,"eneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art. It should be noted that our approach is closely related to the deterministic annealing (DA) technique studied in the optimization literature (Rose, 1998). However, DA has a very different motivation than ours and differs from our approach in a few important algorithmic details, as will be discussed in section 5. When applied to unsupervised grammar learning, DA has been shown to lead to worse parsing accuracy than standard EM (Smith and Eisner, 2004); in contrast, we show that our approach leads to significantly higher parsing accuracy than standard EM in unsupervised dependency grammar learning. The rest of the paper is organized as follows. Section 2 analyzes the degree of unambiguity of natural 1325 language grammars. Section 3 introduces the unambiguity regularization approach and shows that standard EM, Viterbi EM and softmax-EM are its special cases. We show the experimental results in section 4, discuss related work in section 5 and conclude the paper in section 6. 2 The (Un)ambiguity of Natural Language Grammars A grammar is said"
D12-1121,D07-1070,0,0.0547548,"arned grammar. The empirical results of Smith and Eisner (2004) show that DA resulted in lower parsing accuracy compared with standard EM in unsupervised constituent parsing; and a “skew” posterior term had to be inserted into the E-step formulation of DA to boost its accuracy over that of standard EM. In contrast, the results of our experiments show that unambiguity regularization leads to significantly higher parsing accuracy than standard EM. Unambiguity regularization is also related to the minimum entropy regularization framework for semi-supervised learning (Grandvalet and Bengio, 2005; Smith and Eisner, 2007), which tries to minimize the entropy of the class label or hidden variables on unlabeled data in addition to maximizing the likelihood of labeled data. However, entropy regularization is either motivated by the theoretical result that unlabeled data samples are informa1332 tive when classes are well separated (Grandvalet and Bengio, 2005), or derived from the expected conditional log-likelihood (Smith and Eisner, 2007). In contrast, our approach is motivated by the observed unambiguity of natural language grammars. One implication of this difference is that if our approach is applied to semi-"
D12-1121,W10-2902,0,0.286468,"o learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art. 1 Introduction Machine learning offers a potentially powerful approach to learning probabilistic grammars from data. Because of the high cost of manual sentence annotation, there is substantial interest in unsupervised grammar learning, i.e., the induction of a grammar from a corpus of unannotated sentences. The simplest such approaches attempt to maximize the like∗ Part of the work was done while at Iowa State University. More recently, Spitkovsky et al. (2010) and Poon and Domingos (2011) observed that the use of Viterbi EM (also called hard EM) in place of standard EM can lead to significantly improved results in unsupervised learning of probabilistic grammars from natural language and image data respectively, even if no prior information is used. This finding is surprising because Viterbi EM is a degenerate case of standard EM and is therefore generally considered to be less effective in locating the optimum of the objective function. Spitkovsky et al. (2010) speculated that the observed advantage of Viterbi EM over standard EM is due to standard"
D12-1121,nivre-etal-2006-talbanken05,0,\N,Missing
D16-1018,S13-2050,0,0.0699528,"Missing"
D16-1018,D14-1082,0,0.0337509,"ive function to replace the most computationally expensive maxlikelihood objective function. Recently proposed Skip-gram model, CBOW model and GloVe model (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) were more efficient than traditional models by introducing a log-linear layer and making it possible to train word embeddings with a large scale corpus. With the development of neural network and deep learning techniques, there have been a lot of work based on neural network models to obtain word embedding (Turian et al., 2010; Collobert et al., 2011; Maas et al., 2011; Chen and Manning, 2014). All of them have proven that word embedding is helpful in NLP tasks. However, the models above assumed that one word has only one vector as its representation which is problematic for polysemous words. Reisinger and Mooney (2010) proposed a method for constructing multiple sense-specific representation vectors for one word by performing word sense disambiguation with context clustering. Huang et al. (2012) further extended this context clustering method and incorporated global context to learn multi-prototype representation vectors. Chen et al. (2014) extended the context clustering method a"
D16-1018,D14-1110,0,0.175351,"Missing"
D16-1018,W16-2506,0,0.0313562,"the questioned word and consider the negative energy as the confidence of the sense choice. Then we calculate the cosine similarity between all pairs of senses of the questioned words and compute the average of similarity weighted by the confidence of the senses. The first method is named HardSim and the Model Huang Huang Chen Chen Neelakantan Neelakantan Li Tian Tian Bartunov Ours + CBOW Ours + CBOW Ours + Skip-gram Ours + Skip-gram Similarity Metrics AvgSim AvgSimC AvgSim AvgSimC AvgSim AvgSimC Model M Model W AvgSimC HardSim SoftSim HardSim SoftSim probabilistic models. However, note that Faruqui et al. (2016) presented several problems associated with the evaluation of word vectors on word similarity datasets and pointed out that the use of word similarity tasks for evaluation of word vectors is not sustainable. Bartunov et al. (2016) also suggest that SCWS should be of limited use for evaluating word representation models. Therefore, the results on this task shall be taken with caution. We consider that more realistic natural language processing tasks like word sense induction are better for evaluating sense embedding models. ρ × 100 62.8 65.7 66.2 68.9 67.2 69.2 69.7 63.6 65.4 61.2 64.3 65.6 64."
D16-1018,P12-1092,0,0.878201,"ch trains a vector for each sense of a word. There are two key steps in training sense embeddings. First, we need to perform word sense disambiguation (WSD) or word sense induction (WSI) to determine the senses of words in the training corpus. Then, we need to train embedding vectors for word senses according to their contexts. Introduction Distributed representation of words (aka word embedding) aims to learn continuous-valued vectors to ∗ The second author was supported by the National Natural Science Foundation of China (61503248). Early work on sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Kageback et al., 2015; Li and Jurafsky, 2015) proposes context clustering methods which determine the sense of a word by clustering aggregated embeddings of words in its context. This kind of methods is heuristic in nature and relies on external knowledge from lexicon like WordNet (Miller, 1995). 183 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 183–191, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Recently, sense embedding methods based on complete probabilisti"
D16-1018,N15-1070,0,0.083724,"ontext clustering to perform word sense induction tasks. Li and Jurafsky (2015) introduced a multi-sense embedding model based on the Chinese Restaurant Process and applied it to several natural language understanding tasks. Since the context clustering based models are heuristic in nature and rely on external knowledge, recent work tends to create probabilistic models for learning sense embeddings. Tian et al. (2014) proposed a multi-prototype Skip-gram model and designed an Expectation-Maximization (EM) algorithm to do word sense disambiguation and learn sense embedding vectors iteratively. Jauhar et al. (2015) extended the EM training framework and retrofitted embedding vectors to the ontology of 185 WordNet. Bartunov et al. (2016) proposed a nonparametric Bayesian extension of Skip-gram to automatically learn the required numbers of representations for all words and perform word sense induction tasks. 3 Context-Dependent Sense Embedding Model We propose the context-dependent sense embedding model for training high quality sense embeddings which takes into account the dependency between sense choices of neighboring words. Unlike pervious work, we do not learn any word embeddings in our model and he"
D16-1018,S13-2049,0,0.0383121,"Word Sense Induction In this section, we present an evaluation of our model on the word sense induction (WSI) tasks. The WSI task aims to discover the different meanings for words used in sentences. Unlike a word sense disambiguation (WSD) system, a WSI system does not link the sense annotation results to an existing sense inventory. Instead, it produces its own sense inventory and links the sense annotation results to this sense inventory. Our model can be seen as a WSI system, so we can evaluate our model with WSI tasks. We used the dataset from task 13 of SemEval2013 as our evaluation set (Jurgens and Klapaftis, 2013). The dataset contains 4664 instances inflected from one of the 50 lemmas. Both single-sense instances and instances with a graded mixture of senses are included in the dataset. In this paper, we only consider the single sense instances. Jurgens and Klapaftis (2013) propose two fuzzy measures named Fuzzy B-Cubed (FBC) and Fuzzy Normalized Mutual Information (FNMI) for comparing fuzzy sense assignments from WSI systems. the FBC measure summarizes the performance per instance while the FNMI measure is based on sense clusters rather than instances. Table 3 shows the results of our contextdependen"
D16-1018,W15-1504,0,0.186001,"eps in training sense embeddings. First, we need to perform word sense disambiguation (WSD) or word sense induction (WSI) to determine the senses of words in the training corpus. Then, we need to train embedding vectors for word senses according to their contexts. Introduction Distributed representation of words (aka word embedding) aims to learn continuous-valued vectors to ∗ The second author was supported by the National Natural Science Foundation of China (61503248). Early work on sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Kageback et al., 2015; Li and Jurafsky, 2015) proposes context clustering methods which determine the sense of a word by clustering aggregated embeddings of words in its context. This kind of methods is heuristic in nature and relies on external knowledge from lexicon like WordNet (Miller, 1995). 183 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 183–191, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Recently, sense embedding methods based on complete probabilistic models and well-defined learning objective functions (Tian et al.,"
D16-1018,D15-1200,0,0.661598,"mbeddings. First, we need to perform word sense disambiguation (WSD) or word sense induction (WSI) to determine the senses of words in the training corpus. Then, we need to train embedding vectors for word senses according to their contexts. Introduction Distributed representation of words (aka word embedding) aims to learn continuous-valued vectors to ∗ The second author was supported by the National Natural Science Foundation of China (61503248). Early work on sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Kageback et al., 2015; Li and Jurafsky, 2015) proposes context clustering methods which determine the sense of a word by clustering aggregated embeddings of words in its context. This kind of methods is heuristic in nature and relies on external knowledge from lexicon like WordNet (Miller, 1995). 183 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 183–191, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Recently, sense embedding methods based on complete probabilistic models and well-defined learning objective functions (Tian et al., 2014; Bartunov et al.,"
D16-1018,P11-1015,0,0.136949,"a max-margin objective function to replace the most computationally expensive maxlikelihood objective function. Recently proposed Skip-gram model, CBOW model and GloVe model (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) were more efficient than traditional models by introducing a log-linear layer and making it possible to train word embeddings with a large scale corpus. With the development of neural network and deep learning techniques, there have been a lot of work based on neural network models to obtain word embedding (Turian et al., 2010; Collobert et al., 2011; Maas et al., 2011; Chen and Manning, 2014). All of them have proven that word embedding is helpful in NLP tasks. However, the models above assumed that one word has only one vector as its representation which is problematic for polysemous words. Reisinger and Mooney (2010) proposed a method for constructing multiple sense-specific representation vectors for one word by performing word sense disambiguation with context clustering. Huang et al. (2012) further extended this context clustering method and incorporated global context to learn multi-prototype representation vectors. Chen et al. (2014) extended the co"
D16-1018,S07-1006,0,0.237215,"atch size of our algorithm depends on the length of each sentence. The advantage of using mini batch is twofold. First, while our learning objective is highly nonconvex (Tian et al., 2014), the randomness in mini batch hard EM may help us avoid trapping into local optima. Second, the model parameters are updated more frequently in mini batch hard EM, resulting in faster convergence. Note that before running hard-EM, we need to determine, for each word w, the size of S(w). In our experiments, we used the sense inventory provided by Coarse-Grained English All-Words Task of SemEval-2007 Task 07 (Navigli et al., 2007) to determine the number of senses for each word. The sense inventory is a coarse version of WordNet sense kwk XX X ∗ inventory. We do not use the WordNet sense inΘ = arg min Θ ventory because the senses in WordNet are too finew∈C i=1 sneg ∈Sneg (wi )  grained and are difficult to recognize even for human X max 1 − σ( V (sj )T V 0 (si )) annotators (Edmonds and Kilgarriff, 2002). Since i−k≤j≤i+k,j6=i  we do not link our learned senses with external sense X inventories, our approach can be seen as performing + σ( V (sj )T V 0 (sneg )), 0 WSI instead of WSD. Here Θ is the set of all the parame"
D16-1018,D14-1113,0,0.580596,"word. There are two key steps in training sense embeddings. First, we need to perform word sense disambiguation (WSD) or word sense induction (WSI) to determine the senses of words in the training corpus. Then, we need to train embedding vectors for word senses according to their contexts. Introduction Distributed representation of words (aka word embedding) aims to learn continuous-valued vectors to ∗ The second author was supported by the National Natural Science Foundation of China (61503248). Early work on sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Kageback et al., 2015; Li and Jurafsky, 2015) proposes context clustering methods which determine the sense of a word by clustering aggregated embeddings of words in its context. This kind of methods is heuristic in nature and relies on external knowledge from lexicon like WordNet (Miller, 1995). 183 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 183–191, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Recently, sense embedding methods based on complete probabilistic models and well-defined learning objective"
D16-1018,D14-1162,0,0.0811872,"ion to replace the most computationally expensive maxlikelihood objective function. Recently proposed Skip-gram model, CBOW model and GloVe model (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) were more efficient than traditional models by introducing a log-linear layer and making it possible to train word embeddings with a large scale corpus. With the development of neural network and deep learning techniques, there have been a lot of work based on neural network models to obtain word embedding (Turian et al., 2010; Collobert et al., 2011; Maas et al., 2011; Chen and Manning, 2014). All of them have proven that word embedding is helpful in NLP tasks. However, the models above assumed that one word has only one vector as its representation which is problematic for polysemous words. Reisinger and Mooney (2010) proposed a method for constructing multiple sense-specific representation vectors for one word by performing word sense disambiguation with context clustering. Huang et al. (2012) further extended this context clustering method and incorporated global context to learn multi-prototype representation vectors. Chen et al. (2014) extended the context clustering method a"
D16-1018,N10-1013,0,0.818418,"ution is sense embedding which trains a vector for each sense of a word. There are two key steps in training sense embeddings. First, we need to perform word sense disambiguation (WSD) or word sense induction (WSI) to determine the senses of words in the training corpus. Then, we need to train embedding vectors for word senses according to their contexts. Introduction Distributed representation of words (aka word embedding) aims to learn continuous-valued vectors to ∗ The second author was supported by the National Natural Science Foundation of China (61503248). Early work on sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Kageback et al., 2015; Li and Jurafsky, 2015) proposes context clustering methods which determine the sense of a word by clustering aggregated embeddings of words in its context. This kind of methods is heuristic in nature and relies on external knowledge from lexicon like WordNet (Miller, 1995). 183 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 183–191, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Recently, sense embedding methods based on c"
D16-1018,P15-1173,0,0.0643591,"boring words. Unlike pervious work, we do not learn any word embeddings in our model and hence avoid the problem with embedding polysemous words discussed previously. In this section, we will introduce our model and describe our inference and learning algorithms. 3.1 Model We begin with the notation in our model. In a sentence, let wi be the ith word of the sentence and si be the sense of the ith word. S(w) denotes the set of all the senses of word w. We assume that the sets of senses of different words do not overlap. Therefore, in this paper a word sense can be seen as a lexeme of the word (Rothe and Schutze, 2015). Our model can be represented as a Markov network shown in Figure 1. It is similar to a highorder hidden Markov model. The model contains a sequence of observable words (w1 , w2 , . . .) and latent senses (s1 , s2 , . . .). It models the dependency between each word-sense pair and between neighboring senses in the sequence. The energy function is formulated as follows: E(w, s) = X i  E1 (wi , si ) + E2 (si−k , . . . , si+k ) (1) Here w = {wi |1 ≤ i ≤ l} is the set of words in a sentence with length l and s = {si |1 ≤ i ≤ l} is the set of senses. The function E1 models the dependency between"
D16-1018,C14-1016,0,0.606768,"et al., 2015; Li and Jurafsky, 2015) proposes context clustering methods which determine the sense of a word by clustering aggregated embeddings of words in its context. This kind of methods is heuristic in nature and relies on external knowledge from lexicon like WordNet (Miller, 1995). 183 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 183–191, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Recently, sense embedding methods based on complete probabilistic models and well-defined learning objective functions (Tian et al., 2014; Bartunov et al., 2016; Jauhar et al., 2015) become more popular. These methods regard the choice of senses of the words in a sentence as hidden variables. Learning is therefore done with expectationmaximization style algorithms, which alternate between inferring word sense choices in the training corpus and learning sense embeddings. A common problem with these methods is that they model the sense embedding of each center word dependent on the word embeddings of its context words. As we previously explained, word embedding of a polysemous word is not a good representation and may negatively"
D16-1018,P10-1040,0,0.0572726,"cess. Collobert and Weston (2008) introduced a max-margin objective function to replace the most computationally expensive maxlikelihood objective function. Recently proposed Skip-gram model, CBOW model and GloVe model (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) were more efficient than traditional models by introducing a log-linear layer and making it possible to train word embeddings with a large scale corpus. With the development of neural network and deep learning techniques, there have been a lot of work based on neural network models to obtain word embedding (Turian et al., 2010; Collobert et al., 2011; Maas et al., 2011; Chen and Manning, 2014). All of them have proven that word embedding is helpful in NLP tasks. However, the models above assumed that one word has only one vector as its representation which is problematic for polysemous words. Reisinger and Mooney (2010) proposed a method for constructing multiple sense-specific representation vectors for one word by performing word sense disambiguation with context clustering. Huang et al. (2012) further extended this context clustering method and incorporated global context to learn multi-prototype representation"
D16-1073,D10-1117,0,0.505202,"Missing"
D16-1073,D14-1082,0,0.0285852,"ReLU activation function. We have two versions of weight matrix Wdir for the direction dir being left and right respectively. Outputs (CHILD or DECISION) Softmax Layer: … p = Sof tmax(W f ) W Hidden Layer: f = ReLU (Wdir [vh ; vval ]) Wdir Continous Representation: [vh ; vval ] Inputs: Valency Head POS Tag f (h, dir, val) = ReLU(Wdir [vh ; vval ]) Figure 1: Structure of the neural network. Both CHILD and DECISION use the same architecture for the calculation of distributions. based dependency parsing. Stenetorp (2013) applied recursive neural networks to transitional based dependency parsing. Chen and Manning (2014) built a neural network based parser with dense features instead of sparse indicator features. Dyer et al. (2015) proposed a stack long short-term memory approach to supervised dependency parsing. To our knowledge, our work is the first attempt to incorporate neural networks into a generative grammar for unsupervised dependency parsing. 3 Neural DMV In this section, we introduce our neural based grammar induction approach. We describe the model in section 3.1 and the learning method in section 3.2. 3.1 Model Our model is based on the DMV model (section 2.1), except that the CHILD and DECISION"
D16-1073,N09-1009,0,0.69464,"ce and Technology ShanghaiTech University, Shanghai, China Abstract Previous work on unsupervised dependency parsing is mainly based on the dependency model with valence (DMV) (Klein and Manning, 2004) and its extension (Headden III et al., 2009; Gillenwater et al., 2010). To effectively learn the DMV model for better parsing accuracy, a variety of inductive biases and handcrafted features have been proposed to incorporate prior information into learning. One useful type of prior information is that there exist correlations between the parameters of grammar rules involving different POS tags. Cohen and Smith (2009; 2010) employed special prior distributions to encourage learning of correlations between POS tags. Berg-Kirkpatrick et al. (2010) encoded the relations between POS tags using manually designed features. Unsupervised dependency parsing aims to learn a dependency grammar from text annotated with only POS tags. Various features and inductive biases are often used to incorporate prior knowledge into learning. One useful type of prior information is that there exist correlations between the parameters of grammar rules involving different POS tags. Previous work employed manually designed features"
D16-1073,P15-1033,0,0.0225246,"tively. Outputs (CHILD or DECISION) Softmax Layer: … p = Sof tmax(W f ) W Hidden Layer: f = ReLU (Wdir [vh ; vval ]) Wdir Continous Representation: [vh ; vval ] Inputs: Valency Head POS Tag f (h, dir, val) = ReLU(Wdir [vh ; vval ]) Figure 1: Structure of the neural network. Both CHILD and DECISION use the same architecture for the calculation of distributions. based dependency parsing. Stenetorp (2013) applied recursive neural networks to transitional based dependency parsing. Chen and Manning (2014) built a neural network based parser with dense features instead of sparse indicator features. Dyer et al. (2015) proposed a stack long short-term memory approach to supervised dependency parsing. To our knowledge, our work is the first attempt to incorporate neural networks into a generative grammar for unsupervised dependency parsing. 3 Neural DMV In this section, we introduce our neural based grammar induction approach. We describe the model in section 3.1 and the learning method in section 3.2. 3.1 Model Our model is based on the DMV model (section 2.1), except that the CHILD and DECISION probabilities are calculated through two neural networks. We do not compute the ROOT probabilities using a neural"
D16-1073,P11-2003,0,0.0203824,"II (2009) proposed a stochastic search based method to do unsupervised Shift-Reduce transition parsing. Rasooli and Faili (2012) proposed a transition based unsupervised dependency parser together with ""baby-step"" training (Spitkovsky et al., 2010) to improve parsing accuracy. Le and Zuidema (2015) proposed a complicated reranking based unsupervised dependency parsing system and achieved the state-of-the-art performance on the Penn Treebank dataset. 2.4 Neural based Supervised Dependency Parser There exist several previous approaches on using neural networks for supervised dependency parsing. Garg and Henderson (2011) proposed a Temporal Restricted Boltzmann Machine to do transition The full architecture of the neural network is shown in Figure 1. First, we represent each head tag h as a d dimensional vector vh ∈ Rd , represent each value of valence val as a d0 dimensional vector 0 vval ∈ Rd . We concatenate vh and vval as the input embedding vector. Then we map the input layer to a hidden layer with weight matrix Wdir through a ReLU activation function. We have two versions of weight matrix Wdir for the direction dir being left and right respectively. Outputs (CHILD or DECISION) Softmax Layer: … p = Sof t"
D16-1073,W12-1909,0,0.18872,"Missing"
D16-1073,P10-2036,0,0.131163,"ons shows the results. It can be seen that our approach with Viterbi EM significantly outperforms the EM and viterbi EM baselines and also outperforms the two previous approaches. 4.3 WSJ 53.1 53.3 57.0 52.5 57.6 55.7 64.4 65.8 Table 2: Comparison of recent unsupervised dependency parsing systems. Basic setup means learning from POS tags with sentences of length ≤ 10 and punctuation stripped off. Extra information may contain punctuations, longer sentences, lexiResults on the extended DMV model We directly apply our neural approach to learning the extended DMV model (Headden III et al., 2009; Gillenwater et al., 2010) (with the maximum valence value set to 2 for both CHILD and DECISION rules). As shown in Table 2, we achieve comparable accuracy with recent state-of-the-art systems. If we initialize our model with the grammar learned by Tu and Honavar (2012), the accuracy of our approach can be further improved. Most of the recent state-of-the-art systems employ more complicated models and learning algorithms. For example, Spitkovsky et al. (2013) take several grammar induction techniques as modules and connect them in various ways; Le and Zuidema (2015) use a neural-based supervised parser and reranker tha"
D16-1073,N09-1012,0,0.631817,"Missing"
D16-1073,P04-1061,0,0.920814,"outperforms the previous approaches that also utilize POS tag correla763 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 763–771, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics tions, and achieves a comparable result with recent state-of-the-art grammar induction systems. On the datasets of eight additional languages, our approach is able to achieve better performance than the baseline methods without any parameter tuning. 2 2.1 Related work Dependency Model with Valence The dependency model with valence (DMV) (Klein and Manning, 2004) is the first model to outperform the left-branching baseline in unsupervised dependency parsing of English. The DMV model is a generative model of a sentence and its parse tree. It generates a dependency parse from the root in a recursive top-down manner. At each step, a decision is first made as to whether a new child POS tag shall be generated from the current head tag; if the decision is yes, then a new child POS tag is sampled; otherwise, the existing child tags are recursively visited. There are three types of grammar rules in the model: CHILD, DECISION and ROOT, each with a set of multi"
D16-1073,N15-1067,0,0.73857,"In the work of Tu and Honavar (2012), unambiguity of parse trees is incorporated into the training objective function of DMV to obtain a better performance. 2.3 Other Approaches to Unsupervised Dependency Parsing There are many other approaches to unsupervised dependency parsing that are not based on DMV. Daumé III (2009) proposed a stochastic search based method to do unsupervised Shift-Reduce transition parsing. Rasooli and Faili (2012) proposed a transition based unsupervised dependency parser together with ""baby-step"" training (Spitkovsky et al., 2010) to improve parsing accuracy. Le and Zuidema (2015) proposed a complicated reranking based unsupervised dependency parsing system and achieved the state-of-the-art performance on the Penn Treebank dataset. 2.4 Neural based Supervised Dependency Parser There exist several previous approaches on using neural networks for supervised dependency parsing. Garg and Henderson (2011) proposed a Temporal Restricted Boltzmann Machine to do transition The full architecture of the neural network is shown in Figure 1. First, we represent each head tag h as a d dimensional vector vh ∈ Rd , represent each value of valence val as a d0 dimensional vector 0 vval"
D16-1073,W12-0701,0,0.0157412,"nd CHILD parameters. These two approaches both utilize the correlations between POS tags to obtain better probability estimation of grammar rules involving such correlated POS tags. In the work of Tu and Honavar (2012), unambiguity of parse trees is incorporated into the training objective function of DMV to obtain a better performance. 2.3 Other Approaches to Unsupervised Dependency Parsing There are many other approaches to unsupervised dependency parsing that are not based on DMV. Daumé III (2009) proposed a stochastic search based method to do unsupervised Shift-Reduce transition parsing. Rasooli and Faili (2012) proposed a transition based unsupervised dependency parser together with ""baby-step"" training (Spitkovsky et al., 2010) to improve parsing accuracy. Le and Zuidema (2015) proposed a complicated reranking based unsupervised dependency parsing system and achieved the state-of-the-art performance on the Penn Treebank dataset. 2.4 Neural based Supervised Dependency Parser There exist several previous approaches on using neural networks for supervised dependency parsing. Garg and Henderson (2011) proposed a Temporal Restricted Boltzmann Machine to do transition The full architecture of the neural"
D16-1073,N10-1116,0,0.0289921,"timation of grammar rules involving such correlated POS tags. In the work of Tu and Honavar (2012), unambiguity of parse trees is incorporated into the training objective function of DMV to obtain a better performance. 2.3 Other Approaches to Unsupervised Dependency Parsing There are many other approaches to unsupervised dependency parsing that are not based on DMV. Daumé III (2009) proposed a stochastic search based method to do unsupervised Shift-Reduce transition parsing. Rasooli and Faili (2012) proposed a transition based unsupervised dependency parser together with ""baby-step"" training (Spitkovsky et al., 2010) to improve parsing accuracy. Le and Zuidema (2015) proposed a complicated reranking based unsupervised dependency parsing system and achieved the state-of-the-art performance on the Penn Treebank dataset. 2.4 Neural based Supervised Dependency Parser There exist several previous approaches on using neural networks for supervised dependency parsing. Garg and Henderson (2011) proposed a Temporal Restricted Boltzmann Machine to do transition The full architecture of the neural network is shown in Figure 1. First, we represent each head tag h as a d dimensional vector vh ∈ Rd , represent each val"
D16-1073,D13-1204,0,0.185237,"ions, longer sentences, lexiResults on the extended DMV model We directly apply our neural approach to learning the extended DMV model (Headden III et al., 2009; Gillenwater et al., 2010) (with the maximum valence value set to 2 for both CHILD and DECISION rules). As shown in Table 2, we achieve comparable accuracy with recent state-of-the-art systems. If we initialize our model with the grammar learned by Tu and Honavar (2012), the accuracy of our approach can be further improved. Most of the recent state-of-the-art systems employ more complicated models and learning algorithms. For example, Spitkovsky et al. (2013) take several grammar induction techniques as modules and connect them in various ways; Le and Zuidema (2015) use a neural-based supervised parser and reranker that make use of high-order features and lexical information. We expect that the performance of our approach can be further improved when these more advanced techniques are incorporated. 4.4 WSJ10 Systems with Basic Setup EVG (Headden III et al., 2009) 65.0 TSG-DMV (Blunsom and Cohn, 2010) 65.9 PR-S (Gillenwater et al., 2010) 64.3 UR-A E-DMV (Tu and Honavar, 2012) 71.4 Neural E-DMV 69.7 Neural E-DMV (Good Init) 72.5 Systems Using Extra"
D16-1073,D12-1121,1,0.909992,"grammar rules. There have been many more advanced learning algorithms of the DMV model beyond the basic EM algorithm. In the work of Cohen and Smith (2008), a logistic normal prior was used in the DMV model to capture the similarity between POS tags. In the work of Berg-Kirkpatrick et al. (2010), features that group various morphological variants of nouns and verbs are used to predict the DECISION and CHILD parameters. These two approaches both utilize the correlations between POS tags to obtain better probability estimation of grammar rules involving such correlated POS tags. In the work of Tu and Honavar (2012), unambiguity of parse trees is incorporated into the training objective function of DMV to obtain a better performance. 2.3 Other Approaches to Unsupervised Dependency Parsing There are many other approaches to unsupervised dependency parsing that are not based on DMV. Daumé III (2009) proposed a stochastic search based method to do unsupervised Shift-Reduce transition parsing. Rasooli and Faili (2012) proposed a transition based unsupervised dependency parser together with ""baby-step"" training (Spitkovsky et al., 2010) to improve parsing accuracy. Le and Zuidema (2015) proposed a complicated"
D16-1073,N10-1083,0,\N,Missing
D16-1208,N10-1083,0,0.0585417,"Missing"
D16-1208,N09-1009,0,0.0259469,"0 0 0.2 0.4 0.2 0.5 0.5 0.6 0.8 0 1.2 0 1 0.2 0.4 0.45 0.4 0.4 ‐70 ‐60 ‐50 ‐40 ‐30 ‐20 ‐10 0 0 0.25 0.5 0.75 1 Figure 6: Parsing accuracy vs. the value of α VB‐Dir 0.9 0.9 0.85 0.8 0.8 0.75 0.7 0.7 0.65 0.6 0.6 0.6 0.55 0.5 0.5 0.45 0.4 0.4 0.8Dir ‐70 ‐60 ‐50 ‐40 ‐30 ‐20 ‐10 0 HVB‐Dir 0.8 0 0.25 SVB‐Dir 1 0.5 0.75 1.2 1 Figure 7: Sparsity of the learned grammars vs. the value of α r ately negative α values. HEM-mDir consistently produces accuracy around 0.63 with a large range of α values (from -10 to -40), which is on a par with the best published results in learning the original DMV model (Cohen and Smith, 2009; Gillenwater et al., 2010; Berg-Kirkpatrick et al., 2010), even though these previous approaches employed more sophisticated features and advanced regularization techniques than ours. Figure 7 shows the degree of sparsity of the learned dependency grammars. We computed the percentage of dependency rules with probabilities below 10−3 to measure the degree of sparsity. It can be seen that even with positive α values, mDir leads to significantly more sparse grammars than Dir does. With negative values of α, mDir can induce even more sparsity. Figure 8 plots the parsing accuracy with different va"
D16-1208,P10-2036,0,0.0407671,"Missing"
D16-1208,N07-1018,0,0.203644,"cy of parsing the test corpus using the learned dependency models. It can be seen that with positive α values, Dir and mDir have very similar accuracy under the standard, hard and softmax versions of inference respectively. With negative α values, the accuracy of EMmDir decreases; but for HEM-mDir and SEM-mDir, the accuracy is significantly improved with moder4 Unsupervised Dependency Parsing Unsupervised dependency parsing aims to learn a dependency grammar from unannotated text. Previous work has shown that sparsity regularization improves the performance of unsupervised dependency parsing (Johnson et al., 2007; Gillenwater et al., 2010). In our experiments, we tried to learn a dependency model with valence (DMV) (Klein and Manning, 2004) from the Wall Street Journal corpus, with section 2-21 for training and section 23 1989 3 3 3 3 3 2.5 2.5 2.5 2.5 2.5 2 2 2 2 2 1.5 1.5 1.5 1.5 1.5 1 1 1 1 1 0.5 0.5 0.5 0.5 0.5 0 0 1 2 0 3 (a) Ground-truth 0 1 2 0 3 (b) VB-Dir, α = 10 −5 0 1 2 0 3 (c) EM-mDir, α = −2 0 1 2 0 3 (d) VB-Dir, α = 10 −5 0 1 2 3 (e) EM-mDir, α = −30 Figure 5: The ground-truth model and four typical models learned by VB-Dir and EM-mDir. (b),(c): 20 training samples. (d),(e): 200 training"
D16-1208,P04-1061,0,0.0343925,"ve very similar accuracy under the standard, hard and softmax versions of inference respectively. With negative α values, the accuracy of EMmDir decreases; but for HEM-mDir and SEM-mDir, the accuracy is significantly improved with moder4 Unsupervised Dependency Parsing Unsupervised dependency parsing aims to learn a dependency grammar from unannotated text. Previous work has shown that sparsity regularization improves the performance of unsupervised dependency parsing (Johnson et al., 2007; Gillenwater et al., 2010). In our experiments, we tried to learn a dependency model with valence (DMV) (Klein and Manning, 2004) from the Wall Street Journal corpus, with section 2-21 for training and section 23 1989 3 3 3 3 3 2.5 2.5 2.5 2.5 2.5 2 2 2 2 2 1.5 1.5 1.5 1.5 1.5 1 1 1 1 1 0.5 0.5 0.5 0.5 0.5 0 0 1 2 0 3 (a) Ground-truth 0 1 2 0 3 (b) VB-Dir, α = 10 −5 0 1 2 0 3 (c) EM-mDir, α = −2 0 1 2 0 3 (d) VB-Dir, α = 10 −5 0 1 2 3 (e) EM-mDir, α = −30 Figure 5: The ground-truth model and four typical models learned by VB-Dir and EM-mDir. (b),(c): 20 training samples. (d),(e): 200 training samples. EM‐mDir r HEM‐mDir SEM‐mDir VB‐Dir HVB‐Dir SVB‐Dir 0.65 0.8 0.6 EM‐mDir HEM‐mDir SEM‐mDir 0.6 0.6 0.6 0.4 0.4 0.55 0.2 0"
D16-1208,D12-1121,1,0.840265,"he likelihood in posterior inference and cannot effectively prune mixture components. On the other hand, with a highly negative α value, mDir is still effective as a sparsity prior. for testing. Following previous work, we used sentences of length ≤ 10 with punctuation stripped off. Since DMV is an unlexicalized model, the number of dependency rules is small relative to the training corpus size. This suggests that a strong prior can be helpful in counterbalancing the influence of the training data. We tested six approaches. With a mDir prior, we tried EM, hard EM, and softmax-EM with σ = 0.5 (Tu and Honavar, 2012) (denoted by EM-mDir, HEM-mDir, SEM-mDir). With a Dir prior, we tried variational inference, hard variational inference, and softmax variational inference with σ = 0.5 (Tu and Honavar, 2012) (denoted by VB-Dir, HVB-Dir, SVB-Dir). Again, we used symmetric Dir and mDir priors. For mDir, we set  = 10−4 by default. Figure 6 shows the directed accuracy of parsing the test corpus using the learned dependency models. It can be seen that with positive α values, Dir and mDir have very similar accuracy under the standard, hard and softmax versions of inference respectively. With negative α values, the"
D17-1171,N10-1083,0,0.125147,"Missing"
D17-1171,D10-1117,0,0.267404,"Missing"
D17-1171,C96-1058,0,0.43038,"Missing"
D17-1171,W12-1909,0,0.0462715,"Missing"
D17-1171,P15-1133,0,0.263494,"in sentences from unlabeled data, is a very challenging task in natural language processing. Most of the previous work on unsupervised dependency parsing is based on generative models such as the dependency model with valence (DMV) introduced by Klein and Manning (2004). Many approaches have been proposed to enhance these generative models, for example, by designing advanced Bayesian priors (Cohen et al., 2008), representing dependencies with features (Berg-Kirkpatrick et al., 2010), and representing discrete tokens with continuous vectors (Jiang et al., 2016). Besides generative approaches, Grave and Elhadad (2015) proposed an unsupervised discrim∗ This work was supported by the National Natural Science Foundation of China (61503248). Conditional random field autoencoder (Ammar et al., 2014) is a new framework for unsupervised structured prediction. There are two components of this model: an encoder and a decoder. The encoder is a globally normalized feature-rich CRF model predicting the conditional distribution of the latent structure given the observed structured input. The decoder of the model is a generative model generating a transformation of the structured input from the latent structure. Ammar e"
D17-1171,D16-1073,1,0.909968,"parsing, which aims to discover syntactic structures in sentences from unlabeled data, is a very challenging task in natural language processing. Most of the previous work on unsupervised dependency parsing is based on generative models such as the dependency model with valence (DMV) introduced by Klein and Manning (2004). Many approaches have been proposed to enhance these generative models, for example, by designing advanced Bayesian priors (Cohen et al., 2008), representing dependencies with features (Berg-Kirkpatrick et al., 2010), and representing discrete tokens with continuous vectors (Jiang et al., 2016). Besides generative approaches, Grave and Elhadad (2015) proposed an unsupervised discrim∗ This work was supported by the National Natural Science Foundation of China (61503248). Conditional random field autoencoder (Ammar et al., 2014) is a new framework for unsupervised structured prediction. There are two components of this model: an encoder and a decoder. The encoder is a globally normalized feature-rich CRF model predicting the conditional distribution of the latent structure given the observed structured input. The decoder of the model is a generative model generating a transformation o"
D17-1171,P04-1061,0,0.9492,"tic priors. We propose an exact algorithm for parsing as well as a tractable learning algorithm. We evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state-of-the-art approaches. 1 Introduction Unsupervised dependency parsing, which aims to discover syntactic structures in sentences from unlabeled data, is a very challenging task in natural language processing. Most of the previous work on unsupervised dependency parsing is based on generative models such as the dependency model with valence (DMV) introduced by Klein and Manning (2004). Many approaches have been proposed to enhance these generative models, for example, by designing advanced Bayesian priors (Cohen et al., 2008), representing dependencies with features (Berg-Kirkpatrick et al., 2010), and representing discrete tokens with continuous vectors (Jiang et al., 2016). Besides generative approaches, Grave and Elhadad (2015) proposed an unsupervised discrim∗ This work was supported by the National Natural Science Foundation of China (61503248). Conditional random field autoencoder (Ammar et al., 2014) is a new framework for unsupervised structured prediction. There a"
D17-1171,D07-1015,0,0.0737862,"Missing"
D17-1171,N15-1067,0,0.551122,"Missing"
D17-1171,P05-1012,0,0.364621,"oughout this paper, we ˆ = x. set x The encoder in our model is a log-linear model represented by a first-order dependency parser. The score of a dependency tree can be factorized as the sum of scores of its dependencies. For each dependency arc (x, i, j), where i and j are the indices of the head and child of the dependency, a feature vector f (x, i, j) is specified. The score of a dependency is defined as the inner product of the feature vector and a weight vector w, n Y i=1 θxˆi |ti ˆ , y given x is The conditional distribution of x ˆ |x) = P (y|x)P (ˆ P (y, x x|y) 2.1.1 Features Following McDonald et al. (2005) and Grave et al. (2015), we define the feature vector of a dependency based on the part-of-speech tags (POS) of the head, child and context words, the direction, and the distance between the head and child of the dependency. The feature template used in our parser is shown in Table 1. 2.1.2 Parsing Given parameters w and θ, we can parse a sentence x by searching for a dependency tree y which has the highest probability P (ˆ x, y|x). y∗ = arg max log P (ˆ x, y|x) y∈Y(x) = arg max T φ(x, i, j) = w f (x, i, j) y∈Y(x) 1639 n X i=1 φ(x, hi , i) + log θxˆi |ti  POSi × dis × dir POSj × dis × dir PO"
D17-1171,D10-1120,0,0.740682,"formance of unsupervised dependency parsing in comparison with EM. Therefore, instead of using negative conditional log likelihood as our objective function, we choose to use negative conditional Viterbi log likelihood, − N X  log  max P (ˆ xi , y|xi ) + λΩ(w) (1) y∈Y(xi ) i=1 where Ω(w) is a L1 regularization term of the encoder parameter w and λ is a hyper-parameter controlling the strength of regularization. To encourage learning of dependency relations that satisfy universal linguistic knowledge, we add a soft constraint on the parse tree based on the universal syntactic rules following Naseem et al. (2010) and Grave et al. (2015). Hence our objective function becomes − N X i=1  log α  max P (ˆ xi , y|xi )Q (xi , y) +λΩ(w) Table 2: Universal linguistic rules taken into account. Q(x, y) = exp X ! 1[(ti → xi ) ∈ R] i where 1[(ti → xi ) ∈ R] is an indicator function of whether dependency ti → xi satisfies one of the universal linguistic rules in R. The universal linguistic rules that we use are shown in Table 2 (Naseem et al., 2010). 2.2.2 Algorithm We apply coordinate descent to minimize the objective function, which alternately updates w and θ. In each optimization step of w, we run two epochs"
D17-1171,D13-1204,0,0.449817,"Missing"
D17-1171,W10-2902,0,0.202628,"is × dir POSi × POSj × POSj+1 × dis × dir VERB → VERB VERB → NOUN VERB → PRON VERB → ADV VERB → ADP ADJ → ADV Table 1: Feature template of a dependency, where i is the index of the head, j is the index of the child, dis = |i − j|, and dir is the direction of the dependency. For projective dependency parsing, we can use Eisners algorithm (1996) to find the best parse in O(n3 ) time. For non-projective dependency parsing, we can use the Chu-Liu/Edmond algorithm (Chu and Liu, 1965; Edmonds, 1967; Tarjan, 1977) to find the best parse in O(n2 ) time. 2.2 Parameter Learning 2.2.1 Objective Function Spitkovsky et al. (2010) shows that Viterbi EM can improve the performance of unsupervised dependency parsing in comparison with EM. Therefore, instead of using negative conditional log likelihood as our objective function, we choose to use negative conditional Viterbi log likelihood, − N X  log  max P (ˆ xi , y|xi ) + λΩ(w) (1) y∈Y(xi ) i=1 where Ω(w) is a L1 regularization term of the encoder parameter w and λ is a hyper-parameter controlling the strength of regularization. To encourage learning of dependency relations that satisfy universal linguistic knowledge, we add a soft constraint on the parse tree based o"
D17-1171,D12-1121,1,0.840344,"Missing"
D17-1176,D10-1117,0,0.508976,"0; Tu and Honavar, 2012). Lexicalized grammar induction aims to incorporate lexical information into the learned grammar to increase its representational power and improve the learning accuracy. The most straightforward approach to encoding lexical information is full lexicalization (Pate and Johnson, 2016; Spitkovsky et al., 2013). A major problem with ∗ This work was supported by the National Natural Science Foundation of China (61503248). full lexicalization is that the grammar becomes much larger and thus learning is more data demanding. To mitigate this problem, Headden et al. (2009) and Blunsom and Cohn (2010) used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Another straightforward way to mitigate the data scarcity problem of lexicalization is to use training corpora larger than the standard WSJ corpus. For example, Pate and Johnson (2016) used two large corpora containing more than 700k sentences; Marecek and Straka (2013) utilized a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar. Finally, smoothing techniques can be used to reduce the negative impact of data scarcity. One example is Neural DMV (NDMV) (Ji"
D17-1176,P10-2036,0,0.223101,"Missing"
D17-1176,N09-1012,0,0.767629,"Missing"
D17-1176,D16-1073,1,0.826404,"Grammar Induction with Neural Lexicalization and Big Training Data∗ Wenjuan Han, Yong Jiang and Kewei Tu {hanwj, jiangyong ,tukw}@shanghaitech.edu.cn School of Information Science and Technology ShanghaiTech University, Shanghai, China Abstract We study the impact of big models (in terms of the degree of lexicalization) and big data (in terms of the training corpus size) on dependency grammar induction. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence (Jiang et al., 2016). We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art. 1 Introduction Grammar induction is the task of learning a grammar from a set of unannotated sentences. In the most common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpu"
D17-1176,P04-1061,0,0.910121,"us based on Wikipedia in learning an unlexicalized dependency grammar. Finally, smoothing techniques can be used to reduce the negative impact of data scarcity. One example is Neural DMV (NDMV) (Jiang et al., 2016) which incorporates neural networks into DMV and can automatically smooth correlated grammar rule probabilities. Inspired by this background, we conduct a systematic study regarding the impact of the degree of lexicalization and the training data size on the accuracy of grammar induction approaches. We experimented with a lexicalized version of Dependency Model with Valence (L-DMV) (Klein and Manning, 2004) and our lexicalized extension of NDMV (L-NDMV). We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. In comparison, L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when it is enhanced with good model initialization. The performance of L-NDMV is competitive with the current state-of-the-art. 2 2.1 Methods Lexicalized DMV We choose to lexicalize an extended version of DMV (Gillenwater et al., 2010). We adopt a sim1683 Proceedings of the 2017 Conference on Empirical Methods in Natural Langua"
D17-1176,N15-1067,0,0.662554,"s. Second, we train the same neural network across EM iterations without resetting. More details can be found in the supplementary material. Our algorithm can be seen as an extension of online EM (Liang and Klein, 2009) to accommodate neural network training. 2.3 Model Initialization It was previously shown that the heuristic KM initialization method by Klein and Manning (2004) does not work well for lexicalized grammar induction (Headden III et al., 2009; Pate and Johnson, 2016) and it is very helpful to initialize learning with a model learned by a different grammar induction method (Le and Zuidema, 2015; Jiang et al., 2016). We tested both KM initialization and the following initialization method: we first learn 1684 an unlexicalized DMV using the grammar induction method of Naseem et al. (2010) and use it to parse the training corpus; then, from the parse trees we run maximum likelihood estimation to produce the initial lexicalized model. 3 Experimental Setup For English, we used the BLLIP corpus1 in addition to the regular WSJ corpus in our experiments. Note that the BLLIP corpus is collected from the same news article source as the WSJ corpus, so it is in-domain and is ideal for training"
D17-1176,N09-1069,0,0.034026,"Learning Algorithm: The original NDMV learning method is based on hard-EM and is very time-consuming when applied to L-NDMV with a large training corpus. We propose two improvements to achieve significant speedup. First, at each EM iteration we collect grammar rule counts from a different batch of sentences instead of from the whole training corpus and train the neural network using only these counts. Second, we train the same neural network across EM iterations without resetting. More details can be found in the supplementary material. Our algorithm can be seen as an extension of online EM (Liang and Klein, 2009) to accommodate neural network training. 2.3 Model Initialization It was previously shown that the heuristic KM initialization method by Klein and Manning (2004) does not work well for lexicalized grammar induction (Headden III et al., 2009; Pate and Johnson, 2016) and it is very helpful to initialize learning with a model learned by a different grammar induction method (Le and Zuidema, 2015; Jiang et al., 2016). We tested both KM initialization and the following initialization method: we first learn 1684 an unlexicalized DMV using the grammar induction method of Naseem et al. (2010) and use i"
D17-1176,P13-1028,0,0.0603737,"pported by the National Natural Science Foundation of China (61503248). full lexicalization is that the grammar becomes much larger and thus learning is more data demanding. To mitigate this problem, Headden et al. (2009) and Blunsom and Cohn (2010) used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Another straightforward way to mitigate the data scarcity problem of lexicalization is to use training corpora larger than the standard WSJ corpus. For example, Pate and Johnson (2016) used two large corpora containing more than 700k sentences; Marecek and Straka (2013) utilized a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar. Finally, smoothing techniques can be used to reduce the negative impact of data scarcity. One example is Neural DMV (NDMV) (Jiang et al., 2016) which incorporates neural networks into DMV and can automatically smooth correlated grammar rule probabilities. Inspired by this background, we conduct a systematic study regarding the impact of the degree of lexicalization and the training data size on the accuracy of grammar induction approaches. We experimented with a lexicalized version of Dependency M"
D17-1176,D10-1120,0,0.0494955,"ine EM (Liang and Klein, 2009) to accommodate neural network training. 2.3 Model Initialization It was previously shown that the heuristic KM initialization method by Klein and Manning (2004) does not work well for lexicalized grammar induction (Headden III et al., 2009; Pate and Johnson, 2016) and it is very helpful to initialize learning with a model learned by a different grammar induction method (Le and Zuidema, 2015; Jiang et al., 2016). We tested both KM initialization and the following initialization method: we first learn 1684 an unlexicalized DMV using the grammar induction method of Naseem et al. (2010) and use it to parse the training corpus; then, from the parse trees we run maximum likelihood estimation to produce the initial lexicalized model. 3 Experimental Setup For English, we used the BLLIP corpus1 in addition to the regular WSJ corpus in our experiments. Note that the BLLIP corpus is collected from the same news article source as the WSJ corpus, so it is in-domain and is ideal for training grammars to be evaluated on the WSJ test set. In order to solve the compatibility issue as well as improve the POS tagging accuracy, we used the Stanford tagger (Toutanova et al., 2003) to retag t"
D17-1176,C16-1003,0,0.488272,"ed sentences. In the most common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpus with sentences no longer than 10 words) containing no more than 6,000 training sentences (Cohen et al., 2008; Berg-Kirkpatrick et al., 2010; Tu and Honavar, 2012). Lexicalized grammar induction aims to incorporate lexical information into the learned grammar to increase its representational power and improve the learning accuracy. The most straightforward approach to encoding lexical information is full lexicalization (Pate and Johnson, 2016; Spitkovsky et al., 2013). A major problem with ∗ This work was supported by the National Natural Science Foundation of China (61503248). full lexicalization is that the grammar becomes much larger and thus learning is more data demanding. To mitigate this problem, Headden et al. (2009) and Blunsom and Cohn (2010) used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Another straightforward way to mitigate the data scarcity problem of lexicalization is to use training corpora larger than the standard WSJ corpus. For example, Pate and Johnson"
D17-1176,D13-1204,0,0.714584,"t common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpus with sentences no longer than 10 words) containing no more than 6,000 training sentences (Cohen et al., 2008; Berg-Kirkpatrick et al., 2010; Tu and Honavar, 2012). Lexicalized grammar induction aims to incorporate lexical information into the learned grammar to increase its representational power and improve the learning accuracy. The most straightforward approach to encoding lexical information is full lexicalization (Pate and Johnson, 2016; Spitkovsky et al., 2013). A major problem with ∗ This work was supported by the National Natural Science Foundation of China (61503248). full lexicalization is that the grammar becomes much larger and thus learning is more data demanding. To mitigate this problem, Headden et al. (2009) and Blunsom and Cohn (2010) used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Another straightforward way to mitigate the data scarcity problem of lexicalization is to use training corpora larger than the standard WSJ corpus. For example, Pate and Johnson (2016) used two large corp"
D17-1176,N03-1033,0,0.0149554,"ion method of Naseem et al. (2010) and use it to parse the training corpus; then, from the parse trees we run maximum likelihood estimation to produce the initial lexicalized model. 3 Experimental Setup For English, we used the BLLIP corpus1 in addition to the regular WSJ corpus in our experiments. Note that the BLLIP corpus is collected from the same news article source as the WSJ corpus, so it is in-domain and is ideal for training grammars to be evaluated on the WSJ test set. In order to solve the compatibility issue as well as improve the POS tagging accuracy, we used the Stanford tagger (Toutanova et al., 2003) to retag the BLLIP corpus and selected the sentences for which the new tags are consistent with the original tags, which resulted in 182244 sentences with length less than or equal to 10 after removing punctuations. We used this subset of BLLIP and section 2-21 of WSJ10 for training, section 22 of WSJ for validation and section 23 of WSJ for testing. We used training sets of four different sizes: WSJ10 only (5779 sentences) and 20k, 50k, and all sentences from the BLLIP subset. For Chinese, we obtained 4762 sentences for training from Chinese Treebank 6.0 (CTB) after converting data to depend"
D17-1176,D12-1121,1,0.898434,"g training data and lexicalization of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art. 1 Introduction Grammar induction is the task of learning a grammar from a set of unannotated sentences. In the most common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpus with sentences no longer than 10 words) containing no more than 6,000 training sentences (Cohen et al., 2008; Berg-Kirkpatrick et al., 2010; Tu and Honavar, 2012). Lexicalized grammar induction aims to incorporate lexical information into the learned grammar to increase its representational power and improve the learning accuracy. The most straightforward approach to encoding lexical information is full lexicalization (Pate and Johnson, 2016; Spitkovsky et al., 2013). A major problem with ∗ This work was supported by the National Natural Science Foundation of China (61503248). full lexicalization is that the grammar becomes much larger and thus learning is more data demanding. To mitigate this problem, Headden et al. (2009) and Blunsom and Cohn (2010)"
D17-1176,N10-1083,0,\N,Missing
D17-1177,P15-1133,0,0.345211,"ouraging correlations between POS tags (Cohen et al., 2008; Cohen and Smith, 2009; Berg-Kirkpatrick et al., 2010; Jiang et al., 2016), and limiting center embedding (Noji et al., 2016), can be incorporated into generative models to achieve better parsing accuracy. However, due to the strong independence assumption in most generative models, it is difficult for these models to utilize context information that has been shown to benefit supervised parsing. Recently, a feature-rich discriminative model for unsupervised parsing is proposed that captures the global context information of sentences (Grave and Elhadad, 2015). Inspired by discriminative clustering, learning of the model is formulated as convex optimization of both the model parameters and the parses of training sentences. By utilizing language-independent rules between pairs of POS tags to guide learning, the model achieves state-ofthe-art performance on the UD treebank dataset. In this paper we propose to jointly train two state-of-the-art models of unsupervised dependency parsing: a generative model called LC-DMV (Noji et al., 2016) and a discriminative model called Convex-MST (Grave and Elhadad, 2015). We employ a learning algorithm based on th"
D17-1177,P04-1061,0,0.757129,"ies dependencies between words in a sentence, which have been shown to benefit other tasks such as semantic role labeling (Lei et al., 2015) and sentence classification (Ma et al., 2015). Supervised learning of a dependency parser requires annotation of a training corpus by linguistic experts, which can be time and resource consuming. Unsupervised dependency parsing eliminates the need for dependency annotation by directly learning from unparsed text. Previous work on unsupervised dependency parsing mainly focuses on learning generative models, such as the dependency model with valence (DMV) (Klein and Manning, 2004) and combinatory categorial grammars (CCG) (Bisk and Hockenmaier, 2012). Generative models have ∗ This work was supported by the National Natural Science Foundation of China (61503248). many advantages. For example, the learning objective function can be defined as the marginal likelihood of the training data, which is typically easy to compute in a generative model. In addition, many types of inductive bias, such as those favoring short dependency arcs (Smith and Eisner, 2006), encouraging correlations between POS tags (Cohen et al., 2008; Cohen and Smith, 2009; Berg-Kirkpatrick et al., 2010;"
D17-1177,D10-1125,0,0.034125,"is a feature representation f (xα , i, j) of an edge in the dependency graph of sentence xα , v represents whether each dependency arc in yα satisfies a set of prespecified linguistic rules, and λ and µ are hyperparameters. The Frank-Wolfe algorithm is employed to optimize the objective function. 2.3 Dual Decomposition Dual decomposition (Dantzig and Wolfe, 1960), a special case of Lagrangian relaxation, is an optimization method that decomposes a hard problem into several small sub-problems. It has been widely used in machine learning (Komodakis et al., 2007) and natural language processing (Koo et al., 2010; Rush and Collins, 2012). Komodakis et al. (2007) proposed using dual decomposition to do MAP inference for Markov random fields. Koo et al. (2010) proposed a new dependency parser based on dual decomposition by combining a graph based dependency model and a non-projective head automata. In the work of Rush et al. (2010), they showed that dual decomposition can effectively integrate two lexicalized parsing models or two correlated tasks. 2.4 Agreement based Learning Liang et al. (2008) proposed agreement based learning that trains several tractable generative models jointly and encourages the"
D17-1177,N15-1121,0,0.0494344,"Missing"
D17-1177,P15-2029,0,0.0325205,"at learns a generative model and a discriminative model jointly based on the dual decomposition method. Our method is simple and general, yet effective to capture the advantages of both models and improve their learning results. We tested our method on the UD treebank and achieved a state-ofthe-art performance on thirty languages. 1 Introduction Dependency parsing is an important task in natural language processing. It identifies dependencies between words in a sentence, which have been shown to benefit other tasks such as semantic role labeling (Lei et al., 2015) and sentence classification (Ma et al., 2015). Supervised learning of a dependency parser requires annotation of a training corpus by linguistic experts, which can be time and resource consuming. Unsupervised dependency parsing eliminates the need for dependency annotation by directly learning from unparsed text. Previous work on unsupervised dependency parsing mainly focuses on learning generative models, such as the dependency model with valence (DMV) (Klein and Manning, 2004) and combinatory categorial grammars (CCG) (Bisk and Hockenmaier, 2012). Generative models have ∗ This work was supported by the National Natural Science Foundati"
D17-1177,P05-1012,0,0.243032,"parse tree is the probability product of all the rules used in the generation process. To learn the parameters (rule probabilities) of DMV, the expectation maximization algorithm is often used. Noji et al. (2016) exploited two universal syntactic biases in learning DMV: restricting the center-embedding depth and encouraging short dependencies. They achieved a comparable performance with state-of-the-art approaches. 2.2 Convex-MST Convex-MST (Grave and Elhadad, 2015) is a discriminative model for unsupervised dependency parsing based on the first-order maximum spanning tree dependency parser (McDonald et al., 2005). Given a sentence, whether each possible dependency exists or not is predicted based on a set of handcrafted features and a valid parse tree closest to the prediction is identified by the minimum spanning tree algorithm. For each sentence x, a first-order dependency graph is built over the words of the sentence. The weight of each edge is calculated by wT f (x, i, j), where w is the parameters and f (x, i, j) is the handcrafted feature vector of the dependency from the i-th word to the j-th word in sentence x. For sentence x of length n, we can represent it as matrix X where each raw is a fea"
D17-1177,D16-1004,0,0.185812,"Missing"
D17-1177,D10-1001,0,0.0345256,"Dual decomposition (Dantzig and Wolfe, 1960), a special case of Lagrangian relaxation, is an optimization method that decomposes a hard problem into several small sub-problems. It has been widely used in machine learning (Komodakis et al., 2007) and natural language processing (Koo et al., 2010; Rush and Collins, 2012). Komodakis et al. (2007) proposed using dual decomposition to do MAP inference for Markov random fields. Koo et al. (2010) proposed a new dependency parser based on dual decomposition by combining a graph based dependency model and a non-projective head automata. In the work of Rush et al. (2010), they showed that dual decomposition can effectively integrate two lexicalized parsing models or two correlated tasks. 2.4 Agreement based Learning Liang et al. (2008) proposed agreement based learning that trains several tractable generative models jointly and encourages them to agree on certain latent variables. To effectively train the system, a product EM algorithm was used. They showed that the joint model can perform better than each independent model on the accuracy or convergence speed. They also showed that the objective function of the work of Klein and Manning (2004) is a special c"
D17-1177,P06-1072,0,0.408224,"ed dependency parsing mainly focuses on learning generative models, such as the dependency model with valence (DMV) (Klein and Manning, 2004) and combinatory categorial grammars (CCG) (Bisk and Hockenmaier, 2012). Generative models have ∗ This work was supported by the National Natural Science Foundation of China (61503248). many advantages. For example, the learning objective function can be defined as the marginal likelihood of the training data, which is typically easy to compute in a generative model. In addition, many types of inductive bias, such as those favoring short dependency arcs (Smith and Eisner, 2006), encouraging correlations between POS tags (Cohen et al., 2008; Cohen and Smith, 2009; Berg-Kirkpatrick et al., 2010; Jiang et al., 2016), and limiting center embedding (Noji et al., 2016), can be incorporated into generative models to achieve better parsing accuracy. However, due to the strong independence assumption in most generative models, it is difficult for these models to utilize context information that has been shown to benefit supervised parsing. Recently, a feature-rich discriminative model for unsupervised parsing is proposed that captures the global context information of senten"
D17-1177,N10-1083,0,\N,Missing
D17-1177,N09-1009,0,\N,Missing
D17-1179,P16-1231,0,0.124857,"their parameters. Our experimental results over the Part-of-Speech (POS) tagging task on eight different languages, show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised scenarios. 1 Introduction The recent renaissance of deep learning has led to significant strides forward in several AI fields. In Natural Language Processing (NLP), characterized by highly structured tasks, promising results were obtained by models that combine deep learning methods with traditional structured learning algorithms (Chen and Manning, 2014; Durrett and Klein, 2015; Andor et al., 2016; Wiseman and Rush, 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (Andor et al., 2016; Peng and Dredze, 2016; Lam"
D17-1179,N16-1181,0,0.0256035,"r constituent parsing by replacing the forwardbackward algorithm with the inside-outside algorithm. All of these tasks can benefit from semisupervised learning algorithms.1 1 Our code and experimental set up will be available at https://github.com/cosmozhang/NCRF-AE 2 Related Work Neural networks were successfully applied to many NLP tasks, including tagging (Ma and Hovy, 2016; Mesnil et al., 2015; Lample et al., 2016), parsing (Chen and Manning, 2014), text generation (Sutskever et al., 2011), machine translation (Bahdanau et al., 2015), sentiment analysis (Kim, 2014) and question answering (Andreas et al., 2016). Most relevant to this work are structured prediction models capturing dependencies between decisions, either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni e"
D17-1179,D14-1082,0,0.0605891,"der and the decoder simultaneously by decoupling their parameters. Our experimental results over the Part-of-Speech (POS) tagging task on eight different languages, show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised scenarios. 1 Introduction The recent renaissance of deep learning has led to significant strides forward in several AI fields. In Natural Language Processing (NLP), characterized by highly structured tasks, promising results were obtained by models that combine deep learning methods with traditional structured learning algorithms (Chen and Manning, 2014; Durrett and Klein, 2015; Andor et al., 2016; Wiseman and Rush, 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (A"
D17-1179,P15-1030,0,0.117236,"ltaneously by decoupling their parameters. Our experimental results over the Part-of-Speech (POS) tagging task on eight different languages, show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised scenarios. 1 Introduction The recent renaissance of deep learning has led to significant strides forward in several AI fields. In Natural Language Processing (NLP), characterized by highly structured tasks, promising results were obtained by models that combine deep learning methods with traditional structured learning algorithms (Chen and Manning, 2014; Durrett and Klein, 2015; Andor et al., 2016; Wiseman and Rush, 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (Andor et al., 2016; Peng a"
D17-1179,P82-1020,0,0.71631,"Missing"
D17-1179,P16-1087,0,0.0298882,"g/NCRF-AE 2 Related Work Neural networks were successfully applied to many NLP tasks, including tagging (Ma and Hovy, 2016; Mesnil et al., 2015; Lample et al., 2016), parsing (Chen and Manning, 2014), text generation (Sutskever et al., 2011), machine translation (Bahdanau et al., 2015), sentiment analysis (Kim, 2014) and question answering (Andreas et al., 2016). Most relevant to this work are structured prediction models capturing dependencies between decisions, either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient s"
D17-1179,D14-1181,0,0.00281176,"tasks such as dependency parsing or constituent parsing by replacing the forwardbackward algorithm with the inside-outside algorithm. All of these tasks can benefit from semisupervised learning algorithms.1 1 Our code and experimental set up will be available at https://github.com/cosmozhang/NCRF-AE 2 Related Work Neural networks were successfully applied to many NLP tasks, including tagging (Ma and Hovy, 2016; Mesnil et al., 2015; Lample et al., 2016), parsing (Chen and Manning, 2014), text generation (Sutskever et al., 2011), machine translation (Bahdanau et al., 2015), sentiment analysis (Kim, 2014) and question answering (Andreas et al., 2016). Most relevant to this work are structured prediction models capturing dependencies between decisions, either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the H"
D17-1179,D16-1116,0,0.114307,"Missing"
D17-1179,N16-1030,0,0.366426,"016; Wiseman and Rush, 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (Andor et al., 2016; Peng and Dredze, 2016; Lample et al., 2016; Ma and Hovy, 2016; Durrett and Klein, 2015). Despite their promise, wider adoption of these algorithms for new structured prediction tasks can be difficult. Neural networks are notoriously susceptible to over-fitting unless large amounts of training data are available. This problem is exacerbated in the structured settings, as accounting for the dependencies between decisions requires even more data. Providing it through manual annotation is often a difficult labor-intensive task. In this paper we tackle this problem, and propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-"
D17-1179,N15-1144,0,0.36216,"as ˆ ), where Y a generative model, describing P (X|Y is the label. In our model, illustrated in Figure 1b, the encoder is a CRF model with neural networks 1701 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1701–1711 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics as its potential extractors, while the decoder is a generative model, trying to reconstruct the input. Our model carries the merit of autoencoders, which can exploit valuable information from unlabeled data. Recent works (Ammar et al., 2014; Lin et al., 2015) suggested using an autoencoder with a CRF model as an encoder in an unsupervised setting. We significantly expand on these works and suggest the following contributions: 1. We propose a unified model seamlessly accommodating both unlabeled and labeled data. While past work focused on unsupervised structured prediction, neglecting the discriminative power of such models, our model easily supports learning in both fully supervised and semisupervised settings. We developed a variation of the Expectation-Maximization (EM) algorithm, used for optimizing the encoder and the decoder of our model sim"
D17-1179,P16-1101,0,0.255363,", 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (Andor et al., 2016; Peng and Dredze, 2016; Lample et al., 2016; Ma and Hovy, 2016; Durrett and Klein, 2015). Despite their promise, wider adoption of these algorithms for new structured prediction tasks can be difficult. Neural networks are notoriously susceptible to over-fitting unless large amounts of training data are available. This problem is exacerbated in the structured settings, as accounting for the dependencies between decisions requires even more data. Providing it through manual annotation is often a difficult labor-intensive task. In this paper we tackle this problem, and propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-supervised learning"
D17-1179,D16-1028,0,0.0307705,"013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalized autoencoder (Figure 1a) tries to ˆ given the original input X, reconstru"
D17-1179,N06-1020,0,0.0692806,"2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unif"
D17-1179,D16-1031,0,0.0417379,"(Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalized autoencoder (Figure 1a) tries to ˆ given the original input X, reconstruct the input X ˆ aiming to maximize the log probability P (X|X) without knowing the latent variable Y explicitly. Since we focus"
D17-1179,P16-2025,0,0.0234859,", 2015; Andor et al., 2016; Wiseman and Rush, 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (Andor et al., 2016; Peng and Dredze, 2016; Lample et al., 2016; Ma and Hovy, 2016; Durrett and Klein, 2015). Despite their promise, wider adoption of these algorithms for new structured prediction tasks can be difficult. Neural networks are notoriously susceptible to over-fitting unless large amounts of training data are available. This problem is exacerbated in the structured settings, as accounting for the dependencies between decisions requires even more data. Providing it through manual annotation is often a difficult labor-intensive task. In this paper we tackle this problem, and propose an end-to-end neural CRF autoencoder (NCR"
D17-1179,D16-1137,0,0.0510587,"Missing"
D17-1179,P13-1045,0,0.0184407,"eration (Sutskever et al., 2011), machine translation (Bahdanau et al., 2015), sentiment analysis (Kim, 2014) and question answering (Andreas et al., 2016). Most relevant to this work are structured prediction models capturing dependencies between decisions, either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Mar"
D17-1179,N10-1116,0,0.0132529,"16; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalize"
D17-1179,W15-1511,0,0.0137207,"pproaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalized autoencoder (Figure 1a) tries to ˆ given the origi"
D17-1179,D10-1017,0,0.0806109,"or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalized autoencoder (Figure 1a)"
D17-1179,W16-5907,0,0.0258514,"2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalized autoencoder (Figure 1a) tries to ˆ given the original input X, reconstruct the input X ˆ aim"
D17-1179,N16-1027,0,0.0315798,"//github.com/cosmozhang/NCRF-AE 2 Related Work Neural networks were successfully applied to many NLP tasks, including tagging (Ma and Hovy, 2016; Mesnil et al., 2015; Lample et al., 2016), parsing (Chen and Manning, 2014), text generation (Sutskever et al., 2011), machine translation (Bahdanau et al., 2015), sentiment analysis (Kim, 2014) and question answering (Andreas et al., 2016). Most relevant to this work are structured prediction models capturing dependencies between decisions, either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficult"
D19-1148,P10-1131,0,0.559008,"sing. Supervised parsing requires manual labeling of gold parse trees, which is a very labor-intensive task. On the other hand, unsupervised parsing (a.k.a. grammar induction) does not require labeled data and can make use of large amounts of unlabeled data that are freely available. However, grammar induction is very challenging and its accuracy is still far below that of supervised parsing. To compensate the lack of supervision in grammar induction, some previous work considers multilingual grammar induction, i.e., simultaneously learning grammars of multiple languages (Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010; Liu et al., 2013). Existing multilingual approaches require external resources such as parallel corpora, word alignments, and linguistic phylogenetic trees. ∗ Yong Jiang contributed to this work when at ShanghaiTech University. Kewei Tu is the corresponding author. In this paper, we aim at bilingual grammar induction without external resource. We are motivated by our observation that learning the unsupervised Convex-MST model (Grave and Elhadad, 2015) on the English corpus and then directly applying it to parse other languages produces surprisingly good results (Table 1). From the table, we"
D19-1148,D17-1171,1,0.927176,"et of all possible dependency tree, w is the model parameter, X is the unlabeled training corpus, D is the measurement between the parse y and model prediction on sentence x, R(w) is the P regularization term of parameter w, O ∈ {min, } is an operator. Table 2 shows the choices of O, D and R for several widely used models. 2.2 Graph based Dependency Parsing In this paper, we focus on graph based dependency parsers, though we believe that our approaches can be generalized to other types of parsers. Previous work on unsupervised graph based dependency parsing utilizes the autoencoder structure (Cai et al., 2017) or the discriminative clustering techniques (Grave and Elhadad, 2015). Following (McDonald et al., 2005), we can use a discriminative model for dependency parsing with first order factorization such that the score of a dependency tree y is the sum of the scores of its dependency edges. The score of an edge from word h to word m , sw (x, h, m), can be computed as the inner product of a feature vector f (x, h, m) and a parameter vector w. The optimal dependency tree for sentence x be discovered in polynomial time (Eisner, 1996; McDonald et al., 2005). 3 Bilingual Knowledge Sharing Given non-par"
D19-1148,C96-1058,0,0.118802,"ph based dependency parsing utilizes the autoencoder structure (Cai et al., 2017) or the discriminative clustering techniques (Grave and Elhadad, 2015). Following (McDonald et al., 2005), we can use a discriminative model for dependency parsing with first order factorization such that the score of a dependency tree y is the sum of the scores of its dependency edges. The score of an edge from word h to word m , sw (x, h, m), can be computed as the inner product of a feature vector f (x, h, m) and a parameter vector w. The optimal dependency tree for sentence x be discovered in polynomial time (Eisner, 1996; McDonald et al., 2005). 3 Bilingual Knowledge Sharing Given non-parallel corpora of two languages Xs and Xt , our goal is to learn two models with parameters ws and wt for the two languages. The simplest learning objective function is, J(ws , wt ; Xs , Xt ) = J(ws ; Xs ) + J(wt ; Xt ) which contains no interaction between the two models. As suggested by our empirical observation in Table 1, the model of one language may provide a useful inductive bias in learning the model of another language. Note that given a sentence, a graph-based dependency parser has three levels of representations: th"
D19-1148,P15-1133,0,0.577695,"ious work considers multilingual grammar induction, i.e., simultaneously learning grammars of multiple languages (Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010; Liu et al., 2013). Existing multilingual approaches require external resources such as parallel corpora, word alignments, and linguistic phylogenetic trees. ∗ Yong Jiang contributed to this work when at ShanghaiTech University. Kewei Tu is the corresponding author. In this paper, we aim at bilingual grammar induction without external resource. We are motivated by our observation that learning the unsupervised Convex-MST model (Grave and Elhadad, 2015) on the English corpus and then directly applying it to parse other languages produces surprisingly good results (Table 1). From the table, we can see that even with this simplistic method (which we call direct transfer), the dependency accuracy on each language is often very close to the accuracy of the model specifically trained on the corpus of that language. For the Swedish language, the accuracy of direct transfer is even better than that of the specifically trained model. This surprising result suggests that grammars of different languages, even those from different language families (e."
D19-1148,P15-1119,0,0.0241976,"een previous work aiming at solving an unsupervised learning task of a target domain with the help of knowledge learned from a source domain (Dai et al., 2008; Wang et al., 2008; Pan and Yang, 2010). There is no labeled data in both the source and the target domains during training. Our transfer grammar induction setting can be seen as an instance of unsupervised transfer learning. Cross-lingual Supervised Dependency Parsing This task focuses on learning a parser with unlabeled training data and additional labeled training data of a second language (McDonald et al., 2011; Naseem et al., 2012; Guo et al., 2015). The main difference between our approach and theirs is that our approach is fully unsupervised. and do not utilize external information like word alignments or cross-lingual word embeddings. Other Approaches to Multilingual Grammar Induction To the best of our knowledge, this task is first proposed by Kuhn (2004). They assume that the syntax trees induced from parallel sentences share structured regularities and utilize the word alignments to guide parsing. From then on, many approaches are proposed on both constituency grammar induction and dependency grammar induction (Snyder et al., 2009;"
D19-1148,P19-1526,1,0.798037,"Missing"
D19-1148,D16-1073,1,0.89992,"Missing"
D19-1148,D17-1177,1,0.737516,"lhadad, 2015). Our three objective functions can be optimized with coordinate descent in a similar way to Convex-MST. In each iteration, we first fix parse Experiments To enable direct comparison with the ConvexMST model, we use the dataset used in their paper (Grave and Elhadad, 2015), the universal treebanks version 2.01 , introduced by McDonald et al. (2013). The dataset contains ten different languages, which belong to five diverse families. In additional, we test our methods on twelve languages from the more recent UD Treebank 1.42 , which is also used in previous grammar induction work (Jiang et al., 2017; Li et al., 2019). Following previous work, we train all the models on the gold POS tags of sentences no longer than ten. We tune hyper-parameters on the development dataset and report the DDA on sentences no longer than ten and all the sentences in the test dataset. As our goal is to investigate the benefits of our regularization methods, the two hyper-parameters µ and β 1 https://github.com/ryanmcd/ uni-dep-tb. The version is not consistent with recent releases of UD Treebanks. 2 http://universaldependencies.org/ 1425 CODE DE ES FR ID IT JA KO PTBR SV Avg Avg-All UD 1.4∗ C-MST D-T RAN W-R E"
D19-1148,D16-1139,0,0.0206843,"wT f (eat ! with) each sentence, which can be seen as a soft version of weight regularization. J(ws , wt ; Xs , Xt ) = J(ws ; Xs ) + J(wt ; Xt )+ X X ||sws (x, h, m) − swt (x, h, m)||22 λ y I x∈X 0 (h,m)∈G(x) eat sushi with Mary where X 0 = Xs ∪ Xt . G(x) is the weighted dependency graph of sentence x. Figure 1: Three levels of representations of the parser: the parameter w, the edge score s, and the parse y. Regularization on Parse Trees (T-Reg) Another alternative is to encourage similarity between the parse trees predicted by the two models. Motivated by the idea of knowledge distillation (Kim and Rush, 2016), in the learning objective of each model, we add a fourth term to encourage the parse tree to be close to the prediction of the other model. Below we show the objective function for ws . y for each training sentence and update parameters ws and wt by stochastic gradient descent; then we fix ws and wt and update y of each sentence by the Frank-Wolfe algorithm. While our three methods are applicable to any pair of languages, intuitively one may use weight regularization only for similar languages, and use edge regularization and tree regularization for an arbitrary language pair. 4 0 J (ws , wt"
D19-1148,P04-1061,0,0.481788,"Missing"
D19-1148,P04-1060,0,0.156059,"Missing"
D19-1148,D11-1006,0,0.0641094,"s Unsupervised Transfer Learning There has been previous work aiming at solving an unsupervised learning task of a target domain with the help of knowledge learned from a source domain (Dai et al., 2008; Wang et al., 2008; Pan and Yang, 2010). There is no labeled data in both the source and the target domains during training. Our transfer grammar induction setting can be seen as an instance of unsupervised transfer learning. Cross-lingual Supervised Dependency Parsing This task focuses on learning a parser with unlabeled training data and additional labeled training data of a second language (McDonald et al., 2011; Naseem et al., 2012; Guo et al., 2015). The main difference between our approach and theirs is that our approach is fully unsupervised. and do not utilize external information like word alignments or cross-lingual word embeddings. Other Approaches to Multilingual Grammar Induction To the best of our knowledge, this task is first proposed by Kuhn (2004). They assume that the syntax trees induced from parallel sentences share structured regularities and utilize the word alignments to guide parsing. From then on, many approaches are proposed on both constituency grammar induction and dependency"
D19-1148,P12-1066,0,0.0240409,"Learning There has been previous work aiming at solving an unsupervised learning task of a target domain with the help of knowledge learned from a source domain (Dai et al., 2008; Wang et al., 2008; Pan and Yang, 2010). There is no labeled data in both the source and the target domains during training. Our transfer grammar induction setting can be seen as an instance of unsupervised transfer learning. Cross-lingual Supervised Dependency Parsing This task focuses on learning a parser with unlabeled training data and additional labeled training data of a second language (McDonald et al., 2011; Naseem et al., 2012; Guo et al., 2015). The main difference between our approach and theirs is that our approach is fully unsupervised. and do not utilize external information like word alignments or cross-lingual word embeddings. Other Approaches to Multilingual Grammar Induction To the best of our knowledge, this task is first proposed by Kuhn (2004). They assume that the syntax trees induced from parallel sentences share structured regularities and utilize the word alignments to guide parsing. From then on, many approaches are proposed on both constituency grammar induction and dependency grammar induction (S"
D19-1148,D16-1004,0,0.262561,"Missing"
D19-1148,P09-1009,0,0.0271399,"tural language processing. Supervised parsing requires manual labeling of gold parse trees, which is a very labor-intensive task. On the other hand, unsupervised parsing (a.k.a. grammar induction) does not require labeled data and can make use of large amounts of unlabeled data that are freely available. However, grammar induction is very challenging and its accuracy is still far below that of supervised parsing. To compensate the lack of supervision in grammar induction, some previous work considers multilingual grammar induction, i.e., simultaneously learning grammars of multiple languages (Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010; Liu et al., 2013). Existing multilingual approaches require external resources such as parallel corpora, word alignments, and linguistic phylogenetic trees. ∗ Yong Jiang contributed to this work when at ShanghaiTech University. Kewei Tu is the corresponding author. In this paper, we aim at bilingual grammar induction without external resource. We are motivated by our observation that learning the unsupervised Convex-MST model (Grave and Elhadad, 2015) on the English corpus and then directly applying it to parse other languages produces surprisingly good resu"
D19-1148,P13-1105,0,0.301961,"Missing"
D19-1148,P05-1012,0,0.0385333,"is the measurement between the parse y and model prediction on sentence x, R(w) is the P regularization term of parameter w, O ∈ {min, } is an operator. Table 2 shows the choices of O, D and R for several widely used models. 2.2 Graph based Dependency Parsing In this paper, we focus on graph based dependency parsers, though we believe that our approaches can be generalized to other types of parsers. Previous work on unsupervised graph based dependency parsing utilizes the autoencoder structure (Cai et al., 2017) or the discriminative clustering techniques (Grave and Elhadad, 2015). Following (McDonald et al., 2005), we can use a discriminative model for dependency parsing with first order factorization such that the score of a dependency tree y is the sum of the scores of its dependency edges. The score of an edge from word h to word m , sw (x, h, m), can be computed as the inner product of a feature vector f (x, h, m) and a parameter vector w. The optimal dependency tree for sentence x be discovered in polynomial time (Eisner, 1996; McDonald et al., 2005). 3 Bilingual Knowledge Sharing Given non-parallel corpora of two languages Xs and Xt , our goal is to learn two models with parameters ws and wt for"
D19-1576,P10-1131,0,0.252007,". Intuitively, one can couple grammar parameters of different languages with similar typology and learn them simultaneously. However, the lacking of measures of language similarity prevents this idea from being further exploited in practice. Previous work in multilingual grammar induction either does not consider language similarity measures (Iwata et al., 2010) or models lan∗ The first and second authors contributed equally. The third author contributed to this work when at ShanghaiTech University. The fourth author is the corresponding author. guage similarity based on linguistic phylogeny (Berg-Kirkpatrick and Klein, 2010). The phylogenetic knowledge, however, could be misleading in measuring language similarity. For example, English and German are both Germanic languages, but English exhibits dominant SubjectVerb-Object (SVO) word order while German does not. In this paper, we propose a novel approach to multilingual grammar induction. Our induction model represents language identities as continuous vectors (i.e., language embeddings) and employs a neural network to predict the grammar parameters of each language based on its embedding. The neural network parameters are universally shared across languages, whi"
D19-1576,Q13-1007,0,0.493381,"monolingual and multilingual settings. For the monolingual setting we trained the baseline models on each language independently. For the multilingual setting we trained them on the combined training data of all the 15 languages and tested on one of the languages. Table 2 shows the experimental results. It can be seen that our multilingual grammar model (G) performs better on average than all the baselines. The improvement be2 We re-implemented the DMV and the NDMV. We set the ATTACH valence and DECISION valence to 2 and used root constraints, similar to previous work (Gimpel and Smith, 2012; Bisk and Hockenmaier, 2013; Noji et al., 2016). 5730 comes more significant when our model is jointly trained with the auxiliary language identification task (G+I). Note that our approach performs worse than the monolingual baseline on some languages, and we speculate that it is partly caused by data imbalance. In particular, the worst-performing Hindi language has only 4997 training sentences, much smaller than the average 7532. It would be interesting to make training more balanced by assigning weights to training samples of different languages, which we leave for future work. To measure the statistical significance"
D19-1576,N09-1009,0,0.14331,"the DECISION and ATTACH distributions PDECISION (dec|h, dir, val) and PATTACH (c|h, dir), where dir is a binary variable representing the direction of generation (left or right), val is a binary variable representing whether the current head token already has a child in the direction dir or not, dec is a binary variable deciding whether to continue generation in the current direction, c is the child token and h is the head token. Almost all previous methods of multilingual grammar induction are based on DMV. Their focus is on designing various priors to couple DMV parameters across languages: Cohen and Smith (2009) propose a logistic normal prior while BergKirkpatrick and Klein (2010) design a hierarchical Gaussian prior according to linguistic phylogeny. The usage of continuous language embeddings has been explored in other tasks. For example, Ammar et al. (2016) and de Lhoneux et al. (2018) apply language embeddings in supervised multilingual dependency parsing. 3 Approach We perform unlexicalized grammar induction in which a sentence is represented as a sequence of part-of-speech (POS) tags. We assume that all the languages share the same set of POS tags. 3.1 Multilingual Grammar Model Our multilingu"
D19-1576,N12-1069,0,0.616168,"re experimented in both monolingual and multilingual settings. For the monolingual setting we trained the baseline models on each language independently. For the multilingual setting we trained them on the combined training data of all the 15 languages and tested on one of the languages. Table 2 shows the experimental results. It can be seen that our multilingual grammar model (G) performs better on average than all the baselines. The improvement be2 We re-implemented the DMV and the NDMV. We set the ATTACH valence and DECISION valence to 2 and used root constraints, similar to previous work (Gimpel and Smith, 2012; Bisk and Hockenmaier, 2013; Noji et al., 2016). 5730 comes more significant when our model is jointly trained with the auxiliary language identification task (G+I). Note that our approach performs worse than the monolingual baseline on some languages, and we speculate that it is partly caused by data imbalance. In particular, the worst-performing Hindi language has only 4997 training sentences, much smaller than the average 7532. It would be interesting to make training more balanced by assigning weights to training samples of different languages, which we leave for future work. To measure t"
D19-1576,P15-1133,0,0.473457,"nt languages, which we leave for future work. To measure the statistical significance of the advantage of our method, we performed the nonparametric Friedman’s test to support/reject the claim (null hypothesis): there is no difference between the G+I model and the NDMV model in a multilingual setting. Based on the above sample data, the P-value 7.8911 × 10−4 would result in rejection of the claim at the 0.05 significance level, thus showing the significance in our performance gain. In Table 3 we compare our method with recent state-of-the-art approaches on the UD Treebank dataset: Convex-MST (Grave and Elhadad, 2015), LC-DMV (Noji et al., 2016) and D-J (Jiang et al., 2017). For the three approaches we use the results reported by Jiang et al. (2017). Our G+I model performs better than Convex-MST and LC-DMV on average, even though additional priors and delicate biases are integrated into the two methods (e.g, the universal linguistic prior for ConvexMST and the limited center-embedding for LCDMV). Our method also slightly outperforms D-J on average, even though D-J combines ConvexMST and LC-DMV and therefore utilizes even more linguistic prior knowledge. 5 Analysis 5.1 Visualization of Language Embeddings O"
D19-1576,P10-2034,0,0.0228726,"syntactic level in spite of their diversity on the surface, as many studies have revealed (Greenberg, 1963; Hawkins, 2014). This fact provides the basis for multilingual grammar induction which tries to simultaneously induce grammars of multiple languages. Intuitively, one can couple grammar parameters of different languages with similar typology and learn them simultaneously. However, the lacking of measures of language similarity prevents this idea from being further exploited in practice. Previous work in multilingual grammar induction either does not consider language similarity measures (Iwata et al., 2010) or models lan∗ The first and second authors contributed equally. The third author contributed to this work when at ShanghaiTech University. The fourth author is the corresponding author. guage similarity based on linguistic phylogeny (Berg-Kirkpatrick and Klein, 2010). The phylogenetic knowledge, however, could be misleading in measuring language similarity. For example, English and German are both Germanic languages, but English exhibits dominant SubjectVerb-Object (SVO) word order while German does not. In this paper, we propose a novel approach to multilingual grammar induction. Our induct"
D19-1576,D16-1073,1,0.883965,"hile BergKirkpatrick and Klein (2010) design a hierarchical Gaussian prior according to linguistic phylogeny. The usage of continuous language embeddings has been explored in other tasks. For example, Ammar et al. (2016) and de Lhoneux et al. (2018) apply language embeddings in supervised multilingual dependency parsing. 3 Approach We perform unlexicalized grammar induction in which a sentence is represented as a sequence of part-of-speech (POS) tags. We assume that all the languages share the same set of POS tags. 3.1 Multilingual Grammar Model Our multilingual grammar model adopts the NDMV (Jiang et al., 2016), a monolingual model, as the basic component. NDMV predicts grammar rule probabilities using neural networks. In our model, we add a continuous vector representation of the language identity l (i.e., a language embedding) as an additional input to the neural networks in NDMV. Specifically, to predict an ATTACH rule probability PATTACH (c|h, dir, val, l), we use a multilayer neural network that takes the embeddings of the head token h, valence val and language identity l as input, uses a weight matrix Wdir specific to the direction dir in the first layer, and uses a weight matrix Wc consisting"
D19-1576,D17-1177,1,0.889934,"del. 4 C ODE ET FI NL EN DE NO GRC HI JA FR IT LA BG SL EU Avg 4.1 Setup 1 Our code is available at https://github.com/ WinnieHAN/mndmv.git. Language Family Finnic Finnic Germanic Germanic Germanic Germanic Hellenic Indo-Irian Janponic Romance Romance Romance Slavonic Slavonic Vasconic Corpus Size 11404 9648 8783 7674 7447 10017 9387 4997 7441 4976 6492 10136 6507 3800 4271 Table 1: Languages and treebanks used in our experiments. Experiment We selected 15 languages across 8 language families and subfamilies to ensure diversity. To enable comparisons with previous state-of-the-art approaches (Jiang et al., 2017; Li et al., 2019), we conducted our experiments on UD Treebank 1.4. For each language, we show its language family and the training corpus size in Table 1. We trained our method on the training sentences with length ≤ 15 and tested our method on the testing sentences with length ≤ 40 after removing all punctuations. Since we are doing unsupervised learning, gold dependency trees were not used during training. We use the directed dependency accuracy (DDA, the percentage of words in the testing dataset which are assigned the correct head, same to the unlabeled attachment score normally used in"
D19-1576,P04-1061,0,0.354887,"parameters are trained with a standard grammar induction objective without any guidance from prior linguistic phylogenetic knowledge. We also introduce an auxiliary language identification task in which we predict the language identities of input sentences using the language embeddings. We evaluate our approach on corpora of 15 languages across 8 language families and subfamilies. We observe that our approach achieves substantial performance gain on average over monolingual and multilingual baselines. 2 Dependency Model with Valence and Other Related Works Dependency Model with Valence (DMV) (Klein and Manning, 2004) is the best known generative model for dependency grammar induction. The DMV generates a sentence and its dependency tree following three types of grammar rules (ATTACH, DECISION and ROOT). It firstly samples a token c from the ROOT distribution PROOT (c) 5728 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5728–5733, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Auxiliary Language Identiﬁcation Model (I) 0.6 and then recursively deci"
D19-1576,D18-1543,0,0.0658073,"Missing"
D19-1576,D16-1004,0,0.404094,"Missing"
K19-2005,P13-1023,0,0.0273462,"s five different frameworks of graph-based meaning representation. DELPH-IN MRS Bi-Lexical Dependencies (DM) (Ivanova et al., 2012) and Prague Semantic Dependencies (PSD) (Hajiˇc et al., 2012; Miyao et al., 2014) first appeared in SemEval 2014 and 2015 shared task Semantic Dependency Parsing (SDP) (Oepen et al., 2014, 2015). Elementary Dependency Structures (EDS) (Oepen and Lønning, 2006) is the origin of DM Bi-Lexical Dependencies, which encodes English Resource Semantics (Flickinger et al., 2016) in a variable-free semantic dependency graph. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013) targets a level of semantic granularity that abstracts away from syntactic paraphrases. Abstract Meaning Representation (AMR) (Banarescu et al., 2013) targets to abstract away from syntactic representations, which means that sentences have similar meaning should be assigned the same AMR graph. One of the main differences beIn this paper, we present our system for MRP 2019. Our system is a graph-based method which combines an extended pointer-generator network introduced by Zhang et al. (2019) to generate nodes for EDS, UCCA and AMR graphs and a second-order mean field variational inference mo"
K19-2005,W13-2322,0,0.0603808,"pendencies (PSD) (Hajiˇc et al., 2012; Miyao et al., 2014) first appeared in SemEval 2014 and 2015 shared task Semantic Dependency Parsing (SDP) (Oepen et al., 2014, 2015). Elementary Dependency Structures (EDS) (Oepen and Lønning, 2006) is the origin of DM Bi-Lexical Dependencies, which encodes English Resource Semantics (Flickinger et al., 2016) in a variable-free semantic dependency graph. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013) targets a level of semantic granularity that abstracts away from syntactic paraphrases. Abstract Meaning Representation (AMR) (Banarescu et al., 2013) targets to abstract away from syntactic representations, which means that sentences have similar meaning should be assigned the same AMR graph. One of the main differences beIn this paper, we present our system for MRP 2019. Our system is a graph-based method which combines an extended pointer-generator network introduced by Zhang et al. (2019) to generate nodes for EDS, UCCA and AMR graphs and a second-order mean field variational inference module introduced by Wang et al. (2019) to predict edges for all the frameworks. According to the official results, our system gets 94.88 F1 score in the"
K19-2005,P18-1035,0,0.0847419,"variational inference module that predicts edges. Our system achieved 1st and 2nd place for the DM and PSD frameworks respectively on the inframework ranks and achieved 3rd place for the DM framework on the cross-framework ranks. 1 Previous work mostly focused on developing parsers that support only one or two frameworks while few work has explored cross-framework semantic parsing. Peng et al. (2017), Stanovsky and Dagan (2018) and Kurita and Søgaard (2019) proposed methods learning jointly on the three frameworks of SDP and Peng et al. (2018) further proposed to learn from different corpora. Hershcovich et al. (2018) converted UCCA, AMR, DM and UD (Universal Dependencies) into a unified DAG format and proposed a transition-based method for UCCA parsing. Introduction The goal of the Cross-Framework Meaning Representation Parsing (MRP 2019, Oepen et al. (2019)) is learning to parse text to multiple formats of meaning representation with a uniform parsing system. The task combines five different frameworks of graph-based meaning representation. DELPH-IN MRS Bi-Lexical Dependencies (DM) (Ivanova et al., 2012) and Prague Semantic Dependencies (PSD) (Hajiˇc et al., 2012; Miyao et al., 2014) first appeared in Se"
K19-2005,W12-3602,0,0.0555555,"three frameworks of SDP and Peng et al. (2018) further proposed to learn from different corpora. Hershcovich et al. (2018) converted UCCA, AMR, DM and UD (Universal Dependencies) into a unified DAG format and proposed a transition-based method for UCCA parsing. Introduction The goal of the Cross-Framework Meaning Representation Parsing (MRP 2019, Oepen et al. (2019)) is learning to parse text to multiple formats of meaning representation with a uniform parsing system. The task combines five different frameworks of graph-based meaning representation. DELPH-IN MRS Bi-Lexical Dependencies (DM) (Ivanova et al., 2012) and Prague Semantic Dependencies (PSD) (Hajiˇc et al., 2012; Miyao et al., 2014) first appeared in SemEval 2014 and 2015 shared task Semantic Dependency Parsing (SDP) (Oepen et al., 2014, 2015). Elementary Dependency Structures (EDS) (Oepen and Lønning, 2006) is the origin of DM Bi-Lexical Dependencies, which encodes English Resource Semantics (Flickinger et al., 2016) in a variable-free semantic dependency graph. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013) targets a level of semantic granularity that abstracts away from syntactic paraphrases. Abstract Meaning"
K19-2005,P13-2131,0,0.0681961,"rmance dropped when ELMo embedding and BERT embedding are combined. We speculate that the drop is caused by the conflict between the two types of contextual information. For subtoken pooling, we compared the performance of using first subtoken pooling and average pooling as token embedding. We found that average pooling is slightly better than Smatch 63.08 71.55 Table 6: MRP and Smatch score on the development set and the test set. From the official results on the test sets, we find it surprising that there is a huge gap between the test and development results on both the MRP and the Smatch (Cai and Knight, 2013) scores, as shown in Table 6. In future work, we will figure out the reason behind this problem. EDS For EDS, our parser ranks 5th . There are multiple details of our parser that can be improved. For example, our anchor prediction module described 62 Baseline Base-fixed Base-tuned Base-fixed + Glove Base-tuned + Glove Large-fixed + Glove Large-tuned + Glove Large-fixed + Glove + Lemma Large-fixed + Glove + Lemma + Char ELMo + Large-fixed + Glove + Lemma ELMo + Glove + Lemma + Char BERT-First BERT-Avg BERT-Avg + dep-tree LF1 93.41 94.17 94.22 94.45 94.48 94.62 94.64 95.10 95.22 94.78 95.06 95.2"
K19-2005,N19-1423,0,0.0434171,"on where f l is the l-th layer of LSTM, zl0 is the last hidden state rn in Eq. 1. z0t is the concatenation of the label embedding of node ut−1 and attentional vector e zt−1 . e zt is defined by: Previous work found that various word representation could help improve parser performance. Many state-of-the-art parsers use POS tags and pre-trained GloVe (Pennington et al., 2014) embeddings as a part of the word representation. Dozat and Manning (2018) find that characterbased LSTM and lemma embeddings can further improve the performance of semantic dependency parser. Zhang et al. (2019) use BERT (Devlin et al., 2019) embeddings for each token to improve the performance of AMR parsing. In our system, ⊤ etsrc =Wsatt tanh(Wsrc R + Usrc zlt + bsrc ) (2) atsrc =softmax(etsrc ) n X ct = atsrc,i ri (3) i 57 e zt =tanh(Wc [ct ; zlt ] + bc ) (4) Graph Output Edge Prediction Property/Anchor/Attribute Prediction ~ �1,…,� �1,…,� �1,…,� Node Prediction N Y Y DM/PSD? �1,…,� Word Embedding Figure 3: Illustration of our system architecture. where idxj is the copied node index. If ut is a new node: X P (node) (ut ) = pgen Pvocab (ut ) + psrc atsrc [i] Where atsrc is the source attention distribution, and ct is contextual"
K19-2005,P19-1232,0,0.0118727,"g Representation Parsing. Our system is a graph-based parser which combines an extended pointergenerator network that generates nodes and a second-order mean field variational inference module that predicts edges. Our system achieved 1st and 2nd place for the DM and PSD frameworks respectively on the inframework ranks and achieved 3rd place for the DM framework on the cross-framework ranks. 1 Previous work mostly focused on developing parsers that support only one or two frameworks while few work has explored cross-framework semantic parsing. Peng et al. (2017), Stanovsky and Dagan (2018) and Kurita and Søgaard (2019) proposed methods learning jointly on the three frameworks of SDP and Peng et al. (2018) further proposed to learn from different corpora. Hershcovich et al. (2018) converted UCCA, AMR, DM and UD (Universal Dependencies) into a unified DAG format and proposed a transition-based method for UCCA parsing. Introduction The goal of the Cross-Framework Meaning Representation Parsing (MRP 2019, Oepen et al. (2019)) is learning to parse text to multiple formats of meaning representation with a uniform parsing system. The task combines five different frameworks of graph-based meaning representation. DE"
K19-2005,S14-2082,0,0.0645023,"Missing"
K19-2005,P18-2077,0,0.0117138,"om the BiLSTM. For the decoder, at each time step t, we use an l-layer LSTM for generating hidden states ztl sequentially: l zlt = f l (zl−1 t , zt−1 ) 3.1 Word Representation where f l is the l-th layer of LSTM, zl0 is the last hidden state rn in Eq. 1. z0t is the concatenation of the label embedding of node ut−1 and attentional vector e zt−1 . e zt is defined by: Previous work found that various word representation could help improve parser performance. Many state-of-the-art parsers use POS tags and pre-trained GloVe (Pennington et al., 2014) embeddings as a part of the word representation. Dozat and Manning (2018) find that characterbased LSTM and lemma embeddings can further improve the performance of semantic dependency parser. Zhang et al. (2019) use BERT (Devlin et al., 2019) embeddings for each token to improve the performance of AMR parsing. In our system, ⊤ etsrc =Wsatt tanh(Wsrc R + Usrc zlt + bsrc ) (2) atsrc =softmax(etsrc ) n X ct = atsrc,i ri (3) i 57 e zt =tanh(Wc [ct ; zlt ] + bc ) (4) Graph Output Edge Prediction Property/Anchor/Attribute Prediction ~ �1,…,� �1,…,� �1,…,� Node Prediction N Y Y DM/PSD? �1,…,� Word Embedding Figure 3: Illustration of our system architecture. where idxj is"
K19-2005,S14-2056,0,0.388029,"erent corpora. Hershcovich et al. (2018) converted UCCA, AMR, DM and UD (Universal Dependencies) into a unified DAG format and proposed a transition-based method for UCCA parsing. Introduction The goal of the Cross-Framework Meaning Representation Parsing (MRP 2019, Oepen et al. (2019)) is learning to parse text to multiple formats of meaning representation with a uniform parsing system. The task combines five different frameworks of graph-based meaning representation. DELPH-IN MRS Bi-Lexical Dependencies (DM) (Ivanova et al., 2012) and Prague Semantic Dependencies (PSD) (Hajiˇc et al., 2012; Miyao et al., 2014) first appeared in SemEval 2014 and 2015 shared task Semantic Dependency Parsing (SDP) (Oepen et al., 2014, 2015). Elementary Dependency Structures (EDS) (Oepen and Lønning, 2006) is the origin of DM Bi-Lexical Dependencies, which encodes English Resource Semantics (Flickinger et al., 2016) in a variable-free semantic dependency graph. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013) targets a level of semantic granularity that abstracts away from syntactic paraphrases. Abstract Meaning Representation (AMR) (Banarescu et al., 2013) targets to abstract away from synt"
K19-2005,S15-2154,0,0.343692,"Missing"
K19-2005,K19-2001,0,0.0374429,"Missing"
K19-2005,N16-4001,0,0.0195081,") is learning to parse text to multiple formats of meaning representation with a uniform parsing system. The task combines five different frameworks of graph-based meaning representation. DELPH-IN MRS Bi-Lexical Dependencies (DM) (Ivanova et al., 2012) and Prague Semantic Dependencies (PSD) (Hajiˇc et al., 2012; Miyao et al., 2014) first appeared in SemEval 2014 and 2015 shared task Semantic Dependency Parsing (SDP) (Oepen et al., 2014, 2015). Elementary Dependency Structures (EDS) (Oepen and Lønning, 2006) is the origin of DM Bi-Lexical Dependencies, which encodes English Resource Semantics (Flickinger et al., 2016) in a variable-free semantic dependency graph. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013) targets a level of semantic granularity that abstracts away from syntactic paraphrases. Abstract Meaning Representation (AMR) (Banarescu et al., 2013) targets to abstract away from syntactic representations, which means that sentences have similar meaning should be assigned the same AMR graph. One of the main differences beIn this paper, we present our system for MRP 2019. Our system is a graph-based method which combines an extended pointer-generator network introduced b"
K19-2005,S15-2153,0,0.444846,"Missing"
K19-2005,hajic-etal-2012-announcing,0,0.133118,"Missing"
K19-2005,oepen-lonning-2006-discriminant,0,0.037937,"sing. Introduction The goal of the Cross-Framework Meaning Representation Parsing (MRP 2019, Oepen et al. (2019)) is learning to parse text to multiple formats of meaning representation with a uniform parsing system. The task combines five different frameworks of graph-based meaning representation. DELPH-IN MRS Bi-Lexical Dependencies (DM) (Ivanova et al., 2012) and Prague Semantic Dependencies (PSD) (Hajiˇc et al., 2012; Miyao et al., 2014) first appeared in SemEval 2014 and 2015 shared task Semantic Dependency Parsing (SDP) (Oepen et al., 2014, 2015). Elementary Dependency Structures (EDS) (Oepen and Lønning, 2006) is the origin of DM Bi-Lexical Dependencies, which encodes English Resource Semantics (Flickinger et al., 2016) in a variable-free semantic dependency graph. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013) targets a level of semantic granularity that abstracts away from syntactic paraphrases. Abstract Meaning Representation (AMR) (Banarescu et al., 2013) targets to abstract away from syntactic representations, which means that sentences have similar meaning should be assigned the same AMR graph. One of the main differences beIn this paper, we present our system fo"
K19-2005,P17-1186,0,0.047244,"Missing"
K19-2005,N18-1135,0,0.10686,"generator network that generates nodes and a second-order mean field variational inference module that predicts edges. Our system achieved 1st and 2nd place for the DM and PSD frameworks respectively on the inframework ranks and achieved 3rd place for the DM framework on the cross-framework ranks. 1 Previous work mostly focused on developing parsers that support only one or two frameworks while few work has explored cross-framework semantic parsing. Peng et al. (2017), Stanovsky and Dagan (2018) and Kurita and Søgaard (2019) proposed methods learning jointly on the three frameworks of SDP and Peng et al. (2018) further proposed to learn from different corpora. Hershcovich et al. (2018) converted UCCA, AMR, DM and UD (Universal Dependencies) into a unified DAG format and proposed a transition-based method for UCCA parsing. Introduction The goal of the Cross-Framework Meaning Representation Parsing (MRP 2019, Oepen et al. (2019)) is learning to parse text to multiple formats of meaning representation with a uniform parsing system. The task combines five different frameworks of graph-based meaning representation. DELPH-IN MRS Bi-Lexical Dependencies (DM) (Ivanova et al., 2012) and Prague Semantic Depen"
K19-2005,D14-1162,0,0.0816589,"of embeddings for wi , and R = [r1 , . . . , rn ] represents the output from the BiLSTM. For the decoder, at each time step t, we use an l-layer LSTM for generating hidden states ztl sequentially: l zlt = f l (zl−1 t , zt−1 ) 3.1 Word Representation where f l is the l-th layer of LSTM, zl0 is the last hidden state rn in Eq. 1. z0t is the concatenation of the label embedding of node ut−1 and attentional vector e zt−1 . e zt is defined by: Previous work found that various word representation could help improve parser performance. Many state-of-the-art parsers use POS tags and pre-trained GloVe (Pennington et al., 2014) embeddings as a part of the word representation. Dozat and Manning (2018) find that characterbased LSTM and lemma embeddings can further improve the performance of semantic dependency parser. Zhang et al. (2019) use BERT (Devlin et al., 2019) embeddings for each token to improve the performance of AMR parsing. In our system, ⊤ etsrc =Wsatt tanh(Wsrc R + Usrc zlt + bsrc ) (2) atsrc =softmax(etsrc ) n X ct = atsrc,i ri (3) i 57 e zt =tanh(Wc [ct ; zlt ] + bc ) (4) Graph Output Edge Prediction Property/Anchor/Attribute Prediction ~ �1,…,� �1,…,� �1,…,� Node Prediction N Y Y DM/PSD? �1,…,� Word E"
K19-2005,N18-1202,0,0.0140609,"the same reversed start-end anchor predictions, which prevents us from obtaining an MRP score. Smatch 69.1 69.3 4.4 Ablation Study BERT with Other Embeddings Table 5: Smatch F1 score on AMR development set. We compare the results without post-processing. Set test dev MRP 63.59 72.03 We use BERT (Devlin et al., 2019) embedding in our model. We compared the performance of DM in the original SDP dataset with different subtoken pooling methods, and we also explored whether combining other embeddings such as pre-trained word embedding Glove (Pennington et al., 2014) and contextual embedding ELMo (Peters et al., 2018) will further improve the performance. The detailed results are shown in table 7. We found that Glove, lemma and character embeddings are helpful for DM and fine-tuning on the training set slightly improves the performance. ELMo embedding is also helpful but cannot outperform BERT embedding. However, the performance dropped when ELMo embedding and BERT embedding are combined. We speculate that the drop is caused by the conflict between the two types of contextual information. For subtoken pooling, we compared the performance of using first subtoken pooling and average pooling as token embeddin"
K19-2005,W09-1119,0,0.0482487,"Figure 2: An example of EDS reduction. This is a subgraph of sentence #20001001. 2 NER tags to these entities is built to process the test data. In post-processing, we generate the AMR subgraphs from the anonymized words. Then we assign the senses, wiki links and polarity attributes with the method in Zhang et al. (2019). Data Processing In this section, we introduce our data preprocessing and post-processing in our system for all the frameworks. We use sentence tokenizations, POS tags and lemmas from the official companion data and named entity tags extracted by Illinois Named Entity Tagger (Ratinov and Roth, 2009) in the official ‘white-list’. We follow Zhang et al. (2019) to convert each EDS, UCCA, and AMR graph to a tree through duplicating the nodes that have multiple edge entrances, An example is shown in Fig. 1. The node sequences for EDS, UCCA and AMR are decided by depth-first search that starts from the root node and sorts neighbouring nodes in alphanumerical order. 2.2 EDS and UCCA Data Processing In pre-processing we first clean the companion data to make sure the tokens in the companion data is consistent with those in the MRP input. We suppose anchors are continuous for each node, so we rep"
K19-2005,D18-1263,0,0.0116584,"ed task: Cross-Framework Meaning Representation Parsing. Our system is a graph-based parser which combines an extended pointergenerator network that generates nodes and a second-order mean field variational inference module that predicts edges. Our system achieved 1st and 2nd place for the DM and PSD frameworks respectively on the inframework ranks and achieved 3rd place for the DM framework on the cross-framework ranks. 1 Previous work mostly focused on developing parsers that support only one or two frameworks while few work has explored cross-framework semantic parsing. Peng et al. (2017), Stanovsky and Dagan (2018) and Kurita and Søgaard (2019) proposed methods learning jointly on the three frameworks of SDP and Peng et al. (2018) further proposed to learn from different corpora. Hershcovich et al. (2018) converted UCCA, AMR, DM and UD (Universal Dependencies) into a unified DAG format and proposed a transition-based method for UCCA parsing. Introduction The goal of the Cross-Framework Meaning Representation Parsing (MRP 2019, Oepen et al. (2019)) is learning to parse text to multiple formats of meaning representation with a uniform parsing system. The task combines five different frameworks of graph-ba"
K19-2005,P19-1454,1,0.580875,"semantic granularity that abstracts away from syntactic paraphrases. Abstract Meaning Representation (AMR) (Banarescu et al., 2013) targets to abstract away from syntactic representations, which means that sentences have similar meaning should be assigned the same AMR graph. One of the main differences beIn this paper, we present our system for MRP 2019. Our system is a graph-based method which combines an extended pointer-generator network introduced by Zhang et al. (2019) to generate nodes for EDS, UCCA and AMR graphs and a second-order mean field variational inference module introduced by Wang et al. (2019) to predict edges for all the frameworks. According to the official results, our system gets 94.88 F1 score in the cross-framework metric for DM, which is the 3rd place in the ranking. For in-framework metrics, our system gets 92.98 and 81.61 labeled F1 score for DM and PSD respectively, which are ranked 1st and 2nd in the ranking. 55 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 55–65 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2005 proper_q 〈0:28〉 resilient time compo"
P18-1109,J03-4003,0,0.0833331,"tension of the inside-outside algorithm, which enables efficient inference and learning. We apply GM-LVeGs to part-of-speech tagging and constituency parsing and show that GM-LVeGs can achieve competitive accuracies. Our code is available at https://github.com/zhaoyanpeng/lveg. 1 Introduction In constituency parsing, refining coarse syntactic categories of treebank grammars (Charniak, 1996) into fine-grained subtypes has been proven effective in improving parsing results. Previous approaches to refining syntactic categories use tree annotations (Johnson, 1998), lexicalization (Charniak, 2000; Collins, 2003), or linguistically motivated category splitting (Klein and Manning, 2003). Matsuzaki et al. (2005) introduce latent variable grammars, in which each syntactic category (represented by a nonterminal) is split into a fixed number of subtypes and a discrete latent variable is used to indicate the subtype of the nonterminal when it appears in a specific parse tree. Since the latent variables are not observable in treebanks, the grammar is learned using expectation-maximization. Petrov et al. (2006) present a split-merge approach to learning latent variable grammars, which hierarchically splits ea"
P18-1109,P15-1030,0,0.0155501,"on of the inside-outside algorithm. This makes it possible to efficiently compute the gradients during discriminative learning of GM-LVeGs. We evaluate GM-LVeGs on part-of-speech tagging and constituency parsing on a variety of languages and corpora and show that GM-LVeGs achieve competitive results. It shall be noted that many modern state-ofthe-art constituency parsers predict how likely a constituent is based on not only local information (such as the production rules used in composing the constituent), but also contextual information of the constituent. For example, the neural CRF parser (Durrett and Klein, 2015) looks at the words before and after the constituent; and RNNG (Dyer et al., 2016) looks at the constituents that are already predicted (in the stack) and the words that are not processed (in the buffer). In this paper, however, we choose to focus on the basic framework and algorithms of LVeGs and leave the incorporation of contextual information for future work. We believe that by laying a solid foundation for LVeGs, our work can pave the way for many interesting extensions of LVeGs in the future. 2 Latent Vector Grammars A latent vector grammar (LVeG) considers subtypes of nonterminals as co"
P18-1109,N16-1024,0,0.024712,"adients during discriminative learning of GM-LVeGs. We evaluate GM-LVeGs on part-of-speech tagging and constituency parsing on a variety of languages and corpora and show that GM-LVeGs achieve competitive results. It shall be noted that many modern state-ofthe-art constituency parsers predict how likely a constituent is based on not only local information (such as the production rules used in composing the constituent), but also contextual information of the constituent. For example, the neural CRF parser (Durrett and Klein, 2015) looks at the words before and after the constituent; and RNNG (Dyer et al., 2016) looks at the constituents that are already predicted (in the stack) and the words that are not processed (in the buffer). In this paper, however, we choose to focus on the basic framework and algorithms of LVeGs and leave the incorporation of contextual information for future work. We believe that by laying a solid foundation for LVeGs, our work can pave the way for many interesting extensions of LVeGs in the future. 2 Latent Vector Grammars A latent vector grammar (LVeG) considers subtypes of nonterminals as continuous vectors and associates each nonterminal with a latent vector space repres"
P18-1109,J98-4004,0,0.478627,"ons of subtype rules can be computed using an extension of the inside-outside algorithm, which enables efficient inference and learning. We apply GM-LVeGs to part-of-speech tagging and constituency parsing and show that GM-LVeGs can achieve competitive accuracies. Our code is available at https://github.com/zhaoyanpeng/lveg. 1 Introduction In constituency parsing, refining coarse syntactic categories of treebank grammars (Charniak, 1996) into fine-grained subtypes has been proven effective in improving parsing results. Previous approaches to refining syntactic categories use tree annotations (Johnson, 1998), lexicalization (Charniak, 2000; Collins, 2003), or linguistically motivated category splitting (Klein and Manning, 2003). Matsuzaki et al. (2005) introduce latent variable grammars, in which each syntactic category (represented by a nonterminal) is split into a fixed number of subtypes and a discrete latent variable is used to indicate the subtype of the nonterminal when it appears in a specific parse tree. Since the latent variables are not observable in treebanks, the grammar is learned using expectation-maximization. Petrov et al. (2006) present a split-merge approach to learning latent v"
P18-1109,P03-1054,0,0.13973,"inference and learning. We apply GM-LVeGs to part-of-speech tagging and constituency parsing and show that GM-LVeGs can achieve competitive accuracies. Our code is available at https://github.com/zhaoyanpeng/lveg. 1 Introduction In constituency parsing, refining coarse syntactic categories of treebank grammars (Charniak, 1996) into fine-grained subtypes has been proven effective in improving parsing results. Previous approaches to refining syntactic categories use tree annotations (Johnson, 1998), lexicalization (Charniak, 2000; Collins, 2003), or linguistically motivated category splitting (Klein and Manning, 2003). Matsuzaki et al. (2005) introduce latent variable grammars, in which each syntactic category (represented by a nonterminal) is split into a fixed number of subtypes and a discrete latent variable is used to indicate the subtype of the nonterminal when it appears in a specific parse tree. Since the latent variables are not observable in treebanks, the grammar is learned using expectation-maximization. Petrov et al. (2006) present a split-merge approach to learning latent variable grammars, which hierarchically splits each nonterminal and merges ineffective splits. Petrov and Klein (2008b) fur"
P18-1109,P05-1010,0,0.766689,"Missing"
P18-1109,L16-1262,0,0.0363656,"Missing"
P18-1109,P06-1055,0,0.877491,"es to refining syntactic categories use tree annotations (Johnson, 1998), lexicalization (Charniak, 2000; Collins, 2003), or linguistically motivated category splitting (Klein and Manning, 2003). Matsuzaki et al. (2005) introduce latent variable grammars, in which each syntactic category (represented by a nonterminal) is split into a fixed number of subtypes and a discrete latent variable is used to indicate the subtype of the nonterminal when it appears in a specific parse tree. Since the latent variables are not observable in treebanks, the grammar is learned using expectation-maximization. Petrov et al. (2006) present a split-merge approach to learning latent variable grammars, which hierarchically splits each nonterminal and merges ineffective splits. Petrov and Klein (2008b) further allow a nonterminal to have different splits in different production rules, which results in a more compact grammar. Recently, neural approaches become very popular in natural language processing (NLP). An important technique in neural approaches to NLP is to represent discrete symbols such as words and syntactic categories with continuous vectors or embeddings. Since the distances between such vector representations"
P18-1109,N07-1051,0,0.471396,"e vector a. Next, using Equation 8 in Table 1, we calculate the score s(A  BC, i, k, j) (1 ≤ i ≤ k &lt; j ≤ n), where hA  BC, i, k, ji represents a production rule A  BC with nonterminals A, B, and C spanning words wi:j , wi,k , and wk+1:j respectively in the sentence w1:n . Then the expected count (or posterior) of hA  BC, i, k, ji is calculated as: q(A  BC, i, k, j) = s(A  BC, i, k, j) , sI (S, 1, n) (9) where sI (S, 1, n) is the inside score for the start symbol S spanning the whole sentence w1:n . After calculating all the expected counts, we can use the M AX -RULE -P RODUCT algorithm (Petrov and Klein, 2007) for parsing, which returns a parse with the highest probability that all the production rules are correct. Its objective function is given by Y q(e) , (10) Tq∗ = argmax T ∈G(w) e∈T Parsing The goal of parsing is to find the most probable parse tree T ∗ with unrefined nonterminals for a sentence w of n words w1:n = w1 . . . wn . This is formally defined as: T ∗ = argmax P (T |w) , (5) T ∈G(w) where G(w) denotes the set of parse trees with unrefined nonterminals for w. In a PCFG, T ∗ can be found using dynamic programming such as the CYK algorithm. However, parsing becomes intractable with LVeG"
P18-1109,D08-1091,0,0.27668,"ng (Klein and Manning, 2003). Matsuzaki et al. (2005) introduce latent variable grammars, in which each syntactic category (represented by a nonterminal) is split into a fixed number of subtypes and a discrete latent variable is used to indicate the subtype of the nonterminal when it appears in a specific parse tree. Since the latent variables are not observable in treebanks, the grammar is learned using expectation-maximization. Petrov et al. (2006) present a split-merge approach to learning latent variable grammars, which hierarchically splits each nonterminal and merges ineffective splits. Petrov and Klein (2008b) further allow a nonterminal to have different splits in different production rules, which results in a more compact grammar. Recently, neural approaches become very popular in natural language processing (NLP). An important technique in neural approaches to NLP is to represent discrete symbols such as words and syntactic categories with continuous vectors or embeddings. Since the distances between such vector representations often reflect the similarity between the corresponding symbols, this technique facilitates more informed smoothing in learning functions of symbols (e.g., the probabili"
P18-1109,J07-4003,0,0.0238921,"BC (a, b, c). For a production rule of the form A  w where w ∈ Σ, its weight function is WAw (a). The weight functions should be non-negative, continuous and smooth, and hence fine-grained production rules of similar subtypes of a nonterminal would have similar weight assignments. R weights can be normalized P Rule such that B,C b,c WABC (a, b, c)dbdc = 1, which leads to a probabilistic context-free grammar (PCFG). Whether the weights are normalized or not leads to different model classes and accordingly different estimation methods. However, the two model classes are proven equivalent by Smith and Johnson (2007). 2.2 Relation to Other Models Latent variable grammars (LVGs) (Matsuzaki et al., 2005; Petrov et al., 2006) associate each nonterminal with a discrete latent variable, which is used to indicate the subtype of the nonterminal when it appears in a parse tree. Through nonterminal-splitting and the 1182 expectation-maximization algorithm, fine-grained production rules can be automatically induced from a treebank. We show that LVGs can be seen as a special case of LVeGs. Specifically, we can use one-hot vectors in LVeGs to represent latent variables in LVGs and define weight functions in LVeGs acc"
P18-1109,P13-1045,0,0.43523,"s or embeddings. Since the distances between such vector representations often reflect the similarity between the corresponding symbols, this technique facilitates more informed smoothing in learning functions of symbols (e.g., the probability of a production rule). In addition, what a symbol represents may subtly depend on its context, and a continuous vector representation has the potential of representing each instance of the symbol in a more precise manner. For constituency parsing, recursive neural networks (Socher et al., 2011) and their extensions such as compositional vector grammars (Socher et al., 2013) can be seen as representing nonterminals in a context-free grammar with continuous vectors. However, exact inference in these models is intractable. In this paper, we introduce latent vector grammars (LVeGs), a novel framework of grammars with fine-grained nonterminal subtypes. A LVeG associates each nonterminal with a continuous vector space that represents the set of (infinitely many) subtypes of the nonterminal. For each in1181 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1181–1189 c Melbourne, Australia, July 15 - 20, 2018. 2"
P19-1454,S15-2162,0,0.147182,"Missing"
P19-1454,D17-1003,0,0.218232,"ional distribution and tries to iteratively minimize their KL divergence. We can derive the following iterative update equations of distribution Qij (Xij ). X (t−1) (t−1) (sib) (t−1) (cop) Fij = Qik (1)sij,ik + Qkj (1)sij,kj k6=i,j (t−1) + Qjk (gp) (t−1) (1)sij,jk + Qki (gp) (1)ski,ij (7) (t) the idea and unfold both mean field variational inference and loopy belief propagation as recurrent neural network layers that are parameterized by part scores. The time complexity of our inference procedure is O(n3 ), which is lower than the O(n4 ) complexity of the exact quasi-second-order inference of Cao et al. (2017) and on par with the complexity of the approximate second-order inference of Martins and Almeida (2014). 3.3 Learning Given a gold parse graph y ⋆ of sentence w, the (edge) conditional distribution over possible edge yij (label) and corresponding possible label yij is given by: Qij (0) ∝ 1 (t) (edge) Qij (1) ∝ exp{sij (t−1) + Fij (edge) P (yij } (0) The initial distribution Qij (Xij ) is set by normalizing the unary potential φu (Xij ). We iteratively update the distributions for T steps and then out(T ) put Qij (Xij ), where T is a hyperparameter. Loopy Belief Propagation Loopy belief propaga"
P19-1454,D07-1101,0,0.543237,"ims to produce graph-structured semantic dependency representations of sentences instead of treestructured syntactic dependency parses. Existing approaches to semantic dependency parsing can be classified as graph-based approaches and transition-based approaches. In this paper, we investigate graph-based approaches which score each possible parse of a sentence by factorizing over its parts and search for the highest-scoring parse. Previous work in graph-based syntactic dependency parsing has shown that higher-order parsing generally outperforms first-order parsing (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012). While a first-order parser scores dependency edges independently, a higher-order parser takes relationships between two or more edges into consideration. However, most of the previous algorithms for higher-order syntactic dependency tree parsing are not applicable to semantic dependency ∗ Corresponding Author graph parsing, and designing efficient algorithms for higher-order semantic dependency graph parsing is nontrivial. In addition, it becomes a common practice to use neural networks to compute features and scores of parse graph components, which"
P19-1454,P05-1012,0,0.230895,"used discrete optimizing algorithm alternating directions dual decomposition (AD3 ) as their decoder. Cao et al. (2017) also proposed a quasi-second-order semantic dependency parser with dynamic programming. Our model contains second-order information comparing with the first-order approaches and benefits from endto-end training comparing with other second-order approaches. 5.2 Higher-Order Dependency Parsing Higher-order parsing has been extensively studied in the literature of syntactic dependency parsing. Much of these work is based on the first-order maximum spanning tree (MST) parser of McDonald et al. (2005) which factorizes a dependency tree into individual edges and maximizes the summation of the scores of all the edges in a tree. McDonald and Pereira (2006) introduced a secondorder MST that factorizes a dependency tree into not only edges but also second-order sibling parts, which allows interactions between adjacent sibling words. Carreras (2007) defined second-order grandparent parts representing grandparental relations. Koo and Collins (2010) introduced thirdorder grand-sibling and tri-sibling parts. A grandsibling part represents a grandparent with two grandchildren and a tri-sibling part"
P19-1454,E06-1011,0,0.746428,"ncy parsing (Oepen et al.) aims to produce graph-structured semantic dependency representations of sentences instead of treestructured syntactic dependency parses. Existing approaches to semantic dependency parsing can be classified as graph-based approaches and transition-based approaches. In this paper, we investigate graph-based approaches which score each possible parse of a sentence by factorizing over its parts and search for the highest-scoring parse. Previous work in graph-based syntactic dependency parsing has shown that higher-order parsing generally outperforms first-order parsing (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012). While a first-order parser scores dependency edges independently, a higher-order parser takes relationships between two or more edges into consideration. However, most of the previous algorithms for higher-order syntactic dependency tree parsing are not applicable to semantic dependency ∗ Corresponding Author graph parsing, and designing efficient algorithms for higher-order semantic dependency graph parsing is nontrivial. In addition, it becomes a common practice to use neural networks to compute features and scores of parse graph c"
P19-1454,S15-2153,0,0.331136,"Missing"
P19-1454,P81-1022,0,0.61546,"Missing"
P19-1454,P18-2077,0,0.300655,"Missing"
P19-1454,P17-1186,0,0.1263,"Missing"
P19-1454,S15-2154,0,0.256437,"Missing"
P19-1454,N18-1135,0,0.090553,"Missing"
P19-1454,Q15-1035,0,0.0616801,"rt represents a parent with three children. Ma and Zhao (2012) defined grand-tri-sibling parts for fourth-order dependency parsing. Many previous approaches to higher-order dependency parsing perform exact decoding based on dynamic programming, but there is also research in approximate higher-order parsing. Martins et al. (2011) proposed an alternating directions dual decomposition (AD3 ) algorithm which splits the original problem into several local subproblems and solves them iteratively. They employed AD3 for second-order dependency parsing to speed up decoding. Smith and Eisner (2008) and Gormley et al. (2015) proposed to use belief propagation for approximate higher-order parsing, which is closely related to our work. While higher-order parsing has been shown to improve syntactic dependency parsing accuracy, it receives less attention in semantic dependency parsing. Martins and Almeida (2014) proposed second-order semantic dependency parsing and employed AD3 for approximate decoding. Cao et al. (2017) proposed a quasi-second-order parser and used dynamic programming for decoding with time complexity of O(n4 ). 5.3 CRF as Recurrent Neural Networks Zheng et al. (2015) are probably the first to propo"
P19-1454,P10-1001,0,0.426485,"raph-structured semantic dependency representations of sentences instead of treestructured syntactic dependency parses. Existing approaches to semantic dependency parsing can be classified as graph-based approaches and transition-based approaches. In this paper, we investigate graph-based approaches which score each possible parse of a sentence by factorizing over its parts and search for the highest-scoring parse. Previous work in graph-based syntactic dependency parsing has shown that higher-order parsing generally outperforms first-order parsing (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012). While a first-order parser scores dependency edges independently, a higher-order parser takes relationships between two or more edges into consideration. However, most of the previous algorithms for higher-order syntactic dependency tree parsing are not applicable to semantic dependency ∗ Corresponding Author graph parsing, and designing efficient algorithms for higher-order semantic dependency graph parsing is nontrivial. In addition, it becomes a common practice to use neural networks to compute features and scores of parse graph components, which ideally requires backp"
P19-1454,C12-2077,0,0.388585,"c dependency representations of sentences instead of treestructured syntactic dependency parses. Existing approaches to semantic dependency parsing can be classified as graph-based approaches and transition-based approaches. In this paper, we investigate graph-based approaches which score each possible parse of a sentence by factorizing over its parts and search for the highest-scoring parse. Previous work in graph-based syntactic dependency parsing has shown that higher-order parsing generally outperforms first-order parsing (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012). While a first-order parser scores dependency edges independently, a higher-order parser takes relationships between two or more edges into consideration. However, most of the previous algorithms for higher-order syntactic dependency tree parsing are not applicable to semantic dependency ∗ Corresponding Author graph parsing, and designing efficient algorithms for higher-order semantic dependency graph parsing is nontrivial. In addition, it becomes a common practice to use neural networks to compute features and scores of parse graph components, which ideally requires backpropagation of parsin"
P19-1454,D14-1162,0,0.0858156,"(0) We initialize the messages with Mkl→ij = 1. We iteratively update the messages and distributions (T ) for T steps and then output Qij (Xij ). Inference as Recurrent Layers Zheng et al. (2015) proposed that a fixed number of iterations in mean field variational inference can be seen as a recurrent neural network that is parameterized by the potential functions. We follow We tuned the hyperparameters of our baseline model from Dozat and Manning (2018) and our second-order model on the DM development set. We followed Dozat and Manning (2018) using 100-dimensional pretrained GloVe embeddings (Pennington et al., 2014) and transformed them to be 125-dimensional. Words and lemmas appeared less than 7 times are replaced with a special unknown token. We use the same dataset split as in previous approaches (Martins and Almeida, 2014; Du et al., 2015) with 33,964 sentences in the training set, 1,692 sentences in the development set, 4612 Hidden Layer Word/Glove/Lemma/Char POS GloVe Linear BiLSTM LSTM Char LSTM Unary Arc/Label Binary Arc Dropouts Word/GloVe/POS/Lemma Char LSTM (FF/recur) Char Linear BiLSTM (FF/recur) Unary Arc/Label Binary Arc Optimizer & Loss Baseline Interpolation (λ) Second-Order Interpolation"
P19-1454,D08-1016,0,0.0763183,"hildren and a tri-sibling part represents a parent with three children. Ma and Zhao (2012) defined grand-tri-sibling parts for fourth-order dependency parsing. Many previous approaches to higher-order dependency parsing perform exact decoding based on dynamic programming, but there is also research in approximate higher-order parsing. Martins et al. (2011) proposed an alternating directions dual decomposition (AD3 ) algorithm which splits the original problem into several local subproblems and solves them iteratively. They employed AD3 for second-order dependency parsing to speed up decoding. Smith and Eisner (2008) and Gormley et al. (2015) proposed to use belief propagation for approximate higher-order parsing, which is closely related to our work. While higher-order parsing has been shown to improve syntactic dependency parsing accuracy, it receives less attention in semantic dependency parsing. Martins and Almeida (2014) proposed second-order semantic dependency parsing and employed AD3 for approximate decoding. Cao et al. (2017) proposed a quasi-second-order parser and used dynamic programming for decoding with time complexity of O(n4 ). 5.3 CRF as Recurrent Neural Networks Zheng et al. (2015) are p"
P19-1454,S14-2082,0,0.582055,"date equations of distribution Qij (Xij ). X (t−1) (t−1) (sib) (t−1) (cop) Fij = Qik (1)sij,ik + Qkj (1)sij,kj k6=i,j (t−1) + Qjk (gp) (t−1) (1)sij,jk + Qki (gp) (1)ski,ij (7) (t) the idea and unfold both mean field variational inference and loopy belief propagation as recurrent neural network layers that are parameterized by part scores. The time complexity of our inference procedure is O(n3 ), which is lower than the O(n4 ) complexity of the exact quasi-second-order inference of Cao et al. (2017) and on par with the complexity of the approximate second-order inference of Martins and Almeida (2014). 3.3 Learning Given a gold parse graph y ⋆ of sentence w, the (edge) conditional distribution over possible edge yij (label) and corresponding possible label yij is given by: Qij (0) ∝ 1 (t) (edge) Qij (1) ∝ exp{sij (t−1) + Fij (edge) P (yij } (0) The initial distribution Qij (Xij ) is set by normalizing the unary potential φu (Xij ). We iteratively update the distributions for T steps and then out(T ) put Qij (Xij ), where T is a hyperparameter. Loopy Belief Propagation Loopy belief propagation iteratively passes messages between variables and potential functions (factors). Because our CRF c"
P19-1454,D11-1022,0,0.123811,"een adjacent sibling words. Carreras (2007) defined second-order grandparent parts representing grandparental relations. Koo and Collins (2010) introduced thirdorder grand-sibling and tri-sibling parts. A grandsibling part represents a grandparent with two grandchildren and a tri-sibling part represents a parent with three children. Ma and Zhao (2012) defined grand-tri-sibling parts for fourth-order dependency parsing. Many previous approaches to higher-order dependency parsing perform exact decoding based on dynamic programming, but there is also research in approximate higher-order parsing. Martins et al. (2011) proposed an alternating directions dual decomposition (AD3 ) algorithm which splits the original problem into several local subproblems and solves them iteratively. They employed AD3 for second-order dependency parsing to speed up decoding. Smith and Eisner (2008) and Gormley et al. (2015) proposed to use belief propagation for approximate higher-order parsing, which is closely related to our work. While higher-order parsing has been shown to improve syntactic dependency parsing accuracy, it receives less attention in semantic dependency parsing. Martins and Almeida (2014) proposed second-ord"
P19-1454,S14-2008,0,\N,Missing
P19-1457,D08-1083,0,0.0432787,"Tree-LSTM model that adds a top-down component after Tree-LSTM encoding. These models handle sentiment composition implicitly and predict sentiment polarities only based on embeddings of current nodes. In contrast, we model sentiment explicitly. Sentiment composition Moilanen and Pulman (2007) introduced a seminal model for sentiment composition (Montague, 1974), composed positive, negative and neutral (+1/-1/0) singles hierarchically. Taboada et al. (2011) proposed a lexiconbased method for addressing sentence level contextual valence shifting phenomena such as negation and intensification. Choi and Cardie (2008) used a structured linear model to learn semantic compositionality relying on a set of manual features. Dong et al. (2015) developed a statistical parser to learn the sentiment structure of a sentence. Our method is similar in that grammars are used to model semantic compositionality. But we consider neural methods instead of statistical methods for sentiment composition. Teng et al. (2016) proposed a simple weighted-sum model of introducing sentiment lexicon features to LSTM for sentiment analysis. They used -2 to 2 represent sentiment polarities. In contrast, we model sentiment subtypes with"
P19-1457,J15-2004,0,0.0519991,"al., 2017) have been exploited for modeling each phrase independently. Recently, tree structured models (Zhu et al., 2015; Tai et al., 2015; Teng and Zhang, 2017) were leveraged for learning phrase compositions in sentence representation given the syntactic structure. Such models classify the sentiment over each constituent node according to its hidden vector through tree structure encoding. Though effective, existing neural methods do not consider explicit sentiment compositionality (Montague, 1974). Take the sentence “The movie is not very good, but I still like it” in Figure 1 as example (Dong et al., 2015), over the constituent tree, sentiment signals can be propagated from leaf nodes to the root, going through negation, intensification and contrast according to the context. Modeling such signal channels can intuitively lead to more ∗ Work was done when the first author was visiting Westlake University. The third author is the corresponding author. interpretable and reliable results. To model sentiment composition, direct encoding of sentiment signals (e.g., +1/-1 or more fine-grained forms) is necessary. To this end, we consider a neural network grammar with latent variables. In particular, we"
P19-1457,S15-1002,0,0.0184592,"ults than coarsegrained models. Using a bi-attentive classification network (Peters et al., 2018) as the encoder, out final model gives the best results on SST. To our knowledge, we are the first to consider neural network grammars with latent variables for sentiment composition. Our code will be released at https://github.com/Ehaschia/bi-tree-lstm-crf. 2 Related Work Phrase-level sentiment analysis Li et al. (2015) and McCann et al. (2017) proposed sequence structured models that predict the sentiment polarities of the individual phrases in a sentence independently. Zhu et al. (2015), Le and Zuidema (2015), Tai et al. (2015) and Gupta and Zhang (2018) proposed Tree-LSTM models to capture bottom-up dependencies between constituents for sentiment analysis. In order to support information flow bidirectionally over trees, Teng and Zhang (2017) introduced a Bi-directional Tree-LSTM model that adds a top-down component after Tree-LSTM encoding. These models handle sentiment composition implicitly and predict sentiment polarities only based on embeddings of current nodes. In contrast, we model sentiment explicitly. Sentiment composition Moilanen and Pulman (2007) introduced a seminal model for sentime"
P19-1457,D15-1278,0,0.299164,"presentations that capture sentiment subtype expressions by latent variables and Gaussian mixture vectors, respectively. Experiments on Stanford Sentiment Treebank (SST) show the effectiveness of sentiment grammar over vanilla neural encoders. Using ELMo embeddings, our method gives the best results on this benchmark. 1 -1 -1 0 0 The movie is -1 -1 not 0 1 but I still like it , 1 0 1 very good Figure 1: Example of sentiment composition Introduction Determining the sentiment polarity at or below the sentence level is an important task in natural language processing. Sequence structured models (Li et al., 2015; McCann et al., 2017) have been exploited for modeling each phrase independently. Recently, tree structured models (Zhu et al., 2015; Tai et al., 2015; Teng and Zhang, 2017) were leveraged for learning phrase compositions in sentence representation given the syntactic structure. Such models classify the sentiment over each constituent node according to its hidden vector through tree structure encoding. Though effective, existing neural methods do not consider explicit sentiment compositionality (Montague, 1974). Take the sentence “The movie is not very good, but I still like it” in Figure 1 a"
P19-1457,P05-1010,0,0.205135,"To model sentiment composition, direct encoding of sentiment signals (e.g., +1/-1 or more fine-grained forms) is necessary. To this end, we consider a neural network grammar with latent variables. In particular, we employ a grammar as the backbone of our approach in which nonterminals represent sentiment signals and grammar rules specify sentiment compositions. In the simplest version of our approach, nonterminals are sentiment labels from SST directly, resulting in 1 a weighted grammar. To model more fine-grained emotions (Ortony and Turner, 1990), we consider a latent variable grammar (LVG, Matsuzaki et al. (2005), Petrov et al. (2006)), which splits each nonterminal into subtypes to represent subtle sentiment signals and uses a discrete latent variable to denote the sentiment subtype of a phrase. Finally, inspired by the fact that sentiment can be modeled with a low dimensional continuous space (Mehrabian, 1980), we introduce a Gaussian mixture latent vector grammar (GM-LVeG, Zhao et al. (2018)), which 4642 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4642–4651 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics"
P19-1457,D14-1162,0,0.082102,"and very positive, respectively. The root label represents the sentiment label of the whole sentence. The constituent node label represents the sentiment label of the phrase it spans. We perform both binary classification (-1, 1) and fine-grained classification (0-4), called SST-2 and SST-5, respectively. Following previous work, we use the labels of all phrases and gold-standard tree structures for training and testing. For binary classification, we merge all positive labels and negative labels. 5.2 Experimental Settings Hyper-parameters For ConTree, word vectors are initialized using Glove (Pennington et al., 2014) 300-dimensional embeddings and are updated together with other parameters. We set the hidden size of hidden units is 300. Adam (Kingma and Ba, 2014) is used to optimize the parameters with learning rate is 0.001. We adopt Dropout after the Embedding layer with a probability of 0.5. The sentence level mini-batch size is 32. For BCN experiment, we follow the model setting in McCann et al. (2017) except the sentence level mini-batch is set to 8. 5.3 Development Experiments We use the SST development dataset to investigate different configurations of our latent variables and Gaussian mixtures. Th"
P19-1457,N18-1202,0,0.496416,"018)), which 4642 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4642–4651 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics associates each sentiment signal with a continuous vector instead of a discrete variable. Experiments on SST show that explicit modeling of sentiment composition leads to significantly improved performance over standard tree encoding, and models that learn subtle emotions as hidden variables give better results than coarsegrained models. Using a bi-attentive classification network (Peters et al., 2018) as the encoder, out final model gives the best results on SST. To our knowledge, we are the first to consider neural network grammars with latent variables for sentiment composition. Our code will be released at https://github.com/Ehaschia/bi-tree-lstm-crf. 2 Related Work Phrase-level sentiment analysis Li et al. (2015) and McCann et al. (2017) proposed sequence structured models that predict the sentiment polarities of the individual phrases in a sentence independently. Zhu et al. (2015), Le and Zuidema (2015), Tai et al. (2015) and Gupta and Zhang (2018) proposed Tree-LSTM models to capture"
P19-1457,P06-1055,0,0.496334,"ition, direct encoding of sentiment signals (e.g., +1/-1 or more fine-grained forms) is necessary. To this end, we consider a neural network grammar with latent variables. In particular, we employ a grammar as the backbone of our approach in which nonterminals represent sentiment signals and grammar rules specify sentiment compositions. In the simplest version of our approach, nonterminals are sentiment labels from SST directly, resulting in 1 a weighted grammar. To model more fine-grained emotions (Ortony and Turner, 1990), we consider a latent variable grammar (LVG, Matsuzaki et al. (2005), Petrov et al. (2006)), which splits each nonterminal into subtypes to represent subtle sentiment signals and uses a discrete latent variable to denote the sentiment subtype of a phrase. Finally, inspired by the fact that sentiment can be modeled with a low dimensional continuous space (Mehrabian, 1980), we introduce a Gaussian mixture latent vector grammar (GM-LVeG, Zhao et al. (2018)), which 4642 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4642–4651 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics associates each sentim"
P19-1457,N07-1051,0,0.480552,"stead of statistical methods for sentiment composition. Teng et al. (2016) proposed a simple weighted-sum model of introducing sentiment lexicon features to LSTM for sentiment analysis. They used -2 to 2 represent sentiment polarities. In contrast, we model sentiment subtypes with latent variables and combine the strength of neural encoder and hierarchical sentiment composition. Latent Variable Grammar There has been a line of work using discrete latent variables to enrich coarse-grained constituent labels in phrasestructure parsing (Johnson, 1998; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007). Our work is similar in that discrete latent variables are used to model sentiment polarities. To our knowledge, we are the first to consider modeling fine-grained sentiment signals by investigating different types of latent variables. Recently, there has been work using continuous latent vectors for modeling syntactic categories (Zhao et al., 2018). We consider their grammar also in modeling sentiment polarities. 3 Baseline We take the constituent Tree-LSTM as our baseline, which extends sequential LSTM to tree-structured network topologies. Formally, our model computes a parent representati"
P19-1457,D13-1170,0,0.026665,"sion rules. 5 Experiments To investigate the effectiveness of modeling sentiment composition explicitly and using discrete variables or continuous vectors to model sentiment subtypes, we compare standard constituent TreeLSTM (ConTree) with our models ConTree+WG, ConTree+LVG and ConTree+LVeG, respectively. To show the universality of our approaches, we also experiment with the combination of a state-of-theart sequence structured model, bi-attentive classification network (BCN, Peters et al. (2018)), with our model: BCN+WG, BCN+LVG and BCN+LVeG. 5.1 Data We use Stanford Sentiment TreeBank (SST, Socher et al. (2013)) for our experiments. Each constituent node in a phrase-structured tree is manually assigned an integer sentiment polarity from 0 to 4, which correspond to five sentiment classes: very negative, negative, neutral, positive and very positive, respectively. The root label represents the sentiment label of the whole sentence. The constituent node label represents the sentiment label of the phrase it spans. We perform both binary classification (-1, 1) and fine-grained classification (0-4), called SST-2 and SST-5, respectively. Following previous work, we use the labels of all phrases and gold-st"
P19-1457,J11-2001,0,0.0432387,"between constituents for sentiment analysis. In order to support information flow bidirectionally over trees, Teng and Zhang (2017) introduced a Bi-directional Tree-LSTM model that adds a top-down component after Tree-LSTM encoding. These models handle sentiment composition implicitly and predict sentiment polarities only based on embeddings of current nodes. In contrast, we model sentiment explicitly. Sentiment composition Moilanen and Pulman (2007) introduced a seminal model for sentiment composition (Montague, 1974), composed positive, negative and neutral (+1/-1/0) singles hierarchically. Taboada et al. (2011) proposed a lexiconbased method for addressing sentence level contextual valence shifting phenomena such as negation and intensification. Choi and Cardie (2008) used a structured linear model to learn semantic compositionality relying on a set of manual features. Dong et al. (2015) developed a statistical parser to learn the sentiment structure of a sentence. Our method is similar in that grammars are used to model semantic compositionality. But we consider neural methods instead of statistical methods for sentiment composition. Teng et al. (2016) proposed a simple weighted-sum model of introd"
P19-1457,P15-1150,0,0.183571,"Missing"
P19-1457,D16-1169,1,0.895889,"Missing"
P19-1457,Q17-1012,1,0.925999,") show the effectiveness of sentiment grammar over vanilla neural encoders. Using ELMo embeddings, our method gives the best results on this benchmark. 1 -1 -1 0 0 The movie is -1 -1 not 0 1 but I still like it , 1 0 1 very good Figure 1: Example of sentiment composition Introduction Determining the sentiment polarity at or below the sentence level is an important task in natural language processing. Sequence structured models (Li et al., 2015; McCann et al., 2017) have been exploited for modeling each phrase independently. Recently, tree structured models (Zhu et al., 2015; Tai et al., 2015; Teng and Zhang, 2017) were leveraged for learning phrase compositions in sentence representation given the syntactic structure. Such models classify the sentiment over each constituent node according to its hidden vector through tree structure encoding. Though effective, existing neural methods do not consider explicit sentiment compositionality (Montague, 1974). Take the sentence “The movie is not very good, but I still like it” in Figure 1 as example (Dong et al., 2015), over the constituent tree, sentiment signals can be propagated from leaf nodes to the root, going through negation, intensification and contras"
P19-1457,P18-1109,1,0.889839,"Missing"
P19-1457,J98-4004,0,\N,Missing
P19-1457,P18-1197,0,\N,Missing
P19-1526,N10-1083,0,0.149791,"Missing"
P19-1526,D10-1117,0,0.480621,"Missing"
P19-1526,N09-1009,0,0.622546,"language processing. The dependency relations identified by dependency parsing convey syntactic information useful in subsequent applications such as semantic parsing, information extraction, and question answering. In this paper, we ∗ Corresponding author focus on unsupervised dependency parsing, which aims to induce a dependency parser from training sentences without gold parse annotation. Most previous approaches to unsupervised dependency parsing are based on probabilistic generative models, for example, the Dependency Model with Valence (DMV) (Klein and Manning, 2004) and its extensions (Cohen and Smith, 2009; Headden III et al., 2009; Cohen and Smith, 2010; BergKirkpatrick et al., 2010; Gillenwater et al., 2010; Jiang et al., 2016). A disadvantage of such approaches comes from the context-freeness of dependency grammars, a strong independence assumption that limits the information available in determining how likely a dependency is between two words in a sentence. In DMV, the probability of a dependency is computed from only the head and child tokens, the dependency direction, and the number of dependencies already connected from the head token. Additional information used for computing dependenc"
P19-1526,W12-1909,0,0.0607523,"Missing"
P19-1526,P10-2036,0,0.206164,"Missing"
P19-1526,P15-1133,0,0.783648,"In DMV, the probability of a dependency is computed from only the head and child tokens, the dependency direction, and the number of dependencies already connected from the head token. Additional information used for computing dependency probabilities in later work is also limited to local morpho-syntactic features such as word forms, lemmas and categories (Berg-Kirkpatrick et al., 2010), which does not break the context-free assumption. More recently, researchers have started to utilize discriminative methods in unsupervised dependency parsing based on the idea of discriminative clustering (Grave and Elhadad, 2015), the CRFAE framework (Cai et al., 2017) or the neural variational transition-based parser (Li et al., 2019). By conditioning dependency prediction on the whole input sentence, discriminative methods are capable of utilizing not only local information, but also global and contextual information of a dependency in determining its strength. Specifically, both Grave and Elhadad (2015) and Cai et al. (2017) include in the feature set of a dependency the information of the tokens around the head or child token of the dependency. In this way, 5315 Proceedings of the 57th Annual Meeting of the Associ"
P19-1526,D17-1176,1,0.760772,"the-art approaches on the specific dataset. Setup Following previous work, we conducted experiments under the unlexicalized setting where a sentence is represented as a sequence of gold part-of-speech tags with punctuations removed. The embedding length was set to 10 for the head and child tokens and the valence. The sentence embedding length was also set to 10. We trained the neural networks using stochastic gradient descent with batch size 10 and learning rate 0.01. We used the change of the loss on the validation set as the stop criteria. For our methods in the WSJ experiments, we followed Han et al. (2017) and initialized our model using the pre-trained model of Naseem et al. (2010), which significantly increased the accuracy and decreased the variance. For the other experiments, we used a pre-trained NDMV model to initialize our method. We ran our model for 5 times and report the average DDA. 5319 M ETHODS WSJ10 WSJ NDMV Systems in Basic Setup DMV (Klein and Manning, 2004) 58.3 39.4 59.4 40.5 LN (Cohen et al., 2008) Convex-MST (Grave and Elhadad, 2015) 60.8 48.6 61.3 41.4 Shared LN (Cohen and Smith, 2009) Feature DMV (Berg-Kirkpatrick et al., 2010) 63.0 64.3 53.3 PR-S (Gillenwater et al., 2010"
P19-1526,N09-1012,0,0.413013,"Missing"
P19-1526,D16-1073,1,0.648327,"applications such as semantic parsing, information extraction, and question answering. In this paper, we ∗ Corresponding author focus on unsupervised dependency parsing, which aims to induce a dependency parser from training sentences without gold parse annotation. Most previous approaches to unsupervised dependency parsing are based on probabilistic generative models, for example, the Dependency Model with Valence (DMV) (Klein and Manning, 2004) and its extensions (Cohen and Smith, 2009; Headden III et al., 2009; Cohen and Smith, 2010; BergKirkpatrick et al., 2010; Gillenwater et al., 2010; Jiang et al., 2016). A disadvantage of such approaches comes from the context-freeness of dependency grammars, a strong independence assumption that limits the information available in determining how likely a dependency is between two words in a sentence. In DMV, the probability of a dependency is computed from only the head and child tokens, the dependency direction, and the number of dependencies already connected from the head token. Additional information used for computing dependency probabilities in later work is also limited to local morpho-syntactic features such as word forms, lemmas and categories (Be"
P19-1526,D17-1177,1,0.840711,"l=1 z∈Z(x) X = log pΘ (x, z|s(1) ) (10) 4.1 Dataset and Setup English Penn Treebank We conducted experiments on the Wall Street Journal corpus (WSJ) with section 2-21 for training, section 22 for validation and section 23 for testing. We trained our model with training sentences of length ≤ 10, tuned the hyer-parameters on validation sentences of length ≤ 10 the and evaluated on testing sentences of length ≤ 10 (WSJ10) and all sentences (WSJ). We reported the directed dependency accuracy (DDA) of the learned grammars on the test sentences. Universal Dependency Treebank Following the setup of Jiang et al. (2017); Li et al. (2019), we conducted experiments on selected eight languages from the Universal Dependency Treebank 1.4 (Nivre et al., 2016). We trained our model on training sentences of length ≤ 15 and report the DDA on testing sentences of length ≤ 15 and ≤ 40. Datasets from PASCAL Challenge on Grammar Induction We conducted experiments on corpora of eight languages from the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). We trained our model with training sentences of length ≤ 10 and evaluated on testing sentences of length ≤ 10 and all sentences. Note that on the UD Treebanks an"
P19-1526,P04-1061,0,0.505064,"parsing is a very important task in natural language processing. The dependency relations identified by dependency parsing convey syntactic information useful in subsequent applications such as semantic parsing, information extraction, and question answering. In this paper, we ∗ Corresponding author focus on unsupervised dependency parsing, which aims to induce a dependency parser from training sentences without gold parse annotation. Most previous approaches to unsupervised dependency parsing are based on probabilistic generative models, for example, the Dependency Model with Valence (DMV) (Klein and Manning, 2004) and its extensions (Cohen and Smith, 2009; Headden III et al., 2009; Cohen and Smith, 2010; BergKirkpatrick et al., 2010; Gillenwater et al., 2010; Jiang et al., 2016). A disadvantage of such approaches comes from the context-freeness of dependency grammars, a strong independence assumption that limits the information available in determining how likely a dependency is between two words in a sentence. In DMV, the probability of a dependency is computed from only the head and child tokens, the dependency direction, and the number of dependencies already connected from the head token. Additiona"
P19-1526,N15-1067,0,0.349887,"Missing"
P19-1526,D10-1120,0,0.321188,"we conducted experiments under the unlexicalized setting where a sentence is represented as a sequence of gold part-of-speech tags with punctuations removed. The embedding length was set to 10 for the head and child tokens and the valence. The sentence embedding length was also set to 10. We trained the neural networks using stochastic gradient descent with batch size 10 and learning rate 0.01. We used the change of the loss on the validation set as the stop criteria. For our methods in the WSJ experiments, we followed Han et al. (2017) and initialized our model using the pre-trained model of Naseem et al. (2010), which significantly increased the accuracy and decreased the variance. For the other experiments, we used a pre-trained NDMV model to initialize our method. We ran our model for 5 times and report the average DDA. 5319 M ETHODS WSJ10 WSJ NDMV Systems in Basic Setup DMV (Klein and Manning, 2004) 58.3 39.4 59.4 40.5 LN (Cohen et al., 2008) Convex-MST (Grave and Elhadad, 2015) 60.8 48.6 61.3 41.4 Shared LN (Cohen and Smith, 2009) Feature DMV (Berg-Kirkpatrick et al., 2010) 63.0 64.3 53.3 PR-S (Gillenwater et al., 2010) 65.0 E-DMV (Headden III et al., 2009) 65.9 53.1 TSG-DMV (Blunsom and Cohn, 2"
P19-1526,D16-1004,0,0.369401,"Missing"
P19-1526,D13-1204,0,0.370709,"Missing"
P19-1526,W10-2902,0,0.0934217,"propagate the objective Q(Θ, Φ) into the parameters of the neural networks. We initialize the model either heuristically (Klein and Manning, 2004) or using a pre-trained unsupervised parser (Jiang et al., 2016); then we alternate between E-steps and M-steps until convergence. Note that if we require q(z) to be a delta function, then the algorithm becomes hard-EM, which computes the best parse of each training sentence in the E-step and set the expected count to 1 if the rule is used in the parse and 0 otherwise. It has been found that hard-EM outperforms EM in unsupervised dependency parsing (Spitkovsky et al., 2010; Tu and Honavar, 2012), so we use hard-EM in our experiments. 3.2 Variational Variant for D-NDMV Motivated by (Bowman et al., 2016), we propose to model the global representation s as drawing from a prior distribution, generally a standard 5318 Gaussian distribution. We also propose a variational posterior distribution qΦ (s|x) to approximate this prior distribution. In this way, we formalize it into a variational inference framework. We call this model variational variant and illustrate its graphical model in Figure 2 (right). It can be seen from Figure 2 (right) that the variational variant"
P19-1526,D12-1121,1,0.954062,"(Θ, Φ) into the parameters of the neural networks. We initialize the model either heuristically (Klein and Manning, 2004) or using a pre-trained unsupervised parser (Jiang et al., 2016); then we alternate between E-steps and M-steps until convergence. Note that if we require q(z) to be a delta function, then the algorithm becomes hard-EM, which computes the best parse of each training sentence in the E-step and set the expected count to 1 if the rule is used in the parse and 0 otherwise. It has been found that hard-EM outperforms EM in unsupervised dependency parsing (Spitkovsky et al., 2010; Tu and Honavar, 2012), so we use hard-EM in our experiments. 3.2 Variational Variant for D-NDMV Motivated by (Bowman et al., 2016), we propose to model the global representation s as drawing from a prior distribution, generally a standard 5318 Gaussian distribution. We also propose a variational posterior distribution qΦ (s|x) to approximate this prior distribution. In this way, we formalize it into a variational inference framework. We call this model variational variant and illustrate its graphical model in Figure 2 (right). It can be seen from Figure 2 (right) that the variational variant shares the same formul"
W12-1915,afonso-etal-2002-floresta,0,0.30434,"Missing"
W12-1915,erjavec-etal-2010-jos,0,0.150466,"rsity and Unambiguity Biases for Grammar Induction Kewei Tu Departments of Statistics and Computer Science University of California, Los Angeles Los Angeles, CA 90095, USA tukw@ucla.edu Abstract open-resource track which allows other external resources to be used. Ten corpora of nine different languages are used in the challenge: Arabic (Hajiˇc et al., 2004), Basque (Aduriz et al., 2003), Czech (Hajiˇc et al., 2000), Danish (Buch-Kromann et al., 2007), Dutch (Beek et al., 2002), English WSJ (Marcus et al., 1993), English CHILDES (Sagae et al., 2007), Portuguese (Afonso et al., 2002), Slovene (Erjavec et al., 2010), and Swedish (Nivre et al., 2006). For each corpus, a large set of unannotated sentences are provided as the training data, along with a small set of annotated sentences as the development data; the predictions on the unannotated test data submitted by challenge participants are evaluated against the gold standard annotations. In this paper we describe our participating system for the dependency induction track of the PASCAL Challenge on Grammar Induction. Our system incorporates two types of inductive biases: the sparsity bias and the unambiguity bias. The sparsity bias favors a grammar with"
W12-1915,P10-2036,0,0.267682,"Missing"
W12-1915,N07-1018,0,0.106209,"Missing"
W12-1915,P04-1061,0,0.0294316,"we found that removing it significantly decreased the accuracy of the learned grammar. We combined the provided training, development and test set as our training set. We trained our system on the fine POS tags except for the Dutch corpus. In the Dutch corpus, the fine POS tags are the same as the coarse POS tags except that each multi-word unit is annotated with the concatenation of the POS tags of all the component words, making the training data for such tags extremely sparse. So we chose to use the coarse POS tags for the Dutch corpus. We employed the informed initialization proposed in (Klein and Manning, 2004) and ran our two approaches on the training set. We tuned the param1 Available at pr-toolkit/ http://code.google.com/p/ 108 eters by coordinate ascent on the development set. The parameters that we tuned include the maximal length of sentences used in training, the valence and back-off strength of the E-DMV model, the hyperparameter α of Dirichlet priors, the type (PR-S or PRAS) and strength σs of sparsity-inducing posterior regularization, and the strength σu of unambiguity regularization. Sparsity-inducing posterior regularization has a high computational cost. Consequently, we were not able"
W12-1915,J93-2004,0,0.0452962,"Missing"
W12-1915,nivre-etal-2006-talbanken05,0,0.142158,"mmar Induction Kewei Tu Departments of Statistics and Computer Science University of California, Los Angeles Los Angeles, CA 90095, USA tukw@ucla.edu Abstract open-resource track which allows other external resources to be used. Ten corpora of nine different languages are used in the challenge: Arabic (Hajiˇc et al., 2004), Basque (Aduriz et al., 2003), Czech (Hajiˇc et al., 2000), Danish (Buch-Kromann et al., 2007), Dutch (Beek et al., 2002), English WSJ (Marcus et al., 1993), English CHILDES (Sagae et al., 2007), Portuguese (Afonso et al., 2002), Slovene (Erjavec et al., 2010), and Swedish (Nivre et al., 2006). For each corpus, a large set of unannotated sentences are provided as the training data, along with a small set of annotated sentences as the development data; the predictions on the unannotated test data submitted by challenge participants are evaluated against the gold standard annotations. In this paper we describe our participating system for the dependency induction track of the PASCAL Challenge on Grammar Induction. Our system incorporates two types of inductive biases: the sparsity bias and the unambiguity bias. The sparsity bias favors a grammar with fewer grammar rules. The unambigu"
W12-1915,P06-1055,0,0.0244161,"ammar over the maximum-likelihood estimation (Cohen et al., 2008; Gillenwater et al., 2010). 106 The unambiguity bias favors a grammar that leads to unambiguous parses on natural language sentences (Tu and Honavar, 2012). This bias is motivated by the observation that natural language is remarkably unambiguous in the sense that the number of plausible parses of a natural language sentence is very small in comparison with the total number of possible parses. To illustrate this, we randomly sample an English sentence from the Wall Street Journal and parse the sentence using the Berkeley parser (Petrov et al., 2006), one of the state-of-the-art English language parsers. The estimated total number of possible parses of this sentence is 2 × 1020 (by assuming a complete Chomsky normal form grammar with tences in the E-step. When 0 &lt; σu &lt; 1, our approach falls between standard EM and Viterbi EM: it applies a softmax function to the distribution of the parse zi of each training sentence xi in the E-step: 0.25 Probability 0.2 0.15 1 q(zi ) = αi pθ (zi |xi ) 1−σu 0.1 0.05 0 0 20 40 60 100 Best Parses 80 100 Figure 1: The probabilities of the 100 best parses of the sample sentence. the same number of nonterminal"
W12-1915,W07-0604,0,0.320442,"Missing"
W12-1915,D12-1121,1,0.902984,"if the hyperparameters are less than 1, then the Dirichlet prior assigns larger probabilities to vectors that have more elements close to zero. Therefore, Dirichlet priors can be used to encourage parameter sparsity. It has been found that when applied to dependency grammar induction, Dirichlet priors with hyperparamters set to values less than 1 can slightly improve the accuracy of the learned grammar over the maximum-likelihood estimation (Cohen et al., 2008; Gillenwater et al., 2010). 106 The unambiguity bias favors a grammar that leads to unambiguous parses on natural language sentences (Tu and Honavar, 2012). This bias is motivated by the observation that natural language is remarkably unambiguous in the sense that the number of plausible parses of a natural language sentence is very small in comparison with the total number of possible parses. To illustrate this, we randomly sample an English sentence from the Wall Street Journal and parse the sentence using the Berkeley parser (Petrov et al., 2006), one of the state-of-the-art English language parsers. The estimated total number of possible parses of this sentence is 2 × 1020 (by assuming a complete Chomsky normal form grammar with tences in th"
