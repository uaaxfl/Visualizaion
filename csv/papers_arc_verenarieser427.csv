2021.gebnlp-1.4,"{A}lexa, {G}oogle, {S}iri: What are Your Pronouns? Gender and Anthropomorphism in the Design and Perception of Conversational Assistants",2021,-1,-1,4,0,6286,gavin abercrombie,Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing,0,"Technology companies have produced varied responses to concerns about the effects of the design of their conversational AI systems. Some have claimed that their voice assistants are in fact not gendered or human-like{---}despite design features suggesting the contrary. We compare these claims to user perceptions by analysing the pronouns they use when referring to AI assistants. We also examine systems{'} responses and the extent to which they generate output which is gendered and anthropomorphic. We find that, while some companies appear to be addressing the ethical concerns raised, in some cases, their claims do not seem to hold true. In particular, our results show that system outputs are ambiguous as to the humanness of the systems, and that users tend to personify and gender them as a result."
2021.findings-emnlp.133,{M}i{RAN}ews: Dataset and Benchmarks for Multi-Resource-Assisted News Summarization,2021,-1,-1,4,1,6765,xinnuo xu,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"One of the most challenging aspects of current single-document news summarization is that the summary often contains {`}extrinsic hallucinations{'}, i.e., facts that are not present in the source document, which are often derived via world knowledge. This causes summarisation systems to act more like open-ended language models tending to hallucinate facts that are erroneous. In this paper, we mitigate this problem with the help of multiple supplementary resource documents assisting the task. We present a new dataset MiraNews and benchmark existing summarisation models. In contrast to multi-document summarization, which addresses multiple events from several source documents, we still aim at generating a summary for a single document. We show via data analysis that it{'}s not only the models which are to blame: more than 27{\%} of facts mentioned in the gold summaries of MiraNews are better grounded on assisting documents than in the main source articles. An error analysis of generated summaries from pretrained models fine-tuned on MIRANEWS reveals that this has an even bigger effects on models: assisted summarisation reduces 55{\%} of hallucinations when compared to single-document summarisation models trained on the main article only."
2021.emnlp-main.587,"{C}onv{A}buse: Data, Analysis, and Benchmarks for Nuanced Detection in Conversational {AI}",2021,-1,-1,3,1,6287,amanda curry,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"We present the first English corpus study on abusive language towards three conversational AI systems gathered {`}in the wild{'}: an open-domain social bot, a rule-based chatbot, and a task-based system. To account for the complexity of the task, we take a more {`}nuanced{'} approach where our ConvAI dataset reflects fine-grained notions of abuse, as well as views from multiple expert annotators. We find that the distribution of abuse is vastly different compared to other commonly used datasets, with more sexually tinted aggression towards the virtual persona of these systems. Finally, we report results from bench-marking existing models against this data. Unsurprisingly, we find that there is substantial room for improvement with F1 scores below 90{\%}."
2021.emnlp-main.703,What happens if you treat ordinal ratings as interval data? Human evaluations in {NLP} are even more under-powered than you think,2021,-1,-1,2,1,10049,david howcroft,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Previous work has shown that human evaluations in NLP are notoriously under-powered. Here, we argue that there are two common factors which make this problem even worse: NLP studies usually (a) treat ordinal data as interval data and (b) operate under high variance settings while the differences they are hoping to detect are often subtle. We demonstrate through simulation that ordinal mixed effects models are better able to detect small differences between models, especially in high variance settings common in evaluations of generated texts. We release tools for researchers to conduct their own power analysis and test their assumptions. We also make recommendations for improving statistical power."
2021.acl-long.113,{A}gg{G}en: Ordering and Aggregating while Generating,2021,-1,-1,3,1,6765,xinnuo xu,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We present AggGen (pronounced {`}again{'}) a data-to-text model which re-introduces two explicit sentence planning stages into neural data-to-text systems: input ordering and input aggregation. In contrast to previous work using sentence planning, our model is still end-to-end: AggGen performs sentence planning at the same time as generating text by learning latent alignments (via semantic facts) between input representation and target text. Experiments on the WebNLG and E2E challenge data show that by using fact-based alignments our approach is more interpretable, expressive, robust to noise, and easier to control, while retaining the advantages of end-to-end systems in terms of fluency. Our code is available at https://github.com/XinnuoXu/AggGen."
2021.acl-long.194,{OTT}ers: One-turn Topic Transitions for Open-Domain Dialogue,2021,-1,-1,4,0,12979,karin sevegnani,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Mixed initiative in open-domain dialogue requires a system to pro-actively introduce new topics. The one-turn topic transition task explores how a system connects two topics in a cooperative and coherent manner. The goal of the task is to generate a {``}bridging{''} utterance connecting the new topic to the topic of the previous conversation turn. We are especially interested in commonsense explanations of how a new topic relates to what has been mentioned before. We first collect a new dataset of human one-turn topic transitions, which we callOTTers. We then explore different strategies used by humans when asked to complete such a task, and notice that the use of a bridging utterance to connect the two topics is the approach used the most. We finally show how existing state-of-the-art text generation models can be adapted to this task and examine the performance of these baselines on different splits of the OTTers data."
2020.inlg-1.23,Twenty Years of Confusion in Human Evaluation: {NLG} Needs Evaluation Sheets and Standardised Definitions,2020,-1,-1,10,1,10049,david howcroft,Proceedings of the 13th International Conference on Natural Language Generation,0,"Human assessment remains the most trusted form of evaluation in NLG, but highly diverse approaches and a proliferation of different quality criteria used by researchers make it difficult to compare results and draw conclusions across papers, with adverse implications for meta-evaluation and reproducibility. In this paper, we present (i) our dataset of 165 NLG papers with human evaluations, (ii) the annotation scheme we developed to label the papers for different aspects of evaluations, (iii) quantitative analyses of the annotations, and (iv) a set of recommendations for improving standards in evaluation reporting. We use the annotations as a basis for examining information included in evaluation reports, and levels of consistency in approaches, experimental design and terminology, focusing in particular on the 200+ different terms that have been used for evaluated aspects of quality. We conclude that due to a pervasive lack of clarity in reports and extreme diversity in approaches, human evaluation in NLG presents as extremely confused in 2020, and that the field is in urgent need of standard methods and terminology."
2020.gebnlp-1.7,Conversational Assistants and Gender Stereotypes: Public Perceptions and Desiderata for Voice Personas,2020,-1,-1,3,1,6287,amanda curry,Proceedings of the Second Workshop on Gender Bias in Natural Language Processing,0,"Conversational voice assistants are rapidly developing from purely transactional systems to social companions with {``}personality{''}. UNESCO recently stated that the female and submissive personality of current digital assistants gives rise for concern as it reinforces gender stereotypes. In this work, we present results from a participatory design workshop, where we invite people to submit their preferences for a what their ideal persona might look like, both in drawings as well as in a multiple choice questionnaire. We find no clear consensus which suggests that one possible solution is to let people configure/personalise their assistants. We then outline a multi-disciplinary project of how we plan to address the complex question of gender and stereotyping in digital assistants."
2020.emnlp-main.588,{SLURP}: A Spoken Language Understanding Resource Package,2020,-1,-1,4,0,10797,emanuele bastianelli,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https://github.com/pswietojanski/slurp."
2020.acl-main.455,Fact-based Content Weighting for Evaluating Abstractive Summarisation,2020,-1,-1,4,1,6765,xinnuo xu,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Abstractive summarisation is notoriously hard to evaluate since standard word-overlap-based metrics are insufficient. We introduce a new evaluation metric which is based on fact-level content weighting, i.e. relating the facts of the document to the facts of the summary. We fol- low the assumption that a good summary will reflect all relevant facts, i.e. the ones present in the ground truth (human-generated refer- ence summary). We confirm this hypothe- sis by showing that our weightings are highly correlated to human perception and compare favourably to the recent manual highlight- based metric of Hardy et al. (2019)."
2020.acl-main.728,History for Visual Dialog: Do we really need it?,2020,53,0,5,1,5964,shubham agarwal,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Visual Dialogue involves {``}understanding{''} the dialogue history (what has been discussed previously) and the current question (what is asked), in addition to grounding information in the image, to accurately generate the correct response. In this paper, we show that co-attention models which explicitly encode dialoh history outperform models that don{'}t, achieving state-of-the-art performance (72 {\%} NDCG on val set). However, we also expose shortcomings of the crowdsourcing dataset collection procedure, by showing that dialogue history is indeed only required for a small amount of the data, and that the current evaluation metric encourages generic replies. To that end, we propose a challenging subset (VisdialConv) of the VisdialVal set and the benchmark NDCG of 63{\%}."
W19-8644,Automatic Quality Estimation for Natural Language Generation: Ranting (Jointly Rating and Ranking),2019,44,0,4,0.603967,2976,ondvrej duvsek,Proceedings of the 12th International Conference on Natural Language Generation,0,"We present a recurrent neural network based system for automatic quality estimation of natural language generation (NLG) outputs, which jointly learns to assign numerical ratings to individual outputs and to provide pairwise rankings of two different outputs. The latter is trained using pairwise hinge loss over scores from two copies of the rating network. We use learning to rank and synthetic data to improve the quality of ratings assigned by our system: We synthesise training pairs of distorted system outputs and train the system to rank the less distorted one higher. This leads to a 12{\%} increase in correlation with human ratings over the previous benchmark. We also establish the state of the art on the dataset of relative rankings from the E2E NLG Challenge (Dusek et al., 2019), where synthetic data lead to a 4{\%} accuracy increase over the base model."
W19-8652,Semantic Noise Matters for Neural Natural Language Generation,2019,23,2,3,0.603967,2976,ondvrej duvsek,Proceedings of the 12th International Conference on Natural Language Generation,0,"Neural natural language generation (NNLG) systems are known for their pathological outputs, i.e. generating text which is unrelated to the input specification. In this paper, we show the impact of semantic noise on state-of-the-art NNLG models which implement different semantic control mechanisms. We find that cleaned data can improve semantic correctness by up to 97{\%}, while maintaining fluency. We also find that the most common error is omitting information, rather than hallucination."
W19-5942,A Crowd-based Evaluation of Abuse Response Strategies in Conversational Agents,2019,0,0,2,1,6287,amanda curry,Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue,0,"How should conversational agents respond to verbal abuse through the user? To answer this question, we conduct a large-scale crowd-sourced evaluation of abuse response strategies employed by current state-of-the-art systems. Our results show that some strategies, such as {``}polite refusal{''}, score highly across the board, while for other strategies demographic factors, such as age, as well as the severity of the preceding abuse influence the user{'}s perception of which response is appropriate. In addition, we find that most data-driven models lag behind rule-based or commercial systems in terms of their perceived appropriateness."
W19-5945,User Evaluation of a Multi-dimensional Statistical Dialogue System,2019,19,0,4,0,16749,simon keizer,Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue,0,"We present the first complete spoken dialogue system driven by a multiimensional statistical dialogue manager. This framework has been shown to substantially reduce data needs by leveraging domain-independent dimensions, such as social obligations or feedback, which (as we show) can be transferred between domains. In this paper, we conduct a user study and show that the performance of a multi-dimensional system, which can be adapted from a source domain, is equivalent to that of a one-dimensional baseline, which can only be trained from scratch."
W18-6514,Improving Context Modelling in Multimodal Dialogue Generation,2018,9,0,4,1,5964,shubham agarwal,Proceedings of the 11th International Conference on Natural Language Generation,0,"In this work, we investigate the task of textual response generation in a multimodal task-oriented dialogue system. Our work is based on the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017) in the fashion domain. We introduce a multimodal extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model and show that this extension outperforms strong baselines in terms of text-based similarity metrics. We also showcase the shortcomings of current vision and language models by performing an error analysis on our system{'}s output."
W18-6539,Findings of the {E}2{E} {NLG} Challenge,2018,17,3,3,0.768277,2976,ondvrej duvsek,Proceedings of the 11th International Conference on Natural Language Generation,0,"This paper summarises the experimental setup and results of the first shared task on end-to-end (E2E) natural language generation (NLG) in spoken dialogue systems. Recent end-to-end generation systems are promising since they reduce the need for data annotation. However, they are currently limited to small, delexicalised datasets. The E2E NLG shared task aims to assess whether these novel approaches can generate better-quality output by learning from a dataset containing higher lexical richness, syntactic complexity and diverse discourse phenomena. We compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures {--} with the majority implementing sequence-to-sequence models (seq2seq) {--} as well as systems based on grammatical rules and templates."
W18-5709,A Knowledge-Grounded Multimodal Search-Based Conversational Agent,2018,11,0,4,1,5964,shubham agarwal,Proceedings of the 2018 {EMNLP} Workshop {SCAI}: The 2nd International Workshop on Search-Oriented Conversational {AI},0,"Multimodal search-based dialogue is a challenging new task: It extends visually grounded question answering systems into multi-turn conversations with access to an external database. We address this new challenge by learning a neural response generation system from the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017). We introduce a knowledge-grounded multimodal conversational model where an encoded knowledge base (KB) representation is appended to the decoder input. Our model substantially outperforms strong baselines in terms of text-based similarity measures (over 9 BLEU points, 3 of which are solely due to the use of additional information from the KB)."
W18-0802,{\\#}{M}e{T}oo {A}lexa: How Conversational Systems Respond to Sexual Harassment,2018,0,10,2,1,6287,amanda curry,Proceedings of the Second {ACL} Workshop on Ethics in Natural Language Processing,0,"Conversational AI systems, such as Amazon{'}s Alexa, are rapidly developing from purely transactional systems to social chatbots, which can respond to a wide variety of user requests. In this article, we establish how current state-of-the-art conversational systems react to inappropriate requests, such as bullying and sexual harassment on the part of the user, by collecting and analysing the novel {\#}MeTooAlexa corpus. Our results show that commercial systems mainly avoid answering, while rule-based chatbots show a variety of behaviours and often deflect. Data-driven systems, on the other hand, are often non-coherent, but also run the risk of being interpreted as flirtatious and sometimes react with counter-aggression. This includes our own system, trained on {``}clean{''} data, which suggests that inappropriate system behaviour is not caused by data bias."
N18-2012,{R}ank{ME}: Reliable Human Ratings for Natural Language Generation,2018,26,7,3,1,225,jekaterina novikova,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"Human evaluation for natural language generation (NLG) often suffers from inconsistent user ratings. While previous research tends to attribute this problem to individual user preferences, we show that the quality of human judgements can also be improved by experimental design. We present a novel rank-based magnitude estimation method (RankME), which combines the use of continuous scales and relative assessments. We show that RankME significantly improves the reliability and consistency of human ratings compared to traditional evaluation methods. In addition, we show that it is possible to evaluate NLG systems according to multiple, distinct criteria, which is important for error analysis. Finally, we demonstrate that RankME, in combination with Bayesian estimation of system quality, is a cost-effective alternative for ranking multiple NLG systems."
D18-1432,"Better Conversations by Modeling, Filtering, and Optimizing for Coherence and Diversity",2018,0,9,4,1,6765,xinnuo xu,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"We present three enhancements to existing encoder-decoder models for open-domain conversational agents, aimed at effectively modeling coherence and promoting output diversity: (1) We introduce a measure of coherence as the GloVe embedding similarity between the dialogue context and the generated response, (2) we filter our training corpora based on the measure of coherence to obtain topically coherent and lexically diverse context-response pairs, (3) we then train a response generator using a conditional variational autoencoder model that incorporates the measure of coherence as a latent variable and uses a context gate to guarantee topical consistency with the context and promote lexical diversity. Experiments on the OpenSubtitles corpus show a substantial improvement over competitive neural models in terms of BLEU score as well as metrics of coherence and diversity."
W17-5525,The {E}2{E} Dataset: New Challenges For End-to-End Generation,2017,22,16,3,1,225,jekaterina novikova,Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue,0,"This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges: (1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena; (2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances. We also establish a baseline on this dataset, which illustrates some of the difficulties associated with this data."
D17-1238,Why We Need New Evaluation Metrics for {NLG},2017,0,62,4,1,225,jekaterina novikova,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"The majority of NLG evaluation relies on automatic metrics, such as BLEU . In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly."
W16-6627,The a{NAL}o{G}u{E} Challenge: Non Aligned Language {GE}neration,2016,11,6,2,1,225,jekaterina novikova,Proceedings of the 9th International Natural Language Generation conference,0,None
W16-6644,Crowd-sourcing {NLG} Data: Pictures Elicit Better Data.,2016,17,15,3,1,225,jekaterina novikova,Proceedings of the 9th International Natural Language Generation conference,0,"Recent advances in corpus-based Natural Language Generation (NLG) hold the promise of being easily portable across domains, but require costly training data, consisting of meaning representations (MRs) paired with Natural Language (NL) utterances. In this work, we propose a novel framework for crowdsourcing high quality NLG training data, using automatic quality control measures and evaluating different MRs with which to elicit data. We show that pictorial MRs result in better NL data being collected than logic-based MRs: utterances elicited by pictorial MRs are judged as significantly more natural, more informative, and better phrased, with a significant increase in average quality ratings (around 0.5 points on a 6-point scale), compared to using the logical MRs. As the MR becomes more complex, the benefits of pictorial stimuli increase. The collected data will be released as part of this submission."
S16-1077,i{L}ab-{E}dinburgh at {S}em{E}val-2016 Task 7: A Hybrid Approach for Determining Sentiment Intensity of {A}rabic {T}witter Phrases,2016,29,13,2,1,34263,eshrag refaee,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"This paper describes the iLab-Edinburgh Sentiment Analysis system, winner of the Arabic Twitter Task 7 in SemEval-2016. The system employs a hybrid approach of supervised learning and rule-based methods to predict a sentiment intensity (SI) score for a given Arabic Twitter phrase. First, the supervised method uses an ensemble of trained linear regression models to produce an initial SI score for each given text instance. Second, the resulting SI score is adjusted using a set of rules that exploit a number of publicly available sentiment lexica. The system demonstrates strong results of 0.536 Kendall score, ranking top in this task."
P16-2043,Natural Language Generation enhances human decision-making with uncertain information,2016,16,1,3,1,5916,dimitra gkatzia,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Decision-making is often dependent on uncertain data, e.g. data associated with confidence scores or probabilities. We present a comparison of different information presentations for uncertain data and, for the first time, measure their effects on human decision-making. We show that the use of Natural Language Generation (NLG) improves decision-making under uncertainty, compared to state-of-the-art graphical-based representation methods. In a task-based study with 442 adults, we found that presentations using NLG lead to 24% better decision-making on average than the graphical presentations, and to 44% better decision-making when NLG is combined with graphics. We also show that women achieve significantly better results when presented with NLG output (an 87% increase on average compared to graphical presentations)."
L16-1341,The {REAL} Corpus: A Crowd-Sourced Corpus of Human Generated and Evaluated Spatial References to Real-World Urban Scenes,2016,0,0,4,0,35080,phil bartie,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Our interest is in people{'}s capacity to efficiently and effectively describe geographic objects in urban scenes. The broader ambition is to develop spatial models capable of equivalent functionality able to construct such referring expressions. To that end we present a newly crowd-sourced data set of natural language references to objects anchored in complex urban scenes (In short: The REAL Corpus â Referring Expressions Anchored Language). The REAL corpus contains a collection of images of real-world urban scenes together with verbal descriptions of target objects generated by humans, paired with data on how successful other people were able to identify the same object based on these descriptions. In total, the corpus contains 32 images with on average 27 descriptions per image and 3 verifications for each description. In addition, the corpus is annotated with a variety of linguistically motivated features. The paper highlights issues posed by collecting data using crowd-sourcing with an unrestricted input format, as well as using real-world urban scenes."
W15-4715,Generating and Evaluating Landmark-Based Navigation Instructions in Virtual Environments,2015,12,4,3,1,6287,amanda curry,Proceedings of the 15th {E}uropean Workshop on Natural Language Generation ({ENLG}),0,"Referring to landmarks has been identified to lead to improved navigation instructions. However, a previous corpus study suggests that human xe2x80x9cwizardsxe2x80x9d also choose to refer to street names and generate user-centric instructions. In this paper, we conduct a task-based evaluation of two systems reflecting the wizardsxe2x80x99 behaviours and compare them against an improved version of previous landmark-based systems, which resorts to user-centric descriptions if the landmark is estimated to be invisible. We use the GRUVE virtual interactive environment for evaluation. We find that the improved system, which takes visibility into account, outperforms the corpus-based wizard strategies, however not significantly. We also show a significant effect of prior user knowledge, which suggests the usefulness of a user modelling approach."
W15-4720,A Game-Based Setup for Data Collection and Task-Based Evaluation of Uncertain Information Presentation,2015,6,3,3,1,5916,dimitra gkatzia,Proceedings of the 15th {E}uropean Workshop on Natural Language Generation ({ENLG}),0,"Decision-making is often dependent on uncertain data, e.g. data associated with confidence scores, such as probabilities. A concrete example of such data is weather data. We will demo a game-based setup for exploring the effectiveness of different approaches (graphics vs NLG) to communicating uncertainty in rainfall and temperature predictions (www.macs.hw.ac.uk/ InteractionLab/weathergame/ ). The game incorporates a natural language extension of the MetOffice Weather game1. The extended version of the game can be used in three ways: (1) to compare the effectiveness of different information presentations of uncertain data; (2) to collect data for the development of effective data-driven approaches; and (3) to serve as a task-based evaluation setup for Natural Language Generation (NLG)."
N15-2010,Benchmarking Machine Translated Sentiment Analysis for {A}rabic Tweets,2015,20,10,2,1,34263,eshrag refaee,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"Traditional approaches to Sentiment Analysis (SA) rely on large annotated data sets or wide-coverage sentiment lexica, and as such often perform poorly on under-resourced languages. This paper presents empirical evidence of an efficient SA approach using freely available machine translation (MT) systems to translate Arabic tweets to English, which we then label for sentiment using a state-of-theart English SA system. We show that this approach significantly outperforms a number of standard approaches on a gold-standard heldout data set, and performs equally well compared to more cost-intense methods with 76% accuracy. This confirms MT-based SA as a cheap and effective alternative to building a fully fledged SA system when dealing with under-resourced languages."
D15-1224,From the Virtual to the {R}eal{W}orld: Referring to Objects in Real-World Spatial Scenes,2015,25,11,2,1,5916,dimitra gkatzia,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Predicting the success of referring expressions (RE) is vital for real-world applications such as navigation systems. Traditionally, research has focused on studying Referring Expression Generation (REG) in virtual, controlled environments. In this paper, we describe a novel study of spatial references from real scenes rather than virtual. First, we investigate how humans describe objects in open, uncontrolled scenarios and compare our findings to those reported in virtual environments. We show that REs in real-world scenarios differ significantly to those in virtual worlds. Second, we propose a novel approach to quantifying image complexity when complete annotations are not present (e.g. due to poor object recognition capabitlities), and third, we present a model for success prediction of REs for objects in real scenes. Finally, we discuss implications for Natural Language Generation (NLG) systems and future directions."
W14-4336,The {PARLANCE} mobile application for interactive search in {E}nglish and {M}andarin,2014,3,2,15,0.409868,1049,helen hastie,Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ({SIGDIAL}),0,We demonstrate a mobile application in English and Mandarin to test and evaluate components of the Parlance dialogue system for interactive search under real-world conditions.
W14-3624,Evaluating Distant Supervision for Subjectivity and Sentiment Analysis on {A}rabic {T}witter Feeds,2014,23,11,2,1,34263,eshrag refaee,Proceedings of the {EMNLP} 2014 Workshop on {A}rabic Natural Language Processing ({ANLP}),0,"Supervised machine learning methods for automatic subjectivity and sentiment analysis (SSA) are problematic when applied to social media, such as Twitter, since they do not generalise well to unseen topics. A possible remedy of this problem is to apply distant supervision (DS) approaches, which learn from large amounts of automatically annotated data. This research empirically evaluates the performance of DS approaches for SSA on Arabic Twitter feeds. Results for emoticon- and lexiconbased DS show a significant performance gain over a fully supervised baseline, especially for detecting subjectivity, where we achieve 95.19% accuracy, which is a 48.47% absolute improvement over previous fully supervised results."
refaee-rieser-2014-arabic,An {A}rabic {T}witter Corpus for Subjectivity and Sentiment Analysis,2014,15,76,2,1,34263,eshrag refaee,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present a newly collected data set of 8,868 gold-standard annotated Arabic feeds. The corpus is manually labelled for subjectivity and sentiment analysis (SSA) ( = 0:816). In addition, the corpus is annotated with a variety of motivated feature-sets that have previously shown positive impact on performance. The paper highlights issues posed by twitter as a genre, such as mixture of language varieties and topic-shifts. Our next step is to extend the current corpus, using online semi-supervised learning. A first sub-corpus will be released via the ELRA repository as part of this submission."
E14-1074,Cluster-based Prediction of User Ratings for Stylistic Surface Realisation,2014,32,15,4,0.717638,26443,nina dethlefs,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Surface realisations typically depend on their target style and audience. A challenge in estimating a stylistic realiser from data is that humans vary significantly in their subjective perceptions of linguistic forms and styles, leading to almost no correlation between ratings of the same utterance. We address this problem in two steps. First, we estimate a mapping function between the linguistic features of a corpus of utterances and their human style ratings. Users are partitioned into clusters based on the similarity of their ratings, so that ratings for new utterances can be estimated, even for new, unknown users. In a second step, the estimated model is used to re-rank the outputs of a number of surface realisers to produce stylistically adaptive output. Results confirm that the generated styles are recognisable to human judges and that predictive models based on clusters of users lead to better rating predictions than models based on an average population of users."
W13-4026,"Demonstration of the {PARLANCE} system: a data-driven incremental, spoken dialogue system for interactive search",2013,6,21,12,0.50845,1049,helen hastie,Proceedings of the {SIGDIAL} 2013 Conference,0,"The Parlance system for interactive search processes dialogue at a microturn level, displaying dialogue phenomena that play a vital role in human spoken conversation. These dialogue phenomena include more natural turn-taking through rapid system responses, generation of backchannels, and user barge-ins. The Parlance demonstration system dierentiates from other incremental systems in that it is data-driven with an infrastructure that scales well."
W12-1509,Optimising Incremental Generation for Spoken Dialogue Systems: Reducing the Need for Fillers,2012,23,16,3,0.717638,26443,nina dethlefs,{INLG} 2012 Proceedings of the Seventh International Natural Language Generation Conference,0,"Recent studies have shown that incremental systems are perceived as more reactive, natural, and easier to use than non-incremental systems. However, previous work on incremental NLG has not employed recent advances in statistical optimisation using machine learning. This paper combines the two approaches, showing how the update, revoke and purge operations typically used in incremental approaches can be implemented as state transitions in a Markov Decision Process. We design a model of incremental NLG that generates output based on micro-turn interpretations of the user's utterances and is able to optimise its decisions using statistical machine learning. We present a proof-of-concept study in the domain of Information Presentation (IP), where a learning agent faces the trade-off of whether to present information as soon as it is available (for high reactiveness) or else to wait until input ASR hypotheses are more reliable. Results show that the agent learns to avoid long waiting times, fillers and self-corrections, by re-ordering content based on its confidence."
D12-1008,Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems,2012,37,30,3,0.717638,26443,nina dethlefs,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Incremental processing allows system designers to address several discourse phenomena that have previously been somewhat neglected in interactive systems, such as backchannels or barge-ins, but that can enhance the responsiveness and naturalness of systems. Unfortunately, prior work has focused largely on deterministic incremental decision making, rendering system behaviour less flexible and adaptive than is desirable. We present a novel approach to incremental decision making that is based on Hierarchical Reinforcement Learning to achieve an interactive optimisation of Information Presentation (IP) strategies, allowing the system to generate and comprehend backchannels and barge-ins, by employing the recent psycholinguistic hypothesis of information density (ID) (Jaeger, 2010). Results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are not sensitive to ID by more than 23%."
W11-2813,Adaptive Information Presentation for Spoken Dialogue Systems: Evaluation with real users,2011,26,15,1,1,6289,verena rieser,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"We present evaluation results with human subjects for a novel data-driven approach to Natural Language Generation in spoken dialogue systems. We evaluate a trained Information Presentation (IP) strategy in a deployed tourist-information spoken dialogue system. The IP problem is formulated as statistical decision making under uncertainty using Reinforcement Learning, where both content planning and attribute selection are jointly optimised based on data collected in a Wizard-of-Oz study. After earlier work testing and training this model in simulation, we now present results from an extensive online user study, involving 131 users and more than 800 test dialogues, which explores its contribution to overall 'global' task success. We find that the trained Information Presentation strategy significantly improves dialogue task completion, with up to a 9.7% increase (30% relative) compared to the deployed dialogue system which uses conventional, hand-coded presentation prompts. We also present subjective evaluation results and discuss the implications of these results for future work in dialogue management and NLG."
J11-1006,Learning and Evaluation of Dialogue Strategies for New Applications: Empirical Methods for Optimization from Small Data Sets,2011,81,32,1,1,6289,verena rieser,Computational Linguistics,0,"We present a new data-driven methodology for simulation-based dialogue strategy learning, which allows us to address several problems in the field of automatic optimization of dialogue strategies: learning effective dialogue strategies when no initial data or system exists, and determining a data-driven reward function. In addition, we evaluate the result with real users, and explore how results transfer between simulated and real interactions. We use Reinforcement Learning (RL) to learn multimodal dialogue strategies by interaction with a simulated environment which is bootstrapped from small amounts of Wizard-of-Oz (WOZ) data. This use of WOZ data allows data-driven development of optimal strategies for domains where no working prototype is available. Using simulation-based RL allows us to find optimal policies which are not (necessarily) present in the original data. Our results show that simulation-based RL significantly outperforms the average (human wizard) strategy as learned from the data by using Supervised Learning. The bootstrapped RL-based policy gains on average 50 times more reward when tested in simulation, and almost 18 times more reward when interacting with real users. Users also subjectively rate the RL-based policy on average 10% higher. We also show that results from simulated interaction do transfer to interaction with real users, and we explicitly evaluate the stability of the data-driven reward function."
W10-4235,Generation Under Uncertainty,2010,22,6,3,0,2551,oliver lemon,Proceedings of the 6th International Natural Language Generation Conference,0,"We invite the research community to consider challenges for NLG which arise from uncertainty. NLG systems should be able to adapt to their audience and the generation environment in general, but often the important features for adaptation are not known precisely. We explore generation challenges which could employ simulated environments to study NLG which is adaptive under uncertainty, and suggest possible metrics for such tasks. It would be particularly interesting to explore how different planning approaches to NLG perform in challenges involving uncertainty in the generation environment."
P10-1103,Optimising Information Presentation for Spoken Dialogue Systems,2010,35,34,1,1,6289,verena rieser,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We present a novel approach to Information Presentation (IP) in Spoken Dialogue Systems (SDS) using a data-driven statistical optimisation framework for content planning and attribute selection. First we collect data in a Wizard-of-Oz (WoZ) experiment and use it to build a supervised model of human behaviour. This forms a baseline for measuring the performance of optimised policies, developed from this data using Reinforcement Learning (RL) methods. We show that the optimised policies significantly outperform the baselines in a variety of generation scenarios: while the supervised model is able to attain up to 87.6% of the possible reward on this task, the RL policies are significantly better in 5 out of 6 scenarios, gaining up to 91.5% of the total possible reward. The RL policies perform especially well in more complex scenarios. We are also the first to show that adding predictive lower level features (e.g. from the NLG realiser) is important for optimising IP strategies according to user preferences. This provides new insights into the nature of the IP problem for SDS."
E09-1078,Natural Language Generation as Planning Under Uncertainty for Spoken Dialogue Systems,2009,37,66,1,1,6289,verena rieser,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"We present and evaluate a new model for Natural Language Generation (NLG) in Spoken Dialogue Systems, based on statistical planning, given noisy feedback from the current generation context (e.g. a user and a surface realiser). We study its use in a standard NLG problem: how to present information (in this case a set of search results) to users, given the complex tradeoffs between utterance length, amount of information conveyed, and cognitive load. We set these trade-offs by analysing existing match data. We then train a NLG policy using Reinforcement Learning (RL), which adapts its behaviour to noisy feedback from the current generation context. This policy is compared to several baselines derived from previous work in this area. The learned policy significantly outperforms all the prior approaches."
P08-1073,Learning Effective Multimodal Dialogue Strategies from {W}izard-of-{O}z Data: Bootstrapping and Evaluation,2008,32,67,1,1,6289,verena rieser,Proceedings of ACL-08: HLT,1,"We address two problems in the field of automatic optimization of dialogue strategies: learning effective dialogue strategies when no initial data or system exists, and evaluating the result with real users. We use Reinforcement Learning (RL) to learn multimodal dialogue strategies by interaction with a simulated environment which is xe2x80x9cbootstrappedxe2x80x9d from small amounts of Wizard-of-Oz (WOZ) data. This use of WOZ data allows development of optimal strategies for domains where no working prototype is available. We compare the RL-based strategy against a supervised strategy which mimics the wizardsxe2x80x99 policies. This comparison allows us to measure relative improvement over the training data. Our results show that RL significantly outperforms Supervised Learning when interacting in simulation as well as for interactions with real users. The RL-based policy gains on average 50-times more reward when tested in simulation, and almost 18-times more reward when interacting with real users. Users also subjectively rate the RL-based policy on average 10% higher."
rieser-lemon-2008-automatic,Automatic Learning and Evaluation of User-Centered Objective Functions for Dialogue System Optimisation,2008,15,14,1,1,6289,verena rieser,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The ultimate goal when building dialogue systems is to satisfy the needs of real users, but quality assurance for dialogue strategies is a non-trivial problem. The applied evaluation metrics and resulting design principles are often obscure, emerge by trial-and-error, and are highly context dependent. This paper introduces data-driven methods for obtaining reliable objective functions for system design. In particular, we test whether an objective function obtained from Wizard-of-Oz (WOZ) data is a valid estimate of real usersÂ preferences. We test this in a test-retest comparison between the model obtained from the WOZ study and the models obtained when testing with real users. We can show that, despite a low fit to the initial data, the objective function obtained from WOZ data makes accurate predictions for automatic dialogue evaluation, and, when automatically optimising a policy using these predictions, the improvement over a strategy simply mimicking the data becomes clear from an error analysis."
W06-2711,The {SAMMIE} Multimodal Dialogue Corpus Meets the Nite {XML} Toolkit,2006,5,5,2,0.862467,11454,ivana kruijffkorbayova,Proceedings of the 5th Workshop on {NLP} and {XML} ({NLPXML}-2006): Multi-Dimensional Markup in Natural Language Processing,0,"We demonstrate work in progress using the Nite XML Toolkit on a corpus of multimodal dialogues with an MP3 player collected in a Wizard-of-Oz (WOZ) experiments and annotated with a rich feature set at several layers. We designed an NXT data model, converted experiment log file data and manual transcriptions into NXT, and are building annotation tools using NXT libraries."
P06-2085,Using Machine Learning to Explore Human Multimodal Clarification Strategies,2006,17,28,1,1,6289,verena rieser,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"We investigate the use of machine learning in combination with feature engineering techniques to explore human multimodal clarification strategies and the use of those strategies for dialogue systems. We learn from data collected in a Wizard-of-Oz study where different wizards could decide whether to ask a clarification request in a multimodal manner or else use speech alone. We show that there is a uniform strategy across wizards which is based on multiple features in the context. These are generic runtime features which can be implemented in dialogue systems. Our prediction models achieve a weighted f-score of 85.3% (which is a 25.5% improvement over a one-rule baseline). To assess the effects of models, feature discretisation, and selection, we also conduct a regression analysis. We then interpret and discuss the use of the learnt strategy for dialogue systems. Throughout the investigation we discuss the issues arising from using small initial Wizard-of-Oz data sets, and we show that feature engineering is an essential step when learning from such limited data."
kruijff-korbayova-etal-2006-sammie,The {SAMMIE} Corpus of Multimodal Dialogues with an {MP}3 Player,2006,9,11,7,0.862467,11454,ivana kruijffkorbayova,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We describe a corpus of multimodal dialogues with an MP3player collected in Wizard-of-Oz experiments and annotated with a richfeature set at several layers. We are using the Nite XML Toolkit (NXT) to represent and further process the data. We designed an NXTdata model, converted experiment log file data and manualtranscriptions into NXT, and are building tools for additionalannotation using NXT libraries. The annotated corpus will be used to (i) investigate various aspects of multimodal presentation andinteraction strategies both within and across annotation layers; (ii) design an initial policy for reinforcement learning of multimodalclarification requests."
W05-1624,An Experiment Setup for Collecting Data for Adaptive Output Planning in a Multimodal Dialogue System,2005,12,14,4,0,11454,ivana kruijffkorbayova,Proceedings of the Tenth {E}uropean Workshop on Natural Language Generation ({ENLG}-05),0,"We describe a Wizard-of-Oz experiment setup for the collection of multimodal interaction data for a Music Player application. This setup was developed and used to collect experimental data as part of a project aimed at building a flexible multimodal dialogue system which provides an interface to an MP3 player, combining speech and screen input and output. Besides the usual goal of WOZ data collection to get realistic examples of the behavior and expectations of the users, an equally important goal for us was to observe natural behavior of multiple wizards in order to guide our system development. The wizardsxe2x80x99 responses were therefore not constrained by a script. One of the challenges we had to address was to allow the wizards to produce varied screen output a in real time. Our setup includes a preliminary screen output planning module, which prepares several versions of possible screen output. The wizards were free to speak, and/or to select a screen output."
P05-1030,Implications for Generating Clarification Requests in Task-Oriented Dialogues,2005,12,39,1,1,6289,verena rieser,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"Clarification requests (CRs) in conversation ensure and maintain mutual understanding and thus play a crucial role in robust dialogue interaction. In this paper, we describe a corpus study of CRs in task-oriented dialogue and compare our findings to those reported in two prior studies. We find that CR behavior in task-oriented dialogue differs significantly from that in everyday conversation in a number of ways. Moreover, the dialogue type, the modality and the channel quality all influence the decision of when to clarify and at which level of the grounding process. Finally we identify form-function correlations which can inform the generation of CRs."
2005.sigdial-1.11,A Corpus Collection and Annotation Framework for Learning Multimodal Clarification Strategies,2005,13,36,1,1,6289,verena rieser,Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue,0,Current dialogue systems are fairly poor in generating the wide range of clarification strategies as found in human-human dialogue. The overall aim of this work is to learn when and how to best employ different types of clarification strategies in multimodal dialogue systems. This paper describes a framework for learning multimodal clarification strategies for an in-car MP3 music player dialogue system. The framework consists of three major parts. First we collect data on multimodal clarification strategies in a wizard-of-oz study. Second we extract feature in the stateaction space to learn an initial policy from this data. Third we specify a reward function to refine that policy using extensions of existing evaluation schemes.
