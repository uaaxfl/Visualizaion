2021.wassa-1.5,"Emotion Ratings: How Intensity, Annotation Confidence and Agreements are Entangled",2021,-1,-1,2,1,410,enrica troiano,"Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"When humans judge the affective content of texts, they also implicitly assess the correctness of such judgment, that is, their confidence. We hypothesize that people{'}s (in)confidence that they performed well in an annotation task leads to (dis)agreements among each other. If this is true, confidence may serve as a diagnostic tool for systematic differences in annotations. To probe our assumption, we conduct a study on a subset of the Corpus of Contemporary American English, in which we ask raters to distinguish neutral sentences from emotion-bearing ones, while scoring the confidence of their answers. Confidence turns out to approximate inter-annotator disagreements. Further, we find that confidence is correlated to emotion intensity: perceiving stronger affect in text prompts annotators to more certain classification performances. This insight is relevant for modelling studies of intensity, as it opens the question wether automatic regressors or classifiers actually predict intensity, or rather human{'}s self-perceived confidence."
2021.wassa-1.6,Disentangling Document Topic and Author Gender in Multiple Languages: Lessons for Adversarial Debiasing,2021,-1,-1,2,1,413,erenay dayanik,"Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Text classification is a central tool in NLP. However, when the target classes are strongly correlated with other textual attributes, text classification models can pick up {``}wrong{''} features, leading to bad generalization and biases. In social media analysis, this problem surfaces for demographic user classes such as language, topic, or gender, which influence the generate text to a substantial extent. Adversarial training has been claimed to mitigate this problem, but thorough evaluation is missing. In this paper, we experiment with text classification of the correlated attributes of document topic and author gender, using a novel multilingual parallel corpus of TED talk transcripts. Our findings are: (a) individual classifiers for topic and author gender are indeed biased; (b) debiasing with adversarial training works for topic, but breaks down for author gender; (c) gender debiasing results differ across languages. We interpret the result in terms of feature space overlap, highlighting the role of linguistic surface realization of the target classes."
2021.spnlp-1.6,Using Hierarchical Class Structure to Improve Fine-Grained Claim Classification,2021,-1,-1,7,1,413,erenay dayanik,Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021),0,"The analysis of public debates crucially requires the classification of political demands according to hierarchical \textit{claim ontologies} (e.g. for immigration, a supercategory {``}Controlling Migration{''} might have subcategories {``}Asylum limit{''} or {``}Border installations{''}). A major challenge for automatic claim classification is the large number and low frequency of such subclasses. We address it by jointly predicting pairs of matching super- and subcategories. We operationalize this idea by (a) encoding soft constraints in the claim classifier and (b) imposing hard constraints via Integer Linear Programming. Our experiments with different claim classifiers on a German immigration newspaper corpus show consistent performance increases for joint prediction, in particular for infrequent categories and discuss the complementarity of the two approaches."
2020.nlpcss-1.3,Swimming with the Tide? Positional Claim Detection across Political Text Types,2020,-1,-1,4,1,1034,nico blokker,Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,0,"Manifestos are official documents of political parties, providing a comprehensive topical overview of the electoral programs. Voters, however, seldom read them and often prefer other channels, such as newspaper articles, to understand the party positions on various policy issues. The natural question to ask is how compatible these two formats (manifesto and newspaper reports) are in their representation of party positioning. We address this question with an approach that combines political science (manual annotation and analysis) and natural language processing (supervised claim identification) in a cross-text type setting: we train a classifier on annotated newspaper data and test its performance on manifestos. Our findings show a) strong performance for supervised classification even across text types and b) a substantive overlap between the two formats in terms of party positioning, with differences regarding the salience of specific issues."
2020.lrec-1.104,{R}i{Q}u{A}: A Corpus of Rich Quotation Annotation for {E}nglish Literary Text,2020,-1,-1,2,1,14987,sean papay,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We introduce RiQuA (RIch QUotation Annotations), a corpus that provides quotations, including their interpersonal structure (speakers and addressees) for English literary text. The corpus comprises 11 works of 19th-century literature that were manually doubly annotated for direct and indirect quotations. For each quotation, its span, speaker, addressee, and cue are identified (if present). This provides a rich view of dialogue structures not available from other available corpora. We detail the process of creating this dataset, discuss the annotation guidelines, and analyze the resulting corpus in terms of inter-annotator agreement and its properties. RiQuA, along with its annotations guidelines and associated scripts, are publicly available for use, modification, and experimentation."
2020.lrec-1.115,{DE}bate{N}et-mig15:Tracing the 2015 Immigration Debate in {G}ermany Over Time,2020,-1,-1,7,1,628,gabriella lapesa,Proceedings of the 12th Language Resources and Evaluation Conference,0,"DEbateNet-migr15 is a manually annotated dataset for German which covers the public debate on immigration in 2015. The building block of our annotation is the political science notion of a claim, i.e., a statement made by a political actor (a politician, a party, or a group of citizens) that a specific action should be taken (e.g., vacant flats should be assigned to refugees). We identify claims in newspaper articles, assign them to actors and fine-grained categories and annotate their polarity and date. The aim of this paper is two-fold: first, we release the full DEbateNet-mig15 corpus and document it by means of a quantitative and qualitative analysis; second, we demonstrate its application in a discourse network analysis framework, which enables us to capture the temporal dynamics of the political debate"
2020.emnlp-main.396,Dissecting Span Identification Tasks with Performance Prediction,2020,-1,-1,3,1,14987,sean papay,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Span identification (in short, span ID) tasks such as chunking, NER, or code-switching detection, ask models to identify and classify relevant spans in a text. Despite being a staple of NLP, and sharing a common structure, there is little insight on how these tasks{'} properties influence their difficulty, and thus little guidance on what model families work well on span ID tasks, and why. We analyze span ID tasks via performance prediction, estimating how well neural architectures do on different tasks. Our contributions are: (a) we identify key properties of span ID tasks that can inform performance prediction; (b) we carry out a large-scale experiment on English data, building a model to predict performance for unseen span ID tasks that can support architecture choices; (c), we investigate the parameters of the meta model, yielding new insights on how model and task properties interact to affect span ID performance. We find, e.g., that span frequency is especially important for LSTMs, and that CRFs help when spans are infrequent and boundaries non-distinctive."
2020.coling-main.384,Lost in Back-Translation: Emotion Preservation in Neural Machine Translation,2020,-1,-1,3,1,410,enrica troiano,Proceedings of the 28th International Conference on Computational Linguistics,0,"Machine translation provides powerful methods to convert text between languages, and is therefore a technology enabling a multilingual world. An important part of communication, however, takes place at the non-propositional level (e.g., politeness, formality, emotions), and it is far from clear whether current MT methods properly translate this information. This paper investigates the specific hypothesis that the non-propositional level of emotions is at least partially lost in MT. We carry out a number of experiments in a back-translation setup and establish that (1) emotions are indeed partially lost during translation; (2) this tendency can be reversed almost completely with a simple re-ranking approach informed by an emotion classifier, taking advantage of diversity in the n-best list; (3) the re-ranking approach can also be applied to change emotions, obtaining a model for emotion style transfer. An in-depth qualitative analysis reveals that there are recurring linguistic changes through which emotions are toned down or amplified, such as change of modality."
2020.acl-main.404,Masking Actor Information Leads to Fairer Political Claims Detection,2020,-1,-1,2,1,413,erenay dayanik,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"A central concern in Computational Social Sciences (CSS) is fairness: where the role of NLP is to scale up text analysis to large corpora, the quality of automatic analyses should be as independent as possible of textual properties. We analyze the performance of a state-of-the-art neural model on the task of political claims detection (i.e., the identification of forward-looking statements made by political actors) and identify a strong frequency bias: claims made by frequent actors are recognized better. We propose two simple debiasing methods which mask proper names and pronouns during training of the model, thus removing personal information bias. We find that (a) these methods significantly decrease frequency bias while keeping the overall performance stable; and (b) the resulting models improve when evaluated in an out-of-domain setting."
W19-4816,Modeling Paths for Explainable Knowledge Base Completion,2019,0,1,2,0,24014,josua stadelmaier,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,0,"A common approach in knowledge base completion (KBC) is to learn representations for entities and relations in order to infer missing facts by generalizing existing ones. A shortcoming of standard models is that they do not explain their predictions to make them verifiable easily to human inspection. In this paper, we propose the Context Path Model (CPM) which generates explanations for new facts in KBC by providing sets of \textit{context paths} as supporting evidence for these triples. For example, a new triple (Theresa May, nationality, Britain) may be explained by the path (Theresa May, born in, Eastbourne, contained in, Britain). The CPM is formulated as a wrapper that can be applied on top of various existing KBC models. We evaluate it for the well-established TransE model. We observe that its performance remains very close despite the added complexity, and that most of the paths proposed as explanations provide meaningful evidence to assess the correctness."
W19-3614,Learning Trilingual Dictionaries for {U}rdu {--} {R}oman {U}rdu {--} {E}nglish,2019,-1,-1,2,0,24384,moiz rauf,Proceedings of the 2019 Workshop on Widening NLP,0,"In this paper, we present an effort to generate a joint Urdu, Roman Urdu and English trilingual lexicon using automated methods. We make a case for using statistical machine translation approaches and parallel corpora for dictionary creation. To this purpose, we use word alignment tools on the corpus and evaluate translations using human evaluators. Despite different writing script and considerable noise in the corpus our results show promise with over 85{\%} accuracy of Roman Urdu{--}Urdu and 45{\%} English{--}Urdu pairs."
W19-2502,Clustering-Based Article Identification in Historical Newspapers,2019,0,2,3,0.364358,24663,martin riedl,"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"This article focuses on the problem of identifying articles and recovering their text from within and across newspaper pages when OCR just delivers one text file per page. We frame the task as a segmentation plus clustering step. Our results on a sample of 1912 New York Tribune magazine shows that performing the clustering based on similarities computed with word embeddings outperforms a similarity measure based on character n-grams and words. Furthermore, the automatic segmentation based on the text results in low scores, due to the low quality of some OCRed documents."
W19-0425,Frame Identification as Categorization: Exemplars vs Prototypes in Embeddingland,2019,0,0,2,1,24897,jennifer sikos,Proceedings of the 13th International Conference on Computational Semantics - Long Papers,0,"Categorization is a central capability of human cognition, and a number of theories have been developed to account for properties of categorization. Even though many tasks in semantics also involve categorization of some kind, theories of categorization do not play a major role in contemporary research in computational linguistics. This paper follows the idea that embedding-based models of semantics lend themselves well to being formulated in terms of classical categorization theories. The benefit is a space of model families that enables (a) the formulation of hypotheses about the impact of major design decisions, and (b) a transparent assessment of these decisions. We instantiate this idea on the task of frame-semantic frame identification. We define four models that cross two design variables: (a) the choice of prototype vs. exemplar categorization, corresponding to different degrees of generalization applied to the input; and (b) the presence vs. absence of a fine-tuning step, corresponding to generic vs. task-adaptive categorization. We find that for frame identification, generalization and task-adaptive categorization both yield substantial benefits. Our prototype-based, fine-tuned model, which combines the best choices for these variables, establishes a new state of the art in frame identification."
R19-1103,Quotation Detection and Classification with a Corpus-Agnostic Model,2019,0,0,2,1,14987,sean papay,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"The detection of quotations (i.e., reported speech, thought, and writing) has established itself as an NLP analysis task. However, state-of-the-art models have been developed on the basis of specific corpora and incorpo- rate a high degree of corpus-specific assumptions and knowledge, which leads to fragmentation. In the spirit of task-agnostic modeling, we present a corpus-agnostic neural model for quotation detection and evaluate it on three corpora that vary in language, text genre, and structural assumptions. The model (a) approaches the state-of-the-art on the corpora when using established feature sets and (b) shows reasonable performance even when us- ing solely word forms, which makes it applicable for non-standard (i.e., historical) corpora."
R19-1137,Text-Based Joint Prediction of Numeric and Categorical Attributes of Entities in Knowledge Bases,2019,0,0,3,0,25366,thejas,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Collaboratively constructed knowledge bases play an important role in information systems, but are essentially always incomplete. Thus, a large number of models has been developed for Knowledge Base Completion, the task of predicting new attributes of entities given partial descriptions of these entities. Virtually all of these models either concentrate on numeric attributes ({\textless}Italy,GDP,2T{\$}{\textgreater}) or they concentrate on categorical attributes ({\textless}Tim Cook,chairman,Apple{\textgreater}). In this paper, we propose a simple feed-forward neural architecture to jointly predict numeric and categorical attributes based on embeddings learned from textual occurrences of the entities in question. Following insights from multi-task learning, our hypothesis is that due to the correlations among attributes of different kinds, joint prediction improves over separate prediction. Our experiments on seven FreeBase domains show that this hypothesis is true of the two attribute types: we find substantial improvements for numeric attributes in the joint model, while performance remains largely unchanged for categorical attributes. Our analysis indicates that this is the case because categorical attributes, many of which describe membership in various classes, provide useful {`}background knowledge{'} for numeric prediction, while this is true to a lesser degree in the inverse direction."
P19-3018,An Environment for Relational Annotation of Political Debates,2019,0,0,6,0.754823,1033,andre blessing,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"This paper describes the MARDY corpus annotation environment developed for a collaboration between political science and computational linguistics. The tool realizes the complete workflow necessary for annotating a large newspaper text collection with rich information about claims (demands) raised by politicians and other actors, including claim and actor spans, relations, and polarities. In addition to the annotation GUI, the tool supports the identification of relevant documents, text pre-processing, user management, integration of external knowledge bases, annotation comparison and merging, statistical analysis, and the incorporation of machine learning models as {``}pseudo-annotators{''}."
P19-1273,Who Sides with Whom? Towards Computational Construction of Discourse Networks for Political Debates,2019,0,0,1,1,411,sebastian pado,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Understanding the structures of political debates (which actors make what claims) is essential for understanding democratic political decision making. The vision of computational construction of such discourse networks from newspaper reports brings together political science and natural language processing. This paper presents three contributions towards this goal: (a) a requirements analysis, linking the task to knowledge base population; (b) an annotated pilot corpus of migration claims based on German newspaper reports; (c) initial modeling results."
P19-1391,Crowdsourcing and Validating Event-focused Emotion Corpora for {G}erman and {E}nglish,2019,20,1,2,1,410,enrica troiano,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Sentiment analysis has a range of corpora available across multiple languages. For emotion analysis, the situation is more limited, which hinders potential research on crosslingual modeling and the development of predictive models for other languages. In this paper, we fill this gap for German by constructing deISEAR, a corpus designed in analogy to the well-established English ISEAR emotion dataset. Motivated by Scherer{'}s appraisal theory, we implement a crowdsourcing experiment which consists of two steps. In step 1, participants create descriptions of emotional events for a given emotion. In step 2, five annotators assess the emotion expressed by the texts. We show that transferring an emotion classification model from the original English ISEAR to the German crowdsourced deISEAR via machine translation does not, on average, cause a performance drop."
W18-3813,Using Embeddings to Compare {F}rame{N}et Frames Across Languages,2018,-1,-1,2,1,24897,jennifer sikos,Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing,0,"Much interest in Frame Semantics is fueled by the substantial extent of its applicability across languages. At the same time, lexicographic studies have found that the applicability of individual frames can be diminished by cross-lingual divergences regarding polysemy, syntactic valency, and lexicalization. Due to the large effort involved in manual investigations, there are so far no broad-coverage resources with {``}problematic{''} frames for any language pair. Our study investigates to what extent multilingual vector representations of frames learned from manually annotated corpora can address this need by serving as a wide coverage source for such divergences. We present a case study for the language pair English {---} German using the FrameNet and SALSA corpora and find that inferences can be made about cross-lingual frame applicability using a vector space model."
W18-1204,Addressing Low-Resource Scenarios with Character-aware Embeddings,2018,0,1,2,1,14987,sean papay,Proceedings of the Second Workshop on Subword/Character {LE}vel Models,0,"Most modern approaches to computing word embeddings assume the availability of text corpora with billions of words. In this paper, we explore a setup where only corpora with millions of words are available, and many words in any new text are out of vocabulary. This setup is both of practical interests {--} modeling the situation for specific domains and low-resource languages {--} and of psycholinguistic interest, since it corresponds much more closely to the actual experiences and challenges of human language learning and use. We compare standard skip-gram word embeddings with character-based embeddings on word relatedness prediction. Skip-grams excel on large corpora, while character-based embeddings do well on small corpora generally and rare and complex words specifically. The models can be combined easily."
P18-2020,A Named Entity Recognition Shootout for {G}erman,2018,0,6,2,0.364358,24663,martin riedl,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We ask how to practically build a model for German named entity recognition (NER) that performs at the state of the art for both contemporary and historical texts, i.e., a big-data and a small-data scenario. The two best-performing model families are pitted against each other (linear-chain CRFs and BiLSTM) to observe the trade-off between expressiveness and data requirements. BiLSTM outperforms the CRF when large datasets are available and performs inferior for the smallest dataset. BiLSTMs profit substantially from transfer learning, which enables them to be trained on multiple corpora, resulting in a new state-of-the-art model for German NER on two contemporary German corpora (CoNLL 2003 and GermEval 2014) and two historic corpora."
N18-2033,Lexical Substitution for Evaluating Compositional Distributional Models,2018,0,0,2,0,17079,maja buljan,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"Compositional Distributional Semantic Models (CDSMs) model the meaning of phrases and sentences in vector space. They have been predominantly evaluated on limited, artificial tasks such as semantic sentence similarity on hand-constructed datasets. This paper argues for lexical substitution (LexSub) as a means to evaluate CDSMs. LexSub is a more natural task, enables us to evaluate meaning composition at the level of individual words, and provides a common ground to compare CDSMs with dedicated LexSub models. We create a LexSub dataset for CDSM evaluation from a corpus with manual {``}all-words{''} LexSub annotation. Our experiments indicate that the Practical Lexical Function CDSM outperforms simple component-wise CDSMs and performs on par with the context2vec LexSub model using the same context."
D18-2008,{DERE}: A Task and Domain-Independent Slot Filling Framework for Declarative Relation Extraction,2018,0,0,4,0,3876,heike adel,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"Most machine learning systems for natural language processing are tailored to specific tasks. As a result, comparability of models across tasks is missing and their applicability to new tasks is limited. This affects end users without machine learning experience as well as model developers. To address these limitations, we present DERE, a novel framework for declarative specification and compilation of template-based information extraction. It uses a generic specification language for the task and for data annotations in terms of spans and frames. This formalism enables the representation of a large variety of natural language processing challenges. The backend can be instantiated by different models, following different paradigms. The clear separation of frame specification and model backend will ease the implementation of new models and the evaluation of different models across different tasks. Furthermore, it simplifies transfer learning, joint learning across tasks and/or domains as well as the assessment of model generalizability. DERE is available as open-source software."
W17-6904,Living a discrete life in a continuous world: Reference in cross-modal entity tracking,2017,22,0,2,0,11383,gemma boleda,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,None
W17-6922,Are doggies really nicer than dogs? The impact of morphological derivation on emotional valence in {G}erman,2017,-1,-1,2,1,628,gabriella lapesa,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,None
W17-6928,Modeling Derivational Morphology in {U}krainian,2017,16,0,4,0,31327,mariia melymuka,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,None
W17-5203,"Annotation, Modelling and Analysis of Fine-Grained Emotions on a Stance and Sentiment Detection Corpus",2017,35,11,4,0,12073,hendrik schuff,"Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"There is a rich variety of data sets for sentiment analysis (viz., polarity and subjectivity classification). For the more challenging task of detecting discrete emotions following the definitions of Ekman and Plutchik, however, there are much fewer data sets, and notably no resources for the social media domain. This paper contributes to closing this gap by extending the \textit{SemEval 2016 stance and sentiment dataset}with emotion annotation. We (a) analyse annotation reliability and annotation merging; (b) investigate the relation between emotion annotation and the other annotation layers (stance, sentiment); (c) report modelling results as a baseline for future work."
W17-2203,Investigating the Relationship between Literary Genres and Emotional Plot Development,2017,-1,-1,2,0,17015,evgeny kim,"Proceedings of the Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"Literary genres are commonly viewed as being defined in terms of content and stylistic features. In this paper, we focus on one particular class of lexical features, namely emotion information, and investigate the hypothesis that emotion-related information correlates with particular genres. Using genre classification as a testbed, we compare a model that computes lexicon-based emotion scores globally for complete stories with a model that tracks emotion arcs through stories on a subset of Project Gutenberg with five genres. Our main findings are: (a), the global emotion model is competitive with a large-vocabulary bag-of-words genre classifier (80{\%}F1); (b), the emotion arc model shows a lower performance (59 {\%} F1) but shows complementary behavior to the global model, as indicated by a very good performance of an oracle model (94 {\%} F1) and an improved performance of an ensemble model (84 {\%} F1); (c), genres differ in the extent to which stories follow the same emotional arcs, with particularly uniform behavior for anger (mystery) and fear (adventures, romance, humor, science fiction)."
S17-1012,"Distributed Prediction of Relations for Entities: The Easy, The Difficult, and The Impossible",2017,0,1,3,1,5573,abhijeet gupta,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"Word embeddings are supposed to provide easy access to semantic relations such as {``}male of{''} (man{--}woman). While this claim has been investigated for concepts, little is known about the distributional behavior of relations of (Named) Entities. We describe two word embedding-based models that predict values for relational attributes of entities, and analyse them. The task is challenging, with major performance differences between relations. Contrary to many NLP tasks, high difficulty for a relation does not result from low frequency, but from (a) one-to-many mappings; and (b) lack of context patterns expressing the relation that are easy to pick up by word embeddings."
S17-1014,Does Free Word Order Hurt? Assessing the Practical Lexical Function Model for {C}roatian,2017,0,0,3,0,15426,zoran medic,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"The Practical Lexical Function (PLF) model is a model of computational distributional semantics that attempts to strike a balance between expressivity and learnability in predicting phrase meaning and shows competitive results. We investigate how well the PLF carries over to free word order languages, given that it builds on observations of predicate-argument combinations that are harder to recover in free word order languages. We evaluate variants of the PLF for Croatian, using a new lexical substitution dataset. We find that the PLF works about as well for Croatian as for English, but demonstrate that its strength lies in modeling verbs, and that the free word order affects the less robust PLF variant."
E17-2013,Instances and concepts in distributional space,2017,25,6,3,0,11383,gemma boleda,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Instances ({``}Mozart{''}) are ontologically distinct from concepts or classes ({``}composer{''}). Natural language encompasses both, but instances have received comparatively little attention in distributional semantics. Our results show that instances and concepts differ in their distributional properties. We also establish that instantiation detection ({``}Mozart {--} composer{''}) is generally easier than hypernymy detection ({``}chemist {--} scientist{''}), and that results on the influence of input representation do not transfer from hyponymy to instantiation."
W16-2015,Predicting the Direction of Derivation in {E}nglish Conversion,2016,16,4,4,1,31328,max kisselew,"Proceedings of the 14th {SIGMORPHON} Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"Conversion is a word formation operation that changes the grammatical category of a word in the absence of overt morphology. Conversion is extremely productive in English (e.g., tunnel, talk). This paper investigates whether distributional information can be used to predict the diachronic direction of conversion for homophonous nounxe2x80x90verb pairs. We aim to predict, for example, that tunnel was used as a noun prior to its use as a verb. We test two hypotheses: (1) that derived forms are less frequent than their bases, and (2) that derived forms are more semantically specific than their bases, as approximated by information theoretic measures. We find that hypothesis (1) holds for N-to-V conversion, while hypothesis (2) holds for V-to-N conversion. We achieve the best overall account of the historical data by taking both frequency and semantic specificity into account. These results provide a new perspective on linguistic theories regarding the semantic specificity of derivational morphemes, and on the morphosyntactic status of conversion."
S16-2010,Improving Zero-Shot-Learning for {G}erman Particle Verbs by using Training-Space Restrictions and Local Scaling,2016,9,1,4,0,28599,maximilian koper,Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics,0,None
P16-1164,Model Architectures for Quotation Detection,2016,14,2,3,0,34519,christian scheible,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
C16-1122,Predictability of Distributional Semantics in Derivational Word Formation,2016,30,4,1,1,411,sebastian pado,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Compositional distributional semantic models (CDSMs) have successfully been applied to the task of predicting the meaning of a range of linguistic constructions. Their performance on semi-compositional word formation process of (morphological) derivation, however, has been extremely variable, with no large-scale empirical investigation to date. This paper fills that gap, performing an analysis of CDSM predictions on a large dataset (over 30,000 German derivationally related word pairs). We use linear regression models to analyze CDSM performance and obtain insights into the linguistic factors that influence how predictable the distributional context of a derived word is going to be. We identify various such factors, notably part of speech, argument structure, and semantic regularity."
W15-0108,Obtaining a Better Understanding of Distributional Models of {G}erman Derivational Morphology,2015,19,7,2,1,31328,max kisselew,Proceedings of the 11th International Conference on Computational Semantics,0,"Predicting the (distributional) meaning of derivationally related words (read / reader) from one another has recently been recognized as an instance of distributional compositional meaning construction. However, the properties of this task are not yet well understood. In this paper, we present an analysis of two such composition models on a set of German derivation patterns (e.g., -in, durch-). We begin by introducing a rank-based evaluation metric, which reveals the task to be challenging due to specific properties of German (compounding, capitalization). We also find that performance varies greatly between patterns and even among base-derived term pairs of the same pattern. A regression analysis shows that semantic coherence of the base and derived terms within a pattern, as well as coherence of the semantic shifts from base to derived terms, all significantly impact prediction quality."
S15-1005,Combining Seemingly Incompatible Corpora for Implicit Semantic Role Labeling,2015,37,8,2,0,37305,parvin feizabadi,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"Implicit semantic role labeling, the task of retrieving locally unrealized arguments from wider discourse context, is a knowledgeintensive task. At the same time, the annotated corpora that exist are all small and scattered across different annotation frameworks, genres, and classes of predicates. Previous work has treated these corpora as incompatible with one another, and has concentrated on optimizing the exploitation of single corpora. In this paper, we show that corpus combination is effective after all when the differences between corpora are bridged with domain adaptation methods. When we combine the SemEval-2010 Task 10 and Gerber and Chai noun corpora, we obtain substantially improved performance on both corpora, for all roles and parts of speech. We also present new insights into the properties of the implicit semantic role labeling task."
S15-1017,Dissecting the Practical Lexical Function Model for Compositional Distributional Semantics,2015,13,1,3,1,5573,abhijeet gupta,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"The Practical Lexical Function model (PLF) is a recently proposed compositional distributional semantic model which provides an elegant account of composition, striking a balance between expressiveness and robustness and performing at the state-of-the-art. In this paper, we identify an inconsistency in PLF between the objective function at training and the prediction at testing which leads to an overcounting of the predicatexe2x80x99s contribution to the meaning of the phrase. We investigate two possible solutions of which one (the exclusion of simple lexical vector at test time) improves performance significantly on two out of the three composition datasets."
S15-1022,Multi-Level Alignments As An Extensible Representation Basis for Textual Entailment Algorithms,2015,20,9,2,0,37318,taegil noh,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"A major problem in research on Textual Entailment (TE) is the high implementation effort for TE systems. Recently, interoperable standards for annotation and preprocessing have been proposed. In contrast, the algorithmic level remains unstandardized, which makes component re-use in this area very difficult in practice. In this paper, we introduce multi-level alignments as a central, powerful representation for TE algorithms that encourages modular, reusable, multilingual algorithm development. We demonstrate that a pilot open-source implementation of multi-level alignment with minimal features competes with state-of-theart open-source TE engines in three languages."
D15-1002,Distributional vectors encode referential attributes,2015,30,20,4,1,5573,abhijeet gupta,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Distributional methods have proven to excel at capturing fuzzy, graded aspects of meaning (Italy is more similar to Spain than to Germany). In contrast, it is difficult to extract the values of more specific attributes of word referents from distributional representations, attributes of the kind typically found in structured knowledge bases (Italy has 60 million inhabitants). In this paper, we pursue the hypothesis that distributional vectors also implicitly encode referential attributes. We show that a standard supervised regression model is in fact sufficient to retrieve such attributes to a reasonable degree of accuracy: When evaluated on the prediction of both categorical and numeric attributes of countries and cities, the model consistently reduces baseline error by 30%, and is not far from the upper bound. Further analysis suggests that our model is able to xe2x80x9cobjectifyxe2x80x9d distributional representations for entities, anchoring them more firmly in the external world in measurable ways."
Q14-1020,Crosslingual and Multilingual Construction of Syntax-Based Vector Space Models,2014,50,6,2,1,31309,jason utt,Transactions of the Association for Computational Linguistics,0,"Syntax-based distributional models of lexical semantics provide a flexible and linguistically adequate representation of co-occurrence information. However, their construction requires large, accurately parsed corpora, which are unavailable for most languages. In this paper, we develop a number of methods to overcome this obstacle. We describe (a) a crosslingual approach that constructs a syntax-based model for a new language requiring only an English resource and a translation lexicon; and (b) multilingual approaches that combine crosslingual with monolingual information, subject to availability. We evaluate on two lexical semantic benchmarks in German and Croatian. We find that the models exhibit complementary profiles: crosslingual models yield higher accuracies while monolingual models provide better coverage. In addition, we show that simple multilingual models can successfully combine their strengths."
P14-5008,The Excitement Open Platform for Textual Inferences,2014,13,42,7,0,1501,bernardo magnini,Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"This paper presents the Excitement Open Platform (EOP), a generic architecture and a comprehensive implementation for textual inference in multiple languages. The platform includes state-of-art algorithms, a large number of knowledge resources, and facilities for experimenting and testing innovative approaches. The EOP is distributed as an open source software."
frontini-etal-2014-polysemy,Polysemy Index for Nouns: an Experiment on {I}talian using the {PAROLE} {SIMPLE} {CLIPS} Lexical Database,2014,12,1,3,0,19254,francesca frontini,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"An experiment is presented to induce a set of polysemous basic type alternations (such as Animal-Food, or Building-Institution) by deriving them from the sense alternations found in an existing lexical resource. The paper builds on previous work and applies those results to the Italian lexicon PAROLE SIMPLE CLIPS. The new results show how the set of frequent type alternations that can be induced from the lexicon is partly different from the set of polysemy relations selected and explicitely applied by lexicographers when building it. The analysis of mismatches shows that frequent type alternations do not always correpond to prototypical polysemy relations, nevertheless the proposed methodology represents a useful tool offered to lexicographers to systematically check for possible gaps in their resource."
E14-4044,Crowdsourcing Annotation of Non-Local Semantic Roles,2014,13,5,2,0,37305,parvin feizabadi,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"This paper reports on a study of crowdsourcing the annotation of non-local (or implicit) frame-semantic roles, i.e., roles that are realized in the previous discourse context. We describe two annotation setups (marking and gap filling) and find that gap filling works considerably better, attaining an acceptable quality relatively cheaply. The produced data is available for research purposes."
E14-1057,What Substitutes Tell Us - Analysis of an {``}All-Words{''} Lexical Substitution Corpus,2014,33,23,3,0,40088,gerhard kremer,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present the first large-scale English xe2x80x9callwords lexical substitutionxe2x80x9d corpus. The size of the corpus provides a rich resource for investigations into word meaning. We investigate the nature of lexical substitute sets, comparing them to WordNet synsets. We find them to be consistent with, but more fine-grained than, synsets. We also identify significant differences to results for paraphrase ranking in context reported for the SEMEVAL lexical substitution data. This highlights the influence of corpus construction approaches on evaluation results."
C14-1163,Towards Semantic Validation of a Derivational Lexicon,2014,28,4,2,1,40299,britta zeller,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Derivationally related lemmas like friendN xe2x80x90 friendlyA xe2x80x90 friendshipN are derived from a common stem. Frequently, their meanings are also systematically related. However, there are also many examples of derivationally related lemma pairs whose meanings differ substantially, e.g., objectN xe2x80x90 objective N . Most broad-coverage derivational lexicons do not reflect this distinction, mixing up semantically related and unrelated word pairs. In this paper, we investigate strategies to recover the above distinction by recognizing semantically related lemma pairs, a process we call semantic validation. We make two main contributions: First, we perform a detailed data analysis on the basis of a large German derivational lexicon. It reveals two promising sources of information (distributional semantics and structural information about derivational rules), but also systematic problems with these sources. Second, we develop a classification model for the task that reflects the noisy nature of the data. It achieves an improvement of 13.6% in precision and 5.8% in F1-score over a strong majority class baseline. Our experiments confirm that both information sources contribute to semantic validation, and that they are complementary enough that the best results are obtained from a combined model."
W13-3810,Design and Realization of the {EXCITEMENT} Open Platform for Textual Entailment,2013,0,0,2,0,23452,gunter neumann,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,None
W13-0604,The Curious Case of Metonymic Verbs: A Distributional Characterization,2013,21,5,3,1,31309,jason utt,Proceedings of the {IWCS} 2013 Workshop Towards a Formal Distributional Semantics,0,"Logical metonymy combines an event-selecting verb with an entity-denoting noun (e.g., The writer began the novel), triggering a covert event interpretation (e.g., reading, writing). Experimental investigations of logical metonymy must assume a binary distinction between metonymic (i.e. eventselecting) verbs and non-metonymic verbs to establish a control condition. However, this binary distinction (whether a verb is metonymic or not) is mostly made on intuitive grounds, which introduces a potential confounding factor. We describe a corpus-based approach which characterizes verbs in terms of their behavior at the syntax-semantics interface. The model assesses the extent to which transitive verbs prefer event-denoting objects over entity-denoting objects. We then test this xe2x80x9ceventhoodxe2x80x9d measure on psycholinguistic datasets, showing that it can distinguish not only metonymic from non-metonymic verbs, but that it can also capture more fine-grained distinctions among different classes of metonymic verbs, putting such distinctions into a new graded perspective."
W13-0216,"Fitting, Not Clashing! A Distributional Semantic Model of Logical Metonymy",2013,21,5,3,1,2924,alessandra zarcone,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Short Papers,0,"Logical metonymy interpretation (e.g. begin the book xe2x86x92 writing) has received wide attention in linguistics. Experimental results have shown higher processing costs for metonymic conditions compared with non-metonymic ones (read the book). According to a widely held interpretation, it is the type clash between the event-selecting verb and the entity-denoting object (begin the book) that triggers coercion mechanisms and leads to additional processing effort. We propose an alternative explanation and argue that the extra processing effort is an effect of thematic fit. This is a more economical hypothesis that does not need to postulate a separate type clash mechanism: entitydenoting objects simply have a low fit as objects of event-selecting verbs. We test linguistic datasets from psycholinguistic experiments and find that a structured distributional model of thematic fit, which does not encode any explicit argument type information, is able to replicate all significant experimental findings. This result provides evidence for a graded account of coercion phenomena in which thematic fit accounts for both the trigger of the coercion and the retrieval of the covert event."
W13-0116,A corpus study of clause combination,2013,21,0,2,0,41174,olga nikitina,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Long Papers,0,"We present a corpus-based investigation of cases of clause combination that can be expressed both through coordination or with subordination. We analyse the data with a two-step computational model which first distinguishes subordination from coordination and then determines the direction for cases of subordination. We find that a wide range of features help with the prediction, notably frequency of predicate participants, presence of adjuncts and sharing of participants between the clause predicates."
W13-0125,A Search Task Dataset for {G}erman Textual Entailment,2013,23,3,2,1,40299,britta zeller,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Long Papers,0,"We present the first freely available large German dataset for Textual Entailment (TE). Our dataset builds on posts from German online forums concerned with computer problems and models the task of identifying relevant posts for user queries (i.e., descriptions of their computer problems) through TE. We use a sequence of crowdsourcing tasks to create realistic problem descriptions through summarisation and paraphrasing of forum posts. The dataset is represented in RTE-5 Search task style and consists of 172 positive and over 2800 negative pairs. We analyse the properties of the created dataset and evaluate its difficulty by applying two TE algorithms and comparing the results with results on the English RTE-5 Search task. The results show that our dataset is roughly comparable to the RTE-5 data in terms of both difficulty and balancing of positive and negative entailment pairs. Our approach to create task-specific TE datasets can be transferred to other domains and languages."
P13-2128,Derivational Smoothing for Syntactic Distributional Semantics,2013,29,7,1,1,411,sebastian pado,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Syntax-based vector spaces are used widely in lexical semantics and are more versatile than word-based spaces (Baroni and Lenci, 2010). However, they are also sparse, with resulting reliability and coverage problems. We address this problem by derivational smoothing, which uses knowledge about derivationally related words (oldish! old) to improve semantic similarity estimates. We develop a set of derivational smoothing methods and evaluate them on two lexical semantics tasks in German. Even for models built from very large corpora, simple derivational smoothing can improve coverage considerably."
P13-2137,Building and Evaluating a Distributional Memory for {C}roatian,2013,23,7,2,0.178571,23915,jan vsnajder,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We report on the first structured distributional semantic model for Croatian, DM.HR. It is constructed after the model of the English Distributional Memory (Baroni and Lenci, 2010), from a dependencyparsed Croatian web corpus, and covers about 2M lemmas. We give details on the linguistic processing and the design principles. An evaluation shows state-of-theart performance on a semantic similarity task with particularly good performance on nouns. The resource is freely available."
P13-1118,{DE}riv{B}ase: Inducing and Evaluating a Derivational Morphology Resource for {G}erman,2013,38,20,3,1,40299,britta zeller,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Derivational models are still an underresearched area in computational morphology. Even for German, a rather resourcerich language, there is a lack of largecoverage derivational knowledge. This paper describes a rule-based framework for inducing derivational families (i.e., clusters of lemmas in derivational relationships) and its application to create a highcoverage German resource, DERIVBASE, mapping over 280k lemmas into more than 17k non-singleton clusters. We focus on the rule component and a qualitative and quantitative evaluation. Our approach achieves up to 93% precision and 71% recall. We attribute the high precision to the fact that our rules are based on information from grammar books."
2013.mtsummit-european.5,Bridges Across the Language Divide {---} {EU}-{BRIDGE} Excitement: Exploring Customer Interactions through Textual {E}ntail{MENT},2013,-1,-1,4,0,955,ido dagan,Proceedings of Machine Translation Summit XIV: European projects,0,None
2013.mtsummit-european.6,Excitement: Exploring Customer Interactions through Textual {E}ntail{MENT},2013,-1,-1,4,0,955,ido dagan,Proceedings of Machine Translation Summit XIV: European projects,0,None
W12-1707,Modeling covert event retrieval in logical metonymy: probabilistic and distributional accounts,2012,42,8,3,1,2924,alessandra zarcone,Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics ({CMCL} 2012),0,"Logical metonymies (The student finished the beer) represent a challenge to compositionality since they involve semantic content not overtly realized in the sentence (covert events xe2x86x92 drinking the beer). We present a contrastive study of two classes of computational models for logical metonymy in German, namely a probabilistic and a distributional, similarity-based model. These are built using the SDeWaC corpus and evaluated against a dataset from a self-paced reading and a probe recognition study for their sensitivity to thematic fit effects via their accuracy in predicting the correct covert event in a metonymical context. The similarity-based models allow for better coverage while maintaining the accuracy of the probabilistic models."
S12-1023,Regular polysemy: A distributional model,2012,42,14,2,0,11383,gemma boleda,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"Many types of polysemy are not word specific, but are instances of general sense alternations such as ANIMAL-FOOD. Despite their pervasiveness, regular alternations have been mostly ignored in empirical computational semantics. This paper presents (a) a general framework which grounds sense alternations in corpus data, generalizes them above individual words, and allows the prediction of alternations for new words; and (b) a concrete unsupervised implementation of the framework, the Centroid Attribute Model. We evaluate this model against a set of 2,400 ambiguous words and demonstrate that it outperforms two baselines."
todirascu-etal-2012-french,{F}rench and {G}erman Corpora for Audience-based Text Type Classification,2012,12,1,2,0,15684,amalia todirascu,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,This paper presents some of the results of the CLASSYN project which investigated the classification of text according to audience-related text types. We describe the design principles and the properties of the French and German linguistically annotated corpora that we have created. We report on tools used to collect the data and on the quality of the syntactic annotation. The CLASSYN corpora comprise two text collections to investigate general text types difference between scientific and popular science text on the two domains of medical and computer science.
E12-1064,Towards a model of formal and informal address in {E}nglish,2012,33,7,2,1,8204,manaal faruqui,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Informal and formal (T/V) address in dialogue is not distinguished overtly in modern English, e.g. by pronoun choice like in many other languages such as French (tu/vous). Our study investigates the status of the T/V distinction in English literary texts. Our main findings are: (a) human raters can label monolingual English utterances as T or V fairly well, given sufficient context; (b), a bilingual corpus can be exploited to induce a supervised classifier for T/V without human annotation. It assigns T/V at sentence level with up to 68% accuracy, relying mainly on lexical features; (c), there is a marked asymmetry between lexical features for formal speech (which are conventionalized and therefore general) and informal speech (which are text-specific)."
W11-3605,Soundex-based Translation Correction in {U}rdu{--}{E}nglish Cross-Language Information Retrieval,2011,20,3,3,1,8204,manaal faruqui,Proceedings of the Fifth International Workshop On Cross Lingual Information Access,0,"Cross-language information retrieval is difficult for languages with few processing tools or resources such as Urdu. An easy way of translating content words is provided by Google Translate, but due to lexicon limitations named entities (NEs) are transliterated letter by letter. The resulting NEs errors (zynydyny zdn for Zinedine Zidane) hurts retrieval. We propose to replace English non-words in the translation output. First, we determine phonetically similar English words with the Soundex algorithm. Then, we choose among them by a modified Levenshtein distance that models correct transliteration patterns. This strategy yields an improvement of 4% MAP (from 41.2 to 45.1, monolingual 51.4) on the FIRE-2010 dataset."
W11-0111,Acquiring entailment pairs across languages and domains: A Data Analysis,2011,14,4,2,1,8204,manaal faruqui,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"Entailment pairs are sentence pairs of a premise and a hypothesis, where the premise textually entails the hypothesis. Such sentence pairs are important for the development of Textual Entailment systems. In this paper, we take a closer look at a prominent strategy for their automatic acquisition from newspaper corpora, pairing first sentences of articles with their titles. We propose a simple logistic regression model that incorporates and extends this heuristic and investigate its robustness across three languages and three domains. We manage to identify two predictors which predict entailment pairs with a fairly high accuracy across all languages. However, we find that robustness across domains within a language is more difficult to achieve."
W11-0128,Ontology-based Distinction between Polysemy and Homonymy,2011,12,8,2,1,31309,jason utt,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"We consider the problem of distinguishing polysemous from homonymous nouns. This distinction is often taken for granted, but is seldom operationalized in the shape of an empirical model. We present a first step towards such a model, based on WordNet augmented with ontological classes provided by CoreLex. This model provides a polysemy index for each noun which (a), accurately distinguishes between polysemy and homonymy; (b), supports the analysis that polysemy can be grounded in the frequency of the meaning shifts shown by nouns; and (c), improves a regression model that predicts when the one-sense-per-discourse hypothesis fails."
P11-2082,"{``}{I} Thou Thee, Thou Traitor{''}: Predicting Formal vs. Informal Address in {E}nglish Literature",2011,18,4,2,1,8204,manaal faruqui,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"In contrast to many languages (like Russian or French), modern English does not distinguish formal and informal (T/V) address overtly, for example by pronoun choice. We describe an ongoing study which investigates to what degree the T/V distinction is recoverable in English text, and with what textual features it correlates. Our findings are: (a) human raters can label English utterances as T or V fairly well, given sufficient context; (b), lexical cues can predict T/V almost at human level."
S10-1006,{S}em{E}val-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals,2010,4,98,6,0.418724,16715,iris hendrickx,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"SemEval-2 Task 8 focuses on Multi-way classification of semantic relations between pairs of nominals. The task was designed to compare different approaches to semantic relation classification and to provide a standard testbed for future research. This paper defines the task, describes the training and test data and the process of their creation, lists the participating systems (10 teams, 28 runs), and discusses their results."
P10-2017,Exemplar-Based Models for Word Meaning in Context,2010,20,70,2,0.629765,2274,katrin erk,Proceedings of the {ACL} 2010 Conference Short Papers,0,"This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vector-per-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models."
P10-1123,Assessing the Role of Discourse References in Entailment Inference,2010,38,25,3,0,27123,shachar mirkin,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal."
N10-1135,Cross-lingual Induction of Selectional Preferences with Bilingual Vector Spaces,2010,26,21,2,0,41633,yves peirsman,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We describe a cross-lingual method for the induction of selectional preferences for resource-poor languages, where no accurate monolingual models are available. The method uses bilingual vector spaces to translate foreign language predicate-argument structures into a resource-rich language like English. The only prerequisite for constructing the bilingual vector space is a large unparsed corpus in the resource-poor language, although the model can profit from (even noisy) syntactic knowledge. Our experiments show that the cross-lingual predictions correlate well with human ratings, clearly outperforming monolingual baseline models."
J10-4007,"A Flexible, Corpus-Driven Model of Regular and Inverse Selectional Preferences",2010,75,62,2,0.629765,2274,katrin erk,Computational Linguistics,0,"We present a vector space-based model for selectional preferences that predicts plausibility scores for argument headwords. It does not require any lexical resources (such as WordNet). It can be trained either on one corpus with syntactic annotation, or on a combination of a small semantically annotated primary corpus and a large, syntactically analyzed generalization corpus. Our model is able to predict inverse selectional preferences, that is, plausibility scores for predicates given argument heads.n n We evaluate our model on one NLP task (pseudo-disambiguation) and one cognitive task (prediction of human plausibility judgments), gauging the influence of different parameters and comparing our model against other model classes. We obtain consistent benefits from using the disambiguation and semantic role information provided by a semantically tagged primary corpus. As for parameters, we identify settings that yield good performance across a range of experimental conditions. However, frequency remains a major influence of prediction quality, and we also identify more robust parameter settings suitable for applications with many infrequent items."
W09-2501,Multi-word expressions in textual inference: Much ado about nothing?,2009,20,9,2,0,4403,mariecatherine marneffe,Proceedings of the 2009 Workshop on Applied Textual Inference ({T}ext{I}nfer),0,"Multi-word expressions (MWE) have seen much attention from the NLP community. In this paper, we investigate their impact on the recognition of textual entailment (RTE). Using the manual Microsoft Research annotations, we first manually count and classify MWEs in RTE data. We find few, most of which are arguably unlikely to cause processing problems. We then consider the impact of MWEs on a current RTE system. We are unable to confirm that entailment recognition suffers from wrongly aligned MWEs. In addition, MWE alignment is difficult to improve, since MWEs are poorly represented in state-of-the-art paraphrase resources, the only available sources for multi-word similarities. We conclude that RTE should concentrate on other phenomena impacting entailment, and that paraphrase knowledge is best understood as capturing general lexico-syntactic variation."
W09-2415,{S}em{E}val-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals,2009,28,203,6,0.418724,16715,iris hendrickx,Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions ({SEW}-2009),0,"We present a brief overview of the main challenges in the extraction of semantic relations from English text, and discuss the shortcomings of previous data sets and shared tasks. This leads us to introduce a new task, which will be part of SemEval-2010: multi-way classification of mutually exclusive semantic relations between pairs of common nominals. The task is designed to compare different approaches to the problem and to provide a standard testbed for future research, which can benefit many applications in Natural Language Processing."
W09-1201,The {C}o{NLL}-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages,2009,26,269,9,0,17503,jan hajivc,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL} 2009): Shared Task,0,"For the 11th straight year, the Conference on Computational Natural Language Learning has been accompanied by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting. In 2009, the shared task was dedicated to the joint parsing of syntactic and semantic dependencies in multiple languages. This shared task combines the shared tasks of the previous five years under a unique dependency-based formalism similar to the 2008 task. In this paper, we define the shared task, describe how the data sets were created and show their quantitative properties, report the results and summarize the approaches of the participating systems."
W09-0404,Machine Translation Evaluation with Textual Entailment Features,2009,14,7,1,1,411,sebastian pado,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,None
W09-0208,Paraphrase Assessment in Structured Vector Space: Exploring Parameters and Datasets,2009,26,30,2,0.700345,2274,katrin erk,Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,0,"The appropriateness of paraphrases for words depends often on context: grab can replace catch in catch a ball, but not in catch a cold. Structured Vector Space (SVS) (Erk and Pado, 2008) is a model that computes word meaning in context in order to assess the appropriateness of such paraphrases. This paper investigates best-practice parameter settings for SVS, and it presents a method to obtain large datasets for paraphrase assessment from corpora with WSD annotation."
P09-1034,Robust Machine Translation Evaluation with Entailment Features,2009,24,65,1,1,411,sebastian pado,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Existing evaluation metrics for machine translation lack crucial robustness: their correlations with human quality judgments vary considerably across languages and genres. We believe that the main reason is their inability to properly capture meaning: A good translation candidate means the same thing as the reference translation, regardless of formulation. We propose a metric that evaluates MT output based on a rich set of features motivated by textual entailment, such as lexical-semantic (in-)compatibility and argument structure overlap. We compare this metric against a combination metric of four state-of-the-art scores (BLEU, NIST, TER, and METEOR) in two different settings. The combination metric out-performs the individual scores, but is bested by the entailment-based metric. Combining the entailment and traditional features yields further improvements."
I08-1051,"Formalising Multi-layer Corpora in {OWL} {DL} - Lexicon Modelling, Querying and Consistency Control",2008,15,16,2,1,13863,aljoscha burchardt,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"We present a general approach to formally modelling corpora with multi-layered annotation, thereby inducing a lexicon model in a typed logical representation language, OWL DL. This model can be interpreted as a graph structure that offers flexible querying functionality beyond current XML-based query languages and powerful methods for consistency control. We illustrate our approach by applying it to the syntactically and semantically annotated SALSA/TIGER corpus."
D08-1094,A Structured Vector Space Model for Word Meaning in Context,2008,34,285,2,0.998914,2274,katrin erk,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account.n n We present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words' argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases."
C08-1084,Semantic Role Assignment for Event Nominalisations by Leveraging Verbal Data,2008,25,20,1,1,411,sebastian pado,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper presents a novel approach to the task of semantic role labelling for event nominalisations, which make up a considerable fraction of predicates in running text, but are underrepresented in terms of training data and difficult to model. We propose to address this situation by data expansion. We construct a model for nominal role labelling solely from verbal training data. The best quality results from salvaging grammatical features where applicable, and generalising over lexical heads otherwise."
J07-2002,Dependency-Based Construction of Semantic Space Models,2007,78,484,1,1,411,sebastian pado,Computational Linguistics,0,"Traditionally, vector-based semantic space models use word co-occurrence counts from large corpora to represent lexical meaning. In this article we present a novel framework for constructing semantic spaces that takes syntactic relations into account. We introduce a formalization for this class of models, which allows linguistic knowledge to guide the construction process. We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art."
D07-1042,"Flexible, Corpus-Based Modelling of Human Plausibility Judgements",2007,23,35,1,1,411,sebastian pado,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"In this paper, we consider the computational modelling of human plausibility judgements for verb-relation-argument triples, a task equivalent to the computation of selectional preferences. Such models have applications both in psycholinguistics and in computational linguistics. By extending a recent model, we obtain a completely corpus-driven model for this task which achieves significant correlations with human judgements. It rivals or exceeds deeper, resource-driven models while exhibiting higher coverage. Moreover, we show that our model can be combined with deeper models to obtain better predictions than from either model alone."
2007.jeptalnrecital-long.25,Annotation pr{\\'e}cise du fran{\\c{c}}ais en s{\\'e}mantique de r{\\^o}les par projection cross-linguistique,2007,-1,-1,1,1,411,sebastian pado,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans le paradigme FrameNet, cet article aborde le probl{\`e}me de l{'}annotation pr{\'e}cise et automatique de r{\^o}les s{\'e}mantiques dans une langue sans lexique FrameNet existant. Nous {\'e}valuons la m{\'e}thode propos{\'e}e par Pad{\'o} et Lapata (2005, 2006), fond{\'e}e sur la projection de r{\^o}les et appliqu{\'e}e initialement {\`a} la paire anglais-allemand. Nous testons sa g{\'e}n{\'e}ralisabilit{\'e} du point de vue (a) des langues, en l{'}appliquant {\`a} la paire (anglais-fran{\c{c}}ais) et (b) de la qualit{\'e} de la source, en utilisant une annotation automatique du c{\^o}t{\'e} anglais. Les exp{\'e}riences montrent des r{\'e}sultats {\`a} la hauteur de ceux obtenus pour l{'}allemand, nous permettant de conclure que cette approche pr{\'e}sente un grand potentiel pour r{\'e}duire la quantit{\'e} de travail n{\'e}cessaire {\`a} la cr{\'e}ation de telles ressources dans de nombreuses langues."
P06-1146,Optimal Constituent Alignment with Edge Covers for Semantic Projection,2006,23,50,1,1,411,sebastian pado,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Given a parallel corpus, semantic projection attempts to transfer semantic role annotations from one language to another, typically by exploiting word alignments. In this paper, we present an improved method for obtaining constituent alignments between parallel sentences to guide the role projection task. Our extensions are twofold: (a) we model constituent alignment as minimum weight edge covers in a bipartite graph, which allows us to find a globally optimal solution efficiently; (b) we propose tree pruning as a promising strategy for reducing alignment noise. Experimental results on an English-German parallel corpus demonstrate improvements over state-of-the-art models."
burchardt-etal-2006-salsa,The {SALSA} Corpus: a {G}erman Corpus Resource for Lexical Semantics,2006,23,144,5,1,13863,aljoscha burchardt,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes the SALSA corpus, a large German corpus manually annotated with manual role-semantic annotation, based on the syntactically annotated TIGER newspaper corpus. The first release, comprising about 20,000 annotated predicate instances (about half the TIGER corpus), is scheduled for mid-2006. In this paper we discuss the annotation framework (frame semantics) and its cross-lingual applicability, problems arising from exhaustive annotation, strategies for quality control, and possible applications."
burchardt-etal-2006-salto,{SALTO} - A Versatile Multi-Level Annotation Tool,2006,8,67,5,1,13863,aljoscha burchardt,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper, we describe the SALTO tool. It was originally developed for the annotation of semantic roles in the frame semantics paradigm, but can be used for graphical annotation of treebanks with general relational information in a simple drag-and-drop fashion. The tool additionally supports corpus management and quality control."
erk-pado-2006-shalmaneser,Shalmaneser - A Toolchain For Shallow Semantic Parsing,2006,19,54,2,0.8,2274,katrin erk,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper presents Shalmaneser, a software package for shallow semantic parsing, the automatic assignment of semantic classes and roles to free text. Shalmaneser is a toolchain of independent modules communicating through a common XML format. System output can be inspected graphically. Shalmaneser can be used either as a black box to obtain semantic parses for new datasets (classifiers for English and German frame-semantic analysis are included), or as a research platform that can be extended to new parsers, languages, or classification paradigms."
H05-1084,Analyzing Models for Semantic Role Assignment using Confusability,2005,19,5,2,0.75,2274,katrin erk,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We analyze models for semantic role assignment by defining a meta-model that abstracts over features and learning paradigms. This meta-model is based on the concept of role confusability, is defined in information-theoretic terms, and predicts that roles realized by less specific grammatical functions are more difficult to assign. We find that confusability is strongly correlated with the performance of classifiers based on syntactic features, but not for classifiers including semantic features. This indicates that syntactic features approximate a description of grammatical functions, and that semantic features provide an independent second view on the data."
H05-1108,Cross-linguistic Projection of Role-Semantic Information,2005,16,46,1,1,411,sebastian pado,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"This paper considers the problem of automatically inducing role-semantic annotations in the FrameNet paradigm for new languages. We introduce a general framework for semantic projection which exploits parallel texts, is relatively inexpensive and can potentially reduce the amount of effort involved in creating semantic resources. We propose projection models that exploit lexical and syntactic information. Experimental results on an English-German parallel corpus demonstrate the advantages of this approach."
W04-3214,The Influence of Argument Structure on Semantic Role Assignment,2004,17,1,1,1,411,sebastian pado,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,None
W04-2413,Semantic Role Labelling With Chunk Sequences,2004,3,10,3,0,51408,ulrike baldewein,Proceedings of the Eighth Conference on Computational Natural Language Learning ({C}o{NLL}-2004) at {HLT}-{NAACL} 2004,0,"We describe a statistical approach to semantic role labelling that employs only shallow information. We use a Maximum Entropy learner, augmented by EM-based clustering to model the fit between a verb and its argument candidate. The instances to be classified are sequences of chunks that occur frequently as arguments in the training corpus. Our best model obtains an F score of 51.70 on the test set."
W04-0817,Semantic role labelling with similarity-based generalization using {EM}-based clustering,2004,8,20,3,0,51408,ulrike baldewein,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,"We describe a system for semantic role assignment built as part of the Senseval III task, based on an off-the-shelf parser and Maxent and Memory-Based learners. We focus on generalisation using several similarity measures to increase the amount of training data available and on the use of EM-based clustering to improve role assignment. Our final score is Precision=73.6%, Recall=59.4% (F=65.7)."
erk-pado-2004-powerful,A Powerful and Versatile {XML} Format for Representing Role-semantic Annotation,2004,2,35,2,1,2274,katrin erk,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We present two XML formats for the description and encoding of semantic role information in corpora. The TIGER/SALSA XML format provides a modular representation for semantic roles and syntactic structure. The Text-SALSA XML format is a lightweight version of TIGER/SALSA XML designed for manual annotation with an XML editor rather than a special tool. Both formats can deal with underspecification, roles crossing the sentence boundary, compound splitting, and whole-sentence tags for meta-level comments."
heid-etal-2004-querying,Querying Both Time-aligned and Hierarchical Corpora with {NXT} Search,2004,6,20,6,0,24867,ulrich heid,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"One problem of the (re-)usability and exchange of annotated corpora is in the lack of standards in corpus formats and corpus query tools. This paper reports on the NXT Search tool, which was used to query two corpora with very different annotation formats. It is shown that with automatic data format conversion both corpora can be accessed and searched with NXT Search."
P03-1017,Constructing Semantic Space Models from Parsed Corpora,2003,19,40,1,1,411,sebastian pado,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,Traditional vector-based models use word co-occurrence counts from large corpora to represent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations.
P03-1068,Towards a Resource for Lexical Semantics: A Large {G}erman Corpus with Extensive Semantic Annotation,2003,10,68,3,0,2274,katrin erk,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"We describe the ongoing construction of a large, semantically annotated corpus resource as reliable basis for the large-scale acquisition of word-semantic information, e.g. the construction of domain-independent lexica. The backbone of the annotation are semantic roles in the frame semantics paradigm. We report experiences and evaluate the annotated data from the first project stage. On this basis, we discuss the problems of vagueness and ambiguity in semantic annotation."
