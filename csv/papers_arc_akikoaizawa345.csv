2021.mrqa-1.6,Can Question Generation Debias Question Answering Models? A Case Study on Question{--}Context Lexical Overlap,2021,-1,-1,3,0,5180,kazutoshi shinoda,Proceedings of the 3rd Workshop on Machine Reading for Question Answering,0,"Question answering (QA) models for reading comprehension have been demonstrated to exploit unintended dataset biases such as question{--}context lexical overlap. This hinders QA models from generalizing to under-represented samples such as questions with low lexical overlap. Question generation (QG), a method for augmenting QA datasets, can be a solution for such performance degradation if QG can properly debias QA datasets. However, we discover that recent neural QG models are biased towards generating questions with high lexical overlap, which can amplify the dataset bias. Moreover, our analysis reveals that data augmentation with these QG models frequently impairs the performance on questions with low lexical overlap, while improving that on questions with high lexical overlap. To address this problem, we use a synonym replacement-based approach to augment questions with low lexical overlap. We demonstrate that the proposed data augmentation approach is simple yet effective to mitigate the degradation problem with only 70k synthetic examples."
2021.findings-emnlp.101,Neural Media Bias Detection Using Distant Supervision With {BABE} - Bias Annotations By Experts,2021,-1,-1,6,0,6663,timo spinde,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Media coverage has a substantial effect on the public perception of events. Nevertheless, media outlets are often biased. One way to bias news articles is by altering the word choice. The automatic identification of bias by word choice is challenging, primarily due to the lack of a gold standard data set and high context dependencies. This paper presents BABE, a robust and diverse data set created by trained experts, for media bias research. We also analyze why expert labeling is essential within this domain. Our data set offers better annotation quality and higher inter-annotator agreement than existing work. It consists of 3,700 sentences balanced among topics and outlets, containing media bias labels on the word and sentence level. Based on our data, we also introduce a way to detect bias-inducing sentences in news articles automatically. Our best performing BERT-based model is pre-trained on a larger corpus consisting of distant labels. Fine-tuning and evaluating the model on our proposed supervised data set, we achieve a macro F1-score of 0.804, outperforming existing methods."
2021.findings-acl.446,Phrase-Level Action Reinforcement Learning for Neural Dialog Response Generation,2021,-1,-1,2,0,8535,takato yamazaki,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.167,Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models,2021,-1,-1,2,0,8971,taichi iki,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"A method for creating a vision-and-language (V{\&}L) model is to extend a language model through structural modifications and V{\&}L pre-training. Such an extension aims to make a V{\&}L model inherit the capability of natural language understanding (NLU) from the original language model. To see how well this is achieved, we propose to evaluate V{\&}L models using an NLU benchmark (GLUE). We compare five V{\&}L models, including single-stream and dual-stream models, trained with the same pre-training. Dual-stream models, with their higher modality independence achieved by approximately doubling the number of parameters, are expected to preserve the NLU capability better. Our main finding is that the dual-stream scores are not much different than the single-stream scores, contrary to expectation. Further analysis shows that pre-training causes the performance drop in NLU tasks with few exceptions. These results suggest that adopting a single-stream structure and devising the pre-training could be an effective method for improving the maintenance of language knowledge in V{\&}L extensions."
2021.eacl-main.137,Benchmarking Machine Reading Comprehension: A Psychological Perspective,2021,-1,-1,3,1,5181,saku sugawara,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Machine reading comprehension (MRC) has received considerable attention as a benchmark for natural language understanding. However, the conventional task design of MRC lacks explainability beyond the model interpretation, i.e., reading comprehension by a model cannot be explained in human terms. To this end, this position paper provides a theoretical basis for the design of MRC datasets based on psychology as well as psychometrics, and summarizes it in terms of the prerequisites for benchmarking MRC. We conclude that future datasets should (i) evaluate the capability of the model for constructing a coherent and grounded representation to understand context-dependent situations and (ii) ensure substantive validity by shortcut-proof questions and explanation as a part of the task design."
2021.eacl-main.170,Attention-based Relational Graph Convolutional Network for Target-Oriented Opinion Words Extraction,2021,-1,-1,3,0,10777,junfeng jiang,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Target-oriented opinion words extraction (TOWE) is a subtask of aspect-based sentiment analysis (ABSA). It aims to extract the corresponding opinion words for a given opinion target in a review sentence. Intuitively, the relation between an opinion target and an opinion word mostly relies on syntactics. In this study, we design a directed syntactic dependency graph based on a dependency tree to establish a path from the target to candidate opinions. Subsequently, we propose a novel attention-based relational graph convolutional neural network (ARGCN) to exploit syntactic information over dependency graphs. Moreover, to explicitly extract the corresponding opinion words toward the given opinion target, we effectively encode target information in our model with the target-aware representation. Empirical results demonstrate that our model significantly outperforms all of the existing models on four benchmark datasets. Extensive analysis also demonstrates the effectiveness of each component of our models. Our code is available at https://github.com/wcwowwwww/towe-eacl."
2021.eacl-main.304,Communicative-Function-Based Sentence Classification for Construction of an Academic Formulaic Expression Database,2021,-1,-1,2,1,10963,kenichi iwatsuki,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Formulaic expressions (FEs), such as {`}in this paper, we propose{'} are frequently used in scientific papers. FEs convey a communicative function (CF), i.e. {`}showing the aim of the paper{'} in the above-mentioned example. Although CF-labelled FEs are helpful in assisting academic writing, the construction of FE databases requires manual labour for assigning CF labels. In this study, we considered a fully automated construction of a CF-labelled FE database using the top{--}down approach, in which the CF labels are first assigned to sentences, and then the FEs are extracted. For the CF-label assignment, we created a CF-labelled sentence dataset, on which we trained a SciBERT classifier. We show that the classifier and dataset can be used to construct FE databases of disciplines that are different from the training data. The accuracy of in-disciplinary classification was more than 80{\%}, while cross-disciplinary classification also worked well. We also propose an FE extraction method, which was applied to the CF-labelled sentences. Finally, we constructed and published a new, large CF-labelled FE database. The evaluation of the final CF-labelled FE database showed that approximately 65{\%} of the FEs are correct and useful, which is sufficiently high considering practical use."
2021.deelio-1.14,Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals,2021,-1,-1,2,0,11262,taku sakamoto,Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,0,"Numerical common sense (NCS) is necessary to fully understand natural language text that includes numerals. NCS is knowledge about the numerical features of objects in text, such as size, weight, or color. Existing neural language models treat numerals in a text as string tokens in the same way as other words. Therefore, they cannot reflect the quantitative aspects of numerals in the training process, making it difficult to learn NCS. In this paper, we measure the NCS acquired by existing neural language models using a masked numeral prediction task as an evaluation task. In this task, we use two evaluation metrics to evaluate the language models in terms of the symbolic and quantitative aspects of the numerals, respectively. We also propose methods to reflect not only the symbolic aspect but also the quantitative aspect of numerals in the training of language models, using a loss function that depends on the magnitudes of the numerals and a regression model for the masked numeral prediction task. Finally, we quantitatively evaluate our proposed approaches on four datasets with different properties using the two metrics. Compared with methods that use existing language models, the proposed methods reduce numerical absolute errors, although exact match accuracy was reduced. This result confirms that the proposed methods, which use the magnitudes of the numerals for model training, are an effective way for models to capture NCS."
2021.acl-srw.21,Improving the Robustness of {QA} Models to Challenge Sets with Variational Question-Answer Pair Generation,2021,-1,-1,3,0,5180,kazutoshi shinoda,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"Question answering (QA) models for reading comprehension have achieved human-level accuracy on in-distribution test sets. However, they have been demonstrated to lack robustness to challenge sets, whose distribution is different from that of training sets. Existing data augmentation methods mitigate this problem by simply augmenting training sets with synthetic examples sampled from the same distribution as the challenge sets. However, these methods assume that the distribution of a challenge set is known a priori, making them less applicable to unseen challenge sets. In this study, we focus on question-answer pair generation (QAG) to mitigate this problem. While most existing QAG methods aim to improve the quality of synthetic examples, we conjecture that diversity-promoting QAG can mitigate the sparsity of training sets and lead to better robustness. We present a variational QAG model that generates multiple diverse QA pairs from a paragraph. Our experiments show that our method can improve the accuracy of 12 challenge sets, as well as the in-distribution accuracy."
2021.acl-short.109,Embracing Ambiguity: {S}hifting the Training Target of {NLI} Models,2021,-1,-1,4,0,12621,johannes meissner,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Natural Language Inference (NLI) datasets contain examples with highly ambiguous labels. While many research works do not pay much attention to this fact, several recent efforts have been made to acknowledge and embrace the existence of ambiguity, such as UNLI and ChaosNLI. In this paper, we explore the option of training directly on the estimated label distribution of the annotators in the NLI task, using a learning loss based on this ambiguity distribution instead of the gold-labels. We prepare AmbiNLI, a trial dataset obtained from readily available sources, and show it is possible to reduce ChaosNLI divergence scores when finetuning on this data, a promising first step towards learning how to capture linguistic ambiguity. Additionally, we show that training on the same amount of data but targeting the ambiguity distribution instead of gold-labels can result in models that achieve higher performance and learn better representations for downstream tasks."
2020.wosp-1.1,Virtual Citation Proximity ({VCP}): Empowering Document Recommender Systems by Learning a Hypothetical In-Text Citation-Proximity Metric for Uncited Documents,2020,-1,-1,3,0,13632,paul molloy,Proceedings of the 8th International Workshop on Mining Scientific Publications,0,"The relatedness of research articles, patents, court rulings, web pages, and other document types is often calculated with citation or hyperlink-based approaches like co-citation (proximity) analysis. The main limitation of citation-based approaches is that they cannot be used for documents that receive little or no citations. We propose Virtual Citation Proximity (VCP), a Siamese Neural Network architecture, which combines the advantages of co-citation proximity analysis (diverse notions of relatedness / high recommendation performance), with the advantage of content-based filtering (high coverage). VCP is trained on a corpus of documents with textual features, and with real citation proximity as ground truth. VCP then predicts for any two documents, based on their title and abstract, in what proximity the two documents would be co-cited, if they were indeed co-cited. The prediction can be used in the same way as real citation proximity to calculate document relatedness, even for uncited documents. In our evaluation with 2 million co-citations from Wikipedia articles, VCP achieves an MAE of 0.0055, i.e. an improvement of 20{\%} over the baseline, though the learning curve suggests that more work is needed."
2020.sdp-1.16,Towards Grounding of Formulae,2020,-1,-1,3,0,15434,takuto asakura,Proceedings of the First Workshop on Scholarly Document Processing,0,"A large amount of scientific knowledge is represented within mixed forms of natural language texts and mathematical formulae. Therefore, a collaboration of natural language processing and formula analyses, so-called mathematical language processing, is necessary to enable computers to understand and retrieve information from the documents. However, as we will show in this project, a mathematical notation can change its meaning even within the scope of a single paragraph. This flexibility makes it difficult to extract the exact meaning of a mathematical formula. In this project, we will propose a new task direction for grounding mathematical formulae. Particularly, we are addressing the widespread misconception of various research projects in mathematical information retrieval, which presume that mathematical notations have a fixed meaning within a single document. We manually annotated a long scientific paper to illustrate the task concept. Our high inter-annotator agreement shows that the task is well understood for humans. Our results indicate that it is worthwhile to grow the techniques for the proposed task to contribute to the further progress of mathematical language processing."
2020.nlpcovid19-2.13,A System for Worldwide {COVID}-19 Information Aggregation,2020,-1,-1,1,1,5182,akiko aizawa,Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020,0,"The global pandemic of COVID-19 has made the public pay close attention to related news, covering various domains, such as sanitation, treatment, and effects on education. Meanwhile, the COVID-19 condition is very different among the countries (e.g., policies and development of the epidemic), and thus citizens would be interested in news in foreign countries. We build a system for worldwide COVID-19 information aggregation containing reliable articles from 10 regions in 7 languages sorted by topics. Our reliable COVID-19 related website dataset collected through crowdsourcing ensures the quality of the articles. A neural machine translation module translates articles in other languages into Japanese and English. A BERT-based topic-classifier trained on our article-topic pair dataset helps users find their interested information efficiently by putting articles into different categories."
2020.lrec-1.212,An Evaluation Dataset for Identifying Communicative Functions of Sentences in {E}nglish Scholarly Papers,2020,-1,-1,3,1,10963,kenichi iwatsuki,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Formulaic expressions, such as {`}in this paper we propose{'}, are used by authors of scholarly papers to perform communicative functions; the communicative function of the present example is {`}stating the aim of the paper{'}. Collecting such expressions and pairing them with their communicative functions would be highly valuable for various tasks, particularly for writing assistance. However, such collection and paring in a principled and automated manner would require high-quality annotated data, which are not available. In this study, we address this shortcoming by creating a manually annotated dataset for detecting communicative functions in sentences. Starting from a seed list of labelled formulaic expressions, we retrieved new sentences from scholarly papers in the ACL Anthology and asked multiple human evaluators to label communicative functions. To show the usefulness of our dataset, we conducted a series of experiments that determined to what extent sentence representations acquired by recent models, such as word2vec and BERT, can be employed to detect communicative functions in sentences."
2020.findings-emnlp.67,A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial Expressions,2020,-1,-1,3,0,19472,takuma udagawa,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Recent models achieve promising results in visually grounded dialogues. However, existing datasets often contain undesirable biases and lack sophisticated linguistic analyses, which make it difficult to understand how well current models recognize their precise linguistic structures. To address this problem, we make two design choices: first, we focus on OneCommon Corpus (CITATION), a simple yet challenging common grounding dataset which contains minimal bias by design. Second, we analyze their linguistic structures based on spatial expressions and provide comprehensive and reliable annotation for 600 dialogues. We show that our annotation captures important linguistic structures including predicate-argument structure, modification and ellipsis. In our experiments, we assess the model{'}s understanding of these structures through reference resolution. We demonstrate that our annotation can reveal both the strengths and weaknesses of baseline models in essential levels of detail. Overall, we propose a novel framework and resource for investigating fine-grained language understanding in visually grounded dialogues."
2020.findings-emnlp.420,Language-{C}onditioned {F}eature {P}yramids for {V}isual {S}election {T}asks,2020,-1,-1,2,0,8971,taichi iki,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Referring expression comprehension, which is the ability to locate language to an object in an image, plays an important role in creating common ground. Many models that fuse visual and linguistic features have been proposed. However, few models consider the fusion of linguistic features with multiple visual features with different sizes of receptive fields, though the proper size of the receptive field of visual features intuitively varies depending on expressions. In this paper, we introduce a neural network architecture that modulates visual features with varying sizes of receptive field by linguistic features. We evaluate our architecture on tasks related to referring expression comprehension in two visual dialogue games. The results show the advantages and broad applicability of our architecture. Source code is available at https://github.com/Alab-NII/lcfp ."
2020.coling-main.368,Deconstruct to Reconstruct a Configurable Evaluation Metric for Open-Domain Dialogue Systems,2020,-1,-1,3,0,21467,vitou phy,Proceedings of the 28th International Conference on Computational Linguistics,0,"Many automatic evaluation metrics have been proposed to score the overall quality of a response in open-domain dialogue. Generally, the overall quality is comprised of various aspects, such as relevancy, specificity, and empathy, and the importance of each aspect differs according to the task. For instance, specificity is mandatory in a food-ordering dialogue task, whereas fluency is preferred in a language-teaching dialogue system. However, existing metrics are not designed to cope with such flexibility. For example, BLEU score fundamentally relies only on word overlapping, whereas BERTScore relies on semantic similarity between reference and candidate response. Thus, they are not guaranteed to capture the required aspects, i.e., specificity. To design a metric that is flexible to a task, we first propose making these qualities manageable by grouping them into three groups: understandability, sensibleness, and likability, where likability is a combination of qualities that are essential for a task. We also propose a simple method to composite metrics of each aspect to obtain a single metric called USL-H, which stands for Understandability, Sensibleness, and Likability in Hierarchy. We demonstrated that USL-H score achieves good correlations with human judgment and maintains its configurability towards different aspects and metrics."
2020.coling-main.580,Constructing A Multi-hop {QA} Dataset for Comprehensive Evaluation of Reasoning Steps,2020,-1,-1,4,0,21705,xanh ho,Proceedings of the 28th International Conference on Computational Linguistics,0,"A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by requiring a model to read multiple paragraphs to answer a given question. However, current datasets do not provide a complete explanation for the reasoning process from the question to the answer. Further, previous studies revealed that many examples in existing multi-hop datasets do not require multi-hop reasoning to answer a question. In this study, we present a new multi-hop QA dataset, called 2WikiMultiHopQA, which uses structured and unstructured data. In our dataset, we introduce the evidence information containing a reasoning path for multi-hop questions. The evidence information has two benefits: (i) providing a comprehensive explanation for predictions and (ii) evaluating the reasoning skills of a model. We carefully design a pipeline and a set of templates when generating a question-answer pair that guarantees the multi-hop steps and the quality of the questions. We also exploit the structured format in Wikidata and use logical rules to create questions that are natural but still require multi-hop reasoning. Through experiments, we demonstrate that our dataset is challenging for multi-hop models and it ensures that multi-hop reasoning is required."
2020.acl-main.105,Keyphrase Generation for Scientific Document Retrieval,2020,-1,-1,3,0,4245,florian boudin,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Sequence-to-sequence models have lead to significant progress in keyphrase generation, but it remains unknown whether they are reliable enough to be beneficial for document retrieval. This study provides empirical evidence that such models can significantly improve retrieval performance, and introduces a new extrinsic evaluation framework that allows for a better understanding of the limitations of keyphrase generation models. Using this framework, we point out and discuss the difficulties encountered with supplementing documents with -not present in text- keyphrases, and generalizing models across domains. Our code is available at https://github.com/boudinfl/ir-using-kg"
P19-1216,Unsupervised Rewriter for Multi-Sentence Compression,2019,0,3,4,0.740741,6767,yang zhao,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Multi-sentence compression (MSC) aims to generate a grammatical but reduced compression from multiple input sentences while retaining their key information. Previous dominating approach for MSC is the extraction-based word graph approach. A few variants further leveraged lexical substitution to yield more abstractive compression. However, two limitations exist. First, the word graph approach that simply concatenates fragments from multiple sentences may yield non-fluent or ungrammatical compression. Second, lexical substitution is often inappropriate without the consideration of context information. To tackle the above-mentioned issues, we present a neural rewriter for multi-sentence compression that does not need any parallel corpus. Empirical studies have shown that our approach achieves comparable results upon automatic evaluation and improves the grammaticality of compression based on human evaluation. A parallel corpus with more than 140,000 (sentence group, compression) pairs is also constructed as a by-product for future research."
S18-1126,{UC}3{M}-{NII} Team at {S}em{E}val-2018 Task 7: Semantic Relation Classification in Scientific Papers via Convolutional Neural Network,2018,0,0,3,0,26505,victor suarezpaniagua,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper reports our participation for SemEval-2018 Task 7 on extraction and classification of relationships between entities in scientific papers. Our approach is based on the use of a Convolutional Neural Network (CNN) trained on350 abstract with manually annotated entities and relations. Our hypothesis is that this deep learning model can be applied to extract and classify relations between entities for scientific papers at the same time. We use the Part-of-Speech and the distances to the target entities as part of the embedding for each word and we blind all the entities by marker names. In addition, we use sampling techniques to overcome the imbalance issues of this dataset. Our architecture obtained an F1-score of 35.4{\%} for the relation extraction task and 18.5{\%} for the relation classification task with a basic configuration of the one step CNN."
P18-2028,A Language Model based Evaluator for Sentence Compression,2018,0,8,3,0.740741,6767,yang zhao,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We herein present a language-model-based evaluator for deletion-based sentence compression and view this task as a series of deletion-and-evaluation operations using the evaluator. More specifically, the evaluator is a syntactic neural language model that is first built by learning the syntactic and structural collocation among words. Subsequently, a series of trial-and-error deletion operations are conducted on the source sentences via a reinforcement learning framework to obtain the best target compression. An empirical study shows that the proposed model can effectively generate more readable compression, comparable or superior to several strong baselines. Furthermore, we introduce a 200-sentence test set for a large-scale dataset, setting a new baseline for the future research."
L18-1373,{U}niversal {D}ependencies for {A}inu,2018,0,1,2,1,29929,hajime senuma,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-1453,What Makes Reading Comprehension Questions Easier?,2018,0,26,4,1,5181,saku sugawara,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"A challenge in creating a dataset for machine reading comprehension (MRC) is to collect questions that require a sophisticated understanding of language to answer beyond using superficial cues. In this work, we investigate what makes questions easier across recent 12 MRC datasets with three question styles (answer extraction, description, and multiple choice). We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. We then manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions. From this study, we observed that (i) the baseline performances for the hard subsets remarkably degrade compared to those of entire datasets, (ii) hard questions require knowledge inference and multiple-sentence reasoning in comparison with easy questions, and (iii) multiple-choice questions tend to require a broader range of reasoning skills than answer extraction and description questions. These results suggest that one might overestimate recent advances in MRC."
C18-1227,Using Formulaic Expressions in Writing Assistance Systems,2018,0,0,2,1,10963,kenichi iwatsuki,Proceedings of the 27th International Conference on Computational Linguistics,0,"Formulaic expressions (FEs) used in scholarly papers, such as {`}there has been little discussion about{'}, are helpful for non-native English speakers. However, it is time-consuming for users to manually search for an appropriate expression every time they want to consult FE dictionaries. For this reason, we tackle the task of semantic searches of FE dictionaries. At the start of our research, we identified two salient difficulties in this task. First, the paucity of example sentences in existing FE dictionaries results in a shortage of context information, which is necessary for acquiring semantic representation of FEs. Second, while a semantic category label is assigned to each FE in many FE dictionaries, it is difficult to predict the labels from user input, forcing users to manually designate the semantic category when searching. To address these difficulties, we propose a new framework for semantic searches of FEs and propose a new method to leverage both existing dictionaries and domain sentence corpora. Further, we expand an existing FE dictionary to consider building a more comprehensive and domain-specific FE dictionary and to verify the effectiveness of our method."
W17-0417,Toward {U}niversal {D}ependencies for {A}inu,2017,0,0,2,1,29929,hajime senuma,Proceedings of the {N}o{D}a{L}i{D}a 2017 Workshop on Universal Dependencies ({UDW} 2017),0,None
P17-2080,A Conditional Variational Framework for Dialog Generation,2017,21,19,7,0.46875,5981,xiaoyu shen,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Deep latent variable models have been shown to facilitate the response generation for open-domain dialog systems. However, these latent variables are highly randomized, leading to uncontrollable generated responses. In this paper, we propose a framework allowing conditional response generation based on specific attributes. These attributes can be either manually assigned or automatically detected. Moreover, the dialog states for both speakers are modeled separately in order to reflect personal features. We validate this framework on two different scenarios, where the attribute refers to genericness and sentiment states respectively. The experiment result testified the potential of our model, where meaningful responses can be generated in accordance with the specified attributes."
P17-1075,Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability,2017,21,10,4,1,5181,saku sugawara,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Knowing the quality of reading comprehension (RC) datasets is important for the development of natural-language understanding systems. In this study, two classes of metrics were adopted for evaluating RC datasets: prerequisite skills and readability. We applied these classes to six existing datasets, including MCTest and SQuAD, and highlighted the characteristics of the datasets according to each metric and the correlation between the two classes. Our dataset analysis suggests that the readability of RC datasets does not directly affect the question difficulty and that it is possible to create an RC dataset that is easy to read but difficult to answer."
K17-2011,Seq2seq for Morphological Reinflection: When Deep Learning Fails,2017,4,1,2,1,29929,hajime senuma,Proceedings of the {C}o{NLL} {SIGMORPHON} 2017 Shared Task: Universal Morphological Reinflection,0,None
W16-6001,An Analysis of Prerequisite Skills for Reading Comprehension,2016,5,3,2,1,5181,saku sugawara,Proceedings of the Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods,0,None
W16-3419,Measuring Cognitive Translation Effort with Activity Units,2016,5,8,4,0,23536,moritz schaeffer,Proceedings of the 19th Annual Conference of the {E}uropean Association for Machine Translation,0,None
L16-1607,Typed Entity and Relation Annotation on Computer Science Papers,2016,0,2,5,0,35318,yuka tateisi,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We describe our ongoing effort to establish an annotation scheme for describing the semantic structures of research articles in the computer science domain, with the intended use of developing search systems that can refine their results by the roles of the entities denoted by the query keys. In our scheme, mentions of entities are annotated with ontology-based types, and the roles of the entities are annotated as relations with other entities described in the text. So far, we have annotated 400 abstracts from the ACL anthology and the ACM digital library. In this paper, the scheme and the annotated dataset are described, along with the problems found in the course of annotation. We also show the results of automatic annotation and evaluate the corpus in a practical setting in application to topic extraction."
L16-1635,{E}nglish-to-{J}apanese Translation vs. Dictation vs. Post-editing: Comparing Translation Modes in a Multilingual Setting,2016,5,2,2,0,5265,michael carl,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Speech-enabled interfaces have the potential to become one of the most efficient and ergonomic environments for human-computer interaction and for text production. However, not much research has been carried out to investigate in detail the processes and strategies involved in the different modes of text production. This paper introduces and evaluates a corpus of more than 55 hours of English-to-Japanese user activity data that were collected within the ENJA15 project, in which translators were observed while writing and speaking translations (translation dictation) and during machine translation post-editing. The transcription of the spoken data, keyboard logging and eye-tracking data were recorded with Translog-II, post-processed and integrated into the CRITT Translation Process Research-DB (TPR-DB), which is publicly available under a creative commons license. The paper presents the ENJA15 data as part of a large multilingual Chinese, Danish, German, Hindi and Spanish translation process data collection of more than 760 translation sessions. It compares the ENJA15 data with the other language pairs and reviews some of its particularities."
K16-2018,Discourse Relation Sense Classification with Two-Step Classifiers,2016,2,1,2,0,32624,yusuke kido,Proceedings of the {C}o{NLL}-16 shared task,0,None
C16-2029,{S}ide{N}oter: Scholarly Paper Browsing System based on {PDF} Restructuring and Text Annotation,2016,8,6,2,0,35650,takeshi abekawa,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"In this paper, we discuss our ongoing efforts to construct a scientific paper browsing system that helps users to read and understand advanced technical content distributed in PDF. Since PDF is a format specifically designed for printing, layout and logical structures of documents are indistinguishably embedded in the file. It requires much effort to extract natural language text from PDF files, and reversely, display semantic annotations produced by NLP tools on the original page layout. In our browsing system, we tackle these issues caused by the gap between printable document and plain text. Our system provides ways to extract natural language sentences from PDF files together with their logical structures, and also to map arbitrary textual spans to their corresponding regions on page images. We setup a demonstration system using papers published in ACL anthology and demonstrate the enhanced search and refined recommendation functions which we plan to make widely available to NLP researchers."
C16-1261,"Learning Succinct Models: Pipelined Compression with {L}1-Regularization, Hashing, {E}lias-{F}ano Indices, and Quantization",2016,12,0,2,1,29929,hajime senuma,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"The recent proliferation of smart devices necessitates methods to learn small-sized models. This paper demonstrates that if there are $m$ features in total but only $n = o(\sqrt{m})$ features are required to distinguish examples, with $\Omega(\log m)$ training examples and reasonable settings, it is possible to obtain a good model in a \textit{succinct} representation using $n \log_2 \frac{m}{n} + o(m)$ bits, by using a pipeline of existing compression methods: L1-regularized logistic regression, feature hashing, Elias{--}Fano indices, and randomized quantization. An experiment shows that a noun phrase chunking task for which an existing library requires 27 megabytes can be compressed to less than 13 \textit{kilo}bytes without notable loss of accuracy."
Y15-1056,Distant-supervised Language Model for Detecting Emotional Upsurge on {T}witter,2015,20,0,4,0,751,yoshinari fujinuma,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"Event-specific twitter streams often reveal sudden spikes triggered by usersxe2x80x99 upsurge of emotions to crucial moments in the real world. Although upsurge of emotion is usually identified by a sudden rise in the number of tweets, the detection for diverse event streams is not a trivial task. In this paper, we propose a new method to extract spiking tweets with upsurge of emotions based on characteristic expressions used in tweets. The core part of our method is to use a distant-supervised language model (Spike LM) built from tweets in spikes to capture such expressions. We investigate the performance of detecting emotional spiking tweets using language models including Spike LM. Our experimental results show that the natural language expressions used in emotional upsurge fit specifically well to Spike LM."
W15-3602,Technical Term Extraction Using Measures of Neology,2015,46,0,2,0,23937,christopher norman,Proceedings of the {ACL} 2015 Workshop on Novel Computational Approaches to Keyphrase Extraction,0,This study aims to show that frequency of occurrence over time for technical terms differs from general language terms in the sense that technical terms are strongly biased to be recent occurrences ...
N15-3019,{C}ro{V}e{WA}: Crosslingual Vector-Based Writing Assistance,2015,13,2,4,0,37602,hubert soyer,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"We present an interactive web-based writing assistance system that is based on recent advances in crosslingual compositional distributed semantics. Given queries in Japanese or English, our system can retrieve semantically related sentences from high quality English corpora. By employing crosslingually constrained vector space models to represent phrases, our system naturally sidesteps several difficulties that would arise from direct word-to-text matching, and is able to provide novel functionality like the visualization of semantic relationships between phrases interlingually and intralingually."
W14-7008,{J}apanese to {E}nglish Machine Translation using Preordering and Compositional Distributed Semantics,2014,0,3,4,0,37418,sho hoshino,Proceedings of the 1st Workshop on {A}sian Translation ({WAT}2014),0,None
W14-5205,Significance of Bridging Real-world Documents and {NLP} Technologies,2014,7,0,4,0.877043,38278,tadayoshi hara,Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for {HLT},0,"Most conventional natural language processing (NLP) tools assume plain text as their input, whereas real-world documents display text more expressively, using a variety of layouts, sentence structures, and inline objects, among others. When NLP tools are applied to such text, users must first convert the text into the input/output formats of the tools. Moreover, this awkwardly obtained input typically does not allow the expected maximum performance of the NLP tools to be achieved. This work attempts to raise awareness of this issue using XML documents, where textual composition beyond plain text is given by tags. We propose a general framework for data conversion between XML-tagged text and plain text used as input/output for NLP tools and show that text sequences obtained by our framework can be much more thoroughly and efficiently processed by parsers than naively tag-removed text. These results highlight the significance of bridging real-world documents and NLP technologies."
chaimongkol-etal-2014-corpus,Corpus for Coreference Resolution on Scientific Papers,2014,7,2,2,0,39530,panot chaimongkol,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The ever-growing number of published scientific papers prompts the need for automatic knowledge extraction to help scientists keep up with the state-of-the-art in their respective fields. To construct a good knowledge extraction system, annotated corpora in the scientific domain are required to train machine learning models. As described in this paper, we have constructed an annotated corpus for coreference resolution in multiple scientific domains, based on an existing corpus. We have modified the annotation scheme from Message Understanding Conference to better suit scientific texts. Then we applied that to the corpus. The annotated corpus is then compared with corpora in general domains in terms of distribution of resolution classes and performance of the Stanford Dcoref coreference resolver. Through these comparisons, we have demonstrated quantitatively that our manually annotated corpus differs from a general-domain corpus, which suggests deep differences between general-domain texts and scientific texts and which shows that different approaches can be made to tackle coreference resolution for general texts and scientific texts."
tateisi-etal-2014-annotation,Annotation of Computer Science Papers for Semantic Relation Extrac-tion,2014,15,11,4,0.444894,35318,yuka tateisi,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We designed a new annotation scheme for formalising relation structures in research papers, through the investigation of computer science papers. The annotation scheme is based on the hypothesis that identifying the role of entities and events that are described in a paper is useful for intelligent information retrieval in academic literature, and the role can be determined by the relationship between the author and the described entities or events, and relationships among them. Using the scheme, we have annotated research abstracts from the IPSJ Journal published in Japanese by the Information Processing Society of Japan. On the basis of the annotated corpus, we have developed a prototype information extraction system which has the facility to classify sentences according to the relationship between entities mentioned, to help find the role of the entity in which the searcher is interested."
W13-2906,Modeling Comma Placement in {C}hinese Text for Better Readability using Linguistic Features and Gaze Information,2013,21,0,4,0.963294,38278,tadayoshi hara,Proceedings of the Second Workshop on Predicting and Improving Text Readability for Target Reader Populations,0,"Comma placements in Chinese text are relatively arbitrary although there are some syntactic guidelines for them. In this research, we attempt to improve the readability of text by optimizing comma placements through integration of linguistic features of text and gaze features of readers. We design a comma predictor for general Chinese text based on conditional random field models with linguistic features. After that, we build a rule-based filter for categorizing commas in text according to their contribution to readability based on the analysis of gazes of people reading text with and without commas. The experimental results show that our predictor reproduces the comma distribution in the Penn Chinese Treebank with 78.41 in F1-score and commas chosen by our filter smoothen certain gaze behaviors."
W13-2318,Relation Annotation for Understanding Research Papers,2013,17,2,4,0.444894,35318,yuka tateisi,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"We describe a new annotation scheme for formalizing relation structures in research papers. The scheme has been developed through the investigation of computer science papers. Using the scheme, we are building a Japanese corpus to help develop information extraction systems for digital libraries. We report on the outline of the annotation scheme and on annotation experiments conducted on research abstracts from the IPSJ Journal."
I13-1098,Sense Disambiguation: From Natural Language Words to Mathematical Terms,2013,15,1,4,0,11105,minhquoc nghiem,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"This paper addresses the open problem of mathematical term sense disambiguation. We introduce a method that uses a MathML parallel markup corpus to generate relevant training and testing datasets. Based on the dataset generated, we use Support Vector Machine classifier to disambiguate the sense of mathematical terms. Experimental results indicate we can generate such data automatically and with reasonable accuracy."
I13-1195,Diagnosing Causes of Reading Difficulty using {B}ayesian Networks,2013,21,4,2,1,9258,pascual martinezgomez,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"There is a need of matching text difficulty to the expected reading skill of the audience. Readability measures were developed with this objective in mind, first by psycholinguists, and more recently, by practitioners of natural language processing. A common strategy was to extract linguistic features that are good predictors of readability, and then train discriminative classification or regression models that correlate well with human judgment. But correlation does not imply causality, which is a necessary property to explain why documents are not readable. Our objective is to provide mechanisms for text producers to adjust the readability of their content. We propose the use of generative models to diagnose causes of reading difficulty, and bring closer the realization of automatic readability optimization."
W12-4905,Predicting Word Fixations in Text with a {CRF} Model for Capturing General Reading Strategies among Readers,2012,10,4,4,0.963294,38278,tadayoshi hara,Proceedings of the First Workshop on Eye-tracking and Natural Language Processing,0,"Human gaze behavior while reading text reflects a variety of strategies for precise and efficient reading. Nevertheless, the possibility of extracting and importing these strategies from gaze data into natural language processing technologies has not been explored to any extent. In this research, as a first step in this investigation, we examine the possibility of extracting reading strategies through the observation of word-based fixation behavior. Using existing gaze data, we train conditional random field models to predict whether each word is fixated by subjects. The experimental results show that, using both lexical and screen position cues, the model has a prediction accuracy of between 73% and 84% for each subject. Moreover, when focusing on the distribution of fixation/skip behavior of subjects on each word, the total similarity between the predicted and observed distributions is 0.9462, which strongly supports the possibility of capturing general reading strategies from gaze data. Title and Abstract in Japanese xe4xbaxbaxe3x81xaexe4xb8x80xe8x88xacxe7x9ax84xe3x81xaaxe6x96x87xe7xabxa0xe7x90x86xe8xa7xa3xe6x88xa6xe7x95xa5xe3x82x92xe6x8dx89xe3x81x88xe3x82x8bxe3x81x9fxe3x82x81xe3x81xae CRFxe3x83xa2xe3x83x87xe3x83xabxe3x82x92xe7x94xa8xe3x81x84xe3x81x9fxe6x96x87xe7xabxa0xe4xb8xadxe3x81xaexe5x8dx98xe8xaax9exe6xb3xa8xe8xa6x96xe4xbax88xe6xb8xac xe4xbaxbaxe9x96x93xe3x81x8cxe6x96x87xe7xabxa0xe3x82x92xe8xaaxadxe3x82x80xe9x9ax9bxe3x81xaexe8xa6x96xe7xb7x9axe8xa1x8cxe5x8bx95xe3x81xabxe3x81xafxe3x80x81xe6xadxa3xe7xa2xbaxe3x81x8bxe3x81xa4xe5x8axb9xe7x8ex87xe7x9ax84xe3x81xabxe8xaaxadxe3x82x80xe3x81x9fxe3x82x81xe3x81xaexe6xa7x98xe3x80x85xe3x81xaaxe6x88xa6xe7x95xa5xe3x81x8cxe5x8fx8dxe6x98xa0xe3x81x95xe3x82x8cxe3x81xa6 xe3x81x84xe3x82x8bxe3x80x82xe3x81x97xe3x81x8bxe3x81x97xe3x81xaaxe3x81x8cxe3x82x89xe3x80x81xe3x81x9dxe3x81xaexe6x88xa6xe7x95xa5xe3x82x92xe8xa6x96xe7xb7x9axe3x83x87xe3x83xbcxe3x82xbfxe3x81x8bxe3x82x89xe6x8axbdxe5x87xbaxe3x81x97xe3x80x81xe8x87xaaxe7x84xb6xe8xa8x80xe8xaax9exe5x87xa6xe7x90x86xe6x8ax80xe8xa1x93xe3x81xabxe5x8fx96xe3x82x8axe5x85xa5xe3x82x8cxe3x82x8bxe3x81xa8 xe3x81x84xe3x81x86xe5x8fxafxe8x83xbdxe6x80xa7xe3x81xabxe9x96xa2xe3x81x97xe3x81xa6xe3x81xafxe3x80x81xe3x81x93xe3x82x8cxe3x81xbexe3x81xa7xe3x81xbbxe3x81xa8xe3x82x93xe3x81xa9xe7xa0x94xe7xa9xb6xe3x81x95xe3x82x8cxe3x81xa6xe6x9dxa5xe3x81xaaxe3x81x8bxe3x81xa3xe3x81x9fxe3x80x82xe6x9cxacxe7xa0x94xe7xa9xb6xe3x81xa7xe3x81xafxe3x80x81xe3x81x93xe3x81xaexe5x8fxafxe8x83xbdxe6x80xa7xe3x82x92 xe7xa0x94xe7xa9xb6xe3x81x99xe3x82x8bxe3x81x9fxe3x82x81xe3x81xaexe7xacxacxe4xb8x80xe6xadxa9xe3x81xa8xe3x81x97xe3x81xa6xe3x80x81xe5x8dx98xe8xaax9exe3x83x99xe3x83xbcxe3x82xb9xe3x81xaexe6xb3xa8xe8xa6x96xe8xa1x8cxe5x8bx95xe3x81xaexe8xa6xb3xe5xafx9fxe3x82x92xe9x80x9axe3x81x97xe3x81xa6xe6x96x87xe7xabxa0xe7x90x86xe8xa7xa3xe6x88xa6xe7x95xa5xe3x81xaexe6x8axbdxe5x87xbaxe5x8fxaf xe8x83xbdxe6x80xa7xe3x82x92xe8xaaxbfxe6x9fxbbxe3x81x99xe3x82x8bxe3x80x82xe6x88x91xe3x80x85xe3x81xafxe6x97xa2xe5xadx98xe3x81xaexe8xa6x96xe7xb7x9axe3x83x87xe3x83xbcxe3x82xbfxe3x82x92xe7x94xa8xe3x81x84xe3x80x81xe5x90x84xe5x8dx98xe8xaax9exe3x81x8cxe8xa2xabxe9xa8x93xe8x80x85xe3x81xabxe3x82x88xe3x81xa3xe3x81xa6xe6xb3xa8xe8xa6x96xe3x81x95xe3x82x8cxe3x82x8bxe3x81x8bxe3x81xa9 xe3x81x86xe3x81x8bxe3x82x92xe4xbax88xe6xb8xacxe3x81x99xe3x82x8bxe6x9dxa1xe4xbbxb6xe4xbbx98xe3x81x8dxe7xa2xbaxe7x8ex87xe5xa0xb4xe3x83xa2xe3x83x87xe3x83xabxe3x82x92xe8xa8x93xe7xb7xb4xe3x81x99xe3x82x8bxe3x80x82xe5xaex9fxe9xa8x93xe3x81xa7xe3x81xafxe3x80x81xe8xaax9exe5xbdx99xe6x83x85xe5xa0xb1xe3x81xa8xe7x94xbbxe9x9dxa2xe4xbdx8dxe7xbdxaexe6x83x85xe5xa0xb1xe3x82x92xe6x89x8b xe3x81x8cxe3x81x8bxe3x82x8axe3x81xabxe3x81x99xe3x82x8bxe3x81x93xe3x81xa8xe3x81xa7xe3x80x81xe3x81x93xe3x81xaexe3x83xa2xe3x83x87xe3x83xabxe3x81x8cxe5x90x84xe8xa2xabxe9xa8x93xe8x80x85xe3x81xabxe5xafxbexe3x81x97xe3x81xa6 73%xe3x81x8bxe3x82x89 84%xe3x81xaexe4xbax88xe6xb8xacxe7xb2xbexe5xbaxa6xe3x82x92xe4xb8x8exe3x81x88xe3x82x8bxe3x81x93xe3x81xa8 xe3x81x8cxe7xa4xbaxe3x81x95xe3x82x8cxe3x82x8bxe3x80x82xe3x81x95xe3x82x89xe3x81xabxe3x80x81xe5x90x84xe5x8dx98xe8xaax9exe3x81xabxe5xafxbexe3x81x99xe3x82x8bxe8xa2xabxe9xa8x93xe8x80x85xe9x96x93xe3x81xaexe6xb3xa8xe8xa6x96/xe3x82xb9xe3x82xadxe3x83x83xe3x83x97xe3x81xaexe5x88x86xe5xb8x83xe3x81xabxe7x9dx80xe7x9bxaexe3x81x99xe3x82x8bxe3x81xa8xe3x80x81xe4xbax88xe6xb8xac xe3x81x95xe3x82x8cxe3x81x9fxe5x88x86xe5xb8x83xe3x81xa8xe5xaex9fxe9x9ax9bxe3x81xabxe8xa6xb3xe5xafx9fxe3x81x95xe3x82x8cxe3x81x9fxe5x88x86xe5xb8x83xe3x81xa8xe3x81xaexe5x85xa8xe4xbdx93xe7x9ax84xe3x81xaaxe8xbfx91xe4xbcxbcxe5xbaxa6xe3x81xaf 0.9462xe3x81xa7xe3x81x82xe3x82x8bxe3x81x93xe3x81xa8xe3x81x8cxe7xa4xbaxe3x81x95xe3x82x8cxe3x80x81xe8xa6x96xe7xb7x9a xe3x83x87xe3x83xbcxe3x82xbfxe3x81x8bxe3x82x89xe4xb8x80xe8x88xacxe7x9ax84xe3x81xaaxe6x96x87xe7xabxa0xe7x90x86xe8xa7xa3xe6x88xa6xe7x95xa5xe3x82x92xe6x8dx89xe3x81x88xe3x81x86xe3x82x8bxe5x8fxafxe8x83xbdxe6x80xa7xe3x82x92xe5xbcxb7xe3x81x8fxe8xa3x8fxe4xbbx98xe3x81x91xe3x82x8bxe5xaex9fxe9xa8x93xe7xb5x90xe6x9ex9cxe3x81xa8xe3x81xaaxe3x81xa3xe3x81xa6xe3x81x84xe3x82x8bxe3x80x82"
matsubayashi-etal-2012-building,Building {J}apanese Predicate-argument Structure Corpus using Lexical Conceptual Structure,2012,7,2,3,1,9338,yuichiroh matsubayashi,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper introduces our study on creating a Japanese corpus that is annotated using semantically-motivated predicate-argument structures. We propose an annotation framework based on Lexical Conceptual Structure (LCS), where semantic roles of arguments are represented through a semantic structure decomposed by several primitive predicates. As a first stage of the project, we extended Jackendoff 's LCS theory to increase generality of expression and coverage for verbs frequently appearing in the corpus, and successfully created LCS structures for 60 frequent Japanese predicates in Kyoto university Text Corpus (KTC). In this paper, we report our framework for creating the corpus and the current status of creating an LCS dictionary for Japanese predicates."
nanba-etal-2012-automatic,Automatic Translation of Scholarly Terms into Patent Terms Using Synonym Extraction Techniques,2012,13,2,4,0,43352,hidetsugu nanba,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Retrieving research papers and patents is important for any researcher assessing the scope of a field with high industrial relevance. However, the terms used in patents are often more abstract or creative than those used in research papers, because they are intended to widen the scope of claims. Therefore, a method is required for translating scholarly terms into patent terms. In this paper, we propose six methods for translating scholarly terms into patent terms using two synonym extraction methods: a statistical machine translation (SMT)-based method and a distributional similarity (DS)-based method. We conducted experiments to confirm the effectiveness of our method using the dataset of the Patent Mining Task from the NTCIR-7 Workshop. The aim of the task was to classify Japanese language research papers (pairs of titles and abstracts) using the IPC system at the subclass (third level), main group (fourth level), and subgroup (the fifth and most detailed level). The results showed that an SMT-based method (SMT{\_}ABST+IDF) performed best at the subgroup level, whereas a DS-based method (DS+IDF) performed best at the subclass level."
E12-1070,Framework of Semantic Role Assignment based on Extended Lexical Conceptual Structure: Comparison with {V}erb{N}et and {F}rame{N}et,2012,15,3,3,1,9338,yuichiroh matsubayashi,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Widely accepted resources for semantic parsing, such as PropBank and FrameNet, are not perfect as a semantic role labeling framework. Their semantic roles are not strictly defined; therefore, their meanings and semantic characteristics are unclear. In addition, it is presupposed that a single semantic role is assigned to each syntactic argument. This is not necessarily true when we consider internal structures of verb semantics. We propose a new framework for semantic role annotation which solves these problems by extending the theory of lexical conceptual structure (LCS). By comparing our framework with that of existing resources, including VerbNet and FrameNet, we demonstrate that our extended LCS framework can give a formal definition of semantic role labels, and that multiple roles of arguments can be represented strictly and naturally."
C12-1107,Recognizing Personal Characteristics of Readers using Eye-Movements and Text Features,2012,24,2,3,1,9258,pascual martinezgomez,Proceedings of {COLING} 2012,0,"In the present work we raise the hypothesis that eye-movements when reading texts reveal task performance, as measured by the level of understanding of the reader. With the objective of testing that hypothesis, we introduce a framework to integrate geometric information of eye-movements and text layout into natural language processing models via image processing techniques. We evidence the patterns in reading behavior between subjects with similar task performance using principal component analysis and quantify the likelihood of our hypothesis using the concept of linear separability. Finally, we point to potential applications that could benefit from these findings."
Y11-1063,Analyzing the characteristics of academic paper categories by using an index of representativeness,2011,26,0,4,0,40489,takafumi suzuki,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,"This study proposes an index of representativeness for analyzing the characteris- tics of academic paper categories. Many textual indices have been proposed in the field of computational stylistics, but all of the previous indices are limited in that they (a) focus only on the styles of the texts; (b) return an absolute value for every text, and (c) are based on the number of tokens. In this study, we propose an index of representativeness that does not have the weaknesses of the previous indices. Our index is based on the h-index that was originally proposed in the field of scientometrics. We redefine it here for textual data. We show the effectiveness of our index for analyzing the characteristics that differ between four genres and three subfields in Japanese academic papers."
P11-2083,Clustering Comparable Corpora For Bilingual Lexicon Extraction,2011,16,19,3,0,9573,bo li,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"We study in this paper the problem of enhancing the comparability of bilingual corpora in order to improve the quality of bilingual lexicons extracted from comparable corpora. We introduce a clustering-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus, and finally preserves most of the vocabulary of the original corpus. Our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained from the homogeneous corpus are of better quality than the lexicons obtained with previous approaches."
W10-3910,Mining Coreference Relations between Formulas and Text using {W}ikipedia,2010,2,13,4,0,45216,minh quoc,Proceedings of the Second Workshop on {NLP} Challenges in the Information Explosion Era ({NLPIX} 2010),0,"In this paper, we address the problem of discovering coreference relations between formulas and the surrounding text. The task is different from traditional coreference resolution because of the unique structure of the formulas. In this paper, we present an approach, which we call xe2x80x98CDF (Concept Description Formula)xe2x80x99, for mining coreference relations between formulas and the concepts that refer to them. Using Wikipedia articles as a target corpus, our approach is based on surface level text matching between formulas and text, as well as patterns that represent relationships between them. The results showed the potential of our approach for formulas and text coreference mining."
P03-1049,Analysis of Source Identified Text Corpora: Exploring the Statistics of the Reused Text and Authorship,2003,7,2,1,1,5182,akiko aizawa,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"This paper aims at providing a view of text recycled, within a short time, by the authors themselves. We first present a simple and general method for extracting reused term sequences, and then analyze several author-identified text collections to compare the statistical quantities. The ratio of recycling is also measured for each collection. Finally, related research topics are introduced together with some discussion of future research directions."
C02-1045,A Method of Cluster-Based Indexing of Textual Data,2002,12,10,1,1,5182,akiko aizawa,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper presents a framework for clustering in text-based information retrieval systems. The prominent feature of the proposed method is that documents, terms, and other related elements of textual information are clustered simultaneously into small overlapping clusters. In the paper, the mathematical formulation and implementation of the clustering method are briefly introduced, together with some experimental results."
C00-1058,Automatic Thesaurus Generation through Multiple Filtering,2000,17,13,3,0,17687,kyo kageura,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"In this paper, we propose a method of generating bilingual keyword clusters or thesauri from parallel or comparable bilingual corpora. The method combines morphological and lexical processing, bilingual word aligmnent, and graph-theoretic cluster generation. An experiment shows that the method is promising."
